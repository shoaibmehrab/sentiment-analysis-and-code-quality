id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2039660581,Is this fixed in [v0.12.2](https://github.com/EpistasisLab/tpot/releases/tag/v0.12.2)? Looks like https://github.com/EpistasisLab/tpot/pull/1331 is part of that release.,fixed like part release,issue,negative,positive,neutral,neutral,positive,positive
2017424306,"> This was with tpot 0.12.1. I just grabbed tpot 1.12.2 and will update when I get a chance to try it out it.

tpot 0.12.2 can now get past the error. Thanks!

Unfortunately, now there's a different error that I think might be hard to troubleshoot. Running on a single core, tpot starts going through pipelines. But when parallelizing one of the works throws an exception. It might be during a later pipelines (since it doesn't happen on a single core). I'll try to do some digging.

```python
""""""
Traceback (most recent call last):
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py"", line 463, in _process_worker
    r = call_item()
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py"", line 291, in __call__
    return self.fn(*self.args, **self.kwargs)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 589, in __call__
    return [func(*args, **kwargs)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 589, in <listcomp>
    return [func(*args, **kwargs)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/stopit/utils.py"", line 145, in wrapper
    result = func(*args, **kwargs)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/gp_deap.py"", line 424, in _wrapped_cross_val_score
    cv_iter = list(cv.split(features, target, groups))
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/sklearn/model_selection/_split.py"", line 808, in split
    y = check_array(y, input_name=""y"", ensure_2d=False, dtype=None)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/sklearn/utils/validation.py"", line 1097, in check_array
    array.flags.writeable = True
ValueError: cannot set WRITEABLE flag to True of this array
""""""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/base.py"", line 817, in fit
    self._pop, _ = eaMuPlusLambda(
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/gp_deap.py"", line 232, in eaMuPlusLambda
    population[:] = toolbox.evaluate(population)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/base.py"", line 1575, in _evaluate_individuals
    tmp_result_scores = parallel(
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 1952, in __call__
    return output if self.return_generator else list(output)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 1595, in _get_outputs
    yield from self._retrieve()
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 1699, in _retrieve
    self._raise_error_fast()
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 1734, in _raise_error_fast
    error_job.get_result(self.timeout)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 736, in get_result
    return self._return_or_raise()
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/joblib/parallel.py"", line 754, in _return_or_raise
    raise self._result
ValueError: cannot set WRITEABLE flag to True of this array

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/data/jcorn/autodisco/scripts/auto_tpot.py"", line 39, in <module>
    pipeline_optimizer.fit(X, y)
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/base.py"", line 864, in fit
    raise e
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/base.py"", line 855, in fit
    self._update_top_pipeline()
  File ""/home/cornlab/miniconda3/envs/jcorn/lib/python3.10/site-packages/tpot/base.py"", line 963, in _update_top_pipeline
    raise RuntimeError(
RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
```
",update get chance try get past error thanks unfortunately different error think might hard running single core going one work exception might later since happen single core try digging python recent call last file line file line return file line return file line return file line wrapper result file line list target file line split file line true set writeable flag true array exception direct cause following exception recent call last file line fit file line population population file line parallel file line return output else list output file line yield file line file line file line return file line raise set writeable flag true array handling exception another exception recent call last file line module file line fit raise file line fit file line raise pipeline yet please call fit first,issue,positive,positive,positive,positive,positive,positive
2015426318,This was with tpot 0.12.1. I just grabbed tpot 1.12.2 and will update when I get a chance to try it out it.,update get chance try,issue,negative,neutral,neutral,neutral,neutral,neutral
1986483959,"what version of tpot are you using? I was able to reproduce the issue in version 0.12.0, but not 0.12.2 (yet). I haven't nailed down exactly what the issue was but it seems to work for me on the latest version. ",version able reproduce issue version yet exactly issue work latest version,issue,negative,positive,positive,positive,positive,positive
1958468007,"The suggested fix seems to work, but the [Python docs](https://docs.python.org/3.11/library/imp.html) suggest using importlib instead of imp.
We should further assess which solution is better: the suggested `types.ModuleType` or `importlib.util.module_from_spec()` as documented [here](https://docs.python.org/3/library/importlib.html#importlib.util.module_from_spec).",fix work python suggest instead imp ass solution better,issue,negative,positive,positive,positive,positive,positive
1921089183,"I can confirm this. When I tried to run on my local machine using a fresh conda environment on Python 3.10.13, the code gave me the above error. However, when I ran it on a Kaggle environment, it worked. 

After checking around, I tried reinstalling dependencies on my local environment with the versions on the kaggle environment and it worked locally.

These are the versions that worked for me:

`pandas==2.2.0
numpy==1.24.4
matplotlib==3.7.4
seaborn==0.12.2
scikit-learn==1.2.2
scipy==1.11.4
TPOT==0.12.1
gplearn==0.4.2
torch==2.1.2
termcolor==2.4.0
sympy==1.12`

",confirm tried run local machine fresh environment python code gave error however ran environment worked around tried local environment environment worked locally worked,issue,negative,positive,neutral,neutral,positive,positive
1913354780,Assigned to Nick to keep visibility on the issue. Nick to take a look and decide if it needs to be assigned to someone else in the team. ,assigned nick keep visibility issue nick take look decide need assigned someone else team,issue,negative,neutral,neutral,neutral,neutral,neutral
1912662309,closed due to issue being empty and lacking any information,closed due issue empty information,issue,negative,negative,negative,negative,negative,negative
1883959131,Seems nice! I get this error a lot.,nice get error lot,issue,negative,positive,positive,positive,positive,positive
1880052995,"Add te method```get_scorer_names()```in ```_scorer.py```inside sklearn directory and it should work if you try to import again the TPOTClassifier:

 
    
```python
def get_scorer_names():
    """"""Get the names of all available scorers.

    These names can be passed to :func:`~sklearn.metrics.get_scorer` to
    retrieve the scorer object.

    Returns
    -------
    list of str
        Names of all available scorers.
    """"""
    return sorted(SCORERS.keys())
``` ",add te method inside directory work try import python get available retrieve scorer object list available return sorted,issue,negative,positive,positive,positive,positive,positive
1848979239,"Okay, that worked !

Thanks Pedro.
AA
__
Alejandro Ayestaran
***@***.***

> On Dec 8, 2023, at 4:43 PM, Pedro Ribeiro ***@***.***> wrote:
> 
> 
> to install via conda, you need to include the conda-forge channel.
> 
> Here is the command:
> 
> conda install -c conda-forge tpot
> 
> —
> Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/1334#issuecomment-1847932408>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BBUIC5BF5CLYLO6FNEZCYM3YIOJYHAVCNFSM6AAAAABAG4Z5C6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQNBXHEZTENBQHA>.
> You are receiving this because you authored the thread.
> 

",worked thanks pedro aa pedro wrote install via need include channel command install reply directly view thread,issue,negative,positive,positive,positive,positive,positive
1847932408,"to install via conda, you need to include the conda-forge channel.

Here is the command:

`conda install -c conda-forge tpot`",install via need include channel command install,issue,negative,neutral,neutral,neutral,neutral,neutral
1847927487,"I wonder who did you make this work?

Last login: Thu Dec  7 20:09:33 on ttys000
(base) ***@***.*** ~ % conda install tpot
Collecting package metadata (current_repodata.json): done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.
Collecting package metadata (repodata.json): done
Solving environment: unsuccessful initial attempt using frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - tpot

Current channels:

  - https://repo.anaconda.com/pkgs/main/osx-arm64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/osx-arm64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.


(base) ***@***.*** ~ % pip install tpot
Collecting tpot
  Using cached TPOT-0.12.1-py3-none-any.whl.metadata (2.0 kB)
Requirement already satisfied: numpy>=1.16.3 in ./anaconda3/lib/python3.11/site-packages (from tpot) (1.23.5)
Requirement already satisfied: scipy>=1.3.1 in ./anaconda3/lib/python3.11/site-packages (from tpot) (1.11.1)
Requirement already satisfied: scikit-learn>=0.22.0 in ./anaconda3/lib/python3.11/site-packages (from tpot) (1.3.0)
Collecting deap>=1.2 (from tpot)
  Using cached deap-1.4.1.tar.gz (1.1 MB)
  Preparing metadata (setup.py) ... done
Collecting update-checker>=0.16 (from tpot)
  Using cached update_checker-0.18.0-py3-none-any.whl (7.0 kB)
Requirement already satisfied: tqdm>=4.36.1 in ./anaconda3/lib/python3.11/site-packages (from tpot) (4.65.0)
Collecting stopit>=1.1.1 (from tpot)
  Using cached stopit-1.1.2-py3-none-any.whl
Requirement already satisfied: pandas>=0.24.2 in ./anaconda3/lib/python3.11/site-packages (from tpot) (2.0.3)
Requirement already satisfied: joblib>=0.13.2 in ./anaconda3/lib/python3.11/site-packages (from tpot) (1.2.0)
Requirement already satisfied: xgboost>=1.1.0 in ./anaconda3/lib/python3.11/site-packages (from tpot) (1.7.3)
Requirement already satisfied: python-dateutil>=2.8.2 in ./anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot) (2.8.2)
Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot) (2023.3.post1)
Requirement already satisfied: tzdata>=2022.1 in ./anaconda3/lib/python3.11/site-packages (from pandas>=0.24.2->tpot) (2023.3)
Requirement already satisfied: threadpoolctl>=2.0.0 in ./anaconda3/lib/python3.11/site-packages (from scikit-learn>=0.22.0->tpot) (2.2.0)
Requirement already satisfied: requests>=2.3.0 in ./anaconda3/lib/python3.11/site-packages (from update-checker>=0.16->tpot) (2.31.0)
Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=0.24.2->tpot) (1.16.0)
Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (3.4)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.16)
Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.11/site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2023.11.17)
Using cached TPOT-0.12.1-py3-none-any.whl (87 kB)
Building wheels for collected packages: deap
  Building wheel for deap (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [133 lines of output]
      running bdist_wheel
      running build
      running build_py
      creating build
      creating build/lib.macosx-11.1-arm64-cpython-311
      creating build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/cma.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/algorithms.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/gp.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/creator.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      copying deap/base.py -> build/lib.macosx-11.1-arm64-cpython-311/deap
      creating build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/emo.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/constraint.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/support.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/mutation.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/indicator.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/crossover.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/selection.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/init.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      copying deap/tools/migration.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools
      creating build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      copying deap/benchmarks/binary.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      copying deap/benchmarks/gp.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      copying deap/benchmarks/tools.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      copying deap/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      copying deap/benchmarks/movingpeaks.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks
      creating build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume
      copying deap/tools/_hypervolume/__init__.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume
      copying deap/tools/_hypervolume/pyhv.py -> build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume
      running build_ext
      building 'deap.tools._hypervolume.hv' extension
      creating build/temp.macosx-11.1-arm64-cpython-311
      creating build/temp.macosx-11.1-arm64-cpython-311/deap
      creating build/temp.macosx-11.1-arm64-cpython-311/deap/tools
      creating build/temp.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume
      clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/aa/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/aa/anaconda3/include -arch arm64 -I/Users/aa/anaconda3/include/python3.11 -c deap/tools/_hypervolume/_hv.c -o build/temp.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/_hv.o
      clang -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /Users/aa/anaconda3/include -arch arm64 -fPIC -O2 -isystem /Users/aa/anaconda3/include -arch arm64 -I/Users/aa/anaconda3/include/python3.11 -c deap/tools/_hypervolume/hv.cpp -o build/temp.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/hv.o
      clang++ -bundle -undefined dynamic_lookup -Wl,-rpath,/Users/aa/anaconda3/lib -L/Users/aa/anaconda3/lib -Wl,-rpath,/Users/aa/anaconda3/lib -L/Users/aa/anaconda3/lib build/temp.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/_hv.o build/temp.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/hv.o -o build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/hv.cpython-311-darwin.so
      ld: warning: duplicate -rpath '/Users/aa/anaconda3/lib' ignored
      /Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.
      !!
      
              ********************************************************************************
              Please avoid running ``setup.py`` directly.
              Instead, use pypa/build, pypa/installer or other
              standards-based tools.
      
              See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.
              ********************************************************************************
      
      !!
        self.initialize_options()
      installing to build/bdist.macosx-11.1-arm64/wheel
      running install
      running install_lib
      creating build/bdist.macosx-11.1-arm64
      creating build/bdist.macosx-11.1-arm64/wheel
      creating build/bdist.macosx-11.1-arm64/wheel/deap
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/cma.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      creating build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/emo.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/constraint.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/support.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/mutation.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/indicator.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/__init__.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      creating build/bdist.macosx-11.1-arm64/wheel/deap/tools/_hypervolume
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/__init__.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools/_hypervolume
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/hv.cpython-311-darwin.so -> build/bdist.macosx-11.1-arm64/wheel/deap/tools/_hypervolume
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/_hypervolume/pyhv.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools/_hypervolume
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/crossover.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/selection.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/init.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/tools/migration.py -> build/bdist.macosx-11.1-arm64/wheel/deap/tools
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/algorithms.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/gp.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/__init__.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      creating build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks/binary.py -> build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks/gp.py -> build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks/tools.py -> build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks/__init__.py -> build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/benchmarks/movingpeaks.py -> build/bdist.macosx-11.1-arm64/wheel/deap/benchmarks
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/creator.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      copying build/lib.macosx-11.1-arm64-cpython-311/deap/base.py -> build/bdist.macosx-11.1-arm64/wheel/deap
      running install_egg_info
      running egg_info
      writing deap.egg-info/PKG-INFO
      writing dependency_links to deap.egg-info/dependency_links.txt
      writing requirements to deap.egg-info/requires.txt
      writing top-level names to deap.egg-info/top_level.txt
      reading manifest file 'deap.egg-info/SOURCES.txt'
      reading manifest template 'MANIFEST.in'
      warning: no files found matching '*.hpp' under directory 'deap'
      warning: no files found matching '*.h' under directory 'examples'
      no previously-included directories found matching 'doc/_build'
      warning: no previously-included files matching '.DS_Store' found anywhere in distribution
      warning: no previously-included files matching '*.pyc' found anywhere in distribution
      adding license file 'LICENSE.txt'
      writing manifest file 'deap.egg-info/SOURCES.txt'
      Copying deap.egg-info to build/bdist.macosx-11.1-arm64/wheel/deap-1.4.1-py3.11.egg-info
      running install_scripts
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/private/var/folders/js/ftcwjzw169b5lwlcwmwrnv640000gn/T/pip-install-e452kuqm/deap_166390cb0d4c47a2baf2fe5a6e973c89/setup.py"", line 93, in <module>
          run_setup(True)
        File ""/private/var/folders/js/ftcwjzw169b5lwlcwmwrnv640000gn/T/pip-install-e452kuqm/deap_166390cb0d4c47a2baf2fe5a6e973c89/setup.py"", line 63, in run_setup
          setup(name='deap',
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/__init__.py"", line 107, in setup
          return distutils.core.setup(**attrs)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py"", line 185, in setup
          return run_commands(dist)
                 ^^^^^^^^^^^^^^^^^^
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/core.py"", line 201, in run_commands
          dist.run_commands()
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py"", line 969, in run_commands
          self.run_command(cmd)
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/dist.py"", line 1234, in run_command
          super().run_command(command)
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/setuptools/_distutils/dist.py"", line 988, in run_command
          cmd_obj.run()
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/wheel/bdist_wheel.py"", line 328, in run
          impl_tag, abi_tag, plat_tag = self.get_tag()
                                        ^^^^^^^^^^^^^^
        File ""/Users/aa/anaconda3/lib/python3.11/site-packages/wheel/bdist_wheel.py"", line 278, in get_tag
          assert tag in supported_tags, ""would build wheel with unsupported tag {}"".format(tag)
                 ^^^^^^^^^^^^^^^^^^^^^
      AssertionError: would build wheel with unsupported tag ('cp311', 'cp311', 'macosx_11_1_arm64')
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for deap
  Running setup.py clean for deap
Failed to build deap
ERROR: Could not build wheels for deap, which is required to install pyproject.toml-based projects
(base) ***@***.*** ~ % 

Are you sure you are using an ARM Mac?

(base) ***@***.*** ~ % uname -a
Darwin Alejandros-MacBook-Pro.local 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct  9 21:28:45 PDT 2023; root:xnu-10002.41.9~6/RELEASE_ARM64_T6020 arm64

Thanks,
AA
__
Alejandro Ayestaran
***@***.***

> On Dec 8, 2023, at 3:38 PM, Pedro Ribeiro ***@***.***> wrote:
> 
> 
> We were able to install this on a Mac with the new processor.
> 
> I would recommend trying to install Deap/tpot using conda rather than pip.
> 
> —
> Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/1334#issuecomment-1847872961>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BBUIC5GY7OXZWBERT4YHMTTYIOCDTAVCNFSM6AAAAABAG4Z5C6VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTQNBXHA3TEOJWGE>.
> You are receiving this because you authored the thread.
> 

",wonder make work last login base install package done environment unsuccessful initial attempt frozen solve flexible solve package done environment unsuccessful initial attempt frozen solve flexible solve following available current current search alternate may provide package looking navigate use search bar top page base pip install requirement already satisfied requirement already satisfied requirement already satisfied done requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied post requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied building collected building wheel error error python run successfully exit code output running running build running build running building extension clang arm arm clang arm arm warning duplicate install please avoid running directly instead use see running install running running running writing writing writing writing reading manifest file reading manifest template warning found matching directory warning found matching directory found matching warning matching found anywhere distribution warning matching found anywhere distribution license file writing manifest file running recent call last file string line module file line module file line module true file line setup file line setup return file line setup return file line file line file line super command file line file line run file line assert tag would build wheel unsupported tag tag would build wheel unsupported tag end output note error likely problem pip error building wheel running clean build error could build install base sure arm mac base kernel version mon root arm thanks aa pedro wrote able install mac new processor would recommend trying install rather pip reply directly view thread,issue,positive,positive,positive,positive,positive,positive
1847872961,"We were able to install this on a Mac with the new processor. 

I would recommend trying to install Deap/tpot using conda rather than pip. 

",able install mac new processor would recommend trying install rather pip,issue,negative,positive,positive,positive,positive,positive
1847709555,This could be due to an old version of scikit learn being used. I updated the minimum requirements for TPOT. Try updating your sklearn version to the latest and let us know if that resolved the issue.,could due old version learn used minimum try version latest let u know resolved issue,issue,negative,positive,positive,positive,positive,positive
1847677961,"I do not have a mac with the new processors, but I'll ask people on the team who do to test this out. 

In the meantime you could try the next version of TPOT, TPOT2 which can be found here: https://github.com/EpistasisLab/tpot2

This version does not use the deap package and has been working for people on our team with newer Macs.",mac new ask people team test could try next version found version use package working people team,issue,negative,positive,neutral,neutral,positive,positive
1847652359,"Oh, sorry. Right.
But now I see you did it already. Thanks.",oh sorry right see already thanks,issue,negative,negative,neutral,neutral,negative,negative
1847591047,"The setup.py file should also be updated to match. If you can make that change, I can merge this.
Thanks for the contribution!",file also match make change merge thanks contribution,issue,negative,positive,positive,positive,positive,positive
1837636249,"This error can happen when you have only one sample for a class. Since the stratifiedkfold doesn't repeat samples, there could be a fold without examples for a particular class. This can be solved by simply removing these classes. It can also happen if the labels are not encoded properly before TPOT. 

If you can share a minimum example that reproduces the issue we can take a look",error happen one sample class since repeat could fold without particular class simply removing class also happen properly share minimum example issue take look,issue,negative,positive,neutral,neutral,positive,positive
1834601862,"I would recommend trying out TPOT2, the next version of TPOT. You can find it here: https://github.com/EpistasisLab/tpot2
This version is more stable with larger datasets compared to TPOT1. There is also a `memory_limit` parameter that you can use to set the maximum amount of RAM a single pipeline can take up.

For TPOT1:
Perhaps it is simply running out of RAM and crashing?

Some suggestions:
You could try to reduce RAM usage by lowering n_jobs. 
you could try editing the configuration dictionary to use smaller/faster models.
One possibility is that fitting the pipeline is taking too long and timing out. You can increase the timeout by setting the parameter `max_eval_time_mins` .
",would recommend trying next version find version stable also parameter use set maximum amount ram single pipeline take perhaps simply running ram could try reduce ram usage lowering could try configuration dictionary use one possibility fitting pipeline taking long timing increase setting parameter,issue,positive,positive,neutral,neutral,positive,positive
1830530808,"how are you installing tpot?

When creating a new environment with anaconda, and installing with pip, I'm able to install the package very quickly with 


```
conda create -n myenv python
conda activate myenv
pip install tpot
```

Note that this installs the latest Python version, which breaks the current version tpot (#1327), but PR #1331 fixes this. In the meantime, manually installing the fix or using an older version of python should work.
",new environment anaconda pip able install package quickly create python activate pip install note latest python version current version manually fix older version python work,issue,negative,positive,positive,positive,positive,positive
1830520297,Forgot to mention this issue was brought up in #1327,forgot mention issue brought,issue,negative,neutral,neutral,neutral,neutral,neutral
1796341233,"The stacking estimator is defined here: https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/stacking_estimator.py


effectively, what it does is takes the predictions of the model and appends it to the left of the inputted data X. If its a classifier with predict_proba, the all class probabilities are also included. If you have a binary class, that means that there would be two additional columns,  one for each class. 

so in your case trans_x_t is [model 1 predicted labels, model 1 probability for class 0, model 1 probability for class 1, <original X>]


similarly

trans_x_t1 would be [model 2 predicted labels, model 2 probability for class 0, model 2 probability for class 1, <trans_x_t>]


",estimator defined effectively model left data classifier class also included binary class would two additional one class case model model probability class model probability class original similarly would model model probability class model probability class,issue,positive,positive,positive,positive,positive,positive
1756957584,"In Python 3.12, that feature was completely removed, and now I get compile errors because of it. I fixed this by manually erasing it.",python feature completely removed get compile fixed manually,issue,negative,positive,neutral,neutral,positive,positive
1753471300,"great! Glad the issue has been resolved. I would also recommend trying out our next version of TPOT, TPOT2 found here: https://github.com/EpistasisLab/tpot2",great glad issue resolved would also recommend trying next version found,issue,positive,positive,positive,positive,positive,positive
1753446553,"Alright, I confirmed sample size was the issue (e.g. no issue for tpot).

Not only that but tpot worked amazing! I was able to get +10% on an f1 score over the best hand-crafted architecture for this problem.

Thanks again for the help",alright confirmed sample size issue issue worked amazing able get score best architecture problem thanks help,issue,positive,positive,positive,positive,positive,positive
1751366254,"TPOT uses [sklearn.model_selection.KFold]([sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).KFold) for regression and [sklearn.model_selection.StratifiedKFold]([sklearn.model_selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection).StratifiedKFold) for classification. This is returned by [check_cv](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1514) in this line of the estimator, which returns an unshuffled splitter with the number of splits set by the cv parameter. Optionally, you can input your own splitter object, for example: `cv = sklearn.model_selection.StratifiedKFold(n_splits=10, shuffle=True, random_state=42) `


The data that gets passed into the fit command is then split according to the cv parameter, and the cross-validation score (computed by [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)) is computed for each pipeline. 


The example you posted uses the same strategy described in your link. As shown in the figures, the full data is split into Train and Test sets. Then, the Train set is split into n-folds for calculating the cross-validation fold. 

Whether or not you pass in your entire dataset to TPOT or create a separate test set is up to you and depends on what you are trying to do. The held-out test set may be useful for evaluating the final pipeline and comparing performance with different pipelines.


It is important to note that TPOT can become very good at overfitting the CV score itself (particularly for small datasets). Meaning that the final pipeline could have a high CV score that generalizes poorly to held-out data (or even different cv shuffles of the same data). The held-out data is one way of estimating out-of-sample performance.",regression classification returned line estimator unshuffled splitter number set parameter optionally input splitter object example data fit command split according parameter score pipeline example posted strategy link shown full data split train test train set split calculating fold whether pas entire create separate test set trying test set may useful final pipeline performance different important note become good score particularly small meaning final pipeline could high score poorly data even different data data one way performance,issue,positive,positive,positive,positive,positive,positive
1745217913,"The configuration setup is different in TPOT2. Rather than a single configuration dictionary, TPOT2 takes in three. One for the leaves, roots, and inner nodes. Additionally, we allow multiple configurations to be selected simultaneously and have broken up the configuration dictionary into modular pieces (selection, transformers, classifiers, regressors, etc). Some configurations are also not fixed and depend on the shape of your dataset. More information on how to set this up can be found in [tutorial 2 here](https://github.com/EpistasisLab/tpot2/blob/main/Tutorial/2_Defining_Search_Space_(config_dicts).ipynb).

To add a custom configuration to TPOT2, a file defining the search space can be added to the [configs folder here](https://github.com/EpistasisLab/tpot2/tree/main/tpot2/config). Then an option can be added to [this function](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/tpot_estimator/estimator_utils.py#L24) to allow it as an option for the TPOTEstimator. 

This approach could be used to add cuML support or sklearnex. 

We still need to add cuML to TPOT2, which is on the to-do list.
",configuration setup different rather single configuration dictionary three one leaf inner additionally allow multiple selected simultaneously broken configuration dictionary modular selection also fixed depend shape information set found tutorial add custom configuration file search space added folder option added function allow option approach could used add support still need add list,issue,negative,negative,neutral,neutral,negative,negative
1744903224,"Great, I can open up a PR reflecting an integration described with option 2 to continue discussion here. I see in TPOT2 the configs are a bit different in format than in the original library, and I a not seeing the cuML config or other similar custom ones - any suggestions on approach for this?",great open reflecting integration option continue discussion see bit different format original library seeing similar custom approach,issue,positive,positive,positive,positive,positive,positive
1737717040,"I would like to resolve some particular questions, but I would like to have an email to have a more personal conversation.
My email is: joel.frank.hq@gmail.com",would like resolve particular would like personal conversation,issue,positive,positive,neutral,neutral,positive,positive
1732084919,"We have shifted development towards TPOT2 (found here https://github.com/EpistasisLab/tpot2). I wrote a quick description about it in this issue #1322 .

Hopefully the new version is easier to understand. We welcome any feedback on what we have so far! If you think that something could be better organized, clearer, or better implemented, feel free to let us know by opening an issue or pull request. 

 The [Tutorials folder](https://github.com/EpistasisLab/tpot2/tree/main/Tutorial) is a good place to start to get a sense of basic features/functionality. Then the [BaseEvolver](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/evolvers/base_evolver.py#L24) contains all the logic for the evolution of individuals. And the [estimator class](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/tpot_estimator/estimator.py) basically wraps everything in an sklearn estimator format. 

Feel free to contact me with specific questions",development towards found wrote quick description issue hopefully new version easier understand welcome feedback far think something could better organized clearer better feel free let u know opening issue pull request folder good place start get sense basic logic evolution estimator class basically everything estimator format feel free contact specific,issue,positive,positive,positive,positive,positive,positive
1728196826,"we have shifted development to TPOT2, which is a refactored version of TPOT1 that is hopefully easier to work with (We will pin something about it to the issues page soon). You can find that here https://github.com/EpistasisLab/tpot2

But yes, I would be interested in exploring this. I think option 2 makes the most sense. there are other similar accelerated packages we were considering, such as cuML. Option2 would give them all the same interface. ",development version hopefully easier work pin something page soon find yes would interested exploring think option sense similar accelerated considering option would give interface,issue,positive,positive,positive,positive,positive,positive
1725784632,Just following up on this to see if it would be of interest.,following see would interest,issue,negative,neutral,neutral,neutral,neutral,neutral
1720164697,"The default scoring for TPOTRegressor is 'neg_mean_squared_error'. so tpot.score will return the neg_mean_squared_error. But you are comparing it to mean_absolute_error. 


If you want to optimize mean absolute error, you can pass that in as a scorer.

If you change your estimator to the following, you get the same results in your example.
`tpot = TPOTRegressor(generations=5, population_size=5, verbosity=2, random_state=42, scoring='neg_mean_absolute_error')`",default scoring return want optimize mean absolute error pas scorer change estimator following get example,issue,negative,negative,neutral,neutral,negative,negative
1720066284,"We would appreciate the PR! We have moved development to focus on TPOT2, which you can find here: https://github.com/EpistasisLab/tpot2 . It was rebuilt from the ground up to be more modular and easier to develop. 

In TPOT2, we have the parameters validation_fraction and validation_strategy to try to address this issue. It can hold out x% of the training data, which it will use as a validation set for the pareto front models. ",would appreciate development focus find rebuilt ground modular easier develop try address issue hold training data use validation set front,issue,positive,neutral,neutral,neutral,neutral,neutral
1719786610,"Thanks for the reply, and sorry about the links, I forgot the development repo was private.

I think you're right, and yes there is a held-out dataset for the final evaluation score. I was thinking 5 fold CV within tpot would be enough to detect/prevent overfitting, but the training set does have very few positive examples so I probably should've been looking for it. I'll run some tests and if that is the case, then this is definitely non-issue (and sorry about that). If it is the case, maybe I'll add some overfitting detection or limited-examples-warning in a PR.

As for reproducing, no worries, I was just looking for high-level feedback first (rather than a full reproduction/debugging/analysis). If does appear to be a bug, I'll put in the work to isolate the dataset+code and make it fully reproducable on a public repo.",thanks reply sorry link forgot development private think right yes final evaluation score thinking fold within would enough training set positive probably looking run case definitely sorry case maybe add detection looking feedback first rather full appear bug put work isolate make fully public,issue,positive,positive,neutral,neutral,positive,positive
1718293397,"Thank you for your interest in contributing! We would love to have your help.

We have shifted development into our next version, TPOT2. We rewrote the codebase from scratch with the goal of making it clearer, more modular, and easier to extend and maintain. This also resolves many bugs/issues with TPOT1 (such as infinitely stalling), and begins to implement some new features (such as [graph-based pipelines](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/graphsklearn.py)). 

We have an alpha version of the codebase here: https://github.com/EpistasisLab/tpot2/tree/main . The dev branch contains the latest version, while the main is the current ""stable version.""
It is still in development, and things may change over time. We would love feedback on the organization/API/etc , especially from people looking to contribute. 

For your question, about the initial population:
In TPOT2, the initial population is pulled from a generator that yields individuals. The code that is currently used is found [here](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/individual_representations/graph_pipeline_individual/templates.py#L11C29-L11C29 ). Effectively, it loops through the possible root nodes and adds a number of random insertions to create the random population. This would be the function to modify to change how the initial population is generated.

The actual individual class is [defined here](https://github.com/EpistasisLab/tpot2/blob/main/tpot2/individual_representations/graph_pipeline_individual/individual.py): Its essentially a networkX graph of NodeLabel() objects that hold the method class (as types) and the hyperparameters (as dictionaries). the root node is the parent, child nodes feed their output to the parent nodes. 


So if we want the user to provide the initial pipelines, we would just need a function that converts sklearn pipelines to the graphindividual format.

Feel free to open an issue on the TPOT2 page. 

I'm also happy to set up a meeting to discuss any questions/concerns",thank interest would love help development next version scratch goal making clearer modular easier extend maintain also many infinitely stalling implement new alpha version dev branch latest version main current stable version still development may change time would love feedback especially people looking contribute question initial population initial population generator code currently used found effectively possible root number random create random population would function modify change initial population actual individual class defined essentially graph hold method class root node parent child feed output parent want user provide initial would need function format feel free open issue page also happy set meeting discus,issue,positive,positive,positive,positive,positive,positive
1718258044,"Your links go to a 404 error page; the repository is probably private. Your screenshot shows that one of the CV scores has an expected value of around .68. (It would be helpful to copy/paste your code rather than use screenshots, so we can quickly test it ourselves). 

Is the lower accuracy you refer to on the out-of-sample test set? Sometimes TPOT can overfit the CV score with overly complex pipelines. So, while it is ""correctly"" optimizing the objective function, the result is a pipeline that performs poorly on held-out data. One option is that you could look at the pareto_front_fitted_pipelines_ to see if any of those simpler pipelines have better performance. (For example, You could hold out a validation set, use that to select from the Pareto front, and then do a final test on that dataset.) By default tpot uses 5 fold CV, you could try setting cv=10. You can also minimize complexity with template=""selector-transformer-classifier""

When I run the TPOT with a test dataset with this scorer, it seems to accurately maximize the objective function.",link go error page repository probably private one value around would helpful code rather use quickly test lower accuracy refer test set sometimes overfit score overly complex correctly objective function result pipeline poorly data one option could look see simpler better performance example could hold validation set use select front final test default fold could try setting also minimize complexity run test scorer accurately maximize objective function,issue,negative,positive,neutral,neutral,positive,positive
1701610888,"The short answer is no. TPOT only fits the pareto front models (including the best model) to the full training set. TPOT does not save the fitted models for each fold of the CV.

Here are the models that you are able to access.
1. The model with the best cv score fitted to the full training data.
2. The list of Pareto front models fitted to the full training data
3. With some work, you can extract all evaluated pipelines, but they will be unfitted. You can find more information here #516 


```
from tpot import TPOTRegressor, TPOTClassifier
from sklearn.model_selection import train_test_split
import sklearn
import sklearn.datasets
import tpot
import dill as pickle

X, y = sklearn.datasets.load_iris(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20, random_state=42)

est = TPOTClassifier(generations=2, population_size=2, verbosity=2, random_state=42, n_jobs=-2 ,cv=10)

est.fit(X_train, y_train)


# 1 save the model with the best cv score fitted to the full training data.
pickle.dump(est.fitted_pipeline_, open('tpot_iris_pipeline.pkl', 'wb'))

# 2 save the list of unfitted Pareto front models
pickle.dump(list(est.pareto_front_fitted_pipelines_.values()), open('tpot_iris_pareto_front_models.pkl', 'wb'))
```

We are currently working on TPOT2 where you can more easily access all evaluated pipelines without workarounds. However, like in TPOT1, we do not train all pipelines on the full dataset so these pipelines are unfitted. [Example here: ](https://github.com/EpistasisLab/tpot2/blob/main/Tutorial/1_Estimators_Overview.ipynb)",short answer front best model full training set save fitted fold able access model best score fitted full training data list front fitted full training data work extract unfitted find information import import import import import import dill pickle save model best score fitted full training data open save list unfitted front list open currently working easily access without however like train full unfitted example,issue,positive,positive,positive,positive,positive,positive
1692512559,"In TPOT1 you can access the top model with est.fitted_pipeline_ that is fit to the entire training set

You can also access the Pareto front (of scores vs number of nodes) models in est.pareto_front_fitted_pipelines_ . This is a dictionary where the values are the actual pipelines that were fit to the entire training dataset. 

[From the documentation](https://epistasislab.github.io/tpot/api/)

> fitted_pipeline_: scikit-learn Pipeline object
> The best pipeline that TPOT discovered during the pipeline optimization process, fitted on the entire training dataset.
> pareto_front_fitted_pipelines_: Python dictionary
> Dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset.
> 
> The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline.
> 
> Note: pareto_front_fitted_pipelines_ is only available when verbosity=3.

The est.evaluated_individuals_ contains all the string representations of the individuals but not the actual pipeline. There isn't a built-in function to convert from the string to the actual pipeline...  But this issue #516 has instructions for doing the conversion. 


We also have an alpha version of TPOT 2, our next version, that seeks to address issues like these. In TPOT 2, we provide a pandas data frame that contains all evaluated individuals. [Example here](https://github.com/EpistasisLab/tpot2/blob/main/Tutorial/1_Estimators_Overview.ipynb) ",access top model fit entire training set also access front number dictionary actual fit entire training documentation pipeline object best pipeline discovered pipeline optimization process fitted entire training python dictionary dictionary front key string representation pipeline value corresponding pipeline fitted entire training front pipeline complexity number pipeline predictive performance pipeline note available string actual pipeline function convert string actual pipeline issue conversion also alpha version next version address like provide data frame example,issue,positive,positive,positive,positive,positive,positive
1691020679,"> 
Hello, so I've tried a couple of datasets, which I got early crash errors with 0.12.0, by using 0.12.1 version, and until now they run smoothly, thanks so much for a quick response! 
",hello tried couple got early crash version run smoothly thanks much quick response,issue,negative,positive,positive,positive,positive,positive
1689104064,This issue was fixed in PR #1315. Please try again with v0.12.1.,issue fixed please try,issue,negative,positive,neutral,neutral,positive,positive
1676529040,"Running `pip install scikit-learn` installs version `1.3.0` as of 2 days ago, so that seems to be the most recent stable version they're using. I see there's dev/beta versions higher than that, but I would assume that most people who install it will get version `1.3.0` by running `pip install`. Either way, the issue persists.",running pip install version day ago recent stable version see higher would assume people install get version running pip install either way issue,issue,negative,positive,positive,positive,positive,positive
1676178613,"This is actually not the latest version of `scikit-learn`, but an older version.",actually latest version older version,issue,negative,positive,positive,positive,positive,positive
1676088636,"```
Traceback (most recent call last):
  File "".\forward_predict.py"", line 16, in <module>
    from tpot.builtins import StackingEstimator
  File ""C:\Users\chalu\AppData\Local\Programs\Python\Python38\lib\site-packages\tpot\__init__.py"", line 27, in <module>
    from .tpot import TPOTClassifier, TPOTRegressor
  File ""C:\Users\chalu\AppData\Local\Programs\Python\Python38\lib\site-packages\tpot\tpot.py"", line 31, in <module>
    from .base import TPOTBase
  File ""C:\Users\chalu\AppData\Local\Programs\Python\Python38\lib\site-packages\tpot\base.py"", line 82, in <module>
    from .metrics import SCORERS
  File ""C:\Users\chalu\AppData\Local\Programs\Python\Python38\lib\site-packages\tpot\metrics.py"", line 27, in <module>
    from sklearn.metrics import make_scorer, SCORERS
ImportError: cannot import name 'SCORERS' from 'sklearn.metrics' (C:\Users\chalu\AppData\Local\Programs\Python\Python38\lib\site-packages\sklearn\metrics\__init__.py)
```

TPOT doesn't like latest version of `scikit-learn`, version 1.3.0 I believe. `SCORERS` isn't a part of the `sklearn.metrics` class anymore. Have to revert to older versions of scikit-learn.",recent call last file line module import file line module import file line module import file line module import file line module import import name like latest version version believe part class revert older,issue,negative,positive,positive,positive,positive,positive
1675506983,I believe this may be the same thing happening in #1313 ,believe may thing happening,issue,negative,neutral,neutral,neutral,neutral,neutral
1665614982,"Have a similar issue, there is a exception that is not caught that terminates the training loop. I am not sure why this exception doesn't get raised and show a stack trace by default, but if I specifically extend the ""try"" block in base.py:813 to also catch other exceptions, I got:

```
Traceback (most recent call last):
  File ""/home/myusername/.cache/pypoetry/virtualenvs/myproject--nQ0R-Yy-py3.11/lib/python3.11/site-packages/tpot/base.py"", line 817, in fit
    self._pop, _ = eaMuPlusLambda(
                   ^^^^^^^^^^^^^^^
  File ""/home/myusername/.cache/pypoetry/virtualenvs/myproject--nQ0R-Yy-py3.11/lib/python3.11/site-packages/tpot/gp_deap.py"", line 255, in eaMuPlusLambda
    population[:] = toolbox.select(population + offspring, mu)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/myusername/.cache/pypoetry/virtualenvs/myproject--nQ0R-Yy-py3.11/lib/python3.11/site-packages/deap/tools/emo.py"", line 41, in selNSGA2
    assignCrowdingDist(front)
  File ""/home/myusername/.cache/pypoetry/virtualenvs/myproject--nQ0R-Yy-py3.11/lib/python3.11/site-packages/deap/tools/emo.py"", line 132, in assignCrowdingDist
    crowd.sort(key=lambda element: element[0][i])
  File ""/home/myusername/.cache/pypoetry/virtualenvs/myproject--nQ0R-Yy-py3.11/lib/python3.11/site-packages/deap/tools/emo.py"", line 132, in <lambda>
    crowd.sort(key=lambda element: element[0][i])
                                   ~~~~~~~~~~^^^
IndexError: tuple index out of range
```

So ind.fitness.values is a tuple of a larger size for the first individual in the population, and then a later one has a smaller tuple leading to the IndexError. Indeed, the reason is that there are some elements in the population with ind.fitness.valid == False, with a empty tuple for ind.fitness.values.

Not sure why this is.
",similar issue exception caught training loop sure exception get raised show stack trace default specifically extend try block also catch got recent call last file line fit file line population population offspring mu file line front file line element element file line lambda element element index range size first individual population later one smaller leading indeed reason population false empty sure,issue,negative,positive,neutral,neutral,positive,positive
1664744449,"Succinctly, the problem is that you expected:
`some_tpot_classifier.score() == sklearn.metrics.accuracy_score(some_tpot_classifier)`
`some_tpot_regressor.score() == sklearn.metrics.accuracy_score(some_tpot_regressor)`
but instead got:
`some_tpot_classifier.score() == sklearn.metrics.accuracy_score(some_tpot_classifier)`
`some_tpot_regressor.score() != sklearn.metrics.accuracy_score(some_tpot_regressor)`

I doubt what you got is intended for at least for some datasets; the first step would be consolidating your demonstration code into an automated test.

```python
def test_accuracy_scoring(...):
    ...
    some_tpot_classifier.score() == sklearn.metrics.accuracy_score(some_tpot_classifier)
    some_tpot_regressor.score() == sklearn.metrics.accuracy_score(some_tpot_regressor)
```",succinctly problem instead got doubt got intended least first step would demonstration code test python,issue,negative,negative,neutral,neutral,negative,negative
1660655056,"@weixuanfu can you please give an example of using an iterator in detail code?
",please give example detail code,issue,negative,neutral,neutral,neutral,neutral,neutral
1653097820,"**For feature covariate adjustments, we have added a transformer (resAdjTransformer) to TPOT.** This needs to be either the first step of any pipeline or the second step after an FSS (TPOT Template can be used to specify these). The initial input to TPOT adds the covariate columns to X. **One hyperparameter of this transformer is a file specifying which columns of X should be adjusted by which covariate columns.** The transformer applies the no leakage residual adjustments to these columns and removes the covariate columns before passing its output on to the other steps. If no covariate adjustment on the target is needed, classic TPOT can then be run as usual (Figure 1 path A).

https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7528347/

Here, how can I mention the hyperparameters?? Can you please provide the code for it?",feature added transformer need either first step pipeline second step template used specify initial input one transformer file transformer leakage residual passing output adjustment target classic run usual figure path mention please provide code,issue,negative,positive,neutral,neutral,positive,positive
1597723028,"The 'arity' error is due to a bug in the TPOT mutation function. I implemented a fix in this pull request https://github.com/EpistasisLab/tpot/pull/1268

The other errors are due to invalid hyperparameters being included in the config dict found here: https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py

Should be as easy as just removing the invalid hyperparameters from the list.

@nickotto ",error due bug mutation function fix pull request due invalid included found easy removing invalid list,issue,negative,positive,neutral,neutral,positive,positive
1564316035,"Looks like the new release is out. One lingering compatibility issue, noted on https://github.com/conda-forge/tpot-feedstock/pull/34, the use of `load_boston` has been removed from `scikit-learn`, causing the test suite to fail with the following error:

```
ERROR: Failure: ImportError (
`load_boston` has been removed from scikit-learn since version 1.2.
```

We'll do some pins/patches to get it out the door there, and see how it goes :crossed_fingers: ",like new release one compatibility issue noted use removed causing test suite fail following error error failure removed since version get door see go,issue,negative,negative,negative,negative,negative,negative
1561692110,"This was fixed by https://github.com/EpistasisLab/tpot/pull/1280, but until there's a new `tpot` release, you'll either have to use a workaround or do a `pip install git+` to ensure that you're installing with the latest version available on GitHub.",fixed new release either use pip install ensure latest version available,issue,negative,positive,positive,positive,positive,positive
1557831129,"I had the same issue. Monkey patching can be solution which is discouraged in general, but seems harmless to me for this case. Please be cautious to use this workaround. 

```
import numpy as np

# Define a new float class that behaves like the built-in float
class MyFloat(float):
    pass

# Monkey patch the numpy module to replace np.float with MyFloat
np.float = MyFloat

# Now when the library code uses np.float, it will be interpreted as MyFloat
```

Coded by ChatGPT",issue monkey solution general harmless case please cautious use import define new float class like float class float pas monkey patch module replace library code,issue,positive,positive,neutral,neutral,positive,positive
1553479443,"cc @weixuanfu with #1280 merged in, would it be possible to cut a new release of `tpot`?",would possible cut new release,issue,negative,positive,neutral,neutral,positive,positive
1547241651,"Fixed since https://github.com/EpistasisLab/tpot/pull/1280 has been merged.

I recommend using `pip install git+` to ensure that you're installing with these changes.",fixed since recommend pip install ensure,issue,positive,positive,neutral,neutral,positive,positive
1539264080,"This is most likely related to TPOT not being able to terminate some pipelines. The current timeout method doesn't always work on specific modules. If those modules can't be timed out and they run for a long time (such as SVC), then TPOT will get stuck slowly fitting a single pipeline which may never converge. 

This could be resolved by using func_timeout https://pypi.org/project/func-timeout/ 

We think this has been resolved in the next version of TPOT, TPOT2, found here: https://github.com/EpistasisLab/tpot2

It may be a good idea to bring the same fix to this version of TPOT as well. ",likely related able terminate current method always work specific ca timed run long time get stuck slowly fitting single pipeline may never converge could resolved think resolved next version found may good idea bring fix version well,issue,negative,positive,positive,positive,positive,positive
1539257365,"can you provide code to fully reproduce this issue?

It looks like the column names are getting renamed between housing2 to X_train. How is X_train being created?",provide code fully reproduce issue like column getting housing,issue,negative,neutral,neutral,neutral,neutral,neutral
1539236312,"This is supported in the new version of TPOT that we are working on. Found here: https://github.com/EpistasisLab/tpot2

You can pass a list of scorers to the ""scorers"" param. Right now, the ""fitted_pipeline_"" will be the model with the best value in the first scorer. But TPOT2 will also provide a dataframe of all evaluated pipelines, including the pareto optimal models for you to select from.",new version working found pas list param right model best value first scorer also provide optimal select,issue,positive,positive,positive,positive,positive,positive
1539221770,This opens correctly for me on my machine. What versions of tpot and Jupyter are you using? Perhaps a clean install with a new environment will resolve this issue.,correctly machine perhaps clean install new environment resolve issue,issue,positive,positive,positive,positive,positive,positive
1539219357,"
The manual pipeline is not exactly identical to the TPOT output. It is missing the Binarizer step.

Also, TPOT wraps internal classifiers in a StackingEstimator. This will pass through its inputs in addition to its predictions. (https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/stacking_estimator.py). 

Going off memory, I believe this is what the TPOT output would be equivalent to:

```
step1 = Binarizer(threshold=0.0)

base_model = StackingEstimator(GaussianNB())

meta_model = MLPClassifier(random_state=1, 
                        learning_rate_init=0.001,
                        alpha=0.001)


ensemble = sklearn.pipeline.Pipeline(estimators=[('step1',step1),
('base_model', base_model), 
                                                     ('meta_model', meta_model)],
                                         final_estimator=meta_model,
                               n_jobs=-1)

```

The binarized transforms the data -> transformed data -> GaussianNB -> transformed data + predictions -> MLPclassifier",manual pipeline exactly identical output missing step also internal pas addition going memory believe output would equivalent step ensemble step data data data,issue,negative,positive,neutral,neutral,positive,positive
1538958197,"Hello sorry for writing late. I do not think it is actively managed, but you can go to my github fork, and download from the master branch. I will write the authors. And if you face more problems, write it, I will personally fix it :)",hello sorry writing late think actively go fork master branch write face write personally fix,issue,negative,negative,negative,negative,negative,negative
1536569218,Hi @jay-m-dev or @perib I was wondering if you have the permissions to review and merge this?,hi wondering review merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1517274495,"I am hitting the same issue. I installed tpot using anaconda and then tied to do a pip install (including in the jupyter cell), but it did not fix the issue.",issue anaconda tied pip install cell fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1509719027,"Sorry, I totally missed groups parameter in .fit()
having cv=logo, tpt.fit (X,y, groups) will do the trick.
",sorry totally parameter trick,issue,negative,negative,negative,negative,negative,negative
1505105497,"@weixuanfu I'm using TPOT and I want to extract the feature importance for every evaluated individual and not just the best pipeline. I am able to access all the pipelines using `tpot.evaluated_individuals_` but then I want to retrieve feature importance through either `.feature_importances_` or `.coef_` or `permutation_importance`. Is there a way re-evaluate each pipeline to retrieve this information?
I know some models may not have feature_importance attribute therefore, I have used it in an If Else block. ",want extract feature importance every individual best pipeline able access want retrieve feature importance either way pipeline retrieve information know may attribute therefore used else block,issue,positive,positive,positive,positive,positive,positive
1503686241,"Dear Authors, you appears busy. I am closing this now, happy to discuss it further when you become a little more available. Cheers.",dear busy happy discus become little available,issue,positive,positive,positive,positive,positive,positive
1501029160,"Please, someone with write access merge this. Every time I install tpot I have to change this line (I change it to `float` and that seems to work as well).",please someone write access merge every time install change line change float work well,issue,positive,neutral,neutral,neutral,neutral,neutral
1493489682,"@thecasual for temporarily solution, u can change the source code in ur files",temporarily solution change source code ur,issue,negative,neutral,neutral,neutral,neutral,neutral
1458921827,"Hi @weixuanfu .. I have a separate question about TPOT and wondering if I may be able to ask you here. I ran the classifier and it spit out the current best pipeline. Unfortunately I did not export the optimized pipeline to a separate .py file before exiting the kernel. Is there a way I can take the output shown in the attached image and plug it back into TPOT so I can use this optimized model to predict on my dataset? In other words, I do not know how to get back to this point without needing to restart the entire classifier. Is this possible? 
The optimized pipeline is using GradientBoostingClassifier with some feature engineering and transformations. Can I just tell TPOT to use this pipeline without needing to manually do the feature engineering myself and feeding result into GradientBoostingClassifier?




<img width=""911"" alt=""TPOT_output"" src=""https://user-images.githubusercontent.com/58792096/223561559-4d80f093-a363-4678-a630-453156c55c88.png"">
",hi separate question wondering may able ask ran classifier spit current best pipeline unfortunately export pipeline separate file kernel way take output shown attached image plug back use model predict know get back point without needing restart entire classifier possible pipeline feature engineering tell use pipeline without needing manually feature engineering feeding result,issue,negative,positive,positive,positive,positive,positive
1430712272,"Vote for this PR, a required change. Please, merge this",vote change please merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1398166821,"@iura77 your tpot-cuml.yml file might be wrong.  Maybe you downloaded the github html page that displays the file rather than the raw file?
Download the raw file via this github link and try again: https://github.com/EpistasisLab/tpot/raw/master/tpot-cuml.yml",file might wrong maybe page file rather raw file raw file via link try,issue,negative,negative,negative,negative,negative,negative
1380703498,"> If the maintainers are open to it, perhaps we could open a PR that validates the `n_jobs` parameter when the cuML configuration is used.

Yes probably just >0  as I think you can use multiple GPUs",open perhaps could open parameter configuration used yes probably think use multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1380700716,"If the maintainers are open to it, perhaps we could open a PR that validates the `n_jobs` parameter when the cuML configuration is used.",open perhaps could open parameter configuration used,issue,negative,neutral,neutral,neutral,neutral,neutral
1379405594,"cc @weixuanfu , is this something that TPOT would be open to updating to support recent NumPy version?",something would open support recent version,issue,negative,neutral,neutral,neutral,neutral,neutral
1379267839,"I just got caught by this, there needs to be a better error message when using cuML and leaving the n_jobs set to -1. ",got caught need better error message leaving set,issue,negative,positive,positive,positive,positive,positive
1378578560,"There are two issues:

1. The list `groups` is too long, i.e. `len(X_train) == 100` but `len(groups) == 150`. Change it to e.g. `groups=np.concatenate((np.zeros(25),np.ones(25),np.ones(25)*2,np.ones(25)*3))`
2. There are 4 groups but you attempt to do a 5-fold group split. That's not possible. Do a 4-fold split (or less).

The error message unfortunatly isn't helpful here.",two list long change attempt group split possible split le error message helpful,issue,negative,negative,neutral,neutral,negative,negative
1370338946,"@notwopr, looks like repo permissions prevent me from linking this issue to the associated PR (#1282), but I verified the fix there just by updating the NumPY requirement.",like prevent linking issue associated fix requirement,issue,negative,neutral,neutral,neutral,neutral,neutral
1370325251,"i've received this error as well,  and I have all my dependencies up to date.

`Traceback (most recent call last):
  File ""C:\Users\david\github\beluga3\SCRATCHPAPER_6.py"", line 4, in <module>
    from MLModule.AI.featuretoolstpot.ml_framework import MyMLModeler
  File ""C:\Users\david\github\beluga3\MLModule\AI\featuretoolstpot\ml_framework.py"", line 8, in <module>
    from tpot import TPOTClassifier
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\__init__.py"", line 27, in <module>
    from .tpot import TPOTClassifier, TPOTRegressor
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\tpot.py"", line 31, in <module>
    from .base import TPOTBase
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\base.py"", line 70, in <module>
    from .builtins import CombineDFs, StackingEstimator
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\builtins\__init__.py"", line 29, in <module>
    from .one_hot_encoder import OneHotEncoder, auto_select_categorical_features, _transform_selected
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\builtins\one_hot_encoder.py"", line 136, in <module>
    class OneHotEncoder(BaseEstimator, TransformerMixin):
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\tpot\builtins\one_hot_encoder.py"", line 216, in OneHotEncoder
    def __init__(self, categorical_features='auto', dtype=np.float,
  File ""C:\Users\david\AppData\Local\Programs\Python\Python310\lib\site-packages\numpy\__init__.py"", line 284, in __getattr__
    raise AttributeError(""module {!r} has no attribute ""
AttributeError: module 'numpy' has no attribute 'float'. Did you mean: 'cfloat'?`

As you can see, when it tries to instantiate the class OneHotEncoder, the class has one input parameter of dtype, and it uses np.float. But apparently numpy has no such attribute. I checked all my dependencies, and made sure everything was upgrade.

I'm running Python 3.10.2",received error well date recent call last file line module import file line module import file line module import file line module import file line module import file line module import file line module class file line self file line raise module attribute module attribute mean see class class one input parameter apparently attribute checked made sure everything upgrade running python,issue,negative,positive,neutral,neutral,positive,positive
1353644391,"That works for me, too.

There's another instance of `getargspec` in `tests/tpot_test.py`, a simple replacement with `getfullargspec` is sufficient there, too.

@rampdev, would you like to file a PR? Otherwise I'll do it.",work another instance simple replacement sufficient would like file otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
1333538769,"> @GinoWoz1 my guess would be that if you pass in a numpy array (i.e. using `df.values`) at the start, you won't see this problem, but if you pass in the `DataFrame` then you get this issue. Could you confirm?

Thanks a lot for this tip. Had the same problem and your suggestion solves the issue for me!",guess would pas array start wo see problem pas get issue could confirm thanks lot tip problem suggestion issue,issue,negative,positive,positive,positive,positive,positive
1297805615,"Could you please share the full system info, commands you ran, and the error you received? I'm not able to reproduce this error on my system (though it's Ubuntu based).",could please share full system ran error received able reproduce error system though based,issue,negative,positive,positive,positive,positive,positive
1290948179,"Log output thus far (has been stuck on pipeline 55 since I checked this morning).

```
$ cat tpot_log.log
_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.
_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.
_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..
_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.
_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.
Optimization Progress:   2%|▏         | 54/2550 [8:57:48<314:58:55, 454.30s/pipeline]
```",log output thus far stuck pipeline since checked morning cat decorator decorator decorator provided affinity ward work decorator unsupported set combination decorator optimization progress,issue,negative,positive,neutral,neutral,positive,positive
1290900553,"Conda list output:

# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                  2_kmp_llvm    conda-forge
_py-xgboost-mutex         2.0                       cpu_0    conda-forge
abseil-cpp                20211102.0           h93e1e8c_2    conda-forge
arrow-cpp                 8.0.0           py310h3098874_0  
aws-c-common              0.4.57               he6710b0_1  
aws-c-event-stream        0.1.6                h2531618_5  
aws-checksums             0.1.9                he6710b0_0  
aws-sdk-cpp               1.8.185              hce553d0_0  
blas                      1.0                         mkl  
bokeh                     2.4.3              pyhd8ed1ab_3    conda-forge
boost-cpp                 1.78.0               he72f1d9_0    conda-forge
boto3                     1.24.28         py310h06a4308_0    anaconda
botocore                  1.27.28         py310h06a4308_0    anaconda
brotli                    1.0.9                h166bdaf_7    conda-forge
brotli-bin                1.0.9                h166bdaf_7    conda-forge
brotlipy                  0.7.0           py310h5764c6d_1004    conda-forge
bzip2                     1.0.8                h7b6447c_0  
c-ares                    1.18.1               h7f98852_0    conda-forge
ca-certificates           2022.9.24            ha878542_0    conda-forge
certifi                   2022.9.24          pyhd8ed1ab_0    conda-forge
cffi                      1.15.1          py310h74dc2b5_0  
charset-normalizer        2.1.1              pyhd8ed1ab_0    conda-forge
click                     8.1.3           py310hff52083_0    conda-forge
cloudpickle               2.2.0              pyhd8ed1ab_0    conda-forge
colorama                  0.4.5              pyhd8ed1ab_0    conda-forge
cryptography              38.0.2          py310h597c629_1    conda-forge
cudatoolkit               11.3.1               h2bc3f7f_2  
cytoolz                   0.12.0          py310h5764c6d_0    conda-forge
dask                      2022.10.0          pyhd8ed1ab_2    conda-forge
dask-core                 2022.10.0          pyhd8ed1ab_1    conda-forge
dask-glm                  0.2.0                      py_1    conda-forge
dask-ml                   2022.5.27          pyhd8ed1ab_0    conda-forge
deap                      1.3.3           py310h769672d_0    conda-forge
distributed               2022.10.0          pyhd8ed1ab_2    conda-forge
fftw                      3.3.9                h27cfd23_1  
freetype                  2.10.4               h0708190_1    conda-forge
fsspec                    2022.10.0          pyhd8ed1ab_0    conda-forge
gflags                    2.2.2             he1b5a44_1004    conda-forge
giflib                    5.2.1                h36c2ea0_2    conda-forge
glog                      0.6.0                h6f12383_0    conda-forge
greenlet                  1.1.1           py310h295c915_0    anaconda
grpc-cpp                  1.46.1               h33aed49_0  
heapdict                  1.0.1                      py_0    conda-forge
icu                       70.1                 h27087fc_0    conda-forge
idna                      3.4                pyhd8ed1ab_0    conda-forge
intel-openmp              2021.4.0          h06a4308_3561  
jinja2                    3.1.2              pyhd8ed1ab_1    conda-forge
jmespath                  0.10.0             pyhd3eb1b0_0    anaconda
joblib                    1.2.0              pyhd8ed1ab_0    conda-forge
jpeg                      9e                   h166bdaf_2    conda-forge
krb5                      1.19.2               hac12032_0    anaconda
lcms2                     2.12                 hddcbb42_0    conda-forge
ld_impl_linux-64          2.38                 h1181459_1  
lerc                      3.0                  h9c3ff4c_0    conda-forge
libabseil                 20211102.0      cxx17_h48a1fff_2    conda-forge
libbrotlicommon           1.0.9                h166bdaf_7    conda-forge
libbrotlidec              1.0.9                h166bdaf_7    conda-forge
libbrotlienc              1.0.9                h166bdaf_7    conda-forge
libcurl                   7.84.0               h91b91d3_0  
libdeflate                1.8                  h7f8727e_5  
libedit                   3.1.20210910         h7f8727e_0    anaconda
libev                     4.33                 h516909a_1    conda-forge
libevent                  2.1.10               h9b69904_4    conda-forge
libffi                    3.3                  he6710b0_2  
libgcc-ng                 12.2.0              h65d4601_19    conda-forge
libgfortran-ng            11.2.0               h00389a5_1  
libgfortran5              11.2.0               h1234567_1  
libllvm11                 11.1.0               hf817b99_2    conda-forge
libnghttp2                1.46.0               hce63b2e_0  
libpng                    1.6.37               hbc83047_0  
libpq                     12.9                 h16c4e8d_3    anaconda
libprotobuf               3.20.1               h4ff587b_0  
libssh2                   1.10.0               ha56f1ee_2    conda-forge
libstdcxx-ng              12.2.0              h46fd767_19    conda-forge
libthrift                 0.15.0               he6d91bd_0    conda-forge
libtiff                   4.4.0                hecacb30_0  
libuuid                   1.0.3                h7f8727e_2  
libwebp                   1.2.4                h522a892_0    conda-forge
libwebp-base              1.2.4                h166bdaf_0    conda-forge
libxgboost                1.6.2            cpu_ha3b9936_1    conda-forge
llvm-openmp               14.0.6               h9e868ea_0  
llvmlite                  0.39.1          py310he621ea3_0  
locket                    1.0.0              pyhd8ed1ab_0    conda-forge
lz4                       4.0.0           py310h5d5e884_2    conda-forge
lz4-c                     1.9.3                h9c3ff4c_1    conda-forge
markupsafe                2.1.1           py310h5764c6d_1    conda-forge
mkl                       2021.4.0           h06a4308_640  
mkl-service               2.4.0           py310h7f8727e_0  
mkl_fft                   1.3.1           py310hd6ae3a3_0  
mkl_random                1.2.2           py310h00e6091_0  
msgpack-python            1.0.4           py310hbf28c38_0    conda-forge
multipledispatch          0.6.0                      py_0    conda-forge
ncurses                   6.3                  h5eee18b_3  
numba                     0.56.3          py310ha5257ce_0    conda-forge
numpy                     1.22.3          py310hfa59a62_0  
numpy-base                1.22.3          py310h9585f30_0  
openssl                   1.1.1q               h166bdaf_1    conda-forge
orc                       1.7.4                h07ed6aa_0  
packaging                 21.3               pyhd8ed1ab_0    conda-forge
pandas                    1.5.1           py310h769672d_0    conda-forge
partd                     1.3.0              pyhd8ed1ab_0    conda-forge
pillow                    9.2.0           py310hace64e9_1  
pip                       22.2.2          py310h06a4308_0  
psutil                    5.9.3           py310h5764c6d_0    conda-forge
psycopg2                  2.8.6           py310h8f2d780_1    anaconda
py-xgboost                1.6.2           cpu_py310hd1aba9c_1    conda-forge
pyarrow                   8.0.0           py310h468efa6_0  
pycparser                 2.21               pyhd8ed1ab_0    conda-forge
pyopenssl                 22.1.0             pyhd8ed1ab_0    conda-forge
pyparsing                 3.0.9              pyhd8ed1ab_0    conda-forge
pysocks                   1.7.1              pyha2e5f31_6    conda-forge
python                    3.10.6               haa1d7c7_1  
python-dateutil           2.8.2              pyhd8ed1ab_0    conda-forge
python_abi                3.10                    2_cp310    conda-forge
pytorch                   1.12.1          py3.10_cuda11.3_cudnn8.3.2_0    pytorch
pytorch-mutex             1.0                        cuda    pytorch
pytz                      2022.5             pyhd8ed1ab_0    conda-forge
pyyaml                    6.0             py310h5764c6d_4    conda-forge
re2                       2022.04.01           h27087fc_0    conda-forge
readline                  8.1.2                h7f8727e_1  
requests                  2.28.1             pyhd8ed1ab_1    conda-forge
s3transfer                0.6.0           py310h06a4308_0    anaconda
scikit-learn              1.1.2           py310h6a678d5_0  
scipy                     1.9.1           py310hd5efca6_0  
setuptools                63.4.1          py310h06a4308_0  
six                       1.16.0             pyhd3eb1b0_1  
snappy                    1.1.9                hbd366e4_1    conda-forge
sortedcontainers          2.4.0              pyhd8ed1ab_0    conda-forge
sqlalchemy                1.4.39          py310h5eee18b_0    anaconda
sqlite                    3.39.3               h5082296_0  
stopit                    1.1.2                      py_0    conda-forge
tblib                     1.7.0              pyhd8ed1ab_0    conda-forge
threadpoolctl             3.1.0              pyh8a188c0_0    conda-forge
tk                        8.6.12               h1ccaba5_0  
toolz                     0.12.0             pyhd8ed1ab_0    conda-forge
tornado                   6.1             py310h5764c6d_3    conda-forge
tpot                      0.11.7             pyhd8ed1ab_1    conda-forge
tqdm                      4.64.1             pyhd8ed1ab_0    conda-forge
typing_extensions         4.4.0              pyha770c72_0    conda-forge
tzdata                    2022e                h04d1e81_0  
update_checker            0.18.0             pyh9f0ad1d_0    conda-forge
urllib3                   1.26.11            pyhd8ed1ab_0    conda-forge
utf8proc                  2.6.1                h27cfd23_0  
wheel                     0.37.1             pyhd3eb1b0_0  
xz                        5.2.6                h5eee18b_0  
yaml                      0.2.5                h7f98852_2    conda-forge
zict                      2.2.0              pyhd8ed1ab_0    conda-forge
zlib                      1.2.13               h5eee18b_0  
zstd                      1.5.2                ha4553b6_0  
",list output name version build channel blas anaconda anaconda click cryptography distributed greenlet anaconda jinja anaconda anaconda anaconda anaconda locket orc pillow pip anaconda python anaconda six snappy anaconda tornado wheel,issue,negative,neutral,neutral,neutral,neutral,neutral
1270031648,"This happens when your training data has nothing that causes a mid-pipeline infinity or NAN but then the data you are trying to make predictions on (test data or real world data) does have something that causes a mid-pipeline infinity or NAN. One common way this might happen is through a much larger or much smaller number comes up in the data you're trying to make predictions on. I'm sorry that I can't remember exactly which dataset I was on at the time—but you'll see multiple references in an internet search of people saying that a trained logistic regression model can at times return NaN predictions when predicting on something it didn't see during training, something with much bigger or smaller values. Some people recommend normalization / preprocessing to avoid this. See https://stackoverflow.com/questions/62818741/how-to-fix-nan-values-coming-from-the-implementation-of-logistic-regression. Also see https://github.com/scikit-learn/scikit-learn/issues/17925 for an interesting discussion on this popping up on many different types of models—not just logistic regression which is where it happened for me.

Ultimately since tpot is AutoML where it discovers the pipeline and you can't specify that a scaling preprocessor always comes first, you could fix this in one of three ways:
- Institute a rule that you always do normalization prior to Logistic Regression. This would fix it a fair bit of the time but not always. And it doesn't fix all the other estimators out there that sometimes produce NaNs when extrapolating outside of the training dataset, so this would not be a global fix for the underlying issue of no NaNs during training but NaNs pop up later during prediction.
- Fix StackingEstimator so it always works around problems of this sort automatically. See original post for my initial ideas on this.
- Add a descriptive warning in the StackingEstimator code that explains WHY the pipeline blew up while trying to make a prediction. Currently it fails in an entirely impossible to understand way and I had to spend half a day figuring out where the issue was. This could be as simple as explaining that a NaN arose in the specific classifier that was trying to be stacked.
- Somehow find a way to propagate NaN rows all the way through the pipeline so that .predict or .predict_proba don't fail but just slap some NaN values on the predictions at the end.

I'm not an expert so I'm not sure which of these is best.",training data nothing infinity nan data trying make test data real world data something infinity nan one common way might happen much much smaller number come data trying make sorry ca remember exactly see multiple search people saying trained logistic regression model time return nan something see training something much bigger smaller people recommend normalization avoid see also see interesting discussion many different logistic regression ultimately since pipeline ca specify scaling always come first could fix one three way institute rule always normalization prior logistic regression would fix fair bit time always fix sometimes produce outside training would global fix underlying issue training pop later prediction fix always work around sort automatically see original post initial add descriptive warning code pipeline trying make prediction currently entirely impossible understand way spend half day issue could simple explaining nan arose specific classifier trying somehow find way propagate nan way pipeline fail slap nan end expert sure best,issue,positive,positive,neutral,neutral,positive,positive
1262577402,"""there was a row that blew up the LogisticRegression""
Can you clarify the what is in the row that causes LogisticRegression to sometimes yield different number of outputs? I'm trying to reproduce the issue",row clarify row sometimes yield different number trying reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1262558993,"If you set n_jobs to 1, reproducibility is more likely. When using parallel processes, exact reproducibility gets challenging since the order of execution has some randomness that is not controllable.  It is something we are thinking about",set reproducibility likely parallel exact reproducibility since order execution randomness controllable something thinking,issue,negative,positive,neutral,neutral,positive,positive
1249983618,"Bumping this as it would be nice to pass categorical features to tpot. Tpot includes OneHotEncoder in its default estimator set for regressions, but it's only usable for integers as it stands. I see the fit method throws an error on np.isnan. I'm sure there's more to it than changing that though.",bumping would nice pas categorical default estimator set usable see fit method error sure though,issue,positive,positive,positive,positive,positive,positive
1249977412,"Just ran into this myself. The problem is one of your config options is repeated, creating a duplicate. Config options are iterated over, creating terminals for each option so the string ""full"" in PCA__svd_solver is iterated over and ""l"" is added twice. Putting ""full"" in a list should fix the problem.
```
  ""sklearn.decomposition.PCA"": {
      ""n_components"": [.7, .8, .9, .95, .99],
-     ""svd_solver"": ""full""
+     ""svd_solver"": [""full""]
  },
```",ran problem one repeated duplicate option string full added twice full list fix problem full full,issue,negative,positive,positive,positive,positive,positive
1247143058,"What does the rest of the pipeline look like? Sometimes transformers, feature unions, or stackingestimators increase the number of features. ",rest pipeline look like sometimes feature increase number,issue,positive,neutral,neutral,neutral,neutral,neutral
1243289807,"I built a model using Logistic regression in TPOT classifier. I used `model.fitted_pipeline_.steps[-1][1].coef_` to get the coefficients. However, the output returns 23 coefficient values, whereas the features are only 10. 

Can someone help on this, please?",built model logistic regression classifier used get however output coefficient whereas someone help please,issue,positive,neutral,neutral,neutral,neutral,neutral
1243058605,"TPOT raises an error because scikit-learn refuse to fit a regression model using StratifiedKFold as cross-validation method. 

StratifiedKFold is supposed to be used for classification, not regression, so this behavior is not really a bug.

The traceback goes something like this.
```
Traceback (most recent call last):
...
...
...
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.

The above exception was a direct cause of the following exception:

Traceback (most recent call last):
...
...
...
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
...
...
...
RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
```",error refuse fit regression model method supposed used classification regression behavior really bug go something like recent call last target got instead exception direct cause following exception recent call last target got instead handling exception another exception recent call last pipeline yet please call fit first,issue,positive,positive,positive,positive,positive,positive
1215046767,"Hi, I encountered the same issue. Should I just delete cuml=0.16 from the tpot-cuml.yml file and install all the rest?
",hi issue delete file install rest,issue,negative,neutral,neutral,neutral,neutral,neutral
1193486016,"> For now, we do not have built-in help function to covert string to sklearn pipeline. Please check the demo below. It should be helpful for building a function for this purpose.
> 
> ```python
> import numpy as np
> from deap import creator
> from sklearn.model_selection import cross_val_score
> from tpot.export_utils import generate_pipeline_code
> 
> # print part of pipeline dictionary
> print(dict(list(tpot.evaluated_individuals_.items())[0:2]))
> # print a pipeline and its values
> pipeline_str = list(tpot.evaluated_individuals_.keys())[0]
> print(pipeline_str)
> print(tpot.evaluated_individuals_[pipeline_str])
> for pipeline_string in sorted(tpot.evaluated_individuals_.keys()):
>     # convert pipeline string to scikit-learn pipeline object
>     deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
>     sklearn_pipeline = tpot._toolbox.compile(expr=deap_pipeline)
>     # print sklearn pipeline string
>     sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot._pset), tpot.operators)
>     print(sklearn_pipeline_str)
>     # Fix random state when the operator allows  (optional) just for get consistent CV score 
>     tpot._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
> 
>     try:
>         cv_scores = cross_val_score(sklearn_pipeline, training_features, training_target, cv=5, scoring='accuracy', verbose=0)
>         mean_cv_scores = np.mean(cv_scores)
>     except Exception as e:
>         print(e)
>         mean_cv_scores = -float('inf')
>     print(mean_cv_scores)
> ```


Turning it into a function.

```python
def get_dead_pipelines(tpot:TPOTClassifier) -> Container[sklearn.pipeline.Pipeline]:
    pipelines = []
    for pipeline_string, metadata in sorted(tpot.evaluated_individuals_.items()):
        deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
        sklearn_pipeline = tpot._toolbox.compile(expr=deap_pipeline)
        pipelines.append((sklearn_pipeline, metadata['internal_cv_score']))
    return pipelines
```",help function covert string pipeline please check helpful building function purpose python import import creator import import print part pipeline dictionary print list print pipeline list print print sorted convert pipeline string pipeline object print pipeline string print fix random state operator optional get consistent score try except exception print print turning function python container sorted return,issue,positive,negative,negative,negative,negative,negative
1188506051,"I've figured it out! Sort of.

The problem is due to trying to employ the `sample_weight=sample_weights,` line, for some reason the weights are not working correctly or something, and it screws up in each scoring metric. It works fine when I comment out/don't use `sample_weight`'s in each individual scoring metric.

I've tried using both `compute_class_weight` and `compute_sample_weight` but the same result with both.

Maybe I'm not using those properly, and really, I think if you set the `average` parameter in the scoring functions to `'weighted'` it does the same thing? Not sure. But when I don't use sample weights, it works.

I'll close this issue for now. If someone knows what's going on here, let me know.

Thanks!!!",figured sort problem due trying employ line reason working correctly something scoring metric work fine comment use individual scoring metric tried result maybe properly really think set average parameter scoring thing sure use sample work close issue someone going let know thanks,issue,positive,positive,positive,positive,positive,positive
1173134698,"> Those complex combinations of feature transformation were not supported in TPOT. I think [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) is needed for this idea in this issue. Also, as mentioned above, because multi-object optimization should penalize the pipelines with a large number of operators and limited improvement in scores, selection function or calculation of pipeline complexity should be changed for this issue. But we don't have a near plan to include this function. Any contributions are welcome.

need complex feature transformation on my current project, I can use ColumnTransformer in sklearn, but it seems not supported in tpot yet.

really need it!",complex feature transformation think idea issue also optimization penalize large number limited improvement selection function calculation pipeline complexity issue near plan include function welcome need complex feature transformation current project use yet really need,issue,positive,positive,neutral,neutral,positive,positive
1168024964,"Just found this thread myself, and I'm getting the same error as in #747 which is:

```
  File ""C:\Users\...\tpot\base.py"", line 1393, in _check_dataset
    ""Error: Input data is not in a valid format. Please confirm ""
ValueError: Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array.
```

I know it's not officially supported, but would love to be able to use TPOT for a multi-output regression problem.

For a simple reproducible problem, use this code:

```
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from tpot import TPOTRegressor
from numpy import arange

RANDOM_SEED = 42

X, y = make_regression(n_samples=500,
                       n_features=5,
                       n_informative=2,
                       n_targets=2,
                       shuffle=True,
                       random_state=RANDOM_SEED)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=RANDOM_SEED)

regressor_config_dict = {
    'sklearn.multioutput.MultiOutputRegressor': {
        'estimator': {
            'sklearn.ensemble.ExtraTreesRegressor': {
                'n_estimators': [100],
                'max_features': arange(0.05, 1.01, 0.05)
            }
        }
    }
}

tpot = TPOTRegressor(generations=100, 
                     population_size=100,
                     offspring_size=None, 
                     mutation_rate=0.9,
                     crossover_rate=0.1,
                     scoring='neg_mean_squared_error', 
                     cv=3,
                     subsample=1.0, 
                     n_jobs=4,
                     max_time_mins=None, 
                     max_eval_time_mins=5,
                     random_state=None, 
                     config_dict=regressor_config_dict,
                     template=None,
                     warm_start=False,
                     memory=None,
                     use_dask=True,
                     periodic_checkpoint_folder=None,
                     early_stop=2,
                     verbosity=2,
                     disable_update_check=False)

tpot.fit(X_train, y_train)

preds = tpot.predict([1.0,1.0,1.0])
print(r2_score(y_test, preds))
print(preds)
```

How can we make this work? Hacky solutions are welcome! :D",found thread getting error file line error input data valid format please confirm error input data valid format please confirm input data compatible example must array target must array know officially would love able use regression problem simple reproducible problem use code import import import import print print make work hacky welcome,issue,negative,positive,positive,positive,positive,positive
1156422317,"Hi @weixuanfu, I am getting the same message (though it doesn't seem to stop iteration) with a dataset containing ~600 observations and ~1200 features (tpot `0.11.7`). I think it might have to do with the execution using up all the available RAM and then running very slowly on swap. Let me know if there's any other information that might help.",hi getting message though seem stop iteration think might execution available ram running slowly swap let know information might help,issue,negative,positive,neutral,neutral,positive,positive
1129388188,"when use_dask is set to True, the n_jobs argument is actually not used. This should probably be fixed. When using dask it just uses dask.compute without setting the number of threads ([here](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1560)). My understanding is that this defaults to using all the threads available. 


To fix this, you need to have a dask wrapper around your .fit() command. For example:

```
import dask
from dask.distributed import Client, progress, LocalCluster

with LocalCluster(threads_per_worker=threads_per_worker, n_workers=n_workers, processes=processes,memory_limit=memory_limit) as cluster:
        with Client(cluster) as client:
            with dask.distributed.performance_report(filename=Dask_folder): #optional
                  est = tpot.TPOTClassifier()
                  est.fit(data)
```

On a single machine, I think it is best to use processes=False, n_workers=1, and threads_per_worker set to the number of cores/threads you want to use. (It should probably be benchmarked whether to have one worker and many threads or many workers with one thread, not 100% sure which option performs better).
",set true argument actually used probably fixed without setting number understanding available fix need wrapper around command example import import client progress cluster client cluster client optional data single machine think best use set number want use probably whether one worker many many one thread sure option better,issue,positive,positive,positive,positive,positive,positive
1129368842,"An additional useful feature, but may be more difficult to implement, would be to have FSS pass in different data to different ""branches"". For example:

```
    FFS -> classifier
                      \
                        > classifier
                      /
    FFS - > classifier

     (height, age, weight) -> Regression
                                         \
                                          > regression forest
                                         /
      Genes/proteins - > KNN
```

This could be possible by using FeatureUnion to group the outputs of two branches. TPOT could be initialized to have a FeatureUnion with a user specified number of items that begin with with FSS, the rest being determined through GP. ",additional useful feature may difficult implement would pas different data different example classifier classifier classifier height age weight regression regression forest could possible group two could user number begin rest determined,issue,negative,negative,neutral,neutral,negative,negative
1129344225,"I wanted to add another data replication issue. 

The FunctionTransformer module can also be set to exactly copy the input into the next layer. I have generated another pipeline where Several feature unions are stacked with multiple function transformers that are essentially just leading to multiple copies of the data. ",add another data replication issue module also set exactly copy input next layer another pipeline several feature multiple function essentially leading multiple data,issue,negative,positive,neutral,neutral,positive,positive
1117288505,"Hi @SSMK-wq,

did you find a work around to this? I don't see any documentation saying that TPOT handles encoding of categorical features, or different/predefined encoding, for example, ordinal vs one-hot encoding.",hi find work around see documentation saying categorical example ordinal,issue,negative,neutral,neutral,neutral,neutral,neutral
1114928570,"Hey,

Thanks for the update.

Charles

 

From: Joe Romano ***@***.***> 
Sent: Saturday, April 30, 2022 4:46 PM
To: EpistasisLab/tpot ***@***.***>
Cc: Charles Brauer ***@***.***>; Author ***@***.***>
Subject: Re: [EpistasisLab/tpot] My dataet crashed TOP-NN (Issue #1247)

 

It seems to be an issue when templates are used in conjunction with config_dict='TPOT NN'. When I run your code without a template it runs fine, and the error persists when I swap out your data for a different dataset.

I'll need to do some digging to figure out exactly what is going on, but there seem to be 2 possible contributing factors:

*	The feature selector step returning an empty feature set
*	A bug in calling assert_all_finite() with two arguments instead of one at: https://github.com/EpistasisLab/tpot/blob/6448bdb71ba08b4a0447c640d2f05a05e1affc21/tpot/builtins/nn.py#L163

—
Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/1247#issuecomment-1114073334> , or unsubscribe <https://github.com/notifications/unsubscribe-auth/AAKBS4REXJNF65LQF7J545TVHXA3BANCNFSM5UTZPZOQ> .
You are receiving this because you authored the thread.  <https://github.com/notifications/beacon/AAKBS4XJRAHY6WEP4Y5PUMDVHXA3BA5CNFSM5UTZPZO2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOIJTWR5Q.gif> Message ID: ***@***.*** ***@***.***> >

",hey thanks update joe sent author subject issue issue used conjunction run code without template fine error swap data different need digging figure exactly going seem possible feature selector step empty feature set bug calling two instead one reply directly view thread message id,issue,negative,positive,neutral,neutral,positive,positive
1114073334,"It seems to be an issue when templates are used in conjunction with `config_dict='TPOT NN'`. When I run your code without a template it runs fine, and the error persists when I swap out your data for a different dataset.

I'll need to do some digging to figure out exactly what is going on, but there seem to be 2 possible contributing factors:

- The feature selector step returning an empty feature set
- A bug in calling `assert_all_finite()` with two arguments instead of one at: https://github.com/EpistasisLab/tpot/blob/6448bdb71ba08b4a0447c640d2f05a05e1affc21/tpot/builtins/nn.py#L163",issue used conjunction run code without template fine error swap data different need digging figure exactly going seem possible feature selector step empty feature set bug calling two instead one,issue,negative,positive,positive,positive,positive,positive
1113401109,"I suppose you meant to delete the first two lines.  
If I run:
```

def Main(g, p):
    X_train, y_train, X_test, y_test = LoadData()

    # clf = TPOTClassifier(config_dict='TPOT NN',
    #                      template='Selector-Transformer-PytorchLRClassifier',

    clf = TPOTClassifier(verbosity=2,
                         generations=g,
                         population_size=p,
                         random_state=7)
    clf.fit(X_train, y_train)
    print(clf.score(X_test, y_test))
    clf.export('tpot_nn_demo_pipeline.py')
```
I get the following results:
```
H:\HedgeTools\ML_Model_Generation\TPOT-NN>python tpot-NN-rocket-classify.py
Operating system version.... Windows-10-10.0.22000-SP0
Python version is........... 3.8.13
pandas version is........... 1.4.2
numpy version is............ 1.21.5
tpot version is............. 0.11.7
           BoxRatio        Thrust  Acceleration      Velocity      OnBalRun      vwapGain        Expect          Trin
count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000
mean       2.061707      1.677448      1.935544      0.635225      2.412940      0.984372     -3.026383      0.834455
std        4.491026      3.056146      1.956287      0.658155      1.602910      0.932878     10.023122      0.284409
min        0.034120      0.000383      0.000112      0.000839      0.048550      0.100003    -50.341116      0.280000
25%        0.344533      0.228764      0.566531      0.155102      1.463102      0.379476     -6.661925      0.600000
50%        0.693704      0.713193      1.606062      0.460673      2.086361      0.730599     -2.334339      0.800000
75%        1.619198      1.790019      2.705824      0.903497      2.905308      1.275189      1.273494      1.040000
max       74.699990     40.539430     27.995832      7.809622     22.693728     11.762206     51.561442      4.540000
Size of dataset:
 train shape...  (48000, 8) (48000,)
 test shape....  (12000, 8) (12000,)

Generation 1 - Current best internal CV score: 0.9716458333333333

Generation 2 - Current best internal CV score: 0.9843125

Generation 3 - Current best internal CV score: 0.9847291666666667

Generation 4 - Current best internal CV score: 0.9859375

Generation 5 - Current best internal CV score: 0.9871041666666667

Generation 6 - Current best internal CV score: 0.9876875

Generation 7 - Current best internal CV score: 0.9876875

Generation 8 - Current best internal CV score: 0.9892291666666667

Generation 9 - Current best internal CV score: 0.9909791666666667

Generation 10 - Current best internal CV score: 0.9909791666666667

Best pipeline: KNeighborsClassifier(DecisionTreeClassifier(RandomForestClassifier(RFE(CombineDFs(input_matrix, input_matrix), criterion=gini, max_features=0.6500000000000001, n_estimators=100, step=0.1), bootstrap=False, criterion=gini, max_features=0.1, min_samples_leaf=3, min_samples_split=20, n_estimators=100), criterion=gini, max_depth=2, min_samples_leaf=9, min_samples_split=9), n_neighbors=6, p=2, weights=distance)
0.9926666666666667

Total compute time was: 01:23:05

```

I've never had good results with neural networks anyway. And yes, I've tried TabNet.  TPOT beats TabNet every time.
Charles
",suppose meant delete first two run main print get following python operating system version python version version version version thrust acceleration velocity expect trin count mean min size train shape test shape generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline total compute time never good neural anyway yes tried every time,issue,positive,positive,positive,positive,positive,positive
1113270435,"OK, Is this what you wanted?

```
def Main(g, p):
    X_train, y_train, X_test, y_test = LoadData()

    # clf = TPOTClassifier(config_dict='TPOT NN',
    clf = TPOTClassifier(template='Selector-Transformer-PytorchLRClassifier',
                         verbosity=2,
                         generations=g,
                         population_size=p,
                         random_state=7)
    clf.fit(X_train, y_train)
    print(clf.score(X_test, y_test))
    clf.export('tpot_nn_demo_pipeline.py')
```

Now I get:

```
H:\HedgeTools\ML_Model_Generation\TPOT-NN>python tpot-NN-rocket-classify.py
Operating system version.... Windows-10-10.0.22000-SP0
Python version is........... 3.8.13
pandas version is........... 1.4.2
numpy version is............ 1.21.5
tpot version is............. 0.11.7
           BoxRatio        Thrust  Acceleration      Velocity      OnBalRun      vwapGain        Expect          Trin
count  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000  60000.000000
mean       2.061707      1.677448      1.935544      0.635225      2.412940      0.984372     -3.026383      0.834455
std        4.491026      3.056146      1.956287      0.658155      1.602910      0.932878     10.023122      0.284409
min        0.034120      0.000383      0.000112      0.000839      0.048550      0.100003    -50.341116      0.280000
25%        0.344533      0.228764      0.566531      0.155102      1.463102      0.379476     -6.661925      0.600000
50%        0.693704      0.713193      1.606062      0.460673      2.086361      0.730599     -2.334339      0.800000
75%        1.619198      1.790019      2.705824      0.903497      2.905308      1.275189      1.273494      1.040000
max       74.699990     40.539430     27.995832      7.809622     22.693728     11.762206     51.561442      4.540000
Size of dataset:
 train shape...  (48000, 8) (48000,)
 test shape....  (12000, 8) (12000,)
Traceback (most recent call last):
  File ""C:\anaconda3\lib\site-packages\tpot\base.py"", line 496, in _add_operators
    operator = next(
StopIteration

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tpot-NN-rocket-classify.py"", line 84, in <module>
    Main(10, 10)
  File ""tpot-NN-rocket-classify.py"", line 70, in Main
    clf.fit(X_train, y_train)
  File ""C:\anaconda3\lib\site-packages\tpot\base.py"", line 725, in fit
    self._fit_init()
  File ""C:\anaconda3\lib\site-packages\tpot\base.py"", line 618, in _fit_init
    self._setup_pset()
  File ""C:\anaconda3\lib\site-packages\tpot\base.py"", line 437, in _setup_pset
    self._add_operators()
  File ""C:\anaconda3\lib\site-packages\tpot\base.py"", line 500, in _add_operators
    raise ValueError(
ValueError: An error occured while attempting to read the specified template. Please check a step named PytorchLRClassifier

H:\HedgeTools\ML_Model_Generation\TPOT-NN>pause
Press any key to continue . . .
```
",main print get python operating system version python version version version version thrust acceleration velocity expect trin count mean min size train shape test shape recent call last file line operator next handling exception another exception recent call last file line module main file line main file line fit file line file line file line raise error read template please check step pause press key continue,issue,negative,positive,neutral,neutral,positive,positive
1094621188,"@mfWeeWee @pvanwylenmtr @spiros @bollwyvl @zarch 
Sorry to bother you, but is there a chance that you have an update on this? I had the same issue, and I am stuck. Thanks!",sorry bother chance update issue stuck thanks,issue,negative,negative,negative,negative,negative,negative
1094297065,"if you have a chance to reply
was there ever an answer to this? @spiros @bollwyvl @zarch @cottrell @mrocklin 
because I have a similar issue 
![image](https://user-images.githubusercontent.com/39579959/162626992-ca6a5f64-8b14-4437-af5d-a19496fe3e1d.png)
",chance reply ever answer similar issue image,issue,negative,neutral,neutral,neutral,neutral,neutral
1080908932,it may be working now after a refresh. and playing with logs a bit. ,may working refresh bit,issue,negative,neutral,neutral,neutral,neutral,neutral
1078700096,"code :
from fbprophet import Prophet

error:
RuntimeError: 'path' must be None or a list, not <class '_frozen_importlib_external._NamespacePath'>

Please resolve my error",code import prophet error must none list class please resolve error,issue,negative,neutral,neutral,neutral,neutral,neutral
1032257908," Thank you for your reply and clarification.



Regards,

Amar Doshi    On Tuesday, 8 February, 2022, 01:58:22 am IST, Nick Becker ***@***.***> wrote:  
 
 


Hi @GitAronas . It looks like you're using Windows, based on your screenshot. The cuML configuration requires the library cuML, which is not available natively on Windows. cuML requires Ubuntu or similar Linux operating systems.

It may work with the new Windows Subsystem for Linux, but it's not officially supported.

Separately, this cuML version is now quite a bit out of date. I'll build an environment with a more recent cuML, run the unit and integration tests locally, and open a PR to update tpot-cuml.yml as appropriate to trigger CI.

—
Reply to this email directly, view it on GitHub, or unsubscribe.
Triage notifications on the go with GitHub Mobile for iOS or Android.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  ",thank reply clarification amar ist nick becker wrote hi like based configuration library available natively similar operating may work new subsystem officially separately version quite bit date build environment recent run unit integration locally open update appropriate trigger reply directly view triage go mobile android id,issue,positive,positive,positive,positive,positive,positive
1031889347,"Hi @GitAronas . It looks like you're using Windows, based on your screenshot. The cuML configuration requires the library cuML, which is not available natively on Windows. cuML requires Ubuntu or similar Linux operating systems.

It may work with the new Windows Subsystem for Linux, but it's not officially supported.

Separately, this cuML version is now quite a bit out of date. I'll build an environment with a more recent cuML, run the unit and integration tests locally, and open a PR to update `tpot-cuml.yml` as appropriate to trigger CI.",hi like based configuration library available natively similar operating may work new subsystem officially separately version quite bit date build environment recent run unit integration locally open update appropriate trigger,issue,negative,positive,positive,positive,positive,positive
1015320274,"I think I have the same problem. Can work up to a PR but would need a few pointers. 

My /tmp in root directory is the one filling up with long runs and eventually kills the main tpot process (with error message 'None'). Does the `memory='auto'` flag automatically points to 'tmp' or is there somewhere else that points to this?

The solutions above are clearing from RAM, right? If the issue is disk-space, don't files need to be deleted?

Any solution will have to adjust the process and messages re ""Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation."", right?

Thanks. ",think problem work would need root directory one filling long eventually main process error message flag automatically somewhere else clearing ram right issue need solution adjust process pipeline previously optimization process score previous evaluation right thanks,issue,negative,positive,neutral,neutral,positive,positive
990057082,Thanks @vikeshsingh37 . This fixed the issue for me as well. Removing `xgboost.XGBClassifier` in my case did the job.,thanks fixed issue well removing case job,issue,positive,positive,positive,positive,positive,positive
979377431,"You can fix this by defining a new config dict, rather than modifying and using an existing config that contains the unwanted preprocessing steps. Something like this:

```{python}
my_config_dict = {
    'xgboost.XGBClassifier': { 
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01) 
    },
    'sklearn.svm.LinearSVC': {
        'penalty': [""l1"", ""l2""],
        'loss': [""hinge"", ""squared_hinge""],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
    },
    ...
}
```

And so on, with whichever estimators you want to use. If you want to ONLY test the XGBoost classifier, you would just have a block containing that config. Then you can use `my_config_dict` when you instantiate `TPOTClassifier`:

```{python}
clf = TPOTClassifier(...,config_dict=my_config_dict)
...
```

If you want to use many estimators and don't want to type all of them out, you can just copy/paste the default classifier config dict and remove the things you aren't interested in including in your pipelines, like found here: https://github.com/EpistasisLab/tpot/blob/6448bdb71ba08b4a0447c640d2f05a05e1affc21/tpot/config/classifier.py

How does this work for you?",fix new rather unwanted something like python range hinge true false whichever want use want test classifier would block use python want use many want type default classifier remove interested like found work,issue,positive,positive,positive,positive,positive,positive
971682373,Also having this issue. It just stays static at 0/_____ for the entire time it's working. The only progress update I get is when it tells me that a particular generation has finished.,also issue stay static entire time working progress update get particular generation finished,issue,negative,positive,positive,positive,positive,positive
962033113,"I am not sure if I understood the comments correctly.
If I would like to stop TPOT and restart it from the last state I would have to dump TPOT to a file, read it again and start with warm_start=true?
Example usecase: run tpot, shutdown computer, restart computer, run tpot from last state

tpot = TPOTClassifier(generations=100, verbosity=2, memory=""./tpot-memory"", warm_start=true)
tpot.fit(x_train, y_train)
// after interrupting tpot
joblib.dump(clf, ""./tpot"")
// on restart
tpot = joblib.load(""./tpot"")
tpot.fit(x_train, y_train)
",sure understood correctly would like stop restart last state would dump file read start example run shutdown computer restart computer run last state interrupting restart,issue,negative,positive,positive,positive,positive,positive
959756320,"Give this a try:

```
my_dict = list(tpot.evaluated_individuals_.items())

model_scores = pd.DataFrame()
for model in my_dict:
    model_name = model[0]
    model_info = model[1]
    cv_score = model[1].get('internal_cv_score')  # Pull out cv_score as a column (i.e., sortable)
    model_scores = model_scores.append({'model': model_name,
                                        'cv_score': cv_score,
                                        'model_info': model_info,},
                                       ignore_index=True)

model_scores = model_scores.sort_values('cv_score', ascending=False)
top_models = model_scores.iloc[0:5,:]
top_models.to_csv('top_models.csv', index = False)
```
See [https://github.com/EpistasisLab/tpot/issues/703](url)",give try list model model model model pull column sortable index false see,issue,negative,negative,negative,negative,negative,negative
956672002,"Hi @wayneking517, thanks for your issue report. 

Currently, from what I can see, you have implemented the preprocessor into the configuration dictionary correctly. However, when instantiating the TPOT object, you have passed the argument:
`template='Regressor'`

This template means that TPOT will only include regressors in the constructed and evaluated pipelines. 

If you want to include preprocessors/transformers, you should either remove the template argument (so that TPOT will try to use all options in the configuration dictionary and not just the regressors, which in this case is only the GaussianProcessRegressor) or change this argument to:
`template='Transformer-Regressor'`

Hope this helps.",hi thanks issue report currently see configuration dictionary correctly however object argument template include want include either remove template argument try use configuration dictionary case change argument hope,issue,negative,positive,neutral,neutral,positive,positive
931828792,Would love to see multioutput regression/classification working with tpot! Keep going! Haha ,would love see working keep going,issue,positive,positive,positive,positive,positive,positive
918818507,"Hi,tpot team,
Someone encountered the same problem",hi team someone problem,issue,negative,neutral,neutral,neutral,neutral,neutral
912722268,"I have the same Problem and i am on Ubuntu 20.04.3 LTS, too
After 10 h Jobrunning - Working as root.

`Optimization Progress:  11%|████████████▉                                                                                                       | 336/3000 [9:19:43<94:12:09, 127.30s/pipeline]
Killed`


Greetz
Bjoern",problem working root optimization progress,issue,negative,neutral,neutral,neutral,neutral,neutral
911992076,"Looking at the source code of TPOT's `StackingEstimator`, it is just a transformer (like `StandardScaler`) and only depends on numpy and sklearn. So a workaround is to copy this class to your project and you will essentially only use sklearn models.

https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/stacking_estimator.py",looking source code transformer like copy class project essentially use,issue,negative,neutral,neutral,neutral,neutral,neutral
908704985,"I'd like to re-ask the original question. How can one output information about performance of different solvers (perhaps visualizing ranges, colored scatter plots, etc.) and the impact of different parameters on specific solvers in a way that can be easily visualized. I can't see how to pick tpot.evaluated_individuals_ apart 

Here is part of tpot.evaluated_individuals_:


```{'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=100.0, GaussianProcessRegressor__kernel=1**2 * RationalQuadratic(alpha=0.1, length_scale=0.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.8067110730819081}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=0.01, GaussianProcessRegressor__kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=5e-09, GaussianProcessRegressor__kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.7751851811051406}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=10.0, GaussianProcessRegressor__kernel=1**2 * Matern(length_scale=0.5, nu=1.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=1.0, GaussianProcessRegressor__kernel=1**2 * Matern(length_scale=0.5, nu=1.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': -0.013497379992263215}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=1.0, GaussianProcessRegressor__kernel=1**2 * RBF(length_scale=0.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=5e-09, GaussianProcessRegressor__kernel=1**2 * RationalQuadratic(alpha=0.1, length_scale=0.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.7965992910211398}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=0.001, GaussianProcessRegressor__kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=100.0, GaussianProcessRegressor__kernel=0.316**2 * DotProduct(sigma_0=1) ** 2 + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.7844064509089768}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=0.001, GaussianProcessRegressor__kernel=1**2 * Matern(length_scale=0.5, nu=1.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=100.0, GaussianProcessRegressor__kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=True, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.8176275484733164}, 'GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=0.01, GaussianProcessRegressor__kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)': {'generation': 1, 'mutation_count': 1, 'crossover_count': 0, 'predecessor': ('GaussianProcessRegressor(input_matrix, GaussianProcessRegressor__alpha=100.0, GaussianProcessRegressor__kernel=1**2 * RationalQuadratic(alpha=0.1, length_scale=0.5) + WhiteKernel(noise_level=0.1), GaussianProcessRegressor__normalize_y=False, GaussianProcessRegressor__optimizer=fmin_l_bfgs_b)',), 'operator_count': 1, 'internal_cv_score': 0.7799099422133781}}```",like original question one output information performance different perhaps colored scatter impact different specific way easily ca see pick apart part,issue,positive,positive,positive,positive,positive,positive
897103781,"I've tried to tune up the GP model using TPot. 

The optimized model is:

```
exported_pipeline = GaussianProcessRegressor(alpha=0.01, kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3), normalize_y=False, optimizer=""fmin_l_bfgs_b"")
```
When I invoke the regressor in my code, I get:
```
17**2 * ExpSineSquared(length_scale=0.000764, periodicity=0.000302)
/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/gaussian_process/_gpr.py:504: ConvergenceWarning: lbfgs failed to converge (status=2):
ABNORMAL_TERMINATION_IN_LNSRCH.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  _check_optimize_result(""lbfgs"", opt_res)
```
I am wondering if you might know what Tpot does to avoid this error.
",tried tune model model invoke regressor code get converge increase number scale data shown wondering might know avoid error,issue,negative,neutral,neutral,neutral,neutral,neutral
893125061,"I'm thinking it could be because my dataset is too big, even at 5,000 rows. I shrunk it down to just a couple hundred rows and it's much quieter. Maybe once it loads all copies of the dataset into memory it would settle down?? Population size seems to play a factor as well...",thinking could big even shrunk couple hundred much quieter maybe memory would settle population size play factor well,issue,positive,positive,neutral,neutral,positive,positive
891277613,"It seems the following works:

- Pickle dump fitted pipeline and imputed flag
```python
with open(""./tpot_model.pickle"", ""wb"") as f:
    pickle.dump({""template"": tpot.fitted_pipeline_, ""_imputed"": tpot._imputed}, f)
```

- Pickle load and reconstruct object:
```python
with open(""./tpot_model.pickle"", ""rb"") as f:
    tpot_args = pickle.load(f)

tpot = TPOTRegressor(template=tpot_args[""template""])
tpot_args[""fitted_pipeline_""] = tpot_args[""template""]
del tpot_args[""template""]
for k, v in tpot_args.items():
    setattr(tpot, k, v)
```",following work pickle dump fitted pipeline flag python open template pickle load reconstruct object python open template template template,issue,negative,neutral,neutral,neutral,neutral,neutral
890546161,"Thank you for raising this issue @zuliani99. 

I do not have a Dask cluster to test on, unfortunately, so I cannot reproduce your issue properly. However, **you may want to use TPOT's built-in Dask support instead of wrapping it in a Dask call**: https://examples.dask.org/machine-learning/tpot.html. TPOT supports Dask via the `use_dask` argument when instantiating TPOT, and it handles training and evaluating much more intuitively if you use it this way than if you call the fit function within a Dask call. 

**I suspect this issue is in part due to how processes are spawned in Dask/Joblib that then wait for a longer time than TPOT's built-in pretest time limit. Using TPOT's built-in support should mitigate this problem as the processes will be spawned when they are needed (during pipeline evaluation), rather than throughout the entire TPOT process.**

The `max_eval_time_mins` parameter will properly cut off pipelines that exceed that time when evaluating them on the full dataset, but TPOT also has a pretest sanity check that passes a tiny, subsampled subset of the data through a clone of the pipeline just to make sure there aren't errors associated with the structure of the pipeline that occur due to the genetic mutations applied (as a genetic mutation can generate invalid pipelines). This sanity check has a limit of 10 seconds as it is solely for the purpose of making sure the pipeline is a valid pipeline (if not, it will remutate it until it is). 

Dask's spawning of processes likely interacts with this in a bad way and leads processes to accumulate or hang over time, and these processes don't need to be multithreaded.

You bring up a good point - TPOT should fail gracefully if the pretest times out and be clearer that this is the pretest timing out (rather than the evaluation of the pipeline on the full dataset). 


------
For future TPOT developers (in case I am unable to fix this later), we'll need to catch the `TimeoutException` (which inherits from `Exception` and not `BaseException`) here in `decorators.py`: https://github.com/EpistasisLab/tpot/blob/6448bdb71ba08b4a0447c640d2f05a05e1affc21/tpot/decorators.py#L105 
This could be done by just adding another `except` statement in the same way we catch `BaseException` so that TPOT handles the error gracefully.

The issue with Dask causing this timeout is not an issue we can fix as this is most likely caused by using Dask around the entire TPOT call rather than using the `use_dask` parameter when instantiating TPOT (as this will use Dask more intelligently when it needs to be used).",thank raising issue cluster test unfortunately reproduce issue properly however may want use support instead wrapping call via argument training much intuitively use way call fit function within call suspect issue part due wait longer time pretest time limit support mitigate problem pipeline evaluation rather throughout entire process parameter properly cut exceed time full also pretest sanity check tiny subset data clone pipeline make sure associated structure pipeline occur due genetic applied genetic mutation generate invalid sanity check limit solely purpose making sure pipeline valid pipeline spawning likely bad way accumulate time need multithreaded bring good point fail gracefully pretest time clearer pretest timing rather evaluation pipeline full future case unable fix later need catch exception could done another except statement way catch error gracefully issue causing issue fix likely around entire call rather parameter use intelligently need used,issue,positive,positive,neutral,neutral,positive,positive
890542473,"@breaker505 In the future, please follow the template provided - it makes it easier to debug and read your issue.

The problem described in the title of your issue is likely because you have named a file within your directory (or the script you are running itself) ""tpot.py"". Because of this, Python is attempting to import the relevant TPOT classes and functions from this file instead of from the installed TPOT package. Rename this file to something else (maybe ""tpot_script.py"" or something similar), and you should be able to import properly.",breaker future please follow template provided easier read issue problem title issue likely file within directory script running python import relevant class file instead package rename file something else maybe something similar able import properly,issue,negative,positive,positive,positive,positive,positive
890349232,"What if I want to reoptimize the fitted pipeline after loading it? In other words, I'd like to do:

0. Collect initial data
1. Fit `TPOTRegressor` object
2. Pickle dump
3. Collect more data
4. Pickle load
5. Refit `TPOTRegressor` object",want fitted pipeline loading like collect initial data fit object pickle dump collect data pickle load refit object,issue,negative,positive,positive,positive,positive,positive
890021178,I think it would be great if TPOT could detect sparse input data automatically and switch the `config_dict` to 'TPOT sparse' if it wasn't already manually set.,think would great could detect sparse input data automatically switch sparse already manually set,issue,positive,positive,positive,positive,positive,positive
889871914,Yes I think it depends entirely on how hands-off you want the automl experience to be and what the expected data science experience of the user is.,yes think entirely want experience data science experience user,issue,positive,neutral,neutral,neutral,neutral,neutral
889572273,"Note that this PR will fail to pass checks and the proposed regressor will not function due to its use of the kernels within the configuration dictionary, which will cause TPOT to crash when trying to evaluate any pipeline string with these kernels due to them not existing in TPOT's operator space as TPOT does not consider parameters when importing operators. 

#1218 touches on some of the changes that would need to be made to TPOT to support these kind of configurations (needing to either allow the user to pass references to objects used as parameters when instantiating TPOT or finding a way to dynamically assign those references within TPOT itself). Until those changes are made, I don't think this PR can be merged quite yet.",note fail pas regressor function due use within configuration dictionary cause crash trying evaluate pipeline string due operator space consider would need made support kind needing either allow user pas used finding way dynamically assign within made think quite yet,issue,positive,negative,neutral,neutral,negative,negative
889498812,"> Thank you very much for the elaborate response. I was aware of the underlying issue, but I wasn't aware it was a design decision not to address it within TPOT. I understand the decision, feel free to close the issue if desired.

Admittedly, I'm not sure if we should or shouldn't handle this case as opposed to relying on the user to know the drawbacks of imbalanced data and/or the limits of the metrics they choose. My logic is that processing the data in any way that isn't fully transparent to the user and/or consistent across all cases will be problematic and that it's better to leave it up to the user as to how they want to handle the situation (since there are many options and the best one will likely depend on what the user knows about their data and the importance of the outlier class - for example, in biomedical data, imbalances are common but usually highly important, like in cases where you have extraordinarily rare diseases with few cases against a large number of ""control"" cases). 

That being said, I'll have to talk with the rest of the lab that supports TPOT to see what the best choice might be. Thank you for raising the issue! We'll keep it open for now while we think about the best way to handle this - we may need to be clearer about this in the documentation or keep it in mind for future TPOT extensions/modificiations.",thank much elaborate response aware underlying issue aware design decision address within understand decision feel free close issue desired admittedly sure handle case opposed user know data metric choose logic data way fully transparent user consistent across problematic better leave user want handle situation since many best one likely depend user data importance outlier class example data common usually highly important like extraordinarily rare large number control said talk rest lab see best choice might thank raising issue keep open think best way handle may need clearer documentation keep mind future,issue,positive,positive,positive,positive,positive,positive
889489339,"Thank you very much for the elaborate response. I was aware of the underlying issue, but I wasn't aware it was a design decision not to address it within TPOT.  I understand the decision, feel free to close the issue if desired.",thank much elaborate response aware underlying issue aware design decision address within understand decision feel free close issue desired,issue,positive,positive,positive,positive,positive,positive
889327982,"@PGijsbers Thank you for submitting this issue, for the detail, and for the minimally reproducible example. 

It seems that this is an issue when a class is unobserved in any of the cross-validation folds that TPOT generated (by default, it uses `StratifiedKFold` with 5 folds to generate the cross-validation splits). sklearn's `log_loss` metric will then be passed an array that is missing data for one or more of the classes. 

You can reduce the number of folds performed by TPOT so that it is less than the number of instances of the smallest class or create your own cross-fold generator that ensures at least one of each class exists in the data passed when fitting the pipeline for scoring (both of these use the `cv` argument when instantiating TPOT). This is irrelevant with non-probability-based metrics (and TPOT will only broadcast the sklearn warnings from StratifiedKFold) as those handle missing classes appropriately.  

This is an issue that occurs in native sklearn and is due to how `log_loss` handles missing classes (or a lack of handling thereof): see https://github.com/scikit-learn/scikit-learn/issues/11777 and https://github.com/scikit-learn/scikit-learn/issues/15389. We can attempt to write code to handle this on our end, but I'm of the opinion that it's better to leave it up to sklearn to correct these issues and to handle this within their own scoring functions. 

In theory, we could eliminate/ignore sparsely-populated classes either in preprocessing or when evaluating pipelines, but as TPOT can otherwise handle cases like this and properly construct and mutate pipelines with most other metrics (for example, if you use the basic accuracy metric), this doesn't seem like the best approach to take without user input and may be something better left to the user to do, as the approach to removing outliers or handling classes with few instances will likely differ significantly based on the meta-features of the input dataset.

It is possible to handle this and use a larger number of folds without modifying the functionality of TPOT or sklearn and maintain the use of the `log_loss` metric. One option is to write a custom `log_loss` metric that essentially pads the reported probabilities with probabilities of 0 for missing classes before passing them to sklearn's `log_loss`. I've written a demo of this below:

```python
from tpot import TPOTClassifier
import numpy as np
from sklearn.metrics import log_loss, make_scorer

x, y = np.random.random((151, 4)), np.asarray([0] * 75 + [1] * 75 + [2])
labels = np.unique(y)

def mod_log_loss(y_true, y_pred, labels):
	class_diff = len(labels) - len(y_pred[0])

	if(class_diff > 0):
		y_pred_pad = np.array([np.pad(x, pad_width=(0,class_diff)) for x in y_pred])
	else:
		y_pred_pad = y_pred

	return(log_loss(y_true, y_pred_pad, labels=labels))

mod_neg_log_loss = make_scorer(mod_log_loss, greater_is_better=False, labels=labels, needs_proba=True)

t = TPOTClassifier(max_time_mins=1, scoring=mod_neg_log_loss)
t.fit(x,y)
t.predict(x)
```

Note that this demo assumes that the missing classes are the last classes (as it pads at the end of the probability vectors). In theory, you could instead determine which classes are present in `y_true` that are missing from `labels` (as y_true will be passed from the cross-validation scoring) and pad in those locations instead if the sparsely-populated classes are not the last classes in the dataset, though I have not tested this. 

Let us know if you have any thoughts or questions! ",thank issue detail minimally reproducible example issue class unobserved default generate metric array missing data one class reduce number le number class create generator least one class data fitting pipeline scoring use argument irrelevant metric broadcast handle missing class appropriately issue native due missing class lack handling thereof see attempt write code handle end opinion better leave correct handle within scoring theory could class either otherwise handle like properly construct mutate metric example use basic accuracy metric seem like best approach take without user input may something better left user approach removing handling class likely differ significantly based input possible handle use number without functionality maintain use metric one option write custom metric essentially missing class passing written python import import import else return note missing class last class end probability theory could instead determine class present missing scoring pad instead class last class though tested let u know,issue,positive,positive,neutral,neutral,positive,positive
888638002,"Apologies - I misspelled the first instance of ""gaussian_config"" as ""guassian_config"" in the demo above. I've corrected the misspelling above, but this should fix the issue with needing to replace the variable names with another variable (since that would make sure they are the same).

To answer your question: StackingEstimator is a built-in TPOT operator that will take an estimator (a classifier or regressor; in this case, your GaussianProcessRegressor) and append the results of that regressor to the feature set as a synthetic feature. 

In essence, the pipeline here is generating a regression output from the first regressor and appending that as a synthetic feature to the features, then passing the combination to the second regressor which generates its own regression output that is also appended to the features, then finally the total set of features (consisting of your input + 2 synthetic features from the prior regressors) is predicted on by the final regressor. ",first instance corrected misspelling fix issue needing replace variable another variable since would make sure answer question operator take estimator classifier regressor case append regressor feature set synthetic feature essence pipeline generating regression output first regressor synthetic feature passing combination second regressor regression output also finally total set input synthetic prior final regressor,issue,negative,positive,positive,positive,positive,positive
888635791,"Thank you. To get this to work, I had  to replace each instance of ""guassian_config"" with ""c"" (our choose your own variable.) 

```
# Average CV score on the training set was: -0.4411353932258676
exported_pipeline = make_pipeline(
    StackingEstimator(estimator=GaussianProcessRegressor(alpha=1.0, kernel=1**2 * Matern(length_scale=0.5, nu=0.5), normalize_y=False, optimizer=""fmin_l_bfgs_b"")),
    StackingEstimator(estimator=GaussianProcessRegressor(alpha=100.0, kernel=1**2 * ExpSineSquared(length_scale=0.5, periodicity=3), normalize_y=False, optimizer=""fmin_l_bfgs_b"")),
    GaussianProcessRegressor(alpha=5e-09, kernel=0.316**2 * DotProduct(sigma_0=1) ** 2, normalize_y=True, optimizer=""fmin_l_bfgs_b"")
```

What is StackingEstimator?",thank get work replace instance choose variable average score training set,issue,negative,negative,negative,negative,negative,negative
888426257,"@wayneking517 I see the issue - I had assumed RBF and the other kernel functions returned simple arrays/lists, which TPOT can properly handle when evaluating pipelines. However, they return kernel type objects from sklearn, and passing in arbitrary objects as hyperparameter options is not currently supported using TPOT's current methods. 

There is a workaround for this, which involves appending the contexts needed for the kernels to be evaluated properly to TPOT's operator contexts, which I've provided a demo of below:

```
from tpot import TPOTRegressor

import sklearn
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split

from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel, RationalQuadratic, \
    Exponentiation, RBF, ExpSineSquared, DotProduct
from sklearn.gaussian_process import GaussianProcessRegressor


X, y = make_regression()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.22, random_state=2)

gaussian_config = {
    'sklearn.gaussian_process.GaussianProcessRegressor': {
          'kernel': [1.0*RBF(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0)),
                          1.0*RationalQuadratic(length_scale=0.5, alpha=0.1),
                          1.0*ExpSineSquared(length_scale=0.5, periodicity=3.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1.0, 10.0)),
                          ConstantKernel(0.1, (0.01, 10.0))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
                          1.0**2*Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0),nu=0.5)],
          'alpha': [5e-9, 1e-3, 1e-2, 1e-1, 1., 10., 100.],
          'normalize_y': [True, False],
          'optimizer': ['fmin_l_bfgs_b']
    },
}

kernel_dict = {
	""RBF"": RBF,
    ""RationalQuadratic"": RationalQuadratic,
    ""ExpSineSquared"": ExpSineSquared,
    ""ConstantKernel"": ConstantKernel,
    ""DotProduct"": DotProduct,
    ""Matern"": Matern,
}

tpot_obj = TPOTRegressor(generations=5,
	population_size=50,
	verbosity=2,
	cv=5,
	config_dict=gaussian_config,
	random_state=42)

tpot_obj._fit_init()
tpot_obj.operators_context.update(kernel_dict)
tpot_obj.warm_start = True

tpot_obj.fit(X_train, y_train)
```

Modify the line defining `X, y` to the line relevant to your pipeline.

If you are interested in the technical details of what we are doing in the demo above: essentially, we are creating a context dictionary for the kernels (RBF, RationalQuadratic, etc.) so that TPOT knows where to look for them if it finds them in a pipeline string. We then instantiate TPOT and call its internal _fit_init function to set the existing operator contexts (with some special functions that TPOT uses to construct pipelines) and then append the kernel context dictionary we made earlier to that operator context. We set ""warm_start"" to true simply to indicate that we have already called _fit_init() and to prevent TPOT from overwriting the context dictionary we have set, and then we run TPOT.



This demo will allow TPOT to construct a pipeline, though the only operator in its search space with the config above would be the GaussianProcessRegressor. To add more operators, you would need to define more operators in the dictionary you pass in or append this to another config dictionary, which you could do by importing an existing configuration dictionary, appending it to your custom configuration for the Gaussian Process Regressor, and then passing the combined configuration dictionary to TPOT: 

```
from tpot.config.regressor import regressor_config_dict

gaussian_config.update(regressor_config_dict)
```

In the future, we will look into the possibility of including a possible parameter to pass additional contexts to TPOT to support applications like this to further improve TPOT. Thank you for raising this issue and bringing this to our attention!


Hopefully this helps with what you are aiming to do. Please let me know if you have any additional questions!",see issue assumed kernel returned simple properly handle however return kernel type passing arbitrary currently current properly operator provided import import import import import exponentiation import true false true modify line line relevant pipeline interested technical essentially context dictionary look pipeline string call internal function set operator special construct append kernel context dictionary made operator context set true simply indicate already prevent context dictionary set run allow construct pipeline though operator search space would add would need define dictionary pas append another dictionary could configuration dictionary custom configuration process regressor passing combined configuration dictionary import future look possibility possible parameter pas additional support like improve thank raising issue attention hopefully aiming please let know additional,issue,positive,positive,neutral,neutral,positive,positive
887668536,"Here is my code:

```
from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ConstantKernel, RationalQuadratic, \
    Exponentiation, RBF, ExpSineSquared, DotProduct
from sklearn.gaussian_process import GaussianProcessRegressor


X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.22, random_state=2)


tpot_config = {
    'sklearn.gaussian_process.GaussianProcessRegressor': {
          'kernel': [1.0*RBF(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0)),
                          1.0*RationalQuadratic(length_scale=0.5, alpha=0.1),
                          1.0*ExpSineSquared(length_scale=0.5, periodicity=3.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1.0, 10.0)),
                          ConstantKernel(0.1, (0.01, 10.0))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
                          1.0**2*Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0),nu=0.5)],
          'alpha': [5e-9, 1e-3, 1e-2, 1e-1, 1., 10., 100.],
          'normalize_y': [True, False],
          'optimizer': ['fmin_l_bfgs_b']
    },
}
```
Here is the error:
```
Generation 1 - Current best internal CV score: -inf
Traceback (most recent call last):
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 828, in fit
    log_file=self.log_file_,
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/gp_deap.py"", line 281, in eaMuPlusLambda
    per_generation_function(gen)
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 1176, in _check_periodic_pipeline
    self._update_top_pipeline()
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 921, in _update_top_pipeline
    sklearn_pipeline = self._toolbox.compile(expr=pipeline)
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 1414, in _compile_to_sklearn
    sklearn_pipeline = eval(sklearn_pipeline_str, self.operators_context)
  File ""<string>"", line 2, in <module>
NameError: name 'RBF' is not defined

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/lindaking/PycharmProjects/ANN/Hahne_TPot.py"", line 57, in <module>
    tpot.fit(X_train, y_train)
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 863, in fit
    raise e
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 854, in fit
    self._update_top_pipeline()
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 921, in _update_top_pipeline
    sklearn_pipeline = self._toolbox.compile(expr=pipeline)
  File ""/Users/lindaking/PycharmProjects/ANN/venv/lib/python3.7/site-packages/tpot/base.py"", line 1414, in _compile_to_sklearn
    sklearn_pipeline = eval(sklearn_pipeline_str, self.operators_context)
  File ""<string>"", line 2, in <module>
NameError: name 'RBF' is not defined

Process finished with exit code 1
```",code import exponentiation import true false error generation current best internal score recent call last file line fit file line gen file line file line file line file string line module name defined handling exception another exception recent call last file line module file line fit raise file line fit file line file line file string line module name defined process finished exit code,issue,positive,positive,positive,positive,positive,positive
887570365,@ht1625 - please resubmit the issue with the information in the template filled out so we can understand the problem you are experiencing.,please resubmit issue information template filled understand problem,issue,negative,positive,positive,positive,positive,positive
887193576,"Sorry for the delay in responding. You are likely getting these errors because you have not imported the required functions to call RBF, DotProduct, etc. You need to import these from sklearn (include `from sklearn.gaussian_process.kernels import WhiteKernel, Matern, RBF, DotProduct, RationalQuadratic, ExpSineSquared` in your imports) or wherever you would like to import these functions from.

We did not develop or test this config file found in #1186 and #1191 - it seems that this was a custom one made by another user that has not been merged into TPOT because it has many build issues and errors in its current state. You will probably need to tweak the configuration appropriately to get things to work. Let us know if importing the needed functions resolves the issues or provides a different error message. 

If you define your own TPOT configuration, it will only use the operators from the configuration you pass to TPOT. The configuration above will make the `GaussianProcessRegressor` the only operator that TPOT considers. 

If you wish to include other operators in TPOT's search space, you will need to define them and their operators in the same manner you did the `GaussianProcessRegressor` (which you can do by copying the dictionary from a default configuration or by appending your dictionary to one of the default configurations). ",sorry delay likely getting call need import include import wherever would like import develop test file found custom one made another user many build current state probably need tweak configuration appropriately get work let u know different error message define configuration use configuration pas configuration make operator wish include search space need define manner dictionary default configuration dictionary one default,issue,negative,positive,neutral,neutral,positive,positive
886046241,"Thank you for you answer. I found the tpot_config file example at https://www.gitmemory.com/issue/EpistasisLab/tpot/1186/801378125.

I have made the recommended changes.

```
tpot_config = {
    'sklearn.gaussian_process.GaussianProcessRegressor': {
          'kernel': [1.0*RBF(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0)),
                          1.0*RationalQuadratic(length_scale=0.5, alpha=0.1),
                          1.0*ExpSineSquared(length_scale=0.5, periodicity=3.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1.0, 10.0)),
                          ConstantKernel(0.1, (0.01, 10.0))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
                          1.0**2*Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0),nu=0.5)],
          'alpha': [5e-9, 1e-3, 1e-2, 1e-1, 1., 10., 100.],
          'normalize_y': [True, False],
          'optimizer': ['fmin_l_bfgs_b']
    },
}
```

I now get ""NameError: name 'RBF' is not defined"". If I remove RBF, I get 'DotProduct' is not defined. If I remove 'DotProduct', i get TypeError: Cannot clone object '0.099856' (type <class 'float'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.

Also, if I use tpot_config do I have to include my own Preprocessors or will the built in ones be used?

Once again, thank you for tolerating these rookie questions.

",thank answer found file example made true false get name defined remove get defined remove get clone object type class seem estimator implement method also use include built used thank rookie,issue,positive,negative,neutral,neutral,negative,negative
885945170,"Hi @wayneking517. No worries - your question is absolutely alright to ask. In the future, when copying Python code, please use Github's code block by using three backticks to wrap your code as follows:

\```
'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
},
\```

This would be formatted as the following:

```
'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
},
```

This avoids issues where Github views part of your code as markdown (like italics and other formatting tags) and makes it hard to read. 


From first glance, the primary issue is that you aren't defining your TPOT configuration in the appropriate manner - the way you've defined your configuration creates a dictionary with keys 'kernel', 'alpha', 'normalize_y', and 'optimizer' with their corresponding values.

However, TPOT uses *a nested dictionary* to define operators and their parameters. Each key of the dictionary should be the name of an operator for TPOT to import, and the values associated with that key should be a dictionary like the one you have passed. Your dictionary is just the set of parameters but without the operator as a key - this makes TPOT think each of your parameters is an operator and it attempts to import them. As they are not valid operators, TPOT fails to import them and then errors due to having an empty operator set.

You can see an example of a full TPOT configuration dictionary here for information on how the formatting should work: https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py


Assuming you want to use this regressor as the only operator in TPOT: https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html, you can define a configuration dictionary in the vein of as follows (though please check the parameters as I had to infer some parts of your code due to Github mangling the formatting):

```
tpot_config = {
    'sklearn.gaussian_process.GaussianProcessRegressor': {
          'kernel': [1.0*RBF(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0)),
                          1.0*RationalQuadratic(length_scale=0.5, alpha=0.1),
                          1.0*ExpSineSquared(length_scale=0.5, periodicity=3.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1.0, 10.0)),
                          ConstantKernel(0.1, (0.01, 10.0))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
                          1.0**2*Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0),nu=0.5)],
          'alpha': [5e-9, 1e-3, 1e-2, 1e-1, 1., 10., 100.],
          'normalize_y': [True, False],
          'optimizer': ['fmin_l_bfgs_b']
    },
}
```


If you want to add additional operators, you need to continue building the dictionary by adding another element with the name of that operator as the key having a value of a dictionary of parameters with their associated values.

Let me know if you have any additional questions or concerns.",hi question absolutely alright ask future python code please use code block three wrap code true false would following true false part code markdown like italic hard read first glance primary issue configuration appropriate manner way defined configuration dictionary corresponding however dictionary define key dictionary name operator import associated key dictionary like one dictionary set without operator key think operator import valid import due empty operator set see example full configuration dictionary information work assuming want use regressor operator define configuration dictionary vein though please check infer code due mangling true false want add additional need continue building dictionary another element name operator key value dictionary associated let know additional,issue,positive,positive,neutral,neutral,positive,positive
882661257,"Hi @rachitk!

Great, thank you for the thorough and detailed response. Sounds to me like there's no problem here for either of us to be worried about, and this discrepancy is merely a result of sklearn being updated. Cool.

Have a good one,
-Peter",hi great thank thorough detailed response like problem either u worried discrepancy merely result cool good one,issue,positive,positive,positive,positive,positive,positive
882157565,"Hi @PeterGrant! Thanks for your question and for bringing this to our attention.

The [examples](https://epistasislab.github.io/tpot/examples/) in the documentation were run using a specific version of TPOT and sklearn, and the exact pipeline exported may be different based on package versions. The purpose of declaring the random state here is for reproducibility purposes with the same environment - to make it so that you can reproduce the same pipeline and run state across different runs of TPOT if you are using the same package versions. If you would like to explore different pipelines, you can initialize TPOT with different random seeds (or omit the random_state argument entirely for an arbitrary random seed) to see what different pipelines TPOT finds at random based on how it performs mutations and generates the initial population. 

I've tested the example you posted and created a virtual environment with the same package versions, and I do indeed get the same pipeline as you. The **difference from the example isn't due to any error on your part**, and you shouldn't be concerned as the variation here appears to be due to package versions and the pipeline found by TPOT here has comparable performance on the test set to the pipeline proposed in the example (an MSE of ~11.1 compared to an MSE of ~10.8), indicating that there isn't a major regression/degradation in performance that would need investigation as packages are updated/modified.

FYI: sklearn 0.24.0 in particular made several small changes to some of the logic associated with some operators, so the final pipeline generated may be different than the versions the original example was made using (which was back in sklearn <0.22). We will look back at the examples when we get a chance and update them to better reflect what might be expected if using a version of sklearn >0.24 as well as make it clearer that the exact pipelines returned may vary depending on what versions of the dependencies are used. There are some further changes coming in sklearn 0.25 (which they will be rebranding to version 1.0.0), so we may wait until that version is released before updating the documentation fully.

Let us know if you have any other questions or any followup concerns!",hi thanks question attention documentation run specific version exact pipeline may different based package purpose random state reproducibility environment make reproduce pipeline run state across different package would like explore different initialize different random omit argument entirely arbitrary random seed see different random based initial population tested example posted virtual environment package indeed get pipeline difference example due error part concerned variation due package pipeline found comparable performance test set pipeline example major performance would need investigation particular made several small logic associated final pipeline may different original example made back look back get chance update better reflect might version well make clearer exact returned may vary depending used coming version may wait version documentation fully let u know,issue,positive,negative,neutral,neutral,negative,negative
878726043,"You should use `n_jobs=1` (the default). cuML is currently designed for the ""one process per GPU"" paradigm"". Additionally, how are you setting up your Dask cluster?

It might be valuable to test your system and environment with this [example gist](https://gist.github.com/beckernick/340c29e4543eb6056ef945a08ef88d09) or confirm your configuration is similar.",use default currently designed one process per paradigm additionally setting cluster might valuable test system environment example gist confirm configuration similar,issue,negative,neutral,neutral,neutral,neutral,neutral
875977944,"> I believe ^ is what the bestpipeline i mentioned in the question implies. Tpot is somehow using linearSVC and stacking its feature and ultimately training decisiontreeclassifier over it. Please correct me if i am wrong here.

Your interpretation is my understanding as well. TPOT will use a special operator (not always obvious from the base TPOT output, but is usually clearer in the exported Python file) that will append the outputs of classifiers that are not the final classifier in the pipeline to the original input matrix to allow for nonlinear tree generation and for classifiers to be included elsewhere in the pipeline besides at the very end. That is likely what happened here if the pipeline at the end of your issue is the same one causing the error you see (the output of LinearSVC was appended to the original input matrix as a synthetic feature for DecisionTreeClassifier to incorporate). 

Doing `exctracted_best_model = pipeline_optimizer.fitted_pipeline_.steps[-1][1]` only pulls out the final operator within the pipeline, rather than the entire best pipeline itself (that would be just `pipeline_optimizer.fitted_pipeline_`).

By only selecting the last operator in the sklearn pipeline, you're skipping the steps that would add that feature to the original input matrix, causing a feature mismatch. To fix this, you can try to evaluate feature performance for the entire pipeline (rather than the last operator) or try an alternative that was mentioned in #738 - using permutation importance via eli5 (https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html)",believe question somehow feature ultimately training please correct wrong interpretation understanding well use special operator always obvious base output usually clearer python file append final classifier pipeline original input matrix allow nonlinear tree generation included elsewhere pipeline besides end likely pipeline end issue one causing error see output original input matrix synthetic feature incorporate final operator within pipeline rather entire best pipeline would last operator pipeline skipping would add feature original input matrix causing feature mismatch fix try evaluate feature performance entire pipeline rather last operator try alternative permutation importance via,issue,positive,positive,neutral,neutral,positive,positive
875920269,"@rachitk - actually i didnt export the pipeline before closing the problem since i dont work that much with sklearn pipeline. However, I think `exctracted_best_model = pipeline_optimizer.fitted_pipeline_.steps[-1][1]` is not the right way to get the best model and pickle it. 

As suggested by you and by googling more. Ill just do 
`exctracted_best_model = pipeline_optimizer.fitted_pipeline_`
`pickle.dump(exctracted_best_model, open('/xyz/model', 'wb'))`

and i think youre right in saying
""TPOT sometimes includes operators that can stack the output of other classifiers/operators as an additional feature on top of the original input (so while X_train may have 417 features, the eventual input into the final operator of your pipeline might have more features if additional features are stacked by earlier operators).""

I believe ^ is what the bestpipeline i mentioned in the question implies. Tpot is somehow using linearSVC and stacking its feature and ultimately training decisiontreeclassifier over it. Please correct me if i am wrong here.",actually didnt export pipeline problem since dont work much pipeline however think right way get best model pickle ill open think right saying sometimes stack output additional feature top original input may eventual input final operator pipeline might additional believe question somehow feature ultimately training please correct wrong,issue,positive,positive,positive,positive,positive,positive
875858670,"Hello @ud2195,

Your issue may be similar to a previously reported one (attempting something similar) at #738, if any of the information there helps you.

Is the full pipeline that is exported by TPOT when this error is thrown the same one you have at the end of your issue? 

The code you have seems to pull the final, fitted operator in your pipeline and tries to have it predict on the input `X_test` (which presumably has the same number of features as `X_train`, though I would check and make sure `X_train` and `X_test` are the same shape). 

TPOT sometimes includes operators that can stack the output of other classifiers/operators as an additional feature on top of the original input (so while `X_train` may have 417 features, the eventual input into the final operator of your pipeline might have more features if additional features are stacked by earlier operators). 


To recreate the pipeline in sklearn, you can export the pipeline by doing 
```
pipeline_optimizer.export(""exported_pipeline.py"")
```

You can use the code found in this file to build and refit the best pipeline for your data, which may help with the problem that you're having. 

As TPOT uses dynamic classes for its strongly-typed DEAP implementation, the TPOT object is not pickleable. As a workaround, you can pickle the fitted pipeline (`pipeline_optimizer.fitted_pipeline_` in your code). ",hello issue may similar previously one something similar information full pipeline error thrown one end issue code pull final fitted operator pipeline predict input presumably number though would check make sure shape sometimes stack output additional feature top original input may eventual input final operator pipeline might additional recreate pipeline export pipeline use code found file build refit best pipeline data may help problem dynamic class implementation object pickle fitted pipeline code,issue,positive,positive,positive,positive,positive,positive
872694132,I'm not sure what happened that caused me to open this issue. TPOT is fantastic. Thank you for such a great product! ,sure open issue fantastic thank great product,issue,positive,positive,positive,positive,positive,positive
872440817,"In addition to the comment above, there is currently active development on a version of TPOT that supports imblearn (imbalanced learn) and its related operators that may help with imbalanced data in the future. See #547 and #1137 for information.",addition comment currently active development version learn related may help data future see information,issue,positive,negative,neutral,neutral,negative,negative
872439785,"Hi @Ak784, the reason that TPOT does not consider these parameters is because they are not included as parameters for TPOT to optimize or pass in the TPOT configuration file under that operator. Any parameters not included are considered to be the default for that operator.

The configuration for XGBClassifier, as an example taken from the ```classifier.py``` dictionary is found below. As you can see, it does not include the parameter that you are looking for.

```
'xgboost.XGBClassifier': {
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21),
        'n_jobs': [1],
        'verbosity': [0]
    }
```

An alternative version with the `scale_pos_weight` parameter set to 9 can be found below.

```
'xgboost.XGBClassifier': {
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21),
        'n_jobs': [1],
        'verbosity': [0],
        'scale_pos_weight': [9]
    }
```


You should be able to create your own custom configurations for the classifiers/operators that you need to add additional parameters for, including the LogisticRegression() operator. Information on this can be found here: https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters",hi ak reason consider included optimize pas configuration file operator included considered default operator configuration example taken dictionary found see include parameter looking range range alternative version parameter set found range range able create custom need add additional operator information found,issue,positive,positive,positive,positive,positive,positive
872432360,"Hi @DallanQ - it seems you might have submitted this issue in error (the template is still the default template), but if you have an issue that you need addressed, just let us know!",hi might issue error template still default template issue need let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
872378150,"@CBrauer TPOT should naturally support most of sklearn's built-in transformers. You can include such transformers as operators in your own custom configuration dictionary to pass into TPOT: https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters

If you can use this transformer in various pipelines and see that it performs well, we might be able to include it in TPOT's premade configurations as a default option. ",naturally support include custom configuration dictionary pas use transformer various see well might able include default option,issue,positive,positive,positive,positive,positive,positive
872358824,"Hi @qo4on, according to the TPOT documentation (https://epistasislab.github.io/tpot/api/), it seems that the `max_time_mins` argument here should implement what you're looking for. 

For example, if you want a timeout of 10 minutes, you would do:
```
tpot.TPOTClassifier(max_time_mins=10)
```

You can set the other parameters as you wish.
",hi according documentation argument implement looking example want would set wish,issue,negative,neutral,neutral,neutral,neutral,neutral
872353897,"This doesn't seem to happen to me when installing TPOT into a new environment - can you replicate the issue if you install TPOT into a clean environment? 

It seems like this is an issue with Pytorch that occurs when you have two different Pytorch versions coexisting (which could happen due to one being installed through Conda and one being installed through pip).  https://github.com/pytorch/pytorch/issues/32270 - you could try uninstalling Pytorch and reinstalling it to see if that resolves the issue.",seem happen new environment replicate issue install clean environment like issue two different could happen due one one pip could try see issue,issue,positive,positive,neutral,neutral,positive,positive
858701584,Maybe do an GridSearchCV to text feature extraction together with the TPOT setting config_dict='TPOT sparse'. It is a viable solution?,maybe text feature extraction together setting sparse viable solution,issue,negative,neutral,neutral,neutral,neutral,neutral
851713110,"Having same issue. tried force-reinstall, installed dask per website. down grading the scikit-learn package just causes errors about deprecated module. bit stuck on this. Using Anaconda on local pc with Jupyter notebook.",issue tried per grading package module bit stuck anaconda local notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
848946648,"how can solve it. i m stuck here.
import sklearn
print(sklearn.__version__)
0.23.2

from sklearn.model_selection import train_test_split
ImportError                               Traceback (most recent call last)
<ipython-input-87-73edc048c06b> in <module>
----> 1 from sklearn.model_selection import train_test_split

~\anaconda3\lib\site-packages\sklearn\model_selection\__init__.py in <module>
     19 from ._split import check_cv
     20 
---> 21 from ._validation import cross_val_score
     22 from ._validation import cross_val_predict
     23 from ._validation import cross_validate

~\anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in <module>
     26 from ..utils.validation import _num_samples
     27 from ..utils.validation import _deprecate_positional_args
---> 28 from ..utils.fixes import delayed
     29 from ..utils.metaestimators import _safe_split
     30 from ..metrics import check_scoring

ImportError: cannot import name 'delayed' from 'sklearn.utils.fixes' (C:\Users\hp\anaconda3\lib\site-packages\sklearn\utils\fixes.py)
",solve stuck import print import recent call last module import module import import import import module import import import import metric import import name,issue,negative,neutral,neutral,neutral,neutral,neutral
846582389,"Never mind.  I have given up on my project that requires Python 3,5.  To clean up my machine, I spend a day re-installing Anaconda Python.  I hate python. It's a miasma.",never mind given project python clean machine spend day anaconda python hate python miasma,issue,negative,negative,negative,negative,negative,negative
843875732,Had a similar problem and setting njobs to 1 cleared it up not ideal but,similar problem setting ideal,issue,negative,positive,positive,positive,positive,positive
840330484,"FileNotFoundError: [Errno 2] No such file or directory: './tpot_output/tpot_iris_pipeline_0.py'

while testing default iris data. any solution?",file directory testing default iris data solution,issue,negative,neutral,neutral,neutral,neutral,neutral
832806324,"code obtained in https://github.com/EpistasisLab/tpot/issues/703#issuecomment-395632097 cannot be used directly.
Please use the code below to obtain a ready to use version of the code (like in tpot.export(...) ) for the entire pareto

```
from tpot.export_utils import (
    export_pipeline,
    expr_to_tree,
    generate_pipeline_code,
    set_param_recursive,
)

# tpot = TPOTRegressor(generations=25, ...)
# tpot.fit()


for pipeline, pipeline_scores in zip(tpot._pareto_front.items, reversed(tpot._pareto_front.keys)):
    idx = tpot._pareto_front.items.index(pipeline)
    pareto_front_pipeline_score = pipeline_scores.wvalues[1]
    sklearn_pipeline_str = generate_pipeline_code(
        expr_to_tree(pipeline, tpot._pset), tpot.operators
    )
    to_write = export_pipeline(
        pipeline,
        tpot.operators,
        tpot._pset,
        tpot._imputed,
        pareto_front_pipeline_score,
        tpot.random_state,
    )
    print(to_write, '\n----------------------------------------------------------------------------\n\n')
```

example of output:

```
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=53)

# Average CV score on the training set was: -0.21667382164465937
exported_pipeline = GradientBoostingRegressor(alpha=0.85, learning_rate=0.1, loss=""ls"", max_depth=6, max_features=0.9000000000000001, min_samples_leaf=1, min_samples_split=13, n_estimators=100, subsample=0.8)
# Fix random state in exported estimator
if hasattr(exported_pipeline, 'random_state'):
    setattr(exported_pipeline, 'random_state', 53)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
 
---------------------------------------------------------------------------- -0.21667382164465937 


import numpy as np
import pandas as pd
from sklearn.linear_model import LassoLarsCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline, make_union
from tpot.builtins import StackingEstimator
from xgboost import XGBRegressor
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=53)

# Average CV score on the training set was: -0.19100860580801965
exported_pipeline = make_pipeline(
    StackingEstimator(estimator=LassoLarsCV(normalize=False)),
    XGBRegressor(learning_rate=0.1, max_depth=3, min_child_weight=6, n_estimators=100, n_jobs=1, objective=""reg:squarederror"", subsample=1.0, verbosity=0)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 53)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
 
---------------------------------------------------------------------------- -0.19100860580801965 

and 2 more items below...
```

hope this help",code used directly please use code obtain ready use version code like entire import pipeline zip reversed pipeline pipeline pipeline print example output import import import import note make sure outcome column data file average score training set fix random state estimator import import import import import import import import note make sure outcome column data file average score training set reg fix random state pipeline hope help,issue,positive,neutral,neutral,neutral,neutral,neutral
829082199,"After some investigation, I feel this problem should not be a problem anymore. In fact, there are a lot of packages that can plot a machine learning pipeline.  Even the scikit-learn package itself has provided a tool function for plotting the pipeline.  For those users who want to plot a more beautiful pipeline,  there are also numerous open source tools to plot a pipeline.",investigation feel problem problem fact lot plot machine learning pipeline even package provided tool function plotting pipeline want plot beautiful pipeline also numerous open source plot pipeline,issue,negative,positive,positive,positive,positive,positive
828518665,I'd like an update on this as well. Some kind of data preprocessing pipeline on each fold would be great.,like update well kind data pipeline fold would great,issue,positive,positive,positive,positive,positive,positive
825211283,"Looks like that works:

```
        'sklearn.compose.TransformedTargetRegressor': {
            'regressor': {
                'sklearn.linear_model.LinearRegression': {
                    'normalize': [True, False],
                    'copy_X': [True] }
            },
            'func': {'numpy.log': None}, 'inverse_func': {'numpy.exp': None}
            },
```

Is there a way to include a list of regressors?",like work true false true none none way include list,issue,positive,positive,neutral,neutral,positive,positive
819483895,"This will be great to have. Would like to integrate with mlflow if possible so that we have more visibility into what has been done, etc. ",great would like integrate possible visibility done,issue,positive,positive,positive,positive,positive,positive
815122859,"@JDRomano2 sorry for late response, GitHub muted notification somehow. The os I was running is an aws SageMaker instance of  ml.m5.24xlarge which has 96 vCPUs and 384G memory. The file has 2.5M rows with 400M disk size. I didn't see there's any issue with the data but seems tpot will fail when the number of rows exceeds certain amount:

row 1-100,000: success
row 100,000-200,000: success
row 1-200,000: fail

I tested a TPOTClassifier with similar data size without issues. Somehow the self._optimized_pipeline is None for TPOTRegressor when running on larger dataset.",sorry late response notification somehow o running instance memory file disk size see issue data fail number certain amount row success row success row fail tested similar data size without somehow none running,issue,negative,negative,negative,negative,negative,negative
808698694,Still looking forward for this feature to be implemented.,still looking forward feature,issue,negative,neutral,neutral,neutral,neutral,neutral
807578038,"I use a custom configuration, which includes:

```
        'sklearn.feature_selection.SelectKBest': {
            'k': range(3, 20),
            'score_func': {
                'sklearn.feature_selection.mutual_info_regression': None
            }
```
I would like to include additional score_func in the optimization process. Not sure if I just don't know the correct syntax to specify a list of score_func or if it's not supported.

Btw. the same question is related to the Gaussian Regressor, where I would like to include multiple Kernels in the selection process:

```
    'sklearn.gaussian_process.GaussianProcessRegressor': {
        'kernel': {
            'sklearn.gaussian_process.kernels.DotProduct': { 
            }
        }
    }
```",use custom configuration range none would like include additional optimization process sure know correct syntax specify list question related regressor would like include multiple selection process,issue,negative,positive,positive,positive,positive,positive
807566967,"Just to clarify, does this work when you provide a custom configuration dictionary containing the sample configuration you gave in your initial question? ",clarify work provide custom configuration dictionary sample configuration gave initial question,issue,negative,neutral,neutral,neutral,neutral,neutral
807561531,"My apologies, I misunderstood your question. I mistakenly thought you were referring to the overall scoring function used to evaluate a pipeline rather than the scoring function used within the feature selector operator.",misunderstood question mistakenly thought overall scoring function used evaluate pipeline rather scoring function used within feature selector operator,issue,negative,neutral,neutral,neutral,neutral,neutral
807551398,Can you reopen the question? I think it would be an important feature if it is not possible yet.,reopen question think would important feature possible yet,issue,negative,positive,positive,positive,positive,positive
807543795,"I don't understand why that should not work. TPOT is optimizing the pipeline globally based on the defined scoring function and cross validation settings etc.

So you can use the SelectKBest feature selection method using different functions, features are ranked and selected based on the k parameter. Finally tpot could select the  SelectKBest parameters which maximise the tpot test score.",understand work pipeline globally based defined scoring function cross validation use feature selection method different ranked selected based parameter finally could select test score,issue,negative,neutral,neutral,neutral,neutral,neutral
807487068,"Hi @hanshupe, this is likely not possible because different scoring functions have fundamentally different meanings, and are used to tell different things about the performance of a model. E.g., F1-score is a composite score derived from both the precision and the recall, so how would you determine if an F1-score is ""better"" than a recall score? Also, how can you compare Cohen's kappa (which ranges between -1 and +1) with a different score that ranges from 0 to 1?",hi likely possible different scoring fundamentally different used tell different performance model composite score derived precision recall would determine better recall score also compare kappa different score,issue,negative,positive,neutral,neutral,positive,positive
801589889,"@ankitrajixr as another note, please also make sure to include detail describing the exact nature of the changes this pull request (when completed) will introduce. For reference, you may look at the following blog post: https://github.blog/2015-01-21-how-to-write-the-perfect-pull-request/",another note please also make sure include detail exact nature pull request introduce reference may look following post,issue,positive,positive,positive,positive,positive,positive
801440669,"Hi @ankitrajixr , please group all commits for a single pull request together. I'm closing this PR so that this can be added to #1191 instead.",hi please group single pull request together added instead,issue,negative,negative,neutral,neutral,negative,negative
801433930,"Hi @ankitrajixr , thanks for submitting this pull request. There are a couple of issues here:

1. The biggest one is that the tests fail. AppVeyor reports 10 errors, which look like syntax errors. Please also note that you can't put new import statements in the config dictionary files. Every tunable metaparameter must be a predefined property of the operator.
2. Once tests pass, we will need to be sure that code coverage metrics do not drop. It may be necessary to write new tests to make sure we have adequate code coverage with the test suite.
3. Before we can merge these changes we need to have some indication of their performance. This could be as simple as showing training times with and without the new operators included, and an indication of how often the new operators find their way into the final pipelines.",hi thanks pull request couple biggest one fail look like syntax please also note ca put new import dictionary every tunable must property operator pas need sure code coverage metric drop may necessary write new make sure adequate code coverage test suite merge need indication performance could simple showing training time without new included indication often new find way final,issue,positive,positive,positive,positive,positive,positive
801378125,"Thank you for your response @JDRomano2 . I have tried the custom TPOT config dictionary. Below is the code snippet for it. 

```
tpot_config = {
    'kernel' : [1.0*RBF(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0)),
           1.0*RationalQuadratic(length_scale=0.5, alpha=0.1),
           1.0*ExpSineSquared(length_scale=0.5, periodicity=3.0,
                                length_scale_bounds=(1e-05, 100000.0),
                                periodicity_bounds=(1.0, 10.0)),
           ConstantKernel(0.1, (0.01, 10.0))*(DotProduct(sigma_0=1.0, sigma_0_bounds=(0.1, 10.0)) ** 2),
           1.0**2*Matern(length_scale=0.5, length_scale_bounds=(1e-05, 100000.0),
                        nu=0.5)],
        'alpha': [5e-9,1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'normalize_y' : [True, False],
        'optimizer' : ['fmin_l_bfgs_b']
}
```

The above code works fine for smaller datasets.",thank response tried custom dictionary code snippet true false code work fine smaller,issue,positive,positive,neutral,neutral,positive,positive
799433583,"it is not an issue, per se. it is more of a feature request/question about how we can resume TPOT training even after shutting down our computer. apparently, it's not possible since the progress would be stored in the memory which would lose all its information as soon as the computer is switched off. 

My question was, could we somehow resume TPOT training even after shutdown?",issue per se feature resume training even shutting computer apparently possible since progress would memory would lose information soon computer switched question could somehow resume training even shutdown,issue,negative,neutral,neutral,neutral,neutral,neutral
799022794,"Hi @ankitrajixr, these may have previously been found to not play well with other parts of TPOT, and that may be why they are not included by default.

However, you can add any scikit-learn classifier or regressor to TPOT by simply including it in a custom configuration dictionary. Please see:
https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters

If you can use them and they perform well, we can look into adding them to the built-in configuration dictionaries. I'd recommend giving it a try and letting us know (on this thread) how they perform.",hi may previously found play well may included default however add classifier regressor simply custom configuration dictionary please see use perform well look configuration recommend giving try u know thread perform,issue,positive,negative,neutral,neutral,negative,negative
799020076,"Hi @neel04 , can you provide a minimal set of code we can use to reproduce the issue?",hi provide minimal set code use reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
799019655,"Hi @RazHoshia, how did you install TPOT? When I install TPOT on a fresh conda environment (with python 3.7) using `pip install tpot`, the dependencies resolve scikit-learn to 0.24.1:

```
❯ pip show scikit-learn
Name: scikit-learn
Version: 0.24.1
Summary: A set of python modules for machine learning and data mining
Home-page: http://scikit-learn.org
Author: None
Author-email: None
License: new BSD
Location: /Users/jdr2160/anaconda3/envs/tpot/lib/python3.7/site-packages
Requires: scipy, numpy, threadpoolctl, joblib
Required-by: TPOT
```

I agree that we probably need to bump scikit-learn to a newer version in `requirements.txt`, but it's a good idea to first know how to reproduce this issue.

Please include the following info:

- Operating system and version
- Python distribution and version
- Command you used to install TPOT",hi install install fresh environment python pip install resolve pip show name version summary set python machine learning data mining author none none license new location agree probably need bump version good idea first know reproduce issue please include following operating system version python distribution version command used install,issue,positive,positive,positive,positive,positive,positive
792424473,"the hyper-parameter's combinations  of the model which in regressor_config_dict are small,for example     'xgboost.XGBRegressor': {
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21),
        'n_jobs': [1],
        'verbosity': [0],
        'objective': ['reg:squarederror']
    },
if I reset the hyper-parameter of the XGBRegressor as above  .Can I improve performance of the XGBRegressor ?",model small example range range reset improve performance,issue,negative,negative,negative,negative,negative,negative
791666929,"@beckernick thank you for the update! Very glad to have support from the cuML/cuDF team!

@rayshmahilal55 I'm going to close this issue but encourage you to check the link provided above for updates that should fix this. If that does not end up resolving the issue please reopen this.",thank update glad support team going close issue encourage check link provided fix end issue please reopen,issue,positive,positive,positive,positive,positive,positive
791666027,"Yes it makes sense. But after using fit(X,y), it returns the best pipeline but how to get the best features from the result. I tried using tpot.get_support() but it throws error. Is there any chances to know the best features selected for the model. ",yes sense fit best pipeline get best result tried error know best selected model,issue,positive,positive,positive,positive,positive,positive
791659593,I got this working by reshaping the index. Thanks.,got working index thanks,issue,negative,positive,positive,positive,positive,positive
791659146,TPOT is really amazing. Thank you for your response.,really amazing thank response,issue,positive,positive,positive,positive,positive,positive
791548140,"Hi @zhh210 , would you mind posting your OS (and OS version) as well as the amount of RAM your computer has?

I'm also wondering whether the transformations you are applying in the line with `train_test_split` could be doing something unexpected to your dataset. It would be helpful if you could save `X_train`, `X_test`, `y_train`, and `y_test` to a file and share them.",hi would mind posting o o version well amount ram computer also wondering whether line could something unexpected would helpful could save file share,issue,positive,positive,neutral,neutral,positive,positive
791537829,"The subset(s) should be designed manually based on your understanding of the dataset and the task you are trying to accomplish. Briefly, each feature set should roughly correspond to a group of features that are somehow related. The example given in the TPOT documentation is a good one:

> For example, in RNA-seq gene expression analysis, this operator can be used to select one or more gene (feature) set(s) based on GO (Gene Ontology) terms

Here, the sets are groups of genes that have related functions. You, as the user, need to use expert knowledge to decide what those groups of genes are. 

Does this make sense?",subset designed manually based understanding task trying accomplish briefly feature set roughly correspond group somehow related example given documentation good one example gene expression analysis operator used select one gene feature set based go gene ontology related user need use expert knowledge decide make sense,issue,positive,positive,positive,positive,positive,positive
791530332,"Hi @huangqingyi-code , the reason it's slow is because in the second block of code you are trying to build a machine learning pipeline using ONLY the `XGBRegressor` estimator. In the first (fast) block of code you have many other estimators available to TPOT, some of which train much more quickly than the gradient boosting regressor (like feature scalers and transformers, simpler estimators, etc.). This may also do feature selection, which means subsequent gradient boosting estimators will have fewer features and train more quickly.",hi reason slow second block code trying build machine learning pipeline estimator first fast block code many available train much quickly gradient regressor like feature simpler may also feature selection subsequent gradient train quickly,issue,negative,positive,positive,positive,positive,positive
789735651,"We are working on an updated RAPIDS installer now that Colab has upgraded to Python 3.7. When that is available, TPOT cuML should ""just work"" on Colab if you have both packages available. Please follow https://github.com/rapidsai/cudf/issues/7472#issuecomment-789324827 for updates.",working installer python available work available please follow,issue,negative,positive,positive,positive,positive,positive
786131311,Please note that failed tests are not due to changes made in this PR - they fail elsewhere in the codebase. This work will be merged into `master` when the failed tests are resolved.,please note due made fail elsewhere work master resolved,issue,negative,negative,negative,negative,negative,negative
784619204,"This seems to be resolved by #1161, which has been merged into `development` but hasn't yet made its way into `master`. The solution was to use `assert_array_almost_equal` from Numpy instead of from scikit-learn",resolved development yet made way master solution use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
784606795,"This may be because scikit-learn 0.24 is installed, and the most recent update made some breaking changes to the API that your current install of dask-ml will need to address (see #1176).

To fix this, you could update dask-ml (which seems to fix this issue in later versions), or you could add the line (not recommended)

`!pip install 'scikit-learn>=0.22.0,<0.24.0' --force-reinstall` 

after your other pip installs to force an install of an older version of scikit-learn that doesn't have these changes.




EDIT: A previous version of this comment mistakenly attributed the error to TPOT - upon closer reading of this, this is actually an issue with a dask-ml function - you should check your dask-ml version to see if there is an updated one.
",may recent update made breaking current install need address see fix could update fix issue later could add line pip install pip force install older version edit previous version comment mistakenly error upon closer reading actually issue function check version see one,issue,negative,neutral,neutral,neutral,neutral,neutral
782715861,"@carterrees yes, you can do this by starting a Dask CUDA cluster and setting `use_dask=True`. This brief recording shows an example. https://www.youtube.com/watch?v=7z4OJQdY_mw


```python
from dask.distributed import Client
from dask_cuda import LocalCUDACluster
cluster = LocalCUDACluster() # use every GPU on the machine by default
client = Client(cluster)
...
# TPOT as normal, passing use_dask=True and config_dict=""TPOT cuML""
...
```",yes starting cluster setting brief recording example python import client import cluster use every machine default client client cluster normal passing,issue,negative,positive,neutral,neutral,positive,positive
780954485,Follow up question. Is it possible to use multiple gpu's while training tpot? ,follow question possible use multiple training,issue,negative,neutral,neutral,neutral,neutral,neutral
778212690,"Just ran into this, one possible workaround is to just remove it from the search space.

```python
from tpot.config.classifier import classifier_config_dict

new_config = classifier_config_dict.copy()
del new_config['sklearn.svm.LinearSVC']

clf = tpot.TPOTClassifier(config_dict=new_config)
```",ran one possible remove search space python import,issue,negative,neutral,neutral,neutral,neutral,neutral
777937651,"Hey, just wanted to say that I ran into the same problem, and updating to 0.11.7 worked for me. Maybe it was an undetected bug? I can post my specs if needed, I was training on a multi-class classification problem too",hey say ran problem worked maybe undetected bug post spec training classification problem,issue,negative,neutral,neutral,neutral,neutral,neutral
776323220,@ifateeva can you provide some example code that we can use to reproduce what you are seeing?,provide example code use reproduce seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
774058892,"Yes, please use pip install instead.",yes please use pip install instead,issue,positive,neutral,neutral,neutral,neutral,neutral
774027284,"Ahh, right. I was wondering whether it would use a standalone piece of code that would run the model. Still, thanx a ton for the info! ",right wondering whether would use piece code would run model still ton,issue,negative,positive,positive,positive,positive,positive
773645709,"If you look [here in the README](https://github.com/EpistasisLab/tpot#regression) you can see that the exported model _does_ require TPOT because it provides some convenience functions.

This might not always be the case depending on the final model that is exported, though.",look see model require convenience might always case depending final model though,issue,negative,neutral,neutral,neutral,neutral,neutral
771694580,Closing issue - if `pip install scikit-learn` did not resolve the problem please add a follow-up comment.,issue pip install resolve problem please add comment,issue,negative,neutral,neutral,neutral,neutral,neutral
771288645,"Great, thanks, I've now added LightGBM and CatBoost into TPOT. I have to say that for my multiclass classification problem LightGBM ended up giving me a F1_macro score that was significantly better than XGBoost",great thanks added say classification problem ended giving score significantly better,issue,positive,positive,positive,positive,positive,positive
766088428,"TPOT will use a custom score function if you do the following:

1). Define the function in another local module. (e.g., save it in `./score_f.py`)

2.) Give the function the signature `scorer(estimator, X, y)`

3.) Pass the import statement for the argument to the `scorer` argument of `TPOTClassifier()`. (e.g., `scorer='score_f.my_scorer`, if the function is named `my_scorer` and saved in `score_f.py`)

TPOT considers pipelines with higher scores to be better, so if your metric corresponds to ""smaller is better"" your scoring function for that metric should be negated.

Regarding your second question, it does not have to be in a specific range, but should be a floating point number.",use custom score function following define function another local module save give function signature scorer estimator pas import statement argument scorer argument function saved higher better metric smaller better scoring function metric regarding second question specific range floating point number,issue,positive,positive,positive,positive,positive,positive
765900432,Thank you. If I define a custom score function will tpot use that to optimize over pipelines?  Do score functions just have to output a number or does it have to be in some range?,thank define custom score function use optimize score output number range,issue,positive,neutral,neutral,neutral,neutral,neutral
765718706,"I think you might be confusing _loss functions_ with _scoring functions_: Loss functions are an internal attribute of some classification or regression operators. Scoring functions are applied to TPOT pipelines to assess the performance of that entire pipeline.

TPOT does not interact with loss functions (edit: unless you include them as a tunable metaparameter), so there is no way to customize loss functions in the context of TPOT. You can, in theory, define a custom classifier that uses a custom loss function, and then add that custom operator to TPOT via a custom configuration dictionary. But also keep in mind that many classification algorithms don't even use loss functions in their learning process. 

If, however, you want to use a custom _scoring_ function (instead of a loss function), details for how to do this are given in the documentation: https://epistasislab.github.io/tpot/using/#scoring-functions. You can use one of the predefined scoring metrics from scikit-learn (listed at https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values), or you can define your own Python function that accepts a list of y values and predicted y values and returns a score.",think might loss internal attribute classification regression scoring applied ass performance entire pipeline interact loss edit unless include tunable way loss context theory define custom classifier custom loss function add custom operator via custom configuration dictionary also keep mind many classification even use loss learning process however want use custom function instead loss function given documentation use one scoring metric listed define python function list score,issue,negative,positive,positive,positive,positive,positive
764855784,"![TPOT doc](https://user-images.githubusercontent.com/42462113/105396248-ab37ed00-5c45-11eb-970b-3b364828cd81.JPG)

This issue is not occurring every time. Just now I read the highlighted statement. I am using a very large dataset. So, I think, I faced the issue. Hence, closing this ",doc issue every time read statement large think faced issue hence,issue,negative,positive,positive,positive,positive,positive
764679440,Please provide a example to reproduce this issue.,please provide example reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
764639658,Try `pip install scikit-learn` instead ,try pip install instead,issue,negative,neutral,neutral,neutral,neutral,neutral
764479457,"hi guys,
I have issue on:
pip install sklearn.cross_validation

it has no this module

python: 3.8.6
windows 10

read your comments and not useful, can you help to fix it ?",hi issue pip install module python read useful help fix,issue,positive,positive,positive,positive,positive,positive
761739755,"Thank you for taking your time and giving me the solutions to my problem. I am new to tpot, can you please answer these questions.

1)  If  my custom estimator passes sklearns `check_estimator` checks  will it work on tpot ?
2) Do I need to add a score method in my custom estimator?
3)  Can we use tpot for an estimator (say logistic regression) which wass made from a different ML module  (not sklearn)? ",thank taking time giving problem new please answer custom estimator work need add score method custom estimator use estimator say logistic regression made different module,issue,positive,positive,neutral,neutral,positive,positive
761582721,"Hi @levedev, there are a couple of initial issues I notice:


First, your model is missing a number of methods/attributes that are needed to be compliant with the Scikit-Learn API. A good place to start is here:

https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator

E.g., it is required to define both `get_params` and `set_params`.

Briefly, you should be able to do the following without the second line returning errors:

```{python}
>>> from sklearn.utils.estimator_checks import check_estimator
>>> check_estimator(MeanClassifier())
```

(Also, having `y_train` and `y_test` equal `None` will return errors.)


Second, TPOT needs to be able to import the estimator from a file in order for it to pass the preprocessing checks. Currently, TPOT doesn't support modules defined locally. 
https://github.com/EpistasisLab/tpot/blob/6448bdb71ba08b4a0447c640d2f05a05e1affc21/tpot/operator_utils.py#L77 
^ This line in particular raises an exception and causes TPOT to skip your custom module.",hi couple initial notice first model missing number compliant good place start define briefly able following without second line python import also equal none return second need able import estimator file order pas currently support defined locally line particular exception skip custom module,issue,positive,positive,positive,positive,positive,positive
757923624,"Hmm, I am not sure about the issues. Please try tpot 0.11.7. If it doesn’t work, please provide a demo for reproducing this issue.",sure please try work please provide issue,issue,positive,positive,positive,positive,positive,positive
757921356,"@weixuanfu 
I tried to remove sklearn 0.24 and install 0.23.2
Still getting an error exactly at iteration 200.

```
Version 0.11.6.post3 of tpot is outdated. Version 0.11.7 was released 4 days ago.
Optimization Progress: 2%
200/10100 [27:26<24:05:12, 8.76s/pipeline]
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
/usr/local/lib/python3.6/site-packages/sklearn/model_selection/_split.py:668: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.
  % (min_groups, self.n_splits)), UserWarning)
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    741                     per_generation_function=self._check_periodic_pipeline,
--> 742                     log_file=self.log_file_
    743                 )

6 frames
/usr/local/lib/python3.6/site-packages/tpot/gp_deap.py in eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)
    280         if per_generation_function is not None:
--> 281             per_generation_function(gen)
    282 

/usr/local/lib/python3.6/site-packages/tpot/base.py in _check_periodic_pipeline(self, gen)
   1051         """"""
-> 1052         self._update_top_pipeline()
   1053         if self.periodic_checkpoint_folder is not None:

/usr/local/lib/python3.6/site-packages/tpot/base.py in _update_top_pipeline(self)
    837                         break
--> 838                 raise RuntimeError('There was an error in the TPOT optimization '
    839                                    'process. This could be because the data was '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-26-64b955e8a61a> in <module>()
----> 1 tpt.fit(X,y)

/usr/local/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    771                     # raise the exception if it's our last attempt
    772                     if attempt == (attempts - 1):
--> 773                         raise e
    774             return self
    775 

/usr/local/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    762                         self._pbar.close()
    763 
--> 764                     self._update_top_pipeline()
    765                     self._summary_of_best_pipeline(features, target)
    766                     # Delete the temporary cache before exiting

/usr/local/lib/python3.6/site-packages/tpot/base.py in _update_top_pipeline(self)
    836                                                     error_score=""raise"")
    837                         break
--> 838                 raise RuntimeError('There was an error in the TPOT optimization '
    839                                    'process. This could be because the data was '
    840                                    'not formatted properly, or because data for '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/
```",tried remove install still getting error exactly iteration version post outdated version day ago optimization progress least class le least class le least class le least class le least class le least class le least class le least class le least class le least class le recent call last fit self target population toolbox mu verbose none gen self gen none self break raise error optimization could data error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation handling exception another exception recent call last module fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self raise break raise error optimization could data properly data error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation,issue,positive,negative,neutral,neutral,negative,negative
757902236,"TPOTClassifier uses stratified kfold when cv=3,5 or 10.

I think this issue maybe related to #1148. If so, you need update TPOT or downgrade sklearn to 0.23.2.",stratified think issue maybe related need update downgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
757491330,"I can't replicate the issue in MacOS 11, so I'm suspicious that it's on OS or kernel-related memory leak. I'll see if I encounter the same bug in Ubuntu 20.04, like you are using, and report back.",ca replicate issue suspicious o memory leak see encounter bug like report back,issue,negative,neutral,neutral,neutral,neutral,neutral
757443427,"weixuanfu, thank you for the suggestion. 
When I am trying to use BlazingSQL, cuml part works flawlessly.
But when I am trying to install tpot ( with ! pip install tpot) it throws an error about  absence of compatible sklearn ( trace from command is below)

`Collecting tpot
  Downloading https://files.pythonhosted.org/packages/2d/f6/70b0e1bb432e9ea7686190232337848691de6db37c7e67c81bd7650af718/TPOT-0.11.7.tar.gz (908kB)
    100% |████████████████████████████████| 911kB 529kB/s eta 0:00:01
Collecting deap>=1.2 (from tpot)
  Using cached https://files.pythonhosted.org/packages/a0/b5/2adca2bac5727549e8ce36d9851a129c51629ae62323bc31b940ffdb977a/deap-1.3.1-cp27-cp27mu-manylinux1_x86_64.whl
Collecting joblib>=0.13.2 (from tpot)
  Using cached https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl
Collecting numpy>=1.16.3 (from tpot)
  Using cached https://files.pythonhosted.org/packages/3a/5f/47e578b3ae79e2624e205445ab77a1848acdaa2929a00eeef6b16eaaeb20/numpy-1.16.6-cp27-cp27mu-manylinux1_x86_64.whl
Collecting pandas>=0.24.2 (from tpot)
  Using cached https://files.pythonhosted.org/packages/db/83/7d4008ffc2988066ff37f6a0bb6d7b60822367dcb36ba5e39aa7801fda54/pandas-0.24.2-cp27-cp27mu-manylinux1_x86_64.whl
Collecting scikit-learn>=0.22.0 (from tpot)
  Could not find a version that satisfies the requirement scikit-learn>=0.22.0 (from tpot) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18rc2, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2)
No matching distribution found for scikit-learn>=0.22.0 (from tpot)`",thank suggestion trying use part work flawlessly trying install pip install error absence compatible trace command eta could find version requirement matching distribution found,issue,negative,positive,positive,positive,positive,positive
757086842,"Thanks @pedrogarciafreitas , I'm going to try to replicate the issue.

Would you mind providing your OS and OS version please?",thanks going try replicate issue would mind providing o o version please,issue,positive,positive,positive,positive,positive,positive
757079728,"Hi, 

Using the following dataset:
[reference_APSIPA.zip](https://github.com/EpistasisLab/tpot/files/5790031/reference_APSIPA.zip)

The following script:

```python
from tpot import TPOTRegressor
import pandas as pd
import numpy as np

df = pd.read_csv('reference_APSIPA.zip')
features = [col for col in df if col.startswith('d_')]
X_train, y_train = df[features].values, df.SCORE.values
tpot = TPOTRegressor(generations=500, population_size=50,
                     verbosity=2, n_jobs=16, use_dask=True)
tpot.fit(X_train, y_train)
```
Crashes after 2350 iterations:

```
$ python tpot_from_dataset.py 
                                                                                                                                                                                   
Generation 1 - Current best internal CV score: -0.4186788503982938
                                                                                                                                                                                   
Generation 2 - Current best internal CV score: -0.4186788503982938
                                                                                                                                                                                   
Generation 3 - Current best internal CV score: -0.4186788503982938
                                                                                                                                                                                   
Generation 4 - Current best internal CV score: -0.4186788503982938
                                                                                                                                                                                   
Generation 5 - Current best internal CV score: -0.4186788503982938
                                                                                                                                                                                   
Generation 6 - Current best internal CV score: -0.40041785046702455
                                                                                                                                                                                   
Generation 7 - Current best internal CV score: -0.36806708236239455
                                                                                                                                                                                   
Generation 8 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 9 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 10 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 11 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 12 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 13 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 14 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 15 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 16 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 17 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 18 - Current best internal CV score: -0.360919010590871
                                                                                                                                                                                   
Generation 19 - Current best internal CV score: -0.35142348326471307
                                                                                                                                                                                   
Generation 20 - Current best internal CV score: -0.34095032052401014
                                                                                                                                                                                   
Generation 21 - Current best internal CV score: -0.34095032052401014
                                                                                                                                                                                   
Generation 22 - Current best internal CV score: -0.34095032052401014
                                                                                                                                                                                   
Generation 23 - Current best internal CV score: -0.34095032052401014
                                                                                                                                                                                   
Generation 24 - Current best internal CV score: -0.34004340266687294
                                                                                                                                                                                   
Generation 25 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 26 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 27 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 28 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 29 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 30 - Current best internal CV score: -0.3299944198542487
                                                                                                                                                                                   
Generation 31 - Current best internal CV score: -0.32862536127972203
                                                                                                                                                                                   
Generation 32 - Current best internal CV score: -0.32862536127972203
                                                                                                                                                                                   
Generation 33 - Current best internal CV score: -0.32862536127972203
                                                                                                                                                                                   
Generation 34 - Current best internal CV score: -0.32862536127972203
                                                                                                                                                                                   
Generation 35 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 36 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 37 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 38 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 39 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 40 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 41 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 42 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 43 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 44 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 45 - Current best internal CV score: -0.3242168683350396
                                                                                                                                                                                   
Generation 46 - Current best internal CV score: -0.3242168683350396
Optimization Progress:   9%|█████████▋                                                                                             | 2350/25050 [5:28:59<27:53:07,  4.42s/pipeline]
Killed
```


",hi following following script python import import import col col python generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score optimization progress,issue,positive,positive,positive,positive,positive,positive
756944278,"Hmm, it is strange that such a small dataset caused memory leak and it is also a unusual that autosklearn needed 18Gb. Could you please provide more information to reproduce this issue? ",strange small memory leak also unusual could please provide information reproduce issue,issue,negative,negative,neutral,neutral,negative,negative
756137637,Thank you for sharing this idea. TPOT does not support HistGradientBoostingClassifier so far but may easily include it using `config_dict` (see [this link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters)) on a customized configuration based on [Built-in TPOT configurations](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations).,thank idea support far may easily include see link configuration based,issue,positive,positive,positive,positive,positive,positive
756133145,Thank you for this PR. I will merge it to dev branch first. ,thank merge dev branch first,issue,negative,positive,positive,positive,positive,positive
755775589,"
[![Coverage Status](https://coveralls.io/builds/36140103/badge)](https://coveralls.io/builds/36140103)

Coverage remained the same at 95.677% when pulling **f076653f23c53d6c13618b99d4a87500dc4951d2 on bollwyvl:patch-1** into **6448bdb71ba08b4a0447c640d2f05a05e1affc21 on EpistasisLab:master**.
",coverage status coverage master,issue,negative,neutral,neutral,neutral,neutral,neutral
755364450,This issue should be fixed in recent release (v0.11.7) of TPOT. Please update TPOT and feel free to reopen this issue if it still exists. ,issue fixed recent release please update feel free reopen issue still,issue,positive,positive,positive,positive,positive,positive
755364281,"Thank both of you for debuting this issue.

This issue should be fixed in recent release (v0.11.7) of TPOT. Please update TPOT and feel free to reopen this issue if it still exists. ",thank issue issue fixed recent release please update feel free reopen issue still,issue,positive,positive,positive,positive,positive,positive
755359160,"
[![Coverage Status](https://coveralls.io/builds/36128416/badge)](https://coveralls.io/builds/36128416)

Coverage decreased (-0.7%) to 95.677% when pulling **5020c7bebe4dca640d032867678b499021eceedd on development** into **1e6094279edf7b81034fb7e3d3569953aee59775 on master**.
",coverage status coverage development master,issue,negative,neutral,neutral,neutral,neutral,neutral
755350669,"
[![Coverage Status](https://coveralls.io/builds/36127898/badge)](https://coveralls.io/builds/36127898)

Coverage decreased (-0.2%) to 95.677% when pulling **cad11ab719d8719be1c2f1d071c028405d40e765 on weixuanfu:sklearn024** into **247a1588c939129058bcc25b9cd9c27b765e2630 on EpistasisLab:development**.
",coverage status coverage development,issue,negative,neutral,neutral,neutral,neutral,neutral
754949267,"Joe,

 

Good to hear from you, and thanks for the explanation.

 

I will read the contribution guide, but I am comfortable I can develop code for the situations you outlined. I will be in touch.

 

Assuming I pass this test (☺), I would be interested in supporting your pytorch new development. 

 

Thanks for your good work with TPOT: I think it is breaking new ground in the autoML space.

 

David L. Wilt

3272 Bayou Road

Longboat Key FL 34228

dwilt1947@gmail.com

540-420-0844

 

 

From: Joe Romano <notifications@github.com>
Reply-To: EpistasisLab/tpot <reply@reply.github.com>
Date: Tuesday, January 5, 2021 at 5:45 PM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: DLWCMD <dwilt1947@gmail.com>, Mention <mention@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] Built-in Pytorch Classifiers Crash TPOT (#1149)

 

We are actively working on adding multi-class support (as well as support for regression) with PyTorch estimators, but in the meantime it is important we fix TPOT to fail gracefully.

@DLWCMD - Please read the contributing guide (https://epistasislab.github.io/tpot/contributing/) if you would like to add this safety check until we implement support for multi-class targets. We can also patch this check in ourselves; we should be able to do so within the next few days.

Basically, TPOT should raise an exception when TPOTClassifier.fit() is called if both of the following are true:
The TPOTClassifier object was initialized with PyTorch estimators enabled.
The classes argument is multi-class (i.e., non-binary).
—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.

",joe good hear thanks explanation read contribution guide comfortable develop code outlined touch assuming pas test would interested supporting new development thanks good work think breaking new ground space wilt bayou road longboat key joe reply date mention mention subject crash actively working support well support regression important fix fail gracefully please read guide would like add safety check implement support also patch check able within next day basically raise exception following true object class argument reply directly view,issue,positive,positive,positive,positive,positive,positive
754947715,"We are actively working on adding multi-class support (as well as support for regression) with PyTorch estimators, but in the meantime it is important we fix TPOT to fail gracefully.

@DLWCMD - Please read the contributing guide (https://epistasislab.github.io/tpot/contributing/) if you would like to add this safety check until we implement support for multi-class targets. We can also patch this check in ourselves; we should be able to do so within the next few days.

Basically, TPOT should raise an exception when `TPOTClassifier.fit()` is called if both of the following are true:

1. The `TPOTClassifier` object was initialized with PyTorch estimators enabled.
2. The `classes` argument is multi-class (i.e., non-binary).",actively working support well support regression important fix fail gracefully please read guide would like add safety check implement support also patch check able within next day basically raise exception following true object class argument,issue,positive,positive,neutral,neutral,positive,positive
754887922,"Good to hear from you.

 
Scikit-learn 0.23.2
TPOT 0.11.6.post3 
 

I did not note any incompatibilities but assumed the cause was within the pytorch classifiers code, associated with the current limitation to binary use cases. Should I upgrade Scikit-learn to 0.24 and try again?

 

Thanks.

 

P.S. I am very impressed with TPOT and its ease of use and utility. I hope to see more models and deep learning options added over time, but, despite these limitations, TPOT is already my preferred autoML tool.

 

P.P.S Would you prefer I respond within GitHub?

 

 

David L. Wilt

3272 Bayou Road

Longboat Key FL 34228

dwilt1947@gmail.com

540-420-0844

 

 

From: Weixuan Fu <notifications@github.com>
Reply-To: EpistasisLab/tpot <reply@reply.github.com>
Date: Tuesday, January 5, 2021 at 3:31 PM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: DLWCMD <dwilt1947@gmail.com>, Author <author@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] Built-in Pytorch Classifiers Crash TPOT (#1149)

 

Which version of scikit-learn and tpot is in your environment? I doubt that the latest version of scikit-learn (0.24) has an incompatibility issue. See #1157

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.

",good hear post note assumed cause within code associated current limitation binary use upgrade try thanks ease use utility hope see deep learning added time despite already preferred tool would prefer respond within wilt bayou road longboat key fu reply date author author subject crash version environment doubt latest version incompatibility issue see thread reply directly view,issue,negative,positive,positive,positive,positive,positive
754881349,Which version of scikit-learn and tpot is in your environment? I doubt that the latest version of scikit-learn (0.24) has an incompatibility issue. See #1157,version environment doubt latest version incompatibility issue see,issue,negative,positive,positive,positive,positive,positive
754880172,"Thank you for this suggestion. 

Since TPOT already supports xgboost, we don't have plan to add similar algorithms,
like lightGBM or CatBoos, into TPOT's default configuration.  Please check #808 for a workaround. ",thank suggestion since already plan add similar like default configuration please check,issue,positive,neutral,neutral,neutral,neutral,neutral
754875784,So far google colab is not a good environment to run cuML version > 0.15. Please try to use [BlazingSQL](https://app.blazingsql.com),far good environment run version please try use,issue,positive,positive,positive,positive,positive,positive
754695905,"Hi, yes i just referenced #1148 and noticed the similar issue.  Downgraded to 0.23.2 and now all is good :)  thanks for the help!",hi yes similar issue good thanks help,issue,positive,positive,positive,positive,positive,positive
754693133,"Which version of scikit-learn and tpot is in your environment? I doubt that the latest version of scikit-learn (0.24) has a incompatible issue. If you are using 0.24, please downgrade to 0.23.2 for another test.",version environment doubt latest version incompatible issue please downgrade another test,issue,negative,positive,positive,positive,positive,positive
754690063,"Hi,

I also just tried the [example ](https://github.com/EpistasisLab/tpot/blob/master/tutorials/IRIS.ipynb )IRIS dataset and got the similar errors:
```

from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
iris = load_iris()
iris.data[0:5], iris.target
Out[3]: 
(array([[5.1, 3.5, 1.4, 0.2],
        [4.9, 3. , 1.4, 0.2],
        [4.7, 3.2, 1.3, 0.2],
        [4.6, 3.1, 1.5, 0.2],
        [5. , 3.6, 1.4, 0.2]]),
 array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]))
X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target,
                                                    train_size=0.75, test_size=0.25)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
Out[4]: ((112, 4), (38, 4), (112,), (38,))
tpot = TPOTClassifier(verbosity=2, max_time_mins=2)
tpot.fit(X_train, y_train)
                                                                              
Generation 1 - Current best internal CV score: -inf
                                                                              Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 742, in fit
    log_file=self.log_file_
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\gp_deap.py"", line 281, in eaMuPlusLambda
    per_generation_function(gen)
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 1052, in _check_periodic_pipeline
    self._update_top_pipeline()
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 838, in _update_top_pipeline
    raise RuntimeError('There was an error in the TPOT optimization '
RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\IPython\core\interactiveshell.py"", line 3417, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-6-fc63521ba7ad>"", line 1, in <module>
    tpot.fit(X_train, y_train)
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 773, in fit
    raise e
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 764, in fit
    self._update_top_pipeline()
  File ""D:\Anaconda3\envs\tactory\lib\site-packages\tpot\base.py"", line 838, in _update_top_pipeline
    raise RuntimeError('There was an error in the TPOT optimization '
RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/
```
",hi also tried example iris got similar import import import iris array array generation current best internal score recent call last file line fit file line gen file line file line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation handling exception another exception recent call last file line file line module file line fit raise file line fit file line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation,issue,positive,positive,positive,positive,positive,positive
753342467,I confirm that `conda remove scikit-learn --force; conda install -c conda-forge scikit-learn=0.23.2` does indeed fix the issue!,confirm remove force install indeed fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
753257812,Hmm not sure what this PR is for. Please provide more details if you want to reopen this PR.,sure please provide want reopen,issue,positive,positive,positive,positive,positive,positive
753219787,"@beckernick thanks for suggesting downgrading scikit-learn to 0.23.2, it worked perfectly now.",thanks suggesting worked perfectly,issue,positive,positive,positive,positive,positive,positive
753019644,"I can reproduce this locally. It also reproduces with the scikit-learn TPOT, too. With an environment with scikit-learn 0.23.2 (the one that comes by default with the commands below), the snippet below works fine. In the same environment but with `conda remove scikit-learn --force; conda install -c conda-forge scikit-learn=0.24`, I get the same negative infinity RuntimeError (the cuML environment linked above uses scikit-learn 0.24)

Perhaps this is due to general scikit-learn 0.24 compatibility?

```
conda create -n tpot-test python=3.7
conda activate tpot-test
conda install -c conda-forge tpot dask dask-ml ipython
```

```python
from tpot import TPOTRegressor
from sklearn.datasets import make_regression

NSAMPLES = 1000
NFEATURES = 10
SEED = 12

X, y = make_regression(
    n_samples=NSAMPLES,
    n_features=NFEATURES,
    n_informative=NFEATURES,
    random_state=SEED,
    noise=200,
)

tpot = TPOTRegressor(
    generations=2,
    population_size=2,
    random_state=SEED,
    n_jobs=1,
    cv=2,
    verbosity=2,
)

tpot.fit(X, y)
```",reproduce locally also environment one come default snippet work fine environment remove force install get negative infinity environment linked perhaps due general compatibility create activate install python import import seed,issue,negative,positive,neutral,neutral,positive,positive
752963969,"Thanks @weixuanfu! 

[Here](https://github.com/giansegato/tpot/blob/issue-1148/notebooks/cuML_Regression_Example.ipynb) you can find the full Jupyter notebook with the error output (for some weird reasons ""Optimization Progress"" progress bar is presented at 0%; it actually stopped at 20/60, as you can see from my previous screenshot). I forked the notebook from the original examples folder available [here](https://github.com/EpistasisLab/tpot/blob/master/tutorials/cuML_Regression_Example.ipynb).

I'm currently running `tpot=0.11.6.post3`, installed via conda in the following way: `conda env create -f tpot-cuml.yml -n tpot-cuml`, using [this](https://github.com/EpistasisLab/tpot/blob/master/tpot-cuml.yml) conda enviornment file. Python version is `Python 3.7.9`.

This is my current `tpot` dependency tree:

```
tpot 0.11.6.post3 pyhd3deb0d_0
------------------------------
file name   : tpot-0.11.6.post3-pyhd3deb0d_0.tar.bz2
name        : tpot
version     : 0.11.6.post3
build       : pyhd3deb0d_0
build number: 0
size        : 82 KB
license     : LGPL-3.0-or-later
subdir      : noarch
url         : https://conda.anaconda.org/conda-forge/noarch/tpot-0.11.6.post3-pyhd3deb0d_0.tar.bz2
md5         : 78120def91befbeeb56147675f2ef0df
timestamp   : 2020-12-14 22:46:09 UTC
dependencies:
  - deap >=1.2
  - joblib >=0.13.2
  - numpy >=1.16.3
  - pandas >=0.24.2
  - py-xgboost >=0.90
  - python >=3.5
  - scikit-learn >=0.22.0
  - scipy >=1.3.1
  - stopit >=1.1.1
  - tqdm >=4.36.1
  - update_checker >=0.16
```

I made available a full list of the packages installed in the conda environment [in here](https://github.com/giansegato/tpot/blob/issue-1148/notebooks/conda_environment.txt). In particular, `tpot` is making use of `cuml=0.16.0=cuda10.2_py37_gbbe737348_0 `. I noticed that the latest `cuml` version is `0.17`, but from previous tests `0.17` doesn't fix the problem.

I noticed that the `cuml` build is referring `cuda 10.2`, while on my system I've `cuda 11.0` installed. I'm not sure whether this can be relevant to debugging the problem, but I wanted to highlight the discrepancy.

Enjoy your NYE, and have a happy 2021!
",thanks find full notebook error output weird optimization progress progress bar actually stopped see previous forked notebook original folder available currently running post via following way create file python version python current dependency tree post file name name version post build build number size license python made available full list environment particular making use latest version previous fix problem build system sure whether relevant problem highlight discrepancy enjoy nye happy,issue,positive,positive,positive,positive,positive,positive
752959406,Thank you for reporting this issue. I will be back to work for checking it next Wednesday. But I will try to debug it if I get a chance meanwhile. Could you please let me know the versions of TPOT and its dependencies in your environment? Also please share that jupyter notebook with all stderr messages via a GitHub link. Thank you.,thank issue back work next try get chance meanwhile could please let know environment also please share notebook via link thank,issue,positive,neutral,neutral,neutral,neutral,neutral
752937697,"Quick update. 

I got a fresh vanilla Ubuntu 18.04 server install, and I only installed the recommended package as suggested [here](https://epistasislab.github.io/tpot/installing/#installation-for-using-tpot-cuml-configuration) in the documentation.


## Repro steps

All the relevant commands from fresh system install (virgin [Nvidia AMI](https://aws.amazon.com/marketplace/pp/NVIDIA-NVIDIA-Deep-Learning-AMI/B076K31M1S)) to the error:

```
    1  sudo apt-get update
    2  sudo apt-get upgrade
    3  sudo apt-get install python3.8
    4  wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh
    5  bash Anaconda3-2020.11-Linux-x86_64.sh
    6  source ~/.bashrc
    7  wget https://raw.githubusercontent.com/EpistasisLab/tpot/master/tpot-cuml.yml
    8  conda env create -f tpot-cuml.yml -n tpot-cuml
    9  conda activate tpot-cuml
   10  wget https://raw.githubusercontent.com/EpistasisLab/tpot/master/tutorials/cuML_Regression_Example.ipynb
   11  pip install jupyter
   12  jupyter notebook
```

Moving forward, in a Jupyter environment, running the `cuML_Regression_Example.ipynb` file outputs the same error as before.

<img width=""800"" alt=""tpot-error"" src=""https://user-images.githubusercontent.com/5476684/103409494-67325a80-4b67-11eb-9662-ee4029e5a7f3.png"">

_(I decreased the number of population individuals and epochs to get the screenshot faster, but changing them doesn't make the result differ.)_ ",quick update got fresh vanilla server install package documentation relevant fresh system install virgin ami error update upgrade install python bash source create activate pip install notebook moving forward environment running file error number population get faster make result differ,issue,negative,positive,positive,positive,positive,positive
751496768,"OK changes made. I agree, I was thinking a future feature could be making the ColumnTransformer export use the sklearn version instead of the TPOT version, since they are the same anyways and it would make the exported pipeline much more visually simpler.",made agree thinking future feature could making export use version instead version since anyways would make pipeline much visually simpler,issue,negative,neutral,neutral,neutral,neutral,neutral
750486738,"Thanks @weixuanfu for the quick reply!  I tried both removing it and increasing it with the same result locally.  I also increased the max_time_mins parameter as a test.   

It does run in Colab, but not locally in my Juypter notebook.  I also download the ipynb file from Colab and tried it.  Same result, but sometimes I received a ""failed to converge after 200 iterations"" message also.  TPOT version on my local linux version is the same as Colab.  I suppose it could be environment otherwise so closing but thank you again for the reply.",thanks quick reply tried removing increasing result locally also parameter test run locally notebook also file tried result sometimes received converge message also version local version suppose could environment otherwise thank reply,issue,positive,positive,positive,positive,positive,positive
750464808,"I ran a quick test in [Google colab](https://colab.research.google.com/gist/weixuanfu/a9ba804167031eb4dbbf37fedd1f93ff/issue1147.ipynb) and please check it. This issue was not reproduced. 

But I think `max_eval_time_mins=0.04` (about 2 seconds) maybe too small to run the demo in some environment. Please remove it from `TPOTClassifier` and have another try. ",ran quick test please check issue think maybe small run environment please remove another try,issue,positive,positive,neutral,neutral,positive,positive
749184825,"Sorry for the late response. I drafted a comment last week but forgot to push the green comment button. 

Could you please a demo in docs_sources for using this new transformers? Thank you for this PR.

",sorry late response comment last week forgot push green comment button could please new thank,issue,positive,negative,negative,negative,negative,negative
748364642,"This is related to a bug found in #1142. I fixed it via PR #1143  and it was merged to development branch. And It will be included in next release of TPOT.

For testing the development branch, you may install TPOT with patch into your environment via:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",related bug found fixed via development branch included next release testing development branch may install patch environment via pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
748212784,1 million samples is big data for TPOT. Maybe you can only use TPOT cuML and also increase `max_eval_time_mins` to 20 or more.,million big data maybe use also increase,issue,negative,neutral,neutral,neutral,neutral,neutral
748211403,"@weixuanfu I had the same error, and have followed up on your modification, but does a million samples represent ""Big Data""? the dataset in question would just be around 600Mb, and there are many of them bigger than that. Could you please explain why this happens in a bit more detail?",error modification million represent big data question would around many bigger could please explain bit detail,issue,negative,positive,positive,positive,positive,positive
748204706,"No, it is an optional config. Please check [this installation guide for TPOT cuML](https://epistasislab.github.io/tpot/installing/#installation-for-using-tpot-cuml-configuration).",optional please check installation guide,issue,negative,neutral,neutral,neutral,neutral,neutral
748203497,"@Manidills I had the same problem. I recommend you buy Colab PRO for extra RAM which would prevent Colab from crashing. Even then, is there any way to save on Memory? @weixuanfu",problem recommend buy pro extra ram would prevent even way save memory,issue,negative,neutral,neutral,neutral,neutral,neutral
748188011,"Is it compulsory to use `TPOT cuML` for GPU acceleration, or would the vanilla ""pip"" install use GPU anyways?",compulsory use acceleration would vanilla pip install use anyways,issue,negative,neutral,neutral,neutral,neutral,neutral
748112085,"I suppose that you are using the [""TPOT cuML""](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) for using GPU-accelerated estimators in RAPIDS cuML and DMLC XGBoost. Unfortunately, I do not know if RAPIDS cuML supports V100 or P100. I only have very limited experience on using it on regression problem. I have tested ""TPOT cuML"" on 2080Ti before with a regression benchmark with 50000 samples and 50 features and it took 1-2 days to finish 100 generations with 100 population size and cv=5. ",suppose unfortunately know limited experience regression problem tested ti regression took day finish population size,issue,negative,negative,negative,negative,negative,negative
748078938,Alright. Would you also happen to know how much time does TPOT take on a regression problem (Just a rough estimate) on a normal GPU like V100 or a P100? @weixuanfu,alright would also happen know much time take regression problem rough estimate normal like,issue,negative,positive,neutral,neutral,positive,positive
745992582,"Thanks for the support!
The fix seems to work.",thanks support fix work,issue,positive,positive,positive,positive,positive,positive
745426054,"
[![Coverage Status](https://coveralls.io/builds/35734407/badge)](https://coveralls.io/builds/35734407)

Coverage increased (+0.004%) to 95.875% when pulling **e2eb3e7ed8047a59556c08244c9e7a82a33343bb on weixuanfu:issue1142** into **20f90db1c1bb698aeeeceab08ca5830461d74f32 on EpistasisLab:development**.
",coverage status coverage issue development,issue,negative,neutral,neutral,neutral,neutral,neutral
745425584,"I fixed it via PR #1143  and it was merged to development branch. And It will be included in next release of TPOT.

For testing the development branch, you may install TPOT with patch into your environment via:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",fixed via development branch included next release testing development branch may install patch environment via pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
745383698,"In this case, I think it will trigger an issue when you are using TPOTClassifier if it is indeed not a regression problems. 

TPOT should try to randomly sample a subset of 50 samples that can covers all unique y values for pre-testing pipelines. But since it is more than 50 unique values, then that down-sampling step is not working correctly. 

We will fix this issue. ",case think trigger issue indeed regression try randomly sample subset unique since unique step working correctly fix issue,issue,negative,positive,neutral,neutral,positive,positive
745343872,"How many unique classes in y?

-------------------------------------------------------------------------------
Weixuan Fu

Email: weixuanf@pennmedicine.upenn.edu<mailto:weixuanf@pennmedicine.upenn.edu>


On Dec 15, 2020 at 6:24 AM, <Bart<mailto:notifications@github.com>> wrote:


I have a panda's dataset where I want to find the best classifier machinelearning algorithm for.

RangeIndex: 7820 entries, 0 to 7819
Data columns (total 6 columns):

Column Non-Null Count Dtype
________________________________

0 CategoryName 7820 non-null object
1 CategoryId 7820 non-null int64
2 ManufacturerCode 7820 non-null object
3 ManufacturerCategory 7820 non-null object
4 Description 7820 non-null object
5 Description2 7820 non-null object

I've created a preprocessor to transform the text data to numeric data:
`hash_transformer = Pipeline(steps=[
('hasher', FeatureHasher(n_features=2**17, input_type='string')),
])

vector_transformer = Pipeline(steps=[
('vector', HashingVectorizer(n_features=2**20)),
])

preprocessor = ColumnTransformer(
transformers=[
('manufacturerCode', hash_transformer, 'ManufacturerCode'),
('description', vector_transformer, 'Description'),
('description2', vector_transformer, 'Description2'),
('ManufacturerCategory', vector_transformer, 'ManufacturerCategory'),
], n_jobs=-1, verbose=True)
`

When I try to run TPOT I get an error:
`X = preprocessor.fit_transform(data)
y = data['CategoryId'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
tpot = TPOTClassifier(config_dict='TPOT sparse', n_jobs= -1, population_size=2, generations=2, verbosity=0, random_state=42)
tpot.fit(X_train, y_train)
`

This is the whole error message:

________________________________

ValueError Traceback (most recent call last)
in
5 tpot = TPOTClassifier(config_dict='TPOT sparse', n_jobs= -1, population_size=2, generations=2, verbosity=0, random_state=42)
6
----> 7 tpot.fit(X_train, y_train)
8 print(tpot.score(X_test, y_test))
9 #tpot.export('tpot_digits_pipeline.py')

c:\users\b.vanlier\source\bomlist jupyter.env\lib\site-packages\tpot\base.py in fit(self, features, target, sample_weight, groups)
659 features, target = self._check_dataset(features, target, sample_weight)
660
--> 661 self._init_pretest(features, target)
662
663 # Randomly collect a subsample of training samples for pipeline optimization process.

c:\users\b.vanlier\source\bomlist jupyter.env\lib\site-packages\tpot\tpot.py in _init_pretest(self, features, target)
62 if not np.array_equal(np.unique(target),np.unique(self.pretest_y)):
63 unique_target_idx = np.unique(target,return_index=True)[1]
---> 64 self.pretest_y[0:unique_target_idx.shape[0]] =
65 _safe_indexing(target, unique_target_idx)
66

ValueError: could not broadcast input array from shape (105) into shape (50)

It seems it has to do somenthing with the input data, but I can't get a grasp on what that might be. When I limit the input data to the first 4000 rows it works fine. When I try the first 5000 rows I get the error. When I use the rows between 4000 and 5000 it works fine. My first tought was some bogus characters in the input data but I cant find any consistencies to track these records down.
Any help on resolving this would be apreciated.

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/1142>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AFA3WKSUEKTU7F4BPJM4OX3SU5BLRANCNFSM4U4F5QQA>.
",many unique class fu wrote panda want find best classifier algorithm data total column count object object object description object description object transform text data data pipeline pipeline try run get error data data sparse whole error message recent call last sparse print fit self target target target target randomly collect subsample training pipeline optimization process self target target target target could broadcast input array shape shape input data ca get grasp might limit input data first work fine try first get error use work fine first tought bogus input data cant find track help would thread reply directly view,issue,negative,positive,positive,positive,positive,positive
744504111,Sorry I just removed an incorrect comment on this issue. ,sorry removed incorrect comment issue,issue,negative,negative,negative,negative,negative,negative
744503702,I made a patch about this issue in [v0.11.6.post3](https://pypi.org/project/TPOT/0.11.6.post3/). Please check this version. ,made patch issue post please check version,issue,negative,neutral,neutral,neutral,neutral,neutral
744492936,"
[![Coverage Status](https://coveralls.io/builds/35697104/badge)](https://coveralls.io/builds/35697104)

Coverage remained the same at 96.393% when pulling **a8b30140f0c78b84985787b5bcea2edab03f4878 on weixuanfu:issue1139** into **1b013b2db27008952b04273d6cb7a00b1c88c192 on EpistasisLab:master**.
",coverage status coverage issue master,issue,negative,neutral,neutral,neutral,neutral,neutral
744474638,"Regarding a more universal way for getting features importance from pipelines, you may try to estimate [permutation importance](https://scikit-learn.org/stable/modules/permutation_importance.html) with `tpot_obj.fitted_pipeline_`. ",regarding universal way getting importance may try estimate permutation importance,issue,positive,neutral,neutral,neutral,neutral,neutral
742959092,"OK I have an idea. In `TPOTBase.fit()`, add a hook called `_dynamically_modify_config_dict()`, which will modify `config_dict` with a dict like this:

```
    'tpot.builtins.ColumnTransformer': {
        'transformers': [''sklearn.preprocessing.StandardScaler', 'sklearn.preprocessing.RobustScaler', ...],
        'include_col_1': [True, False],
        'include_col_2': [True, False],
        ...
        'include_col_n': [True, False],
    },
```

What do you think @weixuanfu ?",idea add hook modify like true false true false true false think,issue,positive,negative,neutral,neutral,negative,negative
742954283,Interested in working on this. `ColumnTransformer` takes a list of columns. How to make this list available to the genetic algorithm?,interested working list make list available genetic algorithm,issue,negative,positive,positive,positive,positive,positive
742928232,"Please try this workaround below for using TPOTClassifier with xgboost >=1.30. 

```python
from tpot import TPOTClassifier
from tpot.config import classifier_config_dict
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import numpy as np
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)
# fit xgboost v1.3.0 API

classifier_config_dict['xgboost.XGBClassifier'] = {
    'n_estimators': [100],
    'max_depth': range(1, 11),
    'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
    'subsample': np.arange(0.05, 1.01, 0.05),
    'min_child_weight': range(1, 21),
    'n_jobs': [1], # replace ""nthread""
    'verbosity': [0] # add this line to slient warning message
}
        
# for a quick test
tpot = TPOTClassifier(generations=2, population_size=10, verbosity=2,
                      config_dict=classifier_config_dict)
tpot.fit(X_train, y_train)
```

For TPOTRegressor, the config_dict should be:

```python
from tpot.config import regressor_config_dict

regressor_config_dict['xgboost.XGBRegressor'] =
{
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21),
        'objective': ['reg:squarederror'],
        'n_jobs': [1], # replace ""nthread""
        'verbosity': [0] # add this line to slient warning message
}
```
We will add a patch about this issue to TPOT next week. ",please try python import import import import import fit range range replace add line warning message quick test python import range range replace add line warning message add patch issue next week,issue,negative,positive,positive,positive,positive,positive
741513907,"> Also, please test codes above again without `max_time_mins=10`. This parameters will override generation parameter and kill the fit() process in 10 minutes. If the process not get a best pipeline in the time limit, no fitted pipeline will be exported. I think it maybe the reason of this issue.
> 
> Update: the codes below may be used to reproduce the issue. I think we need add a friendly warning for using this parameters when running a time-consuming jobs. Sorry for the confusion.
> 
> ```
> from sklearn.model_selection import train_test_split
> from sklearn.datasets import make_classification
> from tpot import TPOTClassifier
> X, y = make_classification(n_samples=200, n_features=100,
>                                     n_informative=2, n_redundant=10,
>                                     random_state=42)
> X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)
> 
> tpot = TPOTClassifier(verbosity=2, max_time_mins=1)
> tpot.fit(X_train, y_train)
> tpot.export('tpot_pipe.py')
> print(tpot.score(X_test, y_test))
> ```

Thanks, it works with me.",also please test without override generation parameter kill fit process process get best pipeline time limit fitted pipeline think maybe reason issue update may used reproduce issue think need add friendly warning running sorry confusion import import import print thanks work,issue,positive,positive,positive,positive,positive,positive
736706416,"Apologies, I see this feature was started and paused in PR #1001. I'm unlikely to find time for this in the near future but leaving this here to let future visitors of the thread know the current status.",see feature unlikely find time near future leaving let future thread know current status,issue,negative,negative,neutral,neutral,negative,negative
736702801,"Any update on whether this is a planned feature?

As far as I'm aware multi-label is only supported by a subset of sklearn algorithms and some require setting specific options in their `__init__` to enable it which seems to go against how TPOT is set up to specify pipelines. 

However, some sklearn algorithms automatically infer this and adapt accordingly without any extra specification, perhaps TPOT could do something similar in it's `_init_pretest`, detecting multi-label labels and reducing the configuration space to just those algorithms which require no extra steps to support it?

https://scikit-learn.org/stable/modules/multiclass.html",update whether feature far aware subset require setting specific enable go set specify however automatically infer adapt accordingly without extra specification perhaps could something similar reducing configuration space require extra support,issue,negative,positive,neutral,neutral,positive,positive
732426784,It can be predefined as a linear pipeline with `template` option. Please check this [link](https://colab.research.google.com/gist/weixuanfu/6f3dfa1bd1f630c76a6aadf5e867dfde/issue1138.ipynb) for a quick example.,linear pipeline template option please check link quick example,issue,negative,positive,positive,positive,positive,positive
724897441,Thank you very much. I'll have to wait a little bit but will have a shot at them,thank much wait little bit shot,issue,negative,positive,neutral,neutral,positive,positive
724881719,I think I need add more unit tests with config_dict that includes imblearn resampler before merging the changes to master branch. Please feel free to submit another PR if you want to add unit tests. ,think need add unit master branch please feel free submit another want add unit,issue,positive,positive,positive,positive,positive,positive
724881080,I merged this PR to development branch with a commit https://github.com/EpistasisLab/tpot/commit/0716a34719d2942682ce3294c3fe1ca0a3576178 that should fixed the 3 tests above.,development branch commit fixed,issue,negative,positive,neutral,neutral,positive,positive
724860021,"There are now just 3 tests failing 🎉 but I can't work out how to get past them. 

They all seem to be related to the exporting of the pipeline

```
======================================================================
FAIL: Assert that generate_import_code() returns the correct set of dependancies and dependancies are importable.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Miniconda36-x64\envs\test-environment\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""C:\projects\tpot\tests\export_tests.py"", line 277, in test_generate_import_code_2
    assert expected_code == import_code
AssertionError
======================================================================
FAIL: Assert that exported_pipeline() generated a compile source file as expected given a fixed pipeline.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Miniconda36-x64\envs\test-environment\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""C:\projects\tpot\tests\export_tests.py"", line 340, in test_export_pipeline
    assert expected_code == export_pipeline(pipeline, tpot_obj.operators, tpot_obj._pset)
AssertionError
======================================================================
FAIL: Assert that exported_pipeline() generated a compile source file as expected given a fixed simple pipeline with input_matrix in CombineDFs.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Miniconda36-x64\envs\test-environment\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""C:\projects\tpot\tests\export_tests.py"", line 448, in test_export_pipeline_4
    assert expected_code == export_pipeline(pipeline, tpot_obj.operators, tpot_obj._pset)
AssertionError
```",failing ca work get past seem related pipeline fail assert correct set importable recent call last file line file line assert fail assert compile source file given fixed pipeline recent call last file line file line assert pipeline fail assert compile source file given fixed simple pipeline recent call last file line file line assert pipeline,issue,negative,negative,negative,negative,negative,negative
724826208,"> `_get_make_pipeline_func` is called after `self._setup_config(self.config_dict)` is run so I'm confused as to why it's still None

Please check my comment above.

Just using `self._config_dict` instead",run confused still none please check comment instead,issue,negative,negative,negative,negative,negative,negative
724819584,"> Thank you. Would I need to change anything here then or could I leave it as sklearn?
> 
> ```
>     if num_op_root > 1:
>         return {
>             ""sklearn.model_selection"": [""train_test_split""],
>             ""sklearn.pipeline"": [""make_union"", ""make_pipeline""],
>             ""tpot.builtins"": [""StackingEstimator""],
>         }
>     elif num_op > 1:
>         return {
>             ""sklearn.model_selection"": [""train_test_split""],
>             ""sklearn.pipeline"": [""make_pipeline""],
>         }
> ```

Yes, if a pipeline includes imblearn, then it should use make_pipeline from imblearn. If not, then use the one from sklearn. I hope sklearn can support/include imblearn in the future. ",thank would need change anything could leave return return yes pipeline use use one hope future,issue,positive,neutral,neutral,neutral,neutral,neutral
724817323,"Thank you. Would I need to change anything here then or could I leave it as sklearn?

```
    if num_op_root > 1:
        return {
            ""sklearn.model_selection"": [""train_test_split""],
            ""sklearn.pipeline"": [""make_union"", ""make_pipeline""],
            ""tpot.builtins"": [""StackingEstimator""],
        }
    elif num_op > 1:
        return {
            ""sklearn.model_selection"": [""train_test_split""],
            ""sklearn.pipeline"": [""make_pipeline""],
        }
```",thank would need change anything could leave return return,issue,negative,neutral,neutral,neutral,neutral,neutral
724812531,"> I've made it toggleable on whether an `imblearn` component is specified in the `config_dict` for `base.py`, however I also need to change it in `export_utils.py` under `_starting_imports` which `config_dict` isn't being passed to.

Yep, for the pipeline did not involve `imblearn`, then `make_pipeline` from imblearn is not needed.",made whether component however also need change yep pipeline involve,issue,negative,neutral,neutral,neutral,neutral,neutral
724807718,"I've made it toggleable on whether an `imblearn` component is specified in the `config_dict` for `base.py`, however I also need to change it in `export_utils.py` under `_starting_imports` which `config_dict` isn't being passed to.",made whether component however also need change,issue,negative,neutral,neutral,neutral,neutral,neutral
724780123,"How about adding checks whether a configuration `config_dict` includes imblearn or not. If so, TPOT and exported pipeline should use make_pipeline from imblearn.",whether configuration pipeline use,issue,negative,neutral,neutral,neutral,neutral,neutral
724776673,"Unfortunately the samplers require the `imblearn` `Pipeline`

> [The difficulty here is that ImbLearn applies `fit` and `sample`, notice the latter is not `transform` as it does not change features (transformations), only the re-samples (hence sampling). For this reason, ImbLearn provides its own `Pipeline` module, as it needs to wrap the `sample` functionality in a way that makes sense (it only samples on training and not on testing, etc) and is compatible with SciKit-Learn API flow.](https://github.com/EpistasisLab/tpot/issues/547#issuecomment-451115193) - [@romanovzky](https://github.com/romanovzky)  

How would you recommend best integrating it as an optional dependency of TPOT?",unfortunately require pipeline difficulty fit sample notice latter transform change hence sampling reason pipeline module need wrap sample functionality way sense training testing compatible flow would recommend best optional dependency,issue,positive,positive,positive,positive,positive,positive
724758253,"The PR did not referred a correction Appveyor build, the correct one is [here][https://ci.appveyor.com/project/weixuanfu/tpot/builds/36233033]. `ModuleNotFoundError` is gone, but we got other errors. I think the error is from changing import source of `make_pipeline` from `sklearn` to `imblearn`.  Can those samplers in imblearn work with sklearn's `make_pipeline`?

I think we need make imblearn as optional dependency of TPOT.

",correction build correct one gone got think error import source work think need make optional dependency,issue,negative,neutral,neutral,neutral,neutral,neutral
724752354,"Have now added them to both, for some reason the AppVeyor build was cancelled. Thank you for helping with this.",added reason build thank helping,issue,positive,neutral,neutral,neutral,neutral,neutral
724743690,Thank you for the PR. Please add installation of `imblearn` into [ci environment in travis](https://github.com/EpistasisLab/tpot/blob/master/ci/.travis_install.sh) and [appveyor](https://github.com/EpistasisLab/tpot/blob/master/.appveyor.yml) to fix the `ModuleNotFoundError` . This PR seems very similar with #1136. Could you please merge them into one PR?,thank please add installation environment travis fix similar could please merge one,issue,positive,neutral,neutral,neutral,neutral,neutral
724675721,also added `imblearn` to `setup.py` but I'm still getting `ModuleNotFoundError: No module named 'imblearn'`,also added still getting module,issue,negative,neutral,neutral,neutral,neutral,neutral
724665713,"Regarding the specific issue I'm encountering.

**Goal**: Want to be able to resample based on the group specified in a multi-index
**Current progress**: Custom components work fine in a standard imblearn pipeline
**Current issue**: The custom components break the TPOT regression optimisation

A dummy dataset can be created like this
```
from sklearn.datasets import make_regression

flatten = lambda t: [item for sublist in t for item in sublist]
months = flatten([[x]*100*x for x in range(1, 13)])
idx = pd.MultiIndex.from_arrays([range(len(months)), months], names=['unique', 'month'])

X, y = make_regression(n_samples=len(idx), n_features=20)

df_X, s_y = pd.DataFrame(X, index=idx), pd.Series(y, index=idx)

df_X
```

The components are defined in a script called `operators.py`
```
from sklearn.ensemble import RandomForestRegressor
from imblearn.over_sampling import RandomOverSampler

def add_series_index(idx_arg_pos=0):
    def decorator(func):
        def decorator_wrapper(*args, **kwargs):
            input_s = args[idx_arg_pos]
            assert isinstance(input_s, (pd.Series, pd.DataFrame))
            result = pd.Series(func(*args, **kwargs), index=input_s.index)
            return result
        return decorator_wrapper
    return decorator

class PandasRandomForestRegressor(RandomForestRegressor):
    def __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None, score_func=None):
        super().__init__(n_estimators=n_estimators, criterion=criterion, max_depth=max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf, min_weight_fraction_leaf=min_weight_fraction_leaf, max_features=max_features, max_leaf_nodes=max_leaf_nodes, min_impurity_decrease=min_impurity_decrease, min_impurity_split=min_impurity_split, bootstrap=bootstrap, oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, verbose=verbose, warm_start=warm_start, ccp_alpha=ccp_alpha, max_samples=max_samples)
    
        if score_func is None:
            self.score_func = r2_score
        else:
            self.score_func = score_func
            
    @add_series_index(1)
    def predict(self, X):
        pred = super().predict(X)
        return pred
    
    def score(self, X, y, *args, **kwargs):        
        y_pred = self.predict(X)
        score = self.score_func(y, y_pred, *args, **kwargs)
        return score
    
def custom_resampler_helper(X, y, class_col, resample_func):
    # Checking indexes match
    assert X.index.equals(y.index), 'X and y indexes should be the same'

    # Extracting idx names and mapping to y values
    idx_names = X.index.names
    idx_to_y = dict(zip(y.reset_index()[idx_names].apply(tuple, axis=1).values, y.values))

    # Resampling values
    classes = X.reset_index()[class_col]
    X_resampled, _ = resample_func(X.reset_index(), classes)
    y_resampled = X_resampled[idx_names].apply(tuple, axis=1).map(idx_to_y)

    # Formatting indexes
    X_resampled = X_resampled.set_index(idx_names)
    y_resampled.index = X_resampled.index
    
    return X_resampled, y_resampled

class XRandomOverSampler(RandomOverSampler):
    def __init__(self, class_col, sampling_strategy='auto'):
        super().__init__(sampling_strategy='auto')
        self.class_col = class_col
        
    def fit(self, X):
        classes = X.reset_index()[self.class_col]
        super().fit(X, classes)
        return
            
    def fit_resample(self, X, y):
        return custom_resampler_helper(X, y, self.class_col, super().fit_resample)
            
    def fit_sample(self, X, y):
        return self.fit_resample(X, y)
```

I then create a test pipeline like so:
```
import operators
from imblearn.pipeline import Pipeline

pipeline = Pipeline([
    ('xros', operators.XRandomOverSampler('month')),
    ('pandas_RF', operators.PandasRandomForestRegressor(n_estimators=100, n_jobs=-1))
])
```

Which works with the standard sklearn fit/predict
```
pipeline.fit(df_X, s_y)

df_pred = pipeline.predict(df_X)
```

However it breaks with TPOT
```
regressor_config_dict = {
    # Classifiers
    'operators.PandasRandomForestRegressor': {
        'n_estimators': [100],
        'max_features': np.arange(0.05, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },
    
    # Preprocessors
    'operators.XRandomOverSampler': {
        'class_col': ['month']
    },
}

pipeline_optimizer = TPOTRegressor(generations=5, population_size=10, cv=3,
                                   random_state=42, verbosity=2, n_jobs=-1, 
                                   config_dict=regressor_config_dict, 
                                   template='XRandomOverSampler-PandasRandomForestRegressor')

pipeline_optimizer.fit(df_X, s_y)
```

For which I get this error
```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
c:\path\to\tpot\tpot\base.py in fit(self, features, target, sample_weight, groups)
    742                     per_generation_function=self._check_periodic_pipeline,
--> 743                     log_file=self.log_file_
    744                 )

c:\path\to\tpot\tpot\gp_deap.py in eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function, log_file)
    280         if per_generation_function is not None:
--> 281             per_generation_function(gen)
    282 

c:\path\to\tpot\tpot\base.py in _check_periodic_pipeline(self, gen)
   1052         """"""
-> 1053         self._update_top_pipeline()
   1054         if self.periodic_checkpoint_folder is not None:

c:\path\to\tpot\tpot\base.py in _update_top_pipeline(self)
    838                         break
--> 839                 raise RuntimeError('There was an error in the TPOT optimization '
    840                                    'process. This could be because the data was '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/
```
",regarding specific issue goal want able resample based group current progress custom work fine standard pipeline current issue custom break regression dummy like import flatten lambda item item flatten range range defined script import import decorator assert result return result return return decorator class self super none else predict self super return score self score return score match assert zip class class return class self super fit self class super class return self return super self return create test pipeline like import import pipeline pipeline pipeline work standard however range range true false get error recent call last fit self target population toolbox mu verbose none gen self gen none self break raise error optimization could data error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation,issue,positive,positive,positive,positive,positive,positive
724648964,"The error appears to be specific to a custom component I'm using which requries the index of the passed data. In have this working in imblearn but trying to include this in TPOT was what broke it, one step at a time.. 

The good news is that I don't have this issue on standard imblearn components and the following will work if you use the fork I've made [here](https://github.com/AyrtonB/tpot/tree/patch-1), have just made a PR as well.

```
from tpot import TPOTClassifier
from sklearn.datasets import make_classification

classifier_config_dict = {
    # Classifiers
    'sklearn.ensemble.ExtraTreesClassifier': {
        'n_estimators': [100],
        'criterion': [""gini"", ""entropy""],
        'max_features': np.arange(0.05, 1.01, 0.05),
        'min_samples_split': range(2, 21),
        'min_samples_leaf': range(1, 21),
        'bootstrap': [True, False]
    },

    # Preprocessors
    'imblearn.over_sampling.RandomOverSampler': {
    },
    
}

X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,
                           n_redundant=0, n_repeated=0, n_classes=3,
                           n_clusters_per_class=1,
                           weights=[0.01, 0.05, 0.94],
                           class_sep=0.8, random_state=0)

pipeline_optimizer = TPOTClassifier(generations=5, population_size=10, cv=3,
                                   random_state=42, verbosity=2, n_jobs=-1, 
                                   config_dict=classifier_config_dict, 
                                   template='RandomOverSampler-Classifier')

pipeline_optimizer.fit(X, y)

pipeline = pipeline_optimizer.fitted_pipeline_
```

Output

```
Generation 1 - Current best internal CV score: 0.98940007916784

Generation 2 - Current best internal CV score: 0.98940007916784

Generation 3 - Current best internal CV score: 0.9896000391758383

Generation 4 - Current best internal CV score: 0.9896000391758383

Generation 5 - Current best internal CV score: 0.9897999991838367

Best pipeline: ExtraTreesClassifier(RandomOverSampler(input_matrix), bootstrap=False, criterion=gini, max_features=0.3, min_samples_leaf=1, min_samples_split=10, n_estimators=100)
Wall time: 28.4 s
```",error specific custom component index data working trying include broke one step time good news issue standard following work use fork made made well import import entropy range range true false pipeline output generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline wall time,issue,positive,positive,positive,positive,positive,positive
724052770,@AyrtonB Could you please share the link of your branch with those changes and also provide a demo to reproduce the error? I can take a look.,could please share link branch also provide reproduce error take look,issue,negative,neutral,neutral,neutral,neutral,neutral
723973738,"I'm also trying to integrate imblearn with TPOT and have made a number of code changes to try and make it happen. After making changes in what seemed like the obvious places I'm now met with an error which I'm not sure how to deal with.

Any advice would be much appreciated!

```
~\anaconda3\envs\env\lib\site-packages\tpot\base.py in _update_top_pipeline(self)
    837                                                     error_score=""raise"")
    838                         break
--> 839                 raise RuntimeError('There was an error in the TPOT optimization '
    840                                    'process. This could be because the data was '
    841                                    'not formatted properly, or because data for '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/
```

<br>

**Code Additions/Changes**

Need to add a way to check that an object is a resampler
```
from imblearn.over_sampling import RandomOverSampler

def _is_resampler(estimator):
    return hasattr(estimator, ""fit_resample"")

assert _is_resampler(RandomOverSampler)
```

I added `_is_resampler` to operator_utils.py then included it twice within `TPOTOperatorClassFactory` at line 201

```
        if is_classifier(op_obj):
            class_profile[""root""] = True
            optype = ""Classifier""
        elif is_regressor(op_obj):
            class_profile[""root""] = True
            optype = ""Regressor""
        elif _is_transformer(op_obj):
            optype = ""Transformer""
        elif _is_selector(op_obj):
            optype = ""Selector""
        elif _is_resampler(op_obj):
            optype = ""Resampler""
        else:
            raise ValueError(
                ""optype must be one of: Classifier, Regressor, Transformer, Selector, Resampler""
            )
```
 and line 330

```
                    if inspect.isclass(doptype):  # a estimator
                        if (
                            issubclass(doptype, BaseEstimator)
                            or is_classifier(doptype)
                            or is_regressor(doptype)
                            or _is_transformer(doptype)
                            or _is_resampler(doptype)
                            or issubclass(doptype, Kernel)
                        ):

```
<br>

As raised by @ksyme99 the pipeline needs to be changed out for one from imblearn.

In [base.py](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L51) I believe we just need to change the following line 
```
from sklearn.pipeline import make_pipeline, make_union
```
to
```
from sklearn.pipeline import make_union
from imblearn.pipeline import make_pipeline
```

<br>

And in [export_utils](https://github.com/EpistasisLab/tpot/blob/master/tpot/export_utils.py#L229) I believe we need to change
```
def _starting_imports(operators, operators_used):
    
    ...

    if num_op_root > 1:
        return {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline', 'make_union'],
            'tpot.builtins':  ['StackingEstimator'],
        }
    elif num_op > 1:
        return {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_pipeline']
        }
```
to
```
def _starting_imports(operators, operators_used):
    
    ...

    if num_op_root > 1:
        return {
            'sklearn.model_selection':  ['train_test_split'],
            'sklearn.pipeline':         ['make_union'],
            'imblearn.pipeline':         ['make_pipeline'],
            'tpot.builtins':  ['StackingEstimator'],
        }
    elif num_op > 1:
        return {
            'sklearn.model_selection':  ['train_test_split'],
            'imblearn.pipeline':         ['make_pipeline']
        }
```",also trying integrate made number code try make happen making like obvious met error sure deal advice would much self raise break raise error optimization could data properly data error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation code need add way check object import estimator return estimator assert added included twice within line root true classifier root true regressor transformer selector else raise must one classifier regressor transformer selector line estimator kernel raised pipeline need one believe need change following line import import import believe need change return return return return,issue,positive,positive,positive,positive,positive,positive
723937827,"I had two errors with CI but neither are related to the changes in this PR

```
======================================================================
FAIL: Assert that TPOT template option generates pipeline when one of steps is a specific operator.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Miniconda36-x64\envs\test-environment\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""C:\projects\tpot\tests\tpot_tests.py"", line 779, in test_template_3
    assert sklearn_pipeline.steps[0][0] == 'SelectPercentile'.lower()
AssertionError
======================================================================
FAIL: Assert that TPOT operators return their type, e.g. 'Classifier', 'Preprocessor'.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""C:\Miniconda36-x64\envs\test-environment\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""C:\projects\tpot\tests\tpot_tests.py"", line 2445, in test_operator_type
    assert TPOTSelectPercentile.type() == ""Selector""
AssertionError
----------------------------------------------------------------------

```",two neither related fail assert template option pipeline one specific operator recent call last file line file line assert fail assert return type recent call last file line file line assert selector,issue,positive,negative,negative,negative,negative,negative
722430363,"Hmm, [Earth](https://github.com/scikit-learn-contrib/py-earth/blob/master/pyearth/earth.py#L13) can be both regressor and transformer (also see this [notebook](https://colab.research.google.com/gist/weixuanfu/eddcb258b7b9e9dc3eba31c405b2fcab/issue1134.ipynb)), which is very abnormal. I will refine the logic in TPOT to set it as regressor instead of transformer (regressor type has higher priority than transformer).


",earth regressor transformer also see notebook abnormal refine logic set regressor instead transformer regressor type higher priority transformer,issue,negative,positive,positive,positive,positive,positive
717642485,"> Hmm could you please make a demo to reproduce this issue? I tested it in Colab (see [this notebook](https://colab.research.google.com/gist/weixuanfu/d9303d3f1e654e572cecbb56d700de6f/issue1133.ipynb)) but no error shown up.
> 
> BTW, you can update TPOT to 0.11.6 for clearer error messages in this situation.

The problem is that the trainning time is too limited. After two hours trainning, the score is reported.",could please make reproduce issue tested see notebook error shown update clearer error situation problem time limited two score,issue,negative,negative,neutral,neutral,negative,negative
717600231,"I had to add fsspec to the end, but your last comment seems to have worked for me

```
!pip install TPOT
!pip install dask==2.20.0 dask-glm==0.2.0 dask-ml==1.0.0
!pip install tornado==5.0
!pip install distributed==2.2.0
!pip install xgboost==0.90
!pip install fsspec
```",add end last comment worked pip install pip install pip install pip install pip install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
717274553,"> Hmm could you please make a demo to reproduce this issue? I tested it in Colab (see [this notebook](https://colab.research.google.com/gist/weixuanfu/d9303d3f1e654e572cecbb56d700de6f/issue1133.ipynb)) but no error shown up.
> 
> BTW, you can update TPOT to 0.11.6 for clearer error messages in this situation.

After upgrading tpot to 0.11.6, the same error is reported. Maybe it is really the problem of the data. Now I am using npy format data, I will try to change it to csv format.",could please make reproduce issue tested see notebook error shown update clearer error situation error maybe really problem data format data try change format,issue,negative,positive,positive,positive,positive,positive
717250189,"Hmm could you please make a demo to reproduce this issue? I tested it in Colab (see [this notebook](https://colab.research.google.com/gist/weixuanfu/d9303d3f1e654e572cecbb56d700de6f/issue1133.ipynb)) but no error shown up. 

BTW, you can update TPOT to 0.11.6 for clearer error messages in this situation. ",could please make reproduce issue tested see notebook error shown update clearer error situation,issue,negative,neutral,neutral,neutral,neutral,neutral
714600687,"So something in this part of the code needs to be changed.

![image](https://user-images.githubusercontent.com/59226057/96900045-189f7a80-14ab-11eb-83e7-4ead763035d3.png)",something part code need image,issue,negative,neutral,neutral,neutral,neutral,neutral
714539780,"> Hmm, it is strange. It should work. Please check [the demo in Colab](https://colab.research.google.com/gist/weixuanfu/8352982107f46406ef69d940e1fccf4c/tpot_reg_n_jobs.ipynb) and also make sure there is no python scripts named `tpot.py` in your work directory.

Ah, this is caused by the TPOT version. The version is **0.11.5** on Colab while the one in my development image is just **0.6.8**... Thank you!!! ",strange work please check also make sure python work directory ah version version one development image thank,issue,positive,positive,positive,positive,positive,positive
714538907,"Sounds good. cuML 0.16 has been released, so the following environment should work (ran the example notebooks in the `development` branch with it this morning):

`tpot-minimal.yml`
```
channels:
  - rapidsai
  - nvidia
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - cudatoolkit=10.2
  - cuml=0.16
  - scikit-learn
  - ipython
  - ipywidgets
  - jupyterlab
  - pip
  - pip:
    - xgboost
    - git+https://github.com/epistasislab/tpot.git@development
```

`conda env create -f tpot-minimal.yml -n tpot-minimal --force`",good following environment work ran example development branch morning pip pip create force,issue,positive,positive,positive,positive,positive,positive
714529202,"I think the point here is not about reshaping the `final_features` and it should be about making `predict` function in the Flask to get the input X for prediction instead of making one from list object within the function. The input X should has a shape like (N, 2), N is the number of samples in the dataset. ",think point making predict function flask get input prediction instead making one list object within function input shape like number,issue,negative,neutral,neutral,neutral,neutral,neutral
714525994,"Hmm, it is strange. It should work. Please check [the demo in Colab](https://colab.research.google.com/gist/weixuanfu/8352982107f46406ef69d940e1fccf4c/tpot_reg_n_jobs.ipynb) and also make sure there is no python scripts named `tpot.py` in your work directory. ",strange work please check also make sure python work directory,issue,negative,positive,positive,positive,positive,positive
714370395,"> Thank you for your suggestion. But in the near future, we don't have a plan of generating synthetic data in TPOT.
> 
> You may use some transformers/estimators in imbalanced-learn with a [custom configuration](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) in TPOT if they support scikit-learn API.

@weixuanfu is there a possibility to include a `scikit` [learning-curve](https://scikit-learn.org/stable/modules/learning_curve.html) in this module?",thank suggestion near future plan generating synthetic data may use custom configuration support possibility include module,issue,positive,positive,neutral,neutral,positive,positive
714305589,"When I tried to reshape final_features, I got a ValueError saying that an array of size 1 cannot be reshaped into shape (1,2).",tried reshape got saying array size shape,issue,negative,neutral,neutral,neutral,neutral,neutral
713736601,"Thanks. Since final_features is a list object, I used len(final_features) to get the length and the length turned out to be 1 and the model is fitted with 2 features. How to solve this?",thanks since list object used get length length turned model fitted solve,issue,positive,positive,positive,positive,positive,positive
713632869,"Thank you for your suggestion. But in the near future, we don't have a plan of generating synthetic data in TPOT.

You may use some transformers/estimators in imbalanced-learn with a [custom configuration](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) in TPOT if they support scikit-learn API.",thank suggestion near future plan generating synthetic data may use custom configuration support,issue,positive,positive,neutral,neutral,positive,positive
713627720,"We will refine the installation guide on this [link](https://epistasislab.github.io/tpot/installing/) for support dask-ml v1.7, which require scikit-learn >= 0.23 and distributed > 2.*

So you also need update distributed and scikit-learn for using dask in colab, please check this [demo](https://colab.research.google.com/gist/weixuanfu/7e58b6120929a10a53f034cfb2608e85/tpot_dask_check_colab.ipynb). It is noted that you may need restart runtime (not reset) after those `pip install` commends.

",refine installation guide link support require distributed also need update distributed please check noted may need restart reset pip install,issue,positive,neutral,neutral,neutral,neutral,neutral
713029894,Thank you! We will test TPOT with cuML 0.16 stable version and then make a release later this month if there is no further major issue. ,thank test stable version make release later month major issue,issue,positive,positive,neutral,neutral,positive,positive
711053528,"!pip install dask_ml solved this issue. However, when running this code on the real dataset, I'm getting another error:

!pip install tpot
!pip install dask_ml
!pip install dask
!pip install dask-ml

import tpot
from tpot import TPOTRegressor
import pandas as pd
import os
import joblib
from sklearn.model_selection import train_test_split

from dask.distributed import Client
client = Client(n_workers=25, threads_per_worker=2)

from google.colab import drive 
drive.mount('/content/gdrive')
%cd /content/gdrive/My\ Drive/Colab Notebooks

df = pd.read_csv('example.csv', index_col = 0)

x_train, x_test, y_train, y_test = train_test_split(df.iloc[:, 1:], df.iloc[:, 0], train_size=0.8,
                                                    test_size=0.2, random_state=5)

tpot_model = TPOTRegressor(generations = 3, population_size = 50, cv=5, random_state=7, verbosity=2, scoring='neg_mean_absolute_error',
                               periodic_checkpoint_folder=os.getcwd() + '/', n_jobs = -1, use_dask = True)

with joblib.parallel_backend(""dask""):
    tpot_model.fit(x_train, y_train)

This is the error:
---------------------------------------------------------------------------
ContextualVersionConflict                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tpot/gp_deap.py in _wrapped_cross_val_score(sklearn_pipeline, features, target, cv, scoring_function, sample_weight, groups, use_dask)
    412         try:
--> 413             import dask_ml.model_selection  # noqa
    414             import dask  # noqa

14 frames
ContextualVersionConflict: (scikit-learn 0.22.2.post1 (/usr/local/lib/python3.6/dist-packages), Requirement.parse('scikit-learn>=0.23'), {'dask-ml'})

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
ImportError: 'use_dask' requires the optional dask and dask-ml depedencies.
(scikit-learn 0.22.2.post1 (/usr/local/lib/python3.6/dist-packages), Requirement.parse('scikit-learn>=0.23'), {'dask-ml'})

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tpot/base.py in _update_top_pipeline(self)
    827             # If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.
    828             # need raise RuntimeError because no pipeline has been optimized
--> 829             raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
    830 
    831     def _summary_of_best_pipeline(self, features, target):

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
",pip install issue however running code real getting another error pip install pip install pip install pip install import import import import o import import import client client client import drive true error recent call last target try import import post handling exception another exception recent call last optional post handling exception another exception recent call last self user initial generation yet need raise pipeline raise pipeline yet please call fit first self target pipeline yet please call fit first,issue,positive,positive,positive,positive,positive,positive
710201362,"I have a quick look. I think `final_features` only has 1 feature but the model was fitted with 2 features (`X.shape=(18,2)`), which make the prediction did not work. You can add a line like `print(final_features.shape)` before `prediction = model.predict(final_features)` to check that.",quick look think feature model fitted make prediction work add line like print prediction check,issue,negative,positive,positive,positive,positive,positive
710110703,@weixuanfu this is the link to my Colab notebook: https://colab.research.google.com/drive/1jVRhIZEV8rjdsQFPvWJof9R7wJcdF-cd?usp=sharing.  I feel there's some issue with the pickle file that's why the Flask app is not working.,link notebook feel issue pickle file flask working,issue,negative,neutral,neutral,neutral,neutral,neutral
710052024,@RafeyIqbalRahman I cannot check your Colab notebook (maybe permission issue?) and am not sure why Flask app is not working. Could you please provide a demo for reproducing the issue of pickling `tpot.fitted_pipeline_`?,check notebook maybe permission issue sure flask working could please provide issue,issue,positive,positive,positive,positive,positive,positive
710031437,"@fcoppey not yet, we will release it later this Month.",yet release later month,issue,negative,neutral,neutral,neutral,neutral,neutral
709832600,"> @hanshupe thank you for answer the question herein.
> @RafeyIqbalRahman I think the `y`'s shape should `(18, )` instead of `(18, 1)` if `y` is a 1-D array.

I reshaped y using .flatten but still the Flask app is not showing the predicted result.",thank answer question herein think shape instead array still flask showing result,issue,negative,neutral,neutral,neutral,neutral,neutral
709321668,"I fixed it via PR #1129  and it will be merged to development branch soon. And It will be included in next release of TPOT later this month.

For testing the development branch, you may install TPOT with patch into your environment via:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```
",fixed via development branch soon included next release later month testing development branch may install patch environment via pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
709318122,"Thank you for reporting this issue.

Since there are codes (see [here](https://github.com/EpistasisLab/tpot/blob/219f8c5abe43996abb2c19d6a1767083304a23d3/tpot/tpot.py#L63-L66)) to ensure that there is a least one example from each class and I think it is safe to delete that parameter to avoid this issue.
",thank issue since see ensure least one example class think safe delete parameter avoid issue,issue,positive,positive,neutral,neutral,positive,positive
709309870,"I tried it again and it works now, not sure what the problem was.",tried work sure problem,issue,negative,positive,positive,positive,positive,positive
708568772,"Hmm, I tested the config in colab (see the notebook below) and the error cannot be reproduced. Could you please share a demo for reproducing this issue?

[Colab notebook](https://colab.research.google.com/gist/weixuanfu/11ae6ce0be6f42739a96bd93a1bf2e8d/pyearth.ipynb) (edited)
",tested see notebook error could please share issue notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
708555486,"@hanshupe thank you for answer the question herein. 
@RafeyIqbalRahman  I think the `y`'s shape  should `(18, )` instead of `(18, 1)` if `y` is a 1-D array.",thank answer question herein think shape instead array,issue,negative,neutral,neutral,neutral,neutral,neutral
707261850,"As explained, pickling is possible with tpot, also you don't have a multi label problem here, you just have to shape your data correctly.",possible also label problem shape data correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
706765562,Btw. I recommend that you secure your notebook again to not get security issues.,recommend secure notebook get security,issue,positive,positive,positive,positive,positive,positive
706760706,"No, I have a single variable at the output. The shape of X is (18, 2) and that of y is (18, 1).",single variable output shape,issue,negative,negative,neutral,neutral,negative,negative
706760317,Do you have multiple variables at output? If not then it must work. Just check the shapes of your input X and your target y.,multiple output must work check input target,issue,negative,neutral,neutral,neutral,neutral,neutral
706759606,"Also, as seen [here](https://stackoverflow.com/questions/55309176/how-to-correct-numpy-and-tpot-array-shapes-error), TPOT lacks multi-label regression ability.",also seen regression ability,issue,negative,neutral,neutral,neutral,neutral,neutral
706758357,just print the type and shape of your input. There are many threads on stackoverflow how you can convert it into 2D.,print type shape input many convert,issue,negative,positive,positive,positive,positive,positive
706757984,"This doesn't work. Also, I'm getting an error message saying that the features should be 2D while the target should be 1D.",work also getting error message saying target,issue,negative,neutral,neutral,neutral,neutral,neutral
706754596,Can you show a practical example of reshaping the data? Or can you please do the same in the notebook's link so I can get an idea?,show practical example data please notebook link get idea,issue,negative,neutral,neutral,neutral,neutral,neutral
706752837,"Like said above, the shape of your input data must match exactly the shape during training. You pass a list, so it's just 1 column with two rows. You need to do something like np.column_stack((a, b)), but there are different ways.",like said shape input data must match exactly shape training pas list column two need something like different way,issue,positive,positive,positive,positive,positive,positive
706751497,"You are right. Actually, I was checking what is the reason for the error. Turns out that when I did .predict(y), I got the same error as I mentioned above. However, in the Flask code, I'm feeding X and still getting the same error as I got in .predict(y).",right actually reason error turn got error however flask code feeding still getting error got,issue,negative,positive,positive,positive,positive,positive
706749611,"I called the pipeline using pickle.load. The shape of the data is 18x3. The datatype is float. The link is accessible now.
I'll try pipeline.predict to make predictions.",pipeline shape data float link accessible try make,issue,negative,positive,positive,positive,positive,positive
706748550,"How do you call the pipeline when you do predictions and what is the shape and type of your input data.

you can do predictions by executing pipeline.predict(data) where data can be a data frame or  matrix with the same number of columns as you used during training.

(Your link can't be accessed)",call pipeline shape type input data data data data frame matrix number used training link ca,issue,negative,neutral,neutral,neutral,neutral,neutral
706746867,"Thanks for the code. It worked. But when I loaded the model on Flask, I'm getting a strange error. 

`ValueError: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature`

The web app I am creating requires 2 parameters longitude and latitude to show prediction.

Here is the Colab notebook link: https://colab.research.google.com/drive/178OrUuWEigZC-y2A2O83AVDGZdwxIsoW?usp=sharing",thanks code worked loaded model flask getting strange error input operand mismatch core dimension signature web longitude latitude show prediction notebook link,issue,negative,positive,neutral,neutral,positive,positive
706732102,"After fitting your tpot object you just call

pipeline_dump = pickle.dumps(tpot.fitted_pipeline_)
pipeline = pickle.loads(pipeline_dump)
print(pipeline)",fitting object call pipeline print pipeline,issue,negative,positive,positive,positive,positive,positive
706732035,"Another run with log loss TPOT training run gave me this (went close to 0 and once it flipped to a value higher than 0 or 1 - in this case -9.992007221626413e-16), I stopped TPOT:

```
exported_pipeline = make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    VarianceThreshold(threshold=0.001),
    StackingEstimator(estimator=SGDClassifier(alpha=0.01, eta0=0.01, fit_intercept=True, l1_ratio=0.75, learning_rate=""constant"", loss=""squared_hinge"", penalty=""elasticnet"", power_t=0.0)),
    StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=True, criterion=""entropy"", max_features=0.6000000000000001, min_samples_leaf=10, min_samples_split=13, n_estimators=100)),
    StandardScaler(),
    SelectPercentile(score_func=f_classif, percentile=2),
    StackingEstimator(estimator=DecisionTreeClassifier(criterion=""gini"", max_depth=2, min_samples_leaf=1, min_samples_split=6)),
    StackingEstimator(estimator=XGBClassifier(learning_rate=0.001, max_depth=8, min_child_weight=5, n_estimators=100, nthread=1, subsample=0.3)),
    RandomForestClassifier(bootstrap=False, criterion=""gini"", max_features=0.9500000000000001, min_samples_leaf=1, min_samples_split=5, n_estimators=100)
)
```

again this gives me 100% accuracy. Interested to hear your thoughts on this.

I also have a question on the order of the pipeline. 

What was essentially done was that the following:

1) Firsly, preprocessing steps were done: `StandardScaler()`

2) Then the following feature selection steps were done: `SelectPercentile(score_func=f_classif, percentile=2),     VarianceThreshold(threshold=0.001)`

3) Then the following stacking estimators were used: 
```
StackingEstimator(estimator=SGDClassifier(alpha=0.01, eta0=0.01, fit_intercept=True, l1_ratio=0.75, learning_rate=""constant"", loss=""squared_hinge"", penalty=""elasticnet"", power_t=0.0)),
 StackingEstimator(estimator=ExtraTreesClassifier(bootstrap=True, criterion=""entropy"", max_features=0.6000000000000001,  StackingEstimator(estimator=DecisionTreeClassifier(criterion=""gini"", max_depth=2, min_samples_leaf=1, min_samples_split=6)), StackingEstimator(estimator=XGBClassifier(learning_rate=0.001, max_depth=8, min_child_weight=5, n_estimators=100, nthread=1, subsample=0.3))
```

4) Based on the predictions of the 4 estimators, RandomForest `(RandomForestClassifier(bootstrap=False, criterion=""gini"", max_features=0.9500000000000001, min_samples_leaf=1, min_samples_split=5, n_estimators=100)` was used to make the final classifier prediction decision.

Is this correct?

",another run log loss training run gave went close value higher case stopped copy copy constant entropy accuracy interested hear also question order pipeline essentially done following done following feature selection done following used constant entropy based used make final classifier prediction decision correct,issue,negative,positive,neutral,neutral,positive,positive
706729593,"> Where did you get the error? I tried it and it worked.

Can you show the code that works?",get error tried worked show code work,issue,negative,neutral,neutral,neutral,neutral,neutral
706721771,Where did you get the error? I tried it and it worked.,get error tried worked,issue,negative,neutral,neutral,neutral,neutral,neutral
706720130,"> I guess you can just pickle the final pipeline object (tpot.fitted_pipeline_), which is a normal sklearn pipeline and can be used independently from tpot.

I tried but doing that returned an AttributeError.",guess pickle final pipeline object normal pipeline used independently tried returned,issue,negative,positive,neutral,neutral,positive,positive
706697895,"I guess you can just pickle the final pipeline object (tpot.fitted_pipeline_), which is a normal sklearn pipeline and can be used independently from tpot.",guess pickle final pipeline object normal pipeline used independently,issue,negative,positive,neutral,neutral,positive,positive
706567330,"There is definitely something different with Log loss that makes it better or makes it seem better. Training TPOT with the accuracy metric gives me a max accuracy score of 95%. Training TPOT with log loss gives me an accuracy score of 100%, whether that is accuracy or balanced accuracy, or f1_weighted score. Training with log_loss gives me a more complex pipeline for example it has given me the following pipeline:

```
# Average CV score on the training set was: -9.992007221626413e-16 <- explanation for this number after the code)
exported_pipeline = make_pipeline(
    RobustScaler(),
    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.5, max_depth=9, max_features=0.8500000000000001, min_samples_leaf=1, min_samples_split=3, n_estimators=100, subsample=0.6000000000000001)),
    StackingEstimator(estimator=LinearSVC(C=10.0, dual=True, loss=""hinge"", penalty=""l2"", tol=1e-05)),
    StackingEstimator(estimator=LinearSVC(C=10.0, dual=True, loss=""squared_hinge"", penalty=""l2"", tol=0.001)),
    VarianceThreshold(threshold=0.1),
    OneHotEncoder(minimum_fraction=0.05, sparse=False, threshold=10),
    Normalizer(norm=""max""),
    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.5, max_depth=9, max_features=0.8500000000000001, min_samples_leaf=1, min_samples_split=3, n_estimators=100, subsample=0.6000000000000001)),
    StackingEstimator(estimator=LinearSVC(C=0.5, dual=True, loss=""squared_hinge"", penalty=""l2"", tol=0.001)),
    VarianceThreshold(threshold=0.05),
    RandomForestClassifier(bootstrap=False, criterion=""gini"", max_features=0.7500000000000001, min_samples_leaf=1, min_samples_split=7, n_estimators=100)
)
```

The only weird thing is that the number is negative. Log loss numbers should be positive with the lowest number being 0 (best) and 1 being the highest (worst). Generation 1 was -.42 and once it reached -.013, the subsequent generation gave a log loss score of -9.992007221626413e-16 instead of stopping at 0 (which should be the best log loss score possible). 

What do you make of all of this? Is log_loss perhaps way too overfitting? Have I stumbled upon the discovery of the century? I don't know LOL",definitely something different log loss better seem better training accuracy metric accuracy score training log loss accuracy score whether accuracy balanced accuracy score training complex pipeline example given following pipeline average score training set explanation number code hinge normalizer weird thing number negative log loss positive number best highest worst generation subsequent generation gave log loss score instead stopping best log loss score possible make perhaps way upon discovery century know,issue,negative,positive,neutral,neutral,positive,positive
705763837,"cuML 0.16 is now planned for an October 21st release, a delay of 7 days.",st release delay day,issue,negative,neutral,neutral,neutral,neutral,neutral
705647455,"Sorry, I am not familiar with log loss so I am not sure whether it is logic for this case. It also depends on your study objectives. If f1 score is more important for your study, then you should use f1 score. ",sorry familiar log loss sure whether logic case also study score important study use score,issue,negative,positive,positive,positive,positive,positive
705634943,"> You can simply use accuracy or f1 score on all of 20 samples that used in TPOT. It is noted that it is a training score instead of holdout/test score. I think it is hard to make train/test splits for your case.

Yes, I agree train_test_split is definitely not good in my case, I realised this the day I posted this thread... I've been using the whole dataset (X,y) with LOOCV for a few days now. Am I right to think that using TPOT with log loss has a higher chance of yielding a better accuracy score (whether that is accuracy or f1) or is this faulty logic and I should just train TPOT with accuracy/f1 from the get-go?

",simply use accuracy score used noted training score instead score think hard make case yes agree definitely good case day posted thread whole day right think log loss higher chance yielding better accuracy score whether accuracy faulty logic train,issue,positive,positive,positive,positive,positive,positive
705604262,You can simply use accuracy or f1 score on all of 20 samples that used in TPOT. It is noted that it is a training score instead of holdout/test score. I think it is hard to make train/test splits for your case. ,simply use accuracy score used noted training score instead score think hard make case,issue,negative,negative,negative,negative,negative,negative
705599530,"@weixuanfu Great, I will run TPOT with it after log loss run finishes. In your opinion which accuracy metric would you use in my case? Just to reiterate - samples = 20, features = 263, the target is binary and one class is higher than the other (12 vs 8). Again thank you for all your help.",great run log loss run opinion accuracy metric would use case reiterate target binary one class higher thank help,issue,positive,positive,positive,positive,positive,positive
705561957,"
> 
> How would I go about implementing it? Is it this:
> 
> ```
> #############matthews corr coef #############################################
> # # Make a custom metric 
> from sklearn.metrics import matthews_corrcoef
> my_matthews_corrcoef_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
> ######################################################################################
> ```
> 
> ??

Looks good.

`balanced_accuracy` may not work well in your case that the sample size is very small.",would go make custom metric import good may work well case sample size small,issue,positive,positive,positive,positive,positive,positive
705501966,"Well I got a low balanced_accuracy score from a log_loss of 0.053 and then I realised that I included the `X, y = make_classification(n_samples=20, n_features=263, random_state=16)` that you posted above so I basically overwrote my database with a randomly generated database lol. Re-running again...

One last thing. I also want to try running Matthews correlation coefficient in TPOT.

Apparently its better than an f1 score for binary classification?
https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7

How would I go about implementing it? Is it this:

```
#############matthews corr coef #############################################
# # Make a custom metric 
from sklearn.metrics import matthews_corrcoef
my_matthews_corrcoef_scorer = make_scorer(matthews_corrcoef, greater_is_better=True)
######################################################################################
```

??",well got low score included posted basically randomly one last thing also want try running correlation coefficient apparently better score binary classification would go make custom metric import,issue,negative,neutral,neutral,neutral,neutral,neutral
705150727,"Fu you are a rockstar! Thanks, it works great now. The number I am getting through the generations is minus (e.g., -0.15) but as it works through the generations it goes closer to 0 which I think that means it is working fine. I'm still running the code and its getting closer to 0. Will report again when I've made enough algorithmic babies reach no further improvement",fu thanks work great number getting minus work go closer think working fine still running code getting closer report made enough algorithmic reach improvement,issue,positive,positive,positive,positive,positive,positive
705104452,"Sure, in this case, accuracy is just a measure of how often the actual target falls within the predicted range. The predicted range is calculated by subtracting and adding the mean absolute error from the predicted value.",sure case accuracy measure often actual target within range range calculated mean absolute error value,issue,negative,positive,neutral,neutral,positive,positive
705102088,"Thank you for the suggestion. We tried to use Gitter before but it was very quiet for a whole, which is why we stopped using it. I think we can this issues section of the repo for questions and users can easily search them. ",thank suggestion tried use quiet whole stopped think section easily search,issue,negative,positive,positive,positive,positive,positive
705099656,"I did not understand the equations/definition of this custom ""accuracy"" scoring function. Also it is strange to use ""accuracy"" for regression problem. Could you please explain a little more?",understand custom accuracy scoring function also strange use accuracy regression problem could please explain little,issue,negative,negative,negative,negative,negative,negative
705087524,"Actually, my accuracy only depends on minimizing mean absolute error. Both optimizing for r squared and mean absolute error give the same mean absolute error, so I don't know that optimizing a custom scoring function would give better results.",actually accuracy mean absolute error squared mean absolute error give mean absolute error know custom scoring function would give better,issue,negative,positive,neutral,neutral,positive,positive
705081991,Sometimes running more data is too slow.,sometimes running data slow,issue,negative,negative,negative,negative,negative,negative
705081119,"Thanks, I will try that, but why would subsampling help? Surely more data is better?",thanks try would help surely data better,issue,positive,positive,positive,positive,positive,positive
705074035,"I am not sure about this question/issue. It seems that you used r2 in TPOT and got a good test r2 score but the custom ""accuracy"" score is not good. Why not using the custom ""accuracy"" in TPOT? Check this [docs](https://epistasislab.github.io/tpot/using/#scoring-functions)



> I didn't read this in the docs, but are we meant to shuffle before? Also, what specifically does subsample do? I know it randomly selects subset, but does it ignore the rest?

Yes, it should ignore the rest. For example, setting subsample=0.5 tells TPOT to use a random subsample of half of the training data. This subsample will remain the same during the entire pipeline optimization process. ",sure used got good test score custom accuracy score good custom accuracy check read meant shuffle also specifically subsample know randomly subset ignore rest yes ignore rest example setting use random subsample half training data subsample remain entire pipeline optimization process,issue,positive,positive,positive,positive,positive,positive
705055294,"I calculate average test mean absolute error with 10 fold cross validation, and the error standard deviation is very low.",calculate average test mean absolute error fold cross validation error standard deviation low,issue,negative,negative,neutral,neutral,negative,negative
704754359,I encountered the same problem. I used GridSearchCV and gave a random array of possible increased number of iterations and according adjusted my array. Finally at some point it converged at a value.,problem used gave random array possible number according array finally point value,issue,negative,negative,negative,negative,negative,negative
704424995,"```python
from tpot import TPOTClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import LeaveOneOut
from sklearn.metrics import log_loss, make_scorer
import numpy as np
X, y = make_classification(n_samples=20, n_features=10, random_state=16)

loocv = LeaveOneOut()

my_neg_log_loss_scorer = make_scorer(log_loss, greater_is_better=False,
                                  needs_proba=True, labels=np.unique(y))
  
tpot = TPOTClassifier(generations=500, population_size=100, verbosity=2, random_state=16, 
                early_stop = 50, cv = loocv, scoring =my_neg_log_loss_scorer, n_jobs=-1)
tpot.fit(X, y)
print(tpot.score(X_test, y_test))
```

You need use `labels` parameter in `log_loss` scoring function when using LOOCV since only one sample is in test set in each splits of LOOCV. Please check the demo above.",python import import import import import scoring print need use parameter scoring function since one sample test set please check,issue,negative,neutral,neutral,neutral,neutral,neutral
704421649,Thank you for the update. We will release a new version of TPOT with this PR right after cuML 0.16 release. ,thank update release new version right release,issue,negative,positive,positive,positive,positive,positive
704415938,"As a quick update, cuML 0.16 is on target for the planned October 14th release 😄 ",quick update target th release,issue,negative,positive,positive,positive,positive,positive
704402282,"I found the issue, neg_log_loss doesn't work with LOOCV. Not 100% sure why that is. ",found issue work sure,issue,negative,positive,positive,positive,positive,positive
704390354,"Thank you for looking at this. I'm putting my code first to see whether something jumps out.

```
import pandas as pd
import numpy as np
from tpot import TPOTClassifier
import time

tic = time.perf_counter()
dataset = pd.read_csv('D:/data.csv')
dataset_list = list(dataset.columns)
dataset= dataset.drop([('Participant'), ('AAT_1'),  ('AAT_DB')], axis = 1)
dataset_list = list(dataset.columns)


AAT_GROUP_2_dict = {'LOW':1,
                        'HIGH':2}

dataset['AAT_GROUP_2'] = dataset.AAT_GROUP_2.map(AAT_GROUP_2_dict)

y = np.array(dataset[('AAT_GROUP_2')]) 
X = pd.get_dummies(dataset)
X = X.drop([('AAT_GROUP_2')], axis = 1)
X = np.array(X)



# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.80, test_size = 0.20, random_state = 16)



from sklearn.model_selection import LeaveOneOut
loocv = LeaveOneOut()

#log loss requirement##############################
greater_is_better=False
##################################################



tpot = TPOTClassifier(generations=500, population_size=100, verbosity=2, random_state=16, early_stop = 50, cv = loocv, scoring ='neg_log_loss', n_jobs= -1)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))

tpot.export('tpot_GROUP_2_pipeline.py')
toc = time.perf_counter()

print(f""Finished running the code in {toc - tic:0.4f} seconds"")
```",thank looking code first see whether something import import import import time tic list axis list axis splitting training set test set import import log loss requirement scoring print print finished running code tic,issue,negative,positive,positive,positive,positive,positive
704228022,Could you please provide your codes with a example data for reproducing this issue?,could please provide example data issue,issue,negative,neutral,neutral,neutral,neutral,neutral
704157070,How would one go about calling neg_log_loss? I've put it in `scoring='neg_log_loss'` and I declared `greater_is_better=False` but I still get the same error,would one go calling put declared still get error,issue,negative,neutral,neutral,neutral,neutral,neutral
703774004,"Thank you for reporting this bug. I fixed it via PR #1122 and it will be merged to development branch soon. And It will be included in next release of TPOT in middle of Oct.

For testing the development branch, you may install TPOT with patch into your environment via:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```

",thank bug fixed via development branch soon included next release middle testing development branch may install patch environment via pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
703737903,"Below is a simple example to reproduce it. Additionally in base.py as last line in the ""_random_mutation_operator"" function I added:

> print(""\nBef. mutation"", str(individual), ""\nAft. mutation"", str(offspring))

While it does mutate the Selector Primitive it never mutates the Transformer or the Regressor step. It does mutate the hyperparameters / Terminals of those two but not the Primitives:

- > Bef. mutation **LinearSVR(Nystroem**(SelectPercentile(input_matrix, SelectPercentile__percentile=97), Nystroem__gamma=0.8, Nystroem__kernel=additive_chi2, Nystroem__n_components=3), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.01, LinearSVR__loss=epsilon_insensitive, LinearSVR__tol=0.1) 
- > Aft. mutation **LinearSVR(Nystroem**(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.01), Nystroem__gamma=0.8, Nystroem__kernel=additive_chi2, Nystroem__n_components=6), LinearSVR__C=15.0, LinearSVR__dual=True, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.1)


```
import pandas as pd
import numpy as np
import tpot
import random
seed = 42

random.seed(a=seed)
input = pd.DataFrame(np.random.uniform(0,100,size=(25, 3)), columns=list('ABC'))
target = np.random.uniform(0,100, size = 25)

tpot = tpot.TPOTRegressor(template='Selector-Transformer-Regressor',
                                             max_time_mins=60,
                                             cv=5,
                                             n_jobs=1, generations=10000,
                                             population_size=100, mutation_rate=0.9,
                                             crossover_rate=0.1,
                                             verbosity=3, max_eval_time_mins=5,
                                             random_state=seed,
                                             scoring=""r2"", subsample=1,
                                             early_stop=None,
                                             config_dict='TPOT light')

tpot.fit(input, target)
```",simple example reproduce additionally last line function added print mutation individual mutation offspring mutate selector primitive never transformer regressor step mutate two mutation aft mutation import import import import random seed input target size light input target,issue,negative,negative,neutral,neutral,negative,negative
703687216,"> If I just call it from the default TPOT installation then greater_is_better will be set automatically to false yes?
No, it should be set to False manually before using it in TPOT.


> Am I right to think that Cohen Kappa and Matthews correlation coefficient is not included in TPOT?

Yes, both metrics was not included for simplify usage via a string in TPOT. You can use `make_scorer` with metric function from scikit-learn to generate the scorer and then use it in TPOT.",call default installation set automatically false yes set false manually right think kappa correlation coefficient included yes metric included simplify usage via string use metric function generate scorer use,issue,positive,negative,negative,negative,negative,negative
703682781,"Thank you @weixuanfu ,I didn't realise log loss is already included, great! If I just call it from the default TPOT installation then greater_is_better will be set automatically to false yes? I think I'm going to go with balanced_accuracy first as it looks good for binary classification and then log loss and see which one gives better accuracy. Am I right to think that Cohen Kappa and Matthews correlation coefficient is not included in TPOT?",thank log loss already included great call default installation set automatically false yes think going go first good binary classification log loss see one better accuracy right think kappa correlation coefficient included,issue,positive,positive,positive,positive,positive,positive
703671336,"Could you please provide a demo for reproducing this issue?

FYI, TPOT only uses [point mutation function](https://github.com/EpistasisLab/tpot/blob/v0.11.5/tpot/gp_deap.py#L313) when using `template` to randomly generate fixed length pipeline and the point mutation should randomly mutate a Primitive (a step, like mutating SGDRegressor to RandomForestRegressor) or a Terminal (a hyperparameter of a step).

Based on the limited population strings in this issue, I found that hyperparamters of SGDRegressor are different among some individuals, which may means that the point mutation happened on the regressor step's hyperparameter. 

Also, increasing `population_size` may avoid the local optimization issue. 
",could please provide issue point mutation function template randomly generate fixed length pipeline point mutation randomly mutate primitive step like terminal step based limited population issue found different among may point mutation regressor step also increasing may avoid local optimization issue,issue,positive,negative,negative,negative,negative,negative
703657082,"You may use `scoring=""neg_log_loss""` for using Log Loss in TPOT (check this [link](https://scikit-learn.org/stable/modules/model_evaluation.html#the-scoring-parameter-defining-model-evaluation-rules)). BTW, this metric should be  `greater_is_better=False` when using `make_scorer` function.

Also, I suggested to use `scoring=""balanced_accuracy""` (check this [link](https://github.com/EpistasisLab/tpot/blob/master/tpot/metrics.py#L30))",may use log loss check link metric function also use check link,issue,negative,neutral,neutral,neutral,neutral,neutral
703246771,"Actually I just read that for small datasets like mine is best to do LOOCV instead of kfold cv. About to complete a 1000 generation, 100 offspring LOOCV run and will post results. ",actually read small like mine best instead complete generation offspring run post,issue,positive,positive,positive,positive,positive,positive
703163809,"
[![Coverage Status](https://coveralls.io/builds/33909186/badge)](https://coveralls.io/builds/33909186)

Coverage remained the same at 96.6% when pulling **5865228a0c4d2e095d519245b1c10a491db63fdb on andife:master** into **219f8c5abe43996abb2c19d6a1767083304a23d3 on EpistasisLab:master**.
",coverage status coverage master master,issue,negative,neutral,neutral,neutral,neutral,neutral
702687247,"Thank you for your help. Makes sense Am I right to think that a good representative accuracy on a SMALL dataset should give a high % x times (e.g., 5) cv kfold accuracy from the test set but also on the whole data set (X and Y). Is this logic sound or should I just do and only look at cv kfold accuracy score on the train set? Some of my runs with TPOT give me 100% accuracy with 5 kfold cross-validation on the training set but when I run a 5kfold CV on the whole dataset I get 60% accuracy. Some other runs give me a lower 5 kfold cv accuracy on the test set (e.g., 93%) but give me a much higher 5 kfold cv accuracy on the whole set (85%). I feel like I trust the ML pipeline with a lower cv score on the test set if it also provides a higher cv score on the whole dataset. Is this reasoning correct?


Just for some context my dataset has 20 samples with 132 features and it is a binary classification problem (EEG data from a sleep study) trained on .80 of the data. I wish I could collect more samples but corona is making it impossible!

By the way, I've been using PPS on my data first to reduce the number of features and then I apply TPOT and I get much higher scores (+5-10% accuracy) than running TPOT alone. 

https://github.com/8080labs/ppscore 
https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598)

Would be amazing if this was added to TPOT. Maybe have a go on your data to see for yourself if it improves accuracy. Would be very curious to see.



",thank help sense right think good representative accuracy small give high time accuracy test set also whole data set logic sound look accuracy score train set give accuracy training set run whole get accuracy give lower accuracy test set give much higher accuracy whole set feel like trust pipeline lower score test set also higher score whole reasoning correct context binary classification problem data sleep study trained data wish could collect corona making impossible way data first reduce number apply get much higher accuracy running alone would amazing added maybe go data see accuracy would curious see,issue,positive,positive,positive,positive,positive,positive
702159305,"Yes thanks I saw the compile method, but the conversion into the opposite direction is not so easy as I thought :)",yes thanks saw compile method conversion opposite direction easy thought,issue,positive,positive,positive,positive,positive,positive
702149846,@hanshupe we don't have a example for this conversion . But we have [a example for reverse conversion](https://github.com/EpistasisLab/tpot/blob/master/tests/tpot_tests.py#L358-L370).,example conversion example reverse conversion,issue,negative,neutral,neutral,neutral,neutral,neutral
701242109,"> I think it was refereed to [this line](https://github.com/EpistasisLab/tpot/blob/v0.11.5/tpot/base.py#L670)

Can you give me a hint how I can convert a predefined sklearn pipeline into a list of DEAP primitives that is needed for the population?",think line give hint convert pipeline list population,issue,negative,neutral,neutral,neutral,neutral,neutral
700747387,"> This can currently be ""hacked"" into TPOT by modifying the [population instantiation procedure](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L296), but it could be a nice feature to be able to do that through the regular TPOT interface. We'd have to put a lot of thought into how that would look. I'll file this as an enhancement request for now.

I would like to use the ""hack"", but I think the link is outdated. Could someone please update the Line number, where I can add the initial solutions? Thx!",currently hacked population procedure could nice feature able regular interface put lot thought would look file enhancement request would like use hack think link outdated could someone please update line number add initial,issue,positive,positive,positive,positive,positive,positive
699637502,Also another question: is the cross-validation done on the selected training dataset or the whole dataset?,also another question done selected training whole,issue,negative,positive,positive,positive,positive,positive
699636015,"From what I see it doesn't use catboost. Also it would be great if Microsoft's LightGBM was added to TPOT

https://towardsdatascience.com/boosting-showdown-scikit-learn-vs-xgboost-vs-lightgbm-vs-catboost-in-sentiment-classification-f7c7f46fd956
https://lightgbm.readthedocs.io/en/latest/Python-Intro.html

1) Either way, is it easy to add new algorithms for TPOT to test? If yes, could you point me where/how to add them?
2) Could I also have a complete list of the ML algorithms TPOT tests (preprocessing,classification/regression etc)?

Thank you for your help in the matter",see use also would great added either way easy add new test yes could point add could also complete list thank help matter,issue,positive,positive,positive,positive,positive,positive
698959280,"I tested it also without setting a seed, assuming pipelines in different folds use also different seeds, but I am not sure if this would fix the problem. 

Training every pipeline multiple times to check for stability may be not a practical approach due to the computational costs. Instead all pipelines could be retrained in each generation even when a previous score already exists to filter out pipelines which score changed above a certain threshold.

I wonder if other TPOT/AutoML users experienced such problems with such unstable models that performance depend a lot on the random seed?


",tested also without setting seed assuming different use also different sure would fix problem training every pipeline multiple time check stability may practical approach due computational instead could generation even previous score already filter score certain threshold wonder experienced unstable performance depend lot random seed,issue,negative,positive,neutral,neutral,positive,positive
698901686,Thank you for submitting this issue and providing suggestions. TPOT uses average CV scores to evaluate pipelines but all the operators in the pipelines should have a fixed random seed. I think that maybe using different random seeds in N-fold CV could help.,thank issue providing average evaluate fixed random seed think maybe different random could help,issue,positive,negative,negative,negative,negative,negative
697381365,"Here it is:
```python
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25, random_state=42)
tpot = TPOTClassifier(generations=1, population_size=2, verbosity=2, random_state=42, warm_start=True)
# print holdout score every generation or you may reset `generations=1` above for every N generations. 
for _ in range(0, 5): 
    tpot.fit(X_train, y_train)
    print(tpot.score(X_test, y_test))
```

",python import import import import iris print holdout score every generation may reset every range print,issue,negative,neutral,neutral,neutral,neutral,neutral
695003203,"> `evaluated_individuals_` has a few information (like `mean_train_score`) in the `cv_results_` of [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) but I am not sure if it is necessary to include all those information into TPOT API. If so, I think we need replace `evaluated_individuals_`.

It would be great to have TPOT return  cv_results_ . I'm working on a visualization toolkit to compare results from different AutoML tools such as auto-sklearn and TPOT. Generally speaking, that would give TPOT more compatibility with  scikit-learn",information like sure necessary include information think need replace would great return working visualization compare different generally speaking would give compatibility,issue,positive,positive,positive,positive,positive,positive
694569190,"`evaluated_individuals_` has a few information (like `mean_train_score`) in the `cv_results_` of [sklearn.model_selection.GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) but I am not sure if it is necessary to include all those information into TPOT API. If so, I think we need replace `evaluated_individuals_`.",information like sure necessary include information think need replace,issue,positive,positive,positive,positive,positive,positive
694263232,"[xgboost](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py#L101-L108) is in the configuration, TPOT should use it if it is installed in your environment.  

This is [the link of xgboost in regression config](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py#L98-L106)",configuration use environment link regression,issue,negative,neutral,neutral,neutral,neutral,neutral
694259027,"Thanks a lot for your timely response!
As I see it, TPOT has sklearn's 'GradientBoostingRegressor' in its model search space, but not XGBoost. Although the two algorithms are essentially based on the same principle; from my experience, XGBoost provides more memory-efficiency than its sklearn counterpart. Also, it is generally higher when it comes to prediction accuracy.

So my question is why is XGBoost left out of the model search space? If there's a particular reason behind this choice, kindly share it here. I would be glad to know if I have an incomplete knowledge of the picture here. 

Thanks a lot for your time!",thanks lot timely response see model search space although two essentially based principle experience counterpart also generally higher come prediction accuracy question left model search space particular reason behind choice kindly share would glad know incomplete knowledge picture thanks lot time,issue,positive,positive,positive,positive,positive,positive
694226754,Please check this [table](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) for default configurations of TPOT.,please check table default,issue,negative,neutral,neutral,neutral,neutral,neutral
692759284,Nice! Thank you for the info. I will test the new feature and prepare a new TPOT release in Oct.  ,nice thank test new feature prepare new release,issue,positive,positive,positive,positive,positive,positive
692758288,"Sounds good! Please do let me know if there's anything I can do to help out.

The 0.16 release is currently scheduled for Wednesday, October 14th ([release timeline](https://docs.rapids.ai/maintainers)). If there are any changes to that plan, I'll make sure to ping you as well.",good please let know anything help release currently th release plan make sure ping well,issue,positive,positive,positive,positive,positive,positive
692756516,"@beckernick this minor won't block us to merge this PR. I will merge this one to dev branch soon. We will tested it more before merging dev branch to master branch. Also, do you have any info when the stable version of cuML 0.16 will be released, I hope we have a stable version of cuML before next release of TPOT with this cool feature!",minor wo block u merge merge one dev branch soon tested dev branch master branch also stable version hope stable version next release cool feature,issue,positive,positive,neutral,neutral,positive,positive
692736353,"> The only minor issue is that I got a lot of warning message like `[11:10:10.910600] Expected column ('F') major order, but got the opposite. 

After thinking it through, we've decided to make a change upstream to turn off these messages by default (https://github.com/rapidsai/cuml/pull/2824). I've tested this PR with the upstream PR and can confirm that those ""Expected column..."" messages no longer appear. The PR is approved and ""Ready for Merge"", but slated for merging after https://github.com/rapidsai/cuml/pull/2747 which should happen imminently (also approved).

Would you consider this minor issue non-blocking for this PR, given it will be resolved shortly upstream?

EDIT: As of 6 PM EDT, both of the linked PRs have merged 👍 ",minor issue got lot warning message like column major order got opposite thinking decided make change upstream turn default tested upstream confirm column longer appear ready merge happen imminently also would consider minor issue given resolved shortly upstream edit linked,issue,positive,positive,neutral,neutral,positive,positive
692664020,"Thx, in the meantime I added local search after the tpot optimization and I can confirm that it converges very quickly to a local optimum which was not found by the genetic optimization after many hours. I think this hybrid approach is able to find globally a good solution and then with minimal effort locally a slightly better one by local search.",added local search optimization confirm quickly local optimum found genetic optimization many think hybrid approach able find globally good solution minimal effort locally slightly better one local search,issue,positive,positive,positive,positive,positive,positive
692213280,"> The only minor issue is that I got a lot of warning message like `[11:10:10.910600] Expected column ('F') major order, but got the opposite. 

Ah, thanks for catching this. Just needs a one-line change. cuML currently has verbose logging on by default. I'll update the PR. 

> should the input X, y be converted to `cudf` object?

Today, no. For interface consistency to use the necessary scikit-learn APIs, these conversions need to happen automatically by the cuML methods, rather than beforehand by the user.
",minor issue got lot warning message like column major order got opposite ah thanks catching need change currently verbose logging default update input converted object today interface consistency use necessary need happen automatically rather beforehand user,issue,negative,positive,positive,positive,positive,positive
692186793,"Thanks @beckernick I just tested it in a smaller experiment and it worked well in my environment. The only minor issue is that I got a lot of warning message like `[11:10:10.910600] Expected column ('F') major order, but got the opposite. Converting data, this will result in additional memory utilization.` Do you have any idea to turn it off or should the input X, y be converted to `cudf` object?

",thanks tested smaller experiment worked well environment minor issue got lot warning message like column major order got opposite converting data result additional memory idea turn input converted object,issue,positive,positive,neutral,neutral,positive,positive
692164655,"To test if this PR can make a material impact, I ran some small classification timed experiments (`max_time_mins=60, 120, 240, 480 minutes`) using a population size of 30 on a 500,000 row sample from two reasonably standard ML datasets (Higgs Boson and Airline flights). Actual experiment times vary due to how long it took the run to finish after reaching the max_time_mins threshold (quite long, in one case). I used the conda environment in the `tpot-pr.yml` example above. The goal was to compare this PR with the TPOT default. Please do note that this is only a loose test using dual Intel Xeon Platinum 8168 CPUs and one V100 GPU.

For both datasets, the GPU-accelerated PR configuration achieved higher accuracy in one hour than the default in eight hours. Because this PR provides a restricted configuration set (somewhere in between TPOT-Light and TPOT-Default) with GPU-accelerated estimators, it’s able to evaluate quite a few more individuals per given time with medium-to-large datasets. In these experiments, this allowed TPOT to find a higher accuracy pipeline much faster. The graphs below provide a brief summary.

![tpot-higgs-accuracy-actual-time-hours](https://user-images.githubusercontent.com/8457388/93109359-f7fd2d00-f681-11ea-82c2-93d5b3fcca25.png)
![tpot-higgs-num-pipelines-actual-time-hours](https://user-images.githubusercontent.com/8457388/93109370-faf81d80-f681-11ea-9b5c-d50e28098da9.png)

![tpot-airline-accuracy-actual-time-hours](https://user-images.githubusercontent.com/8457388/93109434-0a776680-f682-11ea-9e1a-f62f3cf750ce.png)
![tpot-airline-num-pipelines-actual-time-hours](https://user-images.githubusercontent.com/8457388/93109387-ff243b00-f681-11ea-8a44-411caf0f1301.png)


As a concrete example, in the eight hour airlines dataset experiment, the final default pipeline achieved 87.2% cross-validation accuracy. The final PR pipeline achieved 88.5% accuracy, a 1.3% increase. In line with expectations, the `fitted_pipeline` is quite a bit more complex in the PR result, highlighting the impact of evaluating more pipelines.

TPOT Default Final Pipeline achieving 87.2% accuracy (8 Hour Experiment):
```
Pipeline(steps=[('extratreesclassifier',
                 ExtraTreesClassifier(bootstrap=True,
                                      max_features=0.9500000000000001,
                                      min_samples_split=11, random_state=12))])
```

TPOT PR Final Pipeline achieving 88.5% accuracy (8 Hour Experiment):
```
Pipeline(steps=[('zerocount-1', ZeroCount()),
                ('variancethreshold', VarianceThreshold(threshold=0.01)),
                ('selectpercentile', SelectPercentile(percentile=43)),
                ('pca',
                 PCA(iterated_power=5, random_state=12,
                     svd_solver='randomized')),
                ('zerocount-2', ZeroCount()),
                ('xgbclassifier',
                 XGBClassifier(alpha=1, base_score=0.5, booster='gbtree',
                               colsample_bylevel=1, colsampl...
                               importance_type='gain',
                               interaction_constraints='', learning_rate=0.5,
                               max_delta_step=0, max_depth=9,
                               min_child_weight=3, missing=nan,
                               monotone_constraints='(0,0,0,0,0,0,0,0)',
                               n_estimators=100, n_jobs=1, nthread=1,
                               num_parallel_tree=1, random_state=12,
                               reg_alpha=1, reg_lambda=1, scale_pos_weight=1,
                               subsample=1.0, tree_method='gpu_hist',
                               validate_parameters=1, verbosity=None))])
```

I’ve included the experiment code I used in these two gists below:

[tpot-benchmark.py](https://gist.github.com/beckernick/22e0c117a96832ea2afab05484e30580)
[tpot-benchmark-config.yml](https://gist.github.com/beckernick/a028073af983925f0d8db87fffc59022)",test make material impact ran small classification timed population size row sample two reasonably standard actual experiment time vary due long took run finish reaching threshold quite long one case used environment example goal compare default please note loose test dual platinum one configuration higher accuracy one hour default eight restricted configuration set somewhere able evaluate quite per given time find higher accuracy pipeline much faster provide brief summary concrete example eight hour experiment final default pipeline accuracy final pipeline accuracy increase line quite bit complex result impact default final pipeline accuracy hour experiment pipeline final pipeline accuracy hour experiment pipeline included experiment code used two,issue,negative,positive,neutral,neutral,positive,positive
692082827,I think this issue is related to #318 and #319. The point mutation out  of 3 mutation operators will cover part of local search but indeed it can also replace one operator with its parameters. Maybe we should provide choices of different mutation operators for applying one or more in TPOT's GP progress. ,think issue related point mutation mutation cover part local search indeed also replace one operator maybe provide different mutation one progress,issue,negative,neutral,neutral,neutral,neutral,neutral
692078240,I think current TPOT may not support this kind of application. But I think it need adding the [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) from scikit-learn into TPOT.,think current may support kind application think need,issue,positive,positive,positive,positive,positive,positive
689720953,"> 
> 
> Is this the closest pull to multi-output? any plans to finish it?

sorry, no time for this right now..  feel free to work with my branch. i think the last todo is cloning the multioutputclassifier for every estimator that only supports single outputs as per weixuanfu's review.",pull finish sorry time right feel free work branch think last every estimator single per review,issue,negative,positive,neutral,neutral,positive,positive
687608279,This pull request https://github.com/EpistasisLab/tpot/pull/1071 seems to have satisfied this [though with PyTorch](http://epistasislab.github.io/tpot/using/#neural-networks-in-tpot-tpotnn). Trying it out.,pull request satisfied though trying,issue,negative,positive,positive,positive,positive,positive
686860680,"If anyone is interested, this could easily be revived using https://github.com/adriangb/scikeras. MLP example: https://github.com/adriangb/scikeras/blob/master/tests/mlp_models.py",anyone interested could easily example,issue,positive,positive,positive,positive,positive,positive
686818460,Thank you! I will check it next week.,thank check next week,issue,negative,neutral,neutral,neutral,neutral,neutral
686799802,"@weixuanfu this PR should be ready to test with the 0.16 `rapidsai-nightly` release of cuML and standard XGBoost from `conda-forge`. If you want to build from a small conda environment, the following should work:

`tpot-pr.yml`
```
channels:
  - rapidsai-nightly
  - nvidia
  - conda-forge
  - defaults
dependencies:
  - python=3.7
  - cudatoolkit=10.2
  - cuml
  - scikit-learn
  - ipython
  - ipywidgets
  - jupyterlab
  - xgboost
  - pip
  - pip:
    - jupyter-server-proxy
    - git+https://github.com/beckernick/tpot.git@feature/duck-typed-estimator-op-checks

```

```
conda env create -f tpot-pr.yml -n tpot-pr --force`
conda activate tpot-pr
```
The above would need a different `cudatoolkit` version depending on the system.

In terms of loose memory requirements for the biggest example notebook, when I ran Higgs_Boson.ipynb as is (400k rows in the training set) peak GPU memory appeared to be about 2800 MB.",ready test release standard want build small environment following work pip pip create force activate would need different version depending system loose memory biggest example notebook ran training set peak memory,issue,negative,negative,neutral,neutral,negative,negative
685785482,It actually seems to work when i specify the config_dict.  This is great . Thank you for your answer,actually work specify great thank answer,issue,positive,positive,positive,positive,positive,positive
683713688,"Hi @weixuanfu @rhiever @rasbt @vruusmann,
When the exported pipeline contains _""from sklearn.preprocessing import Normalizer""_,  TPOT fitted pipeline cannot be converted into PMML format, as sklearn2pmml package does not support it. [SkLearn2PMML/JPMML-SkLearn](https://github.com/jpmml/jpmml-sklearn/blob/master/src/main/resources/META-INF/sklearn2pmml.properties)
 How can I solve it ？Is it possible to remove or limit the function like _sklearn.preprocessing.Normalizer_ ?",hi pipeline import normalizer fitted pipeline converted format package support solve possible remove limit function like,issue,positive,neutral,neutral,neutral,neutral,neutral
683451253,"Hmm, @GuillaumeLab is there any error without ` template=""RandomForestRegressor""`? Could you please share a demo for reproducing this issue?",error without could please share issue,issue,negative,neutral,neutral,neutral,neutral,neutral
683294874,"i got an error when trying to use template=""RandomForestRegressor"" : 
`
tpot = TPOTRegressor(scoring=""r2"",generations=2,template=""RandomForestRegressor"", population_size=50,cv=tscv,verbosity=2,warm_start=True, use_dask=False)`

`
RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly. If you enabled PyTorch estimators, please check the data requirements in the online documentation: https://epistasislab.github.io/tpot/using/`",got error trying use error optimization process could data properly data regression problem provided object please make sure data correctly please check data documentation,issue,negative,positive,positive,positive,positive,positive
678778881,"dask 2.24.0 . 

Thanks for your answer. 
I also get another error message : 
Restarting distributed.nanny - WARNING - Worker exceeded 95% memory budget.

I checked this thread : ""https://github.com/dask/distributed/issues/2297"""", and it does not really help solve the issue. Tpot is working fine on a single device, no memory issue. Why distributing it on several devices would cause a memory issue? ",thanks answer also get another error message warning worker memory budget checked thread really help solve issue working fine single device memory issue several would cause memory issue,issue,positive,positive,positive,positive,positive,positive
678774565,"Please find my email under my [Github profile](https://github.com/weixuanfu).

Please send over the dataset (or partial dataset) as csv/tsv file (please note which one is target in your email) and python scripts you used.",please find profile please send partial file please note one target python used,issue,positive,negative,neutral,neutral,negative,negative
678741119,"I had the same issue with increased feature count on this dataset. 

How should I go about sharing the file. Can I save the dataframe in one of the supported formats or is there an email where I can send it as pickle? 
",issue feature count go file save one send pickle,issue,negative,neutral,neutral,neutral,neutral,neutral
678716316,"Hmm only one feature, please check this related [question](https://stackoverflow.com/questions/17232078/is-that-make-sense-to-construct-a-learning-model-using-only-one-feature) for ML analysis.  

But I tested the demo below and this issue is not found. Could you share your data and code with me for a test?

```python
from tpot import TPOTClassifier
import numpy as np
X = np.random.random((23000,1))
y = np.random.randint(2, size=23000)

tpot = TPOTClassifier(generations=2,population_size=2,verbosity=2,random_state=42)

print(X.shape, y.shape)
tpot.fit(X, y)
```",one feature please check related question analysis tested issue found could share data code test python import import print,issue,positive,neutral,neutral,neutral,neutral,neutral
678669492,"I got the same issue. I can't use conda environment. Whenever i use ""use_dask=True"", i get the following error : 

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.

tpot = TPOTRegressor(verbosity=3, scoring = rmsle_loss, generations = 50,population_size=50,offspring_size= 50,max_eval_time_mins=10,warm_start=True, use_dask=True)
tpot.fit(X_train,y_train)

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.

I have tried on azure databricks cluster as well as on my local machine",got issue ca use environment whenever use get following error pipeline yet please call fit first scoring pipeline yet please call fit first tried azure cluster well local machine,issue,positive,positive,positive,positive,positive,positive
677906827,"Sounds great. Just to note, you'd need to use the [0.16 nightly conda package](https://anaconda.org/rapidsai-nightly/cuml) (either locally or in the cloud).

Enjoy your vacation! 🌴 ",great note need use nightly package either locally cloud enjoy vacation,issue,positive,positive,positive,positive,positive,positive
677902217,"Thank you for the information and heads-up. I will be on vacation next week. So I will wait for another test after the stable release of cuML 0.15. (Then I will set up cuML 0.15 with 2080 Ti GPU in my local compute node)

",thank information vacation next week wait another test stable release set ti local compute node,issue,positive,neutral,neutral,neutral,neutral,neutral
677900484,"RAPIDS 0.15 and onward will not support Python 3.6. The rationale comes from the broader community in [NEP-29](https://numpy.org/neps/nep-0029-deprecation_policy.html) (NEP 29 — Recommend Python and Numpy version support as a community policy standard))

In particular, the NumPy recommended [support table](https://numpy.org/neps/nep-0029-deprecation_policy.html#support-table) and drop schedule:
```
Support Table

Date | Python | NumPy
Jan 07, 2020 | 3.6+ | 1.15+
Jun 23, 2020 | 3.7+ | 1.15+
Jul 23, 2020 | 3.7+ | 1.16+
Jan 13, 2021 | 3.7+ | 1.17+
Jul 26, 2021 | 3.7+ | 1.18+
Dec 26, 2021 | 3.8+ | 1.18+
Apr 14, 2023 | 3.9+ | 1.18+

Drop Table
On next release, drop support for Python 3.5 (initially released on Sep 13, 2015)
On Jan 07, 2020 drop support for Numpy 1.14 (initially released on Jan 06, 2018)
On Jun 23, 2020 drop support for Python 3.6 (initially released on Dec 23, 2016)
On Jul 23, 2020 drop support for Numpy 1.15 (initially released on Jul 23, 2018)
On Jan 13, 2021 drop support for Numpy 1.16 (initially released on Jan 13, 2019)
On Jul 26, 2021 drop support for Numpy 1.17 (initially released on Jul 26, 2019)
On Dec 26, 2021 drop support for Python 3.7 (initially released on Jun 27, 2018)
On Apr 14, 2023 drop support for Python 3.8 (initially released on Oct 14, 2019)
```

Given the cuML bugfix PR above has not yet merged, I'm happy to ping you when this current PR is ready for testing. I want to make sure to be respectful of your time. I appreciate your guidance and support on this PR 😄  ",onward support python rationale come community nep recommend python version support community policy standard particular support table drop schedule support table date python drop table next release drop support python initially drop support initially drop support python initially drop support initially drop support initially drop support initially drop support python initially drop support python initially given yet happy ping current ready testing want make sure respectful time appreciate guidance support,issue,positive,positive,positive,positive,positive,positive
677894481,"Hmm I cannot install the nightly build in Colab (stderr below) and I think the reason is that Colab only support python3.6 for now but it seems not working with RAPIDS 0.15. Will RAPIDS 0.15 support python 3.6 later?

```shell
Installing RAPIDS 0.15 packages from the nightly release channel
Please standby, this will take a few minutes...
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed

SpecsConfigurationConflictError: Requested specs conflict with configured specs.
  requested specs: 
    - cudatoolkit=10.1
    - cudf=0.15
    - cugraph
    - cuml
    - cusignal
    - cuspatial
    - dask-cudf
    - gcsfs
    - pynvml
  pinned specs: 
    - python=3.6
    - xgboost
```",install nightly build think reason support python working support python later shell nightly release channel please take package done environment initial frozen solve flexible solve environment retry next source package done environment initial frozen solve flexible solve environment spec conflict spec spec pinned spec,issue,positive,neutral,neutral,neutral,neutral,neutral
677881422,"Great, I will test with nightly build. Thank you for the information. ",great test nightly build thank information,issue,positive,positive,positive,positive,positive,positive
677879663,"Ah, that's my mistake. While debugging I discovered one more necessary cuML change ([PR](https://github.com/rapidsai/cuml/pull/2723)) and ran on that build without thinking about it.

Apologies for not being more clear on the cuML status and timeline earlier. This will not work with cuML 0.14. RAPIDS cuML is [scheduled](https://docs.rapids.ai/maintainers) to release 0.15 next week, which has a significant number of enhancements that enable this functionality (less the bug in the PR above). Conda packages for the nightly release cuML 0.16 are [available](https://anaconda.org/rapidsai-nightly/cuml) currently. The examples will work with what we call the ""0.16 nightly"" packages once the linked PR lands.",ah mistake discovered one necessary change ran build without thinking clear status work release next week significant number enable functionality le bug nightly release available currently work call nightly linked,issue,negative,positive,positive,positive,positive,positive
677858858,"@beckernick thank you for submiting those examples.

I tried to test one of them on Colab but somehow it failed. Here is the [link](https://colab.research.google.com/gist/weixuanfu/0d92343afd862102363721ce20004a88/tpot_cuml.ipynb). Any idea?

Key runtime info:
cuml version 0.14
cudo version 10.1
",thank tried test one somehow link idea key version version,issue,negative,neutral,neutral,neutral,neutral,neutral
677842431,"@weixuanfu I've included two example notebooks for review. Since this PR adds now a configuration, it also should correspondingly update the documentation.

I'll do that shortly, but first will address the discussed upstream cuML compatibility.",included two example review since configuration also correspondingly update documentation shortly first address upstream compatibility,issue,negative,positive,positive,positive,positive,positive
677823497,"Thanks for highlighting that discrepancy and the specific location in the code. We have an [open issue](https://github.com/rapidsai/cuml/issues/1995).

I agree with your assessment. Rather than special case cuML in the TPOT codebase, I'd prefer we resolve it upstream (at which point, the existing code you linked should ""just work"").

I'll push the next commit, and then will shift to address the `random_state` vs `seed` in cuML",thanks discrepancy specific location code open issue agree assessment rather special case prefer resolve upstream point code linked work push next commit shift address seed,issue,positive,positive,positive,positive,positive,positive
677820720,"One issue with cuml is that the [`RandomForestClassifier`](https://github.com/rapidsai/cuml/blob/branch-0.15/python/cuml/ensemble/randomforestclassifier.pyx#L226) uses `seed` instead of `random_state` for setting random seed. But in TPOT, we use `random_state` (see [those lines](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1214-L1215)) to set random seeds of all operators in a pipeline. I think we need a workaround (like adding `set_param_recursive(sklearn_pipeline.steps, 'seed', self.random_state)` there) for this. But I hope cuml could update it based on scikit-learn's API.",one issue seed instead setting random seed use see set random pipeline think need like hope could update based,issue,positive,negative,negative,negative,negative,negative
676623191,"> Also, you could add the cuML configuration under `tpot/config` for simply using it via `config_dict` (something like `config_dict=""TPOT cuML""`. It is also easier for comparing the performance between default TPOT configuration and cuML configuration.

Happy to add this. As a note, as of now I would plan to wrap this logic inside a check to make sure `cuml` is available. Otherwise, a user could pass `config_dict=""TPOT cuML""` and get a failure where they didn't expect one. If you have any thoughts on your preferred style for this, I'm open to suggestions. Currently, I plan to follow this pattern:

```python
def _has_cuml():
    try:
        import cuml   # NOQA
        return True
    except ImportError:
        return False
```

to create a binary flag and raise an informative warning if `False`. If you'd prefer to not add something like this, that's fine with me too 😄 ",also could add configuration simply via something like also easier performance default configuration configuration happy add note would plan wrap logic inside check make sure available otherwise user could pas get failure expect one preferred style open currently plan follow pattern python try import return true except return false create binary flag raise informative warning false prefer add something like fine,issue,positive,positive,positive,positive,positive,positive
676560242,"Thank you for the PR

A Jupyter notebook for demo (like [this one](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Digits.ipynb)) stored in `tutorials` should be fine. 

Also, you could add the cuML configuration under `tpot/config` for simply using it via `config_dict` (something like `config_dict=""TPOT cuML""`. It is also easier for comparing the performance between default TPOT configuration and cuML configuration.  ",thank notebook like one fine also could add configuration simply via something like also easier performance default configuration configuration,issue,positive,positive,positive,positive,positive,positive
676518946,"@weixuanfu , I'd be happy to add ""a demo for using TPOT with a cuML configuration similar to TPOT's default configurations."" I do have a couple of questions to help make sure what I provide is useful.

- Do you have a specific kind of demo format in mind (e.g., a Jupyter Notebook example)?
- Is this something you would like to see/live in this PR, as a gist, or somewhere else? If in this PR, would you prefer it to live in the `tutorials` directory?",happy add configuration similar default couple help make sure provide useful specific kind format mind notebook example something would like gist somewhere else would prefer live directory,issue,positive,positive,positive,positive,positive,positive
676482090,"> Is a possible reason for ""_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler."" that due to some feature selection hyperparameter 0 features are passed to the next step? I will try to change the minimum threshold, maybe that helps.

Yes, I think so. Changing the configuration of those feature selection operator may help.


> So if a pipeline is detected as invalid, is still kept in the population with a low score? In my case it looks like 80% of the pipeline is full of invalid pipelines sometimes, ideally they should not propagate through so many generations.


The invalid pipelines tested in _pre_test decorator should not pass to population unless the newly-generated pipelines from one alteration in GP (crossover or mutation or randomly initial generation) failed ten times in _pre_test. So the population should not have those invalid pipelines in most of cases. ",possible reason decorator found array feature minimum due feature selection next step try change minimum threshold maybe yes think configuration feature selection operator may help pipeline invalid still kept population low score case like pipeline full invalid sometimes ideally propagate many invalid tested decorator pas population unless one alteration crossover mutation randomly initial generation ten time population invalid,issue,positive,positive,positive,positive,positive,positive
676421629,"Sounds good. Please feel free to assign this issue to me, if useful.

I'll open up a PR for further discussion/iteration.",good please feel free assign issue useful open,issue,positive,positive,positive,positive,positive,positive
676412837,"Is a possible reason for ""_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler."" that due to some feature selection hyperparameter 0 features are passed to the next step? I will try to change the minimum threshold, maybe that helps.

So if a pipeline is detected as invalid, is still kept in the population with a low score? In my case it looks like 80% of the pipeline is full of invalid pipelines sometimes, ideally they should not propagate through so many generations.

",possible reason decorator found array feature minimum due feature selection next step try change minimum threshold maybe pipeline invalid still kept population low score case like pipeline full invalid sometimes ideally propagate many,issue,negative,positive,positive,positive,positive,positive
676409222,I think it is very good idea. Please open a PR with a demo for using TPOT with a cuML configuration similar to TPOT's [default configurations](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations).,think good idea please open configuration similar default,issue,positive,positive,positive,positive,positive,positive
676394053,"TPOT may randomly generate invalid pipelines, for example using invalid hyperparameter combinations (eg. calling Logistic Regression with dual=True and penalty=L1), so, to avoild this, the pre_test decorator can evaluates such an pipeline on a small test set with maximum sample size of 50.

So far you may try different `random_state` for better initial population which should be reproduced with the same `random_state`. 

I think it is great idea to pass some good initial pipelines. There are a related issue #296 but we did not implement in a ideal way (there was a  related PR #502 but we revoked changes there). Any contributions are welcome for this new features. ",may randomly generate invalid example invalid calling logistic regression decorator pipeline small test set maximum sample size far may try different better initial population think great idea pas good initial related issue implement ideal way related welcome new,issue,positive,positive,positive,positive,positive,positive
675399842,"@weixuanfu very well, I have 183 features, whats a recommended population size then? It explicitly says that a larger population size gives higher accuracy.",well whats population size explicitly population size higher accuracy,issue,negative,positive,positive,positive,positive,positive
675191826,"Those MDR methods from skrebate is very slow for dataset with a large number of features. How many features are there in your dataset?

Also in your example codes, I saw the `population_size=10000` which is too large for TPOT MDR due to its limited search space of hypeparameters. I suggest to set it as default value, 100. ",slow large number many also example saw large due limited search space suggest set default value,issue,negative,positive,neutral,neutral,positive,positive
674868270,"> I have a few questions regarding the parameters of the genetic algorithm which are not answered in the doc:
> 
>     1. What if mutation_rate and crossover_rate don't sum up to 1, is the rest filled up with new random genoms?

If the sum is less than 1, then GP process can randomly reproduce/copy pipelines from parental generation to its offspring generation. The reproduction rate is 1-mutation_rate-crossover_rate. (see [here](https://github.com/EpistasisLab/tpot/blob/1c009c669190a024671a2d810f90fc097d6610d5/tpot/gp_deap.py#L122-L125))

>     2. What exactly is the offsping_size doing?

`offspring_size` is the number of pipelines generated from parental generation. Please check the [eaMuPlusLambda](https://deap.readthedocs.io/en/0.7-a1/algorithms.html) for more details. 

>     3. Which mutation operators are applied: for example if there is a parameter to optimize between 0-100, is it mutated randomly or by multiplying the current solution with a random factor (e.g. 0.8-1.2)?

Please check this related [issue](https://github.com/EpistasisLab/tpot/issues/318)
> 
> Background to my questions is: my optimization gets stuck in local minima quite fast: it converges too fast, different random seeds leed to quite different optimization results after it converges.

",regarding genetic algorithm doc sum rest filled new random sum le process randomly parental generation offspring generation reproduction rate see exactly number parental generation please check mutation applied example parameter optimize randomly multiplying current solution random factor please check related issue background optimization stuck local minimum quite fast fast different random leed quite different optimization,issue,positive,negative,neutral,neutral,negative,negative
674859386,"Could you please print out the shape of X, y?",could please print shape,issue,negative,neutral,neutral,neutral,neutral,neutral
674514243,"> Updated TPOT to 0.11 via:
> `pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development`
> Same issue with:
> 
> ```
> SCORERS['rmsle_loss'] = make_scorer(RMSLE, greater_is_better=False)
> params ={'cv':5,
>          'scoring': 'rmsle_loss',
>          'generations':10,
>          'random_state':0,
>          'max_eval_time_mins':10}
> ```
> 
> I get:
> 
> `ValueError: 'rmsle_loss' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.`

Thank you @lmsanch Your suggestion saved my day!",via pip install upgrade issue get valid scoring value use sorted get valid thank suggestion saved day,issue,positive,neutral,neutral,neutral,neutral,neutral
674432461,"This looks like a formatting issue with your dataset, not with TPOT. You may need to map your `y` values to integers.",like issue may need map,issue,negative,neutral,neutral,neutral,neutral,neutral
674413715,"Running the following code gives me an error when calling tpot.score

```
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=False)
X_val, X_test, y_val, y_test  = train_test_split(X_test, y_test, test_size=0.50, random_state=1, shuffle=False) # 0.5 x 0.33 = 0.165

tpot = TPOTClassifier(generations=None, population_size=1000, verbosity=2, max_time_mins=60, n_jobs=-1, periodic_checkpoint_folder=""D:\\ReinforcementLearning\\XGBoost\\Output\\Checkpoints"", config_dict=""TPOT NN"")
tpot.fit(X_train, y_train)
tpot.export('D:\\ReinforcementLearning\\XGBoost\Output\\tpot_digits_pipeline.py')
print(tpot.score(X_val, y_val))
```

```
Traceback (most recent call last):
  File ""d:\ReinforcementLearning\XGBoost\XGBoost.py"", line 43, in <module>
    print(tpot.score(X_val, y_val))
  File ""D:\Anaconda\envs\NBA\lib\site-packages\tpot\base.py"", line 957, in score
    testing_target.astype(np.float64)
  File ""D:\Anaconda\envs\NBA\lib\site-packages\pandas\core\generic.py"", line 5537, in astype
    new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors,)
  File ""D:\Anaconda\envs\NBA\lib\site-packages\pandas\core\internals\managers.py"", line 567, in astype
    return self.apply(""astype"", dtype=dtype, copy=copy, errors=errors)
  File ""D:\Anaconda\envs\NBA\lib\site-packages\pandas\core\internals\managers.py"", line 396, in apply
    applied = getattr(b, f)(**kwargs)
  File ""D:\Anaconda\envs\NBA\lib\site-packages\pandas\core\internals\blocks.py"", line 590, in astype
    values = astype_nansafe(vals1d, dtype, copy=True)
  File ""D:\Anaconda\envs\NBA\lib\site-packages\pandas\core\dtypes\cast.py"", line 989, in astype_nansafe
    return arr.astype(dtype, copy=True)
ValueError: could not convert string to float: 'Bull'
```",running following code error calling print recent call last file line module print file line score file line file line return file line apply applied file line file line return could convert string float,issue,negative,neutral,neutral,neutral,neutral,neutral
674315535,Thank you @JDRomano2 I close the issue. Please feel free to reopen it if there are other related questions.,thank close issue please feel free reopen related,issue,positive,positive,positive,positive,positive,positive
674311899,"**tl;dr:** No, you don't need to specify a template to use NN estimators, but you can if you want.

When you say `config_dict='TPOT NN'`, all that does is make the NN estimators available to TPOT. It doesn't necessarily guarantee that TPOT will include them in the final pipeline, but they _can_ be in the pipeline, along with any of the other non-NN estimators that are built in.

The template option is good for if you know that (a.) you definitely want an NN estimator and (b.) you already know what format pipeline you want and just need to optimize hyperparameters.

If you still want TPOT to learn the pipeline format, but also want to restrict it to **only** use NN estimators in that pipeline, you can specify a custom config dict, something like the following:

```python
clf = tpot.TPOTClassifier(
    generations=50,
    population_size=10,
    verbosity=2,
    config_dict={
    'tpot.builtins.PytorchLRClassifier': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'batch_size': [4, 8, 16, 32],
        'num_epochs': [5, 10, 15],
        'weight_decay': [0, 1e-4, 1e-3, 1e-2]
    },

    'tpot.builtins.PytorchMLPClassifier': {
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'batch_size': [4, 8, 16, 32],
        'num_epochs': [5, 10, 15],
        'weight_decay': [0, 1e-4, 1e-3, 1e-2]
    },
    }
)
```",need specify template use want say make available necessarily guarantee include final pipeline pipeline along built template option good know definitely want estimator already know format pipeline want need optimize still want learn pipeline format also want restrict use pipeline specify custom something like following python,issue,positive,positive,positive,positive,positive,positive
674276339,"@JDRomano2 Very well I have CUDA and pytorch and cuda is enabled when running it through pytorch. Now I just want my other question answered.
""Does the TPOT-NN configuration use a NN if you specify the code as follows or does one need to specify a template as well?""",well running want question configuration use specify code one need specify template well,issue,positive,neutral,neutral,neutral,neutral,neutral
674150207,"Confirmed that CUDA is used when available, under the following testing conditions:

- Windows 10
- PyTorch 1.6.0
- CUDA Toolkit 10.1
- Nvidia GeForce GTX 1080

![2020-08-14 12_00_04-Task Manager](https://user-images.githubusercontent.com/843449/90269780-2d1a2380-de27-11ea-991c-97ca8268b326.png)

My testing script:

```python
import tpot
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

X, y = make_classification(n_samples=1000, n_features=20, n_informative=4, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = tpot.TPOTClassifier(generations=50, population_size=10, verbosity=2, config_dict='TPOT NN', template='Selector-Transformer-PytorchMLPClassifier')

clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))
```

@sword134 - would you mind verifying your installed PyTorch version, CUDA version, and [GPU compute capability](https://developer.nvidia.com/cuda-gpus#compute)?

**EDIT**: This works both for the template and non-template ways of running TPOT.",confirmed used available following testing manager testing script python import import import print sword would mind version version compute capability edit work template way running,issue,negative,positive,positive,positive,positive,positive
674109799,"The NN models should automatically use GPU when it's available:
https://github.com/EpistasisLab/tpot/blob/219f8c5abe43996abb2c19d6a1767083304a23d3/tpot/builtins/nn.py#L260

However, we haven't yet written any explicit tests to ensure that this works as intended. Importantly, you need to have installed a CUDA-enabled version of PyTorch as well. I'll report back when I've established whether it works as intended when both (a.) a CUDA compatible GPU is available on the system and (b.) a CUDA version of PyTorch is installed.",automatically use available however yet written explicit ensure work intended importantly need version well report back established whether work intended compatible available system version,issue,positive,positive,positive,positive,positive,positive
673551404,@JDRomano2 Could you please answer this question? Thank you.,could please answer question thank,issue,positive,neutral,neutral,neutral,neutral,neutral
672070645,"Thanks for the super fast response!  

I'm really enjoying using TPOT and finding some very interesting things.  Nice work!",thanks super fast response really enjoying finding interesting nice work,issue,positive,positive,positive,positive,positive,positive
671975231,"Yes, the second interpretation is what happened in that pipeline. But TPOT does not support the 1st interpretation so far.",yes second interpretation pipeline support st interpretation far,issue,positive,positive,neutral,neutral,positive,positive
670987481,"I had a quick look on your benchmark. I saw the time limit is 1 hour but some of datasets has over 40k instances, so TPOT may not pass the initial generation(which is randomly generate pipelines) and should not optimize pipelines via genetic programing. I suggest increasing time limit to 1 day for each large size of datasets if n_jobs=1 or using [parallel training with dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask).",quick look saw time limit hour may pas initial generation randomly generate optimize via genetic suggest increasing time limit day large size parallel training,issue,negative,positive,neutral,neutral,positive,positive
668021325,"Hmm, @vlaskinvlad how about changing the `decorators.MAX_EVAL_SECS` to 5 or 10, I think 100 cannot control the time limit for pretest pipeline with a small subset of data (max sample size = 50). How many features in your datasets that causing this issue? ",think control time limit pretest pipeline small subset data sample size many causing issue,issue,negative,positive,positive,positive,positive,positive
666670411,"I second that question / issue (tpot version: `0.11.5`)

Here is a workaround that may work as a pointer: 

```python
from tpot import decorators
decorators.MAX_EVAL_SECS = 100
tpot_obj.fit(train_X, train_y)
```

Otherwise one could get `stopit.utils.TimeoutException` with a message

 ` ... utils.py:82] Code block execution exceeded 2 seconds timeout`. (MAX_EVAL_SECS is 2 by default)",second question issue version may work pointer python import otherwise one could get message code block execution default,issue,negative,neutral,neutral,neutral,neutral,neutral
661870677,I checked this issue with severely imbalanced dataset in Colab(see the [notebook](https://colab.research.google.com/gist/weixuanfu/84be5cd72abfd99933758138b880bd1a/issue1097.ipynb)) but somehow this issue cannot be reproduced. Please let us know if there are more details.,checked issue severely see notebook somehow issue please let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
660052907,"> y_train should be pandas.Series instead of DataFrame. You could use y_train directly in fit(). Could you please print shape of features and target to check this issue?

@weixuanfu that actually worked, I have converted y_train using pd.series and it worked.

Thanks @weixuanfu , btw y_train.shape = (891,) and x_train.shape = (891,14)",instead could use directly fit could please print shape target check issue actually worked converted worked thanks,issue,positive,positive,positive,positive,positive,positive
659738400,"Which version of scikit-learn was used in your environment? I checked [recall](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html) score in scikit-learn, which should get raise warning and get a 0 recall score in case of zero division instead of raising a error. It is the same for [precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html#sklearn.metrics.precision_score). 

I will have some time to look into it next week. Could you please provide a small demo for reproducing this issue? Thank you.

BTW, [imbalanced-learn](https://imbalanced-learn.readthedocs.io/en/stable/api.html) maybe useful for this severely imbalanced dataset.",version used environment checked recall score get raise warning get recall score case zero division instead raising error time look next week could please provide small issue thank maybe useful severely,issue,negative,positive,neutral,neutral,positive,positive
659720024,"Update: `scoring=""precision""` doesn't work either",update precision work either,issue,negative,neutral,neutral,neutral,neutral,neutral
659139769,"@josiahjohnston Thanks for that script. I created a function out of it 
```python
def cleanup_cache(memory, verbose=False):
    def infinite():
        while(True):
            if verbose: print('\nreducing size of tpot cache at {}'.format(datetime.now()))
            memory.reduce_size()
            if verbose: print('done cleaning; going to sleep for 30 min.')
            time.sleep(1800)
    return infinite
```
So now in my script I just do 
```python
    OUTPUT_DIR = getcwd() + '/tpot-output'
    memory = Memory(location='{DIR}/cache'.format(DIR=OUTPUT_DIR), bytes_limit=10*2**30, verbose=0)
    cleanup_thread = Thread(name='cache_cleanup', target=cleanup_cache(memory), daemon=True)
    cleanup_thread.start()
    main(OUTPUT_DIR, memory) #this fn does some preprocessing then runs TPOTRegressor
```",thanks script function python memory infinite true verbose print size cache verbose print cleaning going sleep min return infinite script python memory memory thread memory main memory,issue,positive,positive,positive,positive,positive,positive
659106375,"I should re-iterate, starting from a brand new ""factory reset""ed session, I run those pip installs shown above, then when it gets to the training session, it errors out, but when you hit ""Restart and Run All"", it works. It's something to do with changing the version of Tornado, and you can only run the new pip installed version after you restart the runtime. Don't factory restart the session a second time, else you're just starting from scratch again. Seems finicky in Colab, but that's what worked for me.",starting brand new factory reset session run pip shown training session hit restart run work something version tornado run new pip version restart factory restart session second time else starting scratch worked,issue,negative,positive,neutral,neutral,positive,positive
659103712,"UPDATE:

Looks like I hit a winning combination with the below:

```
!pip install TPOT
!pip install dask==2.20.0 dask-glm==0.2.0 dask-ml==1.0.0
!pip install tornado==5.0
!pip install distributed==2.2.0
!pip install xgboost==0.90
```

This got my example up and running. I should have cross referenced my `pip freeze` output with the `pip freeze` output on Colab to check versions of the dependencies earlier, but this was able to get it going. Closing!",update like hit winning combination pip install pip install pip install pip install pip install got example running cross pip freeze output pip freeze output check able get going,issue,positive,positive,positive,positive,positive,positive
658764657,y_train should be pandas.Series instead of DataFrame. You could use y_train directly in fit(). Could you please print shape of features and target to check this issue?,instead could use directly fit could please print shape target check issue,issue,positive,positive,positive,positive,positive,positive
658272563,"I have run into the same problem some time ago. The proposed solution with using custom `cv` works for me but I'd like to leave an argument for why this feature could be helpful.

Using cross-validation is a default choice when using small datasets. However:
- with big datasets it might be a better option to just have a train-validation-test split,
- some benchmark datasets define train-validation-test split and this has to be followed.

No ready way to use an external validation set made me decide not to use TPOT at all and I've spend quite some time looking for alternatives. I've found nothing more suitable than TPOT and ended up writing custom `cv`.

To wrap up: I believe this feature is useful and it's worth considering adding it to TPOT.",run problem time ago solution custom work like leave argument feature could helpful default choice small however big might better option split define split ready way use external validation set made decide use spend quite time looking found nothing suitable ended writing custom wrap believe feature useful worth considering,issue,positive,positive,positive,positive,positive,positive
657631193,@weixuanfu Thank you very much! It's working excellent!,thank much working excellent,issue,positive,positive,positive,positive,positive,positive
657573656,Thank you for reporting this issue. This issue is caused by incompatibility of TPOT docs with a older version of `mkdocs` (v1.0.1). I fixed MarkDown docs in #1092 PR and now the webpage was updated.  Please check it. ,thank issue issue incompatibility older version fixed markdown please check,issue,positive,positive,positive,positive,positive,positive
656153730,"> Hmm, it is not right. Please put `tp=TPOTClassifier...` outside loop and `random_state=0` means no random_state in TPOT (I think we need change this in the future).
> 
> I am not sure why only 7 pipeines each loop in the stdout in your notebook. It should add 4 pipelines in each loop unless there is a duplicated pipeline. Please check [this demo](https://colab.research.google.com/gist/weixuanfu/b69afea310a3c2c2109191a6a598f38b/issue337.ipynb) for reference.

Okay @weixuanfu Thanks for the clarification :)",right please put outside loop think need change future sure loop notebook add loop unless pipeline please check reference thanks clarification,issue,positive,positive,positive,positive,positive,positive
656062063,"Hmm, it is not right. Please put `tp=TPOTClassifier...` outside loop and `random_state=0` means no random_state in TPOT (I think we need change this in the future).

I am not sure why only 7 pipeines each loop in the stdout in your notebook. `evaluated_individuals_`  should gain 4 pipelines in each loop unless there is a duplicated pipeline. Please check [this demo](https://colab.research.google.com/gist/weixuanfu/b69afea310a3c2c2109191a6a598f38b/issue337.ipynb) for reference. ",right please put outside loop think need change future sure loop notebook gain loop unless pipeline please check reference,issue,positive,positive,positive,positive,positive,positive
656047801,"@weixuanfu is this the right way ? In generation1 i am getting only 7 pipelines , but it should generate 8.
population_size+generations*offspring_size = 4 + 4 = 8. Please Clarify.
![Screenshot 2020-07-09 at 3 56 33 PM](https://user-images.githubusercontent.com/48690720/87028822-d0f72a80-c1fc-11ea-86a1-dc5330f8bd7b.png)
",right way generation getting generate please clarify,issue,negative,positive,positive,positive,positive,positive
655688060,"I think the current version of TPOT did not encode those string-type numbers but use them directly in sklearn estimator/pipeline. But most of scikit-learn estimators can encode of input X, y during data validation (e.g [here](https://github.com/scikit-learn/scikit-learn/blob/fd237278e/sklearn/ensemble/_forest.py#L303-L304)) so they can convert those string-type numbers to numeric numbers. 

But if there is a column with strings that cannot be converted to numeric number, the fit() process should fail (as the demo below).
```python
import pandas as pd
from tpot import TPOTClassifier

url = 'https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv'
df = pd.read_csv(url)
df = df.astype('str')
df['Astring'] = ""should fail fit() in TPOT""

target = 'Outcome'
X = df.drop(target, axis=1)
y = df.loc[:, target]

tpot = TPOTClassifier(max_time_mins=1, verbosity=2, n_jobs=-1)
tpot.fit(X, y)
```
 

 ",think current version encode use directly encode input data validation convert column converted number fit process fail python import import fail fit target target target,issue,negative,negative,neutral,neutral,negative,negative
655288268,"> You could use a loop of fitting TPOT object with warm_start=True and generation=1 and save `evaluated_individuals_` Python dictionary every generation/loop.

@weixuanfu Thank you.",could use loop fitting object save python dictionary every thank,issue,positive,positive,positive,positive,positive,positive
654945488,You could use a loop of fitting TPOT object with warm_start=True and generation=1 and save `evaluated_individuals_` Python dictionary every generation/loop.,could use loop fitting object save python dictionary every,issue,negative,positive,positive,positive,positive,positive
654910322,"@weixuanfu Thanks for the reply. But if we keep interrupting it will take too much time to complete.
can you suggest any other workaround for this ?",thanks reply keep interrupting take much time complete suggest,issue,negative,positive,positive,positive,positive,positive
654877362,"You may interrupt fit function (like Ctrl+C), then TPOT should store all intermediate pipelines into  `evaluated_individuals_`.",may interrupt fit function like store intermediate,issue,negative,positive,positive,positive,positive,positive
654834326,"@weixuanfu Can we access all the intermediate pipelines that are completed without having the fit function to complete all the pipelines fully and then access using the evaluated_individuals_ on tpot object.
Are there any log files that keep these information like in autosklearn?",access intermediate without fit function complete fully access object log keep information like,issue,negative,positive,positive,positive,positive,positive
653813316,"for e.g. image shown below, I am new in AI and understand faster from images\pictures.
![exm](https://user-images.githubusercontent.com/11360095/86521034-1b4d6580-be11-11ea-9002-ec861d220ab8.png)



",image shown new ai understand faster,issue,negative,positive,positive,positive,positive,positive
653806261,What example? Could you please provide more details about the question/issue? Please feel free to reopen this issue with details.,example could please provide please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
651212247,"Hi @weixuanfu 
Are there any updates about running warm_start in a non-interactive shell? 
Or is there any other possible way to do that? ( ex: using pickle )",hi running shell possible way ex pickle,issue,negative,neutral,neutral,neutral,neutral,neutral
650258141,"@hakeemo - Yes, removing PolynomialFeatures fixed my problem, but it wasn't a disk space issue.  Rather it was a memory issue with individual processes taking 100+ GB. ",yes removing fixed problem disk space issue rather memory issue individual taking,issue,negative,positive,neutral,neutral,positive,positive
649860769,"@KirkDCO  did removing PolynomialFeatures solve the problem with the  disk space ? Are you able to run TPOT with memory=""auto"" without that problem happening? ",removing solve problem disk space able run auto without problem happening,issue,negative,positive,positive,positive,positive,positive
649060795,"Hmm, that is strange. Is this error reproducible with a small benchmark, like Iris dataset? If so, please let us know the versions of TPOT and its dependencies as well as the `config_file`.",strange error reproducible small like iris please let u know well,issue,negative,negative,negative,negative,negative,negative
649057095,"As far I understand, max_eval_time_mins just makes to skip a specific pipeline if it takes more than x minutes, and it doesn't stop the optimization process a priori. We already set this parameter to 60 minutes and the error continues to occur. I have read somewhere else (https://github.com/glenfant/stopit/issues/16) that this type of error message is related to the communicate() method of Popen, but I still can't resolve it.

I left the code we are running:

def run_TPOT_auto_ML(data_path,target,sep='\t',exclude=[],generations=1,population_size=100,cv=5,fold=0,rseed=42,results_path='./'):
        
        if results_path != './' and not os.path.exists(results_path): os.makedirs(results_path)

        # Loading dictionary of pipelines to use
        config_file = np.load('TPOT_config_file.npy',allow_pickle=True).item()
         
        # Loading
        data = pd.read_csv(data_path,sep=sep)
        feats = [c for c in data.columns if c != target and c not in exclude]
        X,y = data[feats].values,data[target].values

        # Split
        idx = [tr for tr,_ in StratifiedKFold(cv,random_state=rseed).split(X,y)][fold]
        with open(os.path.join(results_path,'TPOT_train_idx_{}_{}.json'.format(fold,rseed)), 'w') as outfile:
        	json.dump({
        		'data_path':data_path,
        		'target':target,
        		'cv':cv, 'fold':fold,
        		'idx':idx.tolist(),
        	}, outfile)

        CLF = TPOTClassifier(generations=generations,population_size=population_size,config_dict=config_file,verbosity=2,n_jobs=1,max_eval_time_mins=60)
        CLF.fit(X[idx],y[idx])
        
        # Export the best pipeline
        CLF.export(output_file_name=os.path.join(results_path,'TPOT_best_pipeline_{}.py'.format(rseed)),data_file_path=data_path)
        
        RESULTS = pd.DataFrame([{
                'Generation':CLF.evaluated_individuals_[pipe]['generation'],
                'Model':pipe,
                'Internal_cv_score':CLF.evaluated_individuals_[pipe]['internal_cv_score'],
                'Mutation_count':CLF.evaluated_individuals_[pipe]['mutation_count'],
                'Crossover_count':CLF.evaluated_individuals_[pipe]['crossover_count'],
                'Predecessor':CLF.evaluated_individuals_[pipe]['predecessor'],
                'Operator_count':CLF.evaluated_individuals_[pipe]['operator_count']
        } for pipe in CLF.evaluated_individuals_])
		
        RESULTS.to_csv(os.path.join(results_path,'auto_ML_results_{}.csv'.format(rseed)),sep=sep,index=False)",far understand skip specific pipeline stop optimization process already set parameter error occur read somewhere else type error message related communicate method still ca resolve left code running target loading dictionary use loading data target exclude data data target split fold open fold target fold export best pipeline pipe pipe pipe pipe pipe pipe pipe pipe,issue,negative,positive,positive,positive,positive,positive
649051071,"Hmm, I did not seem this error message before. Could you please provide more details (like versions of all TPOT dependencies and a demo) to reproduce this error?

I think it seems that it was related to `max_eval_time_mins` parameter, which control the maximum run time of each pipeline evaluation. Maybe the dataset in your case is very large and increasing this value assigned to this parameter may be helpful.",seem error message could please provide like reproduce error think related parameter control maximum run time pipeline evaluation maybe case large increasing value assigned parameter may helpful,issue,positive,positive,positive,positive,positive,positive
646247359,"For enabling processing bar, TPOT splits all pipelines of one generation into several chunks with a size of [4Xn_jobs or 2Xcpu_count()](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1321) then evaluates those chunks one by one. After evaluating the pipelines in one chunk, the processing bar can be updated. So that is the reason of this issue.

It is indeed not the most efficient way to handle both parallel jobs and processing bar. Any contribution is welcome.  ",bar one generation several size one one one chunk bar reason issue indeed efficient way handle parallel bar contribution welcome,issue,positive,positive,positive,positive,positive,positive
644816714,Could you please provide a small demo with [Boston benchmark](https://epistasislab.github.io/tpot/examples/#boston-housing-prices-modeling) and `config_dict` of ngboost to reproduce this issue?,could please provide small boston reproduce issue,issue,negative,negative,negative,negative,negative,negative
644814479,"Could you please change the type of input X, y in fit() function into np.ndarray instead of pd.DataFrame (like the codes below)? I think skrebate v0.6 did not have [a fix that I posted](https://github.com/EpistasisLab/scikit-rebate/pull/55) a while ago, which means that skrebate cannot deal with pd.DataFrame as input arrays in fit(). I am not working on project any more but I will let them know if it is a real issue for skrebate. 

```python
y_ds = ds['TARGET'].values
x_ds = ds('TARGET', axis=1).values
```",could please change type input fit function instead like think fix posted ago deal input fit working project let know real issue python,issue,positive,positive,positive,positive,positive,positive
644394388,"For some reason, all of the MDR operators are failing on my dataset.  I can take your exact code and run it on the Boston Housing data and see what you saw, MDR, SURF, etc. operators.

If I substituted my data which is a large matrix (1000 x 5000) continuous values, all of those operators appear to fail.  I went to the skrebate help files and used some of the sample code there as a test, but substituting in different datasets - specifically:

```import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline
from skrebate import SURF
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_boston

housing = load_boston()
features = housing.data
labels = housing.target

clf = make_pipeline(SURF(n_features_to_select=2),
                    RandomForestRegressor(n_estimators=100))
print(np.mean(cross_val_score(clf, features, labels)))
```
This works fine.  But if I substituted my dataset into features and labels, I get this error:

```
/Users/kdelisle/opt/miniconda2/envs/TPOT/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:532: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: 
KeyError: 0

  warnings.warn(""Estimator fit failed. The score on this train-test""
```

My guess is that in TPOT, those operators are failing silently during pipeline construction, so I never see them.  The open question is why do they fail on my dataset?",reason failing take exact code run boston housing data see saw surf substituted data large matrix continuous appear fail went help used sample code test substituting different specifically import import import import surf import import import housing surf print work fine substituted get error estimator fit score partition set nan estimator fit score guess failing silently pipeline construction never see open question fail,issue,negative,positive,neutral,neutral,positive,positive
644324676,"Thanks for checking.

Oddly, I don't see any MDR, Surf, ReliefF operators in mine at all.  Using your code to look at the evaluated individuals, I get no output.  If I look at all the keys in the evaluated_individuals_ dictionary, I see nothing but ElasticNetCV and CombineDF (see below).

EDIT:  I just reran your code exactly as you have it in the notebook and I _do_ see various MDR, Surf, etc operators.  Clearly something is up with my code.  I'll start digging into that next...

*************

Here's the code I'm using to run tpot:

from tpot import TPOTRegressor
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import scipy.stats as stats

ds = pd.read_csv('Dataset.csv')
ds = ds.dropna()

y_ds = ds['TARGET']
x_ds = ds('TARGET', axis=1)

X_train, X_test, y_train, y_test = train_test_split(x_ds, y_ds,
                                                   train_size = 0.75, test_size = 0.25, 
                                                   random_state = 303)
tpot = TPOTRegressor(generations = 1, population_size = 25, 
                     verbosity = 2, random_state = 303, n_jobs = 15,
		     max_eval_time_mins = 5,
		     config_dict = 'TPOT MDR') 
tpot.fit(X_train, y_train)

print('Final model score: ', tpot.score(X_test, y_test))
tpot.export('tpot_ds_pipeline.py')




Here's the code looking at the evaluated_individuals_:

for k in tpot.evaluated_individuals_.keys():
    print(k)

ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.1, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.9, ElasticNetCV__tol=0.0001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.2, ElasticNetCV__tol=0.001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.9, ElasticNetCV__tol=0.001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.4, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.9500000000000001, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.9500000000000001, ElasticNetCV__tol=0.01)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.2, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.1, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.2, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.7000000000000001, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.30000000000000004, ElasticNetCV__tol=0.0001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.2, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.7000000000000001, ElasticNetCV__tol=0.0001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.35000000000000003, ElasticNetCV__tol=0.0001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.9, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.1, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.4, ElasticNetCV__tol=0.01)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.25, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.75, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=1.0, ElasticNetCV__tol=0.1)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.05, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.1, ElasticNetCV__tol=0.01)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.8, ElasticNetCV__tol=0.1)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.4, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.65, ElasticNetCV__tol=1e-05)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.45, ElasticNetCV__tol=0.001)
ElasticNetCV(CombineDFs(input_matrix, input_matrix), ElasticNetCV__l1_ratio=0.55, ElasticNetCV__tol=0.0001)
ElasticNetCV(input_matrix, ElasticNetCV__l1_ratio=0.15000000000000002, ElasticNetCV__tol=0.01)",thanks oddly see surf mine code look get output look dictionary see nothing see edit code exactly notebook see various surf clearly something code start digging next code run import import import import import verbosity print model score code looking print,issue,positive,positive,neutral,neutral,positive,positive
644204263,I ran a quick in colab and found the MDR-related operators in `evaluated_individuals_` but they had lower scores than exported pipeline with only `ElasticNetCV`. Here is the [notebook](https://colab.research.google.com/gist/weixuanfu/a62b5efb609a4b938da40ada1cb2dcc3/issue_1083.ipynb),ran quick found lower pipeline notebook,issue,negative,positive,positive,positive,positive,positive
644188422,"I will have to rerun and test that - this was run remotely and has completed.  I have also done runs of population 100 for 1 generation and seen no MDR-related operators.

I will report back what I find.",rerun test run remotely also done population generation seen report back find,issue,negative,negative,neutral,neutral,negative,negative
644173822,"Could you please check **`evaluated_individuals_`** attribute to find if there are any pipelines with MDR functions? If not, please let me know. If there are some pipelines with MDR functions, maybe the reason is that their average cv scores are lower than exported pipeline. ",could please check attribute find please let know maybe reason average lower pipeline,issue,negative,negative,negative,negative,negative,negative
643639510,"Yes, this is a planned addition for the near future. Since this started as a proof-of-concept, we worked on NN classifiers only to make sure those would function as intended.

I'll update this thread as we add support for NN regressors.",yes addition near future since worked make sure would function intended update thread add support,issue,positive,positive,positive,positive,positive,positive
643450840,I downgraded my scikit-learn version to 0.22 and things seem to be working now.  Thank you again for the fast and accurate response!,version seem working thank fast accurate response,issue,negative,positive,positive,positive,positive,positive
643323319,"I found this issue when using ipython and I did not find this issue when using jupyter notebook. I pushed a path to development branch to fix the issue and we will release a new minor release with the patch.

You may try the dev branch via the command below:

```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```
",found issue find issue notebook path development branch fix issue release new minor release patch may try dev branch via command shell pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
643148227,It was a related one and there is another related issue #998.,related one another related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
643146201,skrebate didn’t support scikit-learn > 0.22 yet. I linked this issue to [one issue](https://github.com/EpistasisLab/scikit-rebate/issues/65) in skrebate. Please downgrade scikit-learn to 0.22 to use TPOT MDR for now.,support yet linked issue one issue please downgrade use,issue,positive,neutral,neutral,neutral,neutral,neutral
642959781,"Thank you for the very fast response! 

I had suspected `PolynomialFeatures` but had not adjusted the code to force the preprocessor to be chosen.  Upon further investigation, it does seem to be the culprit.  

Thank you also for the link to issue #1062 - it does seem to be the same problem.

",thank fast response suspected code force chosen upon investigation seem culprit thank also link issue seem problem,issue,negative,positive,positive,positive,positive,positive
642934591,"Although TPOT limits only one `PolynomialFeatures` in one pipeline, but it still can use a lot of memory with 5k features in the dataset. Maybe removing this operator from TPOT configuration can solve the issue in your case.

It is also related to issue #1062",although one one pipeline still use lot memory maybe removing operator configuration solve issue case also related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
641355524,"Alternatively, you may use template like `Selector-Classifier` to indirectly select features by using GP to optimize best `Selector`.",alternatively may use template like indirectly select optimize best selector,issue,positive,positive,positive,positive,positive,positive
641351892,"Your understanding is correct. TPOT cannot select features using GA without the static set of features based on _priori_ expert knowledge. Maybe it is a good enhancement function to add into TPOT.

",understanding correct select ga without static set based expert knowledge maybe good enhancement function add,issue,negative,positive,positive,positive,positive,positive
641326337,"Thank you @weixuanfu. Yes, I saw FeatureSetSelector but as I understood it is used to specify a static set of features as opposed to optimize the feature set composition. Am I correct in understanding that TPOT does not select features based on genetic optimization, i.e. binary coding all the features and using genetic programming to find the best feature subset? Thanks.",thank yes saw understood used specify static set opposed optimize feature set composition correct understanding select based genetic optimization binary genetic find best feature subset thanks,issue,positive,positive,positive,positive,positive,positive
641313734,TPOT has a built-in FeatureSetSelector (see [this link](https://epistasislab.github.io/tpot/using/#featuresetselector-in-tpot)) for helping user to select best feature set based on _priori_ expert knowledge. Also you may limit ML hyperparameter space in `config_dict` (like using a static estimator in the dictionary) and fix the pipeline with `template` parameter. ,see link helping user select best feature set based expert knowledge also may limit space like static estimator dictionary fix pipeline template parameter,issue,positive,positive,positive,positive,positive,positive
640719061,Thank you! I think the docs are OK now and the issue can be closed.,thank think issue closed,issue,negative,negative,neutral,neutral,negative,negative
639630666,The docs was just updated. Please let us know if there is any other issue. ,please let u know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
637199387,Thanks! `0.11.5` is up on [anaconda.org](https://anaconda.org/conda-forge/tpot/files?version=0.11.5)... though it will take up to a half hour before it's generally available over CDN.,thanks though take half hour generally available,issue,negative,positive,positive,positive,positive,positive
636375133,"Since the `torch` estimators require the user to explicitly ask for them I think it makes sense to mark it as an optional dependency (and make the appropriate checks before importing the NN estimators).

I’ll make sure to push a fix shortly. ",since torch require user explicitly ask think sense mark optional dependency make appropriate make sure push fix shortly,issue,negative,positive,positive,positive,positive,positive
636259842,We recommend user to use anaconda to install tpot in the [installation guide](https://epistasislab.github.io/tpot/installing/) and `ipywidgets` is [default package in anaconda](https://docs.anaconda.com/anaconda/packages/pkg-docs/).  But if user just installs tpot via pip then this ipywidgets package maybe missing. I think it should be a dependency for `tqdm` package. ,recommend user use anaconda install installation guide default package anaconda user via pip package maybe missing think dependency package,issue,negative,negative,negative,negative,negative,negative
636258638,"Thank you for reporting this issue. We added this dependency [in installation guide](https://epistasislab.github.io/tpot/installing/#conda-forge). 

But I agree with you that installing `pytorch` in different environments is complex and maybe it should be an optional dependency.

@JDRomano2 Could you please add some suggestions about using pytorch in GPU environment on the installation guide? Also, is it possible to make it as an optional dependency?",thank issue added dependency installation guide agree different complex maybe optional dependency could please add environment installation guide also possible make optional dependency,issue,positive,negative,neutral,neutral,negative,negative
636251694,"If running in a notebook, `tqdm` might expect `ipywidgets`... which is the notebook in question? Though I thought it would fall back to plain text output if it couldn't find widgets...",running notebook might expect notebook question though thought would fall back plain text output could find,issue,negative,negative,negative,negative,negative,negative
635958101,Thank you for submitting this issue. I think it should be implemented into log_file. Could you please submit a PR for this fix?,thank issue think could please submit fix,issue,positive,neutral,neutral,neutral,neutral,neutral
632935160,Hi @JDRomano2 Thank you for the PR. Could you please push a commit to fix CI tests? I think it is related to [appveyor.yml](https://github.com/EpistasisLab/tpot/blob/master/.appveyor.yml) and [travis_install.sh](https://github.com/EpistasisLab/tpot/blob/master/ci/.travis_install.sh). I will look into it next week.,hi thank could please push commit fix think related look next week,issue,positive,neutral,neutral,neutral,neutral,neutral
632477313,"Hi @weixuanfu , 

I get an attribute error when I execute the below code

exported_pipeline.steps[0].transform(training_features) 

I was able to see that we have only the below attributes from `exported_pipeline`

![image](https://user-images.githubusercontent.com/30723825/82632227-36319500-9c2a-11ea-8861-78e207f18469.png)

",hi get attribute error execute code able see image,issue,negative,positive,positive,positive,positive,positive
632084306,"```
make_union(
        FunctionTransformer(copy),
        RobustScaler()
    ),
```

The 1st step in the pipeline above combined a copy of input X and transformed input X via `RobustScaler`, which is why the feature number doubled. In this case, you may use exported_pipeline.steps[0].transfrom(training_features) to get the combined input for `RFE`. But feature name information is not available after transformation. You may name the 1st 49 features as feature names in `training_features` and name 2nd 49 transformed features with some new names. ",copy st step pipeline combined copy input input via feature number doubled case may use get combined input feature name information available transformation may name st feature name new,issue,negative,positive,positive,positive,positive,positive
630217404,Could you please shared the stderr about missing `ipywidgets`? I think TPOT does not import ipywidgets when running classifier example.,could please missing think import running classifier example,issue,negative,negative,negative,negative,negative,negative
629491239,"This is a bug in v0.11.2 and was fixed in v0.11.3a0. (See #1067)

Please try this version instead for now.

We will release a minor version after some checks next week.

",bug fixed see please try version instead release minor version next week,issue,negative,positive,neutral,neutral,positive,positive
628632667,I released TPOT v0.11.3a0 with the fix. Please upgrade TPOT or downgrade TPOT to v0.11.1 . I closed the issue for now. Please feel free to reopen this issue if the problem still exists. ,fix please upgrade downgrade closed issue please feel free reopen issue problem still,issue,positive,positive,positive,positive,positive,positive
628625232,Thank you for catching and reporting this bug. I will push a patch soon. ,thank catching bug push patch soon,issue,negative,positive,positive,positive,positive,positive
628054311,This issue should be fixed in TPOT v0.11.1. Please feel free to reopen this issue if there is a unsolved issue related to this one.,issue fixed please feel free reopen issue unsolved issue related one,issue,positive,positive,positive,positive,positive,positive
627998408,"Thank you for report this bug.

Balanced Accuracy in TPOT was described in [Urbanowicz2015](https://scikit-learn.org/stable/modules/model_evaluation.html#urbanowicz2015): the average of sensitivity and specificity is computed for each class and then averaged over total number of classes. It is different with sklearn balanced accuracy.",thank report bug balanced accuracy average sensitivity specificity class total number class different balanced accuracy,issue,negative,negative,neutral,neutral,negative,negative
627984078,Thank you for the PR. I merge this patch for a temp fix.,thank merge patch temp fix,issue,negative,neutral,neutral,neutral,neutral,neutral
627955907,"I think the main problem here is that `weights` in `make_score(y_test, y_pred, weights)` can not be used correctly in K-fold CV since in each fold the samples in `y_test` are different so that weights should be matched to that.  I think it is related to the #1039 and I have a hacky demo that may help you. ",think main problem used correctly since fold different think related hacky may help,issue,negative,positive,neutral,neutral,positive,positive
627749967,"Please use:
from sklearn.model_selection import train_test_split

The earlier LOC is not is use: ",please use import use,issue,negative,neutral,neutral,neutral,neutral,neutral
626444243,"I would likewise be interested in seeing Catboost added in the default configuration; even without categorical variables, I've found it has comparable performance to xgboost after tuning, and frequently outperforms when both are run using default settings. Especially for pipelines with shorter runtimes, it could be a real value added.",would likewise interested seeing added default configuration even without categorical found comparable performance tuning frequently run default especially shorter could real value added,issue,positive,positive,positive,positive,positive,positive
625588180,"This works,thanks
 @weixuanfu 
But why?
If I remove dask-xgboost,this works fine",work thanks remove work fine,issue,positive,positive,positive,positive,positive,positive
625547506,"> > ```
> > 1. can we use time series with TPOT and what transformations could we do for this dateTime column
> > ```
> 
> The input dataset should sort based on date time column.
> 
> > ```
> > 2. for catégorical data should we transform the to numerical or TPOT do that for us
> > ```
> 
> Yes, the categorical should be transform to numerical. You may try `OrdinalEncoder` or `OneHotEncoder` in scikit-learn
> 
> > ```
> > 3. for missing values in numerical cases, we don't need to handle them, is TPOT do these transformations for us.
> > ```
> 
> TPOT should use `SimpleImputer` from scikit-learn to impute the missing values.

thank u so much for your reply
but for Time series, I would make this column as an index sort it and transform to int so I can use it as an input for TPOT",use time series could column input sort based date time column data transform numerical u yes categorical transform numerical may try missing numerical need handle u use impute missing thank much reply time series would make column index sort transform use input,issue,negative,negative,neutral,neutral,negative,negative
625239412,"
>     1. can we use time series with TPOT and what transformations could we do for this dateTime column

The input dataset should sort based on date time column.

>     2. for catégorical data should we transform the to numerical or TPOT do that for us

Yes, the categorical should be transform to numerical. You may try `OrdinalEncoder` or `OneHotEncoder`  in scikit-learn
>     3. for missing values in numerical cases, we don't need to handle them, is TPOT do these transformations for us.

TPOT should use `SimpleImputer` from scikit-learn to impute the missing values.


",use time series could column input sort based date time column data transform numerical u yes categorical transform numerical may try missing numerical need handle u use impute missing,issue,negative,negative,negative,negative,negative,negative
625233889,"Because there is one line for setting joblib's backend via `with joblib.parallel_backend(""dask""):`, How about removing `use_dask=True` from your codes? Like:

```python
 clf = TPOTClassifier(generations=5, population_size=100, cv=5, 
                          random_state=42,verbosity=2, 
                          n_jobs=-1
                         )
with joblib.parallel_backend(""dask""):
    clf.fit(X_train, y_train)
```

",one line setting via removing like python,issue,negative,neutral,neutral,neutral,neutral,neutral
625048541,"> When you say ""generated pipeline,"" do you mean the pipeline code generated via the `export` function?
> 
> Otherwise if by ""generated pipeline"" you mean the ""best"" pipeline discovered during the optimization process, then you can simply use the `score` function and provide it with the test data and labels.
> 
> > And also how to handle Time series using TPOT ( any time series examples making use of TPOT available?)
> 
> TPOT would handle time series the same way that scikit-learn does. You can use [TimeSeriesSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for the `cv` parameter.

there is any new solutions for multivariate time series with TPOT? please",say pipeline mean pipeline code via export function otherwise pipeline mean best pipeline discovered optimization process simply use score function provide test data also handle time series time series making use available would handle time series way use parameter new time series please,issue,positive,positive,positive,positive,positive,positive
625046490,"> I am pulling out data from the Quandl API, and using the stock data to predict future stock prices, the model is a linear model and fits well with training data, but provides a straight line for future data. I have converted the dates to a standard int format before processing so I don't know what I am doing wrong :( , any help is appreciated: like code examples and stuff.....
> 
> ## Process to reproduce the issue
> [ordered list the process to finding and recreating the issue, example below]
> 
> 1. Pull Quandl Data
> 2. Convert Dates to numeric format
> 3. Use the TPOT regressor to fit
> 4. Try predicting future data points and plotting but recieve a linear output :(

how did you transform your column to numerical and what type has before this transformation please",data stock data predict future stock model linear model well training data straight line future data converted standard format know wrong help like code stuff process reproduce issue ordered list process finding issue example pull data convert format use regressor fit try future data plotting linear output transform column numerical type transformation please,issue,positive,positive,neutral,neutral,positive,positive
625046118,"> FYI that looks like a massively overfit model. It probably outputs a flat line on new data because that is the last value is learned to predict at the final time point. You definitely need more features to predict stock price here, but that is outside the purview of TPOT support.
> 
> I suggest Googling ""python stock price prediction"" and there will be dozens of articles covering the topic, including how to integrate additional features into the predictive model.

1. can we use time series with TPOT and what transformations could we do for this dateTime column
2. for catégorical data should we transform the to numerical or TPOT do that for us
3. for missing values in numerical cases, we don't need to handle them, is TPOT do these transformations for us.


Thank u for anyone who will help me with one or all question, I'm working on my graduation project and I need these answers",like massively overfit model probably flat line new data last value learned predict final time point definitely need predict stock price outside purview support suggest python stock price prediction covering topic integrate additional predictive model use time series could column data transform numerical u missing numerical need handle u thank anyone help one question working graduation project need,issue,positive,negative,neutral,neutral,negative,negative
624051948,"I will find the dataset I used to help improve this issue. Thank you!
No, I just study on the TPOT best pipeline.",find used help improve issue thank study best pipeline,issue,positive,positive,positive,positive,positive,positive
623020193,"They are based on our benchmark studies and experience of using those methods. Also general computational resources were considered to avoid adding computational expensive methods/parameters.

There were a few related issues. #535 #252 ",based experience also general computational considered avoid computational expensive related,issue,negative,negative,negative,negative,negative,negative
622162240,"Do you have a benchmark dataset that could be used for such a regression?

At the moment I am also using a custom config file to include `MLPRegressor`. With a fair benchmark dataset or two, things like sensitivity to certain scalers could be validated.

Eventually this could lead to a contribution in the form of another config file, one optimised for neural network regression for example.

Have you tried other selectors and/or transformers?",could used regression moment also custom file include fair two like sensitivity certain could eventually could lead contribution form another file one neural network regression example tried,issue,positive,positive,positive,positive,positive,positive
622136253,"You could make use of the checkpoints feature and scrape it out of the comment for each step.

In the source code that is the [`pareto_front_pipeline_score`](https://github.com/EpistasisLab/tpot/blob/6efc21e4a5b3ab7623fc1ba6508f3df7449f582e/tpot/base.py#L1020) variable.",could make use feature scrape comment step source code variable,issue,negative,neutral,neutral,neutral,neutral,neutral
621804199,"Actually you don't have to pass cat_features if you don't have them, you can use the library without categorical features",actually pas use library without categorical,issue,negative,neutral,neutral,neutral,neutral,neutral
621744382,"Then i think  a reasonable solution would be to make an example notebook with Catboost, but not add it to the default configuration since cat_features and the import need to be coded manually.",think reasonable solution would make example notebook add default configuration since import need manually,issue,negative,positive,positive,positive,positive,positive
621726978,"do the catboost classes, e.g., CatBoostRegressor now derive from the sklearn RegressorMixin?

Afterwards catboost could simply be added to the default configs in tpot.",class derive afterwards could simply added default,issue,negative,neutral,neutral,neutral,neutral,neutral
621724057,So is there a plan to add CatBoost? It now supports text features along with categorical ones.,plan add text along categorical,issue,negative,neutral,neutral,neutral,neutral,neutral
620845326,Thanks! I've turned off my early_stop for now but will keep an eye on the dev branch and install it once fixed. Closing issue for now.,thanks turned keep eye dev branch install fixed issue,issue,negative,positive,positive,positive,positive,positive
620840413,"Thank you a lot for the demo. I just caught the bug and submitted a PR #1058 for a fix. I will merge it to dev branch soon. 

You may try to install the dev branch to ur environment for a test via the command below:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",thank lot caught bug fix merge dev branch soon may try install dev branch ur environment test via command pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
620829735,"Here is a reproducible code, and what I got for an output below it. Looks to be about the 17th generation again:

```
from tpot import TPOTClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd

# Set a random seed
random_state = 42

# Load breast cancer (binary classification) dataset and split
breast_cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target,
                                                    train_size=0.75, test_size=0.25, random_state=random_state, shuffle=True)

# Define TPOT classifier
tpot = TPOTClassifier(generations=100, population_size=1, verbosity=2, random_state=random_state,
                            scoring='roc_auc', cv=10, use_dask=True, n_jobs=-2, early_stop=33)

# Fit/start training
tpot.fit(X_train, y_train)
print('Done training/fitting TPOT session.')

# Get TPOT's score on test set (default metric is 'accuracy'; define something else in TPOT classifier if needed)
print('TPOTs score on test set is...')
print(tpot.score(X_test, y_test))
```

Output is:

```
Generation 1 - Current best internal CV score: 0.9847542735042737
Generation 2 - Current best internal CV score: 0.9880460588793921
Generation 3 - Current best internal CV score: 0.9880460588793921
Generation 4 - Current best internal CV score: 0.9880460588793921
Generation 5 - Current best internal CV score: 0.9880460588793921
Generation 6 - Current best internal CV score: 0.9902285137701803
Generation 7 - Current best internal CV score: 0.9902285137701803
Generation 8 - Current best internal CV score: 0.9902285137701803
Generation 9 - Current best internal CV score: 0.9902285137701803
Generation 10 - Current best internal CV score: 0.9902285137701803
Generation 11 - Current best internal CV score: 0.9904665242165243
Generation 12 - Current best internal CV score: 0.9904665242165243
Generation 13 - Current best internal CV score: 0.9904665242165243
Generation 14 - Current best internal CV score: 0.9904665242165243
Generation 15 - Current best internal CV score: 0.9904665242165243
Generation 16 - Current best internal CV score: 0.9904665242165243
Generation 17 - Current best internal CV score: 0.9904665242165243
Generation 18 - Current best internal CV score: 0.9904665242165243
Generation 19 - Current best internal CV score: 0.9904665242165243
Generation 20 - Current best internal CV score: 0.9910155508072174
Generation 21 - Current best internal CV score: 0.9910155508072174
Generation 22 - Current best internal CV score: 0.9928697768281101
Generation 23 - Current best internal CV score: 0.9928697768281101
Generation 24 - Current best internal CV score: 0.9928697768281101
Generation 25 - Current best internal CV score: 0.9928697768281101
Generation 26 - Current best internal CV score: 0.9928697768281101
Generation 27 - Current best internal CV score: 0.9928697768281101
Generation 28 - Current best internal CV score: 0.9928697768281101
Generation 29 - Current best internal CV score: 0.9928697768281101
Generation 30 - Current best internal CV score: 0.9928697768281101
Generation 31 - Current best internal CV score: 0.9928697768281101
Generation 32 - Current best internal CV score: 0.9928697768281101
Generation 33 - Current best internal CV score: 0.9928697768281101
Generation 34 - Current best internal CV score: 0.9928697768281101
Generation 35 - Current best internal CV score: 0.9928697768281101
Generation 36 - Current best internal CV score: 0.9928697768281101
Generation 37 - Current best internal CV score: 0.9928697768281101
Generation 38 - Current best internal CV score: 0.9928697768281101

The optimized pipeline was not improved after evaluating 33 more generations. Will end the optimization process.

TPOT closed prematurely. Will use the current best pipeline.
```",reproducible code got output th generation import import import import set random seed load breast cancer binary classification split define classifier training print session get score test set default metric define something else classifier print score test set print output generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score pipeline end optimization process closed prematurely use current best pipeline,issue,positive,positive,positive,positive,positive,positive
620821558,"Thank you for reporting this issue. I will look into it.

Hmm, it seems that `Pareto front` did not update after 33 generations. 

Could you please provide a demo to reproduce this issue if it can be reproduced with a public benchmarks?",thank issue look front update could please provide reproduce issue public,issue,positive,neutral,neutral,neutral,neutral,neutral
620601685,"With HPC system, usually we setup dask parallel computing environment firstly and then set TPOTClassifier/TPOTRegessor with `generations=100`, `population_size=100` (sometimes `generations=200`, `population_size=200` or even higher if dataset is smaller), `cv=10` and `use_dask=True` for optimizing pipelines. For search space, we usually use default `config_dict` but sometimes use customized one with some specific operators for feature engineering based on domain knowledge. Hope the recommendations are helpful for your researches.  ",system usually setup parallel environment firstly set sometimes even higher smaller search space usually use default sometimes use one specific feature engineering based domain knowledge hope helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
620131789,"Thanks @weixuanfu  I see what you meant now with the `random_state` being included in the new TPOT classifier, so I took out the `set_param_recursive`.

Here is the full code, working on my end. This code gets an ensemble'd, weighted average of all the predicted 1's from the top 5 pipelines, based on their respective CV scores achieved during training, so the better pipelines get more weight in their predictions. This is for a binary classification problem, where the 1's are more important than the 0's, that's why I wrote it the way I did. And like was discussed before, I wrote it in a way such that one could potentially have a .csv file of 5 previously saved classifiers and still be able to reference those without having to have called the .fit() training first. I also left the default scoring metric 'accuracy', but others may want to change that accordingly. Here is an example of the output I got:

```
-----------------------------------------------------------------------
Ensembled Accuracy Score is...
0.972027972027972
-----------------------------------------------------------------------
Again, the Individual models scored...
    cv_score  ...                                         model_info
8   0.950670  ...  {'generation': 'INVALID', 'mutation_count': 2,...
1   0.950643  ...  {'generation': 0, 'mutation_count': 0, 'crosso...
6   0.945964  ...  {'generation': 'INVALID', 'mutation_count': 3,...
12  0.945910  ...  {'generation': 'INVALID', 'mutation_count': 3,...
0   0.934282  ...  {'generation': 0, 'mutation_count': 0, 'crosso...
```

Your individual results may vary. Let me know if any issues. Thanks! This was fun!

```
from tpot import TPOTClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd

# Set a random seed
random_state = 42

# Load breast cancer (binary classification) dataset and split
breast_cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target,
                                                    train_size=0.75, test_size=0.25, random_state=random_state)

# Define TPOT classifier
tpot_model = TPOTClassifier(generations=9, population_size=2, verbosity=2, random_state=random_state)

# Fit/start training
tpot_model.fit(X_train, y_train)
print('Done training/fitting TPOT session.')

# Get TPOT's score on test set (default metric is 'accuracy'; define something else in TPOT classifier if needed)
print('TPOTs score on test set is...')
print(tpot_model.score(X_test, y_test))

# Export the best pipeline
tpot_model.export('tpot_breast_cancer_pipeline.py')


# An attempt at making an ensemble prediction using the top 5 unique CV scored pipelines

# Create sorted by CV (highest to lowest) dataframe 
# (taken from: https://github.com/EpistasisLab/tpot/issues/703)
my_dict = list(tpot_model.evaluated_individuals_.items())
# Create an empty dataframe to append the model strings, model info strings and CV score strings to
model_scores = pd.DataFrame()
for model in my_dict:
    model_name = model[0]
    model_info = model[1] # You could take this out if the values of the pipeline aren't important to you
    cv_score = model[1].get('internal_cv_score')  # Pull out cv_score as a column (i.e., sortable)
    model_scores = model_scores.append({'model': model_name,
                                        'cv_score': cv_score, # }, # You could take this out if the values of the pipeline aren't important to you
                                        'model_info': model_info,},
                                       ignore_index=True)
# Sort by best CV score to worst (top to bottom)
model_scores = model_scores.sort_values('cv_score', ascending=False)
print('Model Scores dataframe is...')
print(model_scores)

# Remove duplicate CV score rows and keep top X pipelines (to get best, 'unique' pipelines)
model_scores = model_scores.drop_duplicates(subset =""cv_score"", keep = False)
model_scores = model_scores.head(5)
# Export to .csv for inspection if desired
model_scores.to_csv('./top_models.csv', index=False)

# Get the sum of the top 5 CV scores for weighting the 1's later
sum_of_cv_scores = model_scores['cv_score'].sum()

# Generate pipeline objects from model strings in above dataframe 
# (taken from: https://github.com/EpistasisLab/tpot/issues/516)
import numpy as np
import tpot
from deap import creator
from sklearn.model_selection import cross_val_score
from tpot.export_utils import generate_pipeline_code, expr_to_tree
from sklearn.metrics import accuracy_score
from tpot.export_utils import set_param_recursive

# Before we start, create a list full of 0's to append to, and apply addition to, each pipeline's weighted predictions
total_weighted_predictions_list = [0] * len(y_test) # Must match y_test length

# A quick note on the weighted average of the predicted class (i.e. the 1's in the y/labels)
# What I'm doing here is calculating a multiplier that I can apply to the predicted 1's in the following predictions.
# This multiplier is based on each pipeline's CV score from the first training run. The multiplier is:
# current pipeline's CV score from training / sum of all CV scores
# So basically, the better the CV score it produced during training, the higher the multiplier will be for that pipeline.

# Then, every time a pipeline predicts a 1, this (as an example) 0.25 multiplier will be applied to that 1 (resulting
# in a prediction of 0.25 for that row). This will then be added to all the other pipeline's predictions, resulting
# in a final list of added together numbers that range between 0 and 1. Anything over 0.5 is considered a 1.

# I do it this way because in this simple example, I don't really care about the 0's as much as I do about the 1's.
# So we need enough pipelines, or rather, enough of the good pipelines, to have predicted a 1 to sway the ensembled
# prediction closer to 1 than to 0.

# As an example, let's say the top 2 pipelines have the multipliers 0.30 and 0.30. The bottom 3 pipelines have the multipliers
# 0.13, 0.13 and 0.13. If the top 2 pipelines predict a 1, and the bottom 3 predict a 0, the resulting ensembled prediction would be
# an added total of 0.6, making it a final prediction of 1. So the weights of each model are taken into account when predicting
# a 1. This creates a basic ""weighted average"" prediction giving more importance to the better performing models, rather than
# just taking a straight average of all the pipeline's predictions which treats crappy pipeline's predictions as importantly as awesome ones)

# Now that we have a list of the top 5 unique models, iterate over them, make predictions and weight those predictions
for i in range(0, len(model_scores), 1):
    
    # Get the first pipeline in string value from the dataframe
    pipeline_string = model_scores['model'].iloc[i]
    print('pipeline_string is...')
    print(pipeline_string)

    # Get the prediction weight multiplier, based on its ratio of CV Score to Sum of CV Scores
    prediction_weight_multiplier = float(model_scores['cv_score'].iloc[i]) / float(sum_of_cv_scores)
    print('prediction_weight_multiplier is...')
    print(prediction_weight_multiplier)
    
    # Convert pipeline string to actual scikit-learn pipeline object
    tpot = TPOTClassifier(random_state=random_state)
    tpot._fit_init()
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
    sklearn_pipeline = tpot._toolbox.compile(expr=deap_pipeline)
    
    # # print sklearn pipeline string (could comment this section out if you wanted)
    # sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot._pset), tpot.operators)
    # print('sklearn_pipeline_str is...')
    # print(sklearn_pipeline_str)
    
    # Took this section out in favour of the TPOT line above
    # set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)

    # Now : 
    # 1. Re-fit the pipeline to the training data
    # 2. Make a prediction on test data
    # 3. Apply the weight_multiplier to its predictions
    # 4. And combine them to the ensembled_predictions_list, using addition of each element when combining
    sklearn_pipeline.fit(X_train, y_train)
    results = sklearn_pipeline.predict(X_test)
    weighted_results = [i * prediction_weight_multiplier for i in results]
    total_weighted_predictions_list = [x + y for x, y in zip(total_weighted_predictions_list, weighted_results)]

    # Move on to the next pipeline string in the dataframe


# When all top 5 pipelines have made predictions, and we have the weighted average'd predictions,
# create and export a dataframe
ensembled_results_df = pd.DataFrame(y_test, columns=['Actuals'])
ensembled_results_df['Raw_Ensembled_Preds'] = pd.DataFrame(total_weighted_predictions_list)
ensembled_results_df['Converted_Ensembled_Preds'] = np.where(ensembled_results_df['Raw_Ensembled_Preds'] >= 0.5, 1, 0)
ensembled_results_df.to_csv('./ensembled_results.csv', index=False)

print('-----------------------------------------------------------------------')
print('Ensembled Accuracy Score is...')
print(accuracy_score(ensembled_results_df['Converted_Ensembled_Preds'], y_test))
print('-----------------------------------------------------------------------')
print('Again, the Individual models scored...')
print(model_scores.head(5))
```",thanks see meant included new classifier took full code working end code ensemble weighted average top based respective training better get weight binary classification problem important wrote way like wrote way one could potentially file previously saved still able reference without training first also left default scoring metric may want change accordingly example output got accuracy score individual scored individual may vary let know thanks fun import import import import set random seed load breast cancer binary classification split define classifier training print session get score test set default metric define something else classifier print score test set print export best pipeline attempt making ensemble prediction top unique scored create sorted highest taken list create empty append model model score model model model could take pipeline important model pull column sortable could take pipeline important sort best score worst top bottom print print remove duplicate score keep top get best subset keep false export inspection desired get sum top weighting later generate pipeline model taken import import import creator import import import import start create list full append apply addition pipeline weighted must match length quick note weighted average class calculating multiplier apply following multiplier based pipeline score first training run multiplier current pipeline score training sum basically better score produced training higher multiplier pipeline every time pipeline example multiplier applied resulting prediction row added pipeline resulting final list added together range anything considered way simple example really care much need enough rather enough good sway prediction closer example let say top bottom top predict bottom predict resulting prediction would added total making final prediction model taken account basic weighted average prediction giving importance better rather taking straight average pipeline pipeline importantly awesome list top unique iterate make weight range get first pipeline string value print print get prediction weight multiplier based ratio score sum float float print print convert pipeline string actual pipeline object print pipeline string could comment section print print took section line pipeline training data make prediction test data apply combine addition element combining zip move next pipeline string top made weighted average create export print print accuracy score print print print individual scored print,issue,positive,positive,positive,positive,positive,positive
619991478,"Hi, you need call tpot._fit_init() to allow new tpot object (like tpot=TPOTClassifier(random_state=42)) to initialize  `tpot._toolbox` and `tpot._pset`. You can add `set_param_recursive` into your codes to reset `random_state` even enough `tpot._toolbox.compil` function can do that if random_state is set in `TPOTClassifier` (check these [lines](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1195-L1197)). `set_param_recursive` can be imported from `tpot.export_utils`.",hi need call allow new object like initialize add reset even enough function set check,issue,positive,positive,neutral,neutral,positive,positive
619724927,"Alright I'm very close to a solution but need two more things to help complete it.

@weixuanfu  What I'm doing is, running the `tpot.fit()` function, and exporting a dataframe of the the pipeline's CV scores, and string values of those pipelines. I'm then referring to that dataframe only (not the tpot session itself) to convert the pipeline string values to actual sklearn pipelines. I'm getting an error at the :

`deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)`

line because `tpot._pset` SHOULD be referencing the TPOT session that was used for the initial fitting, but I have the initial TPOT session renamed to `tpot_model` instead. My goal here is to be able to run the second half of this script at any time, without having to fit pipelines again first. So theoretically a user could have a pre-saved .csv file of the top 5 model strings and be able to re-run the script from that point. This poses two problems that I need some guidance on:

1. How to get that `tpot._pset` to run without prior fitting? (I noticed in #516 that we should run `tpot._fit_init()` first, but I'm not sure what to do there? You'll see it's in there but commented out with a bunch of ??????'s beside it.)

2. I'm still wanting to include the `set_param_recursive` after a pipeline object has been created from a string because, normally in the exported pipeline .py file, there is an attribute set if the user defined a `random_state` during training, so I'm trying to emulate that with the `set_param_recursive`, because we are simulating here that we don't have access to the initial TPOT `.fit()` session where it is defined. Am I correct in doing this? I want the imported pipeline strings to make use of the `random_state` provided that the user knows it already. If that makes sense?

Thanks! Code is below:

```
from tpot import TPOTClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import pandas as pd

# Load breast cancer (binary classification) dataset and split
breast_cancer = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(breast_cancer.data, breast_cancer.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

# Define TPOT classifier
tpot_model = TPOTClassifier(generations=24, population_size=2, verbosity=2, random_state=42)

# Fit/start training
tpot_model.fit(X_train, y_train)
print('Done training/fitting TPOT session.')

# Get TPOT's score on test set (default metric is 'accuracy'; define something else in TPOT classifier if needed)
print('TPOTs score on test set is...')
print(tpot_model.score(X_test, y_test))

# Export the best pipeline
tpot_model.export('tpot_breast_cancer_pipeline.py')


# An attempt at making an ensemble prediction using the top 5 unique CV scored pipelines

# Create sorted by CV (highest to lowest) dataframe 
# (taken from: https://github.com/EpistasisLab/tpot/issues/703)
my_dict = list(tpot_model.evaluated_individuals_.items())
# Create an empty dataframe to append the model strings, model info strings and CV score strings to
model_scores = pd.DataFrame()
for model in my_dict:
    model_name = model[0]
    #model_info = model[1] # You could take this out if the values of the pipeline aren't important to you
    cv_score = model[1].get('internal_cv_score')  # Pull out cv_score as a column (i.e., sortable)
    model_scores = model_scores.append({'model': model_name,
                                        'cv_score': cv_score,}, # See above comment. Took out for now
                                        #'model_info': model_info,},
                                       ignore_index=True)
# Sort by best CV score to worst (top to bottom)
model_scores = model_scores.sort_values('cv_score', ascending=False)
print('Model Scores dataframe is...')
print(model_scores)

# Remove duplicate CV score rows and keep top X pipelines (to get best, 'unique' pipelines)
model_scores = model_scores.drop_duplicates(subset =""cv_score"", keep = False)
model_scores = model_scores.head(5)
# Export to .csv for inspection if desired
model_scores.to_csv('./top_models.csv', index=False)

# Get the sum of the top 5 CV scores for weighting the 1's later
sum_of_cv_scores = model_scores['cv_score'].sum()

# Generate pipeline objects from model strings in above dataframe 
# (taken from: https://github.com/EpistasisLab/tpot/issues/516)
import numpy as np
import tpot
from deap import creator
from sklearn.model_selection import cross_val_score
from tpot.export_utils import generate_pipeline_code, expr_to_tree
from sklearn.metrics import accuracy_score
from tpot.export_utils import set_param_recursive

# Before we start, create a list full of 0's to append to, and apply addition to, each pipeline's weighted predictions
total_weighted_predictions_list = [0] * len(y_test) # Must match y_test length

# A quick note on the weighted average of the predicted class (i.e. the 1's in the y/labels)
# What I'm doing here is calculating a multiplier that I can apply to the predicted 1's in the following predictions.
# This multiplier is based on each pipeline's CV score from the first training run. The multiplier is:
# current pipeline's CV score from training / sum of all CV scores
# So basically, the better the CV score it produced during training, the higher the multiplier will be for that pipeline.

# Then, every time a pipeline predicts a 1, this (as an example) 0.25 multiplier will be applied to that 1 (resulting
# in a prediction of 0.25 for that row). This will then be added to all the other pipeline's predictions, resulting
# in a final list of added together numbers that range between 0 and 1. Anything over 0.5 is considered a 1.

# I do it this way because in this simple example, I don't really care about the 0's as much as I do about the 1's.
# So we need enough pipelines, or rather, enough of the good pipelines, to have predicted a 1 to sway the ensembled
# prediction closer to 1 than to 0.

# As an example, let's say the top 2 pipelines have the multipliers 0.30 and 0.30. The bottom 3 pipelines have the multipliers
# 0.13, 0.13 and 0.13. If the top 2 pipelines predict a 1, and the bottom 3 predict a 0, the resulting ensembled prediction would be
# an added total of 0.6, making it a final prediction of 1. So the weights of each model are taken into account when predicting
# a 1. This creates a basic ""weighted average"" prediction giving more importance to the better performing models, rather than
# just taking a straight average of all the pipeline's predictions which treats crappy pipeline's predictions as importantly as awesome ones)

# Now that we have a list of the top 5 unique models, iterate over them, make predictions and weight those predictions
for i in range(0, len(model_scores), 1):
    
    # Get the first pipeline in string value from the dataframe
    pipeline_string = model_scores['model'].iloc[i]
    print('pipeline_string is...')
    print(pipeline_string)

    # Get the prediction weight multiplier, based on its ratio of CV Score to Sum of CV Scores
    prediction_weight_multiplier = float(model_scores['cv_score'].iloc[i]) / float(sum_of_cv_scores)
    print('prediction_weight_multiplier is...')
    print(prediction_weight_multiplier)
    
    # Convert pipeline string to actual scikit-learn pipeline object
    # tpot._fit_init() # ??????
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
    sklearn_pipeline = tpot._toolbox.compile(expr=deap_pipeline)
    
    # # print sklearn pipeline string (could comment this section out if you wanted)
    # sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot._pset), tpot.operators)
    # print('sklearn_pipeline_str is...')
    # print(sklearn_pipeline_str)
    
    # Set the same random state that was used during training. This will also help if you run this script without
    # having used the .fit() function first.
    set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)

    # Now : 
    # 1. Re-fit the pipeline to the training data
    # 2. Make a prediction on test data
    # 3. Apply the weight_multiplier to its predictions
    # 4. And combine them to the ensembled_predictions_list, using addition of each element when combining
    sklearn_pipeline.fit(X_train, y_train)
    results = sklearn_pipeline.predict(X_test)
    weighted_results = [i * prediction_weight_multiplier for i in results]
    total_weighted_predictions_list = [x + y for x, y in zip(total_weighted_predictions_list, weighted_results)]

    # Move on to the next pipeline string in the dataframe


# When all top 5 pipelines have made predictions, and we have the weighted average'd predictions,
# create and export a dataframe
ensembled_results_df = pd.DataFrame(y_test, columns=['Actuals'])
ensembled_results_df['Raw_Ensembled_Preds'] = pd.DataFrame(total_weighted_predictions_list)
ensembled_results_df['Converted_Ensembled_Preds'] = np.where(ensembled_results_df['Raw_Ensembled_Preds'] >= 0.5, 1, 0)
ensembled_results_df.to_csv('./ensembled_results.csv', index=False)

print('-----------------------------------------------------------------------')
print('Ensembled Accuracy Score is...')
print(accuracy_score(ensembled_results_df['Converted_Ensembled_Preds'], y_test))
print('-----------------------------------------------------------------------')
print('Compare this with the printed table above (scroll up).')
```",alright close solution need two help complete running function pipeline string session convert pipeline string actual getting error line session used initial fitting initial session instead goal able run second half script time without fit first theoretically user could file top model able script point two need guidance get run without prior fitting run first sure see bunch beside still wanting include pipeline object string normally pipeline file attribute set user defined training trying emulate access initial session defined correct want pipeline make use provided user already sense thanks code import import import import load breast cancer binary classification split define classifier training print session get score test set default metric define something else classifier print score test set print export best pipeline attempt making ensemble prediction top unique scored create sorted highest taken list create empty append model model score model model model could take pipeline important model pull column sortable see comment took sort best score worst top bottom print print remove duplicate score keep top get best subset keep false export inspection desired get sum top weighting later generate pipeline model taken import import import creator import import import import start create list full append apply addition pipeline weighted must match length quick note weighted average class calculating multiplier apply following multiplier based pipeline score first training run multiplier current pipeline score training sum basically better score produced training higher multiplier pipeline every time pipeline example multiplier applied resulting prediction row added pipeline resulting final list added together range anything considered way simple example really care much need enough rather enough good sway prediction closer example let say top bottom top predict bottom predict resulting prediction would added total making final prediction model taken account basic weighted average prediction giving importance better rather taking straight average pipeline pipeline importantly awesome list top unique iterate make weight range get first pipeline string value print print get prediction weight multiplier based ratio score sum float float print print convert pipeline string actual pipeline object print pipeline string could comment section print print set random state used training also help run script without used function first pipeline training data make prediction test data apply combine addition element combining zip move next pipeline string top made weighted average create export print print accuracy score print print print printed table scroll,issue,positive,positive,positive,positive,positive,positive
619551298,"You solution looks good. It is noted that the demo in #516 is out of date but I think only one line `tpot._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)` need to be commented/deleted because we move it into `tpot._toolbox.compile()` function.",solution good noted date think one line need move function,issue,positive,positive,positive,positive,positive,positive
619444509,"I may have a sort of solution that I will work on. 

I noticed that someone on this post #703 , created a way to take the current session's `evaluated_pipelines_` and sort them by their CV scores. This creates a dataframe that has the string value of the pipeline used. Then, one could use the solution shown in #516 to then convert those string values into actual pipelines to run future `.predict` functions on. I'm going to take a stab at it and see how I do.

Theoretically, what I'd do is: 
1. Sort all the pipelines in `evaluated_pipelines_` from highest to lowest CV scores, like what's shown in 703.
2. Remove duplicates based on the CV column (because I don't want a bunch of pipelines that are showing the same score for fear that future `.predict`'s would just predict the same values for each pipeline. I'd want a mix of predictions from different models of the top 5 pipelines)
3. Store the string values of each of the top 5 model architectures, and their CV scores, in a list (or two)
4. For Loop/Iterate over those lists, converting each model arch string to back to an actual pipeline, like what's shown in 516
5. Make predictions with each pipeline and weight them according to their CV scores in the list

I am using a binary (1, 0) classification example so the last step will be easy. Not sure how one would do it for multi-class as most of the work I've done to date has been with binary classifications.

If there's interest I will post my progress here; likely having something to show within the week, unless someone beats me to it, or someone spots something that might give me grief in the process. Thanks!

",may sort solution work someone post way take current session sort string value pipeline used one could use solution shown convert string actual run future going take stab see theoretically sort highest like shown remove based column want bunch showing score fear future would predict pipeline want mix different top store string top model list two converting model arch string back actual pipeline like shown make pipeline weight according list binary classification example last step easy sure one would work done date binary interest post progress likely something show within week unless someone someone something might give grief process thanks,issue,positive,positive,neutral,neutral,positive,positive
619427404,"Just to be clear, if I specify ""use_dask=True"" for TPOT API do I still need to specify ""memory='auto' "" if I want caching between transformers? That is, does using dask with TPOT ensure models with the same configurations aren't recomputed?",clear specify still need specify want ensure,issue,positive,positive,positive,positive,positive,positive
619020371,Also thank you for the suggestion. I think we need more meaningful error message. Any contribution is welcome.,also thank suggestion think need meaningful error message contribution welcome,issue,positive,positive,positive,positive,positive,positive
619019751,"It seems that the output of `CountVectorizer` is a sparse matrix and which may cause some incompatibility issue here. Maybe X_train need to be transformed via `CountVectorizer` first and then fitted TPOT with a transformed X_train (e.g. trans_X_train). And please make sure trans_X_train is 2-D np.ndarray and y_train is a 1-D np.ndarray.

```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
trans_X_train = vectorizer.fit_transform(X_train).toarray()
print(type(trans_X_train))
y_train = le.transform(y_train)

tpot.fit(trans_X_train, y_train)
```",output sparse matrix may cause incompatibility issue maybe need via first fitted please make sure python import print type,issue,positive,positive,positive,positive,positive,positive
617969609,"thanks for the quick install upgrade.  was worried i might bring a bug from dev into our production system, but if coverage is good in dev, then we'll go with that until next minor release.  Thanks for your response times on this, really appreciate TPOT and the work y'all do on it",thanks quick install upgrade worried might bring bug dev production system coverage good dev go next minor release thanks response time really appreciate work,issue,positive,positive,positive,positive,positive,positive
617879574,"Hmm, why not just using dev branch? Does dev branch will also break your codebase?

Just FYI, you may try to install the dev branch to ur environment via the command below:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```

We eventually will merge dev branch to master branch after more testing as a minor release. ",dev branch dev branch also break may try install dev branch ur environment via command pip install upgrade eventually merge dev branch master branch testing minor release,issue,negative,negative,neutral,neutral,negative,negative
617825450,Can we go ahead and merge just this fix into master? It's breaking our codebase when we go from scoring in cross-validation to fitting on our full dataset,go ahead merge fix master breaking go scoring fitting full,issue,negative,positive,positive,positive,positive,positive
617781497,Thank you for reporting the bug and suggestion. I just submitted a PR for this issue and will merge it the dev branch. ,thank bug suggestion issue merge dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
616590837,Thank you for the PR. I will merge it to dev branch firstly. ,thank merge dev branch firstly,issue,negative,positive,positive,positive,positive,positive
616590290,"
[![Coverage Status](https://coveralls.io/builds/30196765/badge)](https://coveralls.io/builds/30196765)

Coverage decreased (-0.02%) to 96.747% when pulling **7baa0c74d8f386b7f9794aaa1d93ac78122a4d58 on ckastner:master** into **704cda22d3c78dfa5ed5160e0f5edca1efd12aba on EpistasisLab:development**.
",coverage status coverage master development,issue,negative,neutral,neutral,neutral,neutral,neutral
616541933,Thank you for digging into this issue. Please submit a PR for increasing their relative tolerances for Debian arm64 and ppc64el (as the patch).,thank digging issue please submit increasing relative arm patch,issue,positive,neutral,neutral,neutral,neutral,neutral
616540207,"@Jocker1980 for the pipeline with the more complex selection, I suggest that try the [permutation_importance](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) function with the pipeline to get permutation feature importance.
",jocker pipeline complex selection suggest try function pipeline get permutation feature importance,issue,negative,negative,negative,negative,negative,negative
616483529,"@weixuanfu 
Hi,
How to get the features importance with more complex selection in the pipeline ?
Here is my code, but using Tpot there are so many complex pipeline. Is there a generic manner ?

``` python

from sklearn.decomposition import PCA
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import HuberRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler

# Average CV score on the training set was:-3.3020662381360717
exported_pipeline = make_pipeline(
    MinMaxScaler(),
    VarianceThreshold(threshold=0.005),
    PCA(iterated_power=7, svd_solver=""randomized""),
    HuberRegressor(alpha=0.037700000000000004, epsilon=1.175, max_iter=100)
)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)

```

Thanks for your help.",hi get importance complex selection pipeline code many complex pipeline generic manner python import import import import import average score training set thanks help,issue,positive,negative,neutral,neutral,negative,negative
616265545,"yes,it's work now.thank you very very much.
















--

祝工作顺利，事事顺心！
 
此致，
 
朱密元




At 2020-04-18 18:26:12, ""nivort"" <notifications@github.com> wrote:

@zhuwater If the file name of your script is ""tpot.py"", try to change it to others like 'tpottest.py'. This could be one of the reasons that lead to the problem.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or unsubscribe.",yes work much wrote file name script try change like could one lead problem reply directly view,issue,negative,positive,positive,positive,positive,positive
615918440,"I'm facing the same issue. Apparently an exception in an worker is leading to a KillerWorker exception being raised, which is leading  to a mistimed ""RuntimeError: A pipeline has not yet been optimized. Please call fit() first."" being raised, and then the whole dask machinery is shut down. I'm attaching the relevant portion of the logs and the installed packages (I'm using anaconda inside docker).
[log-excerpt.txt](https://github.com/EpistasisLab/tpot/files/4497295/log-excerpt.txt)
[conda3.list.txt](https://github.com/EpistasisLab/tpot/files/4497297/conda3.list.txt)

",facing issue apparently exception worker leading exception raised leading pipeline yet please call fit first raised whole machinery shut relevant portion anaconda inside docker,issue,positive,positive,positive,positive,positive,positive
615837878,"@zhuwater  If the file name of your script is ""tpot.py"", try to change it to others like 'tpottest.py'. This could be one of the reasons that lead to the problem.",file name script try change like could one lead problem,issue,negative,neutral,neutral,neutral,neutral,neutral
615238381,"Could you please try to import `TPOT Classifier` a Python interactive console without pycharm? If it works, it seems the python environment of pycharm was not configured correctly. Please check this [link](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#) for details about setting the environment variable.",could please try import classifier python interactive console without work python environment correctly please check link setting environment variable,issue,positive,neutral,neutral,neutral,neutral,neutral
615003673,"   thank you!
   I installed TPOT(version 0.11.1) by pycharm of project interpreter. And,when i do 'from tpot importTPOTClassifier' and run ,the 'ImportError: cannot import name 'TPOTClassifier' from 'tpot' ' is appeared.
   then i uninstalled TPOT,and installed by pip install.The tpot is in the pip list.When i open the pycharm,it is still not work.the the 'ImportError: cannot import name 'TPOTClassifier' from 'tpot' ' is appeared again.
  my python version is 3.7.
  And i don't known why.

















--

祝工作顺利，事事顺心！
 
此致，
 
朱密元




在 2020-04-16 21:15:32，""Weixuan Fu"" <notifications@github.com> 写道：

Could you please provide more details about this issue, like environment info? How did you install tpot?

Maybe reinstalling tpot into another conda environment can solve this issue.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or unsubscribe.",thank version project interpreter run import name uninstalled pip pip open still import name python version known fu could please provide issue like environment install maybe another environment solve issue thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
614650521,"This issue is related to issue #919. Please check a possible solution via a custom `cv` setting there. 

TPOT does not provide this option by default since we think CV can help to avoid overfitting issue.",issue related issue please check possible solution via custom setting provide option default since think help avoid issue,issue,positive,neutral,neutral,neutral,neutral,neutral
614645415,"Could you please provide more details about this issue, like environment info? How did you install tpot? 

Maybe reinstalling tpot into another conda environment can solve this issue. ",could please provide issue like environment install maybe another environment solve issue,issue,positive,neutral,neutral,neutral,neutral,neutral
613668529,"You might be running Python in a different environment when in JupyterLab. Try running:
`!pip install tpot`
from a cell in your notebook.",might running python different environment try running pip install cell notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
613623418,@weixuanfu  This works perfectly. Thank you very much. Its giving me a performance auc of around 0.74. Do you have any recommendations on how I can do better please? Thank you. ,work perfectly thank much giving performance around better please thank,issue,positive,positive,positive,positive,positive,positive
613447801,"I checked the notebook, the training process is not right. Like the code box below:

```python
tpot = TPOTClassifier(verbosity=2, max_time_mins=2, max_eval_time_mins=0.04, population_size=40)
t = train[""Class""].values
tpot.fit(train, t)
```
It means that the target column ""Class"" was in the `train` dataframe, which is not right to put dependent target column in `X`. You should change it to

```python
tpot = TPOTClassifier(verbosity=2, max_time_mins=2, population_size=40)
t = train[""Class""].values
X = train.drop(""Class"", axis=1).values
tpot.fit(X, t)
```

Also for using exported codes, you set all the class code to 0 in the box below. And then fit the model via `exported_pipeline.fit(test, test[""Class""].values)` so that is the reason why the predictions of test set are all 0.

```python
test = pd.read_csv(""test.csv"")
test['Class'] = [0]*119748
```

The correct way is to refit the exported model on train set (note: please also drop ""Class"" before the call `exported_pipeline.fit(X, y)`) and then predict it on test set.
",checked notebook training process right like code box python train class train target column class train right put dependent target column change python train class class also set class code box fit model via test test class reason test set python test test correct way refit model train set note please also drop class call predict test set,issue,positive,positive,positive,positive,positive,positive
613352068,"Hi,
 When we pass sample weights into tpot = TPOTClassifier().fit(sample_weight=sample_weights_train). These weights are only used in the Classifier or it is also used by Selector and Transformer (if applicable). I was going through top 5 evaluated_individuals_ based on internal CV score. I found one of them was -
make_pipeline(
    RFE(estimator=ExtraTreesClassifier(criterion=""gini"", max_features=0.35, n_estimators=100), 
           step=0.1),
    FeatureAgglomeration(affinity=""euclidean"", linkage=""average""),
    DecisionTreeClassifier(criterion=""gini"", max_depth=10, min_samples_leaf=4, 
                min_samples_split=19)
                   )
In above pipeline selector step usage ExtraTreesClassifier that can use sample_weights.  When I am calling fit method on above pipeline, Should I pass sample weights to both ExtraTreesClassifier and
DecisionTreeClassifier ? ",hi pas sample used classifier also used selector transformer applicable going top based internal score found one average pipeline selector step usage use calling fit method pipeline pas sample,issue,positive,positive,positive,positive,positive,positive
613181763,"https://colab.research.google.com/drive/1SapPBGJu-Woa9PE39ug1ckhWWUF0PLZ3
Please find my colab notebook here. 
I tried increasing the max_eval_time_mins but still the same result. 
And Yes, the missing column is the target column, so, I have merged it with the original prediction file for tpot. I think that could be the issue but I'm not sure what to do. ",please find notebook tried increasing still result yes missing column target column original prediction file think could issue sure,issue,positive,positive,positive,positive,positive,positive
613148505,"Could you please provide more details about this issue like the codes that you used or error messages? 
My first guess is that `max_eval_time_mins=0.04` may be too small for your dataset. Please reset it to default value 5 and then have a try.

Also, why the test set has 1 less column than the training set? Is the missing column a target column?",could please provide issue like used error first guess may small please reset default value try also test set le column training set missing column target column,issue,positive,negative,neutral,neutral,negative,negative
612937954,"I just made a PR and merged it into development branch of TPOT to add this kernel support. You may try the dev branch via the command below:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",made development branch add kernel support may try dev branch via command pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
612925360,"To my knowledge, `sklearn` does NOT support such a thing for now so that TPOT do not support this usage. But there is a hacky way to do that. Below is a demo for use sample weight in a custom scorer. 


```python
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import make_scorer, log_loss
import numpy as np
import pandas as pd

X, y = load_iris(return_X_y=True)
index = ['r%d' % x for x in range(len(y))]
y_frame = pd.DataFrame(y, index=index)
sample_weight = np.array([1 + 100 * (i % 25) for i in range(len(X))])
sample_weight_frame = pd.DataFrame(sample_weight, index=index)

def score_f(y_true, y_pred, sample_weight):
    return log_loss(y_true.values, y_pred,
                    sample_weight=sample_weight.loc[y_true.index.values].values.reshape(-1),
                    normalize=True)

score_params = {""sample_weight"": sample_weight_frame}
my_scorer = make_scorer(score_f,
                        greater_is_better=False, 
                        needs_proba=True, 
                        needs_threshold=False,
                        **score_params)

tpot = TPOTClassifier(generations=5, population_size=50, 
                      verbosity=3, 
                      scoring=my_scorer,
                      random_state=42)
tpot.fit(X, y_frame)
```",knowledge support thing support usage hacky way use sample weight custom scorer python import import import import import import index range range return,issue,positive,neutral,neutral,neutral,neutral,neutral
612908765,"Thank you for reporting this bug. This info was set to ""INVALID"" when the pipeline was generated ([this line](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1554) and [this one](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1500)). But it should be reset to the generation that the pipeline was evaluated based on [those lines](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L245). The bug is that those lines should update pipelines in `offsprings` instead of `population`. I will make a PR and make a fix for dev branch soon.",thank bug set invalid pipeline line one reset generation pipeline based bug update instead population make make fix dev branch soon,issue,negative,neutral,neutral,neutral,neutral,neutral
612031015,Thank you for the PR. I will merge it to dev branch.,thank merge dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
612025112,"> Hi,
> Should I resolve that conflict in base.py?

 I have resolved it. Thank you!",hi resolve conflict resolved thank,issue,positive,neutral,neutral,neutral,neutral,neutral
611305709,Ok. I tried it and passed all tests. Please take a look. Thanks.,tried please take look thanks,issue,positive,positive,positive,positive,positive,positive
609933947,"Thank you for looking into it.

The issue is that there is no xgboost in the test environment. We intentionally let tpot raise that import error when verbosity=3. 

I think in the CI envs of TPOT unit tests ([Linux](https://github.com/EpistasisLab/tpot/blob/master/ci/.travis_install.sh#L41) and [Windows](https://github.com/EpistasisLab/tpot/blob/master/.appveyor.yml#L23)), xgboost should be installed so that this error should not be raised. 

Could you please build a similar conda env with xgboost for testing unit test with verbosity=3?",thank looking issue test environment intentionally let raise import error think unit error raised could please build similar testing unit test,issue,negative,neutral,neutral,neutral,neutral,neutral
609364754,"> Hi,
> I added a file to test verbosity from 1-3. In case of verbosity as 3, an error was raised. It seems it uses xgboost when verbosity is 3.
> 
> `Traceback (most recent call last): File ""/app/tpot/operator_utils.py"", line 80, in source_decode exec('from {} import {}'.format(import_str, op_str)) File ""<string>"", line 1, in <module> ModuleNotFoundError: No module named 'xgboost'`
> 
> I am not sure the reason so I just commented out the last unit test. And it also happens in master branch. @weixuanfu Could you please help to answer this? If it's an issue, I can try to figure out it.

Hi,

I found the reason why it raise exception only in case of verbosity as 3. 

```
    tmp_path = sourcecode.split('.')
    op_str = tmp_path.pop()
    import_str = '.'.join(tmp_path)
    try:
        if sourcecode.startswith('tpot.'):
            exec('from {} import {}'.format(import_str[4:], op_str))
        else:
            exec('from {} import {}'.format(import_str, op_str))
        op_obj = eval(op_str)
    except Exception as e:
        if verbose > 2:
            raise ImportError('Error: could not import {}.\n{}'.format(sourcecode, e))
        else:
            print('Warning: {} is not available and will not be used by TPOT.'.format(sourcecode))
        op_obj = None

    return import_str, op_str, op_obj
```
I think it can be modified as printing warning simply instead of raising error. I can put my unit test 3 back as well. Any suggestion is welcome. :)",hi added file test verbosity case verbosity error raised verbosity recent call last file line import file string line module module sure reason last unit test also master branch could please help answer issue try figure hi found reason raise exception case verbosity try import else import except exception verbose raise could import else print available used none return think printing warning simply instead raising error put unit test back well suggestion welcome,issue,positive,positive,positive,positive,positive,positive
609060457,"Hi,
I added a file to test verbosity from 1-3. In case of verbosity as 3, an error was raised. It seems it uses xgboost when verbosity is 3. 

`
Traceback (most recent call last):
  File ""/app/tpot/operator_utils.py"", line 80, in source_decode
    exec('from {} import {}'.format(import_str, op_str))
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'xgboost'
`

I am not sure the reason so I just commented out the last unit test. And it also happens in master branch. @weixuanfu Could you please help to answer this? If it's an issue, I can try to figure out it.",hi added file test verbosity case verbosity error raised verbosity recent call last file line import file string line module module sure reason last unit test also master branch could please help answer issue try figure,issue,positive,positive,positive,positive,positive,positive
608505597,"> Thank you for the PR. Could you please provide small examples (with population=20, generations=10) of this log file when verbosity=1, 2, 3?

Of course I can. I am wondering that I should write an example and take screenshots or write an unit test. How do you think?",thank could please provide small log file course wondering write example take write unit test think,issue,positive,negative,negative,negative,negative,negative
606599413,Thank you for catching this error. I fixed it and will update docs soon. Please feel free to reopen this issue if you have more questions/issues.,thank catching error fixed update soon please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
605640020,I just fixed the link. Please feel free to reopen this issue if you have any questions/issues.,fixed link please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
605632300,Thank you for catching this error. The link should be `https://github.com/EpistasisLab/tpot/tree/master/tests`,thank catching error link,issue,negative,positive,positive,positive,positive,positive
605016875,"Thank you, this would work well.",thank would work well,issue,positive,neutral,neutral,neutral,neutral,neutral
605015933,"Hmm, about x0, I think it is about `config_dict` parameter, please check [this link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for example usage. 

And [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py#L101-L108) is xgboost's configuration in default TPOTClassifier. 

For y0, I think it is about `scoring` parameter but TPOT does not support a list of scores for now.

",think parameter please check link example usage configuration default think scoring parameter support list,issue,positive,neutral,neutral,neutral,neutral,neutral
605007791,"Sure. If I have some parameter combinations for a specific algorithm, such as XGBoost, x0_xgboost, and their resulting evaluations, y0_xgboost, I would like to submit these to the GA to help improve the search over xgboost. I could have the same over a set of sklearn algorithms. It would be useful to use these to improve the TPOT search. 

It could be submitted as a dictionary,
```
evals = {""model"":xgbclassifier,
             ""x0"":list of dictionary_of_parameter_combinations,
             ""y0"":list of scores}
```

Then I could call tpot like so,
```
tpot = TPOTClassifier(generations=5, 
                                    population_size=10,
                                    evals=evals)
```",sure parameter specific algorithm resulting would like submit ga help improve search could set would useful use improve search could dictionary model list list could call like,issue,positive,positive,positive,positive,positive,positive
605003642,"Hmm, I don't understand what x0 and y0 stand for? Could you explain a little bit more?",understand stand could explain little bit,issue,negative,negative,negative,negative,negative,negative
605002242,"The API doesn't seem to offer this. It would be nice. Something like this,

```
tpot = TPOTClassifier(generations=5, population_size=10,
                                    x0=params,
                                    y0=score_evaluations)
```",seem offer would nice something like,issue,positive,positive,positive,positive,positive,positive
602293701,I found Issue #664 which advises to use n_jobs=1. That works for me for now.,found issue use work,issue,negative,neutral,neutral,neutral,neutral,neutral
602289955,"Hi, I'm running into the same issue. What was your solution?",hi running issue solution,issue,negative,neutral,neutral,neutral,neutral,neutral
602056320,"标准化未实现，遇到相同的问题，已经解决，标准化一下训练集和测试集就好了。
Standardization is not implemented, encountered the same problem has been solved, standardized training and test sets look just fine.",standardization problem standardized training test look fine,issue,negative,positive,positive,positive,positive,positive
601352056,"Thank you very much @weixuanfu for this nice addition, it's really useful.

The error with DotProduct was specific to the digits dataset, as this kernel works with my data.",thank much nice addition really useful error specific kernel work data,issue,positive,positive,positive,positive,positive,positive
601205411,"Thank you for digging into this issue.

I just made a PR #1032 and merged it into development branch of TPOT to add this kernel support. You may try the dev branch via the command below:

```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```

Here is a [demo](https://gist.github.com/weixuanfu/ede0772ed578a8d31e8e18afc1e6e8bf) for specifying kernel with GaussianProcessRegressor. 

It is noted that I tried to use `DotProduct` for a demo but somehow I got a error message in the end of TPOT process (` (""The kernel, DotProduct(sigma_0=1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator."", '1287-th leading minor of the array is not positive definite')`) so that the final model from TPOT cannot be fitted on the whole training dataset somehow. But maybe you won't see this error message for using it on another dataset.


",thank digging issue made development branch add kernel support may try dev branch via command shell pip install upgrade kernel noted tried use somehow got error message end process kernel positive definite matrix try gradually increasing parameter estimator leading minor array positive definite final model fitted whole training somehow maybe wo see error message another,issue,positive,positive,neutral,neutral,positive,positive
600641017,"Okay. Thinking about what would be needed for TPOT, running a test using a custom configuration including another estimator e.g.:
```python
tpot_config = {
    'sklearn.gaussian_process.GaussianProcessRegressor': {
        'kernel': {
            'sklearn.gaussian_process.kernels.DotProduct': { 
            }
        }
    },
    'sklearn.ensemble.AdaBoostRegressor': {
    }
}
```
The output (with verbosity=3) changes to:
```
_pre_test decorator: _random_mutation_operator: num_test=0 Cannot clone object '<class 'sklearn.gaussian_process.kernels.DotProduct'>' (type <class 'abc.ABCMeta'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' methods..
```
The DotProduct kernel isn't a sklearn estimator though it does have a get_params method, which it inherits from the base class [Kernel](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/gaussian_process/kernels.py#L120), which itself inherits from abc.ABCMeta. This get_params method is slightly different to that use for the [BaseEstimator](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/base.py#L173).

Would the Gaussian process kernels need to inherit from the BaseEstimator for kernel specification to work (e.g. as the [Nystroem](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841/sklearn/kernel_approximation.py#L432) kernel does)?

Or is there a different solution?",thinking would running test custom configuration another estimator python output decorator clone object class type class seem estimator implement kernel estimator though method base class kernel method slightly different use would process need inherit kernel specification work kernel different solution,issue,negative,negative,negative,negative,negative,negative
599973396,"In my case the problem was resolved when I remove de max_time_mins parametter, now I'm traing to change it to higher value.",case problem resolved remove de change higher value,issue,negative,positive,positive,positive,positive,positive
598901314,TPOT does not support this `kernel` usage so far. It should be a nice enhancement feature. Any contribution is welcome. ,support kernel usage far nice enhancement feature contribution welcome,issue,positive,positive,positive,positive,positive,positive
598709135,"Thank you for working on this! Below are my answers.

> I have a doubt can you please help me with that.
> 
>     1. Is that enough to serialize the individuals in the population by converting them into a string using clean_pipeline_string function and use pickle to store them?

The string should be able to pickled.


>     2. Can I Convert the stored string into deap.gp.primitives object and again run a function which has similar logic as eaMuPlusLambda function given in the TPOT?

This is possible with the exact same `tpot_obj._pset`. Please check this [example](https://github.com/EpistasisLab/tpot/blob/master/tests/export_tests.py#L105) in a unit test.

>     3. For this checkpoint, I have planned to develop a class which takes how often the individuals should be saved and checkpoint directory. Is that a good idea? If so where can I add the class?

Do you mean a logger class? If so, I think it is a good idea. You can add this class to a new python script file and then import it into base.

> 
>     4. Is that necessary to store all the individuals of the population or store only the recent one?
> 
> 

Maybe we can provide both options, `low-memory` option for only individuals from the recent generation and default option for all the individuals over all generations.  

> Please help me with these doubts, I am a beginner for contributing to an open-source project

",thank working doubt please help enough serialize population converting string function use pickle store string able convert string object run function similar logic function given possible exact please check example unit test develop class often saved directory good idea add class mean logger class think good idea add class new python script file import base necessary store population store recent one maybe provide option recent generation default option please help beginner project,issue,positive,positive,neutral,neutral,positive,positive
598342950,"I have a doubt can you please help me with that.
1) Is that enough to serialize the individuals in the population by converting them into a string using clean_pipeline_string function and use pickle to store them?
2) Can I Convert the stored string into deap.gp.primitives object and again run a function which has similar logic as eaMuPlusLambda function given in the TPOT?
3) For this checkpoint, I have planned to develop a class which takes how often the individuals should be saved and checkpoint directory. Is that a good idea? If so where can I add the class?
4) Is that necessary to store all the individuals of the population or store only the recent one?

Please help me with these doubts, I am a beginner for contributing to an open-source project",doubt please help enough serialize population converting string function use pickle store convert string object run function similar logic function given develop class often saved directory good idea add class necessary store population store recent one please help beginner project,issue,positive,positive,positive,positive,positive,positive
597202491,"Those complex combinations of feature transformation were not supported in TPOT. I think [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) is needed for this idea in this issue. Also, as mentioned above, because multi-object optimization should penalize the pipelines with a large number of operators and limited improvement in scores, selection function or calculation of pipeline complexity should be changed for this issue. 
But we don't have a near plan to include this function. Any contributions are welcome. ",complex feature transformation think idea issue also optimization penalize large number limited improvement selection function calculation pipeline complexity issue near plan include function welcome,issue,positive,positive,positive,positive,positive,positive
596512744,`titanic_new` is converted and prepossessed from a panda DataFrame `titanic` (see `In [14]`) and then convert to np.ndarray. You may change the steps for converting `titanic_new` back to panda.DataFrame with adding names of columns and then it can be easily exported as a CSV file.,converted prepossessed panda titanic see convert may change converting back easily file,issue,negative,positive,positive,positive,positive,positive
595682345,"After preprocessing the data in the Titanic_Kaggle.ipynb notebook, the preprocessed 'titanic_new'  is a numpy ndarray , right? It oubviously cannot be exported as a CSV.
So what do i put in place of ' PATH/TO/FILE.' ?  
",data notebook right put place,issue,negative,positive,positive,positive,positive,positive
593050064,"@EcoHub is the version of scikit-learn > 0.21? If not, please update it. Thank you ",version please update thank,issue,positive,neutral,neutral,neutral,neutral,neutral
592999417,"I still getting the error even when I changed the cross_validation by from sklearn.cross_validation import train_test_split

ImportError                               Traceback (most recent call last)
<ipython-input-17-4cf8d7814da7> in <module>
      1 #from sklearn.cross_validation import train_test_split
----> 2 from sklearn.model_selection import train_test_split
      3 X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=14)
      4 #print(""There are {} samples in the training dataset"".format(X_train.shape[0]))
      5 #print(""There are {} samples in the testing dataset"".format(X_test.shape[0]))

~\Anaconda3\lib\site-packages\sklearn\model_selection\__init__.py in <module>
     17 from ._split import check_cv
     18 
---> 19 from ._validation import cross_val_score
     20 from ._validation import cross_val_predict
     21 from ._validation import cross_validate

~\Anaconda3\lib\site-packages\sklearn\model_selection\_validation.py in <module>
     24 from ..utils import (indexable, check_random_state, _safe_indexing,
     25                      _message_with_time)
---> 26 from ..utils.validation import _check_fit_params
     27 from ..utils.validation import _is_arraylike, _num_samples
     28 from ..utils.metaestimators import _safe_split

ImportError: cannot import name '_check_fit_params' from 'sklearn.utils.validation'",still getting error even import recent call last module import import print training print testing module import import import import module import import import import import name,issue,negative,neutral,neutral,neutral,neutral,neutral
592982141,"I looked at the code and I'm going to try my best, but I'm not sure if I can find a solution.",code going try best sure find solution,issue,positive,positive,positive,positive,positive,positive
592146497,"Another comment. Testing performance, I found that for small datasets on my laptops CPU, tensorflow/keras was _considerably_ slower than `sklearn.neural_net`. I think running on any decent GPU, distributed or maybe even a workstation CPU, tensorflow would be much faster, but I really don't have the ability to test that.",another comment testing performance found small think running decent distributed maybe even would much faster really ability test,issue,negative,positive,neutral,neutral,positive,positive
591495166,">     lgb_clf.fit(_X_train, _y_train,
>                 eval_set=[(_X_val, _y_val)],
>                 verbose=50,
>                 eval_metric='auc')

Also, TPOT follows basic scikit-learn API, but this usage above seems not a standard scikit-learn API but for xgboost's models only, ",also basic usage standard,issue,negative,neutral,neutral,neutral,neutral,neutral
591494153,"Hmm, what do you mean ""I can't make XGBoost model to fit training data as TPOT""? Are you trying to reproduce the average CV scores which was internally used in evaluation? if so, you may try to use `cross_eval_score` from scikit-learn to reproduce CV score (as we tested them in [one unit test](https://github.com/EpistasisLab/tpot/blob/master/tests/tpot_tests.py#L1386-L1387)).

Also, it seems that `random_state=2` is missing for `xgb.XGBClassifier` in your codes.",mean ca make model fit training data trying reproduce average internally used evaluation may try use reproduce score tested one unit test also missing,issue,negative,negative,neutral,neutral,negative,negative
591479427,"I think it's not correct.

This is my model from Tpot:
```
from tpot import TPOTClassifier

skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1352)
pipeline_optimizer = TPOTClassifier(generations=10,
                                    n_jobs=4,
                                    cv=skfold,
                                    max_eval_time_mins=5,
                                    population_size=10,
                                    periodic_checkpoint_folder=""tpot/state3/"",
                                    random_state=2,
                                    verbosity=2,
                                    scoring=""roc_auc"")

pipeline_optimizer.fit(X_train, y_train)

# Best model: XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=16, n_estimators=100, nthread=1, subsample=0.6000000000000001)

pred_full = pipeline_optimizer.predict_proba(X_test)
# 'Results: [0.028645013, 0.016220812, 0.005172163, 0.0075716097, 0.02187296....]'
```

This is my code:
```
import xgboost as xgb

xgb_model = xgb.XGBClassifier(learning_rate=0.1,
                          max_depth=5,
                          min_child_weight=16,
                          n_estimators=100,
                          nthread=1,
                          subsample=0.6)

pred_full = 0

random_state = 1352
cv = 5

skfold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)

print('On random state:', random_state)
auc_scores = []
current_models = []

for fold, (train_idx, val_idx) in enumerate(skfold.split(X_train, y_train)):
    if fold == 3:
        continue
        
    _X_train, _y_train = X_train.iloc[train_idx], y_train.iloc[train_idx]
    _X_val, _y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]
    
    lgb_clf = copy.deepcopy(xgb_model)
    lgb_clf.fit(_X_train, _y_train,
                eval_set=[(_X_val, _y_val)],
                verbose=50,
                eval_metric='auc')
    
    y_pred = lgb_clf.predict_proba(_X_val)
    y_pred = y_pred[:, 1]
    new_scores = roc_auc_score(_y_val, y_pred)
    auc_scores.append(new_scores)
    current_models.append(copy.deepcopy(lgb_clf))
            
    y_test = lgb_clf.predict_proba(X_test)
    y_test = y_test[:, 1]
    
    pred_full += y_test
    
assert current_models[0] != current_models[1]
    
print('\nCurrent auc scores:', auc_scores)
print('Current gini scores:', make_gini(auc_scores))
print('Gini:', np.mean(make_gini(auc_scores)), ', std:', np.std(make_gini(auc_scores)))

pred_full /= len(current_models)
result = pd.DataFrame({
    'id': X_test.index,
    'label': pred_full
})

result.to_csv('xgb_basic.csv', index=False)

# Result: [0.01660117380820306, 0.016498301470441075, 0.015465876027333536, 0.0156495640327246, 0.016571651544708212, 0.015756741687708097, 0.01537791817766737...]
```

I can't make XGBoost model to fit training data as TPOT. ",think correct model import best model code import print random state fold enumerate fold continue assert print print print result result ca make model fit training data,issue,positive,positive,positive,positive,positive,positive
591306965,"Hmm, so far, there is not a easy way for getting those values during one TPOT run. TPOT did not store those best scores every generation. Maybe it is a nice new feature.",far easy way getting one run store best every generation maybe nice new feature,issue,positive,positive,positive,positive,positive,positive
591302577,"In general, TPOT uses option 1 to evaluate models/pipelines. The best model/pipelines after evaluation should be fitted on all data for scoring and prediction. ",general option evaluate best evaluation fitted data scoring prediction,issue,positive,positive,positive,positive,positive,positive
591061676,"Another update. Although I _was_ able to hack this together to make a `DeepLearningRegressor` class with the exact same API as `MPLClassifier`, there are some bugs and it uses somewhat ugly mixin inheritance patterns. Looking around the `tf.keras` source, I found this PR, which I think would make things a lot cleaner and easier on the `TPOT` side: tensorflow/tensorflow#32533

Hence, I think I am going to focus efforts on reviving that PR and will pause work on this one until then.",another update although able hack together make class exact somewhat ugly inheritance looking around source found think would make lot cleaner easier side hence think going focus reviving pause work one,issue,negative,positive,neutral,neutral,positive,positive
590823793,"Thanks for the quick answer. I'll try to look into it, but i have no experience...

I'm trying to plot the cv scores after every generation for my bachelorthesis. Is there some other way to get these values? I'm currently extracting them from the output...",thanks quick answer try look experience trying plot every generation way get currently output,issue,negative,positive,positive,positive,positive,positive
590765785,"Thank you for your quick response, converting to float via Y.astype(np.int32) did the trick!",thank quick response converting float via trick,issue,negative,positive,positive,positive,positive,positive
590755923,"I think the issue is that `Y=df.values[0:150, 10]` make `Y` a non-numerical object, which is not supported in TPOT. Please try to convert Y to np.int32 or np.float64 via `Y.astype(np.int32)` after `Y=df.values[0:150, 10]`or you could try Y=df.values[""Target_Column""].values instead of `Y=df.values[0:150, 10]`. ",think issue make object please try convert via could try instead,issue,negative,neutral,neutral,neutral,neutral,neutral
590423366,"Great, thank you for taking a look and pointing some things out. I'll do the following three things as next steps:

1. Fix CI issues
2. Fix comments made above
3. Compare performance and accuracy. The primary comparison will be to `sklearn.neaural_net.MLPClassifier`. I will compare accuracy to other models, but I fully expect this to be a lot slower than classic classifiers. I will probably use [this](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) scikit-learn doc as a basepoint for comparison.


Quick update: I'm realizing that in order to do a 'fair' comparison, I'm going to need to tune all of the default parameters for these networks to as similar as possible to `MPLClassifier`. From initial testing with this dataset, these networks can meet or exceed the accuracy of `MPLClassifier`, but the runtime is considerably worse. This is all using `tensorflow` in CPU only on WSL. I'm sure using the GPU version on bare metal would already make this at least as fast as `MPLClassifier`, but I think tuning those parameters as I mentioned should allow the CPU version to be about as fast.",great thank taking look pointing following three next fix fix made compare performance accuracy primary comparison compare accuracy fully expect lot classic probably use doc comparison quick update realizing order comparison going need tune default similar possible initial testing meet exceed accuracy considerably worse sure version bare metal would already make least fast think tuning allow version fast,issue,positive,positive,positive,positive,positive,positive
590413477,"Hmm, I can reproduce this issue in a new environment but I am not sure why it happened. A related ideas: 

_The processes in above 2 ways are not the same. For the 1st version, TPOT should run 6 generations (generation 0 is the initial population with 2 randomly generated individuals) so that [evaluation step](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L250) should trigger 6 times. But for 2nd version, except 1st iteration on the loop, TPOT should use the last generation after selection from pervious iteration as initial population and then run another generations after it. So the 2nd version should run 10 generations and trigger evaluation 10 times. But I don't know if this is related to random seed status in TPOT. _ 

 I am out of office these days and will look into it once I get a chance. Please let me know if you have any ideas. ",reproduce issue new environment sure related way st version run generation initial population randomly evaluation step trigger time version except st iteration loop use last generation selection pervious iteration initial population run another version run trigger evaluation time know related random seed status office day look get chance please let know,issue,positive,negative,neutral,neutral,negative,negative
590226552,Thank you for the PR and it  looks good for fixing the issue #1023. I will merge it to dev branch for now and add this patch to next version of TPOT.,thank good fixing issue merge dev branch add patch next version,issue,positive,positive,positive,positive,positive,positive
589859284,"Hmm, nice suggestion! Please submit a PR for this issue. Thank you!",nice suggestion please submit issue thank,issue,positive,positive,positive,positive,positive,positive
589319920,Using `model.set_params` worked beautifully! I wasn't aware of that API. Thank you.,worked beautifully aware thank,issue,positive,positive,positive,positive,positive,positive
589079679,"But for the original workflow, how about adding the cv into the TPOTClassifier’s param setting? Or for modified workflow how about using [model.set_params](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator.set_params). I think both follow scikit-learn API.",original param setting think follow,issue,negative,positive,positive,positive,positive,positive
589071769,"Hmm, I am not sure why this happened. Could you provide your TPOT config_dict and a Demo for reproducting this issue? And it seems that scikit 0.20.2 is a little out of date and I think updating it may be a solution.",sure could provide issue little date think may solution,issue,positive,positive,positive,positive,positive,positive
589063405,You could try other split models like `cv=TimeSeriesSplit(n_splits=5)` or use indexes of samples based on the example for GrifSearchCV above.,could try split like use based example,issue,negative,neutral,neutral,neutral,neutral,neutral
588975082,Can you please explain a little more how to custom specify training and validation data sets. I don't want to use k-fold cv?,please explain little custom specify training validation data want use,issue,negative,negative,negative,negative,negative,negative
588415552,"You're right, thank you for entertaining the thought.

Maybe a quick summary of my use case will be better at explaining why I was asking. My workflow is as follows (pseudocode but you get the idea):
```python3
# setup
raw_data = pd.DataFrame(...)
feature_extractors = [ext_ft_1, ext_ft_2, ...]   # callable methods ext_ft(raw_data) -> X, y, groups
models = [
     TPOTClassifier(max_time_mins=10),
     KNeighborsClassifier(),
]

# extract featues -> fit -> score
for ft_ext in feature_extractors:
    X, y, groups = ft_ext(raw_data)
    cv = list(LeaveOneGroupOut().split(X, y, groups))
    for model in models:
        model.fit(X, y)
        if isinstance(model, TPOTClassifier):
             model =model.fitted_pipeline_
       score = np.mean(cross_val_score(estimator=model, X=X, y=y, cv=cv)
    # save model, score
```

This works with the `scikit-learn` models, but not with `TPOT`. So I modified, kind of as follows:
```python3
# setup
raw_data = pd.DataFrame(...)
feature_extractors = [ext_ft_1, ext_ft_2, ...]
models = [
    (TPOTClassifier, {""max_time_mins"": 10,}),
    (KNeighborsClassifier, dict()),
]

# extract featues -> fit -> score
for ft_ext in feature_extractors:
    X, y, groups = ft_ext(raw_data)
    cv = list(LeaveOneGroupOut().split(X, y, groups))
    for model_class, opts in models:
        if isinstance(model, TPOTClassifier):
             model = model_class(**opts, cv=cv)
             model.fit(X, y)
             model =model.fitted_pipeline_
       else:
            model = model_class(**opts)
            model.fit(X, y)
       score = np.mean(cross_val_score(estimator=model, X=X, y=y, cv=cv)
      # save model, score
```

I know this is a contrived use case, but I was just wondering if there is a better way.",right thank entertaining thought maybe quick summary use case better explaining get idea python setup callable extract fit score list model model model score save model score work kind python setup extract fit score list model model model else model score save model score know use case wondering better way,issue,positive,positive,positive,positive,positive,positive
588335145,"Hmm, I think that do not follow scikit-learn API for Estimator neither. Also, those cv splits is for evaluating those pipelines in TPOT with average cv scores (as one of fitness scores in GP) which has different purpose with `cross_validate` from scikit-learn that can compute/store all cv scores and even fitted models on all splits",think follow estimator neither also average one fitness different purpose even fitted,issue,negative,negative,neutral,neutral,negative,negative
588331547,Nice check! I didn’t notice the link was broken. We will fix that for next minor release later.,nice check notice link broken fix next minor release later,issue,negative,positive,neutral,neutral,positive,positive
588284034,"That makes sense. What about making a `TPOT.cross_validate` method that essentially achieves the same thing as the current implementation, but abstracts it out to a seperate function to keep the API closer to sklearn?",sense making method essentially thing current implementation function keep closer,issue,negative,neutral,neutral,neutral,neutral,neutral
588241238,"Hmm, it is strange. Could you please provide a small demo to reproduce the issue? Also, please let us know the versions of tpot and its dependencies. Thank you.",strange could please provide small reproduce issue also please let u know thank,issue,positive,negative,negative,negative,negative,negative
588240421,The reason is that we don’t want fit() to take params which don’t follow scikit-learn API.,reason want fit take follow,issue,negative,positive,positive,positive,positive,positive
588026230,"Awesome!





Glad I could help!

I didn't want to include it in this PR, but I did want to bring this up: is there any reason for the `cv` parameter to be in the `TPOT` constructor? It seems like it's not really used until `fit` is called.",awesome glad could help want include want bring reason parameter constructor like really used fit,issue,positive,positive,positive,positive,positive,positive
587066419,"Awesome, will do.

Do you think we can use the `self.classification` parameter instead of checking each pipeline with `is_classifier(sklearn_pipeline)`?",awesome think use parameter instead pipeline,issue,positive,positive,positive,positive,positive,positive
587018104,Thank you for reporting this bug. I think this solution is good. I think [this line](https://github.com/EpistasisLab/tpot/blob/v0.11.1/tpot/gp_deap.py#L416) should be moved before before parallelizing ([here](https://github.com/EpistasisLab/tpot/blob/v0.11.1/tpot/base.py#L1271)). You are welcome to submit a PR to fix it for your credit.,thank bug think solution good think line welcome submit fix credit,issue,positive,positive,positive,positive,positive,positive
586384104,"> Could you please confirm that the hyperparameter combination of RandomForestRegressor that gave you better r2 score are included in the [search space in tpot](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py#L87-L93)? If so, it seems that TPOT failed to explore the simpler solution with better score even though TPOT was set to penalize those complex solutions in selection stage.
> 
> You may try to use `template=""Regressor""` or `template=""Transformer-Regressor""` (or even template=""RandomForestRegressor"" for just tuning `RandomForestRegressor`) to force TPOT to try simple solution instead of complex pipeline solution.

Here is my random forest parameter: 
`      ""random_forest_regressor"":{
         ""n_estimators"":100,
         ""criterion"":""mse"",
         ""min_samples_split"":2,
         ""min_samples_leaf"":1,
         ""min_weight_fraction_leaf"":0,
         ""max_features"":""auto"",
         ""n_jobs"":1,
         ""random_state"":123
      }`

I checked the search space of tpot and it seems to be within the range. I run the best tpot pipeline for 5 fold and the average r-square is 0.6203545133172108. The 5 fold random forest r-square is: 0.623749. Very close but rf is still better. I will try to use template option then. Thank you for your suggestion.",could please confirm combination gave better score included search space explore simpler solution better score even though set penalize complex selection stage may try use regressor even tuning force try simple solution instead complex pipeline solution random forest parameter criterion auto checked search space within range run best pipeline fold average fold random forest close still better try use template option thank suggestion,issue,positive,positive,neutral,neutral,positive,positive
586319214,"
Could you please confirm that the hyperparameter combination of RandomForestRegressor that gave you better r2 score are included in the [search space in tpot](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py#L87-L93)? If so, it seems that TPOT failed to explore the simpler solution with better score even though TPOT was set to penalize those complex solutions in selection stage. 

You may try to use `template=""Regressor""` or `template=""Transformer-Regressor""` (or even template=""RandomForestRegressor"" for just tuning `RandomForestRegressor`) to force TPOT to try simple solution instead of complex pipeline solution. ",could please confirm combination gave better score included search space explore simpler solution better score even though set penalize complex selection stage may try use regressor even tuning force try simple solution instead complex pipeline solution,issue,positive,positive,neutral,neutral,positive,positive
586314758,"> Hmm, is it a holdout score?

All the scores that I show above are on the test dataset",holdout score show test,issue,negative,neutral,neutral,neutral,neutral,neutral
585785356,"TPOT internally use `cross_val_score` to evaluate pipelines, so `score = np.mean(cross_val_score(estimator=best_pipeline, X=X, y=y, cv=5))` should provide same score (if cv is the same and there is no `sample_weight`) after `classifier.fit(X, y)`. I think train/test split is recommended to evaluate the performance of best pipeline via holdout score on test dataset.   

 I think the nested cross-validation should be not necessary and too time-consuming for using TPOT, which is like runing TPOT N-fold times. You could manually perform CV with TPOT but each fold should provide a best pipeline which may be different with others. You can build a ensemble model from all best pipelines (via [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)) to get the best classifier",internally use evaluate score provide score think split evaluate performance best pipeline via holdout score test think necessary like time could manually perform fold provide best pipeline may different build ensemble model best via get best classifier,issue,positive,positive,positive,positive,positive,positive
584147535,"Yep, that is a workaround. So far we did not have a good solution to replace those dynamic classes. ",yep far good solution replace dynamic class,issue,positive,positive,positive,positive,positive,positive
583885617,"I also came across this. ~~Is there any guidance on pickling tpot objects?~~
Seems https://github.com/EpistasisLab/tpot/issues/11#issuecomment-341421022 gives a workaround",also came across guidance,issue,negative,neutral,neutral,neutral,neutral,neutral
581425789,I fixed this error and closed this issue. Thanks again for this report.,fixed error closed issue thanks report,issue,negative,positive,neutral,neutral,positive,positive
580989838,"Hmm, nice catch. That is a error in docs. We will correct it.",nice catch error correct,issue,negative,positive,positive,positive,positive,positive
580271535,"Since there is only one estimator (random forest) for the `config_dict`, you may try template=""Classifier"" to avoid complicated stacking pipelines (>1 estimators) to save computational time for evaluating each pipeline.",since one estimator random forest may try classifier avoid complicated save computational time pipeline,issue,negative,negative,negative,negative,negative,negative
580268652,No problem! I closed this issue and please feel free to reopen it if you have any other questions. ,problem closed issue please feel free reopen,issue,negative,positive,positive,positive,positive,positive
580180162,"Thanks for your response. We tried the option you suggested but now optimization goes up to 100% and sometimes it gives the same error as reported above while the other it reports 50% balanced_accuracy. When running default RF outside TPOT, we get balanced_accuracy > 87 %.  we are not sure where the issue is and how to solve it.",thanks response tried option optimization go sometimes error running default outside get sure issue solve,issue,positive,positive,positive,positive,positive,positive
579977595,"Hello,
Many thanks, for the quick answer. I confirm you that the problem was on my configuration with Anaconda Jupyter. Even after i updated Anaconda and Scikit-learn the latter was still remaining in version 0.21.

After a complete removal of Scikit-Learn and TPOT i reainstalled first Scikit-Learn and now the version is 0.22.1 and everything works fine as you showed it in the Colab.net notebook.

Again many thanks and sorry for my mistake 👍 

Regards",hello many thanks quick answer confirm problem configuration anaconda even anaconda latter still version complete removal first version everything work fine notebook many thanks sorry mistake,issue,negative,positive,positive,positive,positive,positive
579770440,I tested those codes in a Colab notebook (see this [link](https://colab.research.google.com/gist/weixuanfu/f1a0efb0be1f8e8b361df7a04033c9d4/issue1010.ipynb)) and this Error did not shown up.,tested notebook see link error shown,issue,negative,neutral,neutral,neutral,neutral,neutral
579769504,"Could you please let me know what is the versions of TPOT and scikit-learn in your environment?

We recently updated TPOT for supporting new scoring API in scikit-learn v0.22. If both were not up-to-date, which may cause this error.",could please let know environment recently supporting new scoring may cause error,issue,negative,positive,positive,positive,positive,positive
578789894,I closed this issue because it was fixed in master branch. Thank you for reporting it.,closed issue fixed master branch thank,issue,negative,neutral,neutral,neutral,neutral,neutral
578788002,I will merge this PR for now. Please reopen another PR for instruction of installing `tpot-full`. Thank you again!,merge please reopen another instruction thank,issue,positive,neutral,neutral,neutral,neutral,neutral
578753564,"The issue is that `max_eval_time_mins` is too small for evaluating pipeline on this large dataset and random forest maybe very time-consuming with a large number of `n_estimators`. Please increase `max_eval_time_mins` to 20. If it does not work or is too slow, please use `subsample` to randomly down-sampling the dataset.",issue small pipeline large random forest maybe large number please increase work slow please use subsample randomly,issue,positive,negative,negative,negative,negative,negative
576718860,"Thank both of you for updating docs and conda recipes of TPOT. Please update this PR once `tpot-full`'s recipe works in both Windows and Linux. If xgboost is not working with conda-forge in Windows, we can add a note for windows users about installing xgboost from anaconda instead.

Please also use `mkdocs build --clean` commend to update docs after adding instructions of `tpot-full`.",thank please update recipe work working add note anaconda instead please also use build clean commend update,issue,positive,positive,positive,positive,positive,positive
576695539,"> 
> 
> While working on it, i think i spotted a bug:
> 
> https://github.com/EpistasisLab/tpot/blob/aea42a59b52ff9704f9f677a01ebb71fda4520cc/tpot/operator_utils.py#L215
> 
> This should be _dep_import_str_ as the key and not _import_str_, right? It seems this also changes some tests.

Yes, that is a bug. Hmm, I thought I fixed it a while ago but might not merge to master/dev branch. 
",working think spotted bug key right also yes bug thought fixed ago might merge branch,issue,negative,positive,positive,positive,positive,positive
576554952,"If i run the failing tests manually, they work. It has to be some unsuitable combination due to the random seed.. i'm not sure how to debug this.",run failing manually work unsuitable combination due random seed sure,issue,negative,negative,neutral,neutral,negative,negative
576239896,"While working on it, i think i spotted a bug:

https://github.com/EpistasisLab/tpot/blob/aea42a59b52ff9704f9f677a01ebb71fda4520cc/tpot/operator_utils.py#L215


This should be *dep_import_str* as the key and not *import_str*, right? It seems this also changes some tests.",working think spotted bug key right also,issue,negative,positive,positive,positive,positive,positive
576018823,"Created a feature request here https://github.com/conda-forge/tpot-feedstock/issues/19

I’ll do some further testing on the blanket conda install statement above to see how good/bad it is and ensure it still gives a working version of tpot with the other packages.",feature request testing blanket install statement see ensure still working version,issue,negative,neutral,neutral,neutral,neutral,neutral
576015638,"The install instructions seem reasonable, and _should_ result in a working
install.

My only reservation is to suggest people pay the anaconda install tax, then
immediately suggest they use pip to install complicated dependencies into
their base environment. Sure, you'll have python and numpy already, but
there no telling what you'll get for some stuff.

The only thing scarier is the `sudo pip` instructions I sometimes see in
other projects.

As tpot, fully installed with all the bells and whistles, is somewhat
non-trivial, perhaps suggesting virtual/conda envs, created via example
requirements.txt/environment.yml might help a) achieve more predictable
environments and b) set the tone for support issues. Using said files in CI
can further help identify install issues.

On Sun, Jan 19, 2020, 07:12 Nicholas Bollweg <nick.bollweg@gmail.com> wrote:

> Will take a look...
>
> Py-xgboost is just the python bindings, which depends on xgboost, the
> underlying c implementation. This is another one of those choice things,
> but can be confusing coming from a python-only namespace.
>
> Indeed, feel free to request additional package configurations on the
> feedstock. If it makes things easier (e.g will create fewer support issues)
> to have the ""-full"" package before updating the docs, I'm happy to oblige.
>
> On Sat, Jan 18, 2020, 22:58 Ray Bell <notifications@github.com> wrote:
>
>> @bollwyvl <https://github.com/bollwyvl> how's this for a first stab?
>>
>> Regarding your comments in #1005 (comment)
>> <https://github.com/EpistasisLab/tpot/issues/1005#issuecomment-575860780>
>> relating to conda installations e.g. tpot-full
>> Perhaps that can be a separate issue in here or in the feedstock?
>>
>> I'm also not sure what the difference is between
>> https://anaconda.org/conda-forge/xgboost and
>> https://anaconda.org/anaconda/py-xgboost. I see your tests use py-xgboost
>> (
>> https://github.com/regro-cf-autotick-bot/tpot-feedstock/blob/b716a1df7ea2ea8a90c855e2d9fb2af06f7e1ddf/recipe/meta.yaml#L42_
>> Should I use instead for the extended conda-forge install command?
>>
>> —
>> You are receiving this because you were mentioned.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/EpistasisLab/tpot/pull/1006?email_source=notifications&email_token=AAALCRCBU4LJJ7WEW4YB7HLQ6PFWPA5CNFSM4KIW2PR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJKIMFQ#issuecomment-575964694>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AAALCRF5LNVXKHEY6FNT47LQ6PFWPANCNFSM4KIW2PRQ>
>> .
>>
>
",install seem reasonable result working install reservation suggest people pay anaconda install tax immediately suggest use pip install complicated base environment sure python already telling get stuff thing pip sometimes see fully somewhat perhaps suggesting via example might help achieve predictable set tone support said help identify install sun wrote take look python underlying implementation another one choice coming indeed feel free request additional package easier create support package happy oblige sat ray bell wrote first stab regarding comment perhaps separate issue also sure difference see use use instead extended install command reply directly view,issue,positive,positive,positive,positive,positive,positive
575998552,"Will take a look...

Py-xgboost is just the python bindings, which depends on xgboost, the
underlying c implementation. This is another one of those choice things,
but can be confusing coming from a python-only namespace.

Indeed, feel free to request additional package configurations on the
feedstock. If it makes things easier (e.g will create fewer support issues)
to have the ""-full"" package before updating the docs, I'm happy to oblige.

On Sat, Jan 18, 2020, 22:58 Ray Bell <notifications@github.com> wrote:

> @bollwyvl <https://github.com/bollwyvl> how's this for a first stab?
>
> Regarding your comments in #1005 (comment)
> <https://github.com/EpistasisLab/tpot/issues/1005#issuecomment-575860780>
> relating to conda installations e.g. tpot-full
> Perhaps that can be a separate issue in here or in the feedstock?
>
> I'm also not sure what the difference is between
> https://anaconda.org/conda-forge/xgboost and
> https://anaconda.org/anaconda/py-xgboost. I see your tests use py-xgboost
> (
> https://github.com/regro-cf-autotick-bot/tpot-feedstock/blob/b716a1df7ea2ea8a90c855e2d9fb2af06f7e1ddf/recipe/meta.yaml#L42_
> Should I use instead for the extended conda-forge install command?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/pull/1006?email_source=notifications&email_token=AAALCRCBU4LJJ7WEW4YB7HLQ6PFWPA5CNFSM4KIW2PR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJKIMFQ#issuecomment-575964694>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAALCRF5LNVXKHEY6FNT47LQ6PFWPANCNFSM4KIW2PRQ>
> .
>
",take look python underlying implementation another one choice coming indeed feel free request additional package easier create support package happy oblige sat ray bell wrote first stab regarding comment perhaps separate issue also sure difference see use use instead extended install command reply directly view,issue,positive,positive,positive,positive,positive,positive
575964694,"@bollwyvl how's this for a first stab?

Regarding your comments in https://github.com/EpistasisLab/tpot/issues/1005#issuecomment-575860780
relating to conda installations e.g. `tpot-full`
Perhaps that can be a separate issue in here or in the feedstock?

I'm also not sure what the difference is between https://anaconda.org/conda-forge/xgboost and https://anaconda.org/anaconda/py-xgboost. I see your tests use `py-xgboost` (https://github.com/regro-cf-autotick-bot/tpot-feedstock/blob/b716a1df7ea2ea8a90c855e2d9fb2af06f7e1ddf/recipe/meta.yaml#L42_ Should I use this instead of xgboost for the extended conda-forge install command?",first stab regarding perhaps separate issue also sure difference see use use instead extended install command,issue,negative,positive,positive,positive,positive,positive
575860780,"> The other optional packages not included are:
> xgboost
> dask
> dask-ml
> fsspec
> scikit-mdr
> skrebate
> WIP: I'll check to see if the optional packages above are on conda-forge to provide other conda forge install examples e.g. `conda install -c conda-forge tpot dask`

Have a look at the `#!/test/requires` section of the [recipe](https://github.com/regro-cf-autotick-bot/tpot-feedstock/blob/master/recipe/meta.yaml#L40-L44): we do the full test suite with all of them installed, so you're probably good to go.

It's not pulling those extra dependencies in, because people (rightfully) get kind of bent out of shape if they get a bunch of unexpected stuff when they install a particular package.... or the extras prevent it from running on their platform altogether.

conda doesn't really have the `[extras]` notation that pip does. We have a few tools in the toolbag, however, that could be applied once, and be available for the current version and all future versions (probably not worth going back and adding them for old version):
- multiple outputs of a single recipe
  - e.g. https://github.com/conda-forge/opencv-feedstock/blob/master/recipe/meta.yaml#L178
  - the above recipe could generate multiple packages from the same source files
    - so, in addition to `tpot`, it could build
      - `tpot-full` which required `tpot` and all of the extra dependencies
      - `tpot-dask` which required `tpot` and the dask stack
      - ....
    - the advantage here is you can specify version-level pinning _and_ trigger installation, with basically zero maintenance cost (and probably less docs burden)
- constraining versions of non-dependencies
  - https://conda-forge.org/docs/maintainer/adding_pkgs.html#defining-non-dependency-restrictions
  - if there are other packages that _could_ be used, but only if a certain version is available (and it will fail if it's not), you can specify valid versions of said packages next to which `tpot` can be installed
   - alternately, it can be used to make something deliberately _incompatible_ with something else
     - for example `pil` and `pillow` should probably never be installed together!
   - this may not be what you want for `tpot`, and _can_ create issues for downstream users, if they aren't expecting to use the two packages together... ",optional included check see optional provide forge install install look section recipe full test suite probably good go extra people rightfully get kind bent shape get bunch unexpected stuff install particular package prevent running platform altogether really notation pip however could applied available current version future probably worth going back old version multiple single recipe recipe could generate multiple source addition could build extra stack advantage specify pinning trigger installation basically zero maintenance cost probably le burden constraining used certain version available fail specify valid said next alternately used make something deliberately something else example pillow probably never together may want create downstream use two together,issue,positive,positive,positive,positive,positive,positive
575855901,"Sure, go ahead! We've been shipping from cf for several years at this
point, and have multiple maintainers, so I'd say it's as good as any a
source! If you have any additional concerns, please raise them in the
feedstock!

https://github.com/conda-forge/tpot-feedstock

On Fri, Jan 17, 2020, 20:42 Ray Bell <notifications@github.com> wrote:

> What's the status on this?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/165?email_source=notifications&email_token=AAALCREE73QO7K7FEX3YUCTQ6JM6RA5CNFSM4CFYUB4KYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEJJNLJI#issuecomment-575853989>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAALCRHYPZCBAK7PBHKZQKTQ6JM6RANCNFSM4CFYUB4A>
> .
>
",sure go ahead shipping several point multiple say good source additional please raise ray bell wrote status reply directly view,issue,positive,positive,positive,positive,positive,positive
575853989,"What's the status on this?
Looking at https://anaconda.org/conda-forge/tpot it seems like a viable option.
Should I update the docs (https://epistasislab.github.io/tpot/installing/)?",status looking like viable option update,issue,negative,neutral,neutral,neutral,neutral,neutral
575667162,"Oh the shape of Y, yes yes. Perfect thanks so much!!!! ",oh shape yes yes perfect thanks much,issue,positive,positive,positive,positive,positive,positive
575663317,"sample_weight should be array-like with shape of y. Something like:
```python
sample_weight = np.ones(y.shape, dtype=int)
sample_weight[np.where(y = 0)] = 2
```",shape something like python,issue,negative,neutral,neutral,neutral,neutral,neutral
575661505,"@weixuanfu  Right I did see that, so if I wanted to assign say a weight of 2 to the 0’s in my dataset, would the parameter look something like `sample_weight = {‘0’ : 2}` ? Thanks a million again! ",right see assign say weight would parameter look something like thanks million,issue,positive,positive,positive,positive,positive,positive
575648543,"Yes, `sample_weight` is used for setting different weights on samples. In your case here, I think you need assign a bigger weight for samples with actual value was 0 then if the prediction is 1, there is more penalty on accuracy score. 

You can use `sample_weight` on tpot.fit() function via sample_weight parameter. We documented it in [TPOT API](https://epistasislab.github.io/tpot/api/) .

```
sample_weight: array-like {n_samples}, optional

    Per-sample weights. Higher weights indicate more importance. If specified, sample_weight will be passed to any pipeline element whose fit() function accepts a sample_weight argument. By default, using sample_weight does not affect tpot's scoring functions, which determine preferences between pipelines. 
```

",yes used setting different case think need assign bigger weight actual value prediction penalty accuracy score use function via parameter optional higher indicate importance pipeline element whose fit function argument default affect scoring determine,issue,positive,positive,positive,positive,positive,positive
575646685,Thank you for your ideas here. I prefer the 3rd one but I think a better solution based on it is to automatically use `MultiOutputRegressor/Classifier` over pipelines that generated from current default config if TPOT detected the y is a multi-output target. Please let me know if you have any idea. ,thank prefer one think better solution based automatically use current default target please let know idea,issue,positive,positive,positive,positive,positive,positive
575467414,"@weixuanfu  You're right, I lowered my train/test split down to a 50/50 and changed to balanced accuracy and the model performed way better on out of sample. Thanks a lot for the recommendation.

I did have a new question for you, and this one does not really pertain to this issue, but I wanted to check here first before opening another issue. I recently discovered the possibility of class sensitive training for a classification model/adding sample weights during the .fit() function, but I'm a little unclear on how to go about using the feature properly. I've read through a few closed issues on the topic and tried to make sense of it, so I'll try and just describe what I'm after and maybe you can steer me in the right direction again.

I have the same dataset as described before (balanced, multi-variate, binary classification), and what I'd like to do is be able to add a weight to the 1's in my dataset, meaning that when the model predicts a 1, and it's actual value was a 0, that prediction should be penalized somehow, whereas if it predicted a 0 and it was actually supposed to be a 1, that shouldn't be treated as bad of a miss. I know this is something used more with an imbalanced dataset, but I feel it could be an important feature for my particular use case.

So with that in mind, is that what the sample_weight is used for? And if so, what's the proper syntax to add such a thing? I read the function listed in [708 ](https://github.com/EpistasisLab/tpot/issues/708)but just a little unclear on how to make it work. Thanks a million! Love TPOT, works amazing!",right split balanced accuracy model way better sample thanks lot recommendation new question one really pertain issue check first opening another issue recently discovered possibility class sensitive training classification sample function little unclear go feature properly read closed topic tried make sense try describe maybe steer right direction balanced binary classification like able add weight meaning model actual value prediction somehow whereas actually supposed bad miss know something used feel could important feature particular use case mind used proper syntax add thing read function listed little unclear make work thanks million love work amazing,issue,positive,positive,positive,positive,positive,positive
574949888,"Thanks @weixuanfu . This gives me enough to try to for now. I'll close the issue now and try out your solutions and see how I make out. If for whatever reason I'm still having issues, I'll upload a re-producible script and datasets for further testing. Thanks again!",thanks enough try close issue try see make whatever reason still script testing thanks,issue,positive,positive,positive,positive,positive,positive
574940873,"It seems TPOT's optimized pipeline was overfited on your whole training set. 

I think you should try potential solution 2 and 3 first or use `sub_sample` option (discussed in #804) to solve the overfitting issue.

I don't think 1. may help a little but not very much since you already used 10-fold CV. and 3. 

About 4., there are two ways to use the exported pipeline: 
1. as mentioned in this issue, you can refit the pipeline on the same training dataset and then use .predict() on the new unseen dataset. I think those pre-processing step should be only applied on the same training dataset to avoid data leakage. 
2. After finishing tpot.fit() step, dump the tpot.fitted_pipelines_ to a pickle file (please see this [link](https://scikit-learn.org/stable/modules/model_persistence.html) for more details) and then you can used this fitted pipeline on another python kernel without refitting. Or, if you want test the performance on the new unseen dataset within the same python kernel, you can simply use tpot.fitted_pipeline_ on the dataset after reading it. ",pipeline whole training set think try potential solution first use option solve issue think may help little much since already used two way use pipeline issue refit pipeline training use new unseen think step applied training avoid data leakage finishing step dump pickle file please see link used fitted pipeline another python kernel without want test performance new unseen within python kernel simply use reading,issue,negative,positive,neutral,neutral,positive,positive
574936344,"Hmm, I am not sure about the reason about the ImportError, maybe you need update scikit-learn module. Also, It seems there are some environment issues in your python 3.8. Did you use conda to manage the environment? If so, please check if tpot was installed in the environment of jupyter or you may build a new conda environment for tpot. ",sure reason maybe need update module also environment python use manage environment please check environment may build new environment,issue,positive,positive,positive,positive,positive,positive
574535740,"There are several ways to go about this:
- Give a warning if the standard regression / classification config is chosen for multi outputs, which would require the user to create their own config for multi output datasets.
- Make a list with regressors and classifiers that don't support multi output and remove them automatically if such a dataset is detected
- Embed these regressors and classifiers in MultiOutputRegressor/Classifier in the default config. Not sure about the performance losses there",several way go give warning standard regression classification chosen would require user create output make list support output remove automatically embed default sure performance,issue,positive,positive,positive,positive,positive,positive
574223438,"Neat! Thank you for this PR. I think it should work for checking multi output dataset. But I checked this [link](https://stats.stackexchange.com/questions/153853/regression-with-scikit-learn-with-multiple-outputs-svr-or-gbm-possible), I think that not all regressors in our default config are supporting multi-output regression. I think we need a workaround for this issue.",neat thank think work output checked link think default supporting regression think need issue,issue,positive,positive,positive,positive,positive,positive
574211982,"
[![Coverage Status](https://coveralls.io/builds/28099332/badge)](https://coveralls.io/builds/28099332)

Coverage remained the same at 96.747% when pulling **3dd647509de8414b985c56f47356b431d5fee3f7 on jhmenke:development** into **410d88cfb0f403d7b7f2cb9e55d254381adab11f on EpistasisLab:development**.
",coverage status coverage development development,issue,negative,neutral,neutral,neutral,neutral,neutral
574193457,"Can someone confirm that changing this line:
https://github.com/EpistasisLab/tpot/blob/aea42a59b52ff9704f9f677a01ebb71fda4520cc/tpot/base.py#L1160

to 

`X, y = check_X_y(features, target, accept_sparse=True, dtype=None, multi_output=len(target.shape) > 1 and target.shape[1] > 1)`

multioutputs are supported correctly? It seems to work for me and was the only change i made, but i'd rather see it confirmed by someone before making a proper PR.

edit: i just looked up the PR #903 and it seems to be doing the same change, albeit with a manual flag.
",someone confirm line target correctly work change made rather see confirmed someone making proper edit change albeit manual flag,issue,negative,positive,positive,positive,positive,positive
573305825,"With  max_time_mins of 300 and generations=100, population_size=100, tpot has been running for over 5 hours already.  I installed it today and the version is '0.11.1'.
Do i have to install the patch for this to be working ?",running already today version install patch working,issue,negative,neutral,neutral,neutral,neutral,neutral
573278787,"Sorry may be roc_auc not a valid metrics for supporting multiclass.
Figured out i was doing other other things weong in my data.",sorry may valid metric supporting figured data,issue,negative,negative,negative,negative,negative,negative
573046468,"Yes, it is the update checker and you can disable it via disable_update_check=True.",yes update checker disable via,issue,negative,neutral,neutral,neutral,neutral,neutral
572888242,"Thanks for the encouragement! I'll put together a PR when I get a chance.

In case anyone else hits this problem before a proper fix is available, here's my quick workaround tonight on a job that's been running for 8 hours. The cache directory is `tpot_cache`, my script name is `eval_tpot.py` script, and I'm limiting the directory to 100 GB.

```
while [ -n ""$(ps | grep eval_tpot.py | grep -v grep)"" ]; do 
    echo ""reducing size of tpot cache at $(date)""
    python -c ""from joblib import Memory; mem=Memory('tpot_cache', bytes_limit=100*2**30); mem.reduce_size()""
    echo ""done cleaning; going to sleep for an hour""
    sleep 3600
done
```",thanks encouragement put together get chance case anyone else problem proper fix available quick tonight job running cache directory script name script limiting directory echo reducing size cache date python import memory echo done cleaning going sleep hour sleep done,issue,positive,positive,positive,positive,positive,positive
572820967,"okey so

> disable_update_check: boolean, optional (default=False)
> Flag indicating whether the TPOT version checker should be disabled.
> 
> The update checker will tell you when a new version of TPOT has been released.",optional flag whether version checker disabled update checker tell new version,issue,negative,negative,neutral,neutral,negative,negative
572772368,"I see! I tried a longer time limit and also think it is an initialization issue. Also, thanks for the suggested code!",see tried longer time limit also think issue also thanks code,issue,negative,positive,positive,positive,positive,positive
572702321,"I tested your codes. Demo 2. could finished without error (`max_time_mins=0.25`) and maybe because the CPU in my computer is faster, so I think it is a initiation issue.

BTW the codes won't be modified for using this customized config file (please see the [link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for more details). Below is a demo for 2 with TPOT v0.11.1:

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import sys
sys.path.append('tpot')
from tpot import TPOTClassifier

X = pd.read_csv('X.csv', index_col=None, header=None).values
y = pd.read_csv('y.csv', index_col=None, header=None).values[:, 0]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

classifier_config_dict_custom = {

    # Classifiers
    'sklearn.neighbors.KNeighborsClassifier': {
        'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15],
        'p': [1, 2]
    },

    'sklearn.tree.DecisionTreeClassifier': {
        'min_samples_split': [2,4,8,16,32,64,128,256,512,1024,0.01,0.001,0.0001,1e-05]
    },

    'sklearn.ensemble.RandomForestClassifier': {
        'min_samples_split': [2,4,8,16,32,64,128,256,512,1024,0.01,0.001,0.0001,1e-05],
        'criterion': [""gini"", ""entropy""],
    },
    
    'sklearn.ensemble.GradientBoostingClassifier': {
        'learning_rate': [0.001,0.01,0.025,0.05,0.1,0.25,0.5],
        'max_depth': [3, 6],
        'max_features': [None, ""log2""],
    },
    
    'sklearn.ensemble.AdaBoostClassifier': {
        'n_estimators': [50, 100],
        'learning_rate': [1.0, 1.5, 2.0, 2.5, 3.0],
    },
    
    'sklearn.svm.LinearSVC': {
        'C': [0.125,0.25,0.5,0.75,1,2,4,8,16],
    },

    'sklearn.linear_model.LogisticRegression': {
        'penalty': [""l1"", ""l2""],
        'C': [0.25,0.5,0.75,1,1.5,2,3,4],
        'solver': [""liblinear"", ""saga""],
    },
    
    'sklearn.linear_model.Perceptron': {
    },

    'sklearn.naive_bayes.GaussianNB': {
    },
    
    'sklearn.neural_network.MLPClassifier': {
        'learning_rate_init': [0.0001,0.001,0.01],
        'learning_rate': [""adaptive""],
        'solver': [""sgd"", ""adam""],
        'alpha': [0.0001, 0.01],
    },
    
     'sklearn.ensemble.ExtraTreesClassifier': {
        'min_samples_split': [2,4,8,16,32,64,128,256,512,1024,0.1,0.01,0.001,0.0001,1e-05],
        'criterion': [""gini"", ""entropy""],
    },

    # Preprocesssors

    'sklearn.decomposition.PCA': {
        'svd_solver': ['randomized'],
        'iterated_power': range(1, 11)
    },

    'sklearn.preprocessing.StandardScaler': {
    },
    
    'sklearn.preprocessing.OneHotEncoder': {
        'handle_unknown': [""ignore""],
        'sparse': [0],
    },
   
    # Selectors

    'sklearn.feature_selection.VarianceThreshold': {
        'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]
    },
    
    'sklearn.feature_selection.SelectKBest': {
    },
    
    'sklearn.feature_selection.SelectPercentile': {
        'percentile': range(1, 100),
        'score_func': {
            'sklearn.feature_selection.f_classif': None
        }
    },
}

clf = TPOTClassifier(
        max_time_mins=0.25,
        scoring='balanced_accuracy',
        config_dict=classifier_config_dict_custom,
        random_state=42,
        verbosity=2,
)

clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(""finished!"")
```",tested could finished without error maybe computer faster think initiation issue wo file please see link python import import import import import entropy none log saga adaptive entropy range ignore range none print finished,issue,negative,neutral,neutral,neutral,neutral,neutral
572690143,"Sure! A self-contained MWE is here:
[tpot_debugging_mwe.zip](https://github.com/EpistasisLab/tpot/files/4041881/tpot_debugging_mwe.zip)

The observations are:
1. `python run_tpot_light.py` finishes with no error.
2. `python run_tpot_custom.py` (this Python file differs from the previous at just Line 15) gives the aforementioned error.
3. After changing the time limit in `run_tpot_custom.py` to 1 minute (by changing Line 13 to `max_time_mins=1`), `python run_tpot_custom.py` also finishes successfully.

Looks like the error in 2 is because the pipeline cannot be quickly initialized given the configurations I provide in `tpot/config/classifier_custom.py`, but I would like to hear about your thoughts!",sure python error python python file previous line error time limit minute line python also successfully like error pipeline quickly given provide would like hear,issue,negative,positive,positive,positive,positive,positive
572673106,Thank you for posting the idea here. Any contribution is welcome for this issue. ,thank posting idea contribution welcome issue,issue,positive,positive,positive,positive,positive,positive
572579492,Could you please share a demo for reproducing this error? It maybe a format issue of input X or y.,could please share error maybe format issue input,issue,negative,neutral,neutral,neutral,neutral,neutral
572246269,"Good call. I just updated to 0.11.1 and it fixed this issue. I think duck typing would probably be more future-proof than checking sklearn private module names, but this works great for now.

Thanks!!",good call fixed issue think duck would probably private module work great thanks,issue,positive,positive,positive,positive,positive,positive
572244277,Thank you for the PR and fix. I will merge it to dev branch.,thank fix merge dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
572244078,Thank you for reporting this issue. But I think this should be fixed in latest version of TPOT (0.11.1). Could you please run another test with scikit-learn 0.22.1? ,thank issue think fixed latest version could please run another test,issue,positive,positive,positive,positive,positive,positive
572240903,"I just ran into this error, and was banging my head on it for a few hours last night. 
Today I tracked it down to a silly mistake in my custom scoring function (I had neglected `import numpy as np`). 
As best I can tell, `tpot.gp_deap:_wrapped_cross_val_score()` is catching all exceptions from _fit_and_score (including errors in the scoring function), and converting them to a score of -inf. When the scoring function is buggy, that results in all individuals having -inf scores and the misleading error message of:
> RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.

What do you think about making `tpot.gp_deap:_wrapped_cross_val_score()` aware of the verbosity level so it can print the exceptions instead of hiding them. Hiding all exceptions makes code very hard to debug. I'm assuming that the blanket catch is to prune out buggy individuals, but it can have unfortunate side effects for integrating custom scoring (or presumably custom pipeline components).

I just submitted a PR #994 with a minor tweak to the error message. 

Thanks for all your work on this project :)",ran error banging head last night today tracked silly mistake custom scoring function import best tell catching scoring function converting score scoring function buggy misleading error message error optimization process could data properly data regression problem provided object please make sure data correctly think making aware verbosity level print instead code hard assuming blanket catch prune buggy unfortunate side effect custom scoring presumably custom pipeline minor tweak error message thanks work project,issue,negative,positive,positive,positive,positive,positive
572041049,"I meant that that points mutations could only happen on transformer step or hyper-parameters of 1-2 classifier in this small-size population since the population in later iteration is full of solutions with only 1-2 classifier. For example, if only XGBClassifier become dominant pipelines with high fitness score in a iteration, the point mutation in large chance should just tune one hyperparameter of XGBClassifier or Transformer (if hyperparameter is available) via one point mutations and switching to new Classifier via point muation should also happen but the chance is lower. And, unless the solutions with new Classifier has a better or similar fitness score, those solutions can not survive to next generation or iteration after selection step in GP. 
I will dig into this issue a little more and maybe there is a better mutation method can be included into TPOT.",meant could happen transformer step classifier population since population later iteration full classifier example become dominant high fitness score iteration point mutation large chance tune one transformer available via one point switching new classifier via point also happen chance lower unless new classifier better similar fitness score survive next generation iteration selection step dig issue little maybe better mutation method included,issue,positive,positive,positive,positive,positive,positive
572034327,"Thank you for reporting this issue. 
TPOT requires numerical target before and I think it is a good idea to discard this encoding if all the default scikit-learn operators in TPOT supports non-numerical target. If so, we should fix it in next version of TPOT. For now, please use [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) for encoding the target variables for refitting `tpot_obj.fit_pipelines_` attribute or use `tpot_obj.predict()`for getting the prediction of target and then used it for calculating scores.",thank issue numerical target think good idea discard default target fix next version please use target attribute use getting prediction target used calculating,issue,positive,positive,positive,positive,positive,positive
571854589,"Thank you for the fix. I am still unsure though, what does ""mutation could happen on transformer step only in this small-size population"" mean? And are you planning to fix that problem anytime soon?
Again, thank you.",thank fix still unsure though mutation could happen transformer step population mean fix problem soon thank,issue,negative,negative,negative,negative,negative,negative
571819748,"@joseortiz3 Did you ever figure out how to pass per-sample weights into a scoring function?

In my regression problem, most of the per-sample weights are determined by information that is not part of y, so I can't derive sample weights from available info inside the scoring function. The only robust way I can think of would require hacking tpot.gp_deap & sklearn's _fit_and_score which seems less than ideal. 
...Hmmm. Sounds like I'm re-suggesting a strategy @weixuanfu proposed in https://github.com/EpistasisLab/tpot/issues/310",ever figure pas scoring function regression problem determined information part ca derive sample available inside scoring function robust way think would require hacking le ideal like strategy,issue,positive,positive,positive,positive,positive,positive
571546860,"I believe that, with the newer version of _scikit-learn_, the public API is the only one supported, and that corresponds only to the top-level modules.
Thus, for example, on file `gp_deap.py`, line 33, where I see
`from sklearn.model_selection._split import check_cv`
should be
`from sklearn.model_selection import check_cv`

There may be some other places on the code you may want to check for future-proofing your project.",believe version public one thus example file line see import import may code may want check project,issue,negative,neutral,neutral,neutral,neutral,neutral
571173866,Works great! Thank you for your effort.,work great thank effort,issue,positive,positive,positive,positive,positive,positive
571164234,"> Through `fitted_pipeline` attribute, I can only see the `pipeline` and the attributes, Am I right?


Yes, it is a scikit-learn Pipeline object. 

> Later, I have to execute the pipeline and use `get_support` to get the list of features. Am I right?


Yes, the pipeline is interpretable with Pipeline API.

>     1. So in this case, it has picked `RFE` (refer below) as feature selection approach after trying several other feature selection approaches?

>     2. Later once RFE is chosen, it has tried combinations of different parameters and finally arrived at this parameters? (refer below)


>     3. I see in the `evaluated_individuals`, it only has `RFE`, `Select percentile`, `variance threshold` which keeps repeating. Will `tpot` not try other methods like `LASSO`,?

>     4. So am I right to understand that `tpot` tried multiple values for  `max_parameters` and chose the best `n` features? It wasn't using the default value for max_features. Am I right?
> 
>     5. I might be wrong here. Is it possible for `tpot` to miss different combinations of hyperparameters? I mean for example, it produces an `F1-score of 79 pc`. But based on my previous manual experiements, I changed few values and was able to get an `F1-score of 81`. Just trying to understand why wasn't `tpot` able to provide the best `F1-score`? can help me understand this
> 
> 
> **tpot classifier**
> 
> ```
> tpot = TPOTClassifier(generations=5, population_size=5, scoring='f1',verbosity=2, 
>                       template='Selector-Classifier',random_state=42)
> tpot.fit(X_train_std, y_train)
> print(tpot.score(X_test_std, y_test))
> tpot.export('tpot_digits_pipeline.py')
> tpot.fitted_pipeline_
> ```
> 
> **exported pipeline**
> 
> ```
> features = X
> training_features, testing_features, training_target, testing_target = \
>             train_test_split(features, y, random_state=42)
> 
> exported_pipeline = make_pipeline(
>     RFE(estimator=ExtraTreesClassifier(criterion=""gini"", max_features=0.55, n_estimators=100), step=0.6000000000000001),
>     RandomForestClassifier(bootstrap=True, criterion=""entropy"", max_features=0.45, min_samples_leaf=1, min_samples_split=9, n_estimators=100)
> )
> # Fix random state for all the steps in exported pipeline
> set_param_recursive(exported_pipeline.steps, 'random_state', 42)
> 
> exported_pipeline.fit(training_features, training_target)
> y_pred = exported_pipeline.predict(testing_features)
> ```
> 
> ```
> 
> **Feature output**
> ```
> 
> exported_pipeline.named_steps['rfe'].get_support()
> 
> ```
> 
> ![image](https://user-images.githubusercontent.com/30723825/71759629-ec4e4980-2eea-11ea-890e-0c6df349004f.png)
> ```

Answers to 5 questions above:  TPOT uses genetic programming to optimize pipeline, which means that TPOT randomly generates solutions in the initial population (generation 0) and then evolve them via crossover and mutation over generations. TPOT may not try all the estimators unless the number of generations X population size (or offspring size) is large enough. Also TPOT should also try combinations of hyperparameters of those scikit-API estimators via GP. Changing the hyperparamters/values of TPOTClassifier may changing best results herein since the optimization process via GP is changed.  

Could you share the hyperparameters in your case here?",attribute see pipeline right yes pipeline object later execute pipeline use get list right yes pipeline interpretable pipeline case picked refer feature selection approach trying several feature selection later chosen tried different finally refer see select percentile variance threshold try like lasso right understand tried multiple chose best default value right might wrong possible miss different mean example based previous manual able get trying understand able provide best help understand classifier print pipeline entropy fix random state pipeline feature output image genetic optimize pipeline randomly initial population generation evolve via crossover mutation may try unless number population size offspring size large enough also also try via may best herein since optimization process via could share case,issue,positive,positive,positive,positive,positive,positive
570754685,"Hi,

Through `fitted_pipeline` attribute, I can only see the `pipeline` and the attributes, Am I right?

Later, I have to execute the pipeline and use `get_support` to get the list of features. Am I right?

1) So in this case, it has picked `RFE` (refer below) as feature selection approach after trying several other feature selection approaches?  

2) Later once RFE is chosen, it has tried combinations of different parameters and finally arrived at this parameters? (refer below)

3) I see in the `evaluated_individuals`, it only has `RFE`, `Select percentile`, `variance threshold` which keeps repeating. Will `tpot` not try other methods like `LASSO`,?

4) So am I right to understand that `tpot` tried multiple values for  `max_parameters` and chose the best `n` features? It wasn't using the default value for max_features. Am I right?

5) I might be wrong here. Is it possible for `tpot` to miss different combinations of hyperparameters? I mean for example, it produces an `F1-score of 79 pc`. But based on my previous manual experiements, I changed few values and was able to get an `F1-score of 81`. Just trying to understand why wasn't `tpot` able to provide the best `F1-score`? can help me understand this

**tpot classifier**


```
tpot = TPOTClassifier(generations=5, population_size=5, scoring='f1',verbosity=2, 
                      template='Selector-Classifier',random_state=42)
tpot.fit(X_train_std, y_train)
print(tpot.score(X_test_std, y_test))
tpot.export('tpot_digits_pipeline.py')
tpot.fitted_pipeline_
```

**exported pipeline**

```
features = X
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, y, random_state=42)

exported_pipeline = make_pipeline(
    RFE(estimator=ExtraTreesClassifier(criterion=""gini"", max_features=0.55, n_estimators=100), step=0.6000000000000001),
    RandomForestClassifier(bootstrap=True, criterion=""entropy"", max_features=0.45, min_samples_leaf=1, min_samples_split=9, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
y_pred = exported_pipeline.predict(testing_features)

```
```

**Feature output**

```
exported_pipeline.named_steps['rfe'].get_support()
```

![image](https://user-images.githubusercontent.com/30723825/71759629-ec4e4980-2eea-11ea-890e-0c6df349004f.png)
",hi attribute see pipeline right later execute pipeline use get list right case picked refer feature selection approach trying several feature selection later chosen tried different finally refer see select percentile variance threshold try like lasso right understand tried multiple chose best default value right might wrong possible miss different mean example based previous manual able get trying understand able provide best help understand classifier print pipeline entropy fix random state pipeline feature output image,issue,positive,positive,positive,positive,positive,positive
570707457,"Okay I just opened a new notebook, and it is working now.  thank you for your help!",new notebook working thank help,issue,positive,positive,positive,positive,positive,positive
570702853,"So weird, i cut and paste exactly what you put into my colab and i get
an error.

Requirement already satisfied: tpot in
/usr/local/lib/python3.6/dist-packages (0.11.1)
Requirement already satisfied: scikit-learn>=0.22.0 in
/usr/local/lib/python3.6/dist-packages (from tpot) (0.22.1)
Requirement already satisfied: pandas>=0.24.2 in
/usr/local/lib/python3.6/dist-packages (from tpot) (0.25.3)
Requirement already satisfied: update-checker>=0.16 in
/usr/local/lib/python3.6/dist-packages (from tpot) (0.16)
Requirement already satisfied: stopit>=1.1.1 in
/usr/local/lib/python3.6/dist-packages (from tpot) (1.1.2)
Requirement already satisfied: numpy>=1.16.3 in
/usr/local/lib/python3.6/dist-packages (from tpot) (1.17.4)
Requirement already satisfied: deap>=1.2 in
/usr/local/lib/python3.6/dist-packages (from tpot) (1.3.0)
Requirement already satisfied: scipy>=1.3.1 in
/usr/local/lib/python3.6/dist-packages (from tpot) (1.3.3)
Requirement already satisfied: joblib>=0.13.2 in
/usr/local/lib/python3.6/dist-packages (from tpot) (0.14.1)
Requirement already satisfied: tqdm>=4.36.1 in
/usr/local/lib/python3.6/dist-packages (from tpot) (4.41.1)
Requirement already satisfied: pytz>=2017.2 in
/usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->tpot)
(2018.9)
Requirement already satisfied: python-dateutil>=2.6.1 in
/usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->tpot)
(2.6.1)
Requirement already satisfied: requests>=2.3.0 in
/usr/local/lib/python3.6/dist-packages (from
update-checker>=0.16->tpot) (2.21.0)
Requirement already satisfied: six>=1.5 in
/usr/local/lib/python3.6/dist-packages (from
python-dateutil>=2.6.1->pandas>=0.24.2->tpot) (1.12.0)
Requirement already satisfied: urllib3<1.25,>=1.21.1 in
/usr/local/lib/python3.6/dist-packages (from
requests>=2.3.0->update-checker>=0.16->tpot) (1.24.3)
Requirement already satisfied: idna<2.9,>=2.5 in
/usr/local/lib/python3.6/dist-packages (from
requests>=2.3.0->update-checker>=0.16->tpot) (2.8)
Requirement already satisfied: chardet<3.1.0,>=3.0.2 in
/usr/local/lib/python3.6/dist-packages (from
requests>=2.3.0->update-checker>=0.16->tpot) (3.0.4)
Requirement already satisfied: certifi>=2017.4.17 in
/usr/local/lib/python3.6/dist-packages (from
requests>=2.3.0->update-checker>=0.16->tpot) (2019.11.28)



from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test =
train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25,
random_state=42)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=3,
random_state=42)
tpot.fit(X_train, y_train)



else:
---> 80             exec('from {} import {}'.format(import_str,
op_str))     81         op_obj = eval(op_str)


------------------------------
9 frames
------------------------------

ImportError: cannot import name 'lobpcg'

During handling of the above exception, another exception occurred:


ImportError                               Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tpot/operator_utils.py
<https://localhost:8080/#> in source_decode(sourcecode, verbose)
82     except Exception as e:     83         if verbose > 2:
---> 84             raise ImportError('Error: could not import
{}.\n{}'.format(sourcecode, e))     85         else:     86
 print('Warning: {} is not available and will not be used by
TPOT.'.format(sourcecode))


ImportError: Error: could not import sklearn.cluster.FeatureAgglomeration.
cannot import name 'lobpcg'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
---------------------------------



On Fri, Jan 3, 2020 at 4:12 PM Weixuan Fu <notifications@github.com> wrote:

> Hmm, I just used google colab for a quick test and no error showed up too.
> Please check this link
> <https://colab.research.google.com/gist/weixuanfu/addbba317679a10a405512c20f58317b/issue991.ipynb>
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/991?email_source=notifications&email_token=AL6UOVJDQMJMUQFJD6MUZCTQ36S5NA5CNFSM4KCRH5M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEICC77A#issuecomment-570699772>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AL6UOVN4TW5L5NWIUVTTKITQ36S5NANCNFSM4KCRH5MQ>
> .
>
",weird cut paste exactly put get error requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied import import import import iris else import import name handling exception another exception recent call last verbose except exception verbose raise could import else print available used error could import import name note import failing due missing package manually install either pip apt view common click open button fu wrote used quick test error please check link thread reply directly view,issue,positive,positive,positive,positive,positive,positive
570699772,"Hmm, I just used google colab for a quick test and no error showed up too. Please check this [link](https://colab.research.google.com/gist/weixuanfu/addbba317679a10a405512c20f58317b/issue991.ipynb)
",used quick test error please check link,issue,negative,positive,positive,positive,positive,positive
570697674,"hello i am using google colab so not a conda environment.

On Fri, Jan 3, 2020 at 4:01 PM Weixuan Fu <notifications@github.com> wrote:

> Hmm, somehow I cannot install scikit-learn v0.22.1 via conda (maybe
> because it was released yesterday) into my environment for testing this
> issue. I tested it with scikit-learn 0.22 but no error showed up. Could you
> please build a clean conda environment with scikit-learn 0.22 for testing
> it again? I am not sure if it is installation issue of scikit-learn.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/991?email_source=notifications&email_token=AL6UOVILBITXM6GSUY4G7KTQ36RSXA5CNFSM4KCRH5M2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEICCKBQ#issuecomment-570696966>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AL6UOVIB66BAPMO3GLQCVHDQ36RSXANCNFSM4KCRH5MQ>
> .
>
",hello environment fu wrote somehow install via maybe yesterday environment testing issue tested error could please build clean environment testing sure installation issue thread reply directly view,issue,negative,positive,positive,positive,positive,positive
570696966,"Hmm, somehow I cannot install  scikit-learn v0.22.1 via conda (maybe because it was released yesterday) into my environment for testing this issue. I tested it with scikit-learn 0.22 but no error showed up. Could you please build a clean conda environment with scikit-learn 0.22 for testing it again? I am not sure if it is installation issue of scikit-learn.",somehow install via maybe yesterday environment testing issue tested error could please build clean environment testing sure installation issue,issue,positive,positive,positive,positive,positive,positive
570692908,"> Could you please let me know the version of scikit-learn in your environment? you may need update scikit-learn to v0.22

Hi the version is 
Requirement already satisfied, skipping upgrade: scikit-learn>=0.22.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.22.1)",could please let know version environment may need update hi version requirement already satisfied skipping upgrade,issue,positive,positive,positive,positive,positive,positive
570685824,I just googled a related issue in [this link](https://github.com/slundberg/shap/issues/957) and it seemed that something was messed up in your sklearn install.,related issue link something install,issue,negative,neutral,neutral,neutral,neutral,neutral
570685141,Could you please let me know the version of scikit-learn in your environment? you may need update scikit-learn to v0.22,could please let know version environment may need update,issue,negative,neutral,neutral,neutral,neutral,neutral
570684633,Please reopen this issue with codes and more details for reproducing this error.,please reopen issue error,issue,negative,neutral,neutral,neutral,neutral,neutral
570654211,"The issue was fixed in the latest version of TPOT, thus I closed this issue. Please feel free to reopen this issue if you have any questions. ",issue fixed latest version thus closed issue please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
570622933,"For testing the demo above, you may install TPOT with patch into your environment via:
```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```
Note: scikit-learn may need to be updated to 0.22 for using dev branch.",testing may install patch environment via shell pip install upgrade note may need dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
570622009,"I just submit a PR #989  for fixing the bug mentioned in point 1 above.

Below is a updated demo for testing this issue (because in this PR `evaluated_individuals_` is also shared between iterations)
```python
import numpy as np
from tpot import TPOTClassifier
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
# fix random state
np.random.seed(123)
independent=np.random.randint(100,size=1000)
dependent=np.random.randint(2,size=1000)
X_train, X_test, Y_train, Y_test = train_test_split(
independent, dependent, train_size=0.7, test_size=0.3
)
X_train=X_train.reshape(-1,1)

medium_config = {
""sklearn.linear_model.LogisticRegression"": {
""penalty"": [""l1"", ""l2""],
""C"": [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1.0, 5.0, 10.0, 15.0, 20.0, 25.0],
""dual"": [False],
},
""sklearn.tree.DecisionTreeClassifier"": {
""max_depth"": range(1, 21),
""min_samples_split"": range(2, 21),
""min_samples_leaf"": range(1, 21),
},
""sklearn.ensemble.RandomForestClassifier"": {
""n_estimators"": np.arange(10, 201, 5),
""max_features"": np.arange(0.05, 1.01, 0.05),
""min_samples_split"": range(2, 21),
""min_samples_leaf"": range(1, 21),
""bootstrap"": [True, False],
},
""xgboost.XGBClassifier"": {
""objective"": ['reg:squarederror'],
""n_estimators"": np.arange(10, 201, 5),
""max_depth"": range(1, 21),
""learning_rate"": [1e-3, 1e-2, 1e-1, 0.5, 1.0],
""subsample"": np.arange(0.05, 1, 0.05),
""min_child_weight"": range(1, 21),
""nthread"": [1],
},
# Transformers
""sklearn.preprocessing.Binarizer"": {""threshold"": np.arange(0.0, 1.01, 0.05)},
""sklearn.preprocessing.MinMaxScaler"": {},
""sklearn.preprocessing.RobustScaler"": {},
""sklearn.preprocessing.StandardScaler"": {},
}


# In[2]:


testC2 = TPOTClassifier(
generations=2,
population_size=30,
verbosity=2,
config_dict=medium_config,
n_jobs=2,
scoring=""accuracy"",
random_state=123,
use_dask=True,
template=""Transformer-Classifier"",
warm_start=True,
)

scatter = []
pipelineNames=[]

uniq_ind_count = []
old_inds = []
for i in range(20):
    testC2.fit(X_train,Y_train)
    
    all_inds=list(testC2.evaluated_individuals_.items())
    tuples = [i for i in all_inds if i not in old_inds] # new pipelines in a iteration
    old_inds = all_inds
    tuples.sort(key=lambda x: x[1][""internal_cv_score""], reverse=True)
    
    shownModels = []
    
    uniq_ind = []
    for ind in tuples:
        if not uniq_ind.count(ind[0]):
            uniq_ind.append(ind[0])
    print('Iteration', i, '# Unique new pipelines',len(uniq_ind))
    uniq_ind_count.append(len(uniq_ind))
    for x in tuples:
        pipeline = x[0]
        name = pipeline[: pipeline.find(""("")]
        if name in shownModels:
            continue
        if(i==0):
            pipelineNames.append(name)
            scatter.append([])
        shownModels.append(name)
        description = x[1]
        score = description[""internal_cv_score""]
        pipelineNames_idx = pipelineNames.index(name)
        scatter[pipelineNames_idx].append(score)


# In[6]:


fig,ax=plt.subplots(1,1)
for j in range(len(scatter)):
    ax.plot(range(0, len(scatter[j])), scatter[j], ""-x"", label=pipelineNames[j])
ax.legend(pipelineNames)
plt.show()
```
![image](https://user-images.githubusercontent.com/21084970/71734408-ee94a300-2e19-11ea-906c-56963981a372.png)

```python


# In[4]:


print('Number of unique/new pipelines in each iteration', uniq_ind_count)


# In[5]:


print('Total number of unique pipelines in 20 iterations', len(list(testC2.evaluated_individuals_.keys())))


# In[7]:


testC = TPOTClassifier(
generations=2,
population_size=30,
verbosity=2,
config_dict=medium_config,
n_jobs=2,
scoring=""accuracy"",
random_state=123,
use_dask=True,
template=""Classifier"",
warm_start=True,
)

scatter = []
pipelineNames=[]

uniq_ind_count = []
old_inds = []
for i in range(20):
    testC.fit(X_train,Y_train)
    
    all_inds=list(testC.evaluated_individuals_.items())
    tuples = [i for i in all_inds if i not in old_inds] # new pipelines in a iteration
    old_inds = all_inds
    tuples.sort(key=lambda x: x[1][""internal_cv_score""], reverse=True)
    
    shownModels = []
    
    uniq_ind = []
    for ind in tuples:
        if not uniq_ind.count(ind[0]):
            uniq_ind.append(ind[0])
    print('Iteration', i, '# Unique new pipelines',len(uniq_ind))
    uniq_ind_count.append(len(uniq_ind))
    for x in tuples:
        pipeline = x[0]
        name = pipeline[: pipeline.find(""("")]
        if name in shownModels:
            continue
        if(i==0):
            pipelineNames.append(name)
            scatter.append([])
        shownModels.append(name)
        description = x[1]
        score = description[""internal_cv_score""]
        pipelineNames_idx = pipelineNames.index(name)
        scatter[pipelineNames_idx].append(score)


# In[8]:


fig,ax=plt.subplots(1,1)
for j in range(len(scatter)):
    ax.plot(range(0, len(scatter[j])), scatter[j], ""-x"", label=pipelineNames[j])
ax.legend(pipelineNames)
plt.show()
```
![image](https://user-images.githubusercontent.com/21084970/71734432-f9e7ce80-2e19-11ea-808d-e090157e4a49.png)


As the figures above, when template='Classifier', not all models did not appear new evaluated pipelines in > 3 iterations. I think, similar to my guess above, the reason is that points mutations could only happen on transformer step or hyper-parameters of 1-2 classifier in this small-size population since the population became homologous in one or two classifiers with high accuracy scores after a few iterations. ",submit fixing bug point testing issue also python import import import import fix random state independent dependent penalty dual false range range range range range bootstrap true false objective range subsample range threshold accuracy scatter range new iteration print unique new pipeline name pipeline name continue name name description score description name scatter score fig range scatter range scatter scatter image python print iteration print number unique list accuracy classifier scatter range new iteration print unique new pipeline name pipeline name continue name name description score description name scatter score fig range scatter range scatter scatter image appear new think similar guess reason could happen transformer step classifier population since population homologous one two high accuracy,issue,negative,positive,neutral,neutral,positive,positive
570595973,"I think there is one hacky way for feature selection based on optimization. You could try TPOT with `template=""SelectFromModel-Classifier""` or `template=""Selector-Classifier""` and then check which features are selected in the 1st step of the `fitted_pipeline_` attribute of fitted TPOTClassifier object.",think one hacky way feature selection based optimization could try check selected st step attribute fitted object,issue,negative,neutral,neutral,neutral,neutral,neutral
570593671,"Sure. got it. Thanks for the prompt response. Am I then right to understand that tpot currently doesn't support best feature subset selection? I mean based on optimization, getting the best subset of features",sure got thanks prompt response right understand currently support best feature subset selection mean based optimization getting best subset,issue,positive,positive,positive,positive,positive,positive
570592415,"FeatureSetSelector is a special new operator in TPOT. This operator enables feature selection based on _priori_ expert knowledge. So those feature sets are not get from genetic programing in TPOT. We added this operator for analysis big genomes data with _priori_ expert knowledge in biological pathways.
Please check this [link](https://epistasislab.github.io/tpot/using/#featuresetselector-in-tpot) for more details.",special new operator operator feature selection based expert knowledge feature get genetic added operator analysis big data expert knowledge biological please check link,issue,positive,positive,positive,positive,positive,positive
570581503,"Related to the issue #975. It is fixed in dev branch so far and will be fixed in next version of TPOT. you may install TPOT with patch into an environment via:

```pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development```

Note:  scikit-learn may need to be updated to 0.22 for using dev branch.",related issue fixed dev branch far fixed next version may install patch environment via pip install upgrade note may need dev branch,issue,negative,positive,neutral,neutral,positive,positive
570482459,No problem! A PR for an user friendly error message is welcome.,problem user friendly error message welcome,issue,negative,positive,positive,positive,positive,positive
570451586,"Of course! Thanks so much @weixuanfu . Yep, that was it, sorry. I thought my custom dataset was 2 class, but this made me realise there were nans that were forming a third class. I could do a PR to add to the error message?",course thanks much yep sorry thought custom class made forming third class could add error message,issue,negative,negative,neutral,neutral,negative,negative
570356876,"Thank you for reporting this issue. 

I checked the demo for reproducing this issue. In this case with `warm_start=True`, I found that the population became very homologous in the later iteration since population were passed over each iteration when `warm_start=True` and then point mutation operator somehow could not generate new pipeline (which is only happened when `warm_start=True`  and TPOTClassifier was fitted in a loop). My guesses about this issue: 
1. The main reason maybe that the self._pset was also updated each iteration and mutation operator failed to generate new pipeline only when `warm_start=True`
2. mutation could happen on transformer step only in this small-size population and the population became homologous in one or two classifiers with highest scores no matter if the reason 1 exists.

We need fix this bug related to `warm_start`.",thank issue checked issue case found population homologous later iteration since population iteration point mutation operator somehow could generate new pipeline fitted loop issue main reason maybe also iteration mutation operator generate new pipeline mutation could happen transformer step population population homologous one two highest matter reason need fix bug related,issue,negative,positive,neutral,neutral,positive,positive
570010092,"Yes it is, but I decided to report this anyway to ensure you were aware of this aspect too.
Thank you for your attention. It will be a bit of work but your effort is well appreciated.",yes decided report anyway ensure aware aspect thank attention bit work effort well,issue,positive,positive,positive,positive,positive,positive
569970656,Thank you for the suggestion. It is related to the issue #981 and it should be fixed in next version of TPOT. ,thank suggestion related issue fixed next version,issue,negative,positive,neutral,neutral,positive,positive
569970383,"The issue may be about using f1_score scorer in multi-class data, which is related to the issue #665. Please try “f1_macro” instead.",issue may scorer data related issue please try instead,issue,negative,neutral,neutral,neutral,neutral,neutral
569869306,"Was having the same problem.

My mistake, however, was naming my script tpot.py as an idiot haha",problem mistake however naming script idiot,issue,negative,negative,negative,negative,negative,negative
568754728,"It seems a compatibility issue with the latest scikit-learn 0.22 since there is no warning message with scikit-learn 0.21.3. But TPOT should work with those warnings. We will fix it in next version of TPOT. 

",compatibility issue latest since warning message work fix next version,issue,negative,positive,positive,positive,positive,positive
568749915,"@Midhilesh29 Cool, please submit a PR based on dev branch for this feature.",cool please submit based dev branch feature,issue,positive,positive,positive,positive,positive,positive
568732822,"Hi!
I wish to contribute to this feature.",hi wish contribute feature,issue,negative,neutral,neutral,neutral,neutral,neutral
567285288,"I see, my bad for not reading documentation carefully. Thank you so much.",see bad reading documentation carefully thank much,issue,negative,negative,negative,negative,negative,negative
567045704,"Please check the alphas param in [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html), which should a ndarray of shape (n_alphas,) and default alphas is `(0.1, 1.0, 10.0)` (a tuple), so the configuration of `alphas` should be a list of tuples, e.g. `[(0.1, 1.0, 10.0), (0.2, 2.0, 5.0)]` ",please check param shape default configuration list,issue,negative,neutral,neutral,neutral,neutral,neutral
566963547,"If you are still having issue, please try this. It worked for me.

from sklearn.model_selection import train_test_split",still issue please try worked import,issue,negative,neutral,neutral,neutral,neutral,neutral
566832556,"Yes it worked after adding the brackets. I do not understand why though.
Looking at other models, the values for variables to be tested are directly in a list or numpy array, instead of being a tuple within a list which has only 1 element.
May I know why is it that RidgeCV requires such a setting? Thank you.",yes worked understand though looking tested directly list array instead within list element may know setting thank,issue,positive,positive,neutral,neutral,positive,positive
566154314,"https://github.com/EpistasisLab/tpot/issues/946#issuecomment-557847726

As discussed in the issue #946, I think we should not allow TPOT reset `evaluated_individuals_` IF hyperparameters (except `generations`) are the same in next version of TPOT,",issue think allow reset except next version,issue,negative,neutral,neutral,neutral,neutral,neutral
566152753,"`evaluated_individuals_ ` should not include any duplicated pipelines or invalid pipelines (that has more than 1`PolynomialFeatures`, see [here](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/base.py#L1410-L1413)) thus the number of pipelines in `evaluated_individuals_ ` can be less than `population_size + generations × offspring_size` when warm_start=False.

In your case of warm_start=True, the max number of pipelines should be `3 × (population_size + 1× offspring_size) = 30` since `evaluated_individuals_` was reset each run (see [here](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/base.py#L569)). But for the same reason I mentioned above, the number of pipelines in `evaluated_individuals_` is less then that number because of excluding duplicated pipelines and invalid pipelines.
",include invalid see thus number le case number since reset run see reason number le number excluding invalid,issue,negative,neutral,neutral,neutral,neutral,neutral
566112490,"@weixuanfu thanks for merging, you can report false positives on the platform if you find any, we will try to fix them right away or ping me at mohit@deepsource.io for any queries/suggestions.",thanks report false platform find try fix right away ping,issue,negative,positive,neutral,neutral,positive,positive
566092767,"I have a quick look of the codes. There should be a bug in the configuration of `RidgeCV`: it should be `""alphas"": [(0.1, 1.0, 10.0)],` instead of `""alphas"": [0.1, 1.0, 10.0],`.

Please try the demo below to check if it works in your environment. 
```python
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)
test_config = {
    ""sklearn.linear_model.RidgeCV"": {
        ""alphas"": [(0.1, 1.0, 10.0)],
        ""fit_intercept"": [True, False],
        ""normalize"":[True,False],
        ""scoring"":[""neg_mean_squared_error"",""r2""],
        ""store_cv_values"":[True,False]
    }
}

tpot = TPOTRegressor(generations=5, 
                    population_size=50, 
                    verbosity=2, 
                    random_state=42,
                    config_dict=test_config)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))

```
 ",quick look bug configuration instead please try check work environment python import import import housing true false normalize true false scoring true false print,issue,positive,positive,neutral,neutral,positive,positive
566085238,"Thank you for the fixes and suggestion of DeepSource. I will merge this PR to dev branch for now. I checked the [`DeepSource` report](https://deepsource.io/gh/mohi7solanki/tpot/run/4ab15e8a-a2e5-443a-af45-928b2b2d3dd8/) from your forked repo, it seems tpot need a lot of code cleaning but some of major code issues may be not valid. We will check the docs of DeepSource and also clean TPOT codes before merging it to master branch. ",thank suggestion merge dev branch checked report forked need lot code cleaning major code may valid check also clean master branch,issue,positive,positive,positive,positive,positive,positive
564667807,So far it is not possible but I think it would be a nice new feature. Contributions are welcome for this new feature. ,far possible think would nice new feature welcome new feature,issue,positive,positive,positive,positive,positive,positive
564062599,"You may try use `setattr` for setting `random_state`. The exported code be something like:
```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=39)

exported_pipeline= DecisionTreeClassifier(criterion=""gini"", max_depth=8, min_samples_leaf=5, min_samples_split=5)

# Fix random state in exported estimator
if hasattr(exported_pipeline, 'random_state'):
    setattr(exported_pipeline, 'random_state', 39)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
```",may try use setting code something like python import import import import note make sure outcome column data file fix random state estimator,issue,positive,neutral,neutral,neutral,neutral,neutral
564047312,Nice catch! I will merge it into dev branch soon. ,nice catch merge dev branch soon,issue,negative,positive,positive,positive,positive,positive
564046762,Thank you for reporting this bug. I will make a patch for this issue. ,thank bug make patch issue,issue,negative,neutral,neutral,neutral,neutral,neutral
562664331,"Yes, just deleting the codes below and fit `training_features` directly in TPOTClassifier with `template='StandardScaler-Classifier'`. 
```python
sc = StandardScaler()
training_features_scaled = sc.fit_transform(training_features)
testing_features_scaled = sc.transform(testing_features)
```

Then the scaler should fitted on the training set in each cv fold for avoid leakage during TPOT optimization . Then the optimized pipeline should be fitted on `training_features` and can be used for the unseen `testing_features.`. If you prefer to check the transformed unseen  `testing_features`, please try` tpot_obj.fitted_pipeline_.steps[0][1].fit_transform(testing_features)`.

",yes fit directly python scaler fitted training set fold avoid leakage optimization pipeline fitted used unseen prefer check unseen please try,issue,positive,positive,positive,positive,positive,positive
562661128,"Ok, I'll test that out. Doesn't the standard scaler used on the unseen `testing_features` have to be first fitted to the `training_features`? So how does this scaler that was fit to the training features survive the process so I can use it again? Or can I just simply `fit_transform` on the unseen data?",test standard scaler used unseen first fitted scaler fit training survive process use simply unseen data,issue,negative,positive,positive,positive,positive,positive
562660162,"To avoild potential leakage, I don't think you need scale the `testing_features` any more if the template is 'Standardscaler-Classifier'. Because the output pipeline from TPOT has `StandardScaler` as first step and can be directly used on the non-scaled `testing_features`. Is this what you need?",potential leakage think need scale template output pipeline first step directly used need,issue,negative,positive,positive,positive,positive,positive
562658506,"Ok, The template I use is simply `Classifier`. I could add `Standardscaler` to the list. 

But how do I then scale the `testing_features` set that TPOT hasn't seen whatsoever?",template use simply classifier could add list scale set seen whatsoever,issue,negative,neutral,neutral,neutral,neutral,neutral
562627931,How about a template like `StandardScaler-Selector-Transformer-Classifier`? Since `StandardScaler` is [a operator in TPOT default configuration](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/config/classifier.py#L169).,template like since operator default configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
562585826,I am sorry for overlooking this issue for a while. I think a nicer solution is to let TPOT check y's shape to determine if TPOT is running on a multi-output problem and then switch configuration to Regressors which naively supports multi-output problems. Then the new parameter multi-output is not needed. Could you please make a update to this PR?,sorry issue think solution let check shape determine running problem switch configuration naively new parameter could please make update,issue,negative,negative,negative,negative,negative,negative
562584003,"Yes, one of the reasons that we did not merge #903 is that we hoped there was a nice solution without the flag. I forgot to push a comment to that PR.",yes one merge hoped nice solution without flag forgot push comment,issue,positive,positive,positive,positive,positive,positive
562559547,"That pull request is not merged, so it is not native to TPOT and therefore unsupported. 

Also if there is a solution that does not require a flag, that would of course be better (again, i'm looking into it when i find the time).",pull request native therefore unsupported also solution require flag would course better looking find time,issue,negative,positive,positive,positive,positive,positive
562546925,"I am still not getting it :) 

1. In #903 the code changes to use the ""native"" multioutput regressors is already presented. So why is there a need to 

> share the modifications with a demo via pull request?

since it already happened?

2. And if multioutput regressors can be used with adjustments of #903 and also we can make use of MultiOutputRegressor (with currently just one estimator), why @weixuanfu  are you saying that multioutput isn't supported?  ",still getting code use native already need share via pull request since already used also make use currently one estimator saying,issue,negative,neutral,neutral,neutral,neutral,neutral
562260899,"@ben-ix You are right. In the case of `max_time_mins` and `warm_start=True`, TPOT should evaluate those remaining pipelines. Thank you for catching this bug. The possible fix that you purposed should work. Could you please submit a PR for this issue?",right case evaluate thank catching bug possible fix work could please submit issue,issue,positive,positive,positive,positive,positive,positive
562257493,"Hi @weixuanfu,

> TPOT assign (5000, -inf) fitness valid to invalid pipelines that cannot process normally or are too time-consuming to finish in time budget of max_eval_time_mins.

Those are two cases, but theres also a third. When the max_time_mins (not max_eval_time_mins) is hit, so we do not evaluate all in the population. In this case, they are [assigned a bad fitness](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/base.py#L1345) as well.

Say we have 1,000 individuals in the population, and we hit max_time_mins after evaluating 300 of these. With warm start, on the next call to fit, I would expect these remaining 700 to be evaluated before continuing on withe evolution. Instead, on the first call they were assigned the invalid fitness so we never get to evaluate these and may lose good solutions, since we continue effectively using only the 300 from that generation.

I guess the easiest way to check this is to have a large population size, and a small max time mins. 
If you check the [size of individuals in evaluate](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/base.py#L1266) the number will be len(population) in subsequent calls to fit, but really it should be `len(population) - num_eval_ind` where num_eval_ind is the number completed in the last generation.

Perhaps a way is needed to distinguish between the cases you mentioned (the expected behaviour) and this edge case of having valid pipelines that just didn't get reached in time before the timeout",hi assign fitness valid invalid process normally finish time budget two there also third hit evaluate population case assigned bad fitness well say population hit warm start next call fit would expect withe evolution instead first call assigned invalid fitness never get evaluate may lose good since continue effectively generation guess easiest way check large population size small time check size evaluate number population subsequent fit really population number last generation perhaps way distinguish behaviour edge case valid get time,issue,positive,positive,positive,positive,positive,positive
562143816,"@ben-ix  In _evaluate_individuals, the fitness.valid check is to get the list of new-generated individuals via [crossover](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/gp_deap.py#L136) and [mutation](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/gp_deap.py#L92) where the fitness.valid should be deleted. TPOT assign `(5000, -inf)` fitness valid to invalid pipelines that cannot process normally or are too time-consuming to finish in time budget of `max_eval_time_mins`.",check get list via crossover mutation assign fitness valid invalid process normally finish time budget,issue,negative,positive,positive,positive,positive,positive
561847492,"Another related problem.  In _evaluate_individuals, the check for invalid fitnesses: 
`individuals = [ind for ind in population if not ind.fitness.valid]
`
The fitness.valid check just checks if theres a fitness assigned. However, tpot seems to use (5000, -inf) as an ""invalid"" fitness if an individual timeout, but it actually assigns this to an individual. So this check should also capture the individuals which have a fitness of (5000, -inf) right?  Something like

`[ind for ind in population if not ind.fitness.valid or ind.fitness = (5000, -inf)]`",another related problem check invalid population check there fitness assigned however use invalid fitness individual actually individual check also capture fitness right something like population,issue,positive,positive,neutral,neutral,positive,positive
561681728,"> > so using only the regressors that natively perform multi-output regression is not viable for now? because that seems to work already with some slight modifications (changing some sklearn metric iirc)
> 
> Hmm, maybe it is practical workaround. Could you please share the modifications with a demo via pull request?

Sure, i'll look it up when i find the time. But i think there were no changes necessary in tpot directly, just a slight modification of a sklearn metric. Will post once i get around to it.

>Now I am a bit confused :) I already implemented the changes of base.py as suggested in #903 . So now I am already able to run TPOT with the regressors that natively perform multi-output regression.

Yes that was my question. At least it works for those.",natively perform regression viable work already slight metric maybe practical could please share via pull request sure look find time think necessary directly slight modification metric post get around bit confused already already able run natively perform regression yes question least work,issue,positive,negative,neutral,neutral,negative,negative
561513289,"Thanks for your help, problem solved!

Also, thanks for your quick answer.

Mario

",thanks help problem also thanks quick answer,issue,positive,positive,positive,positive,positive,positive
561263699,"> so using only the regressors that natively perform multi-output regression is not viable for now? because that seems to work already with some slight modifications (changing some sklearn metric iirc)

Now I am a bit confused :) I already implemented the changes of base.py as suggested in #903 . So now I am already able to run TPOT with the regressors that natively perform multi-output regression. I didn't need to adjust metrics or whatever.  Am I doing this wrong??
 And well yes, I wanted to compare most regressors, I thought this was the whole point about TPOT and automatic ML: to compare a large set of different pipelines/algorithms. ",natively perform regression viable work already slight metric bit confused already already able run natively perform regression need adjust metric whatever wrong well yes compare thought whole point automatic compare large set different,issue,negative,negative,neutral,neutral,negative,negative
561231493,"> 
> 
> so using only the regressors that natively perform multi-output regression is not viable for now? because that seems to work already with some slight modifications (changing some sklearn metric iirc)

Hmm, maybe it is practical workaround. Could you please share the modifications with a demo via pull request?
",natively perform regression viable work already slight metric maybe practical could please share via pull request,issue,positive,negative,negative,negative,negative,negative
561225665,so using only the regressors that natively perform multi-output regression is not viable for now? because that seems to work already with some slight modifications (changing some sklearn metric iirc),natively perform regression viable work already slight metric,issue,negative,negative,negative,negative,negative,negative
561217098,"> Thank you for the correction!

Thank you for the project!  :-)",thank correction thank project,issue,positive,neutral,neutral,neutral,neutral,neutral
561213065,Do you know an alternative tool like TPOT that can do multi-output regression? Or a workaround? Like starting a loop with TPOT and changing each time the estimator? ,know alternative tool like regression like starting loop time estimator,issue,positive,neutral,neutral,neutral,neutral,neutral
561186932,"TPOT currently do not support multi-output regression and current configuration did not support more than one `estimator` option (similar to #956) within this kind of meta estimators, but we are working on adding supports for > 1 estimators and I think that is the first step for supporting multi-output regression.",currently support regression current configuration support one estimator option similar within kind meta working think first step supporting regression,issue,positive,positive,positive,positive,positive,positive
561184293,"I think this issue should be fixed in the latest version (0.11) of TPOT. Could you please try the small demo below to check if it works with TPOT v0.11 in your environment? If not, you may also need update your scikit-learn version to > 0.21.

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.metrics.scorer import make_scorer

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)
# Make a custom metric function
def my_custom_accuracy(y_true, y_pred):
    return float(sum(y_pred == y_true)) / len(y_true)

# Make a custom a scorer from the custom metric function
# Note: greater_is_better=False in make_scorer below would mean that the scoring function should be minimized.
my_custom_scorer = make_scorer(my_custom_accuracy, greater_is_better=True)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,
                      scoring=my_custom_scorer, n_jobs=-1, random_state=42)
tpot.fit(X_train, y_train)
```",think issue fixed latest version could please try small check work environment may also need update version python import import import import make custom metric function return float sum make custom scorer custom metric function note would mean scoring function,issue,negative,positive,neutral,neutral,positive,positive
561103248,"When I create my own score function and feed it into TPOT, I can't create a model when I set  **n_jobs = -1**.  My new score function is :


![image](https://user-images.githubusercontent.com/47488921/70043220-5c05a780-15c0-11ea-8188-5e5e51a02bdf.png)


When I execute the following:

![image](https://user-images.githubusercontent.com/47488921/70043980-b7846500-15c1-11ea-84bd-4c591f53fc6d.png)

I obtain the next error:


![image](https://user-images.githubusercontent.com/47488921/70044085-ed294e00-15c1-11ea-81cd-4d11817650a6.png)


I would appreciate any help!

Thank you,
Mario
",create score function feed ca create model set new score function image execute following image obtain next error image would appreciate help thank,issue,positive,positive,neutral,neutral,positive,positive
559531333,"That worked like a charm. Thanks so much for your help.

This also fixed an error that was preventing me from using multiple cores. There was a pickling error which has gone away after this fix.
Thanks
",worked like charm thanks much help also fixed error multiple error gone away fix thanks,issue,positive,positive,positive,positive,positive,positive
559519311,"Hi, it seems a issue about cv parameter . Could you please try `cv=TimeSeriesSplit(n_splits=5)` instead?
",hi issue parameter could please try instead,issue,negative,neutral,neutral,neutral,neutral,neutral
559169164,"You could just try to scoring=""brier_score_loss"" since it is available in scikit-learn `SCORERS` object (see this [link](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)). You may need update scikit-learn for using it. 


Or if you have have a customer scorer with y_prob.  try to use [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function for creating a scorer for scoring parameter in `TPOTClassifier`. Please check [the link](https://stackoverflow.com/questions/29664657/creating-scorer-for-brier-score-loss-in-scikit-learn) or [this one](https://github.com/scikit-learn/scikit-learn/blob/1495f69242646d239d89a5713982946b8ffcf9d9/sklearn/metrics/scorer.py#L502) for examples.",could try since available object see link may need update customer scorer try use function scorer scoring parameter please check link one,issue,negative,positive,positive,positive,positive,positive
558737463,"@jrm5100 I pushed the PR #967 with a quick demo for testing this issue. And I will merge it to development branch soon. 

you may install TPOT with patch into an environment via:

```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",quick testing issue merge development branch soon may install patch environment via shell pip install upgrade,issue,negative,positive,positive,positive,positive,positive
558728183,"In this case TPOT ran with a limited search space and template. So the `sklearn_pipeline_list` can be empty for prevent  the `_stop_by_max_time_mins()` from running even after the time limit.

I will push a patch to dev branch soon. ",case ran limited search space template empty prevent running even time limit push patch dev branch soon,issue,negative,negative,neutral,neutral,negative,negative
558725102,"Thank you for reporting this issue.

As you found it in this issue, max_time_mins may not stop by end of the time since TPOT only checks how many seconds it has used once a pipeline evaluation is done (or a chunk of pipelines is done if n_jobs != 1). So TPOT cannot interrupt pipeline evaluation even the max_time_mins limit is reached. 

I think we should refine this function of TPOT.
",thank issue found issue may stop end time since many used pipeline evaluation done chunk done interrupt pipeline evaluation even limit think refine function,issue,negative,positive,positive,positive,positive,positive
558175805,"@ben-ix  I think it is good idea to keep `evaluated_individuals_` when `warm_start=True` to avoid recomputing/regenerating duplicates. Maybe fit() function just don't need call fit_init() if warm_start=True and hasattr(self, '_pareto_front'). So I reopen this issue for fixing it in the future version of TPOT.",think good idea keep avoid maybe fit function need call self reopen issue fixing future version,issue,negative,positive,positive,positive,positive,positive
557847726,"Thanks for the fix @weixuanfu. Another somewhat related follow up problem. fit_init is called at every call to fit, so with warmstart its getting run several times. There is a check for this in fit_init `if not self.warm_start or not hasattr(self, '_pareto_front'):` which only resets some values if its not a warm start run.

However, some values still get reset. For example,[ self.evaluated_individuals_ = {}](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L569), which doesn't seem like it should happen. Even for a warm start, this cache should still be kept to avoid recomputing/regenerating duplicates, right?

Does fit_init even need to be called more than once if its a warmstart?",thanks fix another somewhat related follow problem every call fit getting run several time check self warm start run however still get reset example seem like happen even warm start cache still kept avoid right even need,issue,positive,positive,positive,positive,positive,positive
557547008,"Yes, the `ImportError` should be raised if the module cannot be imported but it is specified in TPOT's config when verbosity > 2.(see [those lines](https://github.com/EpistasisLab/tpot/blob/v0.11.0/tpot/operator_utils.py#L80-L83)). If you prefer using verboisty == 3 without xgboost, please check this [link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters)) for excluding `xgboost` from [TPOT default configuration](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py)",yes raised module verbosity see prefer without please check link excluding default configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
557543474,"If those preprocessing steps has done, they are not necessary in template. The main reason of adding those preprocessing steps into template is to let TPOT explore different combinations of feature selectors and transfomers for higher performance. ",done necessary template main reason template let explore different feature higher performance,issue,negative,positive,positive,positive,positive,positive
557329149,"Just a final question. I have already selected my features and processed them beforehand (imputing and scaling etc). So I don't really need `Selector-Transformer` steps, correct? Or would you still recommend it?",final question already selected beforehand scaling really need correct would still recommend,issue,negative,positive,neutral,neutral,positive,positive
557098949,"Yep, it should be. And `template ='Selector-Transformer-Classifier'` should give you a linear pipeline with 3 steps (1st: feature selection, 2nd: feature transformation; 3rd: a classifier) instead. ",yep template give linear pipeline st feature selection feature transformation classifier instead,issue,negative,neutral,neutral,neutral,neutral,neutral
556780485,"you should make the domain name trusted. run the following code before you install tpot:
pip --trusted-host pypi.python.org --trusted-host pypi.org install --trusted-host files.pythonhosted.org  Active-SQLAlchemy==0.4.0",make domain name run following code install pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
556058236,"In both cases (template ='Selector-Transformer-Classifier' or template='Classifier'), StackingEstimator won't be used.
But if the template is something like 'Selector-Classifier-Transformer-Classifier', the StackingEstimator will be used for stacking the predictions from the 1st Classifier. ",template wo used template something like used st classifier,issue,negative,neutral,neutral,neutral,neutral,neutral
556054179,"So in the docs it says to use the argument like:
```python
tpot_obj = TPOTClassifier(
                template='Selector-Transformer-Classifier'
                )
```
To remove the Stacking Estimator, I would simply set the template as classifier only like the below?:

```python
tpot_obj = TPOTClassifier(
                template='Classifier'
                )
```
",use argument like python remove estimator would simply set template classifier like python,issue,negative,neutral,neutral,neutral,neutral,neutral
556031645,One workaround is to set a template for TPOT where Regressor/Classifior is only the last step of pipeline. (please see this [link](https://epistasislab.github.io/tpot/using/#template-option-in-tpot)). Then TPOT should not use stacking estimator.,one set template last step pipeline please see link use estimator,issue,negative,neutral,neutral,neutral,neutral,neutral
555529757,"I don't have a good answer to this question. We did use TPOT for 1m records before and it indeed are very time-consuming. So far, we have some alternative solutions, like `subsample` parameter, [`TPOT light` config](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) or [`Parallel Training with Dask`](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask), to deal with large dataset, but I think none of them are ideal. There are a few ideas for improving TPOT on large dataset:
1. A new GPU-based config for deal with large dataset (mentioned in the issue #452 )
2. New subsample algorithm (see #888 )
But so far I did not get much time to fully evaluate them yet.
Any contribution is welcome!",good answer question use indeed far alternative like subsample parameter light parallel training deal large think none ideal improving large new deal large issue new subsample algorithm see far get much time fully evaluate yet contribution welcome,issue,positive,positive,positive,positive,positive,positive
555084574,Maybe RAM in colab is not enough. How large are your dataset and maximum RAM requested in colab?,maybe ram enough large maximum ram,issue,negative,positive,positive,positive,positive,positive
553560271,"Thank you, I customized my TPOT config to exclude PolynomialFeatures and XGBoost since I already know that XGBoost does not perform as well on my data set. Hopefully this will speed up the process. Maybe this will fix the joblib errors, I'll post an update after this runs for a while.",thank exclude since already know perform well data set hopefully speed process maybe fix post update,issue,positive,neutral,neutral,neutral,neutral,neutral
553546136,"Skipping pipelines is fine because TPOT should avoid evaluating duplicated pipelines.
About those feature_name error, I think it may be from xgboost, you could convert pandas.DataFrame into numpy.ndarray (e.g `df.values`) as input X (related to the issue #738 )
The joblib error message seems indicating that `TimeoutException` was not working as expected.  I am not sure how this happened. Could you please try your codes without using Jupyter notebook for a test?

BTW, for this large dataset, I suggest to use ""TPOT light"" configuration or customize a TPOT configuration without [`PolynomialFeatures`](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py#L156)",skipping fine avoid error think may could convert input related issue error message working sure could please try without notebook test large suggest use light configuration configuration without,issue,negative,positive,positive,positive,positive,positive
553459800,"Yes, I think it should be stored on your scheduler's hard drive.",yes think hard drive,issue,negative,negative,negative,negative,negative,negative
553418455,"Could you please provide more details about this issue, like versions of TPOT and its dependencies? Also could you please provide codes for reproducing this issue?

Based on the error message, the reason maybe that the input X is not a 2-D array or y is not a 1-D array. Please reshape them based on the error message. ",could please provide issue like also could please provide issue based error message reason maybe input array array please reshape based error message,issue,negative,neutral,neutral,neutral,neutral,neutral
553417874,"Could you please provide more details about this issue, like versions of TPOT and its dependencies? Also could you please provide codes for reproducing this issue?",could please provide issue like also could please provide issue,issue,positive,neutral,neutral,neutral,neutral,neutral
552465138,"@lmsanch there are some related updates in version 0.11 (please [see the release log here](https://github.com/EpistasisLab/tpot/releases)).

The set_param_recursive function has been moved to tpot.export_utils. You may import it via `from tpot.export_utils import set_param_recursive`. About `random_state`, TPOT v0.11 does not use fixed random seed 42 for evaluating pipelines but use the random seed set by `random_state` parameter instead. If random_state=None, then the results may not be reproduced.",related version please see release log function may import via import use fixed random seed use random seed set parameter instead may,issue,negative,negative,negative,negative,negative,negative
552458819,@gulabshah778 which version of TPOT? Maybe updating TPOT will help you solve the issue.,version maybe help solve issue,issue,positive,neutral,neutral,neutral,neutral,neutral
552064326,"I found a solution, but TPOT has a bug in scorer (Although the functions shows up in the SCORER dictionary, it is not taken by TPOT)

```
#SCORERS['rmsle_loss'] = make_scorer(RMSLE, greater_is_better=False)
rmsle_loss= make_scorer(RMSLE, greater_is_better=False)
params ={'cv':5,
         'scoring': rmsle_loss,
         'generations':10,
         'random_state':0,
         'max_eval_time_mins':10}
```
",found solution bug scorer although scorer dictionary taken,issue,negative,neutral,neutral,neutral,neutral,neutral
552025982,"Updated TPOT to 0.11 via:
`pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development`
Same issue with:
```
SCORERS['rmsle_loss'] = make_scorer(RMSLE, greater_is_better=False)
params ={'cv':5,
         'scoring': 'rmsle_loss',
         'generations':10,
         'random_state':0,
         'max_eval_time_mins':10}
```
I get:

`ValueError: 'rmsle_loss' is not a valid scoring value. Use sorted(sklearn.metrics.SCORERS.keys()) to get valid options.`",via pip install upgrade issue get valid scoring value use sorted get valid,issue,negative,neutral,neutral,neutral,neutral,neutral
551222953,@pyacciITT please run `tpot._fit_init()` to get `_pset` attribute without fitting.,please run get attribute without fitting,issue,negative,positive,positive,positive,positive,positive
551220766,"When I try to utilize,  tpot._pset on unfitted TPOTClassifier (as suggested by @CSNoyes), I receive the following error

AttributeError: 'TPOTClassifier' object has no attribute '_pset'

Is there a way to get the _pset attribute without fitting?",try utilize unfitted receive following error object attribute way get attribute without fitting,issue,negative,positive,positive,positive,positive,positive
550027344,The issue has been fixed in a new version of TPOT (v0.10.1). Please feel free to reopen it if you have any questions or suggestions. ,issue fixed new version please feel free reopen,issue,positive,positive,positive,positive,positive,positive
550026844,The issue has been fixed in a new version of TPOT (v0.11.0) because pywin32 package is not needed any more. Please feel free to reopen it if you have any questions or suggestions. ,issue fixed new version package please feel free reopen,issue,positive,positive,positive,positive,positive,positive
550025614,There is no update about this issue for a while. Please feel free to reopen it if you have any questions or suggestions. ,update issue please feel free reopen,issue,positive,positive,positive,positive,positive,positive
550025001,The issue has been fixed in new version of TPOT (v0.11.0) with updating version of tqdm. Please feel free to reopen it if you have any questions or suggestions. ,issue fixed new version version please feel free reopen,issue,positive,positive,positive,positive,positive,positive
550018235,OK changes was pushed to dev branch. We will release a version with those fixes soon.,dev branch release version soon,issue,negative,neutral,neutral,neutral,neutral,neutral
550010231,"@weixuanfu thanks! Seems to work as intended now. 

Only some slight points

 [Line 704](https://github.com/weixuanfu/tpot/blob/d0e87cc1e1ed084b3181b0d06871a118c6fdcced/tpot/base.py#L704) should be self._pop, _ ... (or else just not saving the return values of eaMuPlusLambda). Functionality wont change but the local var pop is just not used now

The [change](https://github.com/weixuanfu/tpot/blob/0def2d34ebffce9dd2d3d79e3fe6ba2d3b97184f/tpot/base.py#L1386) previously added in _evaluate_individuals  to update self._pop should now be removed, since that can set self._pop to offspring (at least temporarily until its updated from eaMuPlusLambda)

I agree with the reset in the case warm_start = False

Feel free to close the issue 
",thanks work intended slight line else saving return functionality wont change local pop used change previously added update removed since set offspring least temporarily agree reset case false feel free close issue,issue,positive,negative,neutral,neutral,negative,negative
549997570,Thank you for debugging. I added some changes in PR #949 (merged to dev) and #952 (I will merged to dev soon). I think `self._pop` should be reset to `[]` if `warm_state=False` since this object may take too much memory.,thank added dev dev soon think reset since object may take much memory,issue,negative,positive,positive,positive,positive,positive
549976805,"Hi @weixuanfu,
> The first assignment exists on this line in original eaMuPlusLambda.

The difference is that fitnesses is a local variable created in eaMuPlusLambda. But then when population is modified in the original code, they are not creating a new local variable called population, they are updated the original list in eaMuPlusLambda directly (population[:] = ). However, in tpot, the first assignment (population = ) is creating a new local variable, so it never modifies the original population passed in. This does not occur in the original eaMuPlusLambda. 


> I think the self._pareto_front should be updated after reaching the max_time_mins limit (see this line)

Yup the pareto front and cache work as expected

> But the bug is caused by those lines. The population update should be moved under expect... in this case.

This is what I initially thought too, but _evaluate_individuals is not always given then population, in the general case its only given the offspring (despite the variable name). So you're storing the population as the offspring, and not the result of select(population + offspring), meaning the results will not be quite as intended in the [fix](https://github.com/EpistasisLab/tpot/pull/949/commits) 

To verify this, you can check the calls to toolbox.evaluate. It works in the first case [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L227) because the population is given, but in the general case [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L250) only the offspring is given. So the population will be saved as the offspring, not the result of select(population + offspring, mu).

My suggestion would be, in eaMuPlusLambda change

`population = toolbox.evaluate(population)`

To

`population[:] = toolbox.evaluate(population)`

So you update the actual population passed in. 

Then in base.py change 

```
if self._pop:
    pop = self._pop
else:
    pop = self._toolbox.population(n=self.population_size)
```

to

```
if not self.warm_start or not self._pop:
    self._pop = self._toolbox.population(n=self.population_size)
```

And update the call to eaMuPlusLambda

```
self._pop, _ = eaMuPlusLambda(
     population= self._pop,
     ....... # Rest stays the same
)
```",hi first assignment line original difference local variable population original code new local variable population original list directly population however first assignment population new local variable never original population occur original think reaching limit see line front cache work bug population update expect case initially thought always given population general case given offspring despite variable name population offspring result select population offspring meaning quite intended fix verify check work first case population given general case offspring given population saved offspring result select population offspring mu suggestion would change population population population population update actual population change pop else pop update call rest stay,issue,positive,positive,positive,positive,positive,positive
549854040,"Also, the exported code is just an example and you should also update the data processioning before `exported_pipeline = ...` based on this [Jupyter notebook](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb)",also code example also update data based notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
549850779,"We are updating our docs now.

Please update the expected codes in the screenshot based on the demo below:

```python
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# NOTE: Make sure that the class is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=None)

exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)

exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)

```",please update based python import import import import note make sure class data file,issue,positive,positive,positive,positive,positive,positive
549847662,"Thank you for reporting this issue.

The first assignment exists on [this line](https://github.com/DEAP/deap/blob/master/deap/algorithms.py#L301) in original `eaMuPlusLambda`.

I think the self._pareto_front should be updated after reaching the max_time_mins limit (see [this line](https://github.com/EpistasisLab/tpot/blob/v0.10.2/tpot/base.py#L1400))

But the bug is caused by [those lines](https://github.com/EpistasisLab/tpot/blob/v0.10.2/tpot/base.py#L755-L757). The population update should be moved under [`expect...`](https://github.com/EpistasisLab/tpot/blob/v0.10.2/tpot/base.py#L760) in this case.

I will fix this bug soon. ",thank issue first assignment line original think reaching limit see line bug population update expect case fix bug soon,issue,positive,positive,positive,positive,positive,positive
549652196,"I fixed this locally by making eaMuPlusLambda modify self._pop directly, because this is what's done in the original deap code anyway. But im not sure if this is the approach you want to take or not.

Its kind of ""half"" there already, in that an in place update of population is done [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L253), but the [first assignment](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L227) to population prevents this from updating the actual self._pop list that gets passed in. In the [original eaMuPlusLambda](https://github.com/DEAP/deap/blob/master/deap/algorithms.py#L248), this first assignment doesn't exist.",fixed locally making modify directly done original code anyway sure approach want take kind half already place update population done first assignment population actual list original first assignment exist,issue,positive,positive,positive,positive,positive,positive
549125520,"So I couldn't actually find any code that makes use of the returning of `True` or `False` based on whether the file was successfully written or not.

This PR therefore proposes to instead return the pipeline text as a string and only export to file when a file name is specified. In this way the function can be used to export to file AND/OR to string.",could actually find code use true false based whether file successfully written therefore instead return pipeline text string export file file name way function used export file string,issue,positive,positive,positive,positive,positive,positive
546919155,Please rebase your working branch to dev branch for avoiding conflicts.,please rebase working branch dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
546918405,It seems not a TPOT-related issue now. Please use the proper issue title and also provide detailed description for resubmitting a issue.,issue please use proper issue title also provide detailed description issue,issue,negative,positive,positive,positive,positive,positive
546916515,Thank you for pointing out this error in dataset descriptions. We will correct them in TPOT README and examples. ,thank pointing error correct,issue,negative,neutral,neutral,neutral,neutral,neutral
546907416,"Personally I think that when you say MNIST dataset you definitely do not mean the test portion of https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits dataset loaded by `load_digits` -- it returns 1797 digits in 8x8 resolution, compared to 60k+10k of 28x28 MNIST.

Furthermore, http://epistasislab.github.io/tpot/examples/ in the Overview says ""MNIST"" dataset, the dataset description links to the real MNIST https://yann.lecun.com/exdb/mnist/, and then there is link to Jupyter notebook https://github.com/EpistasisLab/tpot/blob/master/tutorials/MNIST.ipynb using the `load_digits` method.",personally think say definitely mean test portion loaded resolution furthermore overview description link real link notebook method,issue,negative,negative,neutral,neutral,negative,negative
546774808,"Any tips will be greatly appreciated, because I am really eager to make it.",greatly really eager make,issue,negative,positive,positive,positive,positive,positive
546680856,"Git messed up. Please don't merge yet, i will clean up my branch.",git please merge yet clean branch,issue,positive,positive,positive,positive,positive,positive
546636078,"Found a demo code block by @weixuanfu at https://github.com/EpistasisLab/tpot/issues/337#issuecomment-470681480. I think adding a flag to `.export()` which would run something along the lines of that demo and thus print the `generate_pipeline_code` to screen instead of to file would be exactly what I am looking for!

Edit: Actually, this might be as simple as setting a boolean flag which would print to screen instead of to file.",found code block think flag would run something along thus print screen instead file would exactly looking edit actually might simple setting flag would print screen instead file,issue,negative,positive,neutral,neutral,positive,positive
546634073,Hello this is not a bar or cafe! This section is for reporting issues you encounter while working with this module.,hello bar section encounter working module,issue,negative,neutral,neutral,neutral,neutral,neutral
545917671,"In this case I think you should run TPOT with code (see [this link](https://epistasislab.github.io/tpot/using/#tpot-with-code)). Since `TPOTClassifier`/`TPOTRegessor` follow the scikt-learn API, then you can fit your `TPOTClassifier` by something like `pipeline_optimizer.fit(X_train, y_train)` (here the `pipeline_optimizer=TPOTClassifier(...)`) then get predicted target variable via `y_pred = pipeline_optimizer.predict(X)`",case think run code see link since follow fit something like get target variable via,issue,positive,positive,positive,positive,positive,positive
545519350,Those error messages should be from xgboost. Could you please check if the version of xgboost is up-to-date in Ubuntu?,error could please check version,issue,negative,neutral,neutral,neutral,neutral,neutral
545517845,"I am running your suggestions, seems to be working on and off, trying to find out what's the issue. Nevertheless. when running identical train, test sets  (with same seeds) in both, a mac and Ubuntu, I get different output to console. Regardless of what level of verbosity  is choose, every generation prints the output below in Ubuntu (which I don't get in Mac):
```
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:38] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:41] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:pairwise
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:ndcg
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: rank:map
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:hinge
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softmax
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: multi:softprob
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:linear
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:reg:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logistic
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: gpu:binary:logitraw
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: count:poisson
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: survival:cox
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:gamma
[11:58:42] /workspace/src/objective/objective.cc:21: Objective candidate: reg:tweedie
```",running working trying find issue nevertheless running identical train test mac get different output console regardless level verbosity choose every generation output get mac objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg objective candidate rank pairwise objective candidate rank objective candidate rank map objective candidate binary hinge objective candidate objective candidate objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate reg linear objective candidate reg logistic objective candidate binary logistic objective candidate binary objective candidate count objective candidate survival cox objective candidate reg gamma objective candidate reg,issue,negative,negative,negative,negative,negative,negative
545427190,I like this proposal and I think it's more logical.,like proposal think logical,issue,negative,positive,positive,positive,positive,positive
545405991,"@lmsanch I forgot to mention that this customized loss function should be not supported in the dev branch (and future version of TPOT). Please use [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) to convert it to a scorer callable object / function with signature scorer(estimator, X, y). Here is a [example](https://github.com/EpistasisLab/tpot/blob/master/tpot/metrics.py) within TPOT. 

Thanks for @jhmenke's suggestion. You may check the converted scorer with [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function from scikit-learn.",forgot mention loss function dev branch future version please use convert scorer callable object function signature scorer estimator example within thanks suggestion may check converted scorer function,issue,negative,positive,neutral,neutral,positive,positive
545312758,I will take a look at it as soon i have time. can you assign me?,take look soon time assign,issue,negative,neutral,neutral,neutral,neutral,neutral
545310754,did you debug inside your custom scorer to see if anything is wrong there?,inside custom scorer see anything wrong,issue,negative,negative,negative,negative,negative,negative
545230851,"Thank you @weixuanfu . This might be a new bug. I followed the steps you indicated, and I get the Runtime error below:
It runs for a while, and when it is about to update the results for the first generation (verbose mode 2), the error occurs.
To recap, I am running tpot inside my own function, which does some plotting, generates some metrics, etc. and runs well with built in function. This problem is happening trying to score with a custom RMSE scoring function.

```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/anaconda3/lib/python3.7/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    746                     verbose=self.verbosity,
--> 747                     per_generation_function=self._check_periodic_pipeline
    748                 )

/anaconda3/lib/python3.7/site-packages/tpot/gp_deap.py in eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function)
    235         if per_generation_function is not None:
--> 236             per_generation_function(gen)
    237         # Vary the population

/anaconda3/lib/python3.7/site-packages/tpot/base.py in _check_periodic_pipeline(self, gen)
   1039         """"""
-> 1040         self._update_top_pipeline()
   1041         if self.periodic_checkpoint_folder is not None:

/anaconda3/lib/python3.7/site-packages/tpot/base.py in _update_top_pipeline(self)
    829             if not self._optimized_pipeline:
--> 830                 raise RuntimeError('There was an error in the TPOT optimization '
    831                                    'process. This could be because the data was '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-97-e985e44f3405> in <module>
----> 1 solution = regression_portfolio(train, y_train1, test, params)

<ipython-input-96-352f913cfc5b> in regression_portfolio(train, y_train, predict_oos, params, plot_errors, plot_frontier)
     54 
     55     # here train is done on a cross validation basis, on the train segment
---> 56     tpot.fit(X_train, y_train)
     57     best = tpot.predict(predict_oos)
     58     test_pred = tpot.predict(X_test)

/anaconda3/lib/python3.7/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    777                     # raise the exception if it's our last attempt
    778                     if attempt == (attempts - 1):
--> 779                         raise e
    780             return self
    781 

/anaconda3/lib/python3.7/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    768                         self._pbar.close()
    769 
--> 770                     self._update_top_pipeline()
    771                     self._summary_of_best_pipeline(features, target)
    772                     # Delete the temporary cache before exiting

/anaconda3/lib/python3.7/site-packages/tpot/base.py in _update_top_pipeline(self)
    828 
    829             if not self._optimized_pipeline:
--> 830                 raise RuntimeError('There was an error in the TPOT optimization '
    831                                    'process. This could be because the data was '
    832                                    'not formatted properly, or because data for '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.
```
",thank might new bug get error update first generation verbose mode error recap running inside function plotting metric well built function problem happening trying score custom scoring function recent call last fit self target population toolbox mu verbose none gen vary population self gen none self raise error optimization could data error optimization process could data properly data regression problem provided object please make sure data correctly handling exception another exception recent call last module solution train test train train done cross validation basis train segment best fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self raise error optimization could data properly data error optimization process could data properly data regression problem provided object please make sure data correctly,issue,positive,positive,positive,positive,positive,positive
545138211,"I think this issue is related to #906 and it should be fixed in dev branch. Could you please try dev branch? You may install dev branch via the command below.

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```",think issue related fixed dev branch could please try dev branch may install dev branch via command pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
544314737,"Also happening for me, regardless of whether it is in a Jupyter kernel or not.",also happening regardless whether kernel,issue,negative,neutral,neutral,neutral,neutral,neutral
543736886,Thank you for this PR. But this issue was fixed in PR #924 and merged into development branch. I should close that issue. ,thank issue fixed development branch close issue,issue,negative,positive,neutral,neutral,positive,positive
543249664,"I think both are the same pipeline but in different format. The one in stdout is in deap format but the one in exported pipeline file is scikit-learn Pipeline format. In TPOT, deap is used for generating tree-based pipeline and we also use this deap string format for storing pipeline because it take less resource. For evaluating the pipeline, TPOT should convert the deap format to scikit-learn Pipeline internally. You may use scikit-learn Pipeline in exported pipeline without using deap or TPOT. ",think pipeline different format one format one pipeline file pipeline format used generating pipeline also use string format pipeline take le resource pipeline convert format pipeline internally may use pipeline pipeline without,issue,negative,neutral,neutral,neutral,neutral,neutral
541680747,@gb96 Thank you for this finding. I like the 1st solution about using a GPU_accelerated config. It can be a optional config for users with GPU resources.   ,thank finding like st solution optional,issue,positive,neutral,neutral,neutral,neutral,neutral
541368903,"Have a look at the H2O4GPU project here:
https://github.com/h2oai/h2o4gpu

It provides a ""drop-in replacement for sklearn"".

I was able to get it working in TPOT, however ran into the problem that when you replace some optimizers/estimators with GPU versions, the total run-time of TPOT becomes dominated by:
1. Any remaining optimizers/estimators that are not yet GPU accelerated.
2. GPU initiation and data transfer overheads.

The way to address 1 is to minimize non-accelerated computation by replacing more components with accelerated versions and also removing non-accelerated candidates from the config.

The way to address 2 is to somehow know when to use GPU and when to use CPU based on each algorithm and dataset.",look project replacement able get working however ran problem replace total becomes dominated yet accelerated initiation data transfer way address minimize computation accelerated also removing way address somehow know use use based algorithm,issue,negative,positive,positive,positive,positive,positive
539999467,This message is not related to `memory` or tpot_pipelines folder but about `evaluated_individuals_` attribute of tpot object ([see TPOT API](https://epistasislab.github.io/tpot/api/) for more details about his attribute). TPOT should check if the pipelines that were evaluated in previous generations of a TPOT run because those identical pipelines could be randomly generated via mutation or crossover.  So TPOT should skip evaluating them to save computational time. I don't think this function should effects multiple TPOT for the same dataset since the `evaluated_individuals_`  should be reset when initializing a new TPOT object in each TPOT run.,message related memory folder attribute object see attribute check previous run identical could randomly via mutation crossover skip save computational time think function effect multiple since reset new object run,issue,negative,negative,negative,negative,negative,negative
538417598,"I am sorry for late response due to out of office for more than one month. I think removing  `MLPClassifier`/`MLPRegessor` (only using Keras wraper) in this transformer is more practical because  `MLPClassifier`/`MLPRegessor` are too slow on large dataset. As mentioned above, it will be an optional transformer. Also, it should be highly suggested on user guide that using Keras on GPU is highly recommended when using this transformer. ",sorry late response due office one month think removing transformer practical slow large optional transformer also highly user guide highly transformer,issue,negative,negative,neutral,neutral,negative,negative
537516753,"@weixuanfu, I've decided to remove the link from the beginning since it is already there on the section **Citing TPOT** as you said. I kept just the small addition on the text in the beginning explaining the acronym. Sorry again, just want to keep it very simple for now.",decided remove link beginning since already section said kept small addition text beginning explaining acronym sorry want keep simple,issue,negative,negative,negative,negative,negative,negative
537056174,"Thanks for the PR., 

Please use the hyperlink of the paper instead of using the link directly.

The paper was also mentioned in “Citing TPOT” section below. You may use the similar format there.",thanks please use paper instead link directly paper also section may use similar format,issue,positive,positive,positive,positive,positive,positive
537038426,"@weixuanfu, I've sent a simple PR for this as a test. I want to learn how to contribute and this is my first time doing this, so I didn't want to change much to break anything important. Don't know if you can help me.",sent simple test want learn contribute first time want change much break anything important know help,issue,positive,positive,positive,positive,positive,positive
537000910,It is not possible for now. Any contribution is welcome. [`_add_operators` function](https://github.com/EpistasisLab/tpot/blob/v0.10.2/tpot/base.py#L429-L481) should be a good start point.,possible contribution welcome function good start point,issue,positive,positive,positive,positive,positive,positive
536358570,Thank you for catching this issue and this PR. I merged it to dev branch and will release the fix on next version of TPOT.,thank catching issue dev branch release fix next version,issue,negative,positive,positive,positive,positive,positive
535102261,"I'm not using anaconda, so to solve this issue I needed to:
1. pip install pywin32
2. pip install pypiwin32
3. Follow the instructions from the following post:  https://github.com/michaelgundlach/pyspeech/issues/23#issuecomment-280915608
> I have the same problem for days, but one day I solved it.
> After you installed the pywin32 libs, there is a directory ""Lib/site-packages/pywin32_system32"", which including three dll libs, copy them to the ""/Lib/site-packages/win32"" directory, which including the win32api.pyd or win32api.pyc.
> There will be no ImportError Exception any more

Then no more errors :)",anaconda solve issue pip install pip install follow following post problem day one day directory three copy directory exception,issue,negative,neutral,neutral,neutral,neutral,neutral
534042883,"I think you could use custom validation set(s) via cv parameter (similar to issue #767).

Please check the `cv` parameter in [TPOT API](https://epistasislab.github.io/tpot/api/). You can merge the validation set  with the trainset for fitting in `tpot_obj.fit(X, y)` and then specify train/test splits via an iterable (see [the example](https://stackoverflow.com/questions/27097330/how-to-customize-sklearn-cross-validation-iterator-by-indices) for `GridSearchCV`)",think could use custom validation set via parameter similar issue please check parameter merge validation set fitting specify via iterable see example,issue,negative,positive,positive,positive,positive,positive
533775777,I have right now used CalibratedClassifierCV with LinearSVC to get probability scores for n classes..,right used get probability class,issue,negative,positive,positive,positive,positive,positive
533489272,"Does the configuration of TPOT run only has SVM algorithms? I think I met this issue before but without using TPOT. I ran [`cross_eval_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) on SVM (probability=True) on a very small dataset. Then I got a error including  the predication of SVM contains NaN. But sometimes it works on other datasets. Could you please test this configuration on another dataset, like Iris, to check if this issue is reproducible? A demo of reproducing this issue is preferred.",configuration run think met issue without ran small got error predication nan sometimes work could please test configuration another like iris check issue reproducible issue preferred,issue,negative,negative,negative,negative,negative,negative
531114966,If it's okay to not user SCORERS in that case i might be able to fix it over the weekend.,user case might able fix weekend,issue,negative,positive,positive,positive,positive,positive
531051866,"Thank you for reporting this issue. I think maybe a better workaround is not to SCORERS for custom scorer ([related codes](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L320-L359)) but pass the scorer directly to [`_wrapped_cross_val_score`](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L382-L418). 

BTW, I am on vacation those days and will work on this issue later. But contribution is welcome. ",thank issue think maybe better custom scorer related pas scorer directly vacation day work issue later contribution welcome,issue,positive,positive,positive,positive,positive,positive
530608597,@jhmenke TPOT should regenerate the same best model with the same arguments in the same running environment. ,regenerate best model running environment,issue,positive,positive,positive,positive,positive,positive
530519389,"Does that mean i can recreate the TPOT model exactly by using the same CV and a random_state=42 (wherever possible)?

",mean recreate model exactly wherever possible,issue,negative,negative,neutral,neutral,negative,negative
528841528,"Thanks!

For anyone finding this topic later. You can integrate MLPClassifier easily by adding it to the config:

```
def mlp_layer_conf():
    return tuple(np.random.randint(5, 100) for _ in range(np.random.randint(1, 5)))


'sklearn.neural_network.MLPClassifier': {
    'hidden_layer_sizes': [mlp_layer_conf()],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.001, 0.005, 0.01, 0.05],
    'learning_rate': ['constant', 'adaptive'],
    'max_iter': [100, 200, 500, 1000, 2000, 5000],
    'activation': ['tanh', 'logistic', 'relu'],
    'early_stopping': [True, False],
    'learning_rate_init': [1e-4, 5e-4, 1e-3],
    'validation_fraction': [0.1, 0.15, 0.2, 0.25],
    'n_iter_no_change': [3, 5, 10, 20]
    }
```",thanks anyone finding topic later integrate easily return range true false,issue,positive,positive,positive,positive,positive,positive
528840712,"`MLPClassifier` is very computational intensive for some parameter combinations, which is main reason we did not include it into default setting of TPOT.",computational intensive parameter main reason include default setting,issue,negative,positive,positive,positive,positive,positive
528839744,I think you may easily use `LeaveOneOut` via setting `cv=LeaveOneOut()` within `TPOTClassifier` or `TPOTRegessor`. (see [TPOT API](http://epistasislab.github.io/tpot/api/) for details),think may easily use via setting within see,issue,negative,positive,positive,positive,positive,positive
528136103,"Please add `config_dict=""TPOT sparse""` among TPOTClassifier's arguments when using  umpy sparse matrices. Please check [this link](http://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) for more details.",please add sparse among sparse matrix please check link,issue,positive,neutral,neutral,neutral,neutral,neutral
528124860,I am also having this issue even though my sklearn version is 0.21.2,also issue even though version,issue,negative,neutral,neutral,neutral,neutral,neutral
527019976,"> how was your scoring function to blame?
> 
> I'm running into something similar

Sorry, i'm not sure anymore. Maybe i was using scoring for binary classification in a multi-class classification problem or something like that. I remember having some issues with that.",scoring function blame running something similar sorry sure maybe scoring binary classification classification problem something like remember,issue,negative,neutral,neutral,neutral,neutral,neutral
526925344,"how was your scoring function to blame?

I'm running into something similar",scoring function blame running something similar,issue,negative,neutral,neutral,neutral,neutral,neutral
526803393,"I have followed above process and have installed everything, still I am not getting progressbar. 

I am executing my code on Jupyter notebook and not JupyterLab ., if itmakes any difference.",process everything still getting code notebook difference,issue,negative,neutral,neutral,neutral,neutral,neutral
526780738,"Hmm, I did not have this error before. Maybe converting your input dataset from `pd.DataFrame` to `np.ndarray` can help.

Could you please provide more details about this issue, like a demo for reproducing it?",error maybe converting input help could please provide issue like,issue,positive,neutral,neutral,neutral,neutral,neutral
526739269,"I was running this in ubuntu/  jupyterlab - and realized this is was a tqdm issue: https://github.com/tqdm/tqdm/issues/394

I followed the instructions here to install the necessary jupyterlab exension - and it worked for me

https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-the-jupyterlab-extension

...
For example, if using conda environments, you can install nodejs with:
```
conda install -c conda-forge nodejs
```
Then you can install the labextension:
```
jupyter labextension install @jupyter-widgets/jupyterlab-manager

```",running issue install necessary worked example install install install install,issue,negative,neutral,neutral,neutral,neutral,neutral
525941244,"Please check API of [`roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) from scikit-learn. The 2nd argument of this function should `y_score` instead of `y_pred` in the demo above.

The demo below should help you to reproduce roc_auc score. 

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import numpy as np
from sklearn.metrics import roc_auc_score

bc = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(bc.data.astype(np.float64),
    bc.target.astype(np.float64), train_size=0.15, test_size=0.85)

tpot = TPOTClassifier(generations=5, population_size=50,scoring='roc_auc', verbosity=2)
tpot.fit(X_train, y_train)
print('from tpot.score')
print(tpot.score(X_test, y_test))

try:
    probas_ = tpot.predict_proba(X_test)[:, 1]
except AttributeError:
    probas_ = tpot.decision_function(X_test)
print('from roc_auc_score')
print(roc_auc_score(y_test, probas_))
```

For the 2nd question, we prefer using [`balanced_accuracy`](https://github.com/EpistasisLab/tpot/blob/master/tpot/metrics.py#L69) for imbalanced data.",please check argument function instead help reproduce score python import import import import import print print try except print print question prefer data,issue,negative,neutral,neutral,neutral,neutral,neutral
525767690,"Thanks for the suggestion. It stand for **T**ree-based **P**ipeline **O**ptimization **T**ool mentioned in the first paper of TPOT. I think this acronym info should be added into README on Github repo.

```
@inproceedings{OlsonGECCO2016,
    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},
    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
    series = {GECCO '16},
    year = {2016},
    isbn = {978-1-4503-4206-3},
    location = {Denver, Colorado, USA},
    pages = {485--492},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/2908812.2908918},
    doi = {10.1145/2908812.2908918},
    acmid = {2908918},
    publisher = {ACM},
    address = {New York, NY, USA},
}
```",thanks suggestion stand first paper think acronym added author title evaluation pipeline optimization tool data science genetic evolutionary computation conference series year location colorado publisher address new york,issue,positive,positive,positive,positive,positive,positive
523452736,"Thank you for reporting this issue! TPOT will deprecate the scoring function with signature `y_pred, y_true` in version 0.11 (as mentioned [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L347-L350)) and then the scoring-related function in TPOT should be fixed and simplified by using [`check_scoring`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.check_scoring.html) function from sklearn. Contribution is welcome!",thank issue deprecate scoring function signature version function fixed simplified function contribution welcome,issue,positive,positive,positive,positive,positive,positive
522872701,"Yes, it went fine on the example provided in your documentation

[Untitled.pdf](https://github.com/EpistasisLab/tpot/files/3518989/Untitled.pdf)
",yes went fine example provided documentation,issue,positive,positive,positive,positive,positive,positive
522757525,Did this configuration pass a test with smaller dataset? It seems the dataset is too large to finish cv  even within 15 minutes time budget.,configuration pas test smaller large finish even within time budget,issue,negative,positive,positive,positive,positive,positive
522746391,"It became worse.

I'm wondering if there is something not working with the parralel execution

![Capture d’écran 2019-08-19 à 22 43 43](https://user-images.githubusercontent.com/40396043/63297971-d68d0e80-c2d2-11e9-91a2-5c3244a3890b.jpg)
",worse wondering something working execution capture,issue,negative,negative,negative,negative,negative,negative
522589958,"Thanks for your input.
I'll give a go.
SHould i still keep using dask ?I'm developing on a jupyter notebook",thanks input give go still keep notebook,issue,negative,positive,positive,positive,positive,positive
522569495,"Did this configuration pass in smaller dataset?  If yes, I think the issue maybe caused by high number of `n_iter` in `RepeatedHoldout` and `max_eval_time_mins=5` in default TPOT setting. Increasing `max_eval_time_mins` in `TPOTClassifier` for allowing more time budget of evaluating a single pipeline should be helpful. ",configuration pas smaller yes think issue maybe high number default setting increasing time budget single pipeline helpful,issue,positive,positive,neutral,neutral,positive,positive
522002457,"@windowshopr no problem! Regarding another way for getting features importance from pipelines, you may try to estimate [permutation importance](https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html).",problem regarding another way getting importance may try estimate permutation importance,issue,negative,neutral,neutral,neutral,neutral,neutral
521876705,"@weixuanfu  Moving that section to the bottom of the code seemed to do the trick, as well as refitting the model to the ""features, label"" sets in the predictions dataframe part before calling .predict() there. If I spot another issue, I'll bring it up after a few more tests. Thanks a lot!! Saved me such a headache!",moving section bottom code trick well model label part calling spot another issue bring thanks lot saved headache,issue,positive,positive,positive,positive,positive,positive
521834939,"@weixuanfu  Thanks for your input, but I'm not 100% sure what you're talking about with the `StackingEstimator` or what it's used for.

The section with the `exctracted_best_model = best_model.fitted_pipeline_.steps[-1][1]` is to be able to find the feature importances using the pipeline that was created with TPOT, as per [this answer](https://stackoverflow.com/questions/57369927/getting-feature-importances-after-getting-optimal-tpot-pipeline) on StackOverflow. Is there a better way for me to get the feature importances after the best pipeline has been created?

I'm going to move that entire section:

```
# Extract what the best pipeline was and fit it to the training set
# to get an idea of the most important features used by the model were.
# Is there a way to do this during the first .fit() function so that
# we don't have to run it a second time here? Althought it's not a big deal.
print('Now fit the best pipeline to the training data to find feature importances...')
exctracted_best_model = best_model.fitted_pipeline_.steps[-1][1]

# Train only the `exctracted_best_model` using the training/vildation set
exctracted_best_model.fit(X_train, Y_train)

# plot model's feature importance and save the plot for later
feature_importance = exctracted_best_model.feature_importances_
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)
pos        = np.arange(sorted_idx.shape[0]) + .5
plt.figure(figsize=(40,20)) # play with this to adjust the size of the plot to your specs
plt.barh(pos, feature_importance[sorted_idx], align='center')
plt.yticks(pos, df.columns[sorted_idx])
plt.xlabel('Relative Importance')
plt.title('Variable Importance')
plt.savefig(""feature_importance.png"")
plt.clf()
plt.close()
print('Done!')
```

...to after the section about predicting on the ""prediction dataset"" so that the `exctracted_best_model = best_model.fitted_pipeline_.steps[-1][1]` doesn't interfere with any of the steps before the first .predict() function and see if that does anything?
",thanks input sure talking used section able find feature pipeline per answer better way get feature best pipeline going move entire section extract best pipeline fit training set get idea important used model way first function run second time big deal print fit best pipeline training data find feature train set plot model feature importance save plot later play adjust size plot spec importance importance print section prediction interfere first function see anything,issue,positive,positive,positive,positive,positive,positive
521671254,"



```
exctracted_best_model = best_model.fitted_pipeline_.steps[-1][1]
```
The codes above cause the issue here. I think `best_model.fitted_pipeline_` should have a step including `StackingEstimator` for stacking the predictions of another regressor as synthetic features, thus the last step used 1 more feature. If you only need the regressor in the last step, you should refit it. Or just use the whole pipeline via `exctracted_best_model = best_model.fitted_pipeline_`


> 
> 
> Ok I'm still having this issue and none of what I've read so far seems to be helping my code.
> 
> If I leave the X_train and X_test features as a dataframe, if XGBRegressor is in my pipeline, I get the error Feature Names Mismatch, and a bunch of ""f0, f1, f2"" stuff, so I convert the features of both sets to numpy arrays, trying both .values and .as_matrix(), which is fine, but then if RandomForestRegressor or DecisionTreeRegressor is used in my pipeline, when I run the .predict(X_test) function, I get:
> 
> `ValueError: Number of features of the model must match the input. Model n_features is 117 and input n_features is 118`
> 
> ...even though printing the shape of the X_test array before .predict() is showing 117!!??
> 
> Check out my [Colab workbook here](https://colab.research.google.com/drive/112svKVH5UTW8zQ3DBTlOfviZ-074bOaq#scrollTo=73MZu1j_Tp_i) where I've saved the error of the latter issue. You can make comments there. The code is VERY well commented.
> 
> I'm at a loss as to what to do. I'd like to be able to have both XGBRegressor and all the tree regressors in my project, but it seems that they both prefer having their data sent to them in different ways. It's frustrating running a pipeline for a while, only to have it crash at the predict() phase, EVEN THOUGH IT SHOWS THE X_TEST ARRAY SHAPE AS BEING 117, NOT 118!?!? Any suggestions? Thanks.
> 
> Also see my [question here:](https://stackoverflow.com/questions/57488274/feature-names-mismatch-when-passing-x-test-to-predict-function-again-still)

",cause issue think step another regressor synthetic thus last step used feature need regressor last step refit use whole pipeline via still issue none read far helping code leave pipeline get error feature mismatch bunch stuff convert trying fine used pipeline run function get number model must match input model input even though printing shape array showing check workbook saved error latter issue make code well loss like able tree project prefer data sent different way running pipeline crash predict phase even though array shape thanks also see question,issue,negative,positive,positive,positive,positive,positive
521456627,"Ok I'm still having this issue and none of what I've read so far seems to be helping my code.

If I leave the X_train and X_test features as a dataframe, if XGBRegressor is in my pipeline, I get the error Feature Names Mismatch, and a bunch of ""f0, f1, f2"" stuff, so I convert the features of both sets to numpy arrays, trying both .values and .as_matrix(), which is fine, but then if RandomForestRegressor or DecisionTreeRegressor is used in my pipeline, when I run the .predict(X_test) function, I get:

`ValueError: Number of features of the model must match the input. Model n_features is 117 and input n_features is 118` 

...even though printing the shape of the X_test array before .predict() is showing 117!!??

Check out my [Colab workbook here](https://colab.research.google.com/drive/112svKVH5UTW8zQ3DBTlOfviZ-074bOaq#scrollTo=73MZu1j_Tp_i) where I've saved the error of the latter issue. You can make comments there. The code is VERY well commented.

I'm at a loss as to what to do. I'd like to be able to have both XGBRegressor and all the tree regressors in my project, but it seems that they both prefer having their data sent to them in different ways. It's frustrating running a pipeline for a while, only to have it crash at the predict() phase, EVEN THOUGH IT SHOWS THE X_TEST ARRAY SHAPE AS BEING 117, NOT 118!?!? Any suggestions? Thanks.

Also see my [question here:](https://stackoverflow.com/questions/57488274/feature-names-mismatch-when-passing-x-test-to-predict-function-again-still)",still issue none read far helping code leave pipeline get error feature mismatch bunch stuff convert trying fine used pipeline run function get number model must match input model input even though printing shape array showing check workbook saved error latter issue make code well loss like able tree project prefer data sent different way running pipeline crash predict phase even though array shape thanks also see question,issue,negative,positive,positive,positive,positive,positive
521248363,Thank you for this PR. The unit tests should pass but it failed due to installation of latest dask in CI environment. I will fix this issue in #904 and merge it to dev branch soon. ,thank unit pas due installation latest environment fix issue merge dev branch soon,issue,negative,positive,positive,positive,positive,positive
520830563,"This would be a great feature, e.g., for MLflow to track metrics across generations. What would be required to implement this?",would great feature track metric across would implement,issue,positive,positive,positive,positive,positive,positive
519123091,Maybe pickling `tpot_obj.fitted_pipeline_` (which is fitted scikit-learn pipeline) is better. (Related to this issue #520),maybe fitted pipeline better related issue,issue,negative,positive,positive,positive,positive,positive
519118337,"ok. for now I kind of do the job by doing this after the export:
```
tpot.export('tpot_pipeline_%s.py'%(_scoring))
f = open(""tpot_pipeline_%s.py""%(_scoring),""a+"")
f.write(""from sklearn.externals import joblib\n"")
f.write(""import os\n"")
f.write(""joblib.dump(exported_pipeline,os.path.basename(__file__).replace("".py"","".pkl""))\n"")
```",kind job export open import import,issue,positive,positive,positive,positive,positive,positive
519105503,TPOT don't support this import function yet. I think it should be a interesting function which TPOT may support later.,support import function yet think interesting function may support later,issue,positive,positive,positive,positive,positive,positive
518624855,"While working on the demo, I have followed this installation instructions https://epistasislab.github.io/tpot/installing/ instead of just `conda install -c conda-forge tpot `.
In the new virtualenv, I couldn't reproduce the issue anymore. When I have tried to follow the same installation procedure again in the previous virtualenv, it has turned into an utter mess - I can't even import pandas anymore. The solution therefore is to scrape the old virtualenv and keep conda and pip strictly separated at all times... ",working installation instead install new could reproduce issue tried follow installation procedure previous turned utter mess ca even import solution therefore scrape old keep pip strictly time,issue,negative,negative,neutral,neutral,negative,negative
518368634,"I caught this line `_wrapped_cross_val_score() got an unexpected keyword argument 'timeout'` in the stdout, which is weird since the function was wrapped by `
@threading_timeoutable` decorator from `stopit`. Could you please let us know more details about his issue, like OS, versions of TPOT and its dependencies? A demo for reproducing this issue is preferred. ",caught line got unexpected argument weird since function wrapped decorator could please let u know issue like o issue preferred,issue,negative,negative,negative,negative,negative,negative
517274040,"Please check [TPOT API](http://epistasislab.github.io/tpot/api/), the `fitted_pipeline_` attribute will show the pipeline. Also you could export the pipeline via `clf_tpot.export('xxx.py')` to python codes.",please check attribute show pipeline also could export pipeline via python,issue,negative,neutral,neutral,neutral,neutral,neutral
516850263,"OK, we will refine installation guide for this issue. ",refine installation guide issue,issue,negative,neutral,neutral,neutral,neutral,neutral
516755286,Just wanted to let you know my conda environment also did not come with `pywin32`. After installing it all works well.,let know environment also come work well,issue,negative,neutral,neutral,neutral,neutral,neutral
516551502,"I think this issue is related to how to use `.loc` of pandas.Dataframe. I think the `train` maybe a multidimensional key. If so, please convert it to a 1D indexer.",think issue related use think train maybe multidimensional key please convert indexer,issue,negative,neutral,neutral,neutral,neutral,neutral
516477881,"Hmm, sorry for overlooking this issue. Is this `EnvironmentError` fixed in your environment? ",sorry issue fixed environment,issue,negative,negative,negative,negative,negative,negative
516475160,The default template is `None` since 0.10.2. We will refine error message in this case. ,default template none since refine error message case,issue,negative,neutral,neutral,neutral,neutral,neutral
516238810,"Ok, 

Given the higher level comments @weixuanfu how should we proceed. In my opinion, I think this PR is just too big, what we probably want is:

*  a PR that just adds back ""RandomTree"" as an option so that we can do things like ""PCA-RandomTree"" or similar (have to ensure than RandomTree is the very last option""; in doing so this enables
*  Another PR that proposes how the preprocessing transform should work - there are downsides to this, as it means someone using the preprocessing has to directly alter config options for every model. e.g.

(as per comments above)

```
X_train, y_train = ...
my_meta_data_info = {<insert meta data information>}
config_dict = {**my_meta_data_info, **TPOT_DEFAULT_CONFIG}
tpot = TPOTClassifier(config_dict=config_dict, template='Preprocess-RandomTree')
tpot.fit(X_train, y_train)
```

versus

(as per approach taken in this PR)

```
X_train, y_train = ...
tpot = TPOTClassifier(config_dict=None, preprocess_config_dict ={<insert meta data>})
tpot.fit(X_train, y_train)
```

",given higher level proceed opinion think big probably want back option like similar ensure last option another transform work someone directly alter every model per insert meta data information versus per approach taken insert meta data,issue,positive,positive,neutral,neutral,positive,positive
516087280,"Yes, changing the random_state would definitely incur the cost of increased compute in exchange for the expected benefit of increased generalization robustness of the final model.  Definitely something that should be should be opted into rather than a default.",yes would definitely incur cost compute exchange benefit generalization robustness final model definitely something rather default,issue,positive,neutral,neutral,neutral,neutral,neutral
515985143,"TPOT currently stores average cv scores of evaluated pipelines with a fixed random_state and skip evaluating duplicated pipelines to save computational time. Changing the random_state or cv splits each generation need re-evaluate those duplicated pipelines.

I like the idea about a per generation configuration.",currently average fixed skip save computational time generation need like idea per generation configuration,issue,positive,negative,neutral,neutral,negative,negative
515775824,Thank you for catching this typo. I will merge this PR soon and check your other PRs tomorrow.,thank catching typo merge soon check tomorrow,issue,negative,positive,positive,positive,positive,positive
515681146,"I've had a go at this here: https://github.com/chappers/tpot/tree/feat/text_preprocess 
its not quite 100% working to a level that should have a PR on it. Keen for feedback on the interface. Here is a short snippet using Iris dataset:

```py
X_train_df = pd.DataFrame(X_train, columns=[""num1"", ""num2"", ""num3"", ""num4""])
X_train_df['text'] = np.random.choice([""hello"", ""world"", ""foo"", ""bar world"", ""bar hello""], X_train.shape[0])

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2""]
                      })
tpot2.fit(X_train_df, y_train)

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2"", ""num3"", ""num4""]
                      })
tpot2.fit(X_train_df, y_train)

tpot2 = TPOTClassifier(generations=1, population_size=5, verbosity=2, template=""PCA-LogisticRegression"", 
                      preprocess_config_dict = {
                          'numeric_columns': [""num2""],
                          'text_columns': ['text']
                      })
tpot2.fit(X_train_df, y_train)
```

And the output looks like this:

```
Generation 1 - Current best internal CV score: 0.5908385093167702

Best pipeline: LogisticRegression(PCA(PreprocessTransformer(input_matrix, numeric_columns=['num2']), iterated_power=2, svd_solver=randomized), C=20.0, dual=True, penalty=l2)

Generation 1 - Current best internal CV score: 0.9556935817805383

Best pipeline: LogisticRegression(PCA(PreprocessTransformer(input_matrix, numeric_columns=['num2', 'num3', 'num4']), iterated_power=9, svd_solver=randomized), C=20.0, dual=False, penalty=l1)

Generation 1 - Current best internal CV score: 0.5096273291925466

```

Essentially the approach is for a user to provide some metadata (numeric columns, text columns, categorical columns) which is then injected into the setup (I can imagine several reasons why we shouldn't do what I've done in my code; we can deal with that later). 

You can see from above, that it appears to work; whereby a user can selectively choose which columns to be used in their pipeline (including text). In this pipeline, and through the templates we can also force TPOT to optimise variety of vectorizers as well if we wish.",go quite working level keen feedback interface short snippet iris hello world foo bar world bar hello output like generation current best internal score best pipeline generation current best internal score best pipeline generation current best internal score essentially approach user provide text categorical setup imagine several done code deal later see work whereby user selectively choose used pipeline text pipeline also force variety well wish,issue,positive,positive,positive,positive,positive,positive
515602629,I like the idea of changing the random_state each generation.  Independent of that I've been thinking about being able to specify the population on a per generation basis as I'm thinking it may be useful to be able to keep shrinking it or even dynamically expand/contract it based on the number of pipelines in the pareto front.  Putting the two together leads to the idea of a per generation configuration either provided statically upfront or that is retrieved from user supplied function before each generation is executed.,like idea generation independent thinking able specify population per generation basis thinking may useful able keep shrinking even dynamically based number front two together idea per generation configuration either provided statically user function generation executed,issue,positive,positive,positive,positive,positive,positive
515594815,"I think it should related to random_state=None in final pipeline. 

We have some discussions about it on this related issue #514.",think related final pipeline related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
515584049,"@weixuanfu I would expect using the random_state supplied to TPOTBase for the pipelines during search would allow users to get broader coverage in the searches when they are interested in trying different random seeds.

I'm not sure why the final pipeline (in memory or exported) would want to use None for the random_state.  I haven't checked for reproducibility yet on the project I'm currently working on, but on one for a previous employer I was seeing some cases of the same data and the same configuration for TPOT producing pipelines that would make different predictions, so I'm now wondering if random_state being None was contributing to that (don't have access to verify though).",would expect search would allow get coverage interested trying different random sure final pipeline memory would want use none checked reproducibility yet project currently working one previous employer seeing data configuration would make different wondering none access verify though,issue,positive,positive,neutral,neutral,positive,positive
515569997,"We intentionally set the random_state of 42 during search and there should no random_state in exported pipeline. And the random_state in TPOTBase is for randomly generating pipeline or crossover/mutation. We encountered a few similar issues before, maybe we should use the `random_state` in TPOTBase for search. ",intentionally set search pipeline randomly generating pipeline similar maybe use search,issue,negative,negative,negative,negative,negative,negative
515550123,"The difference in the random_state of the pipelines for search and the final fit results TPOTBase._preprocess_individuals having the following block of code after calling self._toolbox.compile while _summary_of_best_pipeline lacks an equivalent:

                    # Fix random state when the operator allows
                    self._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
                    # Setting the seed is needed for XGBoost support because XGBoost currently stores
                    # both a seed and random_state, and they're not synced correctly.
                    # XGBoost will raise an exception if random_state != seed.
                    if 'XGB' in sklearn_pipeline_str:
                        self._set_param_recursive(sklearn_pipeline.steps, 'seed', 42)

After adding a equivalent block in _summary_of_best_pipeline I no longer see an exception raised during the final fit of that model.",difference search final fit following block code calling equivalent fix random state operator setting seed support currently seed correctly raise exception seed equivalent block longer see exception raised final fit model,issue,negative,positive,neutral,neutral,positive,positive
515542782,"@weixuanfu I've also opened https://github.com/EpistasisLab/tpot/issues/894 as that appears to be an independent issue, although it is generally contributing to allowing an exception to be raised during the final fit of the top model.  In the third comment on this thread, that random_state is also being provided a value of None although the classifier was created with a random_state of 1 in that case as well.",also independent issue although generally exception raised final fit top model third comment thread also provided value none although classifier case well,issue,positive,positive,positive,positive,positive,positive
515528112,"@weixuanfu In this case the cv has been provided as class instance whose split method yields one pairs of training and test indexes.  The indexes are the first (in time) 80% of the data and test set is the last 20% of the data.

It seems that _update_top_pipeline is only called immediately before _summary_of_best_pipeline except when checkpoints are being written out, so it seems like it would be possible that a check in _update_top_pipeline only after the search could find that there is no valid pipeline to replace the top one with if it should be found invalid there.

In my overnight run I encountered another failure fitting the top pipeline in _summary_of_best_pipeline even with my workaround in place on somewhat different data then that workaround initially allowed running through cleanly.  Investigating.",case provided class instance whose split method one training test first time data test set last data immediately except written like would possible check search could find valid pipeline replace top one found invalid overnight run another failure fitting top pipeline even place somewhat different data initially running cleanly investigating,issue,negative,positive,positive,positive,positive,positive
515450461,"@JoshHeitzman Thank for checking this issue again. Based on your findings, I think there are two problems here:

1. `StackingEstimator` stacked NaN/infinite values so that the best pipeline failed when fitting it on entire dataset. This issue can be solved by adding NaN/infinte checking/imputation into `StackingEstimator`

2. `SelectFwe` returned a empty X when fitting it on entire dataset, which I think it should not happen very frequently . For fixing this issue, maybe we should change `_update_top_pipeline` function to checking if the pipeline can be fitted on entire dataset instead of checking it during search.

I am wondering what is the `cv` setting in your case. It seems that the training subsets in CV was very different with entire dataset.
",thank issue based think two best pipeline fitting entire issue returned empty fitting entire think happen frequently fixing issue maybe change function pipeline fitted entire instead search wondering setting case training different entire,issue,positive,positive,positive,positive,positive,positive
515441515,@chappers Thank you for sharing this demo and information. It looks interesting to me so PR is welcome. We need test its performance later.,thank information interesting welcome need test performance later,issue,positive,positive,positive,positive,positive,positive
515430402,"I'm happy to submit a PR with something like this if there is interest. IMO, you'll need parameters to initialise the model dynamically (can be done trivially with some broad assumptions) - if we don't want the annoying `input_shape` hack, we can initialise the model when the `fit` method is called. 

In terms of the `transform` method; specifically the `np.hstack` personally I think this should be avoided; from a design perspective it will suffer from the same challenges as implementing the text one that I demoed on #507 - which is we should be using the in-built `scikit-learn` `ColumnTransformer` rather than a `np.hstack`. 

I think its worth opening up the discussion; though in the meantime people can just hack the pipelines with their own custom transformers :) (though I do realise its not as user friendly). 

We can take inspiration from https://github.com/uber/ludwig/ as well which has a focus on automating pipelines specifically around controlling the embedding and stacking embeddings ontop of each other...",happy submit something like interest need model dynamically done trivially broad want annoying hack model fit method transform method specifically personally think design perspective suffer text one rather think worth opening discussion though people hack custom though user friendly take inspiration well focus specifically around,issue,positive,positive,positive,positive,positive,positive
515423302,"If you know what you want you can write your own custom transformer like this:

```py
# adding keras model
import numpy as np
import pandas as pd

from tpot import TPOTClassifier
from sklearn.base import TransformerMixin, BaseEstimator
from tpot.config import classifier_config_dict_light
from sklearn.datasets import make_classification

from tensorflow.keras import backend as K
import tensorflow.keras as keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
import copy

class CustomKerasFeatures(TransformerMixin, BaseEstimator):
    def __init__(self, input_shape, epochs=1000, batch_size=32):
        model = Sequential()
        model.add(Dense(32, activation='relu', input_dim=100))
        model.add(Dense(20, activation='relu')) # embedding size
        model.add(Dense(2, activation='softmax'))
        model.compile(optimizer='rmsprop',
                      loss='categorical_crossentropy',
                      metrics=['accuracy'])
        self.model = model
        self.epochs = epochs
        self.batch_size = batch_size
        self.input_shape = input_shape
    
    def _prepare_X(self, X):
        if X.shape[1] >= self.input_shape:
            X = X[:, :self.input_shape]
        else:
            result = np.zeros((X.shape[0], self.input_shape))
            result[:X.shape[0], :X.shape[1]] = X
            X = result
        return X
    
    def fit(self, X, y=None):
        X_ = self._prepare_X(X.copy())
        y_ohe = keras.utils.to_categorical(y.copy())
        self.model.fit(X_, y_ohe, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self
    
    def transform(self, X):
        X_ = self._prepare_X(X.copy())
        # get embedding or the 2nd last layer
        get_embedding = K.function([self.model.layers[0].input],
                                   [self.model.layers[-2].output])
        return np.hstack([X, get_embedding([X_])[0]])

X, y = make_classification()

# using TPOT config
config = copy.deepcopy(classifier_config_dict_light)
config[""__main__.CustomKerasFeatures""] = {
    ""input_shape"": [100],
    ""epochs"": [10]
}

tpot = TPOTClassifier(config_dict=config, verbosity=3, generations=5, population_size=2, early_stop=2, max_time_mins=5,
                     template='CustomKerasFeatures-Selector-Transformer-Classifier')
tpot.fit(X, y)
```",know want write custom transformer like model import import import import import import import import import sequential import dense activation import copy class self model sequential dense dense size dense model self else result result result return fit self return self transform self get last layer return,issue,positive,positive,positive,positive,positive,positive
515237572,"@weixuanfu I believe I've figured out what is allowing for exceptions to be thrown in _summary_of_best_pipeline when the pipeline didn't fail during the search.  In _summary_of_best_pipeline the complete set of data is used to fit the pipeline, where as during search its only a subset(s) of the data as some data is reserved for validation.  I ran into another case were it was the fit of the best pipeline that raised an exception in _summary_of_best_pipeline.  In this case the StackingEstimator wasn't involved.  Instead sklearn.feature_selection.SelectFwe was being used and it was doing a transform that resulted in theere being zero feature columns, butut this only happened when it was fit to the entire data set and not to the subset used for training.

My current workaround it is to modify gp_deap._wrapped_cross_val_score to do:
clone(sklearn_pipeline).fit(features, target)

just before the loop on cv_iter, so that the problem is detected at a point where -inf can be returned to cause the pipeline to be ignored in favor other pipelines that won't fail on the final fit to the full data set.",believe figured thrown pipeline fail search complete set data used fit pipeline search subset data data reserved validation ran another case fit best pipeline raised exception case involved instead used transform zero feature fit entire data set subset used training current modify clone target loop problem point returned cause pipeline favor wo fail final fit full data set,issue,positive,positive,positive,positive,positive,positive
515126369,"When the exception is raised, up the stack in _summary_of_best_pipeline I can get the following print of the steps for the pipeline being processed:

for i, step in enumerate(self._toolbox.compile(expr=pipeline).steps):
    print(i, step)

0 ('stackingestimator', StackingEstimator(estimator=LogisticRegression(C=0.01, class_weight=None,
                                               dual=True, fit_intercept=True,
                                               intercept_scaling=1,
                                               l1_ratio=None, max_iter=100,
                                               multi_class='warn', n_jobs=None,
                                               penalty='l2', random_state=None,
                                               solver='warn', tol=0.0001,
                                               verbose=0, warm_start=False)))
1 ('bernoullinb', BernoulliNB(alpha=0.001, binarize=0.0, class_prior=None, fit_prior=False))
",exception raised stack get following print pipeline step enumerate print step,issue,negative,neutral,neutral,neutral,neutral,neutral
515019976,"Thanks for tracking this errors. 

I think it could be a bug in `StackingEstimator` for stacking NAN/infinite prediction/predication probabilities (maybe related to [this issue](https://stackoverflow.com/questions/49525618/sklearns-predict-proba-returns-infinite-probabilties)). But it is strange that the pipeline passed during CV but did not work in `fit` step in `summary_of_best_pipeline`. Could you please let us know all the steps in this pipeline for reproducing this error?",thanks think could bug maybe related issue strange pipeline work fit step could please let u know pipeline error,issue,negative,positive,positive,positive,positive,positive
514659356,Excellent - Thanks so much for quick responses. Will close this now. ,excellent thanks much quick close,issue,positive,positive,positive,positive,positive,positive
514561388,@mhrihab I am also facing the same issue with the sklearn version (0.21.2) . model_selection is getting imported but not working ,also facing issue version getting working,issue,negative,neutral,neutral,neutral,neutral,neutral
514382310,"@weixuanfu - Thanks for the quick response. So let me confirm my understanding of your suggestion. Are you saying, I do the following instead of say setting `generations=10` during instantiation? 

`digits = load_digits()`
`X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25)`

`pipeline_optimizer = TPOTClassifier(generations=1, population_size=20, cv=5,`
                                    `random_state=42, verbosity=2, warm_start=True)`
`for i in range(10): # where 10 is a proxy for gens`
   `  pipeline_optimizer.fit(X_train, y_train)`
   `  # get test performance stats `


",thanks quick response let confirm understanding suggestion saying following instead say setting range proxy gen get test performance,issue,negative,positive,positive,positive,positive,positive
513881597,You could try the `warm_start` parameter in TPOT API with generations=1 to check the test performance every generation. (Related issue #832 ),could try parameter check test performance every generation related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
512246195,The issue was fixed (#887 Thanks @thunfischtoast ) in the latest version of TPOT so I closed it. Please feel free to reopen if there is another issue.,issue fixed thanks latest version closed please feel free reopen another issue,issue,positive,positive,positive,positive,positive,positive
512245410,The issue was fixed in latest version of TPOT 0.10.2 so I closed this issue. Please feel free to reopen if there are any other issues/questions.,issue fixed latest version closed issue please feel free reopen,issue,positive,positive,positive,positive,positive,positive
510881206,Agreed. It is time for future updates of TPOT to drop support for Python 2.,agreed time future drop support python,issue,negative,neutral,neutral,neutral,neutral,neutral
510880490,"I agree with this update. Also, for addressing this and also supporting future version of sklearn (> 0.22), TPOT should not support Python 2 any more. ",agree update also also supporting future version support python,issue,positive,positive,positive,positive,positive,positive
510844660,"Ah okok thank you!
So I guess I need to proceed in this way you suggested me few comments ago.
Can I ask you what is the purpose of `tpot_obj._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)`, why do we need to reset the random_state? 
Thank you very much

> You could use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to reproduce the **average** cv score in TPOT. A demo below:
> 
> ```python
> tpot_obj = TPOTRegessror(cv=5, scoring='r2')
> tpot_obj.fit(X, y)
> sklearn_pipeline = tpot_obj.fitted_pipeline_
> # reset random state of all operators in the pipeline 
> tpot_obj._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
> cv_scores = cross_val_score(sklearn_pipeline, X, y, cv=5, scoring='r2', verbose=0)
> mean_cv_scores = np.mean(cv_scores)
> ```

",ah thank guess need proceed way ago ask purpose need reset thank much could use reproduce average score python reset random state pipeline,issue,positive,negative,negative,negative,negative,negative
510548946,Hello guys i have the same problem knowing that the sklearn version is (0.21.2) and model selection worked before ,hello problem knowing version model selection worked,issue,negative,neutral,neutral,neutral,neutral,neutral
510482792,"Is there a way also to have the standard deviation of the scores?
Because I guess that this value `tpot_obj._optimized_pipeline_score` is the mean value.
Thank you ",way also standard deviation guess value mean value thank,issue,positive,negative,negative,negative,negative,negative
510479138,"Great!! This is what I was looking for.
Thank you very much :) ",great looking thank much,issue,positive,positive,positive,positive,positive,positive
510474596,"Oh, I just recall that `tpot_obj._optimized_pipeline_score` is the average cv score for `tpot_obj.fitted_pipeline_`. ",oh recall average score,issue,negative,negative,negative,negative,negative,negative
510469030,"@shenglinqian @fcoppey you can prevent the deprecation warning by adding the following line to the config dictionary under the key ``xgboost.XGBRegressor``:

`` 'objective': ['reg:squarederror'] ``

If you don't have a dictionary yet just copy the default one from https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations.

See https://github.com/EpistasisLab/tpot/pull/887/files for how it should look like.",prevent deprecation warning following line dictionary key dictionary yet copy default one see look like,issue,negative,neutral,neutral,neutral,neutral,neutral
510463679,"Yes ok, but this is inefficient, since you have already computed that value during tpot_obj.fit(X, y).
I wanted to avoid further computations and get it easily. But I do not know if this value is stored somewhere during the fitting. If it is not, I think your way is the only solution
",yes inefficient since already value avoid get easily know value somewhere fitting think way solution,issue,positive,positive,positive,positive,positive,positive
510463360,"Which scoring function was used in TPOT? If it is default scoring function, then scoring is 'neg_mean_squared_error' and cv scores should be a negative value.  Please check [this link](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values)",scoring function used default scoring function scoring negative value please check link,issue,negative,negative,negative,negative,negative,negative
510461961,"You could use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to reproduce the **average** cv score in TPOT. A demo below:

```python
tpot_obj = TPOTRegessror(cv=5, scoring='r2')
tpot_obj.fit(X, y)
sklearn_pipeline = tpot_obj.fitted_pipeline_
# reset random state of all operators in the pipeline 
tpot_obj._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
cv_scores = cross_val_score(sklearn_pipeline, X, y, cv=5, scoring='r2', verbose=0)
mean_cv_scores = np.mean(cv_scores)
```",could use reproduce average score python reset random state pipeline,issue,negative,negative,negative,negative,negative,negative
510461052,"![image](https://user-images.githubusercontent.com/33513617/61049959-77023000-a403-11e9-85c4-c4b3b814f7a8.png)
I am getting negative values in cv and I tried changes in subsample but the same thing happened again and again",image getting negative tried subsample thing,issue,negative,negative,negative,negative,negative,negative
510460161,I don't think this is a TPOT-related question. Please resubmit this issue to [IBM watson support](https://cloud.ibm.com/login?redirect=%2Funifiedsupport%2Fsupportcenter),think question please resubmit issue support,issue,positive,neutral,neutral,neutral,neutral,neutral
510459183,"@Vamsi5689 Could you please let me know more details about the issue, like a demo?",could please let know issue like,issue,positive,neutral,neutral,neutral,neutral,neutral
510373630,@weixuanfu I tried with 0.1 to 0.9 subsample values but I can't prevent any other suggestions,tried subsample ca prevent,issue,negative,neutral,neutral,neutral,neutral,neutral
509872765,"So putting all the comments together; is the solution to do something like this?

```py
import numpy as np
import pandas as pd

from sklearn.compose import ColumnTransformer
from tpot import TPOTClassifier
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from tpot.config import classifier_config_dict_light 
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.compose import ColumnTransformer
import copy

X = pd.DataFrame({""DESCRIPTION"": np.random.choice([""hello world"", ""foo bar""], 200), ""DESCRIPTION2"": np.random.choice([""hello world"", ""foo bar""], 200),
                  'NUMERIC': np.random.choice([0, 1], 200), ""NUMERIC2"": np.random.choice([0, 1], 200)})
y = np.random.choice([0, 1], 200)

class IdentityTransformer(TransformerMixin, BaseEstimator):
    def fit(self, X, y=None, **fit_params):
        return self
    def transform(self, X):
        return X

class TfidfTransformer(TransformerMixin):
    def __init__(self, text_columns, keep_columns=[], **kwargs):
        self.text_columns = text_columns if type(text_columns) is list else [text_columns]
        self.keep_columns = keep_columns if type(keep_columns) is list else [keep_columns]
        
        column_list = []
        for idx, text in enumerate(self.text_columns):
            column_list.append(('text' + str(idx), TfidfVectorizer(**kwargs), text))
        
        if len(keep_columns) > 0:
            column_list.append(('other', IdentityTransformer(), self.keep_columns))
        
        self.column_transformer = ColumnTransformer(column_list)
    def fit(self, X, y=None):
        self.column_transformer.fit(X, y)
        return self
    def transform(self, X):
        return self.column_transformer.transform(X)        

# using TPOT config
config = copy.deepcopy(classifier_config_dict_light)
config[""__main__.TfidfTransformer""] = {
        ""text_columns"": [[""DESCRIPTION"", ""DESCRIPTION2""]],
        ""keep_columns"": [[""NUMERIC"", ""NUMERIC2""]]
    }

tpot = TPOTClassifier(config_dict=config, verbosity=2, generations=5, population_size=2, early_stop=2, max_time_mins=2,
                     template='TfidfTransformer-Selector-Transformer-Classifier')
tpot.fit(X, y)
```
",together solution something like import import import import import import import import import import copy description hello world foo bar description hello world foo bar class fit self return self transform self return class self type list else type list else text enumerate text fit self return self transform self return description description,issue,positive,positive,positive,positive,positive,positive
509222219,"The `classes_` attribute is only for `TPOTClassifier` class, but both `TPOTRegessor` and `TPOTClassifier` simply inherit from class [`TPOTbase`](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L102). So we didn't add this attribute to TPOT object but expose `fitted_pipeline_` attribute which is a [scikit-learn Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) and should has the `classes_` object for `TPOTClassifier` but not for `TPOTRegressor`.",attribute class simply inherit class add attribute object expose attribute pipeline object,issue,negative,neutral,neutral,neutral,neutral,neutral
509219371,Or you could try the new [template](https://epistasislab.github.io/tpot/using/#template-option-in-tpot) option in TPOT. (Related issue #870),could try new template option related issue,issue,negative,positive,neutral,neutral,positive,positive
509217450,"Hmm, I didn't saw this error before. Could you please try ['--no-cache-dir'](https://pip.pypa.io/en/stable/reference/pip_install/#caching) option in `pip install`?",saw error could please try option pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
509097696,"I am not sure if this is the best answer; but it worked for me (it will also remove CombineDF). I copied the changes from this branch and use simple_pipelines=True:

[https://github.com/EpistasisLab/tpot/commit/e2a92a8cda0d877323ef2a7cd5d5f4bd7aca6745]",sure best answer worked also remove copied branch use,issue,positive,positive,positive,positive,positive,positive
508990821,"Hi,
what about the classes_ attribute for the tpot object? Is there a reason why it is missing? 
I am likely overlooking something but it would be helpful to have such an attribute. I am working with 13 classes and in the near future they will probably become many more: it would be good to know which element of the prediction array is what class. 
Is there a trick to otherwise know this?

Thanks in advance",hi attribute object reason missing likely something would helpful attribute working class near future probably become many would good know element prediction array class trick otherwise know thanks advance,issue,positive,positive,positive,positive,positive,positive
507337127,"got the same problem with XGBoost
WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror
Anyone know how to figure it out?Thanks",got problem warning reg linear favor reg anyone know figure thanks,issue,negative,positive,positive,positive,positive,positive
505967474,"Sometimes Kfold CV fails to get a generalized model. For example, Kfold may have a bad performance when applying to time series data. You could try other [CV iterators](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-for-i-i-d-data). 

Also, there is another open related issue #804.",sometimes get generalized model example may bad performance time series data could try also another open related issue,issue,negative,negative,negative,negative,negative,negative
505952186,"Shouldn’t a good average CV score give me a decent score after the export? And the drop in score is too high, doesn’t kfold show how well the model generalises?",good average score give decent score export drop score high show well model,issue,negative,positive,positive,positive,positive,positive
505950375,"About ""roc_auc of 0.79"" from ""Tpot training file"", do you mean the score in the log during `tpot.fit(X_train,Y_train)`? If so, TPOT was reporting average of cross-validation scores from [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score).",training file mean score log average,issue,negative,negative,negative,negative,negative,negative
505948462,Sorry the line is missing. And the training auc was 0.67 and test was 0.6 on the exported pipeline script,sorry line missing training test pipeline script,issue,negative,negative,negative,negative,negative,negative
505945621,"I thought that `score = tpot.score(X_test, Y_test)` in ""Tpot training file"" was different with  `roc_auc_score(Y_test,results)` in ""Training script on exported pipeline"".

Which line in the exported codes for getting the ""train auc 0.67""?

If you mean that you got lower test_score in `roc_auc_score(Y_test,results)` but higher training score from `exported_pipeline.score(X_train,Y_train)`, it should indicate that there is an overfiting issue in the exported model.",thought score training file different training script pipeline line getting train mean got lower higher training score indicate issue model,issue,negative,negative,neutral,neutral,negative,negative
505943796,"But I’ve set the random state to 42 on my exported pipeline. Shouldn’t it work either way? 
The issue is only in the exported python file, the score in tpot training file works fine",set random state pipeline work either way issue python file score training file work fine,issue,negative,negative,neutral,neutral,negative,negative
505857351,"The issue is that the random_state in tpot.fitted_pipeline_ was not set to 42 by default. Similar to the issue #513. Add the codes below to **Tpot training file** should reproduce the results.
```python
tpot.fit(X_train,Y_train)
tpot.export('tpot_ckd_pipeline.py')

tpot._set_param_recursive(tpot.fitted_pipeline_.steps, 'random_state', 42)
# refit with random_state=42
tpot.fitted_pipeline_.fit(X_train, Y_train)

score = tpot.score(X_test, Y_test)
```

",issue set default similar issue add training file reproduce python refit score,issue,negative,neutral,neutral,neutral,neutral,neutral
505710421,"I am getting a similar problem,  I have train and test data in seperate files, I pass the training data in the tpot fit function. The results of tpot show an roc_auc of 0.79, on running the exported code i get only 0.6.

### Tpot training file
```python
SEED = 42
np.random.seed(SEED)

train_df = load_data('train.csv')
test_df = load_data('test.csv')

# some preprocessing code

X_train = train_df.drop('target',axis = 1)
Y_train = train_df['target']

X_test = test_df.drop('target',axis = 1)
Y_test = test_df['target']

tpot = TPOTClassifier(verbosity=2, 
                      scoring=""roc_auc"", 
                      random_state=SEED, 
                      periodic_checkpoint_folder=""tpot_ckd_ckpt"", 
                      memory='auto',
                      early_stop=3,
                      n_jobs=-2,
                      generations=70, 
                      population_size=100)

tpot.fit(X_train,Y_train)
tpot.export('tpot_ckd_pipeline.py')

score = tpot.score(X_test, Y_test)
print('Score:', score)
```

### Training script on exported pipeline

```python
SEED = 42
np.random.seed(SEED)

train_df = load_data('train.csv')
test_df = load_data('test.csv')

# same preprocessing code as above

X_train = train_df.drop('target',axis = 1)
Y_train = train_df['target']

X_test = test_df.drop('target',axis = 1)
Y_test = test_df['target']

exported_pipeline = RandomForestClassifier(bootstrap=True, criterion=""gini"", max_features=0.05, min_samples_leaf=11, min_samples_split=4, n_estimators=100, random_state=SEED)

exported_pipeline.fit(X_train, Y_train)

results = exported_pipeline.predict(X_test)

from sklearn.metrics import roc_auc_score
roc_auc_score(Y_test,results)
```
gives train auc 0.67, test auc 0.6

Is there something missing? Why do I observe this behavior? I've run tpot multiple times and this behavior is consistent.",getting similar problem train test data pas training data fit function show running code get training file python seed seed code axis axis score print score training script pipeline python seed seed code axis axis import train test something missing observe behavior run multiple time behavior consistent,issue,negative,positive,neutral,neutral,positive,positive
504435519,"Fantastic! Thank you so much. That fixes my above comment as well. If I understand correctly, because the pareto_front for tpot will be the halloffame.",fantastic thank much comment well understand correctly,issue,positive,positive,positive,positive,positive,positive
504428455,"It looks like the above demo won't clear out the hallofame which looks to be a deap object; so when I do the refit I can get stuck on the pipeline that had the best score on any of the past data sets used. For example, if you take the above demo and swap the train and test set around (because you can get a better CV score on the test than train, I also simplified the param set so it was a bit simpler to see whats going on, and made the generations excessive).

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
                        digits.data,
                        digits.target,
                        train_size=0.5,
                        test_size=0.5,
                        random_state=42
                        )

params = {
'sklearn.tree.DecisionTreeClassifier': {
            'criterion': [""gini""],
            'max_depth': range(1, 10),
            'min_samples_split': range(2, 10),
            'min_samples_leaf': range(1, 10)
        }
}

tpot = TPOTClassifier(
                generations=3,
                population_size=1000,
                verbosity=2,
                random_state=42,
                warm_start=True,
                config_dict=params
                )

tpot.fit(X_test, y_test)

for ind in tpot._pop:
    del ind.fitness.values

tpot.fit(X_train, y_train)
```

",like wo clear object refit get stuck pipeline best score past data used example take swap train test set around get better score test train also simplified param set bit simpler see whats going made excessive python import import import range range range,issue,positive,positive,positive,positive,positive,positive
504421537,"`_pre_test decorator: _random_mutation_operator: num_test=0 a must not be a non-empty` this warning message can be ignored since `_pre_test decorator` just test the invalid pipelines and replace them with valid ones.

I think resetting pareto front for different dataset is necessary beacuse  the score across datasets should be different. I added some codes into the demo for resetting pareto front:

```python
# remove fitness values in previous runs
for ind in tpot._pop:
    del ind.fitness.values

# reset pareto front
tpot._last_optimized_pareto_front = None
tpot._last_optimized_pareto_front_n_gens = 0
tpot._pareto_front = None

# refit tpot to another dataset
tpot.fit(X_test, y_test)
```

",decorator must warning message since decorator test invalid replace valid think front different necessary score across different added front python remove fitness previous reset front none none refit another,issue,negative,negative,neutral,neutral,negative,negative
504405791,"Thank you so much for the reply; a refit option would be really powerful. But this is great and looks to be just what I need!

Once the tpot._pop is cleared; on the refit run I get a lot of the following warnings (setting verbosity=3, using my own data, as well as the digits data in your example) . Can these be ignored?
_pre_test decorator: _random_mutation_operator: num_test=0 a must not be a non-empty

When using verbosity=3 it also looks like the pareto front scores are kept between fits. I'm actually not sure, this could be a desirable feature. But it might be surprising because the score across data sets may not be equivalent. Similarly, resetting the pareto front across calls to fit() isn't completely accurate either. I am not sure how this would be best handled if a refit() was to be implemented",thank much reply refit option would really powerful great need refit run get lot following setting data well data example decorator must also like front kept actually sure could desirable feature might surprising score across data may equivalent similarly front across fit completely accurate either sure would best handled refit,issue,positive,positive,positive,positive,positive,positive
504028926,"Thank you for the idea.

TPOT may need add this `refit` option for warm_start to discard the values from previous run.

There is a hacky way to refit tpot on another dataset. Please check the demo below:
```python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(
                                                    digits.data, 
                                                    digits.target,
                                                    train_size=0.5, 
                                                    test_size=0.5, 
                                                    random_state=42
                                                    )


tpot = TPOTClassifier(
                        generations=3, 
                        population_size=10, 
                        verbosity=2, 
                        random_state=42, 
                        warm_start=True
                        )

tpot.fit(X_train, y_train)

# remove fitness values in previous runs
for ind in tpot._pop:
    del ind.fitness.values

# refit tpot to another dataset
tpot.fit(X_test, y_test)

```
",thank idea may need add refit option discard previous run hacky way refit another please check python import import import remove fitness previous refit another,issue,positive,negative,negative,negative,negative,negative
503658786,"`RandomTree` is default behavior in previous version of TPOT (without template option). In this setting, randomly generate pipelines into both tree-like architectures and linear structures with random operators in config_dict. We will change it to 'None' for avoid this confusion.",default behavior previous version without template option setting randomly generate linear random change avoid confusion,issue,negative,negative,negative,negative,negative,negative
503655929,"Hi @trang1618 and @weixuanfu, thank you so much for your responses - they are very helpful. 

I guess, I am bit confused then on what the default template, 'RandomTree', leads to. Does that bias the system towards finding tree-like architectures for the final system? Or is the classifier component selected randomly from the config_dict? 

Thanks! ",hi thank much helpful guess bit confused default template bias system towards finding final system classifier component selected randomly thanks,issue,positive,neutral,neutral,neutral,neutral,neutral
503645019,"The template in TPOT can also begin with a SVM, like `LinearSVC-Selector-Transformer-Classifier`. In this way, the pipelines generated in TPOT should stack predictions from `LinearSVC` to X in 1st step and then pass to `Selector` in 2nd step.",template also begin like way stack st step pas selector step,issue,negative,neutral,neutral,neutral,neutral,neutral
503643659,"Hi @collinskatie, I'm not sure if you would like to allow other feature preprocessing steps in your analysis (e.g. PCA), but if you really want to only tune the hyperparameters in your SVM (assuming classification problem) then one way is to specifying `template = 'LinearSVC'`. 

Another way is to customize your `config_dict` such as
```
config_dict = {
'sklearn.svm.LinearSVC': {
        'penalty': [""l1"", ""l2""],
        'loss': [""hinge"", ""squared_hinge""],
        'dual': [True, False],
        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
    }
}
```
Read more at [`config`](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py).

If you want to limit the classifying model but allow other steps then you can write, say, `template = Selector-Transformer-LinearSVC`.

Hope that helps!
",hi sure would like allow feature analysis really want tune assuming classification problem one way template another way hinge true false read want limit model allow write say template hope,issue,positive,positive,positive,positive,positive,positive
503640257,"Hi, 

I am also confused on how to specify an alternative template. Is there a way to have TPOT begin with a template like an SVM? If so, how would one do this? I saw in the documentation (and the above comment) that alternative forms need to be of the form Selector-Transformer-Classifier: would one need to specify an initial template for each step? 

Thanks for any assistance! ",hi also confused specify alternative template way begin template like would one saw documentation comment alternative need form would one need specify initial template step thanks assistance,issue,negative,negative,neutral,neutral,negative,negative
502705548,"No, we don't have plan to support `partial_fit` so far. But any contributions are welcome. ",plan support far welcome,issue,positive,positive,positive,positive,positive,positive
502697164,Thanks for clearing this up. I suppose there aren't any plans to support it in the (near) future?,thanks clearing suppose support near future,issue,positive,positive,positive,positive,positive,positive
502695794,TPOT does not support `partial_fit` and `warm_start` in TPOT is not for `partial_fit` but for reusing the population (evaluated pipelines) from previous calls to fit() instead of a random generated population. ,support population previous fit instead random population,issue,positive,negative,neutral,neutral,negative,negative
502332053,"Thank you, maybe I should start with examples in official doc, make a few changes every time and see what will happen. ",thank maybe start official doc make every time see happen,issue,negative,neutral,neutral,neutral,neutral,neutral
502322846,"> Does the solution ([#876 (comment)](https://github.com/EpistasisLab/tpot/issues/876#issuecomment-499626973)) from another issue work for you?

That had no effect for me. It's never made it beyond 0% on the progress bar. ",solution comment another issue work effect never made beyond progress bar,issue,negative,neutral,neutral,neutral,neutral,neutral
502043242,"I cant really nail it down to a point, I tried several different things, sometimes it was working, sometimes not. I changed the parallel backend directly in the parallel.py of joblib which sometimes helped. Additionally I changed my random seed to some other value and with the same setting it was working. So the problem might be related to a specific algorithm (maybe just with some specific parameter setting) that makes TPOT freeze. However, I was not able to identify which one it might be.",cant really nail point tried several different sometimes working sometimes parallel directly sometimes additionally random seed value setting working problem might related specific algorithm maybe specific parameter setting freeze however able identify one might,issue,negative,positive,neutral,neutral,positive,positive
501984274,@Chowkah Could you please talk about how can you reach 5%. I still stuck at 0%,could please talk reach still stuck,issue,negative,neutral,neutral,neutral,neutral,neutral
501702883,Thank you for catching this typo. I am merging it to dev branch for now. ,thank catching typo dev branch,issue,negative,positive,positive,positive,positive,positive
501177619,"I could make it work with running the script like that :
 python -W ignore script.py",could make work running script like python ignore,issue,negative,neutral,neutral,neutral,neutral,neutral
500851818,"Does the solution (https://github.com/EpistasisLab/tpot/issues/876#issuecomment-499626973) from another issue work for you?
",solution another issue work,issue,negative,neutral,neutral,neutral,neutral,neutral
500850886,"Hmm, I am not sure why this happened. Maybe related to [this issue](https://github.com/dask/distributed/issues/1078). Can you provide more environment information for reproducing this issue?",sure maybe related issue provide environment information issue,issue,negative,positive,positive,positive,positive,positive
500440000,"So I've been able to test the dev branch a little more. I was able to get TPOT to return exported pipelines with the standard tpot config and these parameters. I think n_jobs =4 was an important change from n_jobs=-2 but it's tough to tell with the silent failures and no error message.

```python
model_pipe = tpot.TPOTRegressor(generations=1, 
                                population_size=2,                        
                                offspring_size=None, 
                                mutation_rate=0.9,                       
                                crossover_rate=0.1,
                                scoring='neg_mean_squared_error', 
                                cv=trn_val_split_idxs,                                         
                                subsample=1.0,
                                n_jobs= 4,                                                     
                                max_time_mins=None,                                            
                                max_eval_time_mins=5,                                        
                                random_state=42,                                            
                                config_dict=None,
                                template=""RandomTree"",
                                warm_start=False,
                                memory='auto',                                                 
                                use_dask=True,                                                
                                periodic_checkpoint_folder='/home/will/Dalton/USA/tpot_tmp',   
                                early_stop=10,                                                 
                                verbosity=4,                                                  
                                disable_update_check=False)
```
but when I tried to run the tpot light config I got the same silent failure where tpot thought it was running but no jobs were running on the CPU. 

",able test dev branch little able get return standard think important change tough tell silent error message python tried run light got silent failure thought running running,issue,negative,positive,neutral,neutral,positive,positive
500384822,"I've refactored TPOT support in [SkLearn2PMML](https://github.com/jpmml/sklearn2pmml) package version 0.46.0 (available in PyPI), and explained some technical details/gotchas in the following technical article:
https://openscoring.io/blog/2019/06/10/converting_sklearn_tpot_pipeline_pmml/

TLDR: TPOT fitted pipelines convert very nicely into PMML data format.",support package version available technical following technical article fitted convert nicely data format,issue,positive,positive,positive,positive,positive,positive
499427039,"Then this should suffice, to be added to the default regressor dict (analogous for classifier)

```
cat_features = [...]  # e.g. features.select_dtypes(include=[""category""])
if 'catboost' in sys.modules.keys():
    from sklearn.base import RegressorMixin
    from catboost import CatBoostRegressor
    CatBoostRegressor.__bases__ += (RegressorMixin,)
    regressor_config_dict['catboost.CatBoostRegressor'] = {
        'logging_level': ['Silent'],
        'cat_features': [cat_features],
    }
```",suffice added default regressor analogous classifier category import import,issue,negative,neutral,neutral,neutral,neutral,neutral
499420788,It is now possible to pass cat_features together with other training parameters. So there should be no problem with them.,possible pas together training problem,issue,negative,neutral,neutral,neutral,neutral,neutral
499371396,"> Hi guys, I've tried the proposed way of only running tpot over catboost shown in #407 ,and modified it accordingly for a classifier problem. However, tpot still goes through other models instead of only catboost. Not sure if anyone also had the same issue.

Can you post your classifier dict and code sample?  For me the method did work, but the issue right now is that there is no feasible way of passing the cat_columns to catboost.",hi tried way running shown accordingly classifier problem however still go instead sure anyone also issue post classifier code sample method work issue right feasible way passing,issue,negative,positive,positive,positive,positive,positive
499367784,"Hi guys, I've tried the proposed way of only running tpot over catboost shown in #407 ,and modified it accordingly for a classifier problem. However, tpot still goes through other models instead of only catboost. Not sure if anyone also had the same issue. ",hi tried way running shown accordingly classifier problem however still go instead sure anyone also issue,issue,negative,positive,positive,positive,positive,positive
498667843,"Your configuration above looks fine to me. Also you may try to use config_dict=""TPOT light"" or/and  n_jobs=4 for a quick test on your dataset/Boston datasets to checking whether this was a resource problem?

```python
model_pipe = tpot.TPOTRegressor(generations=100, 
                   population_size=100,                        
                   offspring_size=None, 
                   mutation_rate=0.9,                       
                   crossover_rate=0.1,
                   scoring='neg_mean_squared_error',
                   cv=list(splitter.split(X_trn)),   
                   subsample=1.0,
                   n_jobs=4,                        
                   max_time_mins=24*60,              
                   max_eval_time_mins=20,           
                   random_state=42,                  
                   config_dict=""TPOT light"",
                   template=""RandomTree"",
                   warm_start=False,
                   memory='auto',                    
                   use_dask=True,                    
                   periodic_checkpoint_folder='/home/will/Dalton/USA/tpot_tmp'
                   early_stop=10,                    
                   verbosity=3,                      
                   disable_update_check=False)
```",configuration fine also may try use light quick test whether resource problem python light,issue,negative,positive,positive,positive,positive,positive
498436035,"ok, i re-ran in a new environment with the dev branch of tpot and the behavior is unchanged. Is there a sample dataset that is known to work for tpot regression with time series cv splits?

I guess I can always add a dummy column of dates to the Boston dataset just to get things running. Is there a base parameter configuration you would recommend for this?

",new environment dev branch behavior unchanged sample known work regression time series guess always add dummy column boston get running base parameter configuration would recommend,issue,negative,negative,negative,negative,negative,negative
498394211,thank you! awesome work that you do guys! I wish I could help but I'm unfortunately not so useful with my knowledge yet hehe. maybe later...,thank awesome work wish could help unfortunately useful knowledge yet maybe later,issue,positive,positive,positive,positive,positive,positive
498392761,"> Hello,
> 
> when I predict my test_set I get endless output of that :
> /usr/local/lib/python3.7/site-packages/sklearn/preprocessing/_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22
> 
> any idea what to modify to make this go away?
> 
> thank you


It seems FutureWarning message from the latest version of scikit-learn (0.21), we will update TPOT for fixing incompatibility issue in the next version of TPOT and should drop the supports of python 2.7 as scikit-learn 0.21 does. 


> same problem with XGBoost :
> WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.
> 
> something to change here too?

This seems from the API changes from [latest version of xgboost](https://github.com/dmlc/xgboost/releases). I am not sure how this warning message is triggered since TPOT does not set `reg:linear`. Maybe it is a bug in this version of xgboost. We will try to find a workaround for it.",hello predict get endless output default idea modify make go away thank message latest version update fixing incompatibility issue next version drop python problem warning reg linear favor reg something change latest version sure warning message triggered since set reg linear maybe bug version try find,issue,negative,positive,positive,positive,positive,positive
498388864,"same problem with XGBoost :
WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.

something to change here too?",problem warning reg linear favor reg something change,issue,negative,neutral,neutral,neutral,neutral,neutral
498299322,"Thanks for the quick reply! 
joblib 0.13.2 is the current version and I cant update joblib (> 0.13.2) or did I get something wrong here?

I installed the development branch and tried it again with joblib == 0.13.2 and scikit-learn >=0.21, but unfortunately it is still freezing.",thanks quick reply current version cant update get something wrong development branch tried unfortunately still freezing,issue,negative,negative,neutral,neutral,negative,negative
498298555,"> The TPOT (v0.10.1) only uses the joblib built in scikit-learn instead of the (joblib 0.13). The development branch of TPOT should use the joblib module due to a recently merge #867.

oh ok perfect, i'll let you know once i've tested on the dev branch. And thank you for all your help troubleshooting by the way. ",built instead development branch use module due recently merge oh perfect let know tested dev branch thank help way,issue,positive,positive,positive,positive,positive,positive
498290817,The TPOT (v0.10.1) only uses the joblib built in scikit-learn instead of the (joblib 0.13). The development branch of TPOT should use the joblib module due to a recently merge #867.,built instead development branch use module due recently merge,issue,negative,negative,neutral,neutral,negative,negative
498285848,"Unfortunately, I'm already running:
joblib = 0.13.2
scikit-learn = 0.21.1

TPOT = 0.10.1
just in case dask is involved:
dask = 1.2.2
dask-core = 1.2.2
dask-glm = 0.1.0 
dask-ml = 0.13.0

I'll create a new conda env with the dev branch and revert back to you. 

",unfortunately already running case involved create new dev branch revert back,issue,negative,negative,negative,negative,negative,negative
498269163,"Relate to issue #876 

It seems that there is a kind of threading deadlock issue (maybe related to [this old issue in joblib](https://github.com/joblib/joblib/issues/138)). Could you please try to update joblib (> 0.13.2) and scikit-learn (>=0.21) via conda or pip and reinstall TPOT development branch via the command below?
```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```
We recently noticed that the internal joblib module (based on a older version of joblib) in scikit-learn (<0.20) was deprecated (see #867) and may cause the issue here  because it did not have some important updates about limiting the number of threads in joblib (>0.12, see [joblib change log](https://github.com/joblib/joblib/blob/master/CHANGES.rst)). LMK if this solution works or now.",relate issue kind deadlock issue maybe related old issue could please try update via pip reinstall development branch via command shell pip install upgrade recently internal module based older version see may cause issue important limiting number see change log solution work,issue,positive,positive,positive,positive,positive,positive
498269021,"It seems that there is a kind of threading deadlock issue (maybe related to [this old issue in joblib](https://github.com/joblib/joblib/issues/138)). Could you please try to update joblib (> 0.13.2) and scikit-learn (>=0.21) via conda or pip and reinstall TPOT development branch via the command below?
```shell
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```
We recently noticed that the internal joblib module (based on a older version of joblib) in scikit-learn (<0.20) was deprecated (see #867) and may cause the issue here  because it did not have some important updates about limiting the number of threads in joblib (>0.12, see [joblib change log](https://github.com/joblib/joblib/blob/master/CHANGES.rst)). LMK if this solution works or now.",kind deadlock issue maybe related old issue could please try update via pip reinstall development branch via command shell pip install upgrade recently internal module based older version see may cause issue important limiting number see change log solution work,issue,positive,positive,positive,positive,positive,positive
498000316,"changing a few more parameters and setting n_jobs to 4 from -2 i finally got an error message:

```python
model_pipe = tpot.TPOTRegressor(generations=1, 
                   population_size=2,                        
                   offspring_size=None, 
                   mutation_rate=0.9,                       
                   crossover_rate=0.1,
                   scoring='neg_mean_squared_error',
                   cv=list(splitter.split(X_trn)),   
                   subsample=1.0,
                   n_jobs=4,                       
                   max_time_mins=24*60,              
                   max_eval_time_mins=20,           
                   random_state=42,                 
                   config_dict=None,
                   template=""RandomTree"",
                   warm_start=False,
                   memory='auto',                   
                   use_dask=False,                    
                   periodic_checkpoint_folder='/home/will/Dalton/USA/tpot_tmp', 
                   early_stop=10,                    
                   verbosity=4,                      
                   disable_update_check=False)

model_pipe.fit(X_trn.drop(columns='Date'), y_trn.drop(columns='Date').values)
print(model_pipe.score(X_tst.drop(columns='Date'), y_tst.drop(columns='Date').values))
```
![image](https://user-images.githubusercontent.com/1115485/58757143-806eb300-84bb-11e9-9e2c-d93fd575a3a3.png)
![image](https://user-images.githubusercontent.com/1115485/58757147-967c7380-84bb-11e9-9995-d762f98140b5.png)

",setting finally got error message python print image image,issue,negative,neutral,neutral,neutral,neutral,neutral
497997646,"Just to update, i tried running with only 12 monthly cross val splits with 1 generation and population of 2 and it had the same behavior. Never shows a completed pipeline and cpu usage goes from heavy to nothing. I tried running with dask on an off with the same behavior. ",update tried running monthly cross generation population behavior never pipeline usage go heavy nothing tried running behavior,issue,negative,negative,neutral,neutral,negative,negative
497992074,"So I don't think it's the size that's the problem, I think it's only 2gb. I think the issue may be the time series cross-validation that I'm using. I'm splitting the DataFrame by date with 1964 to 2012 in the training set and each month from 2012 to 2018 so roughly 72 monthly cross-validation splits which I assume means each model needs to be retrained 72 times which is sort of dumb but also necessary for my problem. 

Here is my code for that:

```python3
import pandas as pd
from pandas.tseries.offsets import MonthEnd
import numpy as np

class TimeSeriesSplitMonthLag():
    def __init__(self, date_col='Date',init_trn_date=None, lag=12):
        """"""
        date_col: string name of column in dataframe containing dates
        init_trn_date: string in format mm/dd/yyy for the first train split
        lag = number of months to lag forward the test set to handle forward lagged targets
        
        Example:
        >>> splitter = TimeSeriesSplitDateLag(date_col= 'Date',init_trn_date= ""05/31/2012"",lag= 12)
        >>> for trn_idx, tst_idx in splitter.split(df):
        ...    print(f""trn index max: {trn_idx.max()}, month {df.loc[trn_idx,'Date'].max()}"")
        ...    print(f""tst index min: {tst_idx.min()}, month {df.loc[tst_idx,'Date'].min()}"")
        trn index max: 1344164, month 2018-02-28 
        tst index min: 1339,    month 2019-02-28 
        trn index max: 1344262, month 2018-03-31 
        tst index min: 1340,    month 2019-03-31 
        trn index max: 1344483, month 2018-04-30 
        tst index min: 1341,    month 2019-04-30 
        
        """"""
        self.init_trn_date = pd.to_datetime(init_trn_date)
        self.lag = lag
        self.Date = date_col

    def split(self, df):
        """"""
        df: dataframe in single index format
        """"""
        max_tst_date = pd.to_datetime(df[self.Date].unique()).max()
        max_trn_date = max_tst_date - MonthEnd(self.lag)
        trn_date_range = pd.date_range(start= self.init_trn_date, end= max_trn_date,freq= 'M')
        
        for date in trn_date_range:
            trn_idxs = df.loc[df[self.Date] <= date,:].index.values
            tst_idxs = df.loc[df[self.Date] == date + MonthEnd(self.lag),:].index.values
            yield (trn_idxs, tst_idxs)

splitter = TimeSeriesSplitMonthLag(date_col='Date', init_trn_date='05/31/2012', lag=12)

#passed as a paramter to tpot:
cv = list(splitter.split(X_trn))
```

I'm going to try running a small sample of the dataset with fewer cv monthly splits just to test. 
",think size problem think think issue may time series splitting date training set month roughly monthly assume model need time sort dumb also necessary problem code python import import import class self string name column string format first train split lag number lag forward test set handle forward lagged example splitter print index month print tst index min month index month tst index min month index month tst index min month index month tst index min month lag split self single index format date date date yield splitter list going try running small sample monthly test,issue,negative,negative,neutral,neutral,negative,negative
497964098,How large is your dataset? I doubt the dask backend maybe crashed somehow after using all the resources. I will double-check it next week.,large doubt maybe somehow next week,issue,negative,positive,positive,positive,positive,positive
497582277,"It is a small dataset, around 1k rows. However, the issue is solved now, figured out that it wasn't working because descriptive text in data has to be vectorized and not numericized. Numericizing  a descriptive text column would result in creating large numeric values for each row which tpot I guess cannot accept.",small around however issue figured working descriptive text data descriptive text column would result large row guess accept,issue,negative,negative,neutral,neutral,negative,negative
497507881,I went into the same issue today and fixed it by making sure both the client and workers have the same version of xgboost installed. ,went issue today fixed making sure client version,issue,negative,positive,positive,positive,positive,positive
497389261,"The error is in the dask worker.
`TypeError: No dispatch for <class 'xgboost.sklearn.XGBRegressor'>`
For some reason the `dask_deserialize` function isn't finding the XBGRegressor class in the environment. 
Here's the line where the issue is arising:
```
File ""/opt/conda/lib/python3.6/site-packages/distributed/protocol/serialize.py"", line 48, in dask_loads
    loads = dask_deserialize.dispatch(typ)
```",error worker dispatch class reason function finding class environment line issue file line,issue,negative,neutral,neutral,neutral,neutral,neutral
497387734,"So maybe my issue stems from Docker? Haven't dug too much into it, but I could see maybe some weird things arising from Dask starting up workers within the container?",maybe issue docker dug much could see maybe weird starting within container,issue,negative,negative,neutral,neutral,negative,negative
497386189,@masonkirchner ... I doubt it's a Python 2 issue... my logs show I was using python 3.7 and still got the issue,doubt python issue show python still got issue,issue,negative,neutral,neutral,neutral,neutral,neutral
497351924,You can check `evaluated_individuals_` attribute to find out if TPOT evaluated pipelines ( there is a similar question in comments of issue #870),check attribute find similar question issue,issue,negative,neutral,neutral,neutral,neutral,neutral
497344534,"I  asked this question because it seems that it can not see any feature selections operator in the last corresponding Python code TPOT exported. is it means that I must define the feature algorithm operators in the tpot_config setting, otherwise we won know if TPOT use any feature selections algorithm？",question see feature operator last corresponding python code must define feature algorithm setting otherwise know use feature,issue,negative,neutral,neutral,neutral,neutral,neutral
497318078,I closed this issue for now. Please feel free to reopen it if there is a update on this issue.,closed issue please feel free reopen update issue,issue,positive,positive,positive,positive,positive,positive
497317641,"In default setting, TPOT will randomly generate pipeline with or without feature selection operators in initial generation and then the best pipelines can include/exclude feature selection operators randomly via crossover/mutation in genetic programming. ",default setting randomly generate pipeline without feature selection initial generation best feature selection randomly via genetic,issue,positive,neutral,neutral,neutral,neutral,neutral
497177416,"@julioasotodv Also having this same issue, but using your suggestion did not help. 

I'm running in Docker using Python 2 -- If I had to bet, this would be the cause. ",also issue suggestion help running docker python bet would cause,issue,negative,neutral,neutral,neutral,neutral,neutral
496584109,"Thank you. Very helpful and very quick feedback, your suggestion will definitely help; but I am thinking about forking the project and working on a utility to do what I want in my regression problem, leveraging regressor_config_dict in regressor.py.
Cheers",thank helpful quick feedback suggestion definitely help thinking project working utility want regression problem,issue,positive,positive,positive,positive,positive,positive
496580253,"Please check the demo in issue #516 for converting the str to pipeline. 

For your 2nd question, TPOT did not save those fitted pipeline because it may make tpot object too big. So far TPOT only save the cv scores and other statistics of those evaluated pipeline. ",please check issue converting pipeline question save fitted pipeline may make object big far save statistic pipeline,issue,positive,positive,neutral,neutral,positive,positive
496285314,"> The former code ""from sklearn.crossvalidation import traintest_split"" has deprecated. The new one is below. I had the same problem, now it's gone.
> 
> ""from sklearn.modelselection import traintest_split""

the best answer",former code import new one problem gone import best answer,issue,negative,positive,positive,positive,positive,positive
495807163,@weixuan Should we change the template default to `None` for no template instead of `RandomTree` to avoid this confusion? I have experienced this confusion before as well! ,change template default none template instead avoid confusion experienced confusion well,issue,negative,positive,positive,positive,positive,positive
495671874,@ballcap231 please check the example in issue #808,please check example issue,issue,negative,neutral,neutral,neutral,neutral,neutral
495662522,"> You could try to customize operators within TPOT with LightBGM’s scikit-learn API. Please check [the link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for more details.

Could you give an example?
I'm confused how one would use lightgbm if it's not provided as an option in TPOT's config dict for operators:
https://github.com/EpistasisLab/tpot/blob/master/tpot/config/regressor.py",could try within please check link could give example confused one would use provided option,issue,negative,negative,negative,negative,negative,negative
495604080,"`RandomTree` should explores all the operators in [TPOT configuration](http://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) so some pipelines could include selector, transformer and classifier for TPOTClassifier (regressor for TPOTRegressor).",configuration could include selector transformer classifier regressor,issue,negative,neutral,neutral,neutral,neutral,neutral
495386938,"Ok, I didn't know about that `evaluated_individuals_`, it looks like I was wrong, some feature selection was performed: when I searched for ""select"" in the output I only found `SelectPercentile` and `SelectFwe` (not SelectFromModel) - though I'm not sure whether it's because of the smaller generations/population size I used to run this quicker.

So, Selector/Transform/Classifier/Regressor are things that you can use in a template either completely separately or completely together, but `RandomTree` is a special case which explores options with and without a Selector (and no Transform)?",know like wrong feature selection select output found though sure whether smaller size used run use template either completely separately completely together special case without selector transform,issue,positive,positive,neutral,neutral,positive,positive
495207153,"In that form, there is little practical purpose for the double-copy. We
allow TPOT to do this as a “building block” toward creating an analysis
that works on two separate copies of the data. e.g., it is feasible that
applying PCA to one copy of the data, applying PolynomialFeatures to
another copy of the data, then merging those features for further
downstream analysis is useful. (Made up example to illustrate the point.)

It’s possible that TPOT didn’t run long enough to take advantage of the
beginning of a split analysis. If the pipelines you’re looking at otherwise
perform well, you can probably remove this step without much effect.

On Wed, May 22, 2019 at 11:59 PM Jan-Hendrik Menke <notifications@github.com>
wrote:

> Context of the issue


I've seen this step multiple times in different pipelines:
>
>     make_union(
>         FunctionTransformer(copy),
>         FunctionTransformer(copy)
>     )
>
> From my understanding, this is like numpy's tile(2), right? Can someone
> give details why this is a viable step at all? Or is it just a ""bug"" in the
> pipeline creation, which can be safely omitted in production use?
>
> Thanks! tpot is awesome!
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/871?email_source=notifications&email_token=AANDXNZIKVZT2ZKFURTTRTDPWY6CNA5CNFSM4HO2PPR2YY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4GVMAW4Q>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AANDXN7NUKS4S36TIC6D3IDPWY6CNANCNFSM4HO2PPRQ>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",form little practical purpose allow building block toward analysis work two separate data feasible one copy data another copy data downstream analysis useful made example illustrate point possible run long enough take advantage beginning split analysis looking otherwise perform well probably remove step without much effect wed may wrote context issue seen step multiple time different copy copy understanding like tile right someone give viable step bug pipeline creation safely production use thanks awesome thread reply directly view mute thread twitter,issue,positive,positive,positive,positive,positive,positive
495206684,"Thanks. That is not a bug in pipeline creation. The FeatureUnion with identical FunctionTransformers in that pipeline means combining input matrix. Since current TPOT randomly generates tree-based pipeline so some pipelines need this FeatureUnion operator to combine transformed features or raw input features from two branches for tree. But sometimes, like the case in this issue, the FeatureUnion just doubles feature spaces of raw input features.
Please check the discussion in the issue #581 and related issue #807.",thanks bug pipeline creation identical pipeline combining input matrix since current randomly pipeline need operator combine raw input two tree sometimes like case issue feature raw input please check discussion issue related issue,issue,positive,negative,negative,negative,negative,negative
494842964,"> From the examples I see online, it looks like before this `template` option existed, some feature selection options were explored, as well as no feature selection option. However, when I tried by myself, it looks like the default template doesn't explore a feature selection step in the default template. Or did I miss something?

Hmm, default template should explore feature selection operator randomly. Did you check `evaluated_individuals_` attribute to find out if TPOT evaluated pipelines with feature selection?",see like template option feature selection well feature selection option however tried like default template explore feature selection step default template miss something default template explore feature selection operator randomly check attribute find feature selection,issue,positive,negative,negative,negative,negative,negative
494839191,"1. `RandomTree` means random tree-based pipeline, which is how previous versions of TPOT did without this template option.
2. For now, the pipeline structure defined by template is fixed. So it would not skip selector or transformer.
3. Yes, XGboost will be explored if it was installed in your environment.",random pipeline previous without template option pipeline structure defined template fixed would skip selector transformer yes environment,issue,negative,negative,negative,negative,negative,negative
494838317,"From the examples I see online, it looks like before this `template` option existed, some feature selection options were explored, as well as no feature selection option. However, when I tried by myself, it looks like the default template doesn't explore a feature selection step in the default template. Or did I miss something?",see like template option feature selection well feature selection option however tried like default template explore feature selection step default template miss something,issue,positive,neutral,neutral,neutral,neutral,neutral
494820575,"The issue is fixed in PR #867 and it was merged to development branch. It will be fixed into next version of TPOT.
BTW, because scikit-learn 0.21 only supports python 3, we may drop python 2 supports in near future due to too many api changes in the future version of scikit-learn.",issue fixed development branch fixed next version python may drop python near future due many future version,issue,negative,positive,neutral,neutral,positive,positive
494743727,"after considerable debugging, my scoring function is to blame. ",considerable scoring function blame,issue,negative,positive,neutral,neutral,positive,positive
494729284,"There is a library that makes an sklearn-compliant estimator out of PyTorch. Although not directly related (not keras), i think this is very relevant: https://github.com/skorch-dev/skorch",library estimator although directly related think relevant,issue,negative,positive,positive,positive,positive,positive
494144044,I closed this PR due to no response for a while. Please feel free to reopen it if you think this PR is needed.,closed due response please feel free reopen think,issue,positive,positive,neutral,neutral,positive,positive
493475858,"Well after some more experimentation I believe it’s not necessary. The ‘forkserver’  method really did the trick thank you!

On 16 May 2019, at 20:49, Florentin Coppey <florentin.coppey@unil.ch<mailto:florentin.coppey@unil.ch>> wrote:

Might be a good one to implement no? Should be quite straight forward.

Because it trains many different classifier in a for loop I can’t set a lower Early-stop because I would risk loosing accuracy on other dataset.

Thanks anyway.

Le 16 mai 2019 à 20:47, Weixuan Fu <notifications@github.com<mailto:notifications@github.com>> a écrit :


Sorry we don't have that stop option so far. Decreasing early_stop may help.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications&email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKFMG6O2JW34EWDCUJRZLA3PVWT35ANCNFSM4HNE5OBA>.

[ { ""@context"": ""http://schema.org<http://schema.org/>"", ""@type"": ""EmailMessage"", ""potentialAction"": { ""@type"": ""ViewAction"", ""target"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460"", ""url"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460"", ""name"": ""View Issue"" }, ""description"": ""View this Issue on GitHub"", ""publisher"": { ""@type"": ""Organization"", ""name"": ""GitHub"", ""url"": ""https://github.com<https://github.com/>"" } } ]

",well experimentation believe necessary method really trick thank may wrote might good one implement quite straight forward many different classifier loop set lower would risk loosing accuracy thanks anyway fu sorry stop option far decreasing may help thread reply directly view mute thread context type type target name view issue description view issue publisher type organization name,issue,negative,positive,positive,positive,positive,positive
493188346,"Might be a good one to implement no? Should be quite straight forward.

Because it trains many different classifier in a for loop I can’t set a lower Early-stop because I would risk loosing accuracy on other dataset.

Thanks anyway.

Le 16 mai 2019 à 20:47, Weixuan Fu <notifications@github.com<mailto:notifications@github.com>> a écrit :


Sorry we don't have that stop option so far. Decreasing early_stop may help.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications&email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKFMG6O2JW34EWDCUJRZLA3PVWT35ANCNFSM4HNE5OBA>.

[ { ""@context"": ""http://schema.org"", ""@type"": ""EmailMessage"", ""potentialAction"": { ""@type"": ""ViewAction"", ""target"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460"", ""url"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6MCS2FZYWAMIASH2EDPVWT35A5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSXDBA#issuecomment-493187460"", ""name"": ""View Issue"" }, ""description"": ""View this Issue on GitHub"", ""publisher"": { ""@type"": ""Organization"", ""name"": ""GitHub"", ""url"": ""https://github.com"" } } ]
",might good one implement quite straight forward many different classifier loop set lower would risk loosing accuracy thanks anyway fu sorry stop option far decreasing may help thread reply directly view mute thread context type type target name view issue description view issue publisher type organization name,issue,negative,positive,positive,positive,positive,positive
493187460,Sorry we don't have that stop option so far. Decreasing `early_stop` may help.,sorry stop option far decreasing may help,issue,negative,negative,negative,negative,negative,negative
493183593,"When the cross validation score x generation equals 1. So when all samples are classified correctly. There is no point continuing the evolution. Would be nice to automatically stop and return that pipeline instead of continuing for no improvement. Because in my case I have set early_stop=15 and it makes a long time before stoping (each pipeline run for 30 to 50 minutes).

Is it clear? :-)

Le 16 mai 2019 à 20:04, Weixuan Fu <notifications@github.com<mailto:notifications@github.com>> a écrit :


Hmm what do you mean "" stop a classifier when cv=1""?

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications&email_token=AKFMG6K3NPAATZC5SA42SHTPVWO4LA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSTQGQ#issuecomment-493172762>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKFMG6NATEECROYG5VXSIXDPVWO4LANCNFSM4HNE5OBA>.

[ { ""@context"": ""http://schema.org"", ""@type"": ""EmailMessage"", ""potentialAction"": { ""@type"": ""ViewAction"", ""target"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6K3NPAATZC5SA42SHTPVWO4LA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSTQGQ#issuecomment-493172762"", ""url"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6K3NPAATZC5SA42SHTPVWO4LA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSTQGQ#issuecomment-493172762"", ""name"": ""View Issue"" }, ""description"": ""View this Issue on GitHub"", ""publisher"": { ""@type"": ""Organization"", ""name"": ""GitHub"", ""url"": ""https://github.com"" } } ]
",cross validation score generation classified correctly point evolution would nice automatically stop return pipeline instead improvement case set long time stoping pipeline run clear fu mean stop classifier thread reply directly view mute thread context type type target name view issue description view issue publisher type organization name,issue,negative,positive,neutral,neutral,positive,positive
493170516,"I’ll try. In the mean time is there a way to stop a classifier when cv=1 ?

Le 16 mai 2019 à 18:49, Weixuan Fu <notifications@github.com<mailto:notifications@github.com>> a écrit :


We didn't support Layered-TPOT so far, but you could try to Layered-TPOT branch for saving some computational time.

Installation command:

pip install --upgrade --no-deps --force-reinstall git+ https://github.com/EpistasisLab/tpot.git@Layered-TPOT

LTPOT options in TPOTRegressor

n_layers=1 is default TPOT and n_layers > 1 is LTPOT

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications&email_token=AKFMG6PTAS5VFTQLLIKWYLDPVWGCPA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSNBGQ#issuecomment-493146266>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKFMG6OUXS2ZTC56EFPG6FDPVWGCPANCNFSM4HNE5OBA>.

[ { ""@context"": ""http://schema.org"", ""@type"": ""EmailMessage"", ""potentialAction"": { ""@type"": ""ViewAction"", ""target"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6PTAS5VFTQLLIKWYLDPVWGCPA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSNBGQ#issuecomment-493146266"", ""url"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6PTAS5VFTQLLIKWYLDPVWGCPA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSNBGQ#issuecomment-493146266"", ""name"": ""View Issue"" }, ""description"": ""View this Issue on GitHub"", ""publisher"": { ""@type"": ""Organization"", ""name"": ""GitHub"", ""url"": ""https://github.com"" } } ]
",try mean time way stop classifier fu support far could try branch saving computational time installation command pip install upgrade default thread reply directly view mute thread context type type target name view issue description view issue publisher type organization name,issue,negative,negative,neutral,neutral,negative,negative
493146266,"We didn't support Layered-TPOT so far, but you could try to Layered-TPOT branch for saving some computational time. 

Installation command:

```shell
pip install --upgrade --no-deps --force-reinstall git+ https://github.com/EpistasisLab/tpot.git@Layered-TPOT
```

LTPOT options in `TPOTRegressor`

`n_layers=1` is default TPOT and `n_layers > 1` is LTPOT",support far could try branch saving computational time installation command shell pip install upgrade default,issue,negative,positive,neutral,neutral,positive,positive
493142466,"What about LTPOT? Is it operational?

On 16 May 2019, at 18:35, Weixuan Fu <notifications@github.com<mailto:notifications@github.com>> wrote:


I think it may not make any difference. Maybe using dask option<https://epistasislab.github.io/tpot/using/#parallel-training-with-dask> for this large dataset is better.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications&email_token=AKFMG6LL63YVECYFQ3X2AMDPVWEOVA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSL3EA#issuecomment-493141392>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AKFMG6O3RO25JP4NPKZEUJDPVWEOVANCNFSM4HNE5OBA>.

[ { ""@context"": ""http://schema.org"", ""@type"": ""EmailMessage"", ""potentialAction"": { ""@type"": ""ViewAction"", ""target"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6LL63YVECYFQ3X2AMDPVWEOVA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSL3EA#issuecomment-493141392"", ""url"": ""https://github.com/EpistasisLab/tpot/issues/866?email_source=notifications\u0026email_token=AKFMG6LL63YVECYFQ3X2AMDPVWEOVA5CNFSM4HNE5OBKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODVSL3EA#issuecomment-493141392"", ""name"": ""View Issue"" }, ""description"": ""View this Issue on GitHub"", ""publisher"": { ""@type"": ""Organization"", ""name"": ""GitHub"", ""url"": ""https://github.com"" } } ]

",operational may fu wrote think may make difference maybe option large better thread reply directly view mute thread context type type target name view issue description view issue publisher type organization name,issue,negative,positive,positive,positive,positive,positive
493141392,I think it may not make any difference. Maybe using [dask option](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) for this large dataset is better.,think may make difference maybe option large better,issue,negative,positive,positive,positive,positive,positive
492763464,"Yes, it does not seem to do much difference. It does not freeze though. It always advances but some pipelines last 30min or more whereas I have set the max_eval_time_mins to 10 minutes.
I simply typed it before declaration of the TPOTRegressor :

if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
tpot = TPOTRegressor(generations=200, early_stop=15, max_eval_time_mins=10, population_size=100, verbosity=2, n_jobs=36)


is it supposed to make it work faster?",yes seem much difference freeze though always last min whereas set simply declaration supposed make work faster,issue,negative,positive,neutral,neutral,positive,positive
491214475,"The exception should already be caught, but isn't. I'm afraid this is a xgboost problem, so im closing this issue.",exception already caught afraid problem issue,issue,negative,negative,negative,negative,negative,negative
489778838,"Agreed, this feature would be super useful.",agreed feature would super useful,issue,positive,positive,positive,positive,positive,positive
489192938,"Unfortunately, the max RAM usage is hard to estimate so far. Some pipelines, like a pipeline with `PolynomialFeatures`, require much more RAM than other simple pipelines. But I think 40Gb is safe for 1Gb dataset based on my experience. ",unfortunately ram usage hard estimate far like pipeline require much ram simple think safe based experience,issue,negative,positive,neutral,neutral,positive,positive
489191263,"Thanks! Are there some heuristics to select the `n_jobs` and available disk memory, given the size of the dataset? The dataset of 1 GB works fine with `n_jobs=8` and seems to use about 40GB of RAM max. Can you make a rough estimate for the memory that it will use like `dataset_size * cv * n_jobs`? That seems to hold up for my dataset `1 * 5 * 8 ~ 40`.",thanks select available disk memory given size work fine use ram make rough estimate memory use like hold,issue,positive,positive,positive,positive,positive,positive
489186388,"`max_time_mins` setting can override the generations parameter and allow TPOT to run until max_time_mins minutes elapse, as mentioned in [TPOT API](https://epistasislab.github.io/tpot/api/)

Also, when n_jobs>1, tpot can only check the run time after evaluating a number of pipelines in a generation (chunk size=4*n_jobs or 2*cpus_in_ur_env), so TPOT cannot stop exactly at 120 minutes in this case.",setting override parameter allow run elapse also check run time number generation chunk stop exactly case,issue,negative,positive,positive,positive,positive,positive
488652803,Please check the [link](https://scikit-learn.org/stable/modules/model_persistence.html) about how to pickle the fitted sklearn pipelines. You may use it to save tpot.fitted_pipeline_ object.,please check link pickle fitted may use save object,issue,positive,neutral,neutral,neutral,neutral,neutral
488647263,That it is `STATUS_STACK_BUFFER_OVERRUN (0xc0000409)` error. How large is RAM in your environment? It maybe due to out of memory.,error large ram environment maybe due memory,issue,negative,positive,neutral,neutral,positive,positive
488633111,"I researched this exception a bit and it comes from xgboost. Possibly, xgboost runs out of RAM at some point and then throws this error.

~Next i'll see if it can be caught~ The exeption could be caught with *except xgboost.core.XGBoostError* i guess, but i'm unsure where this would have to happen.",exception bit come possibly ram point error see could caught except guess unsure would happen,issue,negative,neutral,neutral,neutral,neutral,neutral
488442761,"So as of today, is there no way to continue training models from a loaded pickle TPOT object? I actually can't figure out how to do it for regular sklearn pipelines either so I'd assume the answer is no.",today way continue training loaded pickle object actually ca figure regular either assume answer,issue,negative,neutral,neutral,neutral,neutral,neutral
487995025,"@IvoMerchiers thank you for the great suggestions! 

> Probability distribution. Methods such as RandomizedSearchCV or Bayesian optimization methods allow users to define a probability distribution for the values.

I guess we can always manually create a list of values drawn from a particular distribution for a hyperparameter to add to `config_dict`, but I agree, it would be nice to have this step automated as well. It would also support your following idea:

> Interpolation. When performing mutations or recombination between parents that have the same steps, continous variables could respectively receive a random noise or be interpolated from their parents. This would allow them to discover good hyperparameters in a fashion similar to Bayesian hyperoptimization.

This is very intriguing! We'll have to think more about how this fine tuning affects overall performance and computation time, but if you have an idea on how this will be implemented, we're all ears!",thank great probability distribution optimization allow define probability distribution guess always manually create list drawn particular distribution add agree would nice step well would also support following idea interpolation recombination could respectively receive random noise would allow discover good fashion similar intriguing think fine tuning overall performance computation time idea,issue,positive,positive,positive,positive,positive,positive
487837972,"~isn't the MNIST example multi-label classification already?~
I see now that it's strictly predicting one label only",example classification already see strictly one label,issue,negative,neutral,neutral,neutral,neutral,neutral
487559210,@jhmenke we don't have updates on this so far. Please let us know your findings. Thanks.,far please let u know thanks,issue,positive,positive,positive,positive,positive,positive
487550586,"is there an update on this? if not, i would probably look into it over the next months",update would probably look next,issue,negative,neutral,neutral,neutral,neutral,neutral
486664830,@NaderNazemi Could you please provide the fbeta_score function for letting us to reproduce this issue?,could please provide function u reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
486373042,"I am unable to create a custom scorer and feed it into tpot 
# I try the following: 
my_scorer = make_scorer(fbeta_score, beta=beta, pos_label=1, average='binary')

tpot = TPOTClassifier(verbosity=2, 
                      scoring=my_scorer, 
                      random_state=23, 
                      periodic_checkpoint_folder=""tpot_out.txt"", 
                      n_jobs=-1, 
                      generations=15, # INCREASE to 100
                      population_size=40,
                      cv=5)

tpot.fit(X_train, y_train)

# I get an error:
Need to fit ...",unable create custom scorer feed try following increase get error need fit,issue,negative,negative,neutral,neutral,negative,negative
485804574,"Yes, you can directly use the fitted model from a pickle file. Please check the example in this [link](https://scikit-learn.org/stable/modules/model_persistence.html).",yes directly use fitted model pickle file please check example link,issue,positive,positive,neutral,neutral,positive,positive
485284768,"I'm able to save the sate by pickling futted_pipeline_, but how do I use the saved pickle in sci-kit learn? I would like to use it directly with the exported script without re-training it again.",able save sate use saved pickle learn would like use directly script without,issue,positive,positive,positive,positive,positive,positive
484697304,"> I am not sure if TPOT can do this or not.

Fascinating idea! Theoretically, yes. Practically, I'm not sure if we want to implement this feature because of the exact reason you mentioned: [MULTIPLICATIVELY] more resources required.",sure fascinating idea theoretically yes practically sure want implement feature exact reason multiplicatively,issue,positive,positive,positive,positive,positive,positive
484695971,"Ha, one of the first things Jeremy from the fastai course said was that one of the most common problems for data scientists is that they ""peek"" in the test set to make a model perform better.  I thought, no way would I do that!  I can't believe I just did...  It's so tempting though!  I did read an article that mentioned that nested cross validation (more resources required) would be a better choice for tuning parameters to avoid bias.  I am not sure if TPOT can do this or not.  Anyways, thanks again!",ha one first course said one common data peek test set make model perform better thought way would ca believe tempting though read article cross validation would better choice tuning avoid bias sure anyways thanks,issue,positive,positive,positive,positive,positive,positive
484693679,"@jmrichardson Ah so this would be ""double-dipping"", perhaps another concept you can look up. We can't allow TPOT to look at the test set until after a model is selected. Theoretically, the CV accuracy should be an approximation of the test accuracy, unless the independent test set is very different from training set, e.g. sampled under a different distribution.",ah would perhaps another concept look ca allow look test set model selected theoretically accuracy approximation test accuracy unless independent test set different training set different distribution,issue,negative,neutral,neutral,neutral,neutral,neutral
484692120,"So I just thought of something... If the best way to test for overfitting is compare CV with test, it would be great if tpot would select the best model by also comparing to the hold out test set?  I am not exactly sure how this would work but perhaps TPOT could make recommendations or actually select a model that doesn't appear to overfit.  So, for example if the following models:

model1: 73% CV accuracy, 48% Test accuracy
model2: 75% CV accuracy, 49% Test accuracy
model3: 69% CV accuracy, 65% Test accuracy

Model 3 may be a better choice?



",thought something best way test compare test would great would select best model also hold test set exactly sure would work perhaps could make actually select model appear overfit example following model accuracy test accuracy model accuracy test accuracy model accuracy test accuracy model may better choice,issue,positive,positive,positive,positive,positive,positive
484688429,"Yes, that makes complete sense.  I would rather have a slightly ""overfitted"" model with 95% accuracy than a model that yeilds 70% accuracy on both training and test.   I did ask the [question ](https://forums.fast.ai/t/overfitting-based-on-comparing-training-and-cv-accuracy/44078)on the fastai forums for clarification.  Lol, I literally almost switched platforms because I assumed that sklearn and tpot were not robust enough to avoid overfitting in hyperparameter tuning.  Thank goodness you came to the rescue!

",yes complete sense would rather slightly model accuracy model accuracy training test ask question clarification literally almost switched assumed robust enough avoid tuning thank goodness came rescue,issue,positive,negative,neutral,neutral,negative,negative
484680552,"Ha! I love fastai but this first notebook is in fact quite confusing. Now, if they define overfitting as ""high training accuracy and lower testing accuracy"", that's perfectly fine;  however, if so, overfitting is not necessarily *bad* and hence doesn't need reducing (say, compare model 1 that yields 99% training and 95% testing accuracy vs model 2 that yields 70% accuracy for both training and testing). Maybe you should open an issue/question there!

I'm happy to help! It was quite confusing to me as well when I first used random forest and got almost perfect training accuracy.",ha love first notebook fact quite define high training accuracy lower testing accuracy perfectly fine however necessarily bad hence need reducing say compare model training testing accuracy model accuracy training testing maybe open happy help quite well first used random forest got almost perfect training accuracy,issue,positive,positive,positive,positive,positive,positive
484656625,"Hi, Thank you so much for the the reply and it really clears things up.  After I posted, I read this [article](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html) which echos your explanation as well as the link you provided.  I think my confusion was based on the [fastai ML course](https://github.com/fastai/fastai/blob/master/courses/ml1/lesson1-rf.ipynb) I have been taking that appears to suggest that a high training accuracy and lower CV accuracy indicates overfitting.  Here are some of the snips of the course notebook:

> [0.0904611534175684, 0.2517003033389636, 0.9828975209204237, 0.8868601882297901]
> An r^2 in the high-80's isn't bad at all (and the RMSLE puts us around rank 100 of 470 on the Kaggle leaderboard), but we can see from the validation set score that we're over-fitting badly.

> Is our validation set worse than our training set because we're over-fitting, or because the validation set is for a different time period, or a bit of both?

> [0.2317315086850927, 0.26334275954117264, 0.89225792718146846, 0.87615150359885019, 0.88097587673696554]  <-- Notice how the training and CV accuracy are similar now as opposed to the above

Thanks again for the help!  I have been trying forever to figure out why so many hyperparameter tuning packages don't select the best model with respect to training accuracy (error) to avoid overfitting.  Essentially, the only way is to compare CV with the hold-out test set.  

Sigh... So much to learn :)
",hi thank much reply really posted read article explanation well link provided think confusion based course taking suggest high training accuracy lower accuracy course notebook bad u around rank see validation set score badly validation set worse training set validation set different time period bit notice training accuracy similar opposed thanks help trying forever figure many tuning select best model respect training accuracy error avoid essentially way compare test set sigh much learn,issue,negative,negative,neutral,neutral,negative,negative
484646100,"Hi @jmrichardson thanks for the question!

Perfectly fit models can have very different training and testing accuracies. To assess the amount of overfitting, we need to look at the cross-validated accuracy. I'm not sure what you mean by this:
> shouldn't it select the best model with respect to the training accuracy? That is, omit models where the training accuracy is significantly higher than the CV accuracy. 

Except being useful for detecting something that went horribly wrong (e.g. extremely low training accuracy compare to CV or testing accuracy), I think training accuracy is largely useless. For example, here’s a nice stackexchange thread on the often almost-perfect performance of random forest:
https://stats.stackexchange.com/questions/162353/what-measure-of-training-error-to-report-for-random-forests

In short, to conclude on a model's overfitting, I would compare the cross-validated accuracy (instead of standard training accuracy) and testing accuracy of a model.

Let me know if this clarifies things up!",hi thanks question perfectly fit different training testing ass amount need look accuracy sure mean select best model respect training accuracy omit training accuracy significantly higher accuracy except useful something went horribly wrong extremely low training accuracy compare testing accuracy think training accuracy largely useless example nice thread often performance random forest short conclude model would compare accuracy instead standard training accuracy testing accuracy model let know,issue,positive,positive,neutral,neutral,positive,positive
484634557,"Thank you for the help. I forgot I did have the early stop parameter set but still getting overfit models.  I think I am missing something and sure I am doing something wrong.  My understanding is TPOT will select the best model which maximizes CV accuracy (at least in part).  However, shouldn't it select the best model with respect to the training accuracy?  That is, omit models where the training accuracy is significantly higher than the CV accuracy.  Or perhaps have a setting that enables that?  If all models overfit with higher training accuracy then a warning message?  Is this something that could be enabled or something that would be valuable?  I am not sure how this is done with ""autoML"" or if I should be worried about this scenario.  Thanks",thank help forgot early stop parameter set still getting overfit think missing something sure something wrong understanding select best model accuracy least part however select best model respect training accuracy omit training accuracy significantly higher accuracy perhaps setting overfit higher training accuracy warning message something could something would valuable sure done worried scenario thanks,issue,positive,positive,positive,positive,positive,positive
484506168,"> Hmm, it is weird that this version deap module in your environement cannot find 'base' (I used the same version in my environment). Could you please reinstall deap and then try ` from deap import base, creator, tools, gp` in python shell?

which version shoulid i reinstall, thanks",weird version module find used version environment could please reinstall try import base creator python shell version reinstall thanks,issue,positive,negative,negative,negative,negative,negative
484503880,"By default TPOT use CV scores to deal with overfitting issue. And the default CV splitter is [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold) for classification and [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) for regression. From our experiences, sometimes this CV splitter is not ideal.  So 'cv' may need to be specified in some cases.

TPOT can do early stopping via `early_stop` option in [TPOT API](https://epistasislab.github.io/tpot/api/). For checking learning curve, you could try `warm_start` option (related to #832)?",default use deal issue default splitter classification regression sometimes splitter ideal may need early stopping via option learning curve could try option related,issue,negative,positive,positive,positive,positive,positive
484500349,"Hmm, it is weird that this version deap module in your environement cannot find 'base' (I used the same version in my environment). Could you please reinstall deap and then try ` from deap import base, creator, tools, gp` in python shell? ",weird version module find used version environment could please reinstall try import base creator python shell,issue,negative,negative,negative,negative,negative,negative
484488433,"> Which version of `deap` in your environment? Could you please try `python -c ""import deap; print('deap %s' % deap.__version__)""` to find out the information?

1.2.2",version environment could please try python import print find information,issue,negative,neutral,neutral,neutral,neutral,neutral
483207525,"@cakmakaf 

I am still facing the same issue

ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-2-c6a411538777> in <module>()
      3 from sklearn.tree import DecisionTreeClassifier,_tree
      4 import numpy as np
----> 5 from sklearn.modelselection import traintest_split
      6 from sklearn import cross_validation
      7 from sklearn.tree import export_graphviz

ModuleNotFoundError: No module named 'sklearn.modelselection'

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.",still facing issue recent call last ca module import import import import import module note import failing due missing package manually install either pip apt view common click open button,issue,negative,negative,neutral,neutral,negative,negative
482603180,I close this issue because it is fixed in master branch. Please feel free to reopen it if there is other related issues.,close issue fixed master branch please feel free reopen related,issue,positive,positive,positive,positive,positive,positive
481242713,The current version of TPOT doesn't report/store training accuracy during optimization. Maybe it is a useful enhancement function. ,current version training accuracy optimization maybe useful enhancement function,issue,positive,positive,positive,positive,positive,positive
481111382,"C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\python.exe ""D:/NetOps GNS/Python/datasetocr01.py""
C:\Users\ana2\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22
  warnings.warn(msg, category=DeprecationWarning)
C:\Users\ana2\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22
  warnings.warn(msg, category=DeprecationWarning)
Traceback (most recent call last):
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 1318, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 1239, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 1285, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 1234, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 1026, in _send_output
    self.send(msg)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 964, in send
    self.connect()
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\http\client.py"", line 936, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\socket.py"", line 704, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\socket.py"", line 743, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno 11004] getaddrinfo failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/NetOps GNS/Python/datasetocr01.py"", line 9, in <module>
    mnist=fetch_mldata('MNIST original')
  File ""C:\Users\ana2\AppData\Roaming\Python\Python36\site-packages\sklearn\utils\deprecation.py"", line 78, in wrapped
    return fun(*args, **kwargs)
  File ""C:\Users\ana2\AppData\Roaming\Python\Python36\site-packages\sklearn\datasets\mldata.py"", line 133, in fetch_mldata
    mldata_url = urlopen(urlname)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 223, in urlopen
    return opener.open(url, data, timeout)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 526, in open
    response = self._open(req, data)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 544, in _open
    '_open', req)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 504, in _call_chain
    result = func(*args)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 1346, in http_open
    return self.do_open(http.client.HTTPConnection, req)
  File ""C:\Users\ana2\AppData\Local\Programs\Python\Python36-32\lib\urllib\request.py"", line 1320, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno 11004] getaddrinfo failed>

Process finished with exit code 1
",function version removed version function version removed version recent call last file line file line request method body file line body file line file line file line send file line connect file line host port file line host port family type proto handling exception another exception recent call last file line module original file line wrapped return fun file line file line return data file line open response data file line file line result file line return file line raise err error process finished exit code,issue,negative,positive,neutral,neutral,positive,positive
481010786,"I think the issue was the way I was splitting for GroupKFold.  Seems to be working fine now.  However, I would still like to know if there is a way to see the training accuracy in addition to the cv score?

```
groups = df['id']
tpot_config = {
    'sklearn.ensemble.RandomForestClassifier': {
        'n_estimators': [10, 20],
        'max_features': ['auto', 'sqrt', 'log2'],
        'min_samples_leaf': [5,10],
        'max_depth': [10, None],
    },
    'sklearn.tree.ExtraTreeClassifier': {
    }
}

# Train model
tpot = TPOTClassifier(generations=2, population_size=5, verbosity=3,
                     cv=GroupKFold(n_splits=3),
                     periodic_checkpoint_folder='tmp',
                     scoring='accuracy',
                     early_stop=2,
                     random_state=1,
                     memory='tmp',
                     warm_start=True,
                     config_dict=tpot_config
                     )

# tpot.fit(X, y)
tpot.fit(X, y, groups=groups)
```",think issue way splitting working fine however would still like know way see training accuracy addition score none train model,issue,positive,positive,positive,positive,positive,positive
479535644,"Thanks for the help. It works! It would be nice if XGBoost worked in the same way as normal sklearn algorithnts

Erico",thanks help work would nice worked way normal,issue,positive,positive,positive,positive,positive,positive
479534258,"You may try to convert `train[features]` to `np.ndarray` for avoiding this feature name checking in xgboost, like `pipe_mountain.fit(train[features].values, train['Mountain'].values)`",may try convert train feature name like train train,issue,negative,neutral,neutral,neutral,neutral,neutral
479532593,pywin32 should in default packages on Anaconda environment on Windows (see this [list](https://docs.anaconda.com/anaconda/packages/py3.7_win-64/)). If not you may try `pip install pywin32` or `conda install pywin32`. Please let me know if this will solve the issue.,default anaconda environment see list may try pip install install please let know solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
479509451,"@aleksarias This may work: try this before running `tpot.fit(X_train, y_train)`:

```
from xgboost.sklearn import XGBRegressor
from distributed.protocol.serialize import register_generic

register_generic(XGBRegressor)
```

And then try to execute `tpot.fit(X_train, y_train)`",may work try running import import try execute,issue,negative,neutral,neutral,neutral,neutral,neutral
477590643,"@nielsgl 

There is no `xgboost` in the running environment and the verbosity=3. You may install xgboost or set verbosity=2 for ignoring this ImportError. 

Please check #806 for more details.",running environment may install set please check,issue,negative,neutral,neutral,neutral,neutral,neutral
477587615,Please feel free to reopen this issue if it is a TPOT related issue/question.,please feel free reopen issue related,issue,positive,positive,positive,positive,positive,positive
477513279,"I'm running in this issue as well, although I'm using Ubuntu instead of Windows. I don't have XGBoost installed.

Output:
```
/usr/local/lib/python3.5/dist-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.
  ""module. Expect this to be very slow."", ImportWarning)
Exception during training: Error: could not import xgboost.XGBClassifier.
No module named 'xgboost'
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tpot/operator_utils.py"", line 76, in source_decode
    exec('from {} import {}'.format(import_str, op_str))
  File ""<string>"", line 1, in <module>
ImportError: No module named 'xgboost'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/opt/program/train"", line 67, in train
    clf.fit(train_X, train_y)
  File ""/usr/local/lib/python3.5/dist-packages/tpot/base.py"", line 562, in fit
    self._fit_init()
  File ""/usr/local/lib/python3.5/dist-packages/tpot/base.py"", line 470, in _fit_init
    verbose=self.verbosity
  File ""/usr/local/lib/python3.5/dist-packages/tpot/operator_utils.py"", line 170, in TPOTOperatorClassFactory
    import_str, op_str, op_obj = source_decode(opsourse, verbose=verbose)
  File ""/usr/local/lib/python3.5/dist-packages/tpot/operator_utils.py"", line 80, in source_decode
    raise ImportError('Error: could not import {}.\n{}'.format(sourcecode, e))
ImportError: Error: could not import xgboost.XGBClassifier.
No module named 'xgboost'
```

Pip list:
```
# pip list
Package         Version
--------------- --------
certifi         2019.3.9
chardet         3.0.4
Click           7.0
deap            1.2.2
Flask           1.0.2
gevent          1.4.0
greenlet        0.4.15
gunicorn        19.9.0
idna            2.8
itsdangerous    1.1.0
Jinja2          2.10
joblib          0.13.2
MarkupSafe      1.1.1
numpy           1.16.2
pandas          0.24.2
pip             19.0.3
python-dateutil 2.8.0
pytz            2018.9
requests        2.21.0
scikit-learn    0.20.3
scipy           1.2.1
setuptools      40.8.0
six             1.12.0
stopit          1.1.2
TPOT            0.9.6
tqdm            4.31.1
update-checker  0.16
urllib3         1.24.1
Werkzeug        0.15.1
wheel           0.33.1
```

I'm simply calling the `TPOTClassifier` with:
```python
clf = TPOTClassifier(generations=5, population_size=5, cv=5, verbosity=3)
clf.fit(train_X, train_y)
```",running issue well although instead output falling back python version hypervolume module expect slow module expect slow exception training error could import module recent call last file line import file string line module module handling exception another exception recent call last file line train file line fit file line file line file line raise could import error could import module pip list pip list package version click flask greenlet jinja pip six wheel simply calling python,issue,negative,negative,neutral,neutral,negative,negative
476398960,I've found that if I use the dask backend (`use_dask=True`) then everything runs smoothly.,found use everything smoothly,issue,negative,positive,positive,positive,positive,positive
475739516,Thank you for this PR. I think this IndexError will raise if the `categorical_features` is assigned an array including a incorrect index or more. I think we need this `IndexError` message for letting user to check input array.,thank think raise assigned array incorrect index think need message user check input array,issue,negative,neutral,neutral,neutral,neutral,neutral
475479616,"Thank you! So is it safe to say that, for this large dataset, except for setting TPOT Light mode, it is better to avoid to use TPOT for automated pipelines model selection?",thank safe say large except setting light mode better avoid use model selection,issue,positive,positive,positive,positive,positive,positive
475257446,@lidatou1991 I checked your codes and tested in a small dataset and it worked fine. Since you only has XGBRegessor in the `config_dict`. You may set `n_jobs=1` in TPOTRegessor and set `n_jobs=-1` in XGBRegessor via the `tpot_config` (`'nthread': [-1]` or `'n_jobs': [-1]`). I think that is more safe way.,checked tested small worked fine since may set set via think safe way,issue,positive,positive,positive,positive,positive,positive
474872135,"> > @lidatou1991 I just requested permission to check codes in the colab.
> > For this large dataset, `config_dict='TPOT light'` should be helpful. Also increasing `max_time_mins` should be necessary or you may try `subsample` option. And `n_jobs=-1` may cause out-of-memory issue in this case so that decreasing `n_jobs` maybe needed.
> > I will have another look once I checked your codes in colab.
> 
> Many thanks to quick reply. In fact, I indeed tried `config_dict='TPOT light'` on another dataset and it worked well(same regression problem). But normally it only return K neighbor regressor with TPOT light mode. I am interested in xgboost performance so I ignore ligh mode this time.
> 
> In addition, I shared with you the colab notebook just for convenience. On GCP I ran it with VM (8 cores, 30gb ram and 2 GPUs) and there I set `max_time_mins = 420` and normally that is enough. So it is really strange. I will continue to search for solution and I hope to hear from you. Thanks.

I get the share link directly from Colab so I guess [this](https://colab.research.google.com/drive/19Y7uYyeyMiKlzp_fa6SB1lRSS6jI84wC) will work.

or you can try with google drive https://drive.google.com/file/d/1j7_8Y8E7-tr0B5JooVJAVqnD2t08Rm38/view?usp=sharing. 

Thanks for your suggestion I will get back to you after following it.



",permission check large light helpful also increasing necessary may try subsample option may cause issue case decreasing maybe another look checked many thanks quick reply fact indeed tried light another worked well regression problem normally return neighbor regressor light mode interested performance ignore mode time addition notebook convenience ran ram set normally enough really strange continue search solution hope hear thanks get share link directly guess work try drive thanks suggestion get back following,issue,positive,positive,positive,positive,positive,positive
474836820,"Somehow, I still cannot access the shared Colab with my gmail account. 30gb ram with 8 cores (n_jobs=8) maybe a little bit risky for this large dataset. I suggest to set to n_jobs=4. Also increasing `max_eval_time_mins` (default is 5 minutes) should be helpful in this case. ",somehow still access account ram maybe little bit risky large suggest set also increasing default helpful case,issue,negative,positive,neutral,neutral,positive,positive
474420760,"> @lidatou1991 I just requested permission to check codes in the colab.
> 
> For this large dataset, `config_dict='TPOT light'` should be helpful. Also increasing `max_time_mins` should be necessary or you may try `subsample` option. And `n_jobs=-1` may cause out-of-memory issue in this case so that decreasing `n_jobs` maybe needed.
> 
> I will have another look once I checked your codes in colab.

Many thanks to quick reply. In fact, I indeed tried `config_dict='TPOT light'`  on another dataset and it worked well(same regression problem). But normally it only return K neighbor regressor with TPOT light mode. I am interested in xgboost performance so I ignore ligh mode this time.

In addition, I shared with you the colab notebook just for convenience.  On GCP I ran it with VM (8 cores, 30gb ram and 2 GPUs) and there I set `max_time_mins = 420` and normally that is enough. So it is really strange. I will continue to search for solution and I hope to hear from you. Thanks.",permission check large light helpful also increasing necessary may try subsample option may cause issue case decreasing maybe another look checked many thanks quick reply fact indeed tried light another worked well regression problem normally return neighbor regressor light mode interested performance ignore mode time addition notebook convenience ran ram set normally enough really strange continue search solution hope hear thanks,issue,positive,positive,positive,positive,positive,positive
474411452,"@lidatou1991 I just requested permission to check codes in the colab. 

For this large dataset, `config_dict='TPOT light'` should be helpful. Also increasing `max_time_mins` should be necessary or you may try `subsample` option. And `n_jobs=-1` may cause out-of-memory issue in this case so that decreasing `n_jobs` maybe needed.

I will have another look once I checked your codes in colab.",permission check large light helpful also increasing necessary may try subsample option may cause issue case decreasing maybe another look checked,issue,negative,positive,positive,positive,positive,positive
474401599,@weixuanfu  @rhiever Please help since the problem has bothered me for several days. I am not a professional but I really think TPOT is awesome so I hope I can make something with it. Thanks in advance.,please help since problem several day professional really think awesome hope make something thanks advance,issue,positive,positive,positive,positive,positive,positive
473577730,"@weixuanfu Thank you for your quick reply. Here is my newest version of notebook. Also, i'm giving you the video of a run, where I caught the moment when the program terminates (1:33 - 1:40 in the video). At no point in the execution did the memory load for any individual worker exceeds the limit I set (never >1 GB while the limit I set was 15 GB, see video). *Attempt 2* section in the notebook is the code that corresponds with the run in the video. 

Notebook: https://github.com/hoangthienan95/sharing/blob/master/TPOT_test_error_report.ipynb
Video: https://streamable.com/zenu6

As you can see, Im using v0.9.5 of TPOT, before this attempt, I did not set `n_jobs` because I was afraid it would interfere with `use_dask`. Now I set it to half the worker# like you said, and the problem of only one worker doing all the work is gone. However, my program still terminate with alot of errors. 

There are also quite alot of `Worker restarted, might be due to memory` errors, which really confuses me because of the observation above. I can try running using the joblib backend method, but as another user noted in #779, it doesn't really solve the problem.

This might be a dask/dask-jobqueue/dask-distributed thing, if that's the case please point me to who I can ask for help. Although I have performed other computation with Dask using this cluster and set up, so I thought it could be a problem with TPOT.

Let me know if you need any other information to diagnose the problem!",thank quick reply version notebook also giving video run caught moment program video point execution memory load individual worker limit set never limit set see video attempt section notebook code run video notebook video see attempt set afraid would interfere set half worker like said problem one worker work gone however program still terminate also quite worker might due memory really observation try running method another user noted really solve problem might thing case please point ask help although computation cluster set thought could problem let know need information diagnose problem,issue,negative,negative,neutral,neutral,negative,negative
473282239,"The demo below is for getting those scores after `tpot_obj.fit()`. Please check [this link](https://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) for more scoring metrics.

```python
from sklearn.metrics import SCORERS
best_pipeline = tpot_obj.fitted_pipeline_
# r2 score
holdout_score_r2 = SCORERS['r2'](
                          best_pipeline,
                          testing_features,
                          testing_target
                      )

# neg_mean_squared_error
holdout_score_nmse = SCORERS['neg_mean_squared_error'](
                          best_pipeline,
                          testing_features,
                          testing_target
                      )

```

",getting please check link scoring metric python import score,issue,negative,neutral,neutral,neutral,neutral,neutral
472979451,"> Anecdotally, my custom RandomizedSearchCV pipeline search over XGBRegressor is quickly discovering much more performant models than tpot.

@ghost Would you be willing to share your custom RandomizedSearchCV?",custom pipeline search quickly much performant ghost would willing share custom,issue,negative,positive,positive,positive,positive,positive
472466034,"Hi, is this a TPOT related question? I think it is more related to [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). Could you please post this issue to scikit-learn's repo?",hi related question think related could please post issue,issue,negative,neutral,neutral,neutral,neutral,neutral
472463219,"To clarify, I don't have any experience about using `LSFcluster`. Thank you for submitting this issue and testing TPOT in HPC environemnt. 

Hope the suggestions below are helpful.

Could you please try client object in the demo below before setting TPOT object? Also please print out the `client` to check the connection and configuration of the LSF cluster?

```python
cluster = LSFCluster(queue='corradin_long',
                     cores= 2,
                     walltime='100000:00',
                     memory='10GB',
                     death_timeout=600

# Start 50 workers in 50 jobs that match the description above
cluster.scale(50)  # this may take a few seconds to launch
from dask.distributed import Client
# connect to the cluster
client = Client(cluster)
```
When the setting is 2 cores per worker, what is `n_jobs` setting in TPOT? In the latest version of TPOT (v 0.9.6), we reduced to `n_jobs` to control the chunk size (it should be n_jobs * 4 in this case) for building task graph (The docs for this part is out of date. We will fix that in next release.). Increasing `n_jobs` (e.g n_jobs=25 for population_size >= 100 and workers # =50) may help if you are using this version. 

",clarify experience thank issue testing hope helpful could please try client object setting object also please print client check connection configuration cluster python cluster start match description may take launch import client connect cluster client client cluster setting per worker setting latest version reduced control chunk size case building task graph part date fix next increasing may help version,issue,positive,positive,positive,positive,positive,positive
472258759,"Update:

I tried both of the configs. Both of them fail in the first generation.

For the ""less workers, more cores per worker"" config, I encounter the problem where only one worker takes all the memory and tasks (up to 231GB, which is the limit for one worker!) as shown below. This results in the worker being killed multiple times and ultimately fail the computation after 3 retries. Doing this way, the command `.fit()` fails more quickly, with no task graphs observed on Dask UI.

![image](https://user-images.githubusercontent.com/25307953/54249307-019a5780-4516-11e9-943b-ec29486027c7.png)

The same thing happens when I do 2 cores per worker, here you can see that one worker takes the entire 5.12 GB and 120 tasks, while others do no work. However, it was able to build the task graph and do some computation.

![image](https://user-images.githubusercontent.com/25307953/54250001-80908f80-4518-11e9-86bc-3b75cca2a3e3.png)

![image](https://user-images.githubusercontent.com/25307953/54250093-c8171b80-4518-11e9-9470-18d4f83f9ba8.png)

What's happening here?? TPOT works fine (but incredibly slow) if I use `client` as local machine instead of the LSF cluster:

```
from dask.distributed import Client
client = Client(n_workers=4, threads_per_worker=1)
client
```

",update tried fail first generation le per worker encounter problem one worker memory limit one worker shown worker multiple time ultimately fail computation way command quickly task image thing per worker see one worker entire work however able build task graph computation image image happening work fine incredibly slow use client local machine instead cluster import client client client client,issue,negative,positive,neutral,neutral,positive,positive
472120596,"Hmm, I think it is a good option, which is similar with [`n_jobs` in scikit-learn](https://scikit-learn.org/stable/glossary.html#term-n-jobs). We will add this option into next version of TPOT. ",think good option similar add option next version,issue,negative,positive,positive,positive,positive,positive
472107631,why not less than -1? this implies that I want to reserve some CPUs for general use and the other ones used in parallel here ,le want reserve general use used parallel,issue,negative,positive,neutral,neutral,positive,positive
471579076,"Yes, that is the bug. It should create the dir I think not error.",yes bug create think error,issue,negative,neutral,neutral,neutral,neutral,neutral
471504576,"`ValueError: Could not find directory for memory caching: asdf`

Besed on the error message above, the issue is caused by the wrong `memory` parameter. It should be a valid path of directory.",could find directory memory error message issue wrong memory parameter valid path directory,issue,negative,negative,negative,negative,negative,negative
471502786,"Hi, I have been checking the possibility of adding this to TPOT, but I would like to double check the procedure.

As in Novelty detection, the fit function should get both, the fit data (normal data) and a Xy validation set (mixture of normal and novel data with their labels), in order to score the validity of the evaluated models. 
Also, those labels of validation should be determined, as sklearn uses 1/-1, and many people uses 1/0.
The training time of those models can also be a problem, mostly for the One Class SVM, as the time needed may exceed easily the expected limits.",hi possibility would like double check procedure novelty detection fit function get fit data normal data validation set mixture normal novel data order score validity also validation determined many people training time also problem mostly one class time may exceed easily,issue,positive,positive,positive,positive,positive,positive
471339058,"Here is code

```
In [22]: t = tpot.TPOTClassifier(memory='asdf')                                                                                                                                                                    

In [23]: t.fit(np.random.randn(100, 5), np.random.rand(100) > 0.5)                                                                                                                                                 
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/dev/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    647             with warnings.catch_warnings():
--> 648                 self._setup_memory()
    649                 warnings.simplefilter('ignore')

~/dev/tpot/tpot/base.py in _setup_memory(self)
    708                     raise ValueError(
--> 709                         'Could not find directory for memory caching: {}'.format(self.memory)
    710                     )

ValueError: Could not find directory for memory caching: asdf

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-23-1844720facf9> in <module>
----> 1 t.fit(np.random.randn(100, 5), np.random.rand(100) > 0.5)

~/dev/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    691                     # raise the exception if it's our last attempt
    692                     if attempt == (attempts - 1):
--> 693                         raise e
    694             return self
    695 

~/dev/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    682                         self._pbar.close()
    683 
--> 684                     self._update_top_pipeline()
    685                     self._summary_of_best_pipeline(features, target)
    686                     # Delete the temporary cache before exiting

~/dev/tpot/tpot/base.py in _update_top_pipeline(self)
    756             # If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.
    757             # need raise RuntimeError because no pipeline has been optimized
--> 758             raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
    759 
    760     def _summary_of_best_pipeline(self, features, target):

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.

```
",code recent call last fit self target self raise find directory memory could find directory memory handling exception another exception recent call last module fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self user initial generation yet need raise pipeline raise pipeline yet please call fit first self target pipeline yet please call fit first,issue,positive,positive,positive,positive,positive,positive
470925661,"@miguelehernandez So far, there is no API way or a way without using `tpot_obj`. You're welcome to make contribution to this function! ",far way way without welcome make contribution function,issue,negative,positive,positive,positive,positive,positive
470691346,"@weixuanfu , thanks so much for the quick reply!

Two follow up questions:
1) Is there an API way to do this that doesn't rely on private methods on the `tpot_obj`? 
2) How does one go about initializing the `_pset` field when you want to convert the `pipeline_string` into  a `sklearn.Pipeline` in a session in which the original `tpot_obj` is not available? For example, say I output the `pipeline_string` to a file and wanted to read it into a `sklearn.Pipeline` in another session?

If there is currently no API way to do this, would it be possible for me to contribute something along those lines?",thanks much quick reply two follow way rely private one go field want convert session original available example say output file read another session currently way would possible contribute something along,issue,positive,positive,positive,positive,positive,positive
470681480,"@miguelehernandez Please try the demo below to convert key strings to `Pipeline`

```python
from tpot.export_utils import generate_pipeline_code, expr_to_tree

# print a pipeline and its values
pipeline_str = list(tpot.evaluated_individuals_.keys())[0]
print(pipeline_str)
print(tpot.evaluated_individuals_[pipeline_str])
for pipeline_string in sorted(tpot_obj.evaluated_individuals_.keys()):
    # convert pipeline string to scikit-learn pipeline object
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot_obj._pset)
    sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
    # print sklearn pipeline string
    sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(individual, tpot_obj._pset), tpot_obj.operators)
    print(sklearn_pipeline_str)```",please try convert key pipeline python import print pipeline list print print sorted convert pipeline string pipeline object print pipeline string individual print,issue,negative,neutral,neutral,neutral,neutral,neutral
470637265,"@weixuanfu, is there a way to turn the `evaluated_individuals_` key into an `sklearn.pipeline` without having a fitted TPOTClassifier or TPOTRegressor? I'd like to be record the key strings from `evaluated_individuals_`  and load them as `sklearn.Pipeline` objects in sessions after I've fit my TPOTClassifier/Regressor and no longer have access to the initial TPOTClassifier/Regressor that had the `evaluated_individuals_`.",way turn key without fitted like record key load session fit longer access initial,issue,negative,positive,neutral,neutral,positive,positive
469036155,"Unfortunately a core dependency of TPOT requires LGPL, so unless we re-code TPOT to exclude that dependency, re-licensing isn't possible.",unfortunately core dependency unless exclude dependency possible,issue,negative,negative,negative,negative,negative,negative
469013849,"I still have to do a lot of reading on LGPL implications, but first google hits are not promising https://nikmav.blogspot.com/2013/03/the-perils-of-lgplv3.html (all this reading and possibly paying a lawyer...when I just wanted to go ahead and start coding :) 

What about some good ol' MIT license that allows developers to simply get to coding? :)",still lot reading first promising reading possibly paying lawyer go ahead start good license simply get,issue,positive,positive,positive,positive,positive,positive
468770858,"I think you may manually split dataset into train/test set instead of using 'train_test_split'. (see [this link](https://stackoverflow.com/questions/50879915/how-to-split-data-using-time-based-in-test-and-train-respectively))

Also for using TPOT, you may try `cv=TimeSeriesSplit(n_splits=5)` for time series problem. Please see the [TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for more details.",think may manually split set instead see link also may try time series problem please see,issue,negative,neutral,neutral,neutral,neutral,neutral
468139420,"Hi, I'm trying out tpot for time series problem and i have two separate dataset, one for training and one for testing. While using normal train test split it gives you x_train, y_train, x_test, y_test. In this case im having separate dataset. Since i'm new to machine learning i'm confused then what are the parameters i need to pass for fit function , normally x_train and y_train is passed. In my case what should i pass? If someone could help me perform the timeseries it would be very helpful. My dataset contains 14 columns , one is the date and time and other is output variable and the related features.",hi trying time series problem two separate one training one testing normal train test split case separate since new machine learning confused need pas fit function normally case pas someone could help perform would helpful one date time output variable related,issue,negative,positive,neutral,neutral,positive,positive
468004047,"TPOT does not fully support `pandas.DataFrame` input yet. So I supposed that  `work10k_num.values` is used in your codes.

For reproducing this issue related to `bool` type, try the demo below:


```python
import numpy as np
import pandas as pd
import collections
d = {'col1': [1.0, 2.0], 'col2': [3, 4], 'col3': [1, 2]}
df = pd.DataFrame(data=d)
df['col3'] = df['col3'].astype(np.uint8)
print(collections.Counter(df.dtypes))
print(np.isnan(df.values)) # this will pass without bool type 

d = {'col1': [1.0, 2.0], 'col2': [3, 4], 'col3': [1, 2],'col4': [True, False]}
df = pd.DataFrame(data=d)
df['col3'] = df['col3'].astype(np.uint8)

print(collections.Counter(df.dtypes))
print(np.isnan(df.values)) # this will raise error due to bool type
```",fully support input yet supposed used issue related bool type try python import import import print print pas without bool type true false print print raise error due bool type,issue,negative,negative,neutral,neutral,negative,negative
467996014,"`np.isnan` can be applied to np.arrays of native dtype (such as `np.float64`). I saw 'bool' dtype in your dataframe, so type conversion or encoding may be needed.",applied native saw type conversion may,issue,negative,neutral,neutral,neutral,neutral,neutral
467993347,"I get this error with all-numeric dataframe:
```python
collections.Counter(work10k_num.dtypes)

Counter({dtype('float64'): 13,
         dtype('int64'): 374,
         dtype('bool'): 15,
         dtype('uint8'): 1})
```

and then
```
~/.virtualenvs/.../site-packages/tpot/base.py in _check_dataset(self, features, target, sample_weight)
   1062                 )
   1063         else:
-> 1064             if np.any(np.isnan(features)):
   1065                 self._imputed = True
   1066                 features = self._impute_values(features)

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
",get error python counter self target else true input could safely according casting rule safe,issue,positive,positive,positive,positive,positive,positive
466421627,Sounds a good idea! I think it can be easily added to export function.,good idea think easily added export function,issue,positive,positive,positive,positive,positive,positive
466354812,"I got same error when I installed xgboost. I tried all the solutions above but nothing worked for me. The problem disappeared after uninstalling xgboost. However, xgboost.XGBRegressor was not by TPOT. ",got error tried nothing worked problem however,issue,negative,neutral,neutral,neutral,neutral,neutral
465689131,"Having the same issue. Not sure, but may be a problem inside Dask and its serializer.",issue sure may problem inside,issue,negative,positive,positive,positive,positive,positive
465511748,"Hmm, TPOT should raise a ValueError when the memory folder does not exist. Was the error raised in this case? Could you please provide more details about this issue, like codes for reproducing it?",raise memory folder exist error raised case could please provide issue like,issue,negative,neutral,neutral,neutral,neutral,neutral
465184270,"I understand, it is a very specific case that I'm working on. Just currently there is no way to escape imputation with TPOT unless you modify the source. It is not a necessity perhaps more a nice to have.",understand specific case working currently way escape imputation unless modify source necessity perhaps nice,issue,positive,positive,positive,positive,positive,positive
465136198,TPOT enforces imputation on dataset with NaN because most [operators in TPOT configuration](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py) do not support NaN. We may need another configuration if this `no_impuation` option is added.,imputation nan configuration support nan may need another configuration option added,issue,negative,neutral,neutral,neutral,neutral,neutral
464364024,"High Weixuan,

 

I have made these changes, and the results are posted in the Issues thread.  

I have updated my notebook (https://github.com/CBrauer/CypressPoint.github.io) to show an analysis of the dataset.  

Any comment would be greatly appreciated.

 

Charles

 

From: Weixuan Fu <notifications@github.com> 
Sent: Friday, February 15, 2019 12:46 PM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: Charles Brauer <CBrauer@CypressPoint.com>; State change <state_change@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] Perfect score, Bad prediction version 2 (#835)

 

Maybe turn off early_stop can improve the results and also for reproducing the results, random_state parameter should be set in TPOTClassifier.

—
You are receiving this because you modified the open/close state.
Reply to this email directly,  <https://github.com/EpistasisLab/tpot/issues/835#issuecomment-464193141> view it on GitHub, or  <https://github.com/notifications/unsubscribe-auth/ABQZcij2hoUhGQI2yTTUKG9FLyyNWdXBks5vNxyfgaJpZM4a-BJF> mute the thread.  <https://github.com/notifications/beacon/ABQZcnT2Xj9zmBKbRMizEIUT7WWQQv0Eks5vNxyfgaJpZM4a-BJF.gif> 

",high made posted thread notebook show analysis comment would greatly fu sent state change subject perfect score bad prediction version maybe turn improve also parameter set state reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
464360973,"**I set generations=100 and population_size=100. Here is my log:**
```
Generation 1 - Current best internal CV score: 0.6701103372694357
Generation 2 - Current best internal CV score: 0.6753643158863853
Generation 3 - Current best internal CV score: 0.6753643158863853
Generation 4 - Current best internal CV score: 0.6753643158863853
Generation 5 - Current best internal CV score: 0.6860550982657302
Generation 6 - Current best internal CV score: 0.6860550982657302
Generation 7 - Current best internal CV score: 0.6869510081986385
Generation 8 - Current best internal CV score: 0.6869510081986385
Generation 9 - Current best internal CV score: 0.6869510081986385
Generation 10 - Current best internal CV score: 0.6935570969404476
Generation 11 - Current best internal CV score: 0.6935570969404476
Generation 12 - Current best internal CV score: 0.7009959772395419
Generation 13 - Current best internal CV score: 0.7009959772395419
Generation 14 - Current best internal CV score: 0.7009959772395419
Generation 15 - Current best internal CV score: 0.7009959772395419
Generation 16 - Current best internal CV score: 0.711099106248701
Generation 17 - Current best internal CV score: 0.711099106248701
Generation 18 - Current best internal CV score: 0.711099106248701
Generation 19 - Current best internal CV score: 0.711099106248701
Generation 20 - Current best internal CV score: 0.7138954160468657
Generation 21 - Current best internal CV score: 0.7138954160468657
Generation 22 - Current best internal CV score: 0.7138954160468657
Generation 23 - Current best internal CV score: 0.7155672672138507
Generation 24 - Current best internal CV score: 0.7155672672138507
Generation 25 - Current best internal CV score: 0.7155672672138507
Generation 26 - Current best internal CV score: 0.7155672672138507
Generation 27 - Current best internal CV score: 0.7155672672138507
Generation 28 - Current best internal CV score: 0.7155672672138507
Generation 29 - Current best internal CV score: 0.7155672672138507
Generation 30 - Current best internal CV score: 0.7155672672138507
Generation 31 - Current best internal CV score: 0.7155672672138507
Generation 32 - Current best internal CV score: 0.7155672672138507
Generation 33 - Current best internal CV score: 0.7155672672138507
Generation 34 - Current best internal CV score: 0.7155672672138507
Generation 35 - Current best internal CV score: 0.7155672672138507
Generation 36 - Current best internal CV score: 0.7155672672138507
Generation 37 - Current best internal CV score: 0.7155672672138507
Generation 38 - Current best internal CV score: 0.7155672672138507
Generation 39 - Current best internal CV score: 0.7155672672138507
Generation 40 - Current best internal CV score: 0.7155672672138507
Generation 41 - Current best internal CV score: 0.7155672672138507
Generation 42 - Current best internal CV score: 0.7155672672138507
Generation 43 - Current best internal CV score: 0.7155672672138507
Generation 44 - Current best internal CV score: 0.7155672672138507
Generation 45 - Current best internal CV score: 0.7155672672138507
Generation 46 - Current best internal CV score: 0.7155672672138507
Generation 47 - Current best internal CV score: 0.7155672672138507
Generation 48 - Current best internal CV score: 0.7155672672138507
Generation 49 - Current best internal CV score: 0.7155672672138507
Generation 50 - Current best internal CV score: 0.7155672672138507
Generation 51 - Current best internal CV score: 0.7155672672138507
Generation 52 - Current best internal CV score: 0.7155672672138507
Generation 53 - Current best internal CV score: 0.7155672672138507
Generation 54 - Current best internal CV score: 0.7155672672138507
Generation 55 - Current best internal CV score: 0.7155672672138507
Generation 56 - Current best internal CV score: 0.7155672672138507
Generation 57 - Current best internal CV score: 0.7159585526663005
Generation 58 - Current best internal CV score: 0.7159585526663005
Generation 59 - Current best internal CV score: 0.7159585526663005
Generation 60 - Current best internal CV score: 0.7159585526663005
Generation 61 - Current best internal CV score: 0.7159585526663005
Generation 62 - Current best internal CV score: 0.7159585526663005
Generation 63 - Current best internal CV score: 0.7159585526663005
Generation 64 - Current best internal CV score: 0.7159585526663005
Generation 65 - Current best internal CV score: 0.7159585526663005
Generation 66 - Current best internal CV score: 0.7159585526663005
Generation 67 - Current best internal CV score: 0.7159585526663005
Generation 68 - Current best internal CV score: 0.7159585526663005
Generation 69 - Current best internal CV score: 0.7159585526663005
Generation 70 - Current best internal CV score: 0.7159585526663005
Generation 71 - Current best internal CV score: 0.7159585526663005
Generation 72 - Current best internal CV score: 0.7159585526663005
Generation 73 - Current best internal CV score: 0.7159585526663005
Generation 74 - Current best internal CV score: 0.7159585526663005
Generation 75 - Current best internal CV score: 0.7159585526663005
Generation 76 - Current best internal CV score: 0.7159585526663005
Generation 77 - Current best internal CV score: 0.7159585526663005
Generation 78 - Current best internal CV score: 0.7159585526663005
Generation 79 - Current best internal CV score: 0.7159585526663005
Generation 80 - Current best internal CV score: 0.7159585526663005
Generation 81 - Current best internal CV score: 0.7159585526663005
Generation 82 - Current best internal CV score: 0.7159585526663005
Generation 83 - Current best internal CV score: 0.7159585526663005
Generation 84 - Current best internal CV score: 0.7159585526663005
Generation 85 - Current best internal CV score: 0.7159585526663005
Generation 86 - Current best internal CV score: 0.7159585526663005
Generation 87 - Current best internal CV score: 0.7159585526663005
Generation 88 - Current best internal CV score: 0.7159585526663005
Generation 89 - Current best internal CV score: 0.7159585526663005
Generation 90 - Current best internal CV score: 0.7159585526663005
Generation 91 - Current best internal CV score: 0.7159585526663005
Generation 92 - Current best internal CV score: 0.7159585526663005
Generation 93 - Current best internal CV score: 0.7159585526663005
Generation 94 - Current best internal CV score: 0.7159585526663005
Generation 95 - Current best internal CV score: 0.7159585526663005
Generation 96 - Current best internal CV score: 0.7159585526663005
Generation 97 - Current best internal CV score: 0.7159585526663005
Generation 98 - Current best internal CV score: 0.7159585526663005
Generation 99 - Current best internal CV score: 0.7159585526663005
Generation 100 - Current best internal CV score: 0.7159585526663005
```
**The exported_pipeline was:**

```
# Average CV score on the training set was:0.7159585526663005
exported_pipeline = make_pipeline(
    StackingEstimator(estimator=GradientBoostingClassifier(learning_rate=0.01, 
                                                           max_depth=8,
                                                           max_features=0.5,
                                                           min_samples_leaf=5,
                                                           min_samples_split=17,
                                                           n_estimators=100,
                                                           subsample=0.5)),
    StandardScaler(),
    SelectPercentile(score_func=f_classif, percentile=73),
    BernoulliNB(alpha=0.01, fit_prior=False)
)
```

**When I put this pipeline into my notebook, I get:**

**Current version (TPOT 0.9.5), Test dataset**
accuracy......................................  0.73462844 
precision.....................................  0.15213115 
recall.............................................  0.67052023 
misclassification rate..............  0.26537156 
F1...................................................  0.24799572 
r2................................................... -3.35037229
AUC..............................................  0.70481217 
mse...............................................  0.26537156 
logloss.........................................  9.16580390 ",set log generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score average score training set put pipeline notebook get current version test accuracy precision recall misclassification rate,issue,positive,positive,positive,positive,positive,positive
464193141,"Thank you for the updates. Maybe turning off `early_stop` can improve the results and also for reproducing the results, `random_state` parameter should be set in `TPOTClassifier`.",thank maybe turning improve also parameter set,issue,positive,neutral,neutral,neutral,neutral,neutral
464188746,"I have made your requested changes to my run script:

```
import platform
import sys
import pandas as pd
import numpy as np
import time
from collections import Counter
from imblearn.over_sampling import SMOTE

from tpot import TPOTClassifier

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import RobustScaler
from sklearn.utils import shuffle

class Timer:
    def __init__(self):
        self.start = time.time()

    def restart(self):
        self.start = time.time()

    def get_time(self):
        end = time.time()
        m, s = divmod(end - self.start, 60)
        h, m = divmod(m, 60)
        time_str = ""%02d:%02d:%02d"" % (h, m, s)
        return time_str

def LoadData():

    model_full = pd.read_csv('https://raw.githubusercontent.com/CBrauer/CypressPoint.github.io/master/model-13-1.csv')

    model_full = shuffle(model_full)

    response_column = ['Altitude']
    feature_columns = ['BoxRatio', 'Thrust', 'Velocity', 'OnBalRun', 'vwapGain']
    header = feature_columns + response_column

    model = model_full[header]
    print('Model dataset:\n', model.head(5))
    print('\nDescription of model dataset:\n', model[feature_columns].describe(include='all'))
    
    X = model[feature_columns].values
    y = model[response_column].values.ravel()

    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                        test_size = 0.3,
                                                        random_state = 7)
    print('Size of dataset:')
    print(' train shape... ', X_train.shape, y_train.shape)
    print(' test shape.... ', X_test.shape, y_test.shape)

    return X_train, y_train, X_test, y_test

def Main(g, p):
    X_train, y_train, X_test, y_test = LoadData()

    pipeline_optimizer = TPOTClassifier(generations=g,
                                        population_size=p,
                                        n_jobs=4,
                                        scoring='balanced_accuracy',
                                        periodic_checkpoint_folder='checkpoint',
                                        early_stop=5,
                                        verbosity=2)

    pipeline_optimizer.fit(X_train, y_train)
    score = pipeline_optimizer.score(X_test, y_test)
    print('Score on {0} generations: {1} population: {2}'.format(g, p, score))
    pipeline_optimizer.export('exported_pipeline_' + str(g) + '_' + str(p) + '_precision.py')

if __name__ == ""__main__"":
  
    print('Operating system version....', platform.platform())
    print(""Python version is........... %s.%s.%s"" % sys.version_info[:3])
    print('pandas version is...........', pd.__version__)
    print('numpy version is............', np.__version__)
    import tpot
    print('tpot version is.............', tpot.__version__)

    my_timer = Timer()

    Main(100, 100)

    elapsed = my_timer.get_time()
    print(""\nTotal compute time was: %s"" % elapsed)

```

Here is the log of the run:

```
Operating system version.... Windows-10-10.0.17134-SP0
Python version is........... 3.6.6
pandas version is........... 0.23.4
numpy version is............ 1.15.4
tpot version is............. 0.9.5
Model dataset:
        BoxRatio  Thrust  Velocity  OnBalRun  vwapGain  Altitude
10324     0.426  -0.290     0.863     2.033     0.434         0
7865      0.339  -0.274     0.507     1.793     0.518         0
8293      0.735   0.062     0.507     1.173     0.425         0
2297      0.117  -0.213     0.913     1.597     0.757         0
9332      0.667  -0.193     1.100     1.921     0.425         0

Description of model dataset:
            BoxRatio        Thrust      Velocity      OnBalRun      vwapGain
count  17673.000000  17673.000000  17673.000000  17673.000000  17673.000000
mean       1.235467      0.527925      1.082293      2.626984      0.785762
std        2.877679      2.905166      0.946045      2.322253      1.077768
min        0.000000     -1.000000     -0.373000     -0.570000     -1.006000
25%        0.239000     -0.370000      0.573000      1.428000      0.437000
50%        0.551000     -0.134000      0.825000      1.999000      0.498000
75%        1.220000      0.443000      1.234000      2.968000      0.711000
max       75.629000     90.786000     14.764000     39.942000     23.067000
Size of dataset:
 train shape...  (12371, 5) (12371,)
 test shape....  (5302, 5) (5302,)
Generation 1 - Current best internal CV score: 0.6839717536884107
Generation 2 - Current best internal CV score: 0.6881289056542735
Generation 3 - Current best internal CV score: 0.7041308486113744
Generation 4 - Current best internal CV score: 0.7041308486113744
Generation 5 - Current best internal CV score: 0.7060775600016976
Generation 6 - Current best internal CV score: 0.7060775600016976
Generation 7 - Current best internal CV score: 0.7060775600016976
Generation 8 - Current best internal CV score: 0.7060775600016976
Generation 9 - Current best internal CV score: 0.7060775600016976
Generation 10 - Current best internal CV score: 0.7060775600016976

The optimized pipeline was not improved after evaluating 5 more generations. Will end the optimization process.

TPOT closed prematurely. Will use the current best pipeline.

Best pipeline: BernoulliNB(StandardScaler(RandomForestClassifier(input_matrix, 
                                                                 bootstrap=True, 
                                                                 criterion=entropy, 
                                                                 max_features=0.1, 
                                                                 min_samples_leaf=12,
                                                                 min_samples_split=15,
                                                                 n_estimators=100)),
                                                                 alpha=0.01, 
                                                                 fit_prior=False)
Score on 100 generations: 100 population: 0.7210688088614798

Total compute time was: 01:28:39
```
When I put the new exported pipeline into notebook.ipynb, I get:

**Current version (TPOT 0.9.5),  Test dataset**
accuracy...................................... 0.82370821
precision..................................... 0.81469341
recall............................................. 0.84432454
misclassification rate.............. 0.17629179
F1................................................... 0.82924436
r2...................................................  0.29469495
AUC..............................................  0.82341587
mse...............................................  0.17629179
logloss.........................................  6.08898068 

**Previous version (TPOT 0.9.4),  Test datase**t
accuracy......................................  0.93900709
precision.....................................  0.91998469
recall.............................................  0.96312625
misclassification rate..............  0.06099291
F1...................................................  0.94106129
r2....................................................  0.75599806
AUC...............................................  0.93873526
mse................................................  0.06099291
logloss..........................................  2.10665427

Charles",made run script import platform import import import import time import counter import smote import import import import import import shuffle class timer self restart self self end end return shuffle header model header print print model model model model print print train shape print test shape return main score print population score print system version print python version print version print version import print version timer main print compute time log run operating system version python version version version version model thrust velocity altitude description model thrust velocity count mean min size train shape test shape generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score pipeline end optimization process closed prematurely use current best pipeline best pipeline score population total compute time put new pipeline get current version test accuracy precision recall misclassification rate previous version test accuracy precision recall misclassification rate,issue,positive,positive,positive,positive,positive,positive
464135543,"Hmm, I think there was a related old issue #11. There is a way to convert discussion trees to JSON (see this [link](https://www.garysieling.com/blog/convert-scikit-learn-decision-trees-json)) and it may be a good start point.",think related old issue way convert discussion see link may good start point,issue,negative,positive,positive,positive,positive,positive
464129197,"@CBrauer I saw the part of using SMOTE to oversample the dataset for generating balance dataset. But I think oversampling may cause the issue of overfitting here. So I suggest to use ""balanced_accuracy"" on the original dataset **without using SMOTE**",saw part smote generating balance think may cause issue suggest use original without smote,issue,negative,positive,positive,positive,positive,positive
464127329,"I will add this and run the script again.  However, SMOTE balances the dataset, so I'm wondering why this is needed.  (See notebooks)",add run script however smote wondering see,issue,negative,neutral,neutral,neutral,neutral,neutral
464121786,I think the oversampling for this imbalance dataset may cause the overfitting here. Could you please try `scoring='balanced_accuracy'` on dataset without oversampling? I will look into it more next week.,think imbalance may cause could please try without look next week,issue,negative,neutral,neutral,neutral,neutral,neutral
462772499,You could try the `warm_start` parameter in [TPOT API] with generations=1 to check the test performance every generation. ,could try parameter check test performance every generation,issue,negative,neutral,neutral,neutral,neutral,neutral
462771036,"The default value of `offspring_size` is None. In this case, the `offspring_size`  is equal to `population_size` in genetic programming., ",default value none case equal genetic,issue,negative,neutral,neutral,neutral,neutral,neutral
462151203,"May I suggest using pd.isna instead of np.isnan? I'm having a similar issue with the TPOTClassifier where I am passing in a pandas DataFrame consisting of columns with numpy.int64 and numpy.bool entries and no NaNs, but I still get the Numpy TypeError above. Changing the call to pd.isna fixes the issue.

Even appending .values to the end of my DataFrame when passing it to tpot.fit() doesn't work for me, and there are no strings in the data, just int and bool. ",may suggest instead similar issue passing still get call issue even end passing work data bool,issue,negative,neutral,neutral,neutral,neutral,neutral
462148628,"I'm facing the same problem - running on alpine 3.9, with 8GB mem. (which should suffice given the data); n_jobs=1; any new insights?",facing problem running alpine mem suffice given data new,issue,negative,positive,positive,positive,positive,positive
461588688,I think `StandardScaler()` or `RobustScaler()` may avoid the warning but the pipeline may not work very well on the normalized data since the pipeline was evaluated on the raw data.,think may avoid warning pipeline may work well data since pipeline raw data,issue,negative,negative,negative,negative,negative,negative
461587151,I'm nervous about suppressing warnings.  Should I trust the results anyway? One advice I got from the Web was to normalize the data.  Can I simply put “RobustScaler()” as the first line to the pipe?,nervous trust anyway one advice got web normalize data simply put first line pipe,issue,negative,positive,positive,positive,positive,positive
461582278,"```python
import warnings
with warnings.catch_warnings():
    warnings.simplefilter('ignore')
    exported_pipeline.fit(X_train, y_train)
```
Using `warnings` module may help.
",python import module may help,issue,negative,neutral,neutral,neutral,neutral,neutral
461412083,There is no easy way in TPOT to export the prediction targets of all evaluated pipelines since TPOT won't generate them during optimization. The fitness score in TPOT is average CV score with spliting training set via (Stratified)KFold,easy way export prediction since wo generate optimization fitness score average score training set via stratified,issue,positive,positive,positive,positive,positive,positive
461345218,With what classes could I mess to export the prediction targets? What module should I look into?,class could mess export prediction module look,issue,negative,negative,negative,negative,negative,negative
461018563,"`evaluated_individuals_` attribute (see [TPOT API](https://epistasislab.github.io/tpot/api/)) stores some statistics, like average CV score, of all evaluated pipelines, but the prediction target for X is not stored.",attribute see statistic like average score prediction target,issue,negative,negative,negative,negative,negative,negative
460679306,"If you're running TPOT with `n_jobs=-1`, isn't it using all of the system resources? So you'll want to wait until the first TPOT run finishes before starting the next.

Regardless, I think the existing best solution for this is to rewrite your above code as a function and call the function in separate terminals, e.g.,

```python
from sklearn.externals import joblib
import distributed.joblib
from dask.distributed import Client

def run_tpot(X_train, Y_train):
    # connect to the cluster
    client = Client('schedueler-address')

    # create the estimator normally
    estimator = TPOTClassifier(n_jobs=-1)

    # perform the fit in this context manager
    with joblib.parallel_backend(""dask""):
        estimator.fit(X_train, Y_train)
    return estimator
```

then

terminal 1:

```python
# import/data setup
...
run_tpot(X_train_1, Y_train_1)
```

terminal 2:

```python
# import/data setup
...
run_tpot(X_train_2, Y_train_2)
```

etc.",running system want wait first run starting next regardless think best solution rewrite code function call function separate python import import import client connect cluster client client create estimator normally estimator perform fit context manager return estimator terminal python setup terminal python setup,issue,positive,positive,positive,positive,positive,positive
460400963,"But wouldn't this just calculate a mean accuracy from the two datasets? I want each dataset to represent an independent estimator. So, I want to see how well each type of classifier performs on different datasets (they would have different features).",would calculate mean accuracy two want represent independent estimator want see well type classifier different would different,issue,negative,negative,neutral,neutral,negative,negative
460276433,It is possible. You could merge all the training set with the same number of features. Then using 'cv' parameter to set the customized train/test splits to evaluate the model on multiple training sets.  ,possible could merge training set number parameter set evaluate model multiple training,issue,negative,neutral,neutral,neutral,neutral,neutral
460273873,"I think the `scoring_function` should be updated once the fit() function was called unless it should remind the same as last fit() call. Also `scoring_function` should be internal attribute of the TPOTRegressor but the `scoring` is the parameter of TPOTRegressor. 

If the scoring_function was manually changed to ""r2"" and those internal CV scores from last fit() call won't changed unless rerun fit() again. 
",think fit function unless remind last fit call also internal attribute scoring parameter manually internal last fit call wo unless rerun fit,issue,positive,positive,positive,positive,positive,positive
459947373,"What does a bdbQuit indicate? Alternatively, how do you define a sparse feature?",indicate alternatively define sparse feature,issue,negative,neutral,neutral,neutral,neutral,neutral
458143592,"[`sklearn.compose.TransformedTargetRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html) is an example of how this may work as a meta-estimator compliant with the sklearn API; another option might be [Pipegraph](https://github.com/mcasl/PipeGraph) with a `TPOTRegressor`'s `Y` input/output connected to a separate transformer ""node."" I don't know if tpot classes are compatible with pipegraph, though; am planning on trying this out eventually.

AFAIK [seglearn](https://github.com/dmbee/seglearn) is the only sklearn-related project that extends `sklearn.pipeline.Pipeline` to explicitly allow transformers to modify targets without the use of meta-estimators, has its own custom `Pype` class that allows this. You might follow their schema to write a custom Transformer, but with the understanding that the resulting object class will not work in a vanilla Pipeline. This might not be what you want, depending on use case.",example may work compliant another option might connected separate transformer node know class compatible though trying eventually project explicitly allow modify without use custom class might follow schema write custom transformer understanding resulting object class work vanilla pipeline might want depending use case,issue,negative,neutral,neutral,neutral,neutral,neutral
457261271,"train_dfs[0]['y_portfolio'].portfolio has shape (842,_)
I will use train_dfs[0]['y_portfolio'].values.reshape(842, 1), which gives a (842,1) shape. Let's see if this solves the bug, which again, only kicks in in the last generation.
Thanks",shape use shape let see bug last generation thanks,issue,negative,positive,neutral,neutral,positive,positive
457203234,"`train_dfs[i]['y_portfolio']` seems a 1-D array, but . Could you print out the `train_dfs[i]['y_portfolio'].shape` to check it? If so, you may need try `train_dfs[i]['y_portfolio'].values.reshape(N_ROWS, 1)`.",array could print check may need try,issue,negative,neutral,neutral,neutral,neutral,neutral
456677164,"Thanks for answering. What if I want to focus on 'f1' or 'precision' of minority class in the unseen data, is it still meaningful for using 'f1' score for minority class in balanced training data or 'balanced_accuracy' in unbalanced data?",thanks want focus minority class unseen data still meaningful score minority class balanced training data unbalanced data,issue,positive,positive,positive,positive,positive,positive
456657185,"The former code ""from sklearn.crossvalidation import traintest_split"" has deprecated. The new one is below. I had the same problem, now it's gone.

""from sklearn.modelselection import traintest_split""",former code import new one problem gone import,issue,negative,positive,neutral,neutral,positive,positive
456560970,Maybe `scoring='balanced_accuracy'` can improve the problem. It is related to [this builtin metrics](https://github.com/EpistasisLab/tpot/blob/v0.9.5/tpot/metrics.py#L69)),maybe improve problem related metric,issue,negative,neutral,neutral,neutral,neutral,neutral
454036737,I think it is related to [this issue](https://github.com/scikit-learn/scikit-learn/issues/4143) and I think transforming both X and y is not supported in sklearn-API.,think related issue think transforming,issue,negative,neutral,neutral,neutral,neutral,neutral
451115193,"I guess no progress has been made? The difficulty here is that ImbLearn applies `fit` and `sample`, notice the latter is not `transform` as it does not change features (transformations), only the re-samples (hence sampling).

For this reason, ImbLearn provides its own `Pipeline` module, as it needs to wrap the `sample` functionality in a way that makes sense (it only samples on training and not on testing, etc) and is compatible with SciKit-Learn API flow.

Since most real-life data is highly unbalanced, I think ImbLearn compatibility is highly desired.
",guess progress made difficulty fit sample notice latter transform change hence sampling reason pipeline module need wrap sample functionality way sense training testing compatible flow since data highly unbalanced think compatibility highly desired,issue,positive,positive,positive,positive,positive,positive
451013082,"@ianozsvald Big thanks! I see your point. Although giving up the DataFrame is not a trivial issue, your suggestion should work to me. ^^",big thanks see point although giving trivial issue suggestion work,issue,positive,positive,neutral,neutral,positive,positive
450827815,"@chocone can you try with `training_set, test_set = train_test_split(df.values` instead of `df`, that way you build `DMatrix` using the underlying numpy object and not a DataFrame object? I had to use the numpy objects (with no DataFrames anywhere) to get rid of this problem.
Your situation is different to mine, but removing the DataFrame seems like a sensible first test. ",try instead way build underlying object object use anywhere get rid problem situation different mine removing like sensible first test,issue,negative,positive,positive,positive,positive,positive
450784998,"I am facing the mismatch problem when making a prediction with a trained XGB model, but not when training the model with the pandas dataframe. (My env: XGBoost 0.81, pandas 0.23.4, scikit-learn 0.20.2,  python 3.6, & a Mac anaconda setting) 

I am struggling with this problem a couple of days. Please help.

Here is my code:
---

df = prep_dataset()

training_set, test_set = train_test_split(df, test_size=0.30, random_state=1)
training_x = training_set[selected_x]
training_y = training_set[selected_y]
test_x = test_set[selected_x]
test_y = test_set[selected_y]

dtrain = xgb.DMatrix(training_x, label=training_y)
dtest = xgb.DMatrix(test_x, label=test_y)

param = {
    'max_depth': 3,
    'eta': 0.3,
    'silent': 1,
    'objective': 'multi:softprob', 
    'num_class': 3}  
num_round = 500 

bst = xgb.train(param, dtrain, num_round)
preds = bst.predict(dtest)
best_preds = np.asarray([np.argmax(line) for line in preds])
print(""Overall Precision Score = "", precision_score(test_y, best_preds, average='macro'))


So far there has been no problem.  Now here goes the mismatch problem part:
-------------------
**pred_single =  xgb.DMatrix(np.asmatrix([120, 1, 1, 31, 12, 7]))
best_preds = np.asarray([np.argmax(bst.predict(pred_single))])
print(""Single Point Prediction = "", best_preds )**

Then, it generates the ""ValueError: feature_names mismatch: ...."" problem.",facing mismatch problem making prediction trained model training model python mac anaconda setting struggling problem couple day please help code param param line line print overall precision score far problem go mismatch problem part print single point prediction mismatch problem,issue,negative,positive,neutral,neutral,positive,positive
449592925,"OK, I see. I'll add some preprocessing to convert all object data to floats.",see add convert object data,issue,negative,neutral,neutral,neutral,neutral,neutral
449402858,"Thanks @weixuanfu !  For purposes of transparency, explainability, and trust, it would be lovely to have the ability to connect TPOT to something like eli5 for feature importance inspection and exploration.  This may not be so important for biological work (I don't really know), but for public safety work, it's quite important to be able to be able to explain -- if only very roughly -- how a model works. ",thanks transparency trust would lovely ability connect something like feature importance inspection exploration may important biological work really know public safety work quite important able able explain roughly model work,issue,positive,positive,positive,positive,positive,positive
449024117,"OK, I see. I just found that there are **string** type values in the Kaggle raw data. I think transforming dataset via [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder) before using TPOT can be another solution. ",see found string type raw data think transforming via another solution,issue,negative,negative,negative,negative,negative,negative
449021955,"Hmm, I think those synthetic features should be in those first (left) columns but they usually had very high importance scores in the last operator of pipeline. 

For now, TPOT does not provide this option for disabling synthetic feature generation. But: 


> 
> One of my dev branch of TPOT called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `simple_pipeline`, which can disable both `StackingEstimator` and `CombineDFs` if `simple_pipeline=True` (e.g. `TPOTClassifier(simple_pipeline=True)`). But it is noted that this dev branch is not fully tested yet. If you want to try TPOT without `StackingEstimator` and `FeatureUnion`, you may install this branch in your test environment via the command below:
> 
> ```
> pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
> ```

Please check #152 for more details. We are working on a more advanced pipeline configuration option.  ",think synthetic first left usually high importance last operator pipeline provide option synthetic feature generation one dev branch option disable noted dev branch fully tested yet want try without may install branch test environment via command pip install upgrade please check working advanced pipeline configuration option,issue,positive,positive,positive,positive,positive,positive
448880416,"I run it with ""values"" and the result is the same:
```python
y_train = np.log(train['SalePrice'].values)
X_train = train.drop('SalePrice', axis=1).values
```
`TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''`",run result python train input could safely according casting rule safe,issue,positive,positive,positive,positive,positive,positive
448750365,"I'm using tpot and loving it, but am struggling to join the names of the features I provide tpot with the list of feature importances that I extract using ```tpot._fitted_pipeline.steps[-1][1].feature_importances_```.   I understand that this is because tpot is building and evaluating new synthetic features.  Do you have a recommended method for either or both of the following: (1) disabling synthetic feature generation so I can zip my feature names to the feature importances; or (2) appending names of the generated features to my list of feature names so I can zip them with feature importances?  Ideally, I'd like to be able to do something like this: 

```
for feature_name, feature_score in zip(df.drop('class', axis=1).columns, tpot._fitted_pipeline.steps[-1][1].feature_importances_):
    print(feature_name, '\t', feature_score)
```

Here's an example pipeline to which I would like to apply such a method: 

```
{'config_dict': {'sklearn.ensemble.RandomForestClassifier': {'n_estimators': [100], 'criterion': ['gini', 'entropy'], 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,
       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'bootstrap': [True, False]}, 'sklearn.tree.DecisionTreeClassifier': {'criterion': ['gini', 'entropy'], 'max_depth': range(1, 11), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21)}, 'sklearn.ensemble.ExtraTreesClassifier': {'n_estimators': [100], 'criterion': ['gini', 'entropy'], 'max_features': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,
       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'min_samples_split': range(2, 21), 'min_samples_leaf': range(1, 21), 'bootstrap': [True, False]}, 'sklearn.preprocessing.Binarizer': {'threshold': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,
       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}, 'sklearn.cluster.FeatureAgglomeration': {'linkage': ['ward', 'complete', 'average'], 'affinity': ['euclidean', 'l1', 'l2', 'manhattan', 'cosine']}, 'sklearn.preprocessing.MaxAbsScaler': {}, 'sklearn.preprocessing.MinMaxScaler': {}, 'sklearn.preprocessing.Normalizer': {'norm': ['l1', 'l2', 'max']}, 'sklearn.decomposition.PCA': {'svd_solver': ['randomized'], 'iterated_power': range(1, 11)}, 'sklearn.kernel_approximation.RBFSampler': {'gamma': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 ,
       0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ])}, 'sklearn.preprocessing.RobustScaler': {}, 'sklearn.preprocessing.StandardScaler': {}, 'tpot.builtins.ZeroCount': {}, 'sklearn.feature_selection.SelectFwe': {'alpha': array([0.   , 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008,
       0.009, 0.01 , 0.011, 0.012, 0.013, 0.014, 0.015, 0.016, 0.017,
       0.018, 0.019, 0.02 , 0.021, 0.022, 0.023, 0.024, 0.025, 0.026,
       0.027, 0.028, 0.029, 0.03 , 0.031, 0.032, 0.033, 0.034, 0.035,
       0.036, 0.037, 0.038, 0.039, 0.04 , 0.041, 0.042, 0.043, 0.044,
       0.045, 0.046, 0.047, 0.048, 0.049]), 'score_func': {'sklearn.feature_selection.f_classif': None}}, 'sklearn.feature_selection.SelectPercentile': {'percentile': range(1, 100), 'score_func': {'sklearn.feature_selection.f_classif': None}}, 'sklearn.feature_selection.VarianceThreshold': {'threshold': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]}}, 'crossover_rate': 0.1, 'cv': 5, 'disable_update_check': False, 'early_stop': None, 'generations': 10, 'max_eval_time_mins': 5, 'max_time_mins': None, 'memory': None, 'mutation_rate': 0.9, 'n_jobs': 7, 'offspring_size': 10, 'periodic_checkpoint_folder': None, 'population_size': 10, 'random_state': None, 'scoring': None, 'subsample': 1.0, 'verbosity': 2, 'warm_start': False}
```

Apologies if I missed this being addressed previously.",loving struggling join provide list feature extract understand building new synthetic method either following synthetic feature generation zip feature feature list feature zip feature ideally like able something like zip print example pipeline would like apply method array range range true false range range range array range range true false array range array array none range none false none none none none none none false previously,issue,positive,positive,neutral,neutral,positive,positive
448258056,"If you want only numerical features then it can be done the same way as XGBoost, which is already included.
If you want categorical features then you need to do a little bit more - you need to pass parameter with categorical feature indices to estimator creation or to the fit function.",want numerical done way already included want categorical need little bit need pas parameter categorical feature index estimator creation fit function,issue,positive,positive,positive,positive,positive,positive
448254389,"Hey Anna, it would be great to add in. Not sure if this solution works for catboost but someone else found  a way to add more operators not in the default config.

https://github.com/EpistasisLab/tpot/issues/407
",hey anna would great add sure solution work someone else found way add default,issue,positive,positive,positive,positive,positive,positive
447862724,The reason of the TypeError is that the input `X_train ` is a `pandas.DataFrame` and the current version of TPOT does not fully support `pandas.DataFrame` input.,reason input current version fully support input,issue,negative,neutral,neutral,neutral,neutral,neutral
446637515,"TPOT did not support this application so far, but you could try [scikit-deap](https://github.com/rsteca/sklearn-deap) for this purpose.",support application far could try purpose,issue,negative,positive,neutral,neutral,positive,positive
446533871,"@robertritz how does passing an array resolve this error?

The error is raised in [`_check_dataset()`](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1068-L1081) when it catches an `AssertionError` or `ValueError` from sklearn's `check_X_y()` function (see linked).

For 2-dimensional y, the function would require an additional argument `multi_output=True` to work as expected. Or am I missing something here?

Here's a partial trace:

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tpot/base.py in _check_dataset(self, features, target, sample_weight)
   1069             if target is not None:
-> 1070                 X, y = check_X_y(features, target, accept_sparse=True, dtype=np.float64)
   1071                 return X, y

/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    751     else:
--> 752         y = column_or_1d(y, warn=True)
    753         _assert_all_finite(y)

/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py in column_or_1d(y, warn)
    787 
--> 788     raise ValueError(""bad input shape {0}"".format(shape))
    789 
```",passing array resolve error error raised function see linked function would require additional argument work missing something partial trace recent call last self target target none target return order copy estimator else warn raise bad input shape shape,issue,negative,negative,negative,negative,negative,negative
445861391,I preferred to use `verbosity` to  control whether TPOT should raise this error message. If verbosity>2 then the ImportError should raise if it happened. I merged this PR to dev branch .,preferred use verbosity control whether raise error message verbosity raise dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
445844644,Thank you for this PR. Those python 2.7 builds failed somehow. I will look into it.,thank python somehow look,issue,negative,neutral,neutral,neutral,neutral,neutral
445376672,"Hey @KatherineKing   I had the same issue a while back and had to install via pip with a different name (below). Worked for me after. 

pip install pypiwin32

",hey issue back install via pip different name worked pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
445369247,"Thanks. I'm not successful at installing pywin32 using the conda prompt, so I think the issue is not with TPOT.",thanks successful prompt think issue,issue,positive,positive,positive,positive,positive,positive
445363580,"Ok, I think it is related to the issue #605. Please install pywin32 via conda.",think related issue please install via,issue,negative,neutral,neutral,neutral,neutral,neutral
445362508,"The Python Environments in Visual Studio, which I understand as pip.",python visual studio understand pip,issue,negative,neutral,neutral,neutral,neutral,neutral
445361785,"Hmm, how did you install it, via pip or conda? I think anaconda has pywin32 in default package list.",install via pip think anaconda default package list,issue,negative,neutral,neutral,neutral,neutral,neutral
445360485,"Even though I successfully installed pywin32, I still have this error.",even though successfully still error,issue,negative,positive,positive,positive,positive,positive
445043567,"Sounds like an interesting research project and relates to the 'age-layering' concepts that we briefly explored in the early days of TPOT. The feature would have to be thoroughly evaluated prior to implementation into the dev/master branch of TPOT, given its far-reaching consequence on the software.",like interesting research project briefly early day feature would thoroughly prior implementation branch given consequence,issue,positive,positive,positive,positive,positive,positive
444886345,"Hmm, thank you for posting this idea here. I should be easy to implement since TPOT can record [those statistics](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1145-L1151) during optimization. We will look into the paper for more details.",thank posting idea easy implement since record statistic optimization look paper,issue,positive,positive,positive,positive,positive,positive
444694704,Amazing! well keep me updated about its development!,amazing well keep development,issue,positive,positive,positive,positive,positive,positive
444566948,"I ran the same script with the same environment in a singularity container (running ubuntu 18.04) and also obtained consistent results. 

However, if I try it on an OSX 10.12.6, I get different results for every run. Apart from the underlying operating system, I am using the same package versions in both environments. 

My current environment looks like this:
```
pip3 --no-cache-dir install \
         numpy==1.15.1 \
         pandas==0.23.4 \
         scipy==1.1.0 \
         scikit-learn==0.19.2 \
         nibabel==2.3.0 \
         nilearn==0.4.2 \
         matplotlib==3.0.0 \
         ipython==7.0.1 \
         jupyter notebook==5.6.0 \
         TPOT==0.9.5 \
         bokeh==0.13.0 \
         dask==0.19.4 \
         distributed==1.23.3\
         dask-ml==0.10.0\
         dask-glm==0.1.0\
         joblib==0.12.5\
         tornado==5.1.1\
         seaborn==0.9.0""
```
I will just use the singularity environment in the future. ",ran script environment singularity container running also consistent however try get different every run apart underlying operating system package current environment like pip install use singularity environment future,issue,negative,positive,neutral,neutral,positive,positive
444499857,The best module is trained on the full dataset in `.fit()` function.,best module trained full function,issue,positive,positive,positive,positive,positive,positive
444499332,"One of our side projects is expanding TPOT framework to optimize neural networks. After that, I think we will go deeper.",one side expanding framework optimize neural think go,issue,negative,neutral,neutral,neutral,neutral,neutral
444461777,"as @timkofu is saying.. Aside from Tensorflow, is there any plan to exted the capability of TPOT to genetic automatic search of DNN architectures?
There are many recent papers that are beginning to explore this field, but to my knowlodge no open-source framework (like TPOT)",saying aside plan capability genetic automatic search many recent beginning explore field framework like,issue,negative,positive,positive,positive,positive,positive
444374833,"Right @weixuanfu. 
model.fitted_pipeline_ returns the best model.
Can you please confirm that on how much data is this best model trained on? Is it 75%?",right best model please confirm much data best model trained,issue,positive,positive,positive,positive,positive,positive
444205511,I was coming at it from the angle of MOEAs can be used to evolve near optimal DNN hyperparameters.,coming angle used evolve near optimal,issue,negative,positive,neutral,neutral,positive,positive
444111813,TPOT internally use [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) for evaluating pipeline via mean CV score. In default setting the CV split is 5-fold StratifiedKFold for classification task but KFold for regression task. ,internally use pipeline via mean score default setting split classification task regression task,issue,negative,negative,negative,negative,negative,negative
444110014,I think StratifiedShuffleSplit and default StratifiedKFold can avoid the overfitting issue in most of case. ,think default avoid issue case,issue,negative,neutral,neutral,neutral,neutral,neutral
444018966,"> 1. Yes, the TPOTClassifier will use StratifiedShuffleSplit instead of `StratifiedKFold` (default) when cv is specified as this CV splitter.
> 2. No need to add subsample parameter to split the data. `subsample` is just getting a subset of dataset before CV.

Can StratifiedShuffleSplit avoid the over-fitting to a certain extent? or I should add subsample if I need to do avoidance?

Besides, are there any other methods that can avoid the over-fitting in TPOT?",yes use instead default splitter need add subsample parameter split data subsample getting subset avoid certain extent add subsample need avoidance besides avoid,issue,negative,positive,positive,positive,positive,positive
443967589,Thanks! You just cleared my confusion! Thank you so much!,thanks confusion thank much,issue,negative,positive,positive,positive,positive,positive
443865847,"Thanks.

 

From: Weixuan Fu <notifications@github.com> 
Sent: Monday, December 3, 2018 11:41 AM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: Charles Brauer <CBrauer@CypressPoint.com>; Author <author@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] TPOT persistent problem. joblib.load does not agree with joblib.dump. (#814)

 

I don't think == is useful here to check if the two scikit-learn classes are identical.

You can check str(exported_pipeline.get_params()) == str(new_pipeline.get_params()) or str(exported_pipeline) == str(new_pipeline) instead.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/814#issuecomment-443842688> , or mute the thread <https://github.com/notifications/unsubscribe-auth/ABQZctDzL6jAWpXG77NCvgNc99UbcOcdks5u1X5NgaJpZM4Y9INv> .  <https://github.com/notifications/beacon/ABQZcghHYOm94xEqPV574j7fLLb_3dTKks5u1X5NgaJpZM4Y9INv.gif> 

",thanks fu sent author author subject persistent problem agree think useful check two class identical check instead thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
443842688,"I don't think `==` is useful here to check if the two scikit-learn classes are identical.

You can check `str(exported_pipeline.get_params()) == str(new_pipeline.get_params())` or `str(exported_pipeline) == str(new_pipeline)` instead.",think useful check two class identical check instead,issue,negative,positive,positive,positive,positive,positive
443833120,"1. Yes, the TPOTClassifier will use StratifiedShuffleSplit instead of `StratifiedKFold` (default) when cv is specified as this CV splitter.
2. No need to add subsample parameter to split the data. `subsample` is just getting a subset of dataset before CV.
",yes use instead default splitter need add subsample parameter split data subsample getting subset,issue,negative,neutral,neutral,neutral,neutral,neutral
442851317,"I just tested the demo in my environment, the best pipeline is reproduced with fixed random seed. Could you please let me know more details of the environment for reproducing this issue?
```
In [1]: from tpot import TPOTRegressor
   ...: from sklearn.datasets import load_boston
   ...: from sklearn.model_selection import train_test_split
   ...:
   ...: random_seed = 42
   ...: housing = load_boston()
   ...: X_train, X_test, y_train, y_test = \
   ...: train_test_split(housing.data, housing.target, train_size=0.75, test_size=0.25,
   ...:                 random_state=random_seed)
   ...: scoring = 'neg_mean_absolute_error'
   ...: for _ in range(3):
   ...:     tpot = TPOTRegressor(generations=30,
   ...:                      population_size=50,
   ...:                      verbosity=2,
   ...:                      random_state=random_seed,
   ...:                      config_dict='TPOT light',
   ...:                      scoring=scoring
   ...:                      )
   ...:     tpot.fit(X_train, y_train)
   ...:     print('Test score using optimal model: %f ' %tpot.score(X_test, y_test))
   ...:
Generation 1 - Current best internal CV score: -2.8928189002582902
Generation 2 - Current best internal CV score: -2.8928189002582902
Generation 3 - Current best internal CV score: -2.8928189002582902
Generation 4 - Current best internal CV score: -2.8928189002582902
Generation 5 - Current best internal CV score: -2.8928189002582902
Generation 6 - Current best internal CV score: -2.8928189002582902
Generation 7 - Current best internal CV score: -2.7588987549669652
Generation 8 - Current best internal CV score: -2.7588987549669652
Generation 9 - Current best internal CV score: -2.754552943044516
Generation 10 - Current best internal CV score: -2.754552943044516
Generation 11 - Current best internal CV score: -2.754552943044516
Generation 12 - Current best internal CV score: -2.713925278558578
Generation 13 - Current best internal CV score: -2.713925278558578
Generation 14 - Current best internal CV score: -2.713925278558578
Generation 15 - Current best internal CV score: -2.713925278558578
Generation 16 - Current best internal CV score: -2.713925278558578
Generation 17 - Current best internal CV score: -2.713925278558578
Generation 18 - Current best internal CV score: -2.713925278558578
Generation 19 - Current best internal CV score: -2.688096725715785
Generation 20 - Current best internal CV score: -2.688096725715785
Generation 21 - Current best internal CV score: -2.688096725715785
Generation 22 - Current best internal CV score: -2.688096725715785
Generation 23 - Current best internal CV score: -2.688096725715785
Generation 24 - Current best internal CV score: -2.6739259845515706
Generation 25 - Current best internal CV score: -2.644503760377815
Generation 26 - Current best internal CV score: -2.644503760377815
Generation 27 - Current best internal CV score: -2.644503760377815
Generation 28 - Current best internal CV score: -2.644419942090466
Generation 29 - Current best internal CV score: -2.644419942090466
Generation 30 - Current best internal CV score: -2.617657608544968

Best pipeline: ElasticNetCV(MaxAbsScaler(RobustScaler(DecisionTreeRegressor(SelectFwe(ElasticNetCV(LinearSVR(MaxAbsScaler(DecisionTreeRegressor(ElasticNetCV(input_matrix, l1_ratio=0.5, tol=0.01), max_depth=5, min_samples_leaf=2, min_samples_split=12)), C=20.0, dual=True, epsilon=0.1, loss=epsilon_insensitive, tol=0.0001), l1_ratio=0.5, tol=0.01), alpha=0.024), max_depth=5, min_samples_leaf=2, min_samples_split=12))), l1_ratio=0.1, tol=0.001)
Test score using optimal model: -2.321129
Generation 1 - Current best internal CV score: -2.8928189002582902
Generation 2 - Current best internal CV score: -2.8928189002582902
Generation 3 - Current best internal CV score: -2.8928189002582902
Generation 4 - Current best internal CV score: -2.8928189002582902
Generation 5 - Current best internal CV score: -2.8928189002582902
Generation 6 - Current best internal CV score: -2.8928189002582902
Generation 7 - Current best internal CV score: -2.7588987549669652
Generation 8 - Current best internal CV score: -2.7588987549669652
Generation 9 - Current best internal CV score: -2.754552943044516
Generation 10 - Current best internal CV score: -2.754552943044516
Generation 11 - Current best internal CV score: -2.754552943044516
Generation 12 - Current best internal CV score: -2.713925278558578
Generation 13 - Current best internal CV score: -2.713925278558578
Generation 14 - Current best internal CV score: -2.713925278558578
Generation 15 - Current best internal CV score: -2.713925278558578
Generation 16 - Current best internal CV score: -2.713925278558578
Generation 17 - Current best internal CV score: -2.713925278558578
Generation 18 - Current best internal CV score: -2.713925278558578
Generation 19 - Current best internal CV score: -2.688096725715785
Generation 20 - Current best internal CV score: -2.688096725715785
Generation 21 - Current best internal CV score: -2.688096725715785
Generation 22 - Current best internal CV score: -2.688096725715785
Generation 23 - Current best internal CV score: -2.688096725715785
Generation 24 - Current best internal CV score: -2.6739259845515706
Generation 25 - Current best internal CV score: -2.644503760377815
Generation 26 - Current best internal CV score: -2.644503760377815
Generation 27 - Current best internal CV score: -2.644503760377815
Generation 28 - Current best internal CV score: -2.644419942090466
Generation 29 - Current best internal CV score: -2.644419942090466
Generation 30 - Current best internal CV score: -2.617657608544968

Best pipeline: ElasticNetCV(MaxAbsScaler(RobustScaler(DecisionTreeRegressor(SelectFwe(ElasticNetCV(LinearSVR(MaxAbsScaler(DecisionTreeRegressor(ElasticNetCV(input_matrix, l1_ratio=0.5, tol=0.01), max_depth=5, min_samples_leaf=2, min_samples_split=12)), C=20.0, dual=True, epsilon=0.1, loss=epsilon_insensitive, tol=0.0001), l1_ratio=0.5, tol=0.01), alpha=0.024), max_depth=5, min_samples_leaf=2, min_samples_split=12))), l1_ratio=0.1, tol=0.001)
Test score using optimal model: -2.321129
Generation 1 - Current best internal CV score: -2.8928189002582902
Generation 2 - Current best internal CV score: -2.8928189002582902
Generation 3 - Current best internal CV score: -2.8928189002582902
Generation 4 - Current best internal CV score: -2.8928189002582902
Generation 5 - Current best internal CV score: -2.8928189002582902
Generation 6 - Current best internal CV score: -2.8928189002582902
Generation 7 - Current best internal CV score: -2.7588987549669652
Generation 8 - Current best internal CV score: -2.7588987549669652
Generation 9 - Current best internal CV score: -2.754552943044516
Generation 10 - Current best internal CV score: -2.754552943044516
Generation 11 - Current best internal CV score: -2.754552943044516
Generation 12 - Current best internal CV score: -2.713925278558578
Generation 13 - Current best internal CV score: -2.713925278558578
Generation 14 - Current best internal CV score: -2.713925278558578
Generation 15 - Current best internal CV score: -2.713925278558578
Generation 16 - Current best internal CV score: -2.713925278558578
Generation 17 - Current best internal CV score: -2.713925278558578
Generation 18 - Current best internal CV score: -2.713925278558578
Generation 19 - Current best internal CV score: -2.688096725715785
Generation 20 - Current best internal CV score: -2.688096725715785
Generation 21 - Current best internal CV score: -2.688096725715785
Generation 22 - Current best internal CV score: -2.688096725715785
Generation 23 - Current best internal CV score: -2.688096725715785
Generation 24 - Current best internal CV score: -2.6739259845515706
Generation 25 - Current best internal CV score: -2.644503760377815
Generation 26 - Current best internal CV score: -2.644503760377815
Generation 27 - Current best internal CV score: -2.644503760377815
Generation 28 - Current best internal CV score: -2.644419942090466
Generation 29 - Current best internal CV score: -2.644419942090466
Generation 30 - Current best internal CV score: -2.617657608544968

Best pipeline: ElasticNetCV(MaxAbsScaler(RobustScaler(DecisionTreeRegressor(SelectFwe(ElasticNetCV(LinearSVR(MaxAbsScaler(DecisionTreeRegressor(ElasticNetCV(input_matrix, l1_ratio=0.5, tol=0.01), max_depth=5, min_samples_leaf=2, min_samples_split=12)), C=20.0, dual=True, epsilon=0.1, loss=epsilon_insensitive, tol=0.0001), l1_ratio=0.5, tol=0.01), alpha=0.024), max_depth=5, min_samples_leaf=2, min_samples_split=12))), l1_ratio=0.1, tol=0.001)
Test score using optimal model: -2.321129
```",tested environment best pipeline fixed random seed could please let know environment issue import import import housing scoring range light print score optimal model generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline test score optimal model generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline test score optimal model generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline test score optimal model,issue,positive,positive,positive,positive,positive,positive
442839841,"You shouldn't use the testing score as the optimization criteria, as the algorithm will just overfit to the testing data. That is why the responses in this issue have focused on addressing the underlying issues that would cause the algorithm to overfit on the training data, such as CV scheme and data organization.

What happens if you fit a `RandomForestClassifier` to your training data? What's the 5-fold CV score, and what's the score on the testing set?",use testing score optimization criterion algorithm overfit testing data issue underlying would cause algorithm overfit training data scheme data organization fit training data score score testing set,issue,positive,positive,positive,positive,positive,positive
442830654,"Thanks for the mention! :) Differences will probably become bigger over time, but the documentation should stay up to date.",thanks mention probably become bigger time documentation stay date,issue,negative,positive,neutral,neutral,positive,positive
442724324,"Yap, since the train score, in this case, seems to overfit. It would be great to be able to choose which score to use: the train or the prediction.",yap since train score case overfit would great able choose score use train prediction,issue,positive,positive,positive,positive,positive,positive
442586475,"Hmm, good idea. but it should be a optional function.",good idea optional function,issue,negative,positive,positive,positive,positive,positive
442560740,"@PGijsbers it has been a while for this discussion. I really like the idea of layered TPOT and appreciate for your works, but for now, we don't have a near plan to merge this branch to master branch of TPOT. ",discussion really like idea layered appreciate work near plan merge branch master branch,issue,positive,positive,positive,positive,positive,positive
442550728,Thank you for this PR. Could you please rebase this PR to development branch? ,thank could please rebase development branch,issue,positive,neutral,neutral,neutral,neutral,neutral
442465558,Can you please clarify what you mean by 'score/quantify using the prediction score'? Do you mean use the testing score as the metric for optimization?,please clarify mean prediction score mean use testing score metric optimization,issue,positive,negative,negative,negative,negative,negative
442382212,"> For now you can pickle the `fitted_pipeline_` attribute, e.g `tpot_obj.fitted_pipeline_` for saving the fitted models. (see [TPOT API](https://epistasislab.github.io/tpot/api/))
> 
> It is a good idea about dumping the `pset` after optimization, which may make entire tpot class picklable.

like this?

pickle.dump(tpot_obj.fitted_pipeline_, 'to/my/path')",pickle attribute saving fitted see good idea dumping optimization may make entire class like,issue,positive,positive,positive,positive,positive,positive
442342222,"Yes, I randomize and stratify the train/test data to get an even portions of 0's and 1's.
I wish someone can answer the question: Is there a way to score/quantify using the prediction score?

",yes randomize stratify data get even wish someone answer question way prediction score,issue,positive,neutral,neutral,neutral,neutral,neutral
442316408,"Wow I completely misinterpreted the error message. The issue is not regarding my config at all, but the input data. I'm passing through a dataframe instead of an array. This is what happens when you don't get enough sleep. ",wow completely error message issue regarding input data passing instead array get enough sleep,issue,negative,positive,neutral,neutral,positive,positive
442290090,"Could you provide a bit more help with the custom configuration dictionary? I'm attempting to set up a simple custom configuration using the SelectFromModel example you gave. Here is my current config:

```
tpot_config = {
    'sklearn.multioutput.MultiOutputRegressor': {
        'estimator': {
            'sklearn.ensemble.ExtraTreesRegressor': {
                'n_estimators': [100],
                'max_features': np.arange(0.05, 1.01, 0.05)
            }
        }
    }
}
```

And here is my code to run TPOT:

```
pipeline_optimizer = TPOTRegressor(generations=5, population_size=20, max_time_mins=480, n_jobs=-1, verbosity=2, random_state=12345, config_dict=tpot_config)
pipeline_optimizer.fit(X_train, y_train)
print(pipeline_optimizer.score(X_test, y_test))
pipeline_optimizer.export('tpot_exported_pipeline.py')
```

I receive an error:
`ValueError: Error: Input data is not in a valid format. Please confirm that the input data is scikit-learn compatible. For example, the features must be a 2-D array and target labels must be a 1-D array.`

Is it necessary to specify the parameters to search for each algorithm? Before reading the documentation and your example I naively just passed through a list of algorithms like so:

```
tpot_config = {
    'sklearn.multioutput.MultiOutputRegressor': {
        'estimator': ['ExtraTreesRegressor']
      }
}
```

There are sklearn algorithms that are inherently multioutput, but with MultiOutputRegressor I get many more options. Thanks!",could provide bit help custom configuration dictionary set simple custom configuration example gave current code run print receive error error input data valid format please confirm input data compatible example must array target must necessary specify search algorithm reading documentation example naively list like inherently get many thanks,issue,positive,positive,neutral,neutral,positive,positive
442236822,"Is the data being shuffled? Are all the 0's and 1's grouped together in the dataset?

This smells like a data issue to me.",data grouped together like data issue,issue,negative,neutral,neutral,neutral,neutral,neutral
442015724,"I reduced CV to 5 and able to get perfect score (all 1's). Yet, prediction is not good. 

Is there a way to score/quantify using the prediction score?",reduced able get perfect score yet prediction good way prediction score,issue,positive,positive,positive,positive,positive,positive
441757124,"What about the features? How many / what kind of features?

15 CV folds is likely too many as well. That means each test fold is ~12 samples. With a dataset that size, I'd say go down to 5 folds in CV.",many kind likely many well test fold size say go,issue,positive,positive,positive,positive,positive,positive
441662501,"The FeatureUnion with identical FunctionTransformers in that pipeline means combining input matrix (related to this [function](https://github.com/EpistasisLab/tpot/blob/master/tpot/export_utils.py#L353-L375)). Since current TPOT randomly generates tree-based pipeline so some pipelines need this `FeatureUnion` operator to combine transformed features or raw input features from two branches for tree. But sometimes, like the case in this issue, the `FeatureUnion` just doubles feature spaces of raw input features.",identical pipeline combining input matrix related function since current randomly pipeline need operator combine raw input two tree sometimes like case issue feature raw input,issue,negative,negative,negative,negative,negative,negative
441657451,"Below is a example:
```python
from tpot import TPOTRegressor
from tpot.config import regressor_config_dict 
regressor_config_dict ['lightgbm.LGBMRegressor'] = {
'boosting_type': ['gbdt', 'dart'],
'min_child_samples': [1, 5, 7, 10, 15, 20, 35, 50, 100, 200, 500, 1000],
'num_leaves': [2, 4, 7, 10, 15, 20, 25, 30, 35, 40, 50, 65, 80, 100, 125, 150, 200, 250, 500], 'colsample_bytree': [0.7, 0.9, 1.0],
'subsample': [0.7, 0.9, 1.0],
'learning_rate': [0.01, 0.05, 0.1],
'n_estimators': [5, 20, 35, 50, 75, 100, 150, 200, 350, 500, 750, 1000, 1500, 2000]
}

tpot = TPOTRegressor(generations=100, population_size=300, config_dict=regressor_config_dict , verbosity=2)

```",example python import import,issue,negative,neutral,neutral,neutral,neutral,neutral
440939620,"To weixuanfu:

Thanks for your response. I found the answer. I used python 2 and 3 to compare the result. That's my fault.
:)
",thanks response found answer used python compare result fault,issue,negative,positive,positive,positive,positive,positive
440573480,"My training dataset is: 
1    102
0     96
As far as `subsample`, since the 0's/1's samples are about even I don't think it's an issue.
I'll try to increase the samples, play with the cv split parameters.
Is there a way to set the tpot scoring/quantifying based on prediction?
",training far subsample since even think issue try increase play split way set based prediction,issue,positive,positive,neutral,neutral,positive,positive
440525481,"Tell us more about your data. How big is your training dataset and testing dataset (rows, columns)?",tell u data big training testing,issue,negative,neutral,neutral,neutral,neutral,neutral
440460337,"Curious if there has been more discussion on this. I just hit ""Warning: sklearn.pipelines.FeatureUnion is not available and will not be used by TPOT."" ... and came to the party. ",curious discussion hit warning available used came party,issue,negative,positive,positive,positive,positive,positive
440419964,"Hmm, we didn't detect many overfitting cases in our tests before. Could you please provide a demo for reproducing this issue?

Usually I used `subsample=0.75` or specify train/test splits via `cv` parameter to deal with overfitting issue.",detect many could please provide issue usually used specify via parameter deal issue,issue,negative,positive,positive,positive,positive,positive
440417120,@DataCrane could you provide a demo to reproduce this issue?,could provide reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
440350896,"Hey @jheffez  , also what is your cv method? That could have impact.

In addition, I have been using pareto front optimized pipelines and find out in most cases they overfit. Usually i test pipelines that are 2 or 3 levels deep (as compared to 5-6+ levels) and can get better performing pipelines that dont overfit.

@weixuanfu  what would you suggest for a subsample ratio?",hey also method could impact addition front find overfit usually test deep get better dont overfit would suggest subsample ratio,issue,negative,positive,neutral,neutral,positive,positive
440340493,"For now you can pickle the `fitted_pipeline_` attribute, e.g `tpot_obj.fitted_pipeline_` for saving the fitted models. (see [TPOT API](https://epistasislab.github.io/tpot/api/))

It is a good idea about dumping the `pset` after optimization, which may make entire tpot class picklable. ",pickle attribute saving fitted see good idea dumping optimization may make entire class,issue,negative,positive,positive,positive,positive,positive
440338855,"Hmm, it seems overfitting. `subsample` parameter in [TPOT API](https://epistasislab.github.io/tpot/api/) may help.",subsample parameter may help,issue,negative,neutral,neutral,neutral,neutral,neutral
440182243,"That seems to fix it (still evaluating results, it finishes without errors).
Any chance to fix tpot?",fix still without chance fix,issue,negative,neutral,neutral,neutral,neutral,neutral
439916047,I think it is [related to this issue of using n_jobs](https://epistasislab.github.io/tpot/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux) and maybe [using dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) can be a good solution. ,think related issue maybe good solution,issue,positive,positive,positive,positive,positive,positive
439442664,"It depends. If 10% of whole dataset is bigger enough for validation, it should be OK.",whole bigger enough validation,issue,negative,positive,neutral,neutral,positive,positive
439355888,"So, it's safe to allocate most of the dataset to train and a small portion (~10%) to the final prediction/final verification, right?",safe allocate train small portion final verification right,issue,negative,positive,positive,positive,positive,positive
439256134,"> @OhMyGodness could you please try [Parallel Training with Dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) for this big dataset?

Yes,I have done some try using dask just like this demo  [**,but**](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) ,but find some other mistake in the script way on my aws
Linux instance.Could you give me detail in using dask by script way.",could please try parallel training big yes done try like find mistake script way give detail script way,issue,positive,neutral,neutral,neutral,neutral,neutral
439061879,"Sorry @OhMyGodness so far, TPOT does not support warm start in non-interactive shell.",sorry far support warm start shell,issue,positive,positive,neutral,neutral,positive,positive
439061506,@OhMyGodness could you please try [Parallel Training with Dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) for this big dataset?,could please try parallel training big,issue,negative,neutral,neutral,neutral,neutral,neutral
439060348,"Hmm, somehow the `timeout` argument is not working with dask. I will look into it.",somehow argument working look,issue,negative,neutral,neutral,neutral,neutral,neutral
439060064,"Though average CV scores of evaluated pipelines are used during optimization for prevent overfitting on training set, in our practices/examples, the test set was used for calculating holdout score to examine whether the overfitting happened.",though average used optimization prevent training set test set used calculating holdout score examine whether,issue,negative,negative,negative,negative,negative,negative
439056727,"TPOT uses pareto front hall of fame (`deap.tools.ParetoFront` ) and you may found it via `_pareto_front` attribute, e.g. fitted_tpot_obj._pareto_front. Or you may check `pareto_front_fitted_pipelines_` or `evaluated_individuals_` attributes mentioned in  [TPOT API](https://epistasislab.github.io/tpot/api/)",front hall fame may found via attribute may check,issue,negative,neutral,neutral,neutral,neutral,neutral
439006439,"> 
> 
> I think this is related to #508. Please try to run TPOT like this demo below:
> 
> ```
> import multiprocessing
> if __name__ == '__main__':
>     multiprocessing.set_start_method('forkserver')
>     # Note: need move import sklearn into main unless a RuntimeError (RuntimeError: context has already been set) will raise
>     from sklearn.datasets import make_classification
>     from tpot import TPOTClassifier
>     # your TPOT codes
>     pipeline_optimizer = TPOTClassifier()
>     pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, random_state=0, 
>                                                               verbosity=2,n_jobs = 10)
>     X_train = np.nan_to_num(X_train)
>     pipeline_optimizer.fit(X_train, dataY_train)
> ```
> 
> Please let me know if this way solve this issue.

I am just rewrite my code like this demo,but it still stuck at 0% for two days.
my data is 100w rows and 64 cols,are they too large to cause this problem?
I run my code in aws Linux with 16 cores and 120G RAMs and I set the n_jobs =10.",think related please try run like import note need move import main unless context already set raise import import please let know way solve issue rewrite code like still stuck two day data large cause problem run code set,issue,positive,positive,positive,positive,positive,positive
439006372,"> 
> You can use warm_start in a non-interactive shell by pickling the TPOT object at the end of the script, then in another script you can load that pickled TPOT object and start again. As Weixuan said though, warm_start is primarily intended for use in an interactive shell or notebook.
> On Thu, Sep 6, 2018 at 1:41 AM OhMyGodness ***@***.***> wrote: I want to know if you solve the warm start problem of TPOT.I am now encounter this problem,set the parameter warm_start = true,but it is useless.I want to know how can i go back to the point that last time runs, and continue my tpot.fit().Thank you! — You are receiving this because you modified the open/close state. Reply to this email directly, view it on GitHub <[#628 (comment)](https://github.com/EpistasisLab/tpot/issues/628#issuecomment-419013594)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABo7t28WeuY_5pxpR8ujKOC9MZNiy2Lkks5uYN-pgaJpZM4Qkwtn> .
> -- Cheers, Randal S. Olson, Ph.D. E-mail: rso@randalolson.com | Twitter: @randal_olson <https://twitter.com/randal_olson> http://www.randalolson.com

Sorry to bother you,but I can't find use warm start in non-interactive shell,which parameters can I use except warm_start=True.Thank you!",use shell object end script another script load object start said though primarily intended use interactive shell notebook wrote want know solve warm start problem encounter problem set parameter true want know go back point last time continue state reply directly view comment mute thread twitter sorry bother ca find use warm start shell use except,issue,negative,positive,positive,positive,positive,positive
438878218,"I tried moving to Ubuntu and I get a Segmentation fault.  I also get this on Mac OS X Sierra.
My Python 3 script is:
```
def LoadData():
    df = pd.read_csv(""data.csv"")
    response_name = ['Gain']
    feature_names = ['BoxRatio', 'Thrust', 'Velocity', 'OnBalRun', 'vwapGain']
    mask = feature_names + response_name
    model = df[mask]
    print('model:\n', model.head(5))
    print('\nDescription of model dataset:\n', model[feature_names].describe(include='all'))
    X_train, X_test, y_train, y_test = train_test_split(X,
                                                        y,
                                                       test_size = 0.2,
                                                       random_state = 7)
    return X_train, y_train, X_test, y_test

if __name__ == ""__main__"":
    import multiprocessing
    multiprocessing.set_start_method('forkserver')

    import pandas as pd
    import numpy as np
    import timeit
    from tpot import TPOTRegressor
    from sklearn.model_selection import train_test_split

    start_time = timeit.default_timer()

    X_train, y_train, X_test, y_test = LoadData()

    p = 100
    g = 100
    tpot = TPOTRegressor(generations=g,
                         population_size=p,
                         random_state=7,
                         verbosity=2)

    tpot.fit(X_train, y_train)
    print('\ntpot:\n', tpot) 

    score = tpot.score(X_test, y_test)   
    print('Score: {0}, generations: {1}, population: {2}'.format(score, g, p)) 

    tpot.export('exported_pipeline_' + str(g) + '_' + str(p) + '_regress.py')

    elapsed = timeit.default_timer() - start_time
    print(""\nTotal compute time was: %s"" % elapsed)
    print('Done.') 

```

My Bash screen looks like:
![bash](https://user-images.githubusercontent.com/1317234/48522967-5f9c8400-e82f-11e8-82e9-738a4e223f53.png)

Any suggestions would be greatly appreciated.
Charles
",tried moving get segmentation fault also get mac o sierra python script mask model mask print print model model return import import import import import import print score print population score print compute time print bash screen like bash would greatly,issue,negative,positive,positive,positive,positive,positive
438622373,"Sir Weixuan Fu , 

I think max_eval_time_mins(to evaluate single pipeline ) is not working when we are using dask . 
TPOT version 0.9.6",sir fu think evaluate single pipeline working version,issue,negative,negative,neutral,neutral,negative,negative
438322492,"The PR #798 was just merged with a patch. If you want to try the dev branch, the command below can be used for installing dev branch to your environment:
```
pip install --upgrade --no-deps --force-reinstall git+ https://github.com/EpistasisLab/tpot.git@development
```
",patch want try dev branch command used dev branch environment pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
438309439,The score is average CV score during optimization process. Related issue #691 ,score average score optimization process related issue,issue,negative,negative,neutral,neutral,negative,negative
436348358,"Well... I continued to increase memory from 1GB to 16GB and no cigar.
![dask](https://user-images.githubusercontent.com/1317234/48083675-a690d680-e1aa-11e8-9a7f-2aaa9b624611.png)
",well continued increase memory cigar,issue,positive,neutral,neutral,neutral,neutral,neutral
436320579,Sorry to see that. I saw only the memory limit is 1Gb. I think the issue is related to how dask handle out-of-memory error (see #779). I think increasing memory limit and larger memory will help.,sorry see saw memory limit think issue related handle error see think increasing memory limit memory help,issue,negative,negative,negative,negative,negative,negative
436318090,"I installed dask and ran my notebook.  It ran for a while and then blew up.  Here is a screen capture of the results. This is very discouraging.  
![dask](https://user-images.githubusercontent.com/1317234/48078419-42681580-e19e-11e8-80a7-a301ae94a441.png)
",ran notebook ran screen capture discouraging,issue,negative,neutral,neutral,neutral,neutral,neutral
436294790,"Hmm it seems a xgboost API issue. I tried to reproduce this issue via the demo below but the error didn't show up. I think I recently updated xgboost to 0.80 via `conda install -c anaconda py-xgboost`, maybe updating xgboost will help.

```python
from sklearn.metrics import make_scorer
from tpot import TPOTRegressor
import warnings
import pandas as pd
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
import math
warnings.filterwarnings('ignore')
housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25)
                                                    
def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    try:
        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    except:
        return float('inf')
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            return float('inf')
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5

tpot = TPOTRegressor(verbosity=3, scoring = rmsle_loss, generations = 50,population_size=50,offspring_size= 50,max_eval_time_mins=10,warm_start=True, use_dask=True)
tpot.fit(X_train,y_train)

```",issue tried reproduce issue via error show think recently via install anaconda maybe help python import import import import import import import math housing assert try enumerate except return float return float return sum scoring,issue,negative,neutral,neutral,neutral,neutral,neutral
436286500,"Hey Weixuan,

With the same exact setup, I am now getting the error below. Any idea? I am unable to get TPOT to finish a single run.

![image](https://user-images.githubusercontent.com/17535345/48073102-d7fda800-e192-11e8-9288-72087f7a2b7d.png)

conda create -n test_env python=3.6
activate test_env
pip install missingno
conda install -y -c anaconda ecos
conda install -y -c conda-forge lapack
conda install -y -c cvxgrp cvxpy
conda install -y -c cimcb fancyimpute
pip install rfpimp
conda install -y py-xgboost
pip install tpot msgpack dask[delayed] dask-ml


",hey exact setup getting error idea unable get finish single run image create activate pip install install anaconda install install install pip install install pip install,issue,negative,negative,negative,negative,negative,negative
436265937,"Could you please try one way mentioned in [""Parallel Training with Dask""](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask)? Please check the [installation guide](https://epistasislab.github.io/tpot/installing/) for installing dask.

",could please try one way parallel training please check installation guide,issue,positive,neutral,neutral,neutral,neutral,neutral
436104649,"Hi Weixuan,

 



 

I’m working on Windows 10.  Is there any hope?

 

Charles

 

From: Weixuan Fu <notifications@github.com> 
Sent: Monday, November 5, 2018 6:20 AM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: Charles Brauer <CBrauer@CypressPoint.com>; Author <author@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] Why is tpot sleeping? (#793)

 

I think it is related to this issue <https://epistasislab.github.io/tpot/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux> 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/793#issuecomment-435891641> , or mute the thread <https://github.com/notifications/unsubscribe-auth/ABQZciWIdRWLY_VjKwJ3oBBE6_O6sBUrks5usEkZgaJpZM4YNCnB> .  <https://github.com/notifications/beacon/ABQZclMw5X1h15lWDELghR5BmLyUUPFqks5usEkZgaJpZM4YNCnB.gif> 

",hi working hope fu sent author author subject sleeping think related issue thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
435122661,"Thanks a lot @weixuanfu .

Just a note, if someone wanders here to find a solution. you can get the estimator parameters as well as the entire estimator using.

temp = testp.get_params()
temp['stackingestimator__estimator']

This gives the following output

GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='quantile', max_depth=1,
             max_features=0.9500000000000001, max_leaf_nodes=None,
             min_impurity_decrease=0.0, min_impurity_split=None,
             min_samples_leaf=18, min_samples_split=17,
             min_weight_fraction_leaf=0.0, n_estimators=100,
             presort='auto', random_state=None,
             subsample=0.6500000000000001, verbose=0, warm_start=False)",thanks lot note someone find solution get estimator well entire estimator temp temp following output,issue,positive,positive,neutral,neutral,positive,positive
435057233,"You may print out all parameters via [`get_params`](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator.get_params), like `print(testp.get_params())`.",may print via like print,issue,negative,neutral,neutral,neutral,neutral,neutral
435055646,Thank you for the editing. It seems that the failed unit tests is due to the latest version of scikit-learn (0.20.0). I will fix them and merge this PR.,thank unit due latest version fix merge,issue,negative,positive,positive,positive,positive,positive
433478505,Also happening for me with a sparse dataset only (288176x28) and 128GB memory,also happening sparse memory,issue,negative,neutral,neutral,neutral,neutral,neutral
433135071,"I just downloaded data from Kaggle and had a test. It works in my PS.

Can you provide the versions of tpot and all its dependencies as well as a demo for reproducing this issue?
",data test work provide well issue,issue,negative,neutral,neutral,neutral,neutral,neutral
433103499,"> On tpot of this [jupyter notebook](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb), there is a [link](https://www.kaggle.com/c/titanic) to Kaggle dataset. The example should work with the data downloaded from the Kaggle.

When I use the Kaggle dataset this is the error I see:
ValueError: could not convert string to float: b'892,0'

I see a similar error with all the examples",notebook link example work data use error see could convert string float see similar error,issue,negative,neutral,neutral,neutral,neutral,neutral
432726501,"On tpot of this [jupyter notebook](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb), there is a [link](https://www.kaggle.com/c/titanic) to Kaggle dataset. The example should work with the data downloaded from the Kaggle.",notebook link example work data,issue,negative,neutral,neutral,neutral,neutral,neutral
432724776,"Can you please provide more details for reproducing this issue, like a demo?",please provide issue like,issue,positive,neutral,neutral,neutral,neutral,neutral
432723169,"TPOT will still use NSGA-II selection for 2-objective optimization (score and complexity) to sorting pipelines and then pass top 10% of pipeline to the next generation. I am not worried that pareto length will be too short if we penalize the # of variables. On the counter, if the simple pipeline with less intermediate features after feature selection step can get a similar accuracy score with a pipeline using all features. We are more interested to study those intermediate features. 
",still use selection optimization score complexity pas top pipeline next generation worried length short penalize counter simple pipeline le intermediate feature selection step get similar accuracy score pipeline interested study intermediate,issue,positive,positive,positive,positive,positive,positive
432720352,"here is an example from the titanic notebook that i tried but no data available makes it work

```
# %load tpot_titanic_pipeline.py
import numpy as np

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
test= 'data/submission.csv'
#test= 'data/titanic_test.csv'
#test= 'data/titanic_train.csv'

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv(test, delimiter='COLUMN_SEPARATOR', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=None)

exported_pipeline = RandomForestClassifier(bootstrap=False, max_features=0.4, min_samples_leaf=1, min_samples_split=9)

exported_pipeline.fit(training_features, training_classes)
results = exported_pipeline.predict(testing_features)
```",example titanic notebook tried data available work load import import import note make sure class data file test,issue,negative,positive,positive,positive,positive,positive
432579412,"I tried and it all got messed up so I gave up.
Clearly, tpot would benefit from an easier/cleaner integration with pip...

Update: Used the solution in the following link. []https://github.com/DEAP/deap/issues/240

",tried got gave clearly would benefit integration pip update used solution following link,issue,positive,positive,neutral,neutral,positive,positive
431614777,"Awesome thanks for the explanation. In terms of the top 10% of pipelines being passed on to the next generation, how would this penalty apply? Would you apply a multiplier to the loss score in this case or weight it by the complexity as to weed out those pipelines? 

For # of features, how would this method maintain the depth of the search if we penalize the # of variables? Some of my best pipelines have been the pareto length of 4 or 5. If we weight the # of predictors too much then TPOT may only be stuck exploring pareto lengths of 1 and 2. 

For the template idea you talked about, this would be a non-issue since you can set the template to do 1) Feature Transformation 2) Feature selection then 3) modelling and you should, in theory, get the best models with the least amount of predictors. 

Interested in hearing your thoughts.",awesome thanks explanation top next generation would penalty apply would apply multiplier loss score case weight complexity weed would method maintain depth search penalize best length weight much may stuck exploring template idea would since set template feature transformation feature selection theory get best least amount interested hearing,issue,positive,positive,positive,positive,positive,positive
431359729,"I think the complexity is `(179+180+181+182)/178  = 4.056`. The complexity of this kind of pipeline (without feature selection) is similar to the number of operators (5) used in the current version TPOT (v0.9.5), but there is a little bit penalty in those stacking estimators. If there is feature selector in the pipeline then the complexity should be lower due to reducing the number of features.",think complexity complexity kind pipeline without feature selection similar number used current version little bit penalty feature selector pipeline complexity lower due reducing number,issue,negative,positive,neutral,neutral,positive,positive
431355955,You can manually modify your environment variables. Check [this link](https://medium.com/@GalarnykMichael/install-python-on-windows-anaconda-c63c7c3d1444) for more details. Or you can build a new conda environment for installing TPOT and its dependencies. ,manually modify environment check link build new environment,issue,negative,positive,positive,positive,positive,positive
431327032,"After running the commands (above) I now get: ModuleNotFoundError: No module named 'deap'
It seems the python is not aware of deap anymore.

Running: `conda list|findstr deap`
shows that conda installed deap:
`deap                      1.2.2           py37h830ac7b_1000    conda-forge`

Is there a way to ""point/use' conda to the pip environment?",running get module python aware running way pip environment,issue,negative,positive,positive,positive,positive,positive
431040302,"Thanks. Can you help me illustrate with the pipeline below what you are suggesting? This is for my understanding on what you are suggesting.

In the example below I have 178 features feeding into this pipeline raw.

exported_pipeline = make_pipeline(
    StackingEstimator(estimator=RidgeCV()),  ** 1 feat**
    StackingEstimator(estimator=GradientBoostingRegressor(alpha=0.95, learning_rate=0.1, loss=""lad"", max_depth=3, max_features=0.15000000000000002, min_samples_leaf=20, min_samples_split=15, n_estimators=100, subsample=0.6500000000000001)), **2 feat **
    StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.8, tol=1e-05)), ** 3 feat **
    StackingEstimator(estimator=DecisionTreeRegressor(max_depth=5, min_samples_leaf=2, min_samples_split=9)), # 4 feat ** 4 feat **
    XGBRegressor(learning_rate=0.1, max_depth=6, min_child_weight=2, n_estimators=100, nthread=8, subsample=0.9500000000000001)  **#final model**
)

How are you thinking we should calculate this? Also what are your ideas on weighting the pipelines (or penalizing them) for the next generation? Taking the loss score and multiplying it by the ""# feature output from each operator/ # feature in input data"" weight?

Example

Operator 1: 179/178
Operator 2: 180/179 ( + prediction from first operator)
Operator 3: 181/180 ( + prediction from first 2 operators)
Operator 4:  182/181 ( raw + predictions from first 3 operators)

722/718 or 102% of the original features.

",thanks help illustrate pipeline suggesting understanding suggesting example feeding pipeline raw feat lad feat feat feat feat final model thinking calculate also weighting next generation taking loss score multiplying feature output feature input data weight example operator operator prediction first operator operator prediction first operator raw first original,issue,positive,positive,neutral,neutral,positive,positive
431013862,"Reinstalling `deap` via conda should solve this issue (see the commends below). Please let me know if it works or not. If it works, we will update the docs of installation guide.

```shell
pip uninstall deap
conda install -c conda-forge deap 
```",via solve issue see please let know work work update installation guide shell pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
431010339,I think it is a issue about installing the latest version of deap (v1.2.2) in Windows. Check this [related issue](https://github.com/DEAP/deap/issues/297) in deap repo.,think issue latest version check related issue,issue,negative,positive,positive,positive,positive,positive
431007979,I closed this issue due to no information. Please feel free to reopen this issue with details.,closed issue due information please feel free reopen issue,issue,positive,positive,neutral,neutral,positive,positive
431007340,I didn't see this issue before. Could you please provide more information about running environment and versions of TPOT and its dependencies for reproducing this issue? ,see issue could please provide information running environment issue,issue,negative,neutral,neutral,neutral,neutral,neutral
431006440,"The best way is to weight each of the elements in the stacked pipeline without refitting partial pipeline.

`VarianceThreshold` removes all features with a training-set variance lower than this threshold and the reason of overfitting maybe features' variances (or sample size) are different in testing set. I think `cross_eval_score` used in optimization of TPOT should prevent it with 5-fold CV by default but you may specify a set of train, test splits via `cv` for this issue. ",best way weight pipeline without partial pipeline variance lower threshold reason maybe sample size different testing set think used optimization prevent default may specify set train test via issue,issue,positive,positive,positive,positive,positive,positive
430804404,"Thanks Weixuan. So we will need to loop through each of the elements in the stacked pipeline and or just calculate it at the end?

One issue I ran into recently is that there was feature selection done on my pipeline which overfit my training set (it was SelectVariance threshold of 0.005 or something). How do we handle pipeline complexity where we don't necessarily have a proliferation of features but specific parameters are memorizing our training data set?
",thanks need loop pipeline calculate end one issue ran recently feature selection done pipeline overfit training set threshold something handle pipeline complexity necessarily proliferation specific training data set,issue,negative,positive,neutral,neutral,positive,positive
430738576,"@weixuanfu I'm having this issue too, I'm fairly certain I didn't run out of memory.",issue fairly certain run memory,issue,negative,positive,positive,positive,positive,positive
430670910,"@GinoWoz1 @adrose thank both of you for posting your ideas here. 

For the 1st idea that @GinoWoz1 mentioned, I understand that `PolynomialFeatures` will have larger penalty in pipeline complexity and actually in TPOT, there are some hard codes to limit the number of `PolynomialFeatures` <= 1 in pipelines because it can double features numbers and make the pipeline evaluation very computational expensive. 

But if there is one feature selection step to reducing feature number before `PolynomialFeatures` step in a pipeline, then it will not be a problem for using 1 or more `PolynomialFeatures` in the pipelines.

So we are thinking about using the `# feature output from each operator/ # feature in input data` to weight the pipeline complexity. In this situation, feature selectors should has less penalty and  `PolynomialFeatures` in the very beginning of pipeline should have more penalty.

Please let me know if you have more thoughts about this idea or other ones. Any thoughts will be helpful. Thanks again!",thank posting st idea understand penalty pipeline complexity actually hard limit number double make pipeline evaluation computational expensive one feature selection step reducing feature number step pipeline problem thinking feature output feature input data weight pipeline complexity situation feature le penalty beginning pipeline penalty please let know idea helpful thanks,issue,negative,negative,negative,negative,negative,negative
430659496,"Sorry should have elaborated more from the beginning.

My idea for looking at the covariance matrices would best apply for decomposition techniques that may have more than one ""best"" solution. Something like ICA may yield multiple best solutions. The idea of picking this up by looking for variance, across covariance matrices would be a simple solution to see the stability of the decomposition techniques.

However this would probably be better caught by the CV accuracy scores, so after spending more time thinking about this, it may not be a very good solution.

For my third idea, I thought looking at stability of importance metrics, within the CV framework TPOT already performs it's model building in, could yield some framework for seeing the stability of the model predictors on top of the prediction accuracy. I wouldn't expect many large jumps across internal CV importance metrics, but big jumps could point to some potential problems.

I think a complexity score could be used as a probability to weight a specific pipeline's offspring from spawning w/in the GP framework. As described in the [TPOT](https://link.springer.com/chapter/10.1007/978-3-319-31204-0_9#Sec3) paper, the top 10% of pipelines from generation 1 are placed into generation 2 in the presented example. We could compute a weighted average, or something of the like,  between our ""complexity score"", and the accuracy score, thus allowing more simple pipelines, which may not perform as well, an opportunity to propagate.

I am no expert in genetic programming, just a massive fan, I am not sure if an expert like @rhiever 
 or @weixuanfu would like to chime in on the feasibility and utility of implementing something like this.

LMK what is not clear.",sorry beginning idea looking covariance matrix would best apply decomposition may one best solution something like may yield multiple best idea looking variance across covariance matrix would simple solution see stability decomposition however would probably better caught accuracy spending time thinking may good solution third idea thought looking stability importance metric within framework already model building could yield framework seeing stability model top prediction accuracy would expect many large across internal importance metric big could point potential think complexity score could used probability weight specific pipeline offspring spawning framework paper top generation generation example could compute weighted average something like complexity score accuracy score thus simple may perform well opportunity propagate expert genetic massive fan sure expert like would like chime feasibility utility something like clear,issue,positive,positive,positive,positive,positive,positive
430462975,"Pushing this further.

Regarding complexity scoring, how would a complexity score be used for future generations development? Currently there is the operator count which comes out through the Pareto scores. Would something akin to an additional penalty to the loss score be a feasible idea to limit the generational development of overly complex pipelines or would this need to be approached another way? Curious to hear any thoughts. ",pushing regarding complexity scoring would complexity score used future development currently operator count come would something akin additional penalty loss score feasible idea limit generational development overly complex would need another way curious hear,issue,negative,negative,neutral,neutral,negative,negative
429996453,"Good point on number 1. On number 2, can you please elaborate? On number 3, are you talking about the CV fit on the training set, test set and then the second idea you are talking of? Interesting in what you are thinking.",good point number number please elaborate number talking fit training set test set second idea talking interesting thinking,issue,positive,positive,positive,positive,positive,positive
429948786,"I think one issue with solution 1. is that any time time you look for nonlinearity using `PolynomialFeatures` or something of the like, it would assume a large penalty. This is a common problem with the data I work with.

I think one potentially expensive solution would be to look at the stability of the covariance of predictors assuming solution #3 is implemented - but I am not sure the best way to fully tease this out?

Or looking at rank differences of importance metrics again to build off of #3?

I'm going to spend some more time thinking on this, I have run into similar problems with TPOT building very long complicated pipelines - these were just two potentially terrible ideas that came to mind.",think one issue solution time time look something like would assume large penalty common problem data work think one potentially expensive solution would look stability covariance assuming solution sure best way fully tease looking rank importance metric build going spend time thinking run similar building long complicated two potentially terrible came mind,issue,positive,negative,negative,negative,negative,negative
428022168,"So we've gotten this merged (some time ago) over on:

https://github.com/conda-forge/tpot-feedstock

Currently seeing some test fails in the rebuild of [0.9.5](https://github.com/conda-forge/tpot-feedstock/pull/11) that updates to point at EpistasisLab, perhaps due to some drift in the dependencies:
https://travis-ci.org/conda-forge/tpot-feedstock/jobs/438036423#L1101

I'm going to skip the offending tests:
```
nosetests tests --exclude test_StackingEstimator_4 --exclude test_score_3 --exclude test_sample_weight_func
```

But it might be worth taking a look!",gotten time ago currently seeing test rebuild point perhaps due drift going skip exclude exclude exclude might worth taking look,issue,negative,positive,neutral,neutral,positive,positive
427901894,@GinoWoz1 We'd like to invite you to contribute to this function. I will send you a email soon for scheduling a meeting for some discussions.,like invite contribute function send soon meeting,issue,negative,neutral,neutral,neutral,neutral,neutral
427885309,"Thanks please let me know where I can contribute, I am a full time student and doing a presentation next month on TPOT. I have taken a deep dive into the code for TPOT as well as DEAP to understand its inner working.
",thanks please let know contribute full time student presentation next month taken deep dive code well understand inner working,issue,positive,positive,positive,positive,positive,positive
427884657,@GinoWoz1 We are working and testing a new template function for TPOT (see [this branch](https://github.com/weixuanfu/tpot/tree/template_opt)) which may be similar to the grammar function you mentioned. I need discuss with other members in the TPOT team for this issue and  will keep you in touch. ,working testing new template function see branch may similar grammar function need discus team issue keep touch,issue,negative,positive,neutral,neutral,positive,positive
427877010,"1. Maybe we should just integrate some custom joblib code?
2. I'm not sure about what causes the issues with threads in the first place, but wouldn't multiprocessing.Process provide a timeout ability as well?",maybe integrate custom code sure first place would provide ability well,issue,positive,positive,positive,positive,positive,positive
427874116,"I don't remember the exact number but there were more than 8 generations.
`max_time_mins` was 60.

I don't know if I can reproduce this easily (private dataset).",remember exact number know reproduce easily private,issue,negative,positive,positive,positive,positive,positive
427874058,"It's quite likely timeouts sent by TPOT are not handled correctly right now on the dask backend. That should be possible with a little effort

It'll be a bit before I can look closely at the other errors.",quite likely sent handled correctly right possible little effort bit look closely,issue,negative,positive,neutral,neutral,positive,positive
427873872,"Maybe, I will look into it. But two issues need attentions when using timeout in joblib: 
1. TPOT uses `joblib` in `sklearn` to avoid adding one more dependency, so we need watch if scikit-learn updates the built-in joblib. 
2. this timeout in joblib only works when n_jobs !=1. We need a workaround for this. 
",maybe look two need avoid one dependency need watch work need,issue,negative,neutral,neutral,neutral,neutral,neutral
427872118,"Hmm, how many generations in each loop? how long is `max_time_mins`? I think the population won't change if one generation does not finish. ",many loop long think population wo change one generation finish,issue,negative,positive,positive,positive,positive,positive
427870433,"Oh, the timeout parameter of joblib.Parallel raises a timeout if any task lasts to long.

Would https://github.com/joblib/joblib/pull/366 allow for a more precise time control?",oh parameter task long would allow precise time control,issue,negative,positive,positive,positive,positive,positive
427868857,"I think the relevant code is there https://github.com/EpistasisLab/tpot/blob/507b45db01e8f88651f4ce8e03b607a5b50146f5/tpot/base.py#L1236-L1239

It seems you used the threading_timeoutable from stopit to handle the timeout. Why didn't you use instead the timeout parameter of [joblib.Parallel](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html)? ",think relevant code used handle use instead parameter,issue,negative,positive,positive,positive,positive,positive
427864522,"I saw the code for the population, but I experienced a strange behavior.

I used max_time_mins and a loop to restart a fit every hour, and observed absolutely no improvement (I was very far from a good score according to other runs) and killed it after a few hours.
Moreover, one estimator used to run for a very long time (see #780), and it had the same index every time, which made me think that the population didn't change.",saw code population experienced strange behavior used loop restart fit every hour absolutely improvement far good score according moreover one estimator used run long time see index every time made think population change,issue,positive,positive,positive,positive,positive,positive
427860864,"@weixuanfu I would like to contribute, I will open up a separate issue if this is ignored. All I am asking is what work has been done on this so I can help. I already had a in-depth call with Randy about the project and what aspects need to be worked on to move this forward. I am a masters student in data science who is possibly doing a thesis on this subject.",would like contribute open separate issue work done help already call randy project need worked move forward student data science possibly thesis subject,issue,positive,negative,neutral,neutral,negative,negative
427854779,"The GP trees in TPOT are strongly typed with [pre-select operators and their parameters](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) and those configurations determine search space of nodes and their leaves in the tree-structure. But for now, TPOT don't have a hyperparameter to control the  the height of produced trees.",strongly determine search space leaf control height produced,issue,negative,positive,positive,positive,positive,positive
427841340,"@louisabraham which version of dask you are using right now? I remember we had this kind of error handling issue before but I thought @TomAugspurger fixed it. 

Edit: I think that `MemoryError` or other out-of-resource errors may need another handler in dask.  ",version right remember kind error handling issue thought fixed edit think may need another handler,issue,negative,positive,positive,positive,positive,positive
427840037,"This is a old known issue for python.

The way CPython supports threading and asynchronous features has impacts on the accuracy of the timeout. For more background about this issue - that cannot be fixed - Please read Python gurus thoughts about Python threading, the GIL and context switching like these ones:

http://pymotw.com/2/threading/
https://wiki.python.org/moin/GlobalInterpreterLock

But I think it is a good idea to add this fit time into pipeline statistics. ",old known issue python way asynchronous accuracy background issue fixed please read python python context switching like think good idea add fit time pipeline statistic,issue,positive,positive,positive,positive,positive,positive
427836903,We need those dynamic classes to build strong type via deap (see [here](https://deap.readthedocs.io/en/master/examples/gp_spambase.html)) based on default/customized configuration of operators and parameters (see [here](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters)). ,need dynamic class build strong type via see based configuration see,issue,positive,positive,positive,positive,positive,positive
427835992,The individuals should be different when doing a warm_start since the initial population won't be reset. (see [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L602-L606)). TPOT need reset random state each time fit is called for reproducibility.,different since initial population wo reset see need reset random state time fit reproducibility,issue,negative,negative,neutral,neutral,negative,negative
427662006,"For use_dask, the problem probably comes from `dask_ml.model_selection._search.build_graph` because the argument `error_score=float('-inf')` was used but didn't seem to have an effect.

https://github.com/EpistasisLab/tpot/blob/507b45db01e8f88651f4ce8e03b607a5b50146f5/tpot/gp_deap.py#L443-L454

If one solved the problem with use_dask, it is probable that it will solve the dask joblib backend issue as well.",problem probably come argument used seem effect one problem probable solve issue well,issue,negative,neutral,neutral,neutral,neutral,neutral
427617568,"Unfortunately, it doesn't work either with the joblib backend.

I launched the following code:

    import pandas as pd
    import numpy as np

    X = pd.read_csv('X_train.csv').drop(columns=['id']).values
    y = pd.read_csv('y_train.csv').drop(columns=['id']).values[:, 0]

    from dask_jobqueue import LSFCluster
    cluster = LSFCluster(cores=2, memory='2GB', job_extra=['-R rusage[mem=2048,scratch=16000]'],
                        local_directory='$TMPDIR',
                        walltime='12:00')

    from sklearn.externals import joblib
    import distributed.joblib
    from dask.distributed import Client
    client = Client(cluster)

    cluster.scale(6)

    from tpot import TPOTRegressor

    reg = TPOTRegressor(max_time_mins=60*8, generations=50, population_size=30,
                        cv=5,
                        scoring='r2',
                        memory='auto', random_state=42, verbosity=10, n_jobs=-1)

    with joblib.parallel_backend(""dask.distributed""):
        reg.fit(X, y)

After some time, I got the same errors:

    Skipped pipeline #29 due to time out. Continuing to the next pipeline.
    _pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.
    _pre_test decorator: _random_mutation_operator: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
    _pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
    _pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.
    Skipped pipeline #49 due to time out. Continuing to the next pipeline.

    distributed.scheduler - ERROR - '74911161'
    Traceback (most recent call last):
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/distributed/scheduler.py"", line 1306, in add_worker
        plugin.add_worker(scheduler=self, worker=address)
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/dask_jobqueue/core.py"", line 62, in add_worker
        self.running_jobs[job_id] = self.pending_jobs.pop(job_id)
    KeyError: '74911161'
    distributed.scheduler - ERROR - '74911158'
    Traceback (most recent call last):
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/distributed/scheduler.py"", line 1306, in add_worker
        plugin.add_worker(scheduler=self, worker=address)
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/dask_jobqueue/core.py"", line 62, in add_worker
        self.running_jobs[job_id] = self.pending_jobs.pop(job_id)
    KeyError: '74911158'
    distributed.scheduler - ERROR - '74911160'
    Traceback (most recent call last):
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/distributed/scheduler.py"", line 1306, in add_worker
        plugin.add_worker(scheduler=self, worker=address)
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/dask_jobqueue/core.py"", line 62, in add_worker
        self.running_jobs[job_id] = self.pending_jobs.pop(job_id)
    KeyError: '74911160'
    distributed.scheduler - ERROR - '74911161'
    Traceback (most recent call last):
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/distributed/scheduler.py"", line 1712, in remove_worker
        plugin.remove_worker(scheduler=self, worker=address)
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/dask_jobqueue/core.py"", line 74, in remove_worker
        del self.running_jobs[job_id][name]
    KeyError: '74911161'
    distributed.scheduler - ERROR - '74911161'
    Traceback (most recent call last):
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/distributed/scheduler.py"", line 1306, in add_worker
        plugin.add_worker(scheduler=self, worker=address)
    File ""/cluster/home/abrahalo/.local/lib64/python3.6/site-packages/dask_jobqueue/core.py"", line 62, in add_worker
        self.running_jobs[job_id] = self.pending_jobs.pop(job_id)
    KeyError: '74911161'

and the cell failed without raising an error but early stopped.
The progress bar was displaying 63 iterations but len(reg.evaluated_individuals_) returned 57.
The errors were displayed as the memory usage of some workers exploded.
",unfortunately work either following code import import import cluster import import import client client client cluster import reg time got pipeline due time next pipeline decorator provided affinity ward work decorator got unexpected argument decorator unsupported set combination decorator found array feature minimum pipeline due time next pipeline error recent call last file line file line error recent call last file line file line error recent call last file line file line error recent call last file line file line name error recent call last file line file line cell without raising error early stopped progress bar returned displayed memory usage exploded,issue,negative,negative,neutral,neutral,negative,negative
427614465,"I am not sure, but the timeout handling with the dask backend should be worth testing as well.",sure handling worth testing well,issue,positive,positive,positive,positive,positive,positive
427459881,"Ha, thanks for catching this mistake.",ha thanks catching mistake,issue,negative,positive,positive,positive,positive,positive
427345407,"I agree that there is some data leakage, but I don't think data leakage
is the biggest problem. The biggest problem is **overfitting**.

Notes:

  - I'll illustrate my argumentation by considering a 5-fold
    cross-validation and explaining what happens when fitting on folds
    {2,3,4,5} and testing on fold 1. Of course, the final score is an
    average of the folds that are equivalent so there is no loss of
    generality.
  - I consider the following expressions equivalent: base and stacked
    model ; meta and ensemble model.

# General stacking definition

Stacking, as described in
<https://github.com/civisanalytics/civisml-extensions/blob/master/civismlext/stacking.py#L132>
is:

> Fit the base estimators on CV folds, then use their prediction on the
> validation folds to train the meta-estimator. Then re-fit base
> estimators on full training set.

The variation done by the [kaggle
post](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)
is to forget about the validation folds, which **is** acceptable to me.
This is plausible because for example the model trained on folds {2-5}
that transformes fold 1 just averages the *relationship* between
X\_{2-5} and y\_{2-5}, so the leakage on y\_{2-5} can be neglected.
Since the models are not trained end-to-end (like in a deep learning
fashion), they won't learn to exploit the leakage.

Note that there are three steps:

  - cross\_val\_predict the stacked estimators (each estimator is
    evaluated k times)
  - train the meta-estimator on the out-of-sample predictions
  - re-fit base estimators on whole dataset (each estimator is evaluated
    1 time)

# What I think you do

If there is any error in this part, please correct me.

I'm considering the following sample code:

    make_pipeline(
        [
            StackingEstimator(Model1()),
            StackingEstimator(Model2()),
            StackingEstimator(Model3()])),
            EnsembleModel()
        ]
    )

StackingEstimator is just a wrapper to make scikit-learn think that all
estimators but the last are transformers.

Now, there are only two steps on the pipeline.

**Cross-validation** This corresponds to the ""cross-validation"" of the
stacked estimators and the meta-estimator training. The
StackingEstimators will fit on folds {2-5}, then transform folds {1-5}
and the ensemble will learn on folds {2-5} to predict fold 1.

**Fit** Each StackingEstimator learns on the whole dataset, then makes
predictions on the whole dataset. Then, the Ensemble model is fitted
again on those predictions.

# Why I think it is bad

The problem is not in the architecture or the prediction method (I'm
totally fine with ""adding"" features sequentially), but in the training.

The model selection is done by a cross-validation (averaging some
train/test splits), and the problem is that **the stacked model is
trained without cross-validation**. The consequence is that you train
the meta model to select the most overfitting base model. This problem
happens during both the cross-validation and the fit phase. Doing a
cross-validation on underperforming models is not a critical issue if
all models are affected in the same way. The big problem is that the
final models are not good.

The cross-validation is already a bad thing as the ensemble will just
select the StackingEstimator that overfits the most, but it is quite
complicated to see how bad it is. Of course it will modify the
performance of the model (decrease it), but at this step, all that
counts is how the pipelines are ranked and I have no argument to prove
that this order will be modified. I think it is not even as bad as it
seems because it will really deplete the performance of the pipelines
that use StackingEstimators prone to overfitting.

The really bad thing is that it also happens when you want to fit the
whole model (final step). Basically, you lose the whole point of
stacking because the meta model is trained on the training predictions
of the stacked models, and not on out-of-sample predictions. That is,
you select the models that overfit the most but not the ones that
generalize well.

# Illustration of the overfitting problem

I made an example notebook, hosted on colab:
<https://colab.research.google.com/drive/1mLGhBcshGp1bUbbo4va2RGP-4j0LWV6v>

# Where the problem comes from (in the code)

[`StackingEstimator`](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/stacking_estimator.py#L31)
looks a lot like [this stackexchange
answer](https://stats.stackexchange.com/a/274147/211397), but improved a
lot the code quality.

However, the stacking process is not perfectly compatible with a
straightforward `fit` and `transform` API. When training a stacking
model, one must ensure that the intermediate models make prediction on
samples they were not trained on. Thus, it is not possible to modify
models to put them in a pipeline.

# What I propose to solve it

My solution only modifies the `Pipeline` object to:

  - allow multiple non-transformer objects (estimators)
  - modify `fit` function when there is more than one estimator

This will create two new objects:

  - `StackingPipeline`
  - `make_stacking_pipeline`

It is now important to distinguish the folds that are used in the
cross-validation process, and the folds that are used in the stacking.
They can be different.

**In the kaggle article, no cross-validation is done on the stacking
estimator.**

Here, I write K the number of stacking folds and S the number of
estimators.

My solution only requires fitting N = K\*(S-1) + S estimators.

**Algorithm**:

  - Divide the training dataset in K parts numbered from 1 to K
  - For **i** = 1 to S-1
      - For **j** = 1 to K
          - Fit an instance of estimator **i** on all parts different
            than **j**
          - Predict the part **j** with this estimator
      - Add the out-of-sample predictions of estimator **i** as a new
        column
  - Fit the S estimators on the whole training dataset, using the
    cross-val estimations that were computed at the previous step.

If you want to add cross-validation with K\_cv \> 1 folds, then you have
to fit K\_cv \* N estimators during the selection phase, and again N
estimators on the full training dataset.

However, I think that in this case, taking K\_cv = 1 is good enough, as
we can suppose the meta estimator has a relatively stable performance.

# Comparison with other algorithms

The stacking of the kaggle article can successfully be implemented with
a code like this:

    model = make_stacking_pipeline(
        make_union(KNeighborsClassifier(), SVC()),
        LogisticRegression()
    )

with K=5 and K\_cv=1

I think that

    model = make_stacking_pipeline(
        KNeighborsClassifier(),
        SVC(),
        LogisticRegression()
    )

would have similar (if not greater) performance, because data leakage is
not big. This approach would be equivalent to restacking like in
[StackNet](https://github.com/kaz-Anova/StackNet#restacking-mode) and
[wolpert](https://wolpert.readthedocs.io/en/latest/user_guide/intro.html#restacking,).

# Limitations

K depends on the `StackingPipeline` and K\_cv depends on the
cross-validation function, so there is no way to set K\_cv=1 when a
`StackingPipeline` is used. On the other hand, it is a good thing to
still have cross-validation if the meta model is prone to overfitting.

The two main problems are that:

  - K is a new parameter to set
  - the asymptotic complexity will be multiplied by K.

However, K can be restricted to reasonable values less than 4, if one
supposes that *how the models will be assembled* depends more on the
pipeline than on the quantity of data they were trained on.

Note that in most use cases, S is small so the asymptotic overload is
much less than K. Here is a table of the
overload:

| number of estimators: S | stacking folds: K | estimators: (K \* max(0, S-1) + S) | overload: (K \* max(0, S-1) + S) / S |
| ----------------------- | ----------------- | ---------------------------------- | ------------------------------------ |
| 1                       | 2                 | 1                                  | 1                                    |
| 2                       | 2                 | 4                                  | 2                                    |
| 3                       | 2                 | 7                                  | 2,333333333                          |
| 1                       | 3                 | 1                                  | 1                                    |
| 2                       | 3                 | 5                                  | 2,5                                  |
| 3                       | 3                 | 9                                  | 3                                    |
| 1                       | 4                 | 1                                  | 1                                    |
| 2                       | 4                 | 6                                  | 3                                    |
| 3                       | 4                 | 11                                 | 3,666666667                          |

Thus, in most cases, the overload remains acceptable (having more than 3
estimators is rare).

# Conclusion

Following the discussion on #457, I would be glad to hear feedback from
@rasbt about this ""multilevel"" restacking.

I would also like to have some feedback from the core maintainers as I
am willing to implement the above.
",agree data leakage think data leakage biggest problem biggest problem illustrate argumentation considering explaining fitting testing fold course final score average equivalent loss generality consider following equivalent base model meta ensemble model general definition fit base use prediction validation train base full training set variation done post forget validation acceptable plausible example model trained fold relationship leakage since trained like deep learning fashion wo learn exploit leakage note three estimator time train base whole estimator time think error part please correct considering following sample code model model model wrapper make think last two pipeline training fit transform ensemble learn predict fold fit whole whole ensemble model fitted think bad problem architecture prediction method totally fine sequentially training model selection done problem model trained without consequence train meta model select base model problem fit phase critical issue affected way big problem final good already bad thing ensemble select quite complicated see bad course modify performance model decrease step ranked argument prove order think even bad really deplete performance use prone really bad thing also want fit whole model final step basically lose whole point meta model trained training select overfit generalize well illustration problem made example notebook problem come code lot like answer lot code quality however process perfectly compatible straightforward fit transform training model one must ensure intermediate make prediction trained thus possible modify put pipeline propose solve solution pipeline object allow multiple modify fit function one estimator create two new important distinguish used process used different article done estimator write number number solution fitting algorithm divide training fit instance estimator different predict part estimator add estimator new column fit whole training previous step want add fit selection phase full training however think case taking good enough suppose meta estimator relatively stable performance comparison article successfully code like model think model would similar greater performance data leakage big approach would equivalent like function way set used hand good thing still meta model prone two main new parameter set asymptotic complexity however restricted reasonable le one pipeline quantity data trained note use small asymptotic overload much le table overload number overload thus overload remains acceptable rare conclusion following discussion would glad hear feedback would also like feedback core willing implement,issue,positive,positive,neutral,neutral,positive,positive
427103551,"Yeah, the colab issue with tqdm does not fit here.

The problem is that the normal text output is buggy (see screen).
",yeah issue fit problem normal text output buggy see screen,issue,negative,positive,positive,positive,positive,positive
427101418,"Hmm, I refined this part in #768 and this patch was merged to `development` branch. But I just checked it in Colab, the issue still existed. As mentioned above, `tqdm_notebook` somehow doesn't work in Colab. Maybe this should be a issue for tqdm.",refined part patch development branch checked issue still somehow work maybe issue,issue,negative,neutral,neutral,neutral,neutral,neutral
427095666,"I'm not sure about this, but should't the normal output still work?",sure normal output still work,issue,negative,positive,positive,positive,positive,positive
427061941,"Hmm, I am not sure what happened. It seems that we met a lot of similar issues when using TPOT on Spyder or in Windows OS. Could you please set a `random_state` for reproducing this issue? Or Could you please run TPOT in official python console?",sure met lot similar o could please set issue could please run official python console,issue,positive,positive,positive,positive,positive,positive
426687654,"Thanks so much.... It's working on ipython. 
Please, I want to ask if I can turn my result into a model and use it at API on Flask to build an application for my web? or I can only use the exported (tpot.export) for API?

Can I apply Feature Scaling and Feature Selection using PCA/LDA before training my data? ",thanks much working please want ask turn result model use flask build application web use apply feature scaling feature selection training data,issue,positive,positive,positive,positive,positive,positive
426683631,"@weixuanfu  @rhiever  , bumping this. Any idea? I can help brainstorm a high level grammar and help push this along.

I ran two tpot runs this week. 1 finished within 2 days with relatively good pipelines. Another ran for 4 days and was using crazy computing power going through 5 and 6 level models (double function transformer, one-hot encoder for data already set with dummies etc).
",bumping idea help high level grammar help push along ran two week finished within day relatively good another ran day crazy power going level double function transformer data already set,issue,positive,positive,neutral,neutral,positive,positive
426647952,Can you please try to run the example codes on python console or ipython console instead from Spyder? It seems a import error.,please try run example python console console instead import error,issue,negative,neutral,neutral,neutral,neutral,neutral
425910539,Could you please provide more details about this issue?,could please provide issue,issue,negative,neutral,neutral,neutral,neutral,neutral
425517892,"Hey @weixuanfu , @tonyfast , where did this end up? As a recent adopter of TPOT I am fascinated by these ideas and curious on the status. I am so-so in python but catching on fast - would be willing to allocate some time towards this endeavor. ",hey end recent adopter fascinated curious status python catching fast would willing allocate time towards endeavor,issue,positive,positive,positive,positive,positive,positive
424714121,"Hmm I should close this issue since those two operators were added to the current version of TPOT (v0.9.5)

You can change the configuration of built-in `OneHotEncoder` (see [default one here](https://github.com/EpistasisLab/tpot/blob/v0.9.5/tpot/config/classifier.py#L164-L168)) to specify the categorical columns. Below is a demo for update configuration of `OneHotEncoder` in tpot:

```python
from tpot.config import classifier_config_dict
# assume that col. 2-4 in input features are categorical_features
classifier_config_dict['tpot.builtins.OneHotEncoder']= {
        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],
        'sparse': [False],
        'threshold': [10]
        'categorical_features':  [[1,2,3]] 
}


tpot = TPOTClassifier(config_dict=classifier_config_dict)
``` 

Please see more details about [customizing TPOT's operators and parameters](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters)",close issue since two added current version change configuration see default one specify categorical update configuration python import assume input false please see,issue,negative,negative,negative,negative,negative,negative
424603886,"@weixuanfu is this issue still open?  it depends on issue #756?

Are there any work-around? e.g. specify the categorical columns via `categorical_features`?",issue still open issue specify categorical via,issue,negative,neutral,neutral,neutral,neutral,neutral
424348902,The current version of TPOT cannot specify this parameters in `fit` function. But it is possible to expend [`set_sample_weight`](https://github.com/EpistasisLab/tpot/blob/v0.9.5/tpot/operator_utils.py#L81-L106) function to let TPOT `fit` function accept other additional keywords arguments as TPOT accepts `sample_weight`. ,current version specify fit function possible expend function let fit function accept additional,issue,positive,positive,positive,positive,positive,positive
422825798,"Thanks Weixuan for all you do.

On Wed, Sep 19, 2018 at 6:30 AM Weixuan Fu <notifications@github.com> wrote:

> Thanks for your suggestions. we will add this function into next version
> of TPOT.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/769#issuecomment-422804181>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AQuRcRz4U4JzJBbUYOSa9HF_jCqUU-dLks5uckcPgaJpZM4Wu7cB>
> .
>
",thanks wed fu wrote thanks add function next version thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
422804181,Thanks for your suggestions. we will add this function into next version of TPOT.,thanks add function next version,issue,negative,positive,neutral,neutral,positive,positive
422569144,"It's possible... :-)

To your earlier question, we explored the engineering challenge of allowing users to *force* TPOT to explore pipelines following a certain pipeline format. In that case, you could (for example) force TPOT to use feature synthesis. That unfortunately proved to be a larger engineering effort than our team could handle, but it's still on our wish list if the programming time/funding ever became available for it.",possible question engineering challenge force explore following certain pipeline format case could example force use feature synthesis unfortunately proved engineering effort team could handle still wish list ever available,issue,negative,positive,neutral,neutral,positive,positive
422567346,"Thank you guys. I appreciate your responses.

Another way of putting it: “Hey Charlie, be thankful we didn’t need to do feature synthesis in your case”.
Keep up the good work, I love TPOT.

Charles
",thank appreciate another way hey thankful need feature synthesis case keep good work love,issue,positive,positive,positive,positive,positive,positive
422532623,"@CBrauer, one thing to clarify here is that TPOT uses a stochastic search algorithm. Thus, TPOT may discover the pipeline you discovered previously in one run and find an entirely different pipeline in another run. (Assuming there are multiple pipelines that can achieve similar CV performance.)

If the pipeline with the stacking estimator does indeed perform better than the simpler pipelines, then TPOT will discover it given enough run-time. Hence @weixuanfu's suggestions.",one thing clarify stochastic search algorithm thus may discover pipeline discovered previously one run find entirely different pipeline another run assuming multiple achieve similar performance pipeline estimator indeed perform better simpler discover given enough hence,issue,negative,positive,neutral,neutral,positive,positive
422473359,"Sorry for misunderstanding. We still have feature synthesis and it can be used if one step in a pipeline is an estimator (classifier/regressor) and is not the last step then we must add its guess as synthetic feature(s). And `StackingEstimator` is not in the default configuration but TPOT can use it no matter what includes in default configuration. (see [here](https://github.com/EpistasisLab/tpot/blob/master/tpot/export_utils.py#L320-L328)). You can check the `evaluated_individuals_` attribute of fitted `TPOTClassifier` or `TPOTRegressor`, some evaluated pipeline should include `StackingEstimator`. Below is a demo to check if `StackingEstimator` was used in evaluated pipelines. I tested this demo based on TPOT v0.9.5 

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
from tpot.export_utils import generate_pipeline_code, expr_to_tree
from deap import creator

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25, random_state=42)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,random_state=42)
tpot.fit(X_train, y_train)


for pipeline_string in sorted(tpot.evaluated_individuals_.keys()):
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
    sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot._pset), tpot.operators)
    if sklearn_pipeline_str.count('StackingEstimator'):
        print(sklearn_pipeline_str)
        print('evaluated_pipeline_scores: {}\n'.format(tpot.evaluated_individuals_[pipeline_string]))
```

In the Pareto fronts of TPOT, it assume that less-complex pipelines (with less operator numbers in a pipeline) will generalize better. So `StackingEstimator` may increase pipeline complexity but the overall performance (average CV scores) is not too competitive with some less complex pipeline, which may be the reason that TPOT didn't select it as best pipeline in the end. 
",sorry misunderstanding still feature synthesis used one step pipeline estimator last step must add guess synthetic feature default configuration use matter default configuration see check attribute fitted pipeline include check used tested based python import import import import import import creator iris sorted print print assume le operator pipeline generalize better may increase pipeline complexity overall performance average competitive le complex pipeline may reason select best pipeline end,issue,positive,positive,neutral,neutral,positive,positive
422458582,"I just completed a run with generations = population = 100.  No feature synthesis!  It looks like you guys are turning your back on feature synthesis, which is a axiom of autoML.

You didn't answer my question.  How can I tell TPOT to use StackingEstimator?  I cloned TPOT from GitHUb and I looked at ./tpot/tpot/config/classifier.py.  I assume this is the default configuration.  If so, why is there no StackingEstimator in this file?",run population feature synthesis like turning back feature synthesis axiom answer question tell use assume default configuration file,issue,negative,neutral,neutral,neutral,neutral,neutral
422423788,"Hmm, I am not sure, I tested it in my Macbook Pro on a simulation data with the same size. The error didn't happened.",sure tested pro simulation data size error,issue,negative,positive,positive,positive,positive,positive
422422750,"Hmm, the current version of default TPOT can only evaluate the pipeline that are randomly generated in genetics programming. But increasing population size or offspring size may generate more pipelines with `StackingEstimator`.",current version default evaluate pipeline randomly genetics increasing population size offspring size may generate,issue,negative,negative,negative,negative,negative,negative
422420888,"Please check the `cv` parameter in [TPOT API](https://epistasislab.github.io/tpot/api/). You can merge the validation set  with the trainset for fitting in `tpot_obj.fit(X, y)` and then specify train/test splits via an iterable (see [the example](https://stackoverflow.com/questions/27097330/how-to-customize-sklearn-cross-validation-iterator-by-indices) for `GridSearchCV`)",please check parameter merge validation set fitting specify via iterable see example,issue,negative,positive,positive,positive,positive,positive
422034923,"Thanks I did. Sorry for the bother, it looks like a user error on my side
with my virtual environment. Really hate to inconvenience you. I am going
to do a overview of TPOT soon for some individuals in my area at a meetup
so this will help greatly! I'll make sure to give a shout out to your and
your team.

Sincerely,
Justin

On Fri, Sep 14, 2018 at 8:11 AM Weixuan Fu <notifications@github.com> wrote:

> Hmm, did you also update rmsle_loss in your codes? Can you provide a
> random_state to reproduce the issue?
>
> def rmsle_loss(y_true, y_pred):
>     assert len(y_true) == len(y_pred)
>     try:
>         terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
>     except:
>         return float('inf')
>     if not (y_true >= 0).all() and not (y_pred >= 0).all():
>             return float('inf')
>     return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/764#issuecomment-421389884>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AQuRcbdaaedW_zFed3k6VD2AwKi7dBigks5ua8cTgaJpZM4WmMOo>
> .
>
",thanks sorry bother like user error side virtual environment really hate inconvenience going overview soon area help greatly make sure give shout team sincerely fu wrote also update provide reproduce issue assert try enumerate except return float return float return sum thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
422033630,"Hi Weixuan,

Thanks for the reply.  Is there a way to tell TPOT to use StackingEstimator? The new features generated better results for me.

Charles
",hi thanks reply way tell use new better,issue,positive,positive,positive,positive,positive,positive
422022703,"From v0.9.3 to v0.9.5, TPOT added/refined some operators in the default configuration although TPOT still has `StackingEstimator` for generating synthetic features. But the result may not be reproduced for 2 reasons: 

1. the default configuration is updated so that the initial population (of pipeline) should be changed
2. the `random_state` is `None` in the scripts, so the old result cannot be reproduced via a different random seed.

You can try to [customize the operators's configuration](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) to limit the search space of TPOT. In this case, you can only keep `ExtraTreesClassifier`, `Normalizer` and `Nystroem` from default configuration. But the fitted pipeline maybe the pipeline you had before or a new one with better accuracy. ",default configuration although still generating synthetic result may default configuration initial population pipeline none old result via different random seed try configuration limit search space case keep normalizer default configuration fitted pipeline maybe pipeline new one better accuracy,issue,negative,positive,neutral,neutral,positive,positive
421775407,"Use my loss function @miteshyadav  - it works fine for me. I actually used the same one you had but ran into issues.

```
def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    try:
        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    except:
        return float('inf')
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            return float('inf')
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5

rmsle_loss = make_scorer(rmsle_loss,greater_is_better=False)```
",use loss function work fine actually used one ran assert try enumerate except return float return float return sum,issue,negative,positive,positive,positive,positive,positive
421409761,"Thank you .

On Fri, Sep 14, 2018, 21:09 Weixuan Fu <notifications@github.com> wrote:

> No, TPOTRegressor can not provide coefficient of independent variable
> since the fitted pipeline may not include any model for estimating that.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/765#issuecomment-421398248>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AnWnuaYtD5T9Ln5HTUmSslp77RfdMLpWks5ua82egaJpZM4WopkR>
> .
>
",thank fu wrote provide coefficient independent variable since fitted pipeline may include model thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
421398248,"No, TPOTRegressor can not provide coefficient of independent variable since the fitted pipeline may not include any model for estimating that. ",provide coefficient independent variable since fitted pipeline may include model,issue,negative,neutral,neutral,neutral,neutral,neutral
421395401,"Hi Weixuan,
After installing updates to my operating system, and updates to Anaconda, I tried the test script again.  This time it worked.  All my other tpot scripts are also working fine.  Let's close this out because I cannot re-produce the problem. Must have been a gremlin.   Sorry for taking up you time.
Charles",hi operating system anaconda tried test script time worked also working fine let close problem must gremlin sorry taking time,issue,negative,negative,neutral,neutral,negative,negative
421389884,"Hmm, did you also update `rmsle_loss` in your codes? Can you please provide a `random_state` to reproduce the issue?

```
def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    try:
        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    except:
        return float('inf')
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            return float('inf')
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5
```",also update please provide reproduce issue assert try enumerate except return float return float return sum,issue,negative,neutral,neutral,neutral,neutral,neutral
421179563,"Hmm after the first generation, the same error came up in the virtual environment. Were you able to finish one generation and save a pipeline? I did exactly as you suggested with the virtual env.

![image](https://user-images.githubusercontent.com/17535345/45520533-9fcb8180-b76e-11e8-9f59-94a025fbd14e.png)
",first generation error came virtual environment able finish one generation save pipeline exactly virtual image,issue,negative,positive,positive,positive,positive,positive
421100546,"No, the error message aborts my test script.  I have tried other configurations that give an exported script that works o.k.  The pickle file aborts only with this file. 

 

 

From: Weixuan Fu <notifications@github.com> 
Sent: Thursday, September 13, 2018 10:26 AM
To: EpistasisLab/tpot <tpot@noreply.github.com>
Cc: Charles Brauer <CBrauer@CypressPoint.com>; Author <author@noreply.github.com>
Subject: Re: [EpistasisLab/tpot] Strange error when using a pre-trained model (#763)

 

Hmm, I am not sure what happened. Can the fitted pipeline that loaded from pickle file be used for the training data?

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub <https://github.com/EpistasisLab/tpot/issues/763#issuecomment-421086702> , or mute the thread <https://github.com/notifications/unsubscribe-auth/ABQZcptXU2Q5FGu-NGsRhnLqGLRooOCcks5uapUVgaJpZM4WiEvN> .  <https://github.com/notifications/beacon/ABQZcuZGRshJzMeHURvFDwSf9nzPpntQks5uapUVgaJpZM4WiEvN.gif> 

",error message test script tried give script work pickle file file fu sent author author subject strange error model sure fitted pipeline loaded pickle file used training data thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
421086702,"Hmm, I am not sure what happened. Can the fitted pipeline that loaded from pickle file be used for the training data?",sure fitted pipeline loaded pickle file used training data,issue,negative,positive,positive,positive,positive,positive
421085258,"Thanks, no problem. I can live without it for now just as long as the periodic checkpoints are being saved. You can close this. thanks again!",thanks problem live without long periodic saved close thanks,issue,negative,positive,positive,positive,positive,positive
421084480,"Hmm, I think progress bar should be not easy to catch with tons of warning messages when `dask=True` but it did show up in my test (as stdout below).

We need refine this warning message action when `dask=True`.
```
 **self._backend_args)
D:\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
D:\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
Generation 1 - Current best internal CV score: -5.969518794583038e-15
Optimization Progress:   4%|█▉                                                | 101/2550 [01:08<51:22,  1.26s/pipeline]
```",think progress bar easy catch warning show test need refine warning message action parallel setting parallel setting generation current best internal score optimization progress,issue,positive,positive,positive,positive,positive,positive
421078518,The progress bar doesnt show up.,progress bar doesnt show,issue,negative,neutral,neutral,neutral,neutral,neutral
421078080,"You're welcome. Do you mean no confirmation during installation of packages via conda? If so, the `-y` in the command is for this purpose. ",welcome mean confirmation installation via command purpose,issue,negative,positive,positive,positive,positive,positive
421071295,"Nevermind on the python script question. I was able to setup on my laptop.

Any idea why this install process breaks the verbosity argument? everything else seems to be working fine, thanks a ton for your help.

Sincerely,
Justin",python script question able setup idea install process verbosity argument everything else working fine thanks ton help sincerely,issue,positive,positive,positive,positive,positive,positive
421030409,"Thanks Weixuan. Quick question, how do I run the python script out of the conda environment? I am just used to opening up the script on my desktop and running it there.",thanks quick question run python script environment used opening script running,issue,negative,positive,positive,positive,positive,positive
421018077,"Hmm, I tested those codes under a fresh test conda environment and the error was not reproduced. But I used a easy way to install `fancyimpute` as the commands below. Could you please build a conda environment for a test?

```shell
conda create -n test_env python=3.6
activate test_env
pip install missingno
conda install -y -c anaconda ecos
conda install -y -c conda-forge lapack
conda install -y -c cvxgrp cvxpy
conda install -y -c cimcb fancyimpute
pip install rfpimp
conda install -y py-xgboost
pip install tpot msgpack dask[delayed] dask-ml
```

Another suggestion about the customized scorer in your codes. May it will be more stable if the function does not raise `ValueError` as the example below:

```python
def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    try:
        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    except:
        return float('inf')
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            return float('inf')
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5
```",tested fresh test environment error used easy way install could please build environment test shell create activate pip install install anaconda install install install pip install install pip install another suggestion scorer may stable function raise example python assert try enumerate except return float return float return sum,issue,positive,positive,positive,positive,positive,positive
420723145,"The 2nd way in tpot [docs](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) as below need import dask first but without setting `use_dask=True`.

```python
from sklearn.externals import joblib
import distributed.joblib
from dask.distributed import Client

# connect to the cluster
client = Client('schedueler-address')

# create the estimator normally
estimator = TPOTClassifier(n_jobs=-1)

# perform the fit in this context manager
with joblib.parallel_backend(""dask""):
    estimator.fit(X, y)```",way need import first without setting python import import import client connect cluster client client create estimator normally estimator perform fit context manager,issue,positive,positive,positive,positive,positive,positive
420718307,"Thanks. I will provide the code to reproduce.

For dask, do I have to import it into the python script before running tpot?",thanks provide code reproduce import python script running,issue,negative,positive,positive,positive,positive,positive
420713541,"OK, please provide codes to reproduce the issue when `use_dask=True`. Did you install dask via `pip install dask[delayed] dask-ml`? ",please provide reproduce issue install via pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
420698817,"Thanks for the help. pip installed xgboost is definitely the issue. Can you please close this @weixuanfu 
 and set it as a bug rather than a question? I think this is a pip install xgboost issue.

**Solution**: install xgboost via conda using 'conda install py-xgboost'. DO NOT install xgboost.

 I will open another issue regarding use_dask = True as it is still giving me the error ""A pipeline has not yet been optimized"".

Sincerely,
Justin",thanks help pip definitely issue please close set bug rather question think pip install issue solution install via install install open another issue regarding true still giving error pipeline yet sincerely,issue,positive,positive,positive,positive,positive,positive
420522281,"I tried with dask=True and got after ~3h of processing

Process finished with exit code 137 (interrupted by signal 9: SIGKILL)

I gave this code to colleague with Windows 10 and it computed without issues. Is it OSX thing?",tried got process finished exit code interrupted signal gave code colleague without thing,issue,negative,neutral,neutral,neutral,neutral,neutral
420370695,"Hey Randal, I did try but ran into some other side issues so ill test that again now in google cloud.

On a side note, Google cloud services also crashed on xgboost process via the pip install. 4 cores, 2.3 ghz Intel Xeion(R) (Hyper-threaded) CPU.

Just uninstalled the pip xgboost and installed py-xgboost via conda. I will update on results.
",hey try ran side ill test cloud side note cloud also process via pip install uninstalled pip via update,issue,negative,negative,negative,negative,negative,negative
420368478,"Have you tried installing xgboost via conda? This sounds like an xgboost
issue.

On Tue, Sep 11, 2018 at 11:05 AM Justin <notifications@github.com> wrote:

> I am testing this on google cloud services now.
>
> Aside from the XGBoost issue, when I pass a custom function with use_dask
> = True I get the error ""A Pipeline has not been optimized. Please call
> fit() first"". This is outside of my pc and laptop so I am confused on why
> this is showing up. Here are the exact steps I took for my installation.
>
>    1. Install Anaconda 3.6 for windows 64bit
>    2. pip Install missingno
>    3. pip install these .whl files manually (need to do so for
>    fancyimpute)
>    -ecos-2.0.5-cp36-cp36m-win_amd64.whl
>
>
>    - cvxpy-1.0.8-cp36-cp36m-win_amd64.whl
>
>
>    1. pip install fancimpute
>    2. pip install rfpimp (used for my custom functions import file)
>    3. pip install xgboost
>    4. pip install tpot
>
> This is all I am doing and I get the error above when use_dask = True.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/759#issuecomment-420365982>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t2uBlELbCjuukMX1B-ccxoEvPEWoks5uZ_srgaJpZM4WY4sD>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",tried via like issue tue wrote testing cloud aside issue pas custom function true get error pipeline please call fit first outside confused showing exact took installation install anaconda bit pip install pip install manually need pip install pip install used custom import file pip install pip install get error true reply directly view mute thread twitter,issue,positive,positive,positive,positive,positive,positive
420365982,"I am testing this on google cloud services now as it relates to the XGBoost performance problem. 

Aside from the XGBoost issue, when I pass a custom function with use_dask = True I get the error that we talked about before: ""A Pipeline has not been optimized. Please call fit() first"". This is outside of my pc and laptop so I am confused on why this is showing up. Here are the exact steps I took for my installation.

1) Install Anaconda 3.6 for windows 64bit 
2) pip Install missingno
3) pip install these .whl files manually (need to do so for fancyimpute)
  -ecos-2.0.5-cp36-cp36m-win_amd64.whl   
  -cvxpy-1.0.8-cp36-cp36m-win_amd64.whl
4) pip install fancimpute
5) pip install rfpimp (used for my custom functions import file)
6) pip install xgboost
7) pip install tpot

This is all I am doing and I get the error above when use_dask = True.",testing cloud performance problem aside issue pas custom function true get error pipeline please call fit first outside confused showing exact took install anaconda bit pip install pip install manually need pip install pip install used custom import file pip install pip install get error true,issue,negative,positive,positive,positive,positive,positive
420270374,"Hmm, that may need 3-4Gb memory with `n_jobs=4`. Could you please try `n_jobs=2` or [dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) for parallel training?",may need memory could please try parallel training,issue,negative,neutral,neutral,neutral,neutral,neutral
420150722,"Yep, same issue. XGBboost blowing up my cpu even on multiple threads. I am perplexed. Searching the internet, I have seen this happen on others PCs as well. 

What is your cpu performance with xgboost @weixuanfu ? Does it spike or go above 50%?

![image](https://user-images.githubusercontent.com/17535345/45339595-2fdbb200-b548-11e8-8656-7df471c058fc.png)


",yep issue blowing even multiple perplexed searching seen happen well performance spike go image,issue,negative,positive,positive,positive,positive,positive
420093544,"This seems to be a performance issue from xgboost I am failing on jupyter and command line too.  

I am attempting to feed in the config dict manually (from the source code) and inputing the n_threads = # of cores in my PC. Hoping this fixes it.


Version=1
EventType=APPCRASH
EventTime=131810952156935914
ReportType=2
Consent=1
UploadTime=131810952158132530
ReportIdentifier=126a1bfc-b550-11e8-bf1c-d43d7e1ff8bd
IntegratorReportIdentifier=126a1bfb-b550-11e8-bf1c-d43d7e1ff8bd
NsAppName=python.exe
Response.BucketId=643121521f9f9e1026b6e9f1972f3f6e
Response.BucketTable=4
Response.LegacyBucketId=1636752738428600174
Response.type=4
Sig[0].Name=Application Name
Sig[0].Value=python.exe
Sig[1].Name=Application Version
Sig[1].Value=3.6.5150.1013
Sig[2].Name=Application Timestamp
Sig[2].Value=5abd3210
**_Sig[3].Name=Fault Module Name
Sig[3].Value=xgboost.dll_**
Sig[4].Name=Fault Module Version
Sig[4].Value=0.0.0.0
Sig[5].Name=Fault Module Timestamp
Sig[5].Value=5b7235fc
Sig[6].Name=Exception Code
Sig[6].Value=c0000005
Sig[7].Name=Exception Offset
Sig[7].Value=00000000000a90c7
DynamicSig[1].Name=OS Version
DynamicSig[1].Value=6.3.9600.2.0.0.768.101
DynamicSig[2].Name=Locale ID
DynamicSig[2].Value=1033
DynamicSig[22].Name=Additional Information 1
DynamicSig[22].Value=52d4
DynamicSig[23].Name=Additional Information 2
DynamicSig[23].Value=52d4c7c3252b4dd0889558b089378dbf
DynamicSig[24].Name=Additional Information 3
DynamicSig[24].Value=19bc
DynamicSig[25].Name=Additional Information 4
DynamicSig[25].Value=19bcb909475ac0373ab00a55b0d1043b
UI[2]=c:\users\jjonus\anaconda3\python.exe
UI[3]=Python has stopped working
UI[4]=Windows can check online for a solution to the problem.
UI[5]=Check online for a solution and close the program
UI[6]=Check online for a solution later and close the program
UI[7]=Close the program
LoadedModule[0]=c:\users\jjonus\anaconda3\python.exe
LoadedModule[1]=C:\WINDOWS\SYSTEM32\ntdll.dll
LoadedModule[2]=C:\WINDOWS\system32\KERNEL32.DLL
LoadedModule[3]=C:\WINDOWS\system32\KERNELBASE.dll
LoadedModule[4]=c:\users\jjonus\anaconda3\python36.dll
LoadedModule[5]=c:\users\jjonus\anaconda3\VCRUNTIME140.dll
LoadedModule[6]=c:\users\jjonus\anaconda3\api-ms-win-crt-runtime-l1-1-0.dll
LoadedModule[7]=c:\users\jjonus\anaconda3\api-ms-win-crt-math-l1-1-0.dll
LoadedModule[8]=c:\users\jjonus\anaconda3\api-ms-win-crt-stdio-l1-1-0.dll
LoadedModule[9]=c:\users\jjonus\anaconda3\api-ms-win-crt-locale-l1-1-0.dll
LoadedModule[10]=c:\users\jjonus\anaconda3\api-ms-win-crt-heap-l1-1-0.dll
LoadedModule[11]=C:\WINDOWS\SYSTEM32\VERSION.dll
LoadedModule[12]=C:\WINDOWS\system32\SHLWAPI.dll
LoadedModule[13]=C:\WINDOWS\system32\WS2_32.dll
LoadedModule[14]=C:\WINDOWS\system32\ADVAPI32.dll
LoadedModule[15]=c:\users\jjonus\anaconda3\api-ms-win-crt-string-l1-1-0.dll
LoadedModule[16]=c:\users\jjonus\anaconda3\api-ms-win-crt-convert-l1-1-0.dll
LoadedModule[17]=c:\users\jjonus\anaconda3\api-ms-win-crt-time-l1-1-0.dll
LoadedModule[18]=c:\users\jjonus\anaconda3\api-ms-win-crt-environment-l1-1-0.dll
LoadedModule[19]=c:\users\jjonus\anaconda3\api-ms-win-crt-process-l1-1-0.dll
LoadedModule[20]=c:\users\jjonus\anaconda3\api-ms-win-crt-conio-l1-1-0.dll
LoadedModule[21]=c:\users\jjonus\anaconda3\api-ms-win-crt-filesystem-l1-1-0.dll
LoadedModule[22]=C:\WINDOWS\system32\msvcrt.dll
LoadedModule[23]=C:\WINDOWS\SYSTEM32\combase.dll
LoadedModule[24]=C:\WINDOWS\system32\USER32.dll
LoadedModule[25]=C:\WINDOWS\system32\GDI32.dll
LoadedModule[26]=C:\WINDOWS\system32\NSI.dll
LoadedModule[27]=C:\WINDOWS\system32\RPCRT4.dll
LoadedModule[28]=C:\WINDOWS\SYSTEM32\sechost.dll
LoadedModule[29]=c:\users\jjonus\anaconda3\ucrtbase.DLL
LoadedModule[30]=C:\WINDOWS\system32\SspiCli.dll
LoadedModule[31]=C:\WINDOWS\SYSTEM32\CRYPTBASE.DLL
LoadedModule[32]=C:\WINDOWS\SYSTEM32\bcryptPrimitives.dll
LoadedModule[33]=C:\WINDOWS\system32\IMM32.DLL
LoadedModule[34]=C:\WINDOWS\system32\MSCTF.dll
LoadedModule[35]=C:\WINDOWS\SYSTEM32\CRYPTSP.dll
LoadedModule[36]=C:\WINDOWS\system32\rsaenh.dll
LoadedModule[37]=C:\WINDOWS\SYSTEM32\bcrypt.dll
LoadedModule[38]=c:\users\jjonus\anaconda3\python3.dll
LoadedModule[39]=c:\users\jjonus\anaconda3\DLLs\_hashlib.pyd
LoadedModule[40]=C:\WINDOWS\SYSTEM32\api-ms-win-crt-utility-l1-1-0.dll
LoadedModule[41]=c:\users\jjonus\anaconda3\DLLs\_socket.pyd
LoadedModule[42]=c:\users\jjonus\anaconda3\DLLs\select.pyd
LoadedModule[43]=c:\users\jjonus\anaconda3\DLLs\_bz2.pyd
LoadedModule[44]=c:\users\jjonus\anaconda3\DLLs\_lzma.pyd
LoadedModule[45]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\core\multiarray.cp36-win_amd64.pyd
LoadedModule[46]=c:\users\jjonus\anaconda3\Library\bin\mkl_rt.dll
LoadedModule[47]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\core\umath.cp36-win_amd64.pyd
LoadedModule[48]=c:\users\jjonus\anaconda3\DLLs\_ctypes.pyd
LoadedModule[49]=C:\WINDOWS\system32\ole32.dll
LoadedModule[50]=C:\WINDOWS\system32\OLEAUT32.dll
LoadedModule[51]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\linalg\lapack_lite.cp36-win_amd64.pyd
LoadedModule[52]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\linalg\_umath_linalg.cp36-win_amd64.pyd
LoadedModule[53]=c:\users\jjonus\anaconda3\DLLs\_decimal.pyd
LoadedModule[54]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\_mklinit.cp36-win_amd64.pyd
LoadedModule[55]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\fft\fftpack_lite.cp36-win_amd64.pyd
LoadedModule[56]=c:\users\jjonus\anaconda3\lib\site-packages\mkl_fft\_pydfti.cp36-win_amd64.pyd
LoadedModule[57]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\core\multiarray_tests.cp36-win_amd64.pyd
LoadedModule[58]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\random\mtrand.cp36-win_amd64.pyd
LoadedModule[59]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\_lib\_ccallback_c.cp36-win_amd64.pyd
LoadedModule[60]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\_sparsetools.cp36-win_amd64.pyd
LoadedModule[61]=C:\WINDOWS\SYSTEM32\MSVCP140.dll
LoadedModule[62]=C:\WINDOWS\SYSTEM32\api-ms-win-crt-multibyte-l1-1-0.dll
LoadedModule[63]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\_csparsetools.cp36-win_amd64.pyd
LoadedModule[64]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\csgraph\_shortest_path.cp36-win_amd64.pyd
LoadedModule[65]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\csgraph\_tools.cp36-win_amd64.pyd
LoadedModule[66]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\csgraph\_traversal.cp36-win_amd64.pyd
LoadedModule[67]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\csgraph\_min_spanning_tree.cp36-win_amd64.pyd
LoadedModule[68]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\csgraph\_reordering.cp36-win_amd64.pyd
LoadedModule[69]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\__check_build\_check_build.cp36-win_amd64.pyd
LoadedModule[70]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\murmurhash.cp36-win_amd64.pyd
LoadedModule[71]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\linalg\isolve\_iterative.cp36-win_amd64.pyd
LoadedModule[72]=c:\users\jjonus\anaconda3\Library\bin\libmmd.dll
LoadedModule[73]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\_fblas.cp36-win_amd64.pyd
LoadedModule[74]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\_flapack.cp36-win_amd64.pyd
LoadedModule[75]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\_flinalg.cp36-win_amd64.pyd
LoadedModule[76]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\_solve_toeplitz.cp36-win_amd64.pyd
LoadedModule[77]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\_decomp_update.cp36-win_amd64.pyd
LoadedModule[78]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\cython_blas.cp36-win_amd64.pyd
LoadedModule[79]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\linalg\cython_lapack.cp36-win_amd64.pyd
LoadedModule[80]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\linalg\dsolve\_superlu.cp36-win_amd64.pyd
LoadedModule[81]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\sparse\linalg\eigen\arpack\_arpack.cp36-win_amd64.pyd
LoadedModule[82]=c:\users\jjonus\anaconda3\Library\bin\libifcoremd.dll
LoadedModule[83]=C:\WINDOWS\system32\imagehlp.dll
LoadedModule[84]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\special\_ufuncs.cp36-win_amd64.pyd
LoadedModule[85]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\special\_ufuncs_cxx.cp36-win_amd64.pyd
LoadedModule[86]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\special\specfun.cp36-win_amd64.pyd
LoadedModule[87]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\special\_comb.cp36-win_amd64.pyd
LoadedModule[88]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\special\_ellip_harm_2.cp36-win_amd64.pyd
LoadedModule[89]=c:\users\jjonus\anaconda3\DLLs\_multiprocessing.pyd
LoadedModule[90]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\interpolate\_fitpack.cp36-win_amd64.pyd
LoadedModule[91]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\interpolate\dfitpack.cp36-win_amd64.pyd
LoadedModule[92]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\interpolate\_bspl.cp36-win_amd64.pyd
LoadedModule[93]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\interpolate\_ppoly.cp36-win_amd64.pyd
LoadedModule[94]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\interpolate\interpnd.cp36-win_amd64.pyd
LoadedModule[95]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\spatial\ckdtree.cp36-win_amd64.pyd
LoadedModule[96]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\spatial\qhull.cp36-win_amd64.pyd
LoadedModule[97]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\_lib\messagestream.cp36-win_amd64.pyd
LoadedModule[98]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\spatial\_voronoi.cp36-win_amd64.pyd
LoadedModule[99]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\spatial\_distance_wrap.cp36-win_amd64.pyd
LoadedModule[100]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\spatial\_hausdorff.cp36-win_amd64.pyd
LoadedModule[101]=c:\users\jjonus\anaconda3\lib\site-packages\PIL\_imaging.cp36-win_amd64.pyd
LoadedModule[102]=c:\users\jjonus\anaconda3\Library\bin\zlib.dll
LoadedModule[103]=c:\users\jjonus\anaconda3\Library\bin\tiff.dll
LoadedModule[104]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\minpack2.cp36-win_amd64.pyd
LoadedModule[105]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_trlib\_trlib.cp36-win_amd64.pyd
LoadedModule[106]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_group_columns.cp36-win_amd64.pyd
LoadedModule[107]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_lbfgsb.cp36-win_amd64.pyd
LoadedModule[108]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\moduleTNC.cp36-win_amd64.pyd
LoadedModule[109]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_cobyla.cp36-win_amd64.pyd
LoadedModule[110]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_slsqp.cp36-win_amd64.pyd
LoadedModule[111]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_minpack.cp36-win_amd64.pyd
LoadedModule[112]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_lsq\givens_elimination.cp36-win_amd64.pyd
LoadedModule[113]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_zeros.cp36-win_amd64.pyd
LoadedModule[114]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\optimize\_nnls.cp36-win_amd64.pyd
LoadedModule[115]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\integrate\_odepack.cp36-win_amd64.pyd
LoadedModule[116]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\integrate\_quadpack.cp36-win_amd64.pyd
LoadedModule[117]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\integrate\vode.cp36-win_amd64.pyd
LoadedModule[118]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\integrate\_dop.cp36-win_amd64.pyd
LoadedModule[119]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\integrate\lsoda.cp36-win_amd64.pyd
LoadedModule[120]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\stats\_stats.cp36-win_amd64.pyd
LoadedModule[121]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\stats\statlib.cp36-win_amd64.pyd
LoadedModule[122]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\stats\mvn.cp36-win_amd64.pyd
LoadedModule[123]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\_logistic_sigmoid.cp36-win_amd64.pyd
LoadedModule[124]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\sparsefuncs_fast.cp36-win_amd64.pyd
LoadedModule[125]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\metrics\cluster\expected_mutual_info_fast.cp36-win_amd64.pyd
LoadedModule[126]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\lgamma.cp36-win_amd64.pyd
LoadedModule[127]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\metrics\pairwise_fast.cp36-win_amd64.pyd
LoadedModule[128]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\_random.cp36-win_amd64.pyd
LoadedModule[129]=c:\users\jjonus\anaconda3\DLLs\_ssl.pyd
LoadedModule[130]=C:\WINDOWS\system32\CRYPT32.dll
LoadedModule[131]=C:\WINDOWS\system32\MSASN1.dll
LoadedModule[132]=C:\WINDOWS\system32\mswsock.dll
LoadedModule[133]=c:\users\jjonus\anaconda3\DLLs\unicodedata.pyd
LoadedModule[134]=c:\users\jjonus\anaconda3\lib\site-packages\cryptography\hazmat\bindings\_constant_time.cp36-win_amd64.pyd
LoadedModule[135]=c:\users\jjonus\anaconda3\lib\site-packages\_cffi_backend.cp36-win_amd64.pyd
LoadedModule[136]=c:\users\jjonus\anaconda3\lib\site-packages\cryptography\hazmat\bindings\_openssl.cp36-win_amd64.pyd
LoadedModule[137]=c:\users\jjonus\anaconda3\Library\bin\LIBEAY32.dll
LoadedModule[138]=c:\users\jjonus\anaconda3\Library\bin\SSLEAY32.dll
LoadedModule[139]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\feature_extraction\_hashing.cp36-win_amd64.pyd
LoadedModule[140]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\io\matlab\mio_utils.cp36-win_amd64.pyd
LoadedModule[141]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\io\matlab\mio5_utils.cp36-win_amd64.pyd
LoadedModule[142]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\io\matlab\streams.cp36-win_amd64.pyd
LoadedModule[143]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\datasets\_svmlight_format.cp36-win_amd64.pyd
LoadedModule[144]=c:\users\jjonus\anaconda3\Library\bin\mkl_intel_thread.dll
LoadedModule[145]=c:\users\jjonus\anaconda3\Library\bin\mkl_core.dll
LoadedModule[146]=c:\users\jjonus\anaconda3\Library\bin\libiomp5md.dll
LoadedModule[147]=c:\users\jjonus\anaconda3\Library\bin\mkl_avx.dll
LoadedModule[148]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\decomposition\cdnmf_fast.cp36-win_amd64.pyd
LoadedModule[149]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\seq_dataset.cp36-win_amd64.pyd
LoadedModule[150]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\arrayfuncs.cp36-win_amd64.pyd
LoadedModule[151]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\linear_model\cd_fast.cp36-win_amd64.pyd
LoadedModule[152]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\linear_model\sgd_fast.cp36-win_amd64.pyd
LoadedModule[153]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\weight_vector.cp36-win_amd64.pyd
LoadedModule[154]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\linear_model\sag_fast.cp36-win_amd64.pyd
LoadedModule[155]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\svm\libsvm.cp36-win_amd64.pyd
LoadedModule[156]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\svm\liblinear.cp36-win_amd64.pyd
LoadedModule[157]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\svm\libsvm_sparse.cp36-win_amd64.pyd
LoadedModule[158]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\neighbors\ball_tree.cp36-win_amd64.pyd
LoadedModule[159]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\neighbors\dist_metrics.cp36-win_amd64.pyd
LoadedModule[160]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\neighbors\typedefs.cp36-win_amd64.pyd
LoadedModule[161]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\neighbors\kd_tree.cp36-win_amd64.pyd
LoadedModule[162]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\decomposition\_online_lda.cp36-win_amd64.pyd
LoadedModule[163]=c:\users\jjonus\anaconda3\DLLs\pyexpat.pyd
LoadedModule[164]=c:\users\jjonus\anaconda3\lib\site-packages\win32\win32api.pyd
LoadedModule[165]=C:\WINDOWS\system32\SHELL32.dll
LoadedModule[166]=c:\users\jjonus\anaconda3\lib\site-packages\win32\pywintypes36.dll
LoadedModule[167]=C:\WINDOWS\SYSTEM32\secur32.dll
LoadedModule[168]=c:\users\jjonus\anaconda3\lib\site-packages\win32\_win32sysloader.pyd
LoadedModule[169]=c:\users\jjonus\anaconda3\Library\bin\pythoncom36.dll
LoadedModule[170]=C:\WINDOWS\SYSTEM32\kernel.appcore.dll
LoadedModule[171]=C:\WINDOWS\system32\uxtheme.dll
LoadedModule[172]=C:\WINDOWS\SYSTEM32\urlmon.dll
LoadedModule[173]=C:\WINDOWS\SYSTEM32\iertutil.dll
LoadedModule[174]=C:\WINDOWS\SYSTEM32\WININET.dll
LoadedModule[175]=C:\WINDOWS\SYSTEM32\USERENV.dll
LoadedModule[176]=C:\WINDOWS\SYSTEM32\profapi.dll
LoadedModule[177]=c:\users\jjonus\anaconda3\DLLs\_sqlite3.pyd
LoadedModule[178]=c:\users\jjonus\anaconda3\DLLs\sqlite3.dll
LoadedModule[179]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslib.cp36-win_amd64.pyd
LoadedModule[180]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\conversion.cp36-win_amd64.pyd
LoadedModule[181]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\np_datetime.cp36-win_amd64.pyd
LoadedModule[182]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\nattype.cp36-win_amd64.pyd
LoadedModule[183]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\timedeltas.cp36-win_amd64.pyd
LoadedModule[184]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\timezones.cp36-win_amd64.pyd
LoadedModule[185]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\parsing.cp36-win_amd64.pyd
LoadedModule[186]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\ccalendar.cp36-win_amd64.pyd
LoadedModule[187]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\strptime.cp36-win_amd64.pyd
LoadedModule[188]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\timestamps.cp36-win_amd64.pyd
LoadedModule[189]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\fields.cp36-win_amd64.pyd
LoadedModule[190]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\hashtable.cp36-win_amd64.pyd
LoadedModule[191]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\missing.cp36-win_amd64.pyd
LoadedModule[192]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\lib.cp36-win_amd64.pyd
LoadedModule[193]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\algos.cp36-win_amd64.pyd
LoadedModule[194]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\properties.cp36-win_amd64.pyd
LoadedModule[195]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\hashing.cp36-win_amd64.pyd
LoadedModule[196]=c:\users\jjonus\anaconda3\lib\site-packages\bottleneck\reduce.cp36-win_amd64.pyd
LoadedModule[197]=c:\users\jjonus\anaconda3\lib\site-packages\bottleneck\nonreduce.cp36-win_amd64.pyd
LoadedModule[198]=c:\users\jjonus\anaconda3\lib\site-packages\bottleneck\nonreduce_axis.cp36-win_amd64.pyd
LoadedModule[199]=c:\users\jjonus\anaconda3\lib\site-packages\bottleneck\move.cp36-win_amd64.pyd
LoadedModule[200]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\index.cp36-win_amd64.pyd
LoadedModule[201]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\period.cp36-win_amd64.pyd
LoadedModule[202]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\frequencies.cp36-win_amd64.pyd
LoadedModule[203]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\resolution.cp36-win_amd64.pyd
LoadedModule[204]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\tslibs\offsets.cp36-win_amd64.pyd
LoadedModule[205]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\join.cp36-win_amd64.pyd
LoadedModule[206]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\ops.cp36-win_amd64.pyd
LoadedModule[207]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\interval.cp36-win_amd64.pyd
LoadedModule[208]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\indexing.cp36-win_amd64.pyd
LoadedModule[209]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\internals.cp36-win_amd64.pyd
LoadedModule[210]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\sparse.cp36-win_amd64.pyd
LoadedModule[211]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_path.cp36-win_amd64.pyd
LoadedModule[212]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\window.cp36-win_amd64.pyd
LoadedModule[213]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\skiplist.cp36-win_amd64.pyd
LoadedModule[214]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\reduction.cp36-win_amd64.pyd
LoadedModule[215]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\groupby.cp36-win_amd64.pyd
LoadedModule[216]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\reshape.cp36-win_amd64.pyd
LoadedModule[217]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\parsers.cp36-win_amd64.pyd
LoadedModule[218]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\json.cp36-win_amd64.pyd
LoadedModule[219]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\writers.cp36-win_amd64.pyd
LoadedModule[220]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\io\msgpack\_packer.cp36-win_amd64.pyd
LoadedModule[221]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\io\msgpack\_unpacker.cp36-win_amd64.pyd
LoadedModule[222]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\util\_move.cp36-win_amd64.pyd
LoadedModule[223]=c:\users\jjonus\anaconda3\lib\site-packages\pandas\_libs\testing.cp36-win_amd64.pyd
LoadedModule[224]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\cluster\_vq.cp36-win_amd64.pyd
LoadedModule[225]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\cluster\_hierarchy.cp36-win_amd64.pyd
LoadedModule[226]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\cluster\_optimal_leaf_ordering.cp36-win_amd64.pyd
LoadedModule[227]=c:\users\jjonus\anaconda3\lib\site-packages\fastcache\_lrucache.cp36-win_amd64.pyd
LoadedModule[228]=c:\users\jjonus\anaconda3\lib\site-packages\_cvxcore.cp36-win_amd64.pyd
LoadedModule[229]=c:\users\jjonus\anaconda3\lib\site-packages\_ecos.cp36-win_amd64.pyd
LoadedModule[230]=c:\users\jjonus\anaconda3\lib\site-packages\osqp\_osqp.cp36-win_amd64.pyd
LoadedModule[231]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\_errors.cp36-win_amd64.pyd
LoadedModule[232]=c:\users\jjonus\anaconda3\Library\bin\hdf5.dll
LoadedModule[233]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\_conv.cp36-win_amd64.pyd
LoadedModule[234]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5r.cp36-win_amd64.pyd
LoadedModule[235]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\_objects.cp36-win_amd64.pyd
LoadedModule[236]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\defs.cp36-win_amd64.pyd
LoadedModule[237]=c:\users\jjonus\anaconda3\Library\bin\hdf5_hl.dll
LoadedModule[238]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5t.cp36-win_amd64.pyd
LoadedModule[239]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\utils.cp36-win_amd64.pyd
LoadedModule[240]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5.cp36-win_amd64.pyd
LoadedModule[241]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5z.cp36-win_amd64.pyd
LoadedModule[242]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5a.cp36-win_amd64.pyd
LoadedModule[243]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5s.cp36-win_amd64.pyd
LoadedModule[244]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5p.cp36-win_amd64.pyd
LoadedModule[245]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5ac.cp36-win_amd64.pyd
LoadedModule[246]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\_proxy.cp36-win_amd64.pyd
LoadedModule[247]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5d.cp36-win_amd64.pyd
LoadedModule[248]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5ds.cp36-win_amd64.pyd
LoadedModule[249]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5f.cp36-win_amd64.pyd
LoadedModule[250]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5g.cp36-win_amd64.pyd
LoadedModule[251]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5i.cp36-win_amd64.pyd
LoadedModule[252]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5fd.cp36-win_amd64.pyd
LoadedModule[253]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5o.cp36-win_amd64.pyd
LoadedModule[254]=c:\users\jjonus\anaconda3\lib\site-packages\h5py\h5l.cp36-win_amd64.pyd
LoadedModule[255]=c:\users\jjonus\anaconda3\lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd
LoadedModule[256]=C:\WINDOWS\SYSTEM32\WSOCK32.dll
LoadedModule[257]=C:\WINDOWS\SYSTEM32\VCOMP140.DLL
LoadedModule[258]=c:\users\jjonus\anaconda3\lib\site-packages\google\protobuf\internal\_api_implementation.cp36-win_amd64.pyd
LoadedModule[259]=c:\users\jjonus\anaconda3\lib\site-packages\google\protobuf\pyext\_message.cp36-win_amd64.pyd
LoadedModule[260]=c:\users\jjonus\anaconda3\lib\site-packages\_yaml.cp36-win_amd64.pyd
LoadedModule[261]=c:\users\jjonus\anaconda3\Library\bin\yaml.dll
LoadedModule[262]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\ndimage\_nd_image.cp36-win_amd64.pyd
LoadedModule[263]=c:\users\jjonus\anaconda3\lib\site-packages\scipy\ndimage\_ni_label.cp36-win_amd64.pyd
LoadedModule[264]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_contour.cp36-win_amd64.pyd
LoadedModule[265]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\ft2font.cp36-win_amd64.pyd
LoadedModule[266]=c:\users\jjonus\anaconda3\Library\bin\freetype.dll
LoadedModule[267]=c:\users\jjonus\anaconda3\Library\bin\libpng16.dll
LoadedModule[268]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_png.cp36-win_amd64.pyd
LoadedModule[269]=c:\users\jjonus\anaconda3\lib\site-packages\kiwisolver.cp36-win_amd64.pyd
LoadedModule[270]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_image.cp36-win_amd64.pyd
LoadedModule[271]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_tri.cp36-win_amd64.pyd
LoadedModule[272]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\_qhull.cp36-win_amd64.pyd
LoadedModule[273]=c:\users\jjonus\anaconda3\lib\site-packages\matplotlib\backends\_backend_agg.cp36-win_amd64.pyd
LoadedModule[274]=c:\users\jjonus\anaconda3\lib\site-packages\sip.pyd
LoadedModule[275]=c:\users\jjonus\anaconda3\lib\site-packages\PyQt5\QtCore.pyd
LoadedModule[276]=c:\users\jjonus\anaconda3\Library\bin\Qt5Core.dll
LoadedModule[277]=C:\WINDOWS\SYSTEM32\MPR.dll
LoadedModule[278]=C:\WINDOWS\SYSTEM32\WINMM.dll
LoadedModule[279]=c:\users\jjonus\anaconda3\Library\bin\icuin58.dll
LoadedModule[280]=c:\users\jjonus\anaconda3\Library\bin\icuuc58.dll
LoadedModule[281]=C:\WINDOWS\SYSTEM32\WINMMBASE.dll
LoadedModule[282]=c:\users\jjonus\anaconda3\Library\bin\icudt58.dll
LoadedModule[283]=C:\WINDOWS\SYSTEM32\cfgmgr32.dll
LoadedModule[284]=C:\WINDOWS\SYSTEM32\DEVOBJ.dll
LoadedModule[285]=c:\users\jjonus\anaconda3\lib\site-packages\PyQt5\QtGui.pyd
LoadedModule[286]=c:\users\jjonus\anaconda3\Library\bin\Qt5Gui.dll
LoadedModule[287]=c:\users\jjonus\anaconda3\lib\site-packages\PyQt5\QtWidgets.pyd
LoadedModule[288]=c:\users\jjonus\anaconda3\Library\bin\Qt5Widgets.dll
LoadedModule[289]=C:\WINDOWS\SYSTEM32\dwmapi.dll
LoadedModule[290]=c:\users\jjonus\anaconda3\lib\site-packages\statsmodels\nonparametric\linbin.cp36-win_amd64.pyd
LoadedModule[291]=c:\users\jjonus\anaconda3\lib\site-packages\statsmodels\nonparametric\_smoothers_lowess.cp36-win_amd64.pyd
LoadedModule[292]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\constants.cp36-win_amd64.pyd
LoadedModule[293]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\error.cp36-win_amd64.pyd
LoadedModule[294]=c:\users\jjonus\anaconda3\Library\bin\libzmq-mt-4_2_5.dll
LoadedModule[295]=c:\users\jjonus\anaconda3\Library\bin\libsodium.dll
LoadedModule[296]=C:\WINDOWS\SYSTEM32\IPHLPAPI.DLL
LoadedModule[297]=C:\WINDOWS\SYSTEM32\WINNSI.DLL
LoadedModule[298]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\message.cp36-win_amd64.pyd
LoadedModule[299]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\context.cp36-win_amd64.pyd
LoadedModule[300]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\socket.cp36-win_amd64.pyd
LoadedModule[301]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\utils.cp36-win_amd64.pyd
LoadedModule[302]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\_poll.cp36-win_amd64.pyd
LoadedModule[303]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\_version.cp36-win_amd64.pyd
LoadedModule[304]=c:\users\jjonus\anaconda3\lib\site-packages\zmq\backend\cython\_device.cp36-win_amd64.pyd
LoadedModule[305]=c:\users\jjonus\anaconda3\lib\site-packages\tornado\speedups.cp36-win_amd64.pyd
LoadedModule[306]=c:\users\jjonus\anaconda3\DLLs\_overlapped.pyd
LoadedModule[307]=c:\users\jjonus\anaconda3\DLLs\_asyncio.pyd
LoadedModule[308]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\tree\_criterion.cp36-win_amd64.pyd
LoadedModule[309]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\tree\_splitter.cp36-win_amd64.pyd
LoadedModule[310]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\tree\_tree.cp36-win_amd64.pyd
LoadedModule[311]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\neighbors\quad_tree.cp36-win_amd64.pyd
LoadedModule[312]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\tree\_utils.cp36-win_amd64.pyd
LoadedModule[313]=c:\users\jjonus\anaconda3\lib\site-packages\numpy\core\umath_tests.cp36-win_amd64.pyd
LoadedModule[314]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\ensemble\_gradient_boosting.cp36-win_amd64.pyd
LoadedModule[315]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\graph_shortest_path.cp36-win_amd64.pyd
LoadedModule[316]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\_isotonic.cp36-win_amd64.pyd
LoadedModule[317]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\manifold\_utils.cp36-win_amd64.pyd
LoadedModule[318]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\manifold\_barnes_hut_tsne.cp36-win_amd64.pyd
LoadedModule[319]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\cluster\_k_means.cp36-win_amd64.pyd
LoadedModule[320]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\cluster\_k_means_elkan.cp36-win_amd64.pyd
LoadedModule[321]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\cluster\_hierarchical.cp36-win_amd64.pyd
LoadedModule[322]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\utils\fast_dict.cp36-win_amd64.pyd
LoadedModule[323]=c:\users\jjonus\anaconda3\lib\site-packages\sklearn\cluster\_dbscan_inner.cp36-win_amd64.pyd
LoadedModule[324]=c:\users\jjonus\anaconda3\xgboost\xgboost.dll
State[0].Key=Transport.DoneStage1
State[0].Value=1
FriendlyEventName=Stopped working
ConsentKey=APPCRASH
AppName=Python
AppPath=c:\users\jjonus\anaconda3\python.exe
NsPartner=windows
NsGroup=windows8
ApplicationIdentity=0920103314D9505A1A73B6FD36D5EC01




",performance issue failing command line feed manually source code sig name sig sig version sig sig sig module name sig sig module version sig sig module sig sig code sig sig offset sig version id information information information information stopped working check solution problem solution close program solution later close program program state state working,issue,negative,neutral,neutral,neutral,neutral,neutral
420065591,"Thanks @rhiever and @weixuanfu  for the help so far. 

I am using spyder and the python installation is through the anaconda package. I have python installed in a virtual environment (for a website project) but besides that it is installed in GIMP and Microsoft office as it seems to have come with those packages.

Regarding the command line, how do I call my own scoring function? Do I create .py file named 'Test' with a function 'score' and then call in 'Test.Score' to the command line?

Sincerely,
Justin

",thanks help far python installation anaconda package python virtual environment project besides gimp office come regarding command line call scoring function create file function call command line sincerely,issue,positive,positive,positive,positive,positive,positive
420022895,"Hmm, SIGSEGV signal seems due to out of memory. How large is your input dataset?",signal due memory large input,issue,negative,positive,neutral,neutral,positive,positive
420017978,"What editor are you using to run that script? Have you tried running it on the command line? Are you sure you don't have multiple Python/Anaconda installations?

You can use `which python` to see the default Python your system is using.",editor run script tried running command line sure multiple use python see default python system,issue,negative,positive,positive,positive,positive,positive
419629423,"@GinoWoz1 my guess would be that if you pass in a numpy array (i.e. using `df.values`) at the start, you won't see this problem, but if you pass in the `DataFrame` then you get this issue. Could you confirm?",guess would pas array start wo see problem pas get issue could confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
419606123,"I think the custom scorer is one issue but I am running n_jobs = 1 on only a 1000 row data set and TPOT cannot complete a full run. 50 generations, 50 population size. 

Is there a suggested editor you use that you dont have problems on?",think custom scorer one issue running row data set complete full run population size editor use dont,issue,negative,positive,positive,positive,positive,positive
419584141,"I have just run into the same issue as well. 

The exported pipeline is the below....

exported_pipeline = make_pipeline(
    StackingEstimator(estimator=XGBRegressor(learning_rate=0.01, max_depth=8, min_child_weight=16, n_estimators=100, nthread=1, subsample=0.6000000000000001)),
    ExtraTreesRegressor(bootstrap=False, max_features=0.45, min_samples_leaf=1, min_samples_split=10, n_estimators=100)
)


![image](https://user-images.githubusercontent.com/17535345/45245987-a6ac4d00-b2b3-11e8-92cd-f7123ef1ad3e.png)
",run issue well pipeline image,issue,negative,neutral,neutral,neutral,neutral,neutral
419433785,"Hmm, I didn't see the loop of runing TPOT in your codes so I am not sure why it kept running all the time. And yes, if `warm_start=True`, the next generation loop is initial by the last time population. 

For large dataset with `n_jobs=-1` in Linux, we suggest to [use dask](https://epistasislab.github.io/tpot/using/#parallel-training-with-dask) for parallel computing. ",see loop sure kept running time yes next generation loop initial last time population large suggest use parallel,issue,positive,positive,positive,positive,positive,positive
419307933,"I am now use the warm start in the notebook.here is the code:
`%%timeit
tpot = TPOTClassifier(generations=5,population_size=50,
                     random_state=30,verbosity=2,
                      n_jobs=1,warm_start=True)

X_train = X_train[-100:,:]
y_train = y_train[-100:]
tpot.fit(X_train, y_train)
print('val accuracy:',tpot.score(X_val, y_val))
print('test accuracy:',tpot.score(X_test, y_test))
tpot.export('./em_tpot_notebook1.py')
import pickle
with open('./test_tpot_notebook.pkl','wb') as f:
    pickle.dump(tpot.fitted_pipeline_, f)
`
`Generation 1 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 2 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 3 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 4 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 5 - Current best internal CV score: 0.940375939849624

                                                                                                                       


Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.8500000000000001, min_samples_leaf=2, min_samples_split=16, n_estimators=100)
val accuracy: 0.8263020075647367
test accuracy: 0.8313113807047949

                                                                                                                       

Generation 1 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 2 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 3 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 4 - Current best internal CV score: 0.940375939849624

                                                                                                                       

Generation 5 - Current best internal CV score: 0.940375939849624

                                                                                                                       


Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.8500000000000001, min_samples_leaf=2, min_samples_split=16, n_estimators=100)
val accuracy: 0.8280477160314228
test accuracy: 0.8324667822068169

                                                                                                                       

Generation 1 - Current best internal CV score: 0.940375939849624
`
I find the cell will continue run although the generation is over,and keepping running all the time .
I want to know if the next generation loop is initial by the last time population?That is mean warm_start?
Wiil the cell keepping running all the time?,how can I stop it? ",use warm start code print accuracy print accuracy import pickle open generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline accuracy test accuracy generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline accuracy test accuracy generation current best internal score find cell continue run although generation running time want know next generation loop initial last time population mean cell running time stop,issue,positive,positive,positive,positive,positive,positive
419287348,"Thank you for your reply I will try the warm_start by jupyter notebook later.

By the way,I find when I set n_jobs=-1 in Linux,it works when the dataset is samll about 1000,but when I use it in the large dateset about 20000,it stucks at 0% for a long time,and my RAM is  enough， I am confuse  about it.Could you give some suggestions,thanks a lot!",thank reply try notebook later way find set work use large long time ram confuse give thanks lot,issue,positive,positive,neutral,neutral,positive,positive
419136986,Please follow the [contributing guide](https://epistasislab.github.io/tpot/contributing/). The docs you can check/edit in this issue is [`using.md`](https://github.com/EpistasisLab/tpot/blob/v0.9.5/docs_sources/using.md) and [`api.md`](https://github.com/EpistasisLab/tpot/blob/v0.9.5/docs_sources/api.md),please follow guide issue,issue,negative,neutral,neutral,neutral,neutral,neutral
419116687,I can help in improving the user guides. Let me know how to contribute.,help improving user let know contribute,issue,positive,neutral,neutral,neutral,neutral,neutral
419105911,"You can use warm_start in a non-interactive shell by pickling the TPOT
object at the end of the script, then in another script you can load that
pickled TPOT object and start again.

As Weixuan said though, warm_start is primarily intended for use in an
interactive shell or notebook.

On Thu, Sep 6, 2018 at 1:41 AM OhMyGodness <notifications@github.com> wrote:

> I want to know if you solve the warm start problem of TPOT.I am now
> encounter this problem,set the parameter warm_start = true,but it is
> useless.I want to know how can i go back to the point that last time runs,
> and continue my tpot.fit().Thank you!
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/628#issuecomment-419013594>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t28WeuY_5pxpR8ujKOC9MZNiy2Lkks5uYN-pgaJpZM4Qkwtn>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",use shell object end script another script load object start said though primarily intended use interactive shell notebook wrote want know solve warm start problem encounter problem set parameter true want know go back point last time continue state reply directly view mute thread twitter,issue,negative,positive,positive,positive,positive,positive
419085611,"Now, `warm_start` is only working for interactive shell and the kernel cannot be restarted or stopped.",working interactive shell kernel stopped,issue,negative,neutral,neutral,neutral,neutral,neutral
419013594,"I want to know if you solve the warm start problem of TPOT.I am now encounter this problem,set the parameter warm_start = true,but it is useless.I want to know how can i go back to the point that last time runs, and continue my tpot.fit().Thank you!",want know solve warm start problem encounter problem set parameter true want know go back point last time continue,issue,negative,positive,positive,positive,positive,positive
418848120,I am not sure what happened based on those error messages. So I need reproduce it with your update codes.,sure based error need reproduce update,issue,negative,positive,positive,positive,positive,positive
418829443,"Thanks, any comment on my error? I only installed fancyimpute, rfpimp and missingno. Set my path and got this error.",thanks comment error set path got error,issue,negative,positive,positive,positive,positive,positive
418822164,Please check this [cheatsheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet) for posting codes in markdown format. Or you can simply post the link of codes to your github repo as you did before. ,please check posting markdown format simply post link,issue,negative,neutral,neutral,neutral,neutral,neutral
418812237,"Thanks. I re-installed anaconda. When I DO NOT enter in use_dask=True, the fit function works.

However, when I set use_dask = True I get the error below. I also tested removing max_eval time and then removing my scoring function and same error.

Also the correct code would have y_train as y_train[1], how do I post a code block as you did? I am having trouble. 

![image](https://user-images.githubusercontent.com/17535345/45109854-92225600-b0f5-11e8-824e-484ef40c5228.png)
",thanks anaconda enter fit function work however set true get error also tested removing time removing scoring function error also correct code would post code block trouble image,issue,negative,positive,positive,positive,positive,positive
418732299,"Hmm, it seems that the anaconda environment is messed up. There are two possible solutions:

1st soluation: updating conda
```
conda update -y conda
conda update -y conda-build
conda create -q -n test-environment python=3.6.4 numpy scipy scikit-learn nose cython pandas
```

2nd soluation: you could reinstall anaconda/miniconda in your environment.
",anaconda environment two possible st update update create nose could reinstall environment,issue,negative,neutral,neutral,neutral,neutral,neutral
418530826,"Took a while and errored out - the command you provided.

This is the command I used 

conda create -q -n test-environment python=3.6.4 numpy scipy scikit-learn nose cython pandas

![image](https://user-images.githubusercontent.com/17535345/45059885-7a45c600-b052-11e8-9e65-c87a9fa503c8.png)
",took command provided command used create nose image,issue,negative,neutral,neutral,neutral,neutral,neutral
418495310,"Hmm, can you please try to create a test environment via conda and then test the example? The commands in Powershell of Windows for create an conda environment are below:

```
conda create -q -n test-environment python=%PYTHON_VERSION% numpy scipy scikit-learn nose cython pandas
activate test-environment
pip install deap tqdm update_checker pypiwin32 stopit dask[delayed] dask-ml
pip install tpot
```",please try create test environment via test example create environment create nose activate pip install pip install,issue,positive,neutral,neutral,neutral,neutral,neutral
418490673,"Thanks. I tested on my laptop on 0.9.3 and the fit function worked. I upgraded to 0.9.4 and the same error popped up with or without the dask argument. Also I uninstalled and installed 0.9.5 directly and had the same error.

Also what did you mean by example running under folder tpot.py? I created a .py file in my github repository and then called on the script you provided. I am 1-2 years into python so excuse my ignorance. I never had this error show up until i upgraded. 

I installed python via anaconda and have the latest version. 

The file location of tpot is under the Anaconda lib folder.",thanks tested fit function worked error without argument also uninstalled directly error also mean example running folder file repository script provided python excuse ignorance never error show python via anaconda latest version file location anaconda folder,issue,negative,positive,positive,positive,positive,positive
418487334,"Hmm, I just tested the example in my Windows environment and it works. I am not sure what happened. Could you reproduce the error in other terminal or machine? Was the example running under folder with `tpot.py` or a folder named `tpot`?",tested example environment work sure could reproduce error terminal machine example running folder folder,issue,negative,positive,positive,positive,positive,positive
418479263,"Thanks. I just did what you said and it uninstalled 0.9.4 and installed 0.9.5. Still same error. 


",thanks said uninstalled still error,issue,negative,positive,positive,positive,positive,positive
418477599,"Hmm, odds. Could you please try `pip install --upgrade --no-deps --force-reinstall tpot`? Then try the [example](https://epistasislab.github.io/tpot/examples/#boston-housing-prices-modeling). Please let me know if the example is working or not",odds could please try pip install upgrade try example please let know example working,issue,positive,neutral,neutral,neutral,neutral,neutral
418468250,"I just installed 0.9.4 and running your code I get the error below.


  File ""<ipython-input-19-e82509bf295c>"", line 36, in <module>
    tpot.fit(X_train,y_train[0])

  File ""PATH\Anaconda3\lib\site-packages\tpot\base.py"", line 577, in fit
    self._fit_init()

AttributeError: 'TPOTRegressor' object has no attribute '_fit_init'


",running code get error file line module file line fit object attribute,issue,negative,positive,positive,positive,positive,positive
418462364,"Yep, the customized scorer is not working with `n_jobs=-1` or any integer > 1. ",yep scorer working integer,issue,negative,neutral,neutral,neutral,neutral,neutral
418437736,Thanks for your suggestion. We will start tagging every minor release of TPOT since TPOT 0.9.5. I just added the release information of 0.9.5 in this repo. ,thanks suggestion start every minor release since added release information,issue,negative,positive,neutral,neutral,positive,positive
418434072,"Thanks, so the scoring is the main cause of this issue? Sorry for the error in the codes, I just put that together really quick for something reproducible.

The genetic algorithm finished on my laptop. First one that completed in 2 days after restarting numerous times.",thanks scoring main cause issue sorry error put together really quick something reproducible genetic algorithm finished first one day numerous time,issue,negative,positive,neutral,neutral,positive,positive
418431117,"```python
The following commands were written to file `test_code.py`:
from sklearn.metrics import make_scorer
#from xgboost import XGBRegressor
from tpot import TPOTRegressor
#import xgboost as xgb
import warnings
import pandas as pd
import math

warnings.filterwarnings('ignore')

# load in data

url = 'https://github.com/GinoWoz1/AdvancedHousePrices/raw/master/'

X_train = pd.read_csv(url + 'train_tpot_issue.csv')
y_train = pd.read_csv(url + 'y_train_tpot_issue.csv', header=None)

# loss function

def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            print('error')
            raise ValueError(""Mean Squared Logarithmic Error cannot be used when ""
                             ""targets contain negative values."")
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5

rmsle_loss = make_scorer(rmsle_loss,greater_is_better=False)

# run tpot

tpot = TPOTRegressor(verbosity=3, scoring=rmsle_loss, generations = 50,population_size=50,offspring_size= 50,max_eval_time_mins=10,warm_start=True, n_jobs=-1)
#
tpot.fit(X_train,y_train[0])
```

The scripts in the issue have bugs (about `import math` and `y_train`), so I update the codes as above.
The issue is related to #645 and it is about the customized scorer `rmsle_loss` is not pickable in parallel computing using joblib. 

Since TPOT 0.9.4, there is a work-around for solving this issue with dask. Below is the example codes for this issue.

```python
# coding: utf-8
from sklearn.metrics import make_scorer
#from xgboost import XGBRegressor
from tpot import TPOTRegressor
#import xgboost as xgb
import warnings
import pandas as pd
import math
warnings.filterwarnings('ignore')

# load in data

url = 'https://github.com/GinoWoz1/AdvancedHousePrices/raw/master/'

X_train = pd.read_csv(url + 'train_tpot_issue.csv')
y_train = pd.read_csv(url + 'y_train_tpot_issue.csv', header=None)

# loss function

def rmsle_loss(y_true, y_pred):
    assert len(y_true) == len(y_pred)
    try:
        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y_true[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]
    except:
        return float('inf')
    if not (y_true >= 0).all() and not (y_pred >= 0).all():
            raise ValueError(""Mean Squared Logarithmic Error cannot be used when ""
                             ""targets contain negative values."")
    return (sum(terms_to_sum) * (1.0/len(y_true))) ** 0.5

rmsle_loss = make_scorer(rmsle_loss,greater_is_better=False)

# run tpot

tpot = TPOTRegressor(verbosity=3, scoring = rmsle_loss, generations = 50,population_size=50,offspring_size= 50,max_eval_time_mins=10,warm_start=True, use_dask=True)
tpot.fit(X_train,y_train[0])

```


",python following written file import import import import import import import math load data loss function assert enumerate print raise mean squared logarithmic error used contain negative return sum run issue import math update issue related scorer pickable parallel since issue example issue python import import import import import import import math load data loss function assert try enumerate except return float raise mean squared logarithmic error used contain negative return sum run scoring,issue,negative,negative,negative,negative,negative,negative
418418517,"Oh, please check [this link](https://github.com/EpistasisLab/tpot/blob/master/tpot/builtins/one_hot_encoder.py#L151-L173) for intent of arguments.",oh please check link intent,issue,negative,neutral,neutral,neutral,neutral,neutral
418397250,"Understood, thank you. I have added comments to see if I understand args or not, could you verify my understanding of the intent of the arguments?

```python
'tpot.builtins.OneHotEncoder': {
        'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25], # minimum number of categories to use, otherwise use ""other"" category?
        'sparse': [False],
        'threshold': [10] # minimum number of records per category?
    },
```",understood thank added see understand could verify understanding intent python minimum number use otherwise use category false minimum number per category,issue,negative,negative,negative,negative,negative,negative
418391937,"From TPOT v0.9 we added [`OneHotEncoder`](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/config/classifier.py#L158-L161) to handle Categorical features.

From TPOT v0.9.5, we added two builtin operators to handle mixed features (Related PR #560 and issue #549)",added handle categorical added two handle mixed related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
417840553,"Is this closed as ""won't fix""? Just curious.",closed wo fix curious,issue,negative,negative,neutral,neutral,negative,negative
417550742,@PGijsbers Thank you for catching this bug. I just added a patch into TPOT 0.9.5,thank catching bug added patch,issue,negative,positive,positive,positive,positive,positive
417507283,"From the looks of things, while the current (0.9.4) implementation does produce predictions, but the imputer gets refitted on the test data.

A call to `predict_proba` creates a call to [`_check_dataset`](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L906), in turn `_check_dataset` sets [imputer to None](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1053) which means that in the consequent [call](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1070) to `_impute_values` this causes a the Imputer to be [refit](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1021) on the test data.

When calling `predict` and `predict_proba` the imputer that was fit to the training data should be used instead. `_check_dataset` should not always force an imputer to be fit on the passed data.",current implementation produce imputer test data call call turn imputer none consequent call imputer refit test data calling predict imputer fit training data used instead always force imputer fit data,issue,positive,positive,positive,positive,positive,positive
417465849,This issue should be fixed in TPOT 0.9.4. Please feel free to reopen this issue if you have any questions.,issue fixed please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
417428521,@TomAugspurger `n_jobs` parameter is not used when `use_dask=True`. I will add docs for this case.,parameter used add case,issue,negative,neutral,neutral,neutral,neutral,neutral
417413282,"For those following this issue, https://github.com/EpistasisLab/tpot/pull/730 has been merged into `development`. If you're interested you could try that out with tpot dev and dask-ml >= 0.9.0.

#730 solved the relatively easy task of training many individuals in parallel (on a cluster). It did not address some of the points in the original issue like some individuals in a generation being relatively slow, or caching between generations, or parallelizing the crossover and mutation stage. If anyone is interested in working on those I could assist, but I don't have plans to work on it right now.",following issue development interested could try dev relatively easy task training many parallel cluster address original issue like generation relatively slow crossover mutation stage anyone interested working could assist work right,issue,positive,positive,positive,positive,positive,positive
417376524,Fixed the link to the binder example (which is live now).,fixed link binder example live,issue,negative,positive,positive,positive,positive,positive
417173001,"I am facing the same issues. I have tried running it on Windows 7 and Ubuntu 16.04 (AWS EC2 instance c5.4xlarge machine 32 GB RAM). I have always used n_jobs=1.
On my windows machine it runs successfully but when I use Spyder IDE on Ubuntu, it unexpectedly quits saying Kernel unexpectedly stopped. I tried running it through the terminal and got a 'Segmentation fault'

My dataset size is not that huge either. Its around 335 samples x 80 features 


![image](https://user-images.githubusercontent.com/9207997/44826955-e2be1e80-abde-11e8-95b5-affddcc2a7d6.png)
",facing tried running instance machine ram always used machine successfully use ide unexpectedly quits saying kernel unexpectedly stopped tried running terminal got size huge either around image,issue,positive,positive,positive,positive,positive,positive
417083420,"Thank you for reporting this issue here. I just posted a PR #751 to fix this issue and we will release a new version of TPOT soon with this fix. For now, there are two work-arounds:

1. install the PR with fix via the command below. But it is noted that it is based on development branch.
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@issue750
```

2. add a useless parameter (`'priors': [None]` in the demo below) to bypass the bug.

```python
# coding: utf-8
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot_config = {
                                                         
'sklearn.ensemble.AdaBoostClassifier': {
'base_estimator': {
'sklearn.naive_bayes.GaussianNB': {
'priors': [None]
}
}
}
}

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,
                      config_dict=tpot_config)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```",thank issue posted fix issue release new version soon fix two install fix via command noted based development branch pip install upgrade add useless parameter none bypass bug python import import import none print,issue,negative,negative,negative,negative,negative,negative
417032281,"Yep, 0.1% coverage decrease can be ignored for now. Thanks!",yep coverage decrease thanks,issue,positive,positive,positive,positive,positive,positive
417009736,"Alrightly, all green other than the coveralls failure, which I think can be ignored.",green coverall failure think,issue,negative,negative,negative,negative,negative,negative
417002969,"Thanks @weixuanfu, I just pushed a commit removing some print statements I added and fixing a doc issue. Once that passes I think this will be good to go.",thanks commit removing print added fixing doc issue think good go,issue,positive,positive,positive,positive,positive,positive
416998755,"@TomAugspurger Please let me know if you are OK with merging this PR. I will go ahead to merge it to dev branch and then merge #740. After update docs and some final checks, I think we can have a minor release of TPOT.",please let know go ahead merge dev branch merge update final think minor release,issue,negative,negative,neutral,neutral,negative,negative
416996800,"@TomAugspurger Thank you very much for debugging CI. 

The answers to those questions above:

> 1. Should I split the quick fix for the failing test_driver_5 test off to another PR?

Yes, I think it is fine to pass that test for now. 

> 2. This currently requires dask-ml dev. I'm hoping to do a release in a week or two that has dask/dask-ml#339, which is required by tpot

Great, we could update docs of installation after the new release.

> 3. Do the docs make sense?

Thanks for the docs. I think we should **highly recommend** TPOT users to use `dask` for big data or with Jupyter notebook since `joblib` is somehow not very stable and we had a lot of related issues (like #645).

> 4. Are we OK with just parallelizing this stage, and leaving parallelization of mutation and crossover as a followup?

I agree we can add parallelization of mutation and crossover as a followup. 

",thank much split quick fix failing test another yes think fine pas test currently dev release week two great could update installation new release make sense thanks think highly recommend use big data notebook since somehow stable lot related like stage leaving parallelization mutation crossover agree add parallelization mutation crossover,issue,positive,positive,positive,positive,positive,positive
416986353,"Ok, this seems to be reliably passing now that I've ensured that Dask's `single-threaded` scheduler is being used for the test. I suspect that resolving https://github.com/DEAP/deap/issues/75 would take care of all this.

Some questions on how to proceed:

1. Should I split the quick fix for the failing `test_driver_5` test off to another PR?
2. This currently requires dask-ml dev. I'm hoping to do a release in a week or two that has https://github.com/dask/dask-ml/pull/339, which is required by tpot
3. Do the docs make sense?
4. Are we OK with just parallelizing this stage, and leaving parallelization of mutation and crossover as a followup?",reliably passing used test suspect would take care proceed split quick fix failing test another currently dev release week two make sense stage leaving parallelization mutation crossover,issue,negative,positive,positive,positive,positive,positive
416964717,"My attempted fix didn't work. Just skipping for now on this branch, though that's probably not the best solution.",fix work skipping branch though probably best solution,issue,positive,positive,positive,positive,positive,positive
416961985,"There seems to be an unrelated failure on master now. I didn't dig deeply, but IIUC, it comes from NumPy 1.15.0 (or maybe 1.15.1) raising a warning when an internal umath module is imported. Older versions of scikit-learn imported this, and there's a test ensuring stdout is quiet, which is now failing because of the warning.

I've attempted to fix that in https://github.com/EpistasisLab/tpot/pull/730/commits/d279253994dd5df0eeef7a4c354cd412d6d54d2d, which I can split into a separate PR if desired.",unrelated failure master dig deeply come maybe raising warning internal module older test quiet failing warning fix split separate desired,issue,negative,negative,neutral,neutral,negative,negative
416956692,"`StackingEstimator` in 1st step should add one synthetic feature to left of the input features and the synthetic feature is the prediction of nested estimator `RandomForestRegressor(bootstrap=True, max_features=0.05, min_samples_leaf=19, min_samples_split=12, n_estimators=100)`. So the return array from `tpot._fitted_pipeline.steps[-1][1].feature_importances_` has one item longer than the original list of features.",st step add one synthetic feature left input synthetic feature prediction estimator return array one item longer original list,issue,negative,positive,positive,positive,positive,positive
416694063,"If you export the final pipeline, what are the pipeline components? We
might be able to tease apart what it’s doing and find out.

On Tue, Aug 28, 2018 at 10:51 AM Kevin Du <notifications@github.com> wrote:

> I am using tpot._fitted_pipeline.steps[-1][1].feature_importances_ to get
> a list of feature importances.
>
> It returns an array one item longer than my original list of features. How
> does this array map back to my feature list?
>
> Also, is tpot supposed to be trying out polynomial feature combinations?
> How do we tell if a certain combination of features is important?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/749>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t3bjD4OAeWQdqORVDuJy1ut7261iks5uVYMLgaJpZM4WQGVB>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",export final pipeline pipeline might able tease apart find tue wrote get list feature array one item longer original list array map back feature list also supposed trying polynomial feature tell certain combination important thread reply directly view mute thread twitter,issue,negative,positive,positive,positive,positive,positive
416420342,"Thank you for reporting this issue. TPOT internally didn't use `self.scoring` but uses `self.scoring_function` generated by [`_setup_scoring_function`](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L349-L388) and scoring parameter in pipeline evaluation, so I think TPOT uses the right scoring function as expected during optimization. But, indeed, this is a API bug related to #739. I think I fixed this API bug in PR #740 and we will fixed this issue in the next version of TPOT. ",thank issue internally use scoring parameter pipeline evaluation think right scoring function optimization indeed bug related think fixed bug fixed issue next version,issue,positive,positive,neutral,neutral,positive,positive
415977883,"You would have to create a custom TPOT configuration that used operations that support multi-output regression, e.g., the sklearn [MultiOutputRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputRegressor.html#sklearn.multioutput.MultiOutputRegressor). As `MultiOutputRegressor` takes another estimator as a parameter, see our [SelectFromModel example](https://github.com/EpistasisLab/tpot/blob/9c712821c400d3b67f4045b451c687d7fb783a5d/tpot/config/regressor.py#L186-L194) in another configuration dictionary.

I'm not 100% familiar with multi-output support in sklearn, but any operations that work with the `MultiOutputRegressor` and `cross_val_score` should also work with TPOT.

You can read more about custom configuration dictionaries [here](http://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters).",would create custom configuration used support regression another estimator parameter see example another configuration dictionary familiar support work also work read custom configuration,issue,positive,positive,positive,positive,positive,positive
415499448,"No, it wasn't backported.

On Thu, Aug 23, 2018 at 11:53 AM saddy001 <notifications@github.com> wrote:

> It's 0.19.1. The fix is from 2017. Isn't it pip-packaged already?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/pull/730#issuecomment-415489476>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABQHIgjv-1ZQnbmZUn6_wm_WsxwHERVmks5uTt3ygaJpZM4VRUUv>
> .
>
",wrote fix already reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
415490788,"If you set `warm_start=True` when creating the TPOTClassifier object, TPOT will store its current state after finishing/interruption so you can call `fit` again starting from that point.",set object store current state call fit starting point,issue,negative,positive,positive,positive,positive,positive
415424459,@weixuanfu Running with `config_dict='TPOT light'` allows the script to complete. I am rerunning now with a larger number of generations and population_size. Thanks for the help.,running light script complete number thanks help,issue,positive,positive,positive,positive,positive,positive
415421387,@weixuanfu Thanks for your tips. I will rerun with those parameters and report back.,thanks rerun report back,issue,negative,positive,neutral,neutral,positive,positive
415415571,"@lpatruno I still suspect the dataset may need more memory (~2G) since some pipelines (especially for pipelines with [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) which can double feature number in intermediates steps) require more memory which may cause crash due to out of memory. `top` commend should has a refresh rate, maybe 2-3 seconds, so it maybe not accurate to check the maximum memory usage during optimization  

Could you please try to run the dataset in a machine with large memory or using [`TPOT light` configuration](https://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) via `config_dict='TPOT light'`?

@g-vega-cl Hmm, it seems the crash only happened in PC but not in Linux in your case, right? ",still suspect may need memory since especially double feature number require memory may cause crash due memory top commend refresh rate maybe maybe accurate check maximum memory usage optimization could please try run machine large memory light configuration via light crash case right,issue,negative,positive,positive,positive,positive,positive
415180291,"I have a similar issue, I'm working with Anaconda and the spyder IDE, after TPOT runs a few generations I get a message saying that the kernel died;
![2018_08_22-spyderkerneldiedforgithub](https://user-images.githubusercontent.com/13635035/44490774-deb16000-a624-11e8-9ca1-87510a177d18.PNG)
I have tried this with PyCharm and the same happens (altough pycharm returns the following error; Process finished with exit code -1073741819 (0xC0000005) ). 
The enviroment im working on has;
![2018_08_22-pythonversionspyderforgithub](https://user-images.githubusercontent.com/13635035/44490142-20410b80-a623-11e8-8325-36a7b190f8cb.PNG)

I have worked with large (> 10'000k datapoints) and small (< 1000k datapoints) datasets, I'm attaching a copy of a part of the dataset;
[2018_08_22-XSLX_SnipForGithub.xlsx](https://github.com/EpistasisLab/tpot/files/2311982/2018_08_22-XSLX_SnipForGithub.xlsx)
 
Below is my code;

`import numpy as np
import pandas as pd
import os
from tpot import TPOTRegressor


dataset = pd.read_csv(r'2018_08_22-XSLX_SnipForGithub.csv', index_col = 0)

values = dataset
values = values.values

n_train_hours = 4000
train = values[:n_train_hours,]
test = values[n_train_hours:, ]
train_X, train_y = train[:, 1:], train[:, 0]
test_X, test_y = test[:, 1:], test[:, 0]
print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)

tpot = TPOTRegressor(scoring = 'neg_mean_absolute_error', 
                     max_time_mins = 100,
                     n_jobs = 1, #Sometimes I run it with -1 and it also crashes.
                     verbosity = 2,
                     cv = 5,
                     warm_start = True,
                     periodic_checkpoint_folder = 'C:/mydir')


tpot.fit(train_X, train_y)
tpot.export('2018_08_22-tpot_exported_pipeline.py')`

Note; I ran the same process in another PC and it lasted longer working, but it still crashed after 12h or so, I also have run the program in google colab and Kaggle and it does not seem to crash there.
Note 2; I do not have admin rights in the PC's i am using, maybe that matters.
Thank you, sorry if this has already been resolved, i did not find the answer.
",similar issue working anaconda ide get message saying kernel tried following error process finished exit code working worked large small copy part code import import import o import train test train train test test print scoring sometimes run also verbosity true note ran process another longer working still also run program seem crash note maybe thank sorry already resolved find answer,issue,negative,negative,neutral,neutral,negative,negative
415164846,What scikit-learn version? 0.19.x has a thread safety bug with a similar error.  Fixed in https://github.com/scikit-learn/scikit-learn/pull/9569 on master.,version thread safety bug similar error fixed master,issue,negative,positive,neutral,neutral,positive,positive
415121675,"I've got another one, but this time a crash. 
```
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.
  from numpy.core.umath_tests import inner1d
Optimization Progress:   0%|                                                                                                                                                      | 0/100100 [00:00<?, ?pipeline/s]/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py:547: UserWarning: Multiprocessing-backed parallel loops cannot be nested below threads, setting n_jobs=1
  **self._backend_args)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
corrupted size vs. prev_size
Abgebrochen (Speicherabzug geschrieben)
```
Last line means core dumped. Do you have an idea only from ""corrupted size vs. prev_size""? I couldn't get a minimal example crashing.",got another one time crash internal module removed future release import optimization progress parallel setting parallel setting precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average precision set due average parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting precision set due average parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting precision set due average parallel setting precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average precision set due average parallel setting parallel setting parallel setting parallel setting precision set due average precision set due average parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting parallel setting precision set due average precision set due average precision set due average precision set due average precision set due average classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list precision set due average precision set due average classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list classifier fit score partition set empty list corrupted size last line core idea corrupted size could get minimal example,issue,positive,negative,neutral,neutral,negative,negative
415032911,"Unfortunately, I cannot share the dataset as it is proprietary.  However, here are some summary stats from one of the training sets:
```
	count	mean	std	min	25%	50%	75%	max
col_0	410.0	0.314634	0.464937	0.000000	0.000000	0.000000	1.000000	1.000000
col_1	410.0	85.010912	52.331006	26.908183	49.232402	68.898860	100.652436	456.152245
col_2	410.0	68.839790	49.453209	8.106944	38.153472	52.422222	77.461285	422.319444
col_3	410.0	123.073782	73.341779	34.219294	72.132352	102.015683	156.204155	521.180868
col_4	410.0	16.171122	20.801288	0.013542	4.021528	10.857558	21.449815	229.918414
col_5	410.0	54.233992	55.214856	2.070602	20.907862	35.166267	65.538912	367.913113
col_6	410.0	38.062870	51.508699	0.011088	7.069893	20.134896	45.943032	353.163113
col_7	178.0	3.629213	2.958783	1.000000	1.250000	3.000000	5.000000	17.000000
col_8	410.0	1.621951	1.363516	0.000000	1.000000	1.000000	2.000000	9.000000
col_9	410.0	0.097561	0.297083	0.000000	0.000000	0.000000	0.000000	1.000000
col_10	410.0	0.546341	0.498456	0.000000	0.000000	1.000000	1.000000	1.000000
col_11	410.0	0.348780	0.477167	0.000000	0.000000	0.000000	1.000000	1.000000
col_12	410.0	0.014634	0.120230	0.000000	0.000000	0.000000	0.000000	1.000000
col_13	410.0	0.034146	0.181827	0.000000	0.000000	0.000000	0.000000	1.000000
col_14	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_15	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_16	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_17	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_18	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_19	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_20	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_21	410.0	0.036585	0.187971	0.000000	0.000000	0.000000	0.000000	1.000000
col_22	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_23	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_24	410.0	0.012195	0.109890	0.000000	0.000000	0.000000	0.000000	1.000000
col_25	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_26	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_27	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_28	410.0	0.039024	0.193890	0.000000	0.000000	0.000000	0.000000	1.000000
col_29	410.0	0.029268	0.168764	0.000000	0.000000	0.000000	0.000000	1.000000
col_30	410.0	0.009756	0.098410	0.000000	0.000000	0.000000	0.000000	1.000000
col_31	410.0	0.009756	0.098410	0.000000	0.000000	0.000000	0.000000	1.000000
col_32	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_33	410.0	0.248780	0.432834	0.000000	0.000000	0.000000	0.000000	1.000000
col_34	410.0	0.109756	0.312967	0.000000	0.000000	0.000000	0.000000	1.000000
col_35	410.0	0.009756	0.098410	0.000000	0.000000	0.000000	0.000000	1.000000
col_36	410.0	0.007317	0.085330	0.000000	0.000000	0.000000	0.000000	1.000000
col_37	410.0	0.017073	0.129702	0.000000	0.000000	0.000000	0.000000	1.000000
col_38	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_39	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_40	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_41	410.0	0.024390	0.154446	0.000000	0.000000	0.000000	0.000000	1.000000
col_42	410.0	0.097561	0.297083	0.000000	0.000000	0.000000	0.000000	1.000000
col_43	410.0	0.004878	0.069758	0.000000	0.000000	0.000000	0.000000	1.000000
col_44	410.0	0.002439	0.049386	0.000000	0.000000	0.000000	0.000000	1.000000
col_45	410.0	0.397561	0.489992	0.000000	0.000000	0.000000	1.000000	1.000000
col_46	410.0	0.014634	0.120230	0.000000	0.000000	0.000000	0.000000	1.000000
col_47	410.0	0.397561	0.489992	0.000000	0.000000	0.000000	1.000000	1.000000
```",unfortunately share proprietary however summary one training count mean min,issue,negative,negative,negative,negative,negative,negative
415029864,"Hmm, it is weird. Could you please share the dataset or codes of making a simulation dataset to let us reproduce this issue?",weird could please share making simulation let u reproduce issue,issue,negative,negative,negative,negative,negative,negative
414802519,"yeah, that's pretty accurate. thanks, saw #744 regarding stacking estimator usage.
",yeah pretty accurate thanks saw regarding estimator usage,issue,positive,positive,positive,positive,positive,positive
414730076,This branch is still under dev/test now. I am not sure if `max_eval_time_mins` parameter will help to stop those time-consuming pipelines when using dask.  ,branch still sure parameter help stop,issue,negative,positive,positive,positive,positive,positive
414717514,"![unbenannt](https://user-images.githubusercontent.com/13658554/44411411-bd8a3a00-a566-11e8-93f0-cb62c7baff08.png)
Now this is getting useful. However, there are still long mostly idle periods because of the generational model. Is there some branch for the steady-state model?",getting useful however still long mostly idle generational model branch model,issue,negative,positive,positive,positive,positive,positive
414709735,"@saddy001 not hijacking at all!. Thank you for reporting issues here.

TPOT will print everything (all caught warning and exception) when evaluating pipelines or when generating pipelines via randomly creator in initial generation or crossover/mutation in later generations. But most of warnings/exceptions are from scikit-learn operators/TPOT builtin operators and they should be raised in 3 cases:
1. invalid parameter combinations can be randomly generated via creator/crossover/mutation, so TPOT has a pre_test decorator to exclude them before evaluation on training dataset
2. feature selection step in a pipeline is too restricted to allow any feature pass to next step
3. input/intermediate features format issue, like `X needs to contain only non-negative integers.`, which means the input/intermediate features passed to one-hot-encoder operators should only contain only non-negative integers presenting categorical variables. 

Those warning/exceptions when using 3 verbose levels are for diagnosis.
",thank print everything caught warning exception generating via randomly creator initial generation later raised invalid parameter randomly via decorator exclude evaluation training feature selection step pipeline restricted allow feature pas next step format issue like need contain contain categorical verbose diagnosis,issue,negative,negative,negative,negative,negative,negative
414709092,"Ok, now I did
```
$ pip install git+https://github.com/dask/dask-ml
$ pip install git+https://github.com/EpistasisLab/tpot@mrocklin-dask

$ pip list|egrep ""dask|TPOT""
dask                           0.18.2              
dask-glm                       0.1.0               
dask-ml                        0.8.1.dev5+g581026c 
TPOT                           0.9.3               
```
Looks good so far. No exceptions for several runs. And of course, I did not check the resulting pipelines. Only difference is some warnings that usually do not appear in verbose level 2:
```
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 70  71  76  77  88  89  94  95 100 101 106 107 112 113 118 119 124 125
 136 137 148 149 154 155 166 167] are constant.
  UserWarning)
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:114: RuntimeWarning: invalid value encountered in true_divide
  f = msb / msw
/home/saddy/miniconda3/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.
  'precision', 'predicted', average, warn_for)
 136 137 148 149 166 167] are constant.
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:75: FitFailedWarning: Classifier fit failed. The score on this train-test partition for these parameters will be set to -inf. Details: 
IndexError('pop from empty list',)
  FitFailedWarning,
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 76  77  88  89  94  95 100 101 106 107 112 113 118 119 124 125 136 137
 148 149] are constant.
/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 76  77  88  89  94  95 100 101 112 113 118 119 124 125 136 137 148 149] are constant.
/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py:349: RuntimeWarning: invalid value encountered in subtract
  np.average((array - array_means[:, np.newaxis]) ** 2, axis=1, weights=weights)
```
As far as I can tell, TPOT uses 3 verbose levels. Only level 3 shows everything and even level 2 is quite silent, only deprecation warnings, errors and essential scores from time to time. I think level 1 is for users that aren't that much interested in what TPOT does ;-)",pip install pip install pip good far several course check resulting difference usually appear verbose level constant invalid value sequence multidimensional indexing use instead future array index result either error different result return sorted indexer precision set due average constant classifier fit score partition set empty list constant constant invalid value subtract array far tell verbose level everything even level quite silent deprecation essential time time think level much interested,issue,positive,positive,neutral,neutral,positive,positive
414699067,">  However to make my hijack complete, here's another exception that should be fixed with your commit:

Not hijacking, you turned up a bug in dask-ml :)

Can you try in dask-ml master? I believe https://github.com/dask/dask-ml/pull/339 handles your original error, and this one as well.

> that this same exception is only a warning in usual TPOT, with verbose=3.

I think that matching TPOTs warnings handling may be difficult, but I haven't explored too deeply.",however make hijack complete another exception fixed commit turned bug try master believe original error one well exception warning usual think matching handling may difficult deeply,issue,negative,negative,neutral,neutral,negative,negative
414698919,"`StackingEstimator` in step 3 should add synthetic feature(s) to left of the features from step 2 RobustScaler.

There is not a very simply way to access those transformed features in the intermediate steps. One work-around is to build a partial pipeline to refit dataset and then to get the transformed features (see a demo below). But by using this way, please set a fix `random_state` if the operators in the pipeline (including the nested operator 'BernoulliNB') has the parameter for getting consistent results.

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import RobustScaler
from sklearn.tree import DecisionTreeClassifier
from tpot.builtins import StackingEstimator
from sklearn.feature_selection import SelectFwe, f_classif
from sklearn.naive_bayes import BernoulliNB


digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)


partial_pipeline = make_pipeline(
SelectFwe(score_func=f_classif, alpha=0.048),
RobustScaler(),
StackingEstimator(estimator=BernoulliNB(alpha=0.01, fit_prior=True)),
)

partial_pipeline.fit(X_train, y_train)
# get transformed X_train after step 3
X_transformed_after_step3 = partial_pipeline.transform(X_train)
print(X_transformed_after_step3.shape)
```",step add synthetic feature left step simply way access intermediate one build partial pipeline refit get see way please set fix pipeline operator parameter getting consistent python import import import import import import import import get step print,issue,negative,positive,neutral,neutral,positive,positive
414691695,"Yes, that corresponds to my observation, that this same exception is only a warning in usual TPOT, with verbose=3. 
However to make my hijack complete, here's another exception that should be fixed with your commit:
```
Traceback (most recent call last):                                                                                                                                                                                 
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/base.py"", line 650, in fit
    per_generation_function=self._check_periodic_pipeline
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/gp_deap.py"", line 231, in eaMuPlusLambda
    fitnesses = toolbox.evaluate(invalid_ind)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/base.py"", line 1228, in _evaluate_individuals
    result_score_list = list(dask.compute(*result_score_list))
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/base.py"", line 402, in compute
    results = schedule(dsk, keys, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/threaded.py"", line 75, in get
    pack_exception=pack_exception, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 505, in get_async
    raise_exception(exc, tb)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/compatibility.py"", line 69, in reraise
    raise exc
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 274, in execute_task
    result = _execute_task(task, data)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 255, in _execute_task
    return func(*args2)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py"", line 289, in score
    test_score = _score(est, X_test, y_test, scorer)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py"", line 283, in _score
    return scorer(est, X) if y is None else scorer(est, X, y)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py"", line 101, in __call__
    y_pred = estimator.predict(X)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py"", line 115, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py"", line 306, in predict
    Xt = transform.transform(Xt)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/builtins/one_hot_encoder.py"", line 497, in transform
    copy=True
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/builtins/one_hot_encoder.py"", line 128, in _transform_selected
    X_sel = transform(X_sel)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/builtins/one_hot_encoder.py"", line 412, in _transform
    raise ValueError(""X needs to contain only non-negative integers."")
ValueError: X needs to contain only non-negative integers.
```
I think it can be reproduced by just setting any value negative in the training matrix.",yes observation exception warning usual however make hijack complete another exception fixed commit recent call last file line fit file line file line list file line compute schedule file line get file line file line reraise raise file line result task data file line return file line score scorer file line return scorer none else scorer file line file line lambda lambda file line predict file line transform file line transform file line raise need contain need contain think setting value negative training matrix,issue,negative,positive,neutral,neutral,positive,positive
414679197,Here is [lightgbm's scikit-learn API](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api).  Please check [the link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for more details about how to make a custom configuration for the operators and parameters in TPOT.,please check link make custom configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
414678061,"TPOT just uses genetic programming to tune the parameters. If the lightgbm has a scikit-learn API then the answer is yes. 

A related issue #335 #688 ",genetic tune answer yes related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
414675994,"@JainVikas What is the exported pipeline? I made up a exported pipeline example based on your question as below. The scikit-learn Pipeline object has the API to access `feature_importances_` in the intermediate step of a Pipeline. But it is noted that the `feature_importances_` in step N would be for features from step N-1 instead of input features for the Pipeline. So if there are feature selection/stacking estimators to exclude/add feature before step N then the `feature_importances_` should have different feature number with input features (X).

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import ExtraTreeClassifier

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)


exported_pipeline = make_pipeline(
    MinMaxScaler(),
    ExtraTreeClassifier()
)

exported_pipeline.fit(X_train, y_train)
# print steps
print(exported_pipeline.steps)
# getting the  feature importances from step 2 extratreeclassifier
exported_pipeline.named_steps['extratreeclassifier'].feature_importances_```",pipeline made pipeline example based question pipeline object access intermediate step pipeline noted step would step instead input pipeline feature feature step different feature number input python import import import import import print print getting feature step,issue,negative,neutral,neutral,neutral,neutral,neutral
414545614,"@weixuanfu 👍  for the #735, and thank you for this awesome work.
Quick follow up, I am able to use the workaround if the fitted pipeline contains SelectFwe method, but in my case, fitted pipeline contains an extraTreeClassifier and doesn't have SelectFwe. I think, ""exported_pipeline.feature_importances_ "" should be okay. Can you share your thoughts on the same?",thank awesome work quick follow able use fitted pipeline method case fitted pipeline think share,issue,positive,positive,positive,positive,positive,positive
414434671,"Indeed, I'm not handling errors correctly:

```python
In [40]: from sklearn.datasets import make_classification

In [41]: from tpot import TPOTClassifier

In [42]: X, y = make_classification(n_samples=5)

In [43]: tpot_config = {
    ...:     'sklearn.neighbors.KNeighborsClassifier': {
    ...:         'n_neighbors': [1, 100],
    ...:     }
    ...: }

In [44]: clf = TPOTClassifier(use_dask=True, config_dict=tpot_config, generati
    ...: ons=2, population_size=5, cv=3, random_state=0,)

In [45]: clf.fit(X, y)
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
~/sandbox/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    649                     verbose=self.verbosity,
--> 650                     per_generation_function=self._check_periodic_pipeline
    651                 )

~/sandbox/tpot/tpot/gp_deap.py in eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function)
    230
--> 231     fitnesses = toolbox.evaluate(invalid_ind)
    232     for ind, fit in zip(invalid_ind, fitnesses):

~/sandbox/tpot/tpot/base.py in _evaluate_individuals(self, individuals, features, target, sample_weight, groups)
   1227                     warnings.simplefilter('ignore')
-> 1228                     result_score_list = list(dask.compute(*result_score_list))
   1229

~/sandbox/dask/dask/base.py in compute(*args, **kwargs)
    394     postcomputes = [x.__dask_postcompute__() for x in collections]
--> 395     results = schedule(dsk, keys, **kwargs)
    396     return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])

~/sandbox/dask/dask/threaded.py in get(dsk, result, cache, num_workers, **kwargs)
     74                         cache=cache, get_id=_thread_get_id,
---> 75                         pack_exception=pack_exception, **kwargs)
     76

~/sandbox/dask/dask/local.py in get_async(apply_async, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, **kwargs)
    500                     else:
--> 501                         raise_exception(exc, tb)
    502                 res, worker_id = loads(res_info)

~/sandbox/dask/dask/compatibility.py in reraise(exc, tb)
    111             raise exc.with_traceback(tb)
--> 112         raise exc
    113

~/sandbox/dask/dask/local.py in execute_task(key, task_info, dumps, loads, get_id, pack_exception)
    271         task, data = loads(task_info)
--> 272         result = _execute_task(task, data)
    273         id = get_id()

~/sandbox/dask/dask/local.py in _execute_task(arg, cache, dsk)
    252         args2 = [_execute_task(a, cache) for a in args]
--> 253         return func(*args2)
    254     elif not ishashable(arg):

~/sandbox/dask-ml/dask_ml/model_selection/methods.py in score(est_and_time, X_test, y_test, X_train, y_train, scorer)
    288     start_time = default_timer()
--> 289     test_score = _score(est, X_test, y_test, scorer)
    290     score_time = default_timer() - start_time

~/sandbox/dask-ml/dask_ml/model_selection/methods.py in _score(est, X, y, scorer)
    282         return {k: v(est, X) if y is None else v(est, X, y) for k, v in scorer.items()}
--> 283     return scorer(est, X) if y is None else scorer(est, X, y)
    284

~/Envs/dask-dev/lib/python3.6/site-packages/sklearn/metrics/scorer.py in __call__(self, estimator, X, y_true, sample_weight)
    100                                              sample_weight=sample_weight)
--> 101         y_pred = estimator.predict(X)
    102         if sample_weight is not None:

~/Envs/dask-dev/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args, **kwargs)
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
    116         # update the docstring of the returned function

~/Envs/dask-dev/lib/python3.6/site-packages/sklearn/pipeline.py in predict(self, X)
    306                 Xt = transform.transform(Xt)
--> 307         return self.steps[-1][-1].predict(Xt)
    308

~/Envs/dask-dev/lib/python3.6/site-packages/sklearn/neighbors/classification.py in predict(self, X)
    144
--> 145         neigh_dist, neigh_ind = self.kneighbors(X)
    146

~/Envs/dask-dev/lib/python3.6/site-packages/sklearn/neighbors/base.py in kneighbors(self, X, n_neighbors, return_distance)
    346                 "" but n_samples = %d, n_neighbors = %d"" %
--> 347                 (train_size, n_neighbors)
    348             )

ValueError: Expected n_neighbors <= n_samples,  but n_samples = 3, n_neighbors = 100

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-45-b0953fbb1d6e> in <module>()
----> 1 clf.fit(X, y)

~/sandbox/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    680                     # raise the exception if it's our last attempt
    681                     if attempt == (attempts - 1):
--> 682                         raise e
    683             return self
    684

~/sandbox/tpot/tpot/base.py in fit(self, features, target, sample_weight, groups)
    671                         self._pbar.close()
    672
--> 673                     self._update_top_pipeline()
    674                     self._summary_of_best_pipeline(features, target)
    675                     # Delete the temporary cache before exiting

~/sandbox/tpot/tpot/base.py in _update_top_pipeline(self)
    745             # If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.
    746             # need raise RuntimeError because no pipeline has been optimized
--> 747             raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
    748
    749     def _summary_of_best_pipeline(self, features, target):

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
```

That will never succeed for `n_neighbors=100`, since there aren't that many samples. With `use_dask=False`, the run completes successfully.",indeed handling correctly python import import recent call last fit self target population toolbox mu verbose fit zip self target list compute schedule return repack zip get result cache result cache else reraise raise raise key task data result task data id cache cache return score scorer scorer scorer return none else return scorer none else scorer self estimator none lambda lambda partial help work lambda update returned function predict self return predict self self handling exception another exception recent call last module fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self user initial generation yet need raise pipeline raise pipeline yet please call fit first self target pipeline yet please call fit first never succeed since many run successfully,issue,positive,positive,positive,positive,positive,positive
414430874,"@saddy001 Your dask-ml version looks incorrect, though maybe that's just a typo.

A reproducible example would be helpful. Based just on the traceback, that seems like an error that could come up using just TPOT (though it's certainly possible that the dask backend may be not handling errors correctly).",version incorrect though maybe typo reproducible example would helpful based like error could come though certainly possible may handling correctly,issue,positive,neutral,neutral,neutral,neutral,neutral
414368114,"Ok, I don't want to hijack this thread, so let me know if it is the case. 
I did the following
```
pip install dask dask-ml  # versions 0.18.2 and 0.1.dev27+gcaa3023
pip install git+https://github.com/EpistasisLab/tpot@mrocklin-dask
```
Then I set n_jobs=1 and use_dask=True and I got
```
Traceback (most recent call last):                                                                                                                                                                                 
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/base.py"", line 650, in fit
    per_generation_function=self._check_periodic_pipeline
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/gp_deap.py"", line 231, in eaMuPlusLambda
    fitnesses = toolbox.evaluate(invalid_ind)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/tpot/base.py"", line 1228, in _evaluate_individuals
    result_score_list = list(dask.compute(*result_score_list))
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/base.py"", line 402, in compute
    results = schedule(dsk, keys, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/threaded.py"", line 75, in get
    pack_exception=pack_exception, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 505, in get_async
    raise_exception(exc, tb)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/compatibility.py"", line 69, in reraise
    raise exc
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 274, in execute_task
    result = _execute_task(task, data)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask/local.py"", line 255, in _execute_task
    return func(*args2)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py"", line 289, in score
    test_score = _score(est, X_test, y_test, scorer)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/dask_ml/model_selection/methods.py"", line 283, in _score
    return scorer(est, X) if y is None else scorer(est, X, y)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py"", line 101, in __call__
    y_pred = estimator.predict(X)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py"", line 115, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py"", line 307, in predict
    return self.steps[-1][-1].predict(Xt)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/neighbors/classification.py"", line 145, in predict
    neigh_dist, neigh_ind = self.kneighbors(X)
  File ""/home/saddy/miniconda3/lib/python3.6/site-packages/sklearn/neighbors/base.py"", line 347, in kneighbors
    (train_size, n_neighbors)
ValueError: Expected n_neighbors <= n_samples,  but n_samples = 38, n_neighbors = 73
```
Is there something obviously wrong with my installed packages?

Edit:
Some calls live longer than a few seconds and distribute over multiple cores as expected. It seems random which estimators are evaluated first.",want hijack thread let know case following pip install pip install set got recent call last file line fit file line file line list file line compute schedule file line get file line file line reraise raise file line result task data file line return file line score scorer file line return scorer none else scorer file line file line lambda lambda file line predict return file line predict file line something obviously wrong edit live longer distribute multiple random first,issue,negative,negative,neutral,neutral,negative,negative
414352804,"The failure in ci seems very randomly happened. Sometimes it just passed. I caught the error message in failed ci tests. Any idea?

```
Traceback (most recent call last):
  File ""/home/travis/miniconda/envs/testenv/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 885, in __del__
    self.close()
  File ""/home/travis/miniconda/envs/testenv/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 1090, in close
    self._decr_instances(self)
  File ""/home/travis/miniconda/envs/testenv/lib/python3.6/site-packages/tqdm/_tqdm.py"", line 454, in _decr_instances
    cls.monitor.exit()
  File ""/home/travis/miniconda/envs/testenv/lib/python3.6/site-packages/tqdm/_monitor.py"", line 52, in exit
    self.join()
  File ""/home/travis/miniconda/envs/testenv/lib/python3.6/threading.py"", line 1053, in join
    raise RuntimeError(""cannot join current thread"")
RuntimeError: cannot join current thread
```",failure randomly sometimes caught error message idea recent call last file line file line close self file line file line exit file line join raise join current thread join current thread,issue,negative,negative,negative,negative,negative,negative
414328404,"Thank you! I appreciate the positive feedback. 

There is a related issue #735 although it is not a very simple work-around to access the features. 

We may add an attribute/function in TPOT object to output the features used in each step of pipeline. 

",thank appreciate positive feedback related issue although simple access may add object output used step pipeline,issue,positive,positive,neutral,neutral,positive,positive
414146537,"Yep, that should work, with dask-ml>=0.8.

On Sun, Aug 19, 2018 at 7:17 AM saddy001 <notifications@github.com> wrote:

> Is there something testable right now? Like ""get this branch and then set
> use_dask=True to TPOTClassifier""?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/pull/730#issuecomment-414123904>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABQHIgrtNQMXS0szplXphicqbOD0rEqMks5uSVdAgaJpZM4VRUUv>
> .
>
",yep work sun wrote something testable right like get branch set reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
414123904,"Is there something testable right now? Like ""get this branch and then set use_dask=True to TPOTClassifier""?",something testable right like get branch set,issue,negative,positive,positive,positive,positive,positive
413958549,Hmm weird. @TomAugspurger I will take a look on CI on next Monday. Thank you!,weird take look next thank,issue,negative,negative,negative,negative,negative,negative
413672792,I haven't been able to reproduce the CI failures locally yet :/,able reproduce locally yet,issue,negative,positive,positive,positive,positive,positive
413540613,BTW I noticed some warning message in the video. I saw those warning before. It seems that it is related to installation of deap (see [issue 240](https://github.com/DEAP/deap/issues/240) in deap repo).,warning message video saw warning related installation see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
413539291,"@TomAugspurger Nice!

I agree that ""we can document that if you use dask with the distributed scheduler, you'll get different results than otherwise, even with the same random state"" since the same results can be got from run-to-run with Dask and the distributed scheduler.

For parallelizing random mutation and crossover, I think you just need apply scheduler [for this part of codes](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/gp_deap.py#L131-L150)
",nice agree document use distributed get different otherwise even random state since got distributed random mutation crossover think need apply part,issue,positive,negative,neutral,neutral,negative,negative
413509212,"A few observations on what you're seeing (I plan to record audio over the top of it, and add it to https://github.com/dask/dask-examples eventually)

- Each bar in the UI is a single core. We seem to have good parallelism within each generation. The communication that does happen (in red) is I believe mostly just scores (which are small)
- There's a ""straggler"" problem in some generations (generation 3 is most noticeable I think), where one of the estimators, GradientBoostingClassifer in this case, takes much longer than the rest so all but a few cores of our cluster are idle. I've seen references to timeouts in TPOT, but haven't investigated if they work well here.
- There are decent gaps between generations, where the cluster is idle and just the client machine is doing computation. I assume this is the random mutation and cross over computation? I don't know how easy it would be to parallelize these blocks.

I'll try to get the CI passing today.",seeing plan record audio top add eventually bar single core seem good parallelism within generation communication happen red believe mostly small straggler problem generation noticeable think one case much longer rest cluster idle seen work well decent cluster idle client machine computation assume random mutation cross computation know easy would parallelize try get passing today,issue,positive,positive,positive,positive,positive,positive
413411017,"That's a very fun video to watch :)

I'd be curious to know from TPOT devs if it's also pragmatically useful",fun video watch curious know also pragmatically useful,issue,positive,positive,positive,positive,positive,positive
413269758,"Ah... I suppose it could be for setting the random state used by deap? https://github.com/DEAP/deap/issues/75 seems related.

I probably won't have time to push on DEAP, but I can at least get this PR in a solid place. To summarize

1. We can implement tests using just the threaded scheduler, which will ensure that the dask-ml output exactly matches the scikit-learn output
2. We can document that if you use dask with the distributed scheduler, you'll get different results than  otherwise, even with the same random state. To be clear, you'll get the same results from run-to-run with Dask and the distributed scheduler, but you won't get the same result as without the distributed scheduler. Maybe users will be OK with that.",ah suppose could setting random state used related probably wo time push least get solid place summarize implement threaded ensure output exactly output document use distributed get different otherwise even random state clear get distributed wo get result without distributed maybe,issue,positive,negative,negative,negative,negative,negative
413265077,"OK, narrowed it down to https://github.com/dask/distributed/blob/cb10d6b10ad7800648b44ab3b194220d1e6fc5f8/distributed/utils_comm.py#L49

Changing that to just `addr = list(addresses - bad_addresses)[0]` removed all the differences between TPOT and Dask.

@mrocklin not sure if you have any thoughts on Dask's use of random here.

IMO, the global random state is ""free for grabs"". If you want reproducibility (like TPOT) then you should ensure you don't rely on the global random state, just since anyone can come along and modify it.",list removed sure use random global random state free want reproducibility like ensure rely global random state since anyone come along modify,issue,positive,negative,neutral,neutral,negative,negative
413260472,"So, when we use either the threaded or processes scheduler.

Right now, my best guess is that both TPOT and distributed are sharing NumPy's global random state. Somewhere in distributed we draw some random data, which leads to diverging paths. Trying to confirm this now.

This isn't necessarily a bug in TPOT, but typically (e.g. in scikit-learn), the estimator creates it's own random state object. in https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/base.py#L390-L393 and in `fit` the random seed is set. I wonder if that can be avoided.",use either threaded right best guess distributed global random state somewhere distributed draw random data diverging trying confirm necessarily bug typically estimator random state object fit random seed set wonder,issue,positive,negative,neutral,neutral,negative,negative
413207600,Thanks for narrowing it down! I can probably take it from here. Will report back later today.,thanks probably take report back later today,issue,negative,positive,neutral,neutral,positive,positive
413206053,"@TomAugspurger Hi, sorry, I just get some time to look into the issue. Below are my findings:

1. The individuals in initial generation (generation 0) and their fitness score are the same among `tp1-3`

2. The inconsistency happened after random mutation and crossover ([this line](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/gp_deap.py#L222)). Somehow the random seed setting (which should be set [here](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/base.py#L390-L393)) is not working as expected when generating offspring after running `self._evaluate_individuals()`.

3. If you add a line of setting random state before random mutation and crossover as below. The tp1.evaluated_individuals_ is equal to tp2.evaluated_individuals_. But keeping setting random state every generation are not a nice work-around.

```
        # Vary the population
        np.random.seed(42)
        offspring = varOr(population, toolbox, lambda_, cxpb, mutpb)
```

I need look into why random state changed after generation 0. Please let me know if you have any idea. Thank you!",hi sorry get time look issue initial generation generation fitness score among inconsistency random mutation crossover line somehow random seed setting set working generating offspring running add line setting random state random mutation crossover equal keeping setting random state every generation nice vary population offspring population toolbox need look random state generation please let know idea thank,issue,positive,negative,negative,negative,negative,negative
413182122,"@weixuanfu have you had a chance to check on the differing output of `self._preprocess_individuals(individuals)` for different values of `use_dask`? Not a problem if you haven't. I'm going to do some investigation this afternoon, so thought I'd check to see if you've discovered anything already.",chance check output different problem going investigation afternoon thought check see discovered anything already,issue,negative,neutral,neutral,neutral,neutral,neutral
412847228,"I think this is the same issue that I have with ELI5 using `XGBoost`, `DataFrame` and `sklearn`: https://github.com/TeamHG-Memex/eli5/issues/256

The above bug links to the xgboost #2334 that you've noted above. With ELI5 I've had to use `.values` to get a `ndarray` out of the `DataFrame` otherwise the `PermutationImportance` function complains with the same error you note.

As noted in #2334 I guess this is somewhere in the interface between `sklearn` and `XGBoost`, but I don't know where.",think issue bug link noted use get otherwise function error note noted guess somewhere interface know,issue,negative,neutral,neutral,neutral,neutral,neutral
411889865,"@weixuanfu Sorry for not being that clear. I am inserting `pandas datadrame` with column names consisting of characters from the set `{a..zA..Z0..9-_}` into `fit` and `predict`. Usually, the process goes through without any problem but if the `XGBoost` is in the final pipeline I get an exception that seems to be related to the set of characters I am using for the column names of the dataframe. I have observed the error several times, let me dump the stacktrace once I observe it once again.",sorry clear column set za fit predict usually process go without problem final pipeline get exception related set column error several time let dump observe,issue,negative,negative,neutral,neutral,negative,negative
411874814,"Sure, I've pushed my branch with some debugging code in it.

Here's a notebook with some test code: https://gist.github.com/ee3c440b578071c74e6f267eec557b30

> Even when creating two estimators with identical parameters, they seem to sometimes differ.

I haven't been able to reproduce this. I may have been mistaken earlier. The difference seems to only be present when `use_dask=True`.",sure branch code notebook test code even two identical seem sometimes differ able reproduce may mistaken difference present,issue,negative,positive,positive,positive,positive,positive
411870508,"Hmm, @TomAugspurger could you please share the test codes herein? I will check it later. Thank you!",could please share test herein check later thank,issue,positive,neutral,neutral,neutral,neutral,neutral
411858253,"I notice that the dask-based implementation isn't getting identical results, when in principal it should. As far as I can tell, it comes down to different values for

```
operator_counts, eval_individuals_str, sklearn_pipeline_list, stats_dicts = self._preprocess_individuals(individuals)
```

As an example, with the regular scikit-learn scoring (`use_dask=False`), for `eval_individuals_str`, I get

```python
['LogisticRegression(input_matrix, LogisticRegression__C=0.0001, LogisticRegression__dual=False, LogisticRegression__penalty=l1)',
 'RandomForestClassifier(FastICA(input_matrix, FastICA__tol=0.30000000000000004), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.9000000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=9, RandomForestClassifier__n_estimators=100)',
 'LogisticRegression(input_matrix, LogisticRegression__C=0.1, LogisticRegression__dual=True, LogisticRegression__penalty=l2)',
 'GaussianNB(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100))']
```

but with `use_dask=True`, I get

```python
['LogisticRegression(FastICA(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=20, GradientBoostingClassifier__min_samples_split=6, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.2), FastICA__tol=0.30000000000000004), LogisticRegression__C=0.0001, LogisticRegression__dual=False, LogisticRegression__penalty=l1)',
 'RandomForestClassifier(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.001, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.6500000000000001, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=8, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.8500000000000001), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.9000000000000001, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=15, RandomForestClassifier__n_estimators=100)',
 'LogisticRegression(CombineDFs(input_matrix, input_matrix), LogisticRegression__C=0.1, LogisticRegression__dual=False, LogisticRegression__penalty=l2)',
 'GaussianNB(FastICA(input_matrix, FastICA__tol=0.1))',
 'RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.05, RandomForestClassifier__min_samples_leaf=16, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)']
```

This seems to be somewhat random. Even when creating two estimators with identical parameters, they seem to sometimes differ. Is this something I should be concerned about?",notice implementation getting identical principal far tell come different example regular scoring get python get python somewhat random even two identical seem sometimes differ something concerned,issue,negative,negative,neutral,neutral,negative,negative
411787970,"OK, thanks! I think moving those logic to fit/predict is more clear way to fix this issue. I will submit a PR for it.",thanks think moving logic clear way fix issue submit,issue,positive,positive,positive,positive,positive,positive
411783867,"Traveling right now. But yeah, no validation or anything in init.

Sent from phone. Please excuse spelling and brevity.

On Thu, Aug 9, 2018, 16:14 Randy Olson <notifications@github.com> wrote:

> @amueller <https://github.com/amueller>, any thoughts on how this should
> be resolved? Is it generally recommended to avoid parameter handling logic
> in the __init__ of an estimator? If so, we could move said logic to
> fit/predict.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/739#issuecomment-411772523>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAbcFg01NXfaLVTQRzmwY1tLa_KyEKUpks5uPEO9gaJpZM4V1wnf>
> .
>
",traveling right yeah validation anything sent phone please excuse spelling brevity randy wrote resolved generally avoid parameter handling logic estimator could move said logic reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
411772523,"@amueller, any thoughts on how this should be resolved? Is it generally recommended to avoid parameter handling logic in the `__init__` of an estimator? If so, we could move said logic to fit/predict.",resolved generally avoid parameter handling logic estimator could move said logic,issue,negative,positive,neutral,neutral,positive,positive
411765839,"What version of TPOT and what version of sklearn are you using? Do you have
XGBoost installed, and if so, what version?

On Wed, Aug 8, 2018 at 10:59 PM Rootmannii <notifications@github.com> wrote:

> DeprecationWarning: This module was deprecated in version 0.18 in favor of
> the model_selection module into which all the refactored classes and
> functions are moved. Also note that the interface of the new CV iterators
> are different from that of this module. This module will be removed in 0.20.
> ""This module will be removed in 0.20."", DeprecationWarning)
> THis keeps showing after I did the update
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/314#issuecomment-411629287>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t2IV9OORd2gEE-3PWqXjTu55BGTmks5uO7OJgaJpZM4Kwsbj>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",version version version wed wrote module version favor module class also note interface new different module module removed module removed showing update reply directly view mute thread twitter,issue,negative,positive,neutral,neutral,positive,positive
411750038,"@jaksmid I labeled this one as question for now since there is no clear description of this issue so far. I will relabeled this one if I think this issue is caused by a bug in TPOT. Please let me know the pipeline for some clues about this issue (like, why `'Interval90-120_Ratio'` was generated?)",one question since clear description issue far one think issue bug please let know pipeline issue like,issue,positive,positive,neutral,neutral,positive,positive
411671654,"@weixuanfu Should this be labeled as question though? 
The pipeline can be running for days and crashes in the last step.",question though pipeline running day last step,issue,negative,neutral,neutral,neutral,neutral,neutral
411669924,"Let me export the pipeline once the error reappears. 

But I think it is related to this: https://github.com/dmlc/xgboost/issues/2334
Although the bug was closed, it was concluded with the tip on how to handle such cases.",let export pipeline error think related although bug closed tip handle,issue,negative,negative,neutral,neutral,negative,negative
411629287,"DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
THis keeps showing after I did the update",module version favor module class also note interface new different module module removed module removed showing update,issue,negative,positive,neutral,neutral,positive,positive
411411628,"Thank you a lot @TomAugspurger for good process and nice video. 

wrt API, I prefer 2nd approach but I agree with @rhiever that dask should be an optional dependency for users while it should be highly recommended on installation guide. 

I think we could add one parameter (I like `parallel_backend`) in `TPOTbase` to support both approaches above within TPOT:

- if `parallel_backend` is `multiprocessing` or `dask`, then TPOT use 1st approach and set the backend before parallel computing jobs
- if `parallel_backend` is `dask_graph`, then TPOT use 2nd approach.

BTW, I think [steps about crossover and mutation](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/gp_deap.py#L97-L150) are also very time-consuming, but we can also add parallel computing support on that one later.",thank lot good process nice video prefer approach agree optional dependency highly installation guide think could add one parameter like support within use st approach set parallel use approach think crossover mutation also also add parallel support one later,issue,positive,positive,positive,positive,positive,positive
411258550,"Thanks for the heads-up about `random_state`, I'm glad it's that simple :)

The API design here is a bit tricky, since Dask could conceivably help TPOT in two ways

1. Via the distributed joblib backend. This requires no changes to TPOT itself (assuming it doesn't hardcode the joblib backend anywhere). Users would update their code to look like

```python
from sklearn.externals import joblib

with joblib.parallel_backend('dask', scatter=[X, y]):
    tpot.fit(X, y)
```
This would only potentially be useful for users with a cluster.

2. (this PR) Via Dask's dependency tracking and ability to transform a pipeline to a Dask graph, allowing for sharing intermediate results and executing independent tasks in parallel. This would be useful for people on a single machine, and also scales out to a cluster.

Without having thought about it too much, I'd propose we reserve the ""backend"" terminology for which joblib backend is used (multiprocessing / threaded / dask), and guide users to the  `joblib.parallel_backened` API. I'm not yet sure what keyword is best for the ""Use Dask's graph stuff"" mode.

I'll do some benchmarking on the two approaches on a cluster tomorrow.",thanks glad simple design bit tricky since could conceivably help two way via distributed assuming anywhere would update code look like python import would potentially useful cluster via dependency ability transform pipeline graph intermediate independent parallel would useful people single machine also scale cluster without thought much propose reserve terminology used threaded guide yet sure best use graph stuff mode two cluster tomorrow,issue,positive,positive,positive,positive,positive,positive
411227574,"wrt dependencies: Given that we give a *strong* recommendation to install TPOT on top of an Anaconda Python install, I suspect it wouldn't be too painful to add dask and related packages as dependencies. However, I would like for it to be an optional dependency for users that don't want/need parallelization at dask's scale.

wrt API, thinking out loud: We definitely want to maintain a `n_jobs` parameter in the TPOTClassifier/Regressor classes. We could then add another parameter (`parallel_backend`??) that allows the user to specify whether they want to use sklearn's multiprocessing (`'multiprocessing'`, default) or the various versions of dask parallelization (`'dask-multithreading'`, `'dask-multiprocessing'`, or `'dask-scheduling'`). During initialization, TPOT would check that parameter and set up the appropriate scheduler as needed, assuming the interfaces aren't too different. Thoughts?",given give strong recommendation install top anaconda python install suspect would painful add related however would like optional dependency parallelization scale thinking loud definitely want maintain parameter class could add another parameter user specify whether want use default various parallelization would check parameter set appropriate assuming different,issue,positive,positive,neutral,neutral,positive,positive
411220335,">Is it possible to specify parameters such that all the randomness is removed from a tpot.fit call? I'd like to directly compare the scores, etc. between the two fit calls.

Yes, TPOT has a `random_state` parameter that allows you to designate the RNG for the run. If the results aren't reproducible between two TPOT runs with the same `random_state`, then there is a bug.

>(possibly) shared intermediate results between fits. It's not clear to me that there is actually anything to share between the various fits, though I may be wrong here.

There should be shared information between pipelines, especially with larger population sizes. The optimization procedure within TPOT often creates slightly-modified copies of existing pipelines, so there is a possibility to share information between previously-evaluated pipelines and new pipelines (e.g., a ""parent"" and ""child"" with shared components), as well as information between pairs of new pipelines (e.g., two ""twin"" pipelines).",possible specify randomness removed call like directly compare two fit yes parameter designate run reproducible two bug possibly intermediate clear actually anything share various though may wrong information especially population size optimization procedure within often possibility share information new parent child well information new two twin,issue,positive,positive,neutral,neutral,positive,positive
411193421,"Indeed, this would be entirely optional. An ImportError could be raised during early in `.fit` if the required dask-ml isn't installed.

> Also I'm curious, why processes=False?

Leftover from a debugging session.

> I would hope that this would be pretty apparent by looking at the Graph diagnostic page

It's a *bit* messy, though there seems to be some sharing...

https://streamable.com/5v9dv",indeed would entirely optional could raised early also curious leftover session would hope would pretty apparent looking graph diagnostic page bit messy though,issue,positive,positive,neutral,neutral,positive,positive
411188923,"> shared intermediate results between fits. It's not clear to me that there is actually anything to share between the various fits, though I may be wrong here

I would hope that this would be pretty apparent by looking at the Graph diagnostic page (I recommend avoding the newest bokeh 0.13.0, which has layout issues if so)",intermediate clear actually anything share various though may wrong would hope would pretty apparent looking graph diagnostic page recommend layout,issue,positive,negative,neutral,neutral,negative,negative
411188386,"I suspect that we could put the imports inside the `if delayed` block to keep this optional.

That's a nice video @TomAugspurger :)

Seeing this scale would be interesting.  ",suspect could put inside block keep optional nice video seeing scale would interesting,issue,negative,positive,positive,positive,positive,positive
411186773,"Going forward, are the TPOT devs interested in an optional dependency on Dask-ML, which would add new dependencies on dask (pure python) six, and multipledispatch? The diagnostics also require distributed (which depends on tornado) and Bokeh. To use things on a cluster, you'll need distributed.

The benefits would be

1. Diagnostics of whats happening during a `.fit`
2. Distributed training
3. (possibly) shared intermediate results between fits. It's not clear to me that there is actually anything to share between the various fits, though I may be wrong here.

If so, what API do you envision? `TPOTClassifier(..., dask=False)`?",going forward interested optional dependency would add new pure python six diagnostics also require distributed tornado use cluster need distributed would diagnostics whats happening distributed training possibly intermediate clear actually anything share various though may wrong envision,issue,positive,positive,neutral,neutral,positive,positive
411182352,"Pushed a rough attempt at 1 (this is still very much a work in progress).

https://streamable.com/s9e6x

Having the visibility into what's actually going on is nice. I'll test this out on an actual cluster later, but things should work nicely for CPU-bound problems like this.

I'm still working a bit to understand the various types and computation as things flow through wrapped_cross_score, _fit_and_score, etc. I *think* that the `build_graph` I'm re-using from dask_ml is doing the same actual work as scikit-learns _fit_and_score and score but I need to verify that.

Is it possible to specify parameters such that all the randomness is removed from a `tpot.fit` call? I'd like to directly compare the scores, etc. between the two fit calls.",rough attempt still much work progress visibility actually going nice test actual cluster later work nicely like still working bit understand various computation flow think actual work score need verify possible specify randomness removed call like directly compare two fit,issue,positive,positive,positive,positive,positive,positive
408923006,"In my experiments, tpot still ignores the `max_eval_time_mins=5` parameter for datasets between 1000 and 5000 observations (5 to 25 columns). When `fit()` is called, tpot runs for an indefinitely long time period (at least several hours).

While I am able to stop the process by using the `early_stop` parameter, I would really like to set a specific time period. 

I am using version 0.9.3 of tpot, python 3.6.2 and OSX 10.13.6. tpot runs in single thread mode (`n_jobs=1`).

Please let me know if you need any further infos.",still parameter fit indefinitely long time period least several able stop process parameter would really like set specific time period version python single thread mode please let know need,issue,positive,positive,neutral,neutral,positive,positive
408891761,IMO a better long-term solution to this problem is to refactor the imputation logic to insert a median imputer at the start of every TPOT pipeline when missing values are detected in the training data.,better solution problem imputation logic insert median imputer start every pipeline missing training data,issue,negative,positive,positive,positive,positive,positive
408878210,"Thank you so much. @weixuanfu 
It was very helpful.

You may close this issue.",thank much helpful may close issue,issue,positive,positive,positive,positive,positive,positive
408870534,Thank you for reporting this issue. I will refine check_dataset step.,thank issue refine step,issue,negative,neutral,neutral,neutral,neutral,neutral
408870161,"Please check the example in the [link for scikit-learn Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (see codes after ""# getting the selected features chosen by anova_filter"")

Below is a demo for the pipeline in this issue.

```python
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.feature_selection import SelectFwe, f_classif
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)


exported_pipeline = make_pipeline(
    SelectFwe(score_func=f_classif, alpha=0.038),
    MinMaxScaler(),
    LogisticRegression(C=10.0, dual=False, penalty=""l1"")
)


exported_pipeline.fit(X_train, y_train)
# print steps
print(exported_pipeline.steps)
# getting the selected features chosen by selectfwe
exported_pipeline.named_steps['selectfwe'].get_support()
```
",please check example link pipeline see getting selected chosen pipeline issue python import import import import import import print print getting selected chosen,issue,negative,neutral,neutral,neutral,neutral,neutral
408605268,It is a documented open issue. We are trying to use dask backend to solve it. Related to #730,open issue trying use solve related,issue,negative,neutral,neutral,neutral,neutral,neutral
408605060,"I have fixed this issue in dev branch (see [this line](
https://github.com/EpistasisLab/tpot/blob/development/tpot/base.py#L888) and it will be fixed in next version of TPOT ",fixed issue dev branch see line fixed next version,issue,negative,positive,neutral,neutral,positive,positive
408600242,"Dosnt work, use something else.",work use something else,issue,negative,neutral,neutral,neutral,neutral,neutral
408600205,"Someone commit some example code, whats the point if this cannot be scaled? Have tried every combination of forked, nothing works. Have 32 processors, and no progress after 30 minutes at verbosity 3. Garbage.

Wasted 4 hours trying to get this to do anything with more than 1 cpu.",someone commit example code whats point scaled tried every combination forked nothing work progress verbosity garbage wasted trying get anything,issue,negative,negative,negative,negative,negative,negative
408502707,"Thank you for the prompt response, here is the pipeline:

```
# Score on the training set was:0.999646646481159
exported_pipeline = make_pipeline(
    SelectFwe(score_func=f_classif, alpha=0.038),
    MinMaxScaler(),
    LogisticRegression(C=10.0, dual=False, penalty=""l1"")
)
```
Thanks",thank prompt response pipeline score training set thanks,issue,positive,positive,positive,positive,positive,positive
408451054,"Can you please post the full pipeline generated by TPOT?
",please post full pipeline,issue,negative,positive,positive,positive,positive,positive
407908185,"I've worked on something similar, this might be helpful! 
https://arxiv.org/abs/1603.06560",worked something similar might helpful,issue,negative,neutral,neutral,neutral,neutral,neutral
407789436,"@TomAugspurger you should now have push access.

FWIW I'm less concerned about getting tests to pass here than I am about investigating how dask can be useful.  I think that the two approaches to that are:

1.  break up fit_and_score to use delayed to see if we recapture the benefits of shared intermediate results (and get to see a cool graph).  This is probably on dask devs.
2.  design a more steady state algorithm that could benefit from dask's support of increased dynamism.  This is probably first on TPOT devs to design such an algorithm.",push access le concerned getting pas investigating useful think two break use see recapture intermediate get see cool graph probably design steady state algorithm could benefit support dynamism probably first design algorithm,issue,positive,positive,positive,positive,positive,positive
407778170,"> I just had a quick test in my Window environment, I got the 2 IndexError as below:

When `_fit_and_score` throws an exception, @mrocklin currently sets the score to `-inf`. It should be `[[-inf, -inf]]` to match the return type when `_fit_and_score` is successful (though this may have just been a quick hack to get things working and not intended to stay around; haven't looked closely yet).

```diff
diff --git a/tpot/gp_deap.py b/tpot/gp_deap.py
index 3a5bfcf..e99d494 100644
--- a/tpot/gp_deap.py
+++ b/tpot/gp_deap.py
@@ -433,7 +433,7 @@ def _wrapped_cross_val_score(sklearn_pipeline, features, target,
         try:
             return _fit_and_score(*args, **kwargs)
         except Exception:
-            return -float('inf')
+            return [[- float('inf'), -float('inf')]]
 
     with warnings.catch_warnings():
         warnings.simplefilter('ignore')
```

@mrocklin are you able to give me push access to this branch?",quick test window environment got exception currently score match return type successful though may quick hack get working intended stay around closely yet git index target try return except exception return return float able give push access branch,issue,positive,positive,positive,positive,positive,positive
407593691,"Yes, see the periodic_checkpoint_folder parameter in the API.

On Tue, Jul 24, 2018 at 11:43 AM lesshaste <notifications@github.com> wrote:

> I apologise if this is documented already, but is it possible for tpot to
> show you and/or save the best model so far at the end of each generation?
> For long running tpot jobs this would be very helpful and interesting to
> see.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/736>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t-bqe7KnSEKS2gZp7cB-eghFl1Rfks5uJ2qngaJpZM4VdQrd>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",yes see parameter tue wrote already possible show save best model far end generation long running would helpful interesting see thread reply directly view mute thread twitter,issue,positive,positive,positive,positive,positive,positive
407593482,"Benchmarking on a full set of datasets, such as PMLB, would be more
informative.

https://github.com/EpistasisLab/penn-ml-benchmarks

On Tue, Jul 24, 2018 at 11:34 AM Sourav Singh <notifications@github.com>
wrote:

> @rhiever <https://github.com/rhiever> I am interested in working on this.
> Do I need to benchmark this on MNIST?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/731#issuecomment-407508267>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t55gLewD46HaxN0InjRWzwxgGZVMks5uJ2idgaJpZM4VU4WR>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",full set would informative tue singh wrote interested working need reply directly view mute thread twitter,issue,negative,positive,positive,positive,positive,positive
406671890,"Not that I know of. I have seen some papers that compare TPOT and auto-sklearn to other algorithms on a handful of datasets and most of the time they come out even, which has been my experience too. IMO the search algorithm matters much less than the pipeline components and the AutoML tool's ability to represent complex pipelines.

I think there has been more focus on developing the offerings of the individual algorithms rather than comparing existing algorithms. For anyone who is interested, I encourage them to pursue this project.",know seen compare handful time come even experience search algorithm much le pipeline tool ability represent complex think focus individual rather anyone interested encourage pursue project,issue,positive,positive,neutral,neutral,positive,positive
406669180,Removed it because the devs aren't following it closely any more. Filing issues are a better solution for now. Thanks!,removed following closely filing better solution thanks,issue,positive,positive,positive,positive,positive,positive
406518012,Just looking into this topic & found this thread. May I know if there is any progress since?,looking topic found thread may know progress since,issue,negative,neutral,neutral,neutral,neutral,neutral
406494485,"Pandas on Ray may have some helpful performance optimizations that may be helpful for spark as well:

- https://github.com/modin-project/modin/blob/master/README.rst#pandas-on-ray
- https://github.com/modin-project/modin/tree/master/modin/pandas

See also:

- ""Refactor TPOT to work directly with numpy matrix instead of pandas DataFrames"" https://github.com/EpistasisLab/tpot/issues/113",ray may helpful performance may helpful spark well see also work directly matrix instead,issue,positive,positive,neutral,neutral,positive,positive
406491346,"For people who already have spark setup. We have a first cut out of parallelising TPOT (+ DEAP) with spark, for quite some time now, in our private fork. It's (alpha) tested and fairly profiled in terms of memory. 

We are using parallel delayed to call fit_and_score. And tested it with sklearn's transformer caching to remove redundant computations and have seen impressive results (though it takes huge amount of disk if number of individuals and generations are high.).

We might think of ways to send similar tasks to executors such that transformers caching can be more effective. 

[gp_deap.py](https://github.fkinternal.com/karan10111/l_tpot/blob/n_tasks/gp_deap.py)
checkout the `_wrapped_cross_val_score_spark` method, line 458.

We'll open separate PR soon. Suggestions will be highly appreciated. Thanks.

PS - We had to make some changes to DEAP. [DEAP#268](https://github.com/DEAP/deap/issues/268). These were serialisation related changes.

",people already spark setup first cut spark quite time private fork alpha tested fairly memory parallel call tested transformer remove redundant seen impressive though huge amount disk number might think way send similar effective method line open separate soon highly thanks make related,issue,positive,positive,positive,positive,positive,positive
406430376,"Yup.  That would be pretty easy to use with Dask.  You would need to use
the distributed scheduler (which, despite it's name, is quite lightweight
on a single machine (or single process (or single thread)).  This would
force a dependency on Tornado though.
http://dask.pydata.org/en/latest/scheduling.html#dask-distributed-local

On Thu, Jul 19, 2018 at 6:04 PM Randy Olson <notifications@github.com>
wrote:

> I should add: Currently, TPOT uses a *generational* evolutionary
> algorithm (EA) model. That means that the EA keeps some fixed population of
> pipelines (by default, 100) and has to evaluate all of them before
> proceeding to the next generation. From an engineering perspective,
> generational EAs can be problematic because there can be a single pipeline
> that takes forever to evaluate and that will hold up the whole process.
>
> For the longest time, we have wanted to re-engineer TPOT to follow a *steady
> state* EA. With a steady state EA, pipelines are evaluated in parallel
> and the ""parents"" of the new pipelines are chosen based on the current
> population of pipelines that are already evaluated. That way, if one
> pipeline takes forever to evaluate, the EA can keep on optimizing based on
> the results from the faster pipelines.
>
> The diagram below sort of communicates those differences, if you stare at
> it long enough.
>
> [image: image]
> <https://user-images.githubusercontent.com/1719223/42972661-084d073c-8b65-11e8-9692-c54c68ff2002.png>
>
> Just something to keep in mind as this branch is worked on. I suspect that
> dask would seamlessly support this huge upgrade to TPOT.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/pull/730#issuecomment-406428702>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AASszCsW9p_wxyPFacT4_sKqxdy2LJv8ks5uIQKCgaJpZM4VRUUv>
> .
>
",would pretty easy use would need use distributed despite name quite lightweight single machine single process single thread would force dependency tornado though randy wrote add currently generational evolutionary algorithm ea model ea fixed population default evaluate proceeding next generation engineering perspective generational problematic single pipeline forever evaluate hold whole process time follow steady state ea steady state ea parallel new chosen based current population already way one pipeline forever evaluate ea keep based faster diagram sort stare long enough image image something keep mind branch worked suspect would seamlessly support huge upgrade reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
406428702,"I should add: Currently, TPOT uses a *generational* evolutionary algorithm (EA) model. That means that the EA keeps some fixed population of pipelines (by default, 100) and has to evaluate all of them before proceeding to the next generation. From an engineering perspective, generational EAs can be problematic because there can be a single pipeline that takes forever to evaluate and that will hold up the whole process. The generational EA cannot proceed until every pipeline is evaluated.

For the longest time, we have wanted to re-engineer TPOT to follow a *steady state* EA. With a steady state EA, the EA still maintains a fixed population of pipelines (still 100 by default), but the pipelines are evaluated in parallel and the ""parents"" of the new pipelines are chosen based on the current population of pipelines that are already evaluated. That way, if one pipeline takes forever to evaluate, the EA can keep on optimizing based on the results from the faster pipelines.

The diagram below sort of communicates those differences, if you stare at it long enough.

![image](https://user-images.githubusercontent.com/1719223/42972661-084d073c-8b65-11e8-9692-c54c68ff2002.png)

Just something to keep in mind as this branch is worked on. I suspect that dask would seamlessly support this huge upgrade to TPOT.",add currently generational evolutionary algorithm ea model ea fixed population default evaluate proceeding next generation engineering perspective generational problematic single pipeline forever evaluate hold whole process generational ea proceed every pipeline time follow steady state ea steady state ea ea still fixed population still default parallel new chosen based current population already way one pipeline forever evaluate ea keep based faster diagram sort stare long enough image something keep mind branch worked suspect would seamlessly support huge upgrade,issue,negative,positive,neutral,neutral,positive,positive
405300146,"I just had a quick test in my Window environment, I got the 2 IndexError as below:
```
======================================================================
ERROR: Assert that _wrapped_cross_val_score return Timeout in a time limit.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""D:\Anaconda3\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""D:\Documents\GitHub\TPOT_Lab\tpot\tests\tpot_tests.py"", line 298, in test_timeout
    timeout=1)
  File ""D:\Anaconda3\lib\site-packages\stopit\utils.py"", line 145, in wrapper
    result = func(*args, **kwargs)
  File ""D:\Documents\GitHub\TPOT_Lab\tpot\tpot\gp_deap.py"", line 453, in _wrapped_cross_val_score
    CV_score = delayed(np.array)(scores)[:, 0]
IndexError: too many indices for array

======================================================================
ERROR: Assert that _wrapped_cross_val_score return -float('inf') with a invalid_pipeline
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""D:\Anaconda3\lib\site-packages\nose\case.py"", line 197, in runTest
    self.test(*self.arg)
  File ""D:\Documents\GitHub\TPOT_Lab\tpot\tests\tpot_tests.py"", line 321, in test_invalid_pipeline
    timeout=300)
  File ""D:\Anaconda3\lib\site-packages\stopit\utils.py"", line 145, in wrapper
    result = func(*args, **kwargs)
  File ""D:\Documents\GitHub\TPOT_Lab\tpot\tpot\gp_deap.py"", line 453, in _wrapped_cross_val_score
    CV_score = delayed(np.array)(scores)[:, 0]
IndexError: too many indices for array
```
Any idea?",quick test window environment got error assert return time limit recent call last file line file line file line wrapper result file line many index array error assert return recent call last file line file line file line wrapper result file line many index array idea,issue,negative,positive,positive,positive,positive,positive
405284770,Hi @mrocklin Thank you for this PR. I rebased it to development branch and I will check it later.,hi thank development branch check later,issue,negative,neutral,neutral,neutral,neutral,neutral
405273300,"Currently, some statistics for evaluated pipelines are saved into `evaluated_individuals_ ` via [this function](https://github.com/EpistasisLab/tpot/blob/a53ba31d9aa62133a25eb137340558ff01481fe8/tpot/base.py#L1092) ",currently statistic saved via function,issue,negative,neutral,neutral,neutral,neutral,neutral
405244623,"@lesshaste sorry for overlooking this issue. For now, there is no official way to disable both operators.

One of my dev branch of TPOT called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `simple_pipeline`, which can disable both `StackingEstimator` and `CombineDFs` if `simple_pipeline=True` (e.g. `TPOTClassifier(simple_pipeline=True)`). But it is noted that this dev branch is not fully tested yet. If you want to try TPOT without `StackingEstimator` and `FeatureUnion`, you may install this branch in your test environment via the command below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
```",sorry issue official way disable one dev branch option disable noted dev branch fully tested yet want try without may install branch test environment via command pip install upgrade,issue,negative,negative,negative,negative,negative,negative
405043210,That would be nice - would allow for simultaneous parallelization of the CV evaluations.,would nice would allow simultaneous parallelization,issue,positive,positive,positive,positive,positive,positive
405043055,"The requests wrt adding a more detailed user guide for TPOT are fair. @ran88dom99, can you please file a separate issue for improving the user guide? I can't guarantee that it will be a top priority, but let's at least get an issue filed for it.

To answer your specific questions:

>Should I set population_size always less than offspring_size?

In general, you want `population_size` to be at least 100, preferably 500+ if you have the time. It's likely not worth changing `offspring_size`. For my 'long' runs, I will often set `population_size` to 1000 and `generations` to 1000. You can also make use of the `early_stopping` parameter to end the TPOT run early if no improvements are being made.

>What to do with small (130) datasets?

Overfitting is a major concern here. I suggest using the 'TPOT light' configuration, as 130 records is likely not enough for the more complex methods used in the default TPOT configuration. Make sure to have a good test set to verify that TPOT is not overfitting on your training set.

>Running TPOT for a day with the right parameters gets cv RMSE from 1 to almost 0 but .44 RMSE on a holdout set. Anything more I can do?

The best solution is to collect more samples. Barring that, you can try the LeaveOneOut `cv` scheme and use the 'TPOT light' configuration if you suspect overfitting is an issue.",detailed user guide fair random please file separate issue improving user guide ca guarantee top priority let least get issue answer specific set always le general want least preferably time likely worth often set also make use parameter end run early made small major concern suggest light configuration likely enough complex used default configuration make sure good test set verify training set running day right almost holdout set anything best solution collect barring try scheme use light configuration suspect issue,issue,positive,positive,positive,positive,positive,positive
404972774,"It looks like we might want to also dive into the `_wrapped_cross_val_score` function and wrap around the `_fit_and_score` function

```python
def _wrapped_cross_val_score(sklearn_pipeline, features, target,
                             cv, scoring_function, sample_weight=None, groups=None):
    """"""Fit estimator and compute scores for a given dataset split.
    Parameters
    ----------
    sklearn_pipeline : pipeline object implementing 'fit'
        The object to use to fit the data.
    features : array-like of shape at least 2D
        The data to fit.
    target : array-like, optional, default: None
        The target variable to try to predict in the case of
        supervised learning.
    cv: int or cross-validation generator
        If CV is a number, then it is the number of folds to evaluate each
        pipeline over in k-fold cross-validation during the TPOT optimization
         process. If it is an object then it is an object to be used as a
         cross-validation generator.
    scoring_function : callable
        A scorer callable object / function with signature
        ``scorer(estimator, X, y)``.
    sample_weight : array-like, optional
        List of sample weights to balance (or un-balanace) the dataset target as needed
    groups: array-like {n_samples, }, optional
        Group labels for the samples used while splitting the dataset into train/test set
    """"""
    sample_weight_dict = set_sample_weight(sklearn_pipeline.steps, sample_weight)

    features, target, groups = indexable(features, target, groups)

    cv = check_cv(cv, target, classifier=is_classifier(sklearn_pipeline))
    cv_iter = list(cv.split(features, target, groups))
    scorer = check_scoring(sklearn_pipeline, scoring=scoring_function)

    try:
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            scores = [_fit_and_score(estimator=clone(sklearn_pipeline),
                                    X=features,
                                    y=target,
                                    scorer=scorer,
                                    train=train,
                                    test=test,
                                    verbose=0,
                                    parameters=None,
                                    fit_params=sample_weight_dict)
                                for train, test in cv_iter]
            CV_score = np.array(scores)[:, 0]
            return np.nanmean(CV_score)
```",like might want also dive function wrap around function python target fit estimator compute given split pipeline object object use fit data shape least data fit target optional default none target variable try predict case learning generator number number evaluate pipeline optimization process object object used generator callable scorer callable object function signature scorer estimator optional list sample balance target optional group used splitting set target target target list target scorer try train test return,issue,positive,positive,positive,positive,positive,positive
404961006,"> My guess is that we'll eventually want to go further in order to avoid recomputation of shared results

Related: dask-searchcv has some caching to avoid repeated tuning for sections of pipelines: https://dask-ml.readthedocs.io/en/latest/hyper-parameter-search.html#avoid-repeated-work. This would be most useful when some of the first elements in a pipeline take a long time and have a couple parameters to tune (e.g., [text feature extraction][1]).

[1]:http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html",guess eventually want go order avoid recomputation related avoid repeated tuning would useful first pipeline take long time couple tune text feature extraction,issue,negative,positive,positive,positive,positive,positive
404935063,"> Would these memory mapping and zero-copy approaches help with parallelism here?

.

>> Generally for workloads like this my recommendations would be to stay  within a single process per node if possible, so zero-copy isn't really a concern. This would differ if you're handling mostly text data. In that case serialization will kill you anyway.
>> Generally speaking my guess is that very few workloads of this type are at the point where zero-copy is something they should worry about.

Because none of these parallelizable algorithms yet require synchronization?
https://en.wikipedia.org/wiki/Bulk_synchronous_parallel


Thanks for the links to the docs.
",would memory help parallelism generally like would stay within single process per node possible really concern would differ handling mostly text data case serialization kill anyway generally speaking guess type point something worry none yet require synchronization thanks link,issue,negative,positive,positive,positive,positive,positive
404856194,Happy to help. Please re-open this issue if you have any more related questions.,happy help please issue related,issue,positive,positive,positive,positive,positive,positive
404854891,Yes that makes sense. Thank you for your reply.,yes sense thank reply,issue,positive,neutral,neutral,neutral,neutral,neutral
404854480,"Sure, here's a basic TPOT workflow:

```Python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=100, population_size=100, # TPOT will evaluate 100x100 pipelines
                      verbosity=2, # TPOT will show a progress bar during optimization
                      n_jobs=-1, # Enable multiprocessing
)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```

More examples available in the docs [here](http://epistasislab.github.io/tpot/examples/).",sure basic python import import import evaluate show progress bar optimization enable print available,issue,positive,positive,positive,positive,positive,positive
404852856,"During the optimization process, TPOT passes whatever is in the `cv` parameter directly to the `cross_val_score` `cv` parameter. Thus, the 'training' score is from whatever cross-validation scheme you passed to the `cv` parameter.

You're correct that the generated Python code from the `export` function is based on a fixed template that uses `train_test_split` for the initial train/test split on the data. We don't have the `export` function generate code with a cross-validation scheme, so there is no configuration for that. You could add a cross-validation scheme yourself by, e.g.,

```Python
training_features, testing_features, training_target, testing_target = \
train_test_split(features, tpot_data['target'].values, random_state=42)

# Score on the training set was:0.7281553398058253
exported_pipeline = make_pipeline(
    FeatureAgglomeration(affinity=""l1"", linkage=""average""),
    DecisionTreeClassifier(criterion=""gini"", max_depth=10, min_samples_leaf=5, min_samples_split=11)
)

cross_val_score(exported_pipeline, training_features, training_target, cv=LeaveOneOut())
```

Essentially, the generated Python code is meant only to be a starting point for you to integrate the pipeline into your own ML workflow. The `exported_pipeline` (and related imports) are the important part.",optimization process whatever parameter directly parameter thus score whatever scheme parameter correct python code export function based fixed template initial split data export function generate code scheme configuration could add scheme python score training set average essentially python code meant starting point integrate pipeline related important part,issue,positive,positive,neutral,neutral,positive,positive
404662962,"The first thing to try is probably just using the `joblib.parallel_backend('dask')` solution and see how that performs on a distributed system.  My guess is that we'll eventually want to go further in order to avoid recomputation of shared results and such, but it would be good to have a baseline.

Is there a standard workflow to try this out on?  

This might be a good blogpost around the dask-joblib integration being useful for things outside of just Scikit-Learn.

Also cc @stsievert who might find this conversation interesting.",first thing try probably solution see distributed system guess eventually want go order avoid recomputation would good standard try might good around integration useful outside also might find conversation interesting,issue,positive,positive,positive,positive,positive,positive
404659736,"TPOT pipeline parallelization is primarily done [here](https://github.com/EpistasisLab/tpot/blob/12eba6779793f77ae853476da8777397d22df502/tpot/base.py#L1168:L1178), where we're currently using scikit-learn's port of joblib.

Some additional considerations:

* **The parallelized evaluations need to be interruptible**, both because TPOT has options to stop evaluation on a per-pipeline basis (e.g., each pipeline gets 5 minutes to evaluate) and options to stop the optimization process after a fixed amount of time (e.g., TPOT gets 1 hour to evaluate as many pipelines as possible). Preferably the evaluations can be self-interruptible.

* **The parallelization procedure needs to be optional**, currently achieved with the `if` statement checking whether `n_jobs==1`.

* **The parallelization procedure needs to be customizable by the user** through (preferably) 1 option in the instantiation of `TPOTClassifier`/`TPOTRegressor`. Currently it's customizable with the `n_jobs` parameter. I am open to opening more customization options, if useful.",pipeline parallelization primarily done currently port additional need interruptible stop evaluation basis pipeline evaluate stop optimization process fixed amount time hour evaluate many possible preferably parallelization procedure need optional currently statement whether parallelization procedure need user preferably option currently parameter open opening useful,issue,negative,positive,positive,positive,positive,positive
404652844,Is there an obvious central place within TPOT from where most computation is planned that would make sense for someone familiar with dask to look at?,obvious central place within computation would make sense someone familiar look,issue,negative,positive,positive,positive,positive,positive
404652441,"> Would these memory mapping and zero-copy approaches help with parallelism here?

Generally for workloads like this my recommendations would be to stay within a single process per node if possible, so zero-copy isn't really a concern.  This would differ if you're handling mostly text data.  In that case serialization will kill you anyway.  Generally speaking my guess is that very few workloads of this type are at the point where zero-copy is something they should worry about.

",would memory help parallelism generally like would stay within single process per node possible really concern would differ handling mostly text data case serialization kill anyway generally speaking guess type point something worry,issue,negative,positive,positive,positive,positive,positive
404284346,"I find that in tpot/tpot/export_utils.py
 the periodically generated output file is based off a template string which has train_test_split statically defined, but the score is taken from the pipeline.

Could someone point me to where TPOT would assign the cross validation scheme, and close this issue?",find periodically output file based template string statically defined score taken pipeline could someone point would assign cross validation scheme close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
403977925,"@weixuanfu any specific pointer in the code base to look at ?  
I've started but I'm not very familiar with tpot and I haven't yet found where this is happening. I was hoping that `_pop` attribute of the `TPOTClassifier()` object would contain useful information about the population and hence the selected features as in [here](https://github.com/EpistasisLab/tpot/blob/a53ba31d9aa62133a25eb137340558ff01481fe8/tpot/base.py#L610)...Is it directly in `eaMuPlusLambda` that I should try to modify `pop`attributes to retain informations about the features ?

Any help would be greatly appreciated, thanks !",specific pointer code base look familiar yet found happening attribute object would contain useful information population hence selected directly try modify pop retain help would greatly thanks,issue,positive,positive,neutral,neutral,positive,positive
403936348,"@IamAVB, sorry for overlooking this issue. Yes, you can add additional feature selection operators into configuration and use it via `config_dict` parameter.",sorry issue yes add additional feature selection configuration use via parameter,issue,negative,negative,negative,negative,negative,negative
403935844,@dsleo Thank you. The feature selection/construction performs within the scikit-learn pipelines which are generated via GP in TPOT. ,thank feature within via,issue,negative,neutral,neutral,neutral,neutral,neutral
403931058,"Hello,

Thanks for the very nice library. I would love to work on a adding this as a feature to the API. Could you point me to where the selection/construction of features is happening ?
",hello thanks nice library would love work feature could point happening,issue,positive,positive,positive,positive,positive,positive
403833434,"@weixuanfu great news, thanks for this answer.",great news thanks answer,issue,positive,positive,positive,positive,positive,positive
403832372,"Hmm, it may take a while for each pipeline. You could try 'TPOT light' configuration instead or test it without jupyther. There is a unsolved issue that TPOT may stuck in jupyter notebook #645  ",may take pipeline could try light configuration instead test without unsolved issue may stuck notebook,issue,negative,positive,positive,positive,positive,positive
403814156,"Hey weixuanfu, Can you throw some light upon them please?",hey throw light upon please,issue,negative,positive,positive,positive,positive,positive
403800525,Two new operators related to this issue were merged to dev branch.,two new related issue dev branch,issue,negative,positive,neutral,neutral,positive,positive
403274771,"> Pretty unlikely that two separate instances would both have defective RAM

Really? I don't think so. Do a memory test.

PS: They seem to use ECC RAM. Nevertheless I would try to exclude faulty RAM as a reason, although I may be a little biased now.",pretty unlikely two separate would defective ram really think memory test seem use ram nevertheless would try exclude faulty ram reason although may little,issue,negative,negative,neutral,neutral,negative,negative
403250483,"Am also running running into this issue on an AWS instance with 137 Gb RAM and 72 CPUs. I created a new instance with the same specs and ran into the same segfault after three generations. Pretty unlikely that two separate instances would both have defective RAM.

```
[ec2-user@ip-172-31-20-193 ~]$ python classification.py
/usr/local/lib/python2.7/site-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.
  ""module. Expect this to be very slow."", ImportWarning)
/usr/local/lib64/python2.7/site-packages/scipy/spatial/__init__.py:96: ImportWarning: Not importing directory '/usr/local/lib64/python2.7/site-packages/scipy/spatial/qhull': missing __init__.py
  from .qhull import *
/usr/local/lib64/python2.7/site-packages/scipy/optimize/_minimize.py:37: ImportWarning: Not importing directory '/usr/local/lib64/python2.7/site-packages/scipy/optimize/lbfgsb': missing __init__.py
  from .lbfgsb import _minimize_lbfgsb
/usr/local/lib/python2.7/site-packages/tpot/operator_utils.py:1: ImportWarning: Not importing directory '/home/ec2-user/xgboost': missing __init__.py
  # -*- coding: utf-8 -*-
/usr/local/lib/python2.7/site-packages/xgboost-0.72-py2.7.egg/xgboost/training.py:11: ImportWarning: Not importing directory '/usr/local/lib/python2.7/site-packages/xgboost-0.72-py2.7.egg/xgboost/rabit': missing __init__.py
  from . import rabit
Generation 1 - Current best internal CV score: 0.870559367619
Generation 2 - Current best internal CV score: 0.875569109325
Optimization Progress:   3%|███▏                                                                                                        | 
300/10100 [03:28<40:30:15, 14.88s/pipeline]Segmentation fault
```",also running running issue instance ram new instance spec ran three pretty unlikely two separate would defective ram python falling back python version hypervolume module expect slow module expect slow directory missing import directory missing import directory missing directory missing import generation current best internal score generation current best internal score optimization progress segmentation fault,issue,positive,positive,neutral,neutral,positive,positive
401575281,Thanks for the fast response! So basically I can define a tpot_config with all the classifiers and selectors that I want to try and pass it to the TPOTClassifier and than optimize it. Thanks for the clarification ,thanks fast response basically define want try pas optimize thanks clarification,issue,positive,positive,positive,positive,positive,positive
401557287,"yes, you'll want to modify the corresponding config directory to remove the desired modeling technique.

The standard dictionaries can be found here: https://github.com/EpistasisLab/tpot/tree/master/tpot/config",yes want modify corresponding directory remove desired modeling technique standard found,issue,positive,neutral,neutral,neutral,neutral,neutral
400350390,"Please check `scoring` parameter in [TPOT API](https://epistasislab.github.io/tpot/api/)

> scoring: string or callable, optional (default='accuracy')
> Function used to evaluate the quality of a given pipeline for the classification problem. The following built-in scoring functions can be used: 
> 
> 'accuracy', 'adjusted_rand_score', 'average_precision', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'neg_log_loss','precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc' 
> 
> If you would like to use a custom scorer, you can pass the callable object/function with signature scorer(estimator, X, y). 
> 
> If you would like to use a metric function, you can pass the callable function to this parameter with the signature score_func(y_true, y_pred). TPOT assumes that any function with ""error"" or ""loss"" in the function name is meant to be minimized, whereas any other functions will be maximized. This scoring type was deprecated in version 0.9.1 and will be removed in version 0.11. 
> 
> See the section on scoring functions for more details. ",please check scoring parameter scoring string callable optional function used evaluate quality given pipeline classification problem following scoring used would like use custom scorer pas callable signature scorer estimator would like use metric function pas callable function parameter signature function error loss function name meant whereas scoring type version removed version see section scoring,issue,negative,neutral,neutral,neutral,neutral,neutral
400080137,Accidentally merged dev into master... will resubmit pull request,accidentally dev master resubmit pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
399889790,"I've been trying to understand how pipeline elements work with TPOT/DEAP (it seems almost magical from the outside). I would like to ask some questions about how your class works:

I see from your class definition for [DatasetSelector](https://github.com/weixuanfu/tpot/blob/template_opt/tpot/builtins/dataset_selector.py) you made it inherit from two sklearn superclasses: `class DatasetSelector(BaseEstimator, TransformerMixin):` I have learned from reading the TPOT code that `TransformerMixin` is a superclass that TPOT uses to identify transforming nodes (operators) in the pipeline, so I suppose that is why DatasetSelector needs to inherit from `TransformerMixin`? But I don't know why `BaseEstimator` is also necessary, maybe it's for sklearn?

I see the `DatasetSelector` has both fit() and transform() methods defined, in which fit() determines which of the currently-available features passed to it will be selected, and transform() selects this subset. What would happen if fit() was not defined? Do all pipeline elements have to have a fit() function?

I am curious what the entry in the `config-dictionary` would be for your class would be? Would it look something like the following?:
```python
    'tpot.builtins.DatasetSelector': {
        'subset_dir': ['path/to/directory']
        'sel_subset_fname': ['subsets_1.csv', 'subsets_2.csv', 'subsets_3.csv']
    },
```

I also have some questions about how TPOT uses this class:

How does TPOT mutate, cross, mate, etc using this dictionary? My current understanding is mutation switches one node in a pipeline tree with a randomly-generated one, cross switches subtrees between trees, and I mating (?) does something similar.

In contrast, I want a list of _n booleans_ (`[1, 0, 0, 1, 0 ....]`) to control the inclusion of _n subsets_. I want crossover/mutation/genetic-stuff to occur to this list of booleans, using it as ""DNA"", if that makes sense. I suppose this is very different than the genetic operators currently defined in TPOT.

So maybe my idea is difficult to implement with TPOT. I'm working on it as an independent deap project currently, in which only the selection is optimized, while the rest of the pipeline is fixed.

Thanks.",trying understand pipeline work almost magical outside would like ask class work see class definition made inherit two class learned reading code superclass identify transforming pipeline suppose need inherit know also necessary maybe see fit transform defined fit selected transform subset would happen fit defined pipeline fit function curious entry would class would would look something like following python also class mutate cross mate dictionary current understanding mutation one node pipeline tree one cross mating something similar contrast want list control inclusion want occur list sense suppose different genetic currently defined maybe idea difficult implement working independent project currently selection rest pipeline fixed thanks,issue,positive,positive,neutral,neutral,positive,positive
399701757,"This still happens when `n_jobs` is set to somethig bigger than 1. I have tested it today several times.
Same code with  `n_jobs=1` works but when  `n_jobs=n` it stays forever in `Optimization Progress: 0%`.
  
I have tried to add the `multiprocessing` forkserver  explained in the documentation but I actually get an error

This 
```python
import multiprocessing
from tpot import TPOTRegressor
multiprocessing.set_start_method('forkserver')

if __name__ == '__main__':
    #mycode
```

returns 

```bash
Traceback (most recent call last):
  File ""test_tpot_santander.py"", line 3, in <module>
    multiprocessing.set_start_method('forkserver')
  File ""/Users/davidbuchaca1/anaconda3/lib/python3.6/multiprocessing/context.py"", line 242, in set_start_method
    raise RuntimeError('context has already been set')
RuntimeError: context has already been set
```

Nevertheless

```python
import multiprocessing
multiprocessing.set_start_method('forkserver')
from tpot import TPOTRegressor
if __name__ == '__main__':
    #mycode
```

Does not return any error but the same behaviour occurs. Nothing happens (even though CPU goes to 100 all threads for a long time).

Probably there is something weird in my multiprocessing in OSX because in Ubuntu It works fine.",still set bigger tested today several time code work stay forever optimization progress tried add documentation actually get error python import import bash recent call last file line module file line raise already set context already set nevertheless python import import return error behaviour nothing even though go long time probably something weird work fine,issue,negative,negative,neutral,neutral,negative,negative
399452746,TPOT has this [StandardScaler](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py#L158) as well as other data transformer in the default configuration and uses GP to check if the data need data transformation in pipeline.,well data transformer default configuration check data need data transformation pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
399337754,"Also I found that there currently is no check for invalid sample_weights. If there is a single `NaN`, tpot fails later in `_update_top_pipeline`, when it finds that all the pipelines have `-inf` scores.",also found currently check invalid single nan later,issue,negative,negative,neutral,neutral,negative,negative
399101782,The issue was fixed and fixes were pushed to master branch. Thanks again.,issue fixed master branch thanks,issue,negative,positive,positive,positive,positive,positive
399065897,Thank you for catching this. I will fix it soon.,thank catching fix soon,issue,negative,positive,positive,positive,positive,positive
399053282,The `SubsetSelector` in TPOT (we called it `DatasetSelector`) is still under development/testing for now. The method is similar as you mentioned in the issue. But we made this selector not as a general feature selector but as selector for GWAS or bioinformatics for taking tons of subsets. But it could be used in general case. I will add more details in docs in that [branch](https://github.com/weixuanfu/tpot/blob/template_opt/tpot/builtins/dataset_selector.py).,still method similar issue made selector general feature selector selector taking could used general case add branch,issue,negative,positive,neutral,neutral,positive,positive
398857042,"FYI that looks like a massively overfit model. It probably outputs a flat line on new data because that is the last value is learned to predict at the final time point. You definitely need more features to predict stock price here, but that is outside the purview of TPOT support.

I suggest Googling ""python stock price prediction"" and there will be dozens of articles covering the topic, including how to integrate additional features into the predictive model.",like massively overfit model probably flat line new data last value learned predict final time point definitely need predict stock price outside purview support suggest python stock price prediction covering topic integrate additional predictive model,issue,positive,positive,neutral,neutral,positive,positive
398856077,Stock price can’t be predicted by date only. I think you need merge more features (if there are any from that API) with date.,stock price date think need merge date,issue,negative,neutral,neutral,neutral,neutral,neutral
398853852,"I am afraid not, how do I incorporate more features in stock data?
",afraid incorporate stock data,issue,negative,negative,negative,negative,negative,negative
398852362,"The model fits the training data, but on unseen data it is linear
",model training data unseen data linear,issue,negative,neutral,neutral,neutral,neutral,neutral
398850241,Gotcha. Is there a pattern based on the date in the time series? Otherwise you'll likely need more features to build an effective predictive model.,pattern based date time series otherwise likely need build effective predictive model,issue,negative,positive,positive,positive,positive,positive
398830180,"How many samples do you have in the time series? How many features?

My next suggestion is to give TPOT more time to explore more pipelines. That entails increasing the `generations` and `population_size` parameters. I recommend 100 and 100 for both parameters to start with, and give TPOT plenty of time (and patience :-) ) to work.",many time series many next suggestion give time explore increasing recommend start give plenty time patience work,issue,negative,positive,positive,positive,positive,positive
398826991,"tpot = TPOTRegressor(generations=5, population_size=50,verbosity=2, cv=TimeSeriesSplit(n_splits=15))

This is the way I am calling the regressor, and then I am using tpot.fitted_pipeline_ for making predictions...",way calling regressor making,issue,negative,neutral,neutral,neutral,neutral,neutral
398825275,"Thank you, @joseortiz3. We look forward to reviewing the docs PR.

TPOT provides access to a `scoring` parameter when creating a TPOT instance. With that parameter, you can use any [built-in sklearn metric](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics) or even custom metrics (see the [TPOT API](https://epistasislab.github.io/tpot/api/) for details). For example, the [accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) function in sklearn takes a `sample_weight` parameter.",thank look forward access scoring parameter instance parameter use metric even custom metric see example function parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
398748282,You could try to check the example in the [link](https://epistasislab.github.io/scikit-rebate/using/#acquiring-feature-importance-scores) to get importance scores from MultiSURF or other relief-based feature selection algorithms in the optimized pipeline from TPOT if the pipeline is a simply linear pipeline. I answered this question in details via replying your email.,could try check example link get importance feature selection pipeline pipeline simply linear pipeline question via,issue,negative,neutral,neutral,neutral,neutral,neutral
398616929,"I can submit a PR for the documentation, now that you guys have explained it to me.

Now I need to figure out how to change the TPOT scoring function to use sample_weights. Before I begin this journey, any tips?",submit documentation need figure change scoring function use begin journey,issue,negative,neutral,neutral,neutral,neutral,neutral
398507695,"AFAIK no operator in sklearn uses `sample_weights` without having a `fit` function. In fact, we couldn't use an operator in sklearn if it didn't have a `fit` function because it wouldn't work in a sklearn pipeline. Furthermore, because we use sklearn pipelines as our solution representation, sklearn pipeline only use `fit` and `prediction`/`transform` separately.

>Also I wanted to know if sample_weights is used in tpot's scoring function, which determines preferences between pipelines?

No, the `sample_weights` are not used in the scoring function unless you separately tell TPOT to use a scoring function that takes sample weights into account. Even in sklearn, setting `sample_weights` affects only the training process of the algorithms, not their evaluation.

👍 to expanding the docs to expand the description of `sample_weights` to address some of the questions in this thread. Are you willing to submit that PR, @joseortiz3?",operator without fit function fact could use operator fit function would work pipeline furthermore use solution representation pipeline use fit prediction transform separately also know used scoring function used scoring function unless separately tell use scoring function sample account even setting training process evaluation expanding expand description address thread willing submit,issue,negative,positive,positive,positive,positive,positive
398506168,"Because TPOT adheres to the scikit-learn data requirements, we do not support datasets that have a varying number of features per sample.

However, it is possible to fit such a dataset into the scikit-learn data requirements by creating a column for every feature for every sample and simply placing a missing value (`NaN`) in a column when a sample does not have a value for that feature. From there, TPOT will use median imputation to ""guess"" the best value for that feature when a sample is missing that value.

You can also format your dataset as described above, but manually perform the imputation yourself. If you have a better sense of a numerical value that should be present when a feature is not available for a sample, this is a better option. One common example of this approach with text data is setting the feature = 0 when a word is not present in a sentence, and setting the feature = 1 when a word is present in a sentence.",data support number per sample however possible fit data column every feature every sample simply missing value nan column sample value feature use median imputation guess best value feature sample missing value also format manually perform imputation better sense numerical value present feature available sample better option one common example approach text data setting feature word present sentence setting feature word present sentence,issue,positive,positive,positive,positive,positive,positive
398411177,"Thank you for your idea about sample weight.

For 2 questions, I think all the operators in TPOT should follow the scikit-learn API which means that they should has `fit`. But I will double-check it.
",thank idea sample weight think follow fit,issue,positive,positive,positive,positive,positive,positive
398407860,"Hmm, for now TPOT cannot handle the samples with different number of measurements. You may try some sample selection methods first.  ",handle different number may try sample selection first,issue,negative,positive,positive,positive,positive,positive
397350950,"Two more reasons for using GP (and population-based EC in general) are:

1) GP and EC allow for flexible solution representations, which means TPOT pipelines can vary in size and complexity during the optimization process. Not all optimization algorithms easily support this feature, which has become integral to TPOT.

2) Population-based optimizers allow us to use Pareto optimization, which optimizes solutions across multiple criteria simultaneously. In TPOT, the optimizer maximizes prediction accuracy while minimizing pipeline complexity, and at the end provides the Pareto front showing the discovered optimal trade-off between those criteria. Most other optimization techniques do not offer this feature.

So yes, GP and EC are old ideas, but so is gradient descent. New doesn't imply better, and in fact old ideas are often more refined (see: deep learning).",two general allow flexible solution vary size complexity optimization process optimization easily support feature become integral allow u use optimization across multiple criterion simultaneously prediction accuracy pipeline complexity end front showing discovered optimal criterion optimization offer feature yes old gradient descent new imply better fact old often refined see deep learning,issue,positive,positive,positive,positive,positive,positive
396908585,"We found GP worked better in most of our benchmarks, mostly included in pmlb (which is mentioned by @rhiever in that related issue).",found worked better mostly included related issue,issue,negative,positive,positive,positive,positive,positive
396906693,@weixuanfu I suppose that issue is not for what I asked. I am looking for any particular reason to use GP to optimize the model,suppose issue looking particular reason use optimize model,issue,negative,positive,positive,positive,positive,positive
396798215,I know pareto front in general. Where Can I find materials to read how pareto front is used in TPOT exactly? ,know front general find read front used exactly,issue,negative,positive,positive,positive,positive,positive
396630950,"Not actually. Although looking into other comments, and importing `multiprocessing` helped to run the training, although the time to update the bar is very long (maybe it waits all the processes finish benchmarking or something). Like for 5 minutes it’s 0 model, then it goes to 1%, 10%, …
On Jun 12, 2018, 5:21 PM +0200, Randy Olson <notifications@github.com>, wrote:
> @HamedMP, have you tried running it with n_jobs!=1 on the command line?
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
",actually although looking run training although time update bar long maybe finish something like model go randy wrote tried running command line reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
396628915,"@HamedMP, have you tried running TPOT with `n_jobs`!=1 on the command line?",tried running command line,issue,negative,neutral,neutral,neutral,neutral,neutral
396622915,"Thanks for clarifying the doubt. So I understand that, TPOT provides 4 feature selector operators which are mentioned in 'TPOT MDR' built in configuration. Will I be able to add additional feature selection operators (as custom operators) which can be used to create more comprehensive feature set in population?.  ",thanks doubt understand feature selector built configuration able add additional feature selection custom used create comprehensive feature set population,issue,positive,positive,positive,positive,positive,positive
396597243,I think this issue is for notebook only. I will try to find a work around for this.,think issue notebook try find work around,issue,negative,neutral,neutral,neutral,neutral,neutral
396594888,"I try to set `n_jobs` to any value other than 1, like -1, 2, 4, ..., the cpu core usage goes to 100% but no progress is shown on the notebook. And no result get returned after waiting for hours, tested with latest version of tpot, both on mac and ubuntu I have the same problem. It would be good to parallelize training. 
Found related to #645 ",try set value like core usage go progress shown notebook result get returned waiting tested latest version mac problem would good parallelize training found related,issue,positive,positive,positive,positive,positive,positive
396580409,"We put a set of feature selector into TPOT default configuration. TPOT can randomly use those feature selectors (even combine the selected features in different selectors in tree structure) for optimizing accuracy. But so far, we are working/evaluating on a new template function to allow user to specify pipeline structure for feature selection.",put set feature selector default configuration randomly use feature even combine selected different tree structure accuracy far new template function allow user specify pipeline structure feature selection,issue,negative,negative,neutral,neutral,negative,negative
396516800,"I understood that TPOT involves hyperparameters, different classification models while creating pipelines using GP. But does it involve features too? means does mutation, crossover operators are applied on feature sets too while creating each new pipeline?.
",understood different classification involve mutation crossover applied feature new pipeline,issue,negative,positive,neutral,neutral,positive,positive
396241611,"We fixed random state within pipeline to ensure that the fitness metrics is the same in every generation. Also, the training data should be the same over generations in TPOT. Saving the fitness metrics for evaluated pipeline is for reducing computation time in optimization process. ",fixed random state within pipeline ensure fitness metric every generation also training data saving fitness metric pipeline reducing computation time optimization process,issue,positive,negative,negative,negative,negative,negative
396239262,Thank you for catching this. I will merge this soon,thank catching merge soon,issue,negative,positive,positive,positive,positive,positive
396062559,It seems that TPOT strongly assumes that data and scoring context remain constant. I'll have to go with subclassing.,strongly data scoring context remain constant go,issue,negative,positive,positive,positive,positive,positive
396061362,Fixed a silly mistake. One thing to note is that this PR currently does not re-evaluate the pareto front after every generation.,fixed silly mistake one thing note currently front every generation,issue,negative,negative,negative,negative,negative,negative
395786697,Is there any updates on this ? When I try to pickle TPOTClassifier I get this error : `_pickle.PicklingError: Can't pickle <class 'tpot.operator_utils.FeatureAgglomeration__affinity'>: attribute lookup FeatureAgglomeration__affinity on tpot.operator_utils failed`,try pickle get error ca pickle class attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
395632097,"Thanks @weixuanfu and @rhiever! This answered my question.



For anyone with similar needs I've included some code using the minimal working example on the MNIST data from the tpot documentation.

```
import pandas as pd
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```

Data frame of all models and sort by best CV error:

```
my_dict = list(tpot.evaluated_individuals_.items())

model_scores = pd.DataFrame()
for model in my_dict:
    model_name = model[0]
    model_info = model[1]
    cv_score = model[1].get('internal_cv_score')  # Pull out cv_score as a column (i.e., sortable)
    model_scores = model_scores.append({'model': model_name,
                                        'cv_score': cv_score,
                                        'model_info': model_info,},
                                       ignore_index=True)

model_scores = model_scores.sort_values('cv_score', ascending=False)
```

E.g., top 5 rows of output

  | cv_score | model | model_info
-- | -- | -- | --
276 | 0.986708 | KNeighborsClassifier(DecisionTreeClassifier(input_matrix,   DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=9,   DecisionTreeClassifier__min_samples_leaf=3,   DecisionTreeClassifier__min_samples_split=14), KNeighborsClassifier__n_neighbors=5,   KNeighborsClassifier__p=2, KNeighborsClassifier__weights=uniform) | {'generation': 'INVALID', 'mutation_count': 3, 'crossover_count': 0,   'predecessor': ('KNeighborsClassifier(input_matrix,   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=uniform)',), 'operator_count': 2, 'internal_cv_score':   0.9867084418027815}
251 | 0.986686 | KNeighborsClassifier(DecisionTreeClassifier(input_matrix,   DecisionTreeClassifier__criterion=entropy,   DecisionTreeClassifier__max_depth=3,   DecisionTreeClassifier__min_samples_leaf=18,   DecisionTreeClassifier__min_samples_split=16), KNeighborsClassifier__n_neighbors=5,   KNeighborsClassifier__p=2, KNeighborsClassifier__weights=uniform) | {'generation': 'INVALID', 'mutation_count': 1, 'crossover_count': 2,   'predecessor': ('KNeighborsClassifier(DecisionTreeClassifier(input_matrix,   DecisionTreeClassifier__criterion=entropy,   DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=18,   DecisionTreeClassifier__min_samples_split=16),   KNeighborsClassifier__n_neighbors=19, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=uniform)', 'KNeighborsClassifier(input_matrix,   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=distance)'), 'operator_count': 2,   'internal_cv_score': 0.9866863255542502}
244 | 0.986686 | KNeighborsClassifier(GradientBoostingClassifier(input_matrix,   GradientBoostingClassifier__learning_rate=1.0,   GradientBoostingClassifier__max_depth=1,   GradientBoostingClassifier__max_features=0.15000000000000002,   GradientBoostingClassifier__min_samples_leaf=20,   GradientBoostingClassifier__min_samples_split=11,   GradientBoostingClassifier__n_estimators=100,   GradientBoostingClassifier__subsample=0.7500000000000001),   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=uniform) | {'generation': 'INVALID', 'mutation_count': 3, 'crossover_count': 0,   'predecessor': ('KNeighborsClassifier(input_matrix,   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=uniform)',), 'operator_count': 2, 'internal_cv_score':   0.9866863255542502}
202 | 0.986686 | KNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=5,   KNeighborsClassifier__p=2, KNeighborsClassifier__weights=uniform) | {'generation': 'INVALID', 'mutation_count': 2, 'crossover_count': 0,   'predecessor': ('KNeighborsClassifier(RFE(input_matrix,   RFE__ExtraTreesClassifier__criterion=gini,   RFE__ExtraTreesClassifier__max_features=0.1,   RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001),   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=uniform)',), 'operator_count': 1,   'internal_cv_score': 0.9866863255542502}
89 | 0.986678 | KNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=5,   KNeighborsClassifier__p=2, KNeighborsClassifier__weights=distance) | {'generation': 'INVALID', 'mutation_count': 1, 'crossover_count': 0,   'predecessor': ('KNeighborsClassifier(RFE(input_matrix,   RFE__ExtraTreesClassifier__criterion=gini,   RFE__ExtraTreesClassifier__max_features=0.1,   RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001),   KNeighborsClassifier__n_neighbors=5, KNeighborsClassifier__p=2,   KNeighborsClassifier__weights=distance)',), 'operator_count': 1,   'internal_cv_score': 0.9866781855461101}

",thanks question anyone similar need included code minimal working example data documentation import import import import print data frame sort best error list model model model model pull column sortable top output model,issue,positive,positive,positive,positive,positive,positive
395464146,"You can also access the `pareto_front_fitted_pipelines_` attribute of TPOT, which provides a list of all pipelines along the Pareto front. This Pareto front shows the trade-off between pipeline predictive performance (e.g., accuracy) and pipeline complexity (i.e., the number of steps in the pipeline).

See the [TPOT API](http://epistasislab.github.io/tpot/api/) for more info on the `pareto_front_fitted_pipelines_` attribute.",also access attribute list along front front pipeline predictive performance accuracy pipeline complexity number pipeline see attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
395426224,"For now, tpot will save all pipelines into `evaluated_individuals_` attributes in [TPOT API](https://epistasislab.github.io/tpot/api/). Similar issue #516 

evaluated_individuals_: Python dictionary
Dictionary containing all pipelines that were evaluated during the pipeline optimization process, where the key is the string representation of the pipeline and the value is a tuple containing (# of steps in pipeline, accuracy metric for the pipeline). 

This attribute is primarily for internal use, but may be useful for looking at the other pipelines that TPOT evaluated.",save similar issue python dictionary dictionary pipeline optimization process key string representation pipeline value pipeline accuracy metric pipeline attribute primarily internal use may useful looking,issue,positive,positive,neutral,neutral,positive,positive
395184899,"@weixuanfu,
Thank you for your encouragement.

I did built a custom_config with only **lightgbm** to my local config_dict. Tried IRIS.ipynb classification with it.

Seems to run well, but I have some newbie questions.

lightgbm.LGBMClassifier has a parameter **objective** which has possible values [‘binary’, ‘multiclass’ ] 

lightgbm.LGBMRegressor has parameter **objective** which has only one value [‘regression’ ] 

 lightgbm.LGBMRanker  has parameter **objective** which has only one value [‘lambdarank’]

lightgbm.LGBMClassifier needs a value for a parameter num_class which should be inherited from set(y_train) (obviously greater than 1) when **objective**  is **‘multiclass’** 

How do I manage these dependancies? I added num_class : [3] in the config for this test with IRIS dataset as a brute force method.

How can I default num_class from unique values of y_train?

I see that xgboost package also has a parameter called objective which for IRIS dataset would need a value  **'multi:softmax'**",thank encouragement built local tried classification run well parameter objective possible binary parameter objective one value regression parameter objective one value need value parameter set obviously greater objective manage added test iris brute force method default unique see package also parameter objective iris would need value,issue,positive,positive,neutral,neutral,positive,positive
395049500,"TPOT uses 5-fold CV accuracy to evaluate pipeline accuracy by default.
There is a “cv” parameter that allows you to change that to whatever
validation strategy you prefer.

On Wed, Jun 6, 2018 at 4:42 AM cpereir1 <notifications@github.com> wrote:

> When running the tpot pipeline optimization search, the output file
> includes a comment for the score on the training set, as shown in the
> figure below.
>
> [image: image]
> <https://user-images.githubusercontent.com/23639131/41030349-6df101f0-697e-11e8-8996-c49392593337.png>
>
> From literature I realize that the score on the training set is always
> very optimistic, and ideally, a model should be chosen based on its
> performance on the testing set.
> But is tpot choosing the best pipeline based on training set accuracy?
> What is the reason for that?
>
> If so, the is the overfitting minimized in some way, such as by using
> cross validation? How many folds?
>
> Thanks in advance.
>
> BR
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/702>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t0-A-49zabxP2tSHciNYRRvFs6QGks5t56PVgaJpZM4UcU7D>
> .
>
-- 
Cheers,

Randal S. Olson, Ph.D.

E-mail: rso@randalolson.com | Twitter: @randal_olson
<https://twitter.com/randal_olson>
http://www.randalolson.com
",accuracy evaluate pipeline accuracy default parameter change whatever validation strategy prefer wed wrote running pipeline optimization search output file comment score training set shown figure image image literature realize score training set always optimistic ideally model chosen based performance testing set choosing best pipeline based training set accuracy reason way cross validation many thanks advance thread reply directly view mute thread twitter,issue,positive,positive,positive,positive,positive,positive
394878671,"I think  
from sklearn.model_selection import train_test_split
is going to work out.......... ",think import going work,issue,negative,neutral,neutral,neutral,neutral,neutral
393504277,"Ok, I understand what happened.

The `scoring = 'precision' ` in your codes is for binary targets only. Please try 'precision_macro' or other scores instead. It is documented in the [link](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) and please check suffixes of f1.




On May 31, 2018 at 7:22 AM, <prabhant<mailto:notifications@github.com>> wrote:


Hi, I tried precision, recall on iris dataset

tpot = TPOTClassifier(generations=2, population_size=20, verbosity=3, scoring = 'precision',config_dict='TPOT light')
tpot.fit(X_train, y_train)


—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/700#issuecomment-393499161>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KmenC6IvThK93mvpeWOKzOOqUBrEks5t39JTgaJpZM4UUzxJ>.
",understand scoring binary please try instead link please check may wrote hi tried precision recall iris scoring light reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
393499161,"Hi, I tried precision, recall on iris dataset
```
tpot = TPOTClassifier(generations=2, population_size=20, verbosity=3, scoring = 'precision',config_dict='TPOT light')
tpot.fit(X_train, y_train)
```",hi tried precision recall iris scoring light,issue,negative,positive,positive,positive,positive,positive
393489203,"Which scoring method? Could you please provide a example to reproduce the error?


On May 31, 2018 at 6:30 AM, <prabhant<mailto:notifications@github.com>> wrote:


Errors in multiclass scenario while using TPOT with another scoring measure than accuracy

Context of the issue

I'm getting this error while using tpot on iris data and using any other scoring measure than accuracy.

---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
/usr/local/lib/python3.5/dist-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    629                     verbose=self.verbosity,
--> 630                     per_generation_function=self._check_periodic_pipeline
    631                 )

/usr/local/lib/python3.5/dist-packages/tpot/gp_deap.py in eaMuPlusLambda(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, pbar, stats, halloffame, verbose, per_generation_function)
    243         if per_generation_function is not None:
--> 244             per_generation_function()
    245

/usr/local/lib/python3.5/dist-packages/tpot/base.py in _check_periodic_pipeline(self)
    916         """"""
--> 917         self._update_top_pipeline()
    918         if self.periodic_checkpoint_folder is not None:

/usr/local/lib/python3.5/dist-packages/tpot/base.py in _update_top_pipeline(self)
    708             if not self._optimized_pipeline:
--> 709                 raise RuntimeError('There was an error in the TPOT optimization '
    710                                    'process. This could be because the data was '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.

During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-68-b3eb03c9c133> in <module>()
      1 tpot = TPOTClassifier(generations=2, population_size=20, verbosity=3, scoring = em,config_dict='TPOT light')
----> 2 tpot.fit(X_train, y_train)
      3 print(tpot.score(X_test, y_test))

/usr/local/lib/python3.5/dist-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    660                     # raise the exception if it's our last attempt
    661                     if attempt == (attempts - 1):
--> 662                         raise e
    663             return self
    664

/usr/local/lib/python3.5/dist-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    651                         self._pbar.close()
    652
--> 653                     self._update_top_pipeline()
    654                     self._summary_of_best_pipeline(features, target)
    655                     # Delete the temporary cache before exiting

/usr/local/lib/python3.5/dist-packages/tpot/base.py in _update_top_pipeline(self)
    707
    708             if not self._optimized_pipeline:
--> 709                 raise RuntimeError('There was an error in the TPOT optimization '
    710                                    'process. This could be because the data was '
    711                                    'not formatted properly, or because data for '

RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT `correctly.`



—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/EpistasisLab/tpot/issues/700>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KvZ119DTWOu6pUMUrpBn88pZ-zxeks5t38ZLgaJpZM4UUzxJ>.
",scoring method could please provide example reproduce error may wrote scenario another scoring measure accuracy context issue getting error iris data scoring measure accuracy recent call last fit self target population toolbox mu verbose none self none self raise error optimization could data error optimization process could data properly data regression problem provided object please make sure data correctly handling exception another exception recent call last module scoring em light print fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self raise error optimization could data properly data error optimization process could data properly data regression problem provided object please make sure data thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
393093421,"Thank you, following your links I found that RepeatedKFold is what I was looking for. I will use TPOT from Python then as the API seems much more feature-rich.",thank following link found looking use python much,issue,negative,positive,neutral,neutral,positive,positive
392867460,"`maxeval` can be any positive float. We will update document for this issue.

For integer inputs of cv parameter in command-line, if the estimator is a classifier and y is either binary or multiclass, [StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) is used. In all other cases, [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) is used. For more advanced usage of `cv` parameter, please check `cv` parameter in [TPOT API](https://epistasislab.github.io/tpot/api/) to customize CV with an object to be used as a cross-validation generator or an iterable yielding train/test splits.",positive float update document issue integer parameter estimator classifier either binary used used advanced usage parameter please check parameter object used generator iterable yielding,issue,positive,positive,positive,positive,positive,positive
392862630,"[StackingEstimator](https://github.com/EpistasisLab/tpot/blob/e0c526b543fd8175d67105f08711dc4a9d82aeb0/tpot/builtins/stacking_estimator.py) is a meta-transformer for adding predictions and/or class probabilities as synthetic feature(s). Your interpretation is correct for the part below:

```
make_pipeline(
SelectPercentile(score_func=f_classif, percentile=42),
StackingEstimator(estimator=KNeighborsClassifier(n_neighbors=35, p=1, weights=""uniform"")),
GaussianNB()
)
```
After the predictions in this part should be added to input X as synthetic features and then pass to DecisionTreeClassifier.",class synthetic feature interpretation correct part uniform part added input synthetic pas,issue,negative,neutral,neutral,neutral,neutral,neutral
392225173,https://github.com/ambv/black seems to be a popular automatic code-formatting tool that would solve PEP8 lint errors,popular automatic tool would solve pep lint,issue,positive,positive,positive,positive,positive,positive
391772998,Please try n_jobs = 8 or other number lower than your CPU cores to reduce memory usage.,please try number lower reduce memory usage,issue,negative,neutral,neutral,neutral,neutral,neutral
391724541,"What is likely going on is, the higher these values, the more memory is demanded from a system. You should consider finding means to measure available memory before and during these runs, and as you change these values observe how memory usage changes. My guess is, at some point your system simply refuses to allocate more. If you get an `OSError` in Python, you are getting it from the underlying system. This may of course be limited to the space of an individual process. There may be a constraint in place on how much memory any unpriv'd process can alloc before it is denied, or could well just be a system limitation. At any rate, the starting point is figuring out if you are running out of memory.",likely going higher memory system consider finding measure available memory change observe memory usage guess point system simply allocate get python getting underlying system may course limited space individual process may constraint place much memory process could well system limitation rate starting point running memory,issue,negative,positive,neutral,neutral,positive,positive
391327394,"This looks like a problem at a much lower layer. The system on which this was running could have been critically low on available resources at the time this happened. Is there any way to track down whether there were any memory shortfalls when this error occurred? You got back an OSError, which Python got back from the system, and this really has nothing to do with the package.",like problem much lower layer system running could critically low available time way track whether memory error got back python got back system really nothing package,issue,negative,positive,positive,positive,positive,positive
391007693,"Please check [TPOT API](https://epistasislab.github.io/tpot/api/). The `fitted_pipeline_` attribute, like `tpot_obj.fitted_pipeline_`, is the scikit-learn Pipeline for the best nested model. You can use it directly.  Or you can use export function in [this example](https://epistasislab.github.io/tpot/examples/#iris-flower-classification) to export codes.",please check attribute like pipeline best model use directly use export function example export,issue,positive,positive,positive,positive,positive,positive
390842150,"Hmm, how many python environments in your MacOS? Please let us know the version of python and $PYTHONPATH",many python please let u know version python,issue,negative,positive,positive,positive,positive,positive
389883346,"Nice! This is the point! Should not the TPOT address this issue? Like apply some LabelEncoder or something else? This functionality isn't about this?
Thanks for the help!",nice point address issue like apply something else functionality thanks help,issue,positive,positive,positive,positive,positive,positive
389488783,TPOT uses [stopit](https://github.com/glenfant/stopit) module to kill the evaluation of a pipelines if it takes too long (5 minutes by default). You can find more details in the [stopit](https://github.com/glenfant/stopit)'s GitHub repo.,module kill evaluation long default find,issue,negative,negative,neutral,neutral,negative,negative
389336821,"@weixuanfu Thanks. 

Quick question about this [line](https://github.com/EpistasisLab/tpot/blob/master/tpot/gp_deap.py#L396). How does `@threading_timeoutable(default=""Timeout"")` stop pipelines that take too long? Is this actually done at the thread level (individual threads are killed) or does TPOT know to ignore those pipelines and let them die on their own? Spark manages its own threads/processes, so I can't really use thread management that's independent of Spark. 

I'm noticing that when I run my code on my Spark cluster (I have a working proof-of-concept) on AWS, I often have just 1 or 2 sklearn pipelines that take a long time and hold up the entire generation, and that really hurts parallelization since most of the cluster resources go idle. I'd like to use something like the timeout check that you have but implement it in a way that's compatible with Spark. ",thanks quick question line stop take long actually done thread level individual know ignore let die spark ca really use thread management independent spark run code spark cluster working often take long time hold entire generation really parallelization since cluster go idle like use something like check implement way compatible spark,issue,positive,positive,neutral,neutral,positive,positive
389254382,"oh, I understood what happened. The input of `tpot.fit()` need to be a numeric array. You need convert these string types, like `""Braund, Mr. Owen Harris""`, to numbers (like 1, 2 or 3) or just remove this feature if it is not important.

",oh understood input need array need convert string like like remove feature important,issue,positive,positive,positive,positive,positive,positive
389245815,"Sorry for the `config_dict='TPOT sparse'`. It was a mistake!

I have tried your sugestion but I got the same error:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-109-a060d5622dbf> in <module>()
     18 )
     19 
---> 20 tpot.fit(df.drop('Survived', axis=1).values, df.Survived.values)

~/.virtualenvs/playground/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    540         """"""
    541 
--> 542         features, target = self._check_dataset(features, target)
    543 
    544         # Randomly collect a subsample of training samples for pipeline optimization process.

~/.virtualenvs/playground/lib/python3.6/site-packages/tpot/base.py in _check_dataset(self, features, target)
   1020                 )
   1021         else:
-> 1022             if np.any(np.isnan(features)):
   1023                 self._imputed = True
   1024                 features = self._impute_values(features)

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
I have used the `classifier_config_dict` defined in the first post at this time.

I believe the problem is the array type (`dtype=object`) but it was defined as object because it have a mixed data types (int, float, str)
```
df.drop('Survived', axis=1).values
array([[1, 3, 'Braund, Mr. Owen Harris', ..., 7.25, nan, 'S'],
       [2, 1, 'Cumings, Mrs. John Bradley (Florence Briggs Thayer)', ...,
        71.2833, 'C85', 'C'],
       [3, 3, 'Heikkinen, Miss. Laina', ..., 7.925, nan, 'S'],
       ...,
       [154, 3, 'van Billiard, Mr. Austin Blyler', ..., 14.5, nan, 'S'],
       [155, 3, 'Olsen, Mr. Ole Martin', ..., 7.3125, nan, 'S'],
       [156, 1, 'Williams, Mr. Charles Duane', ..., 61.3792, nan, 'C']],
      dtype=object)
```",sorry sparse mistake tried got error recent call last module fit self target target target randomly collect subsample training pipeline optimization process self target else true input could safely according casting rule safe used defined first post time believe problem array type defined object mixed data float array nan florence miss nan billiard nan martin nan nan,issue,positive,positive,neutral,neutral,positive,positive
389245403,@RyanZotti yes the order of result_score_list after evaluation in [this part](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1158-L1175) should matches the order of sklearn_pipeline_list,yes order evaluation part order,issue,negative,neutral,neutral,neutral,neutral,neutral
389240889,"Thanks @weixuanfu. 

How does [result_score_list](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1175) associate each `sklearn_pipeline` with its CV score? It looks like `result_score_list` is just a list of CVs and not a dict. Is the assumption that the order of `result_score_list` matches the order of `sklearn_pipeline_list`?",thanks associate score like list assumption order order,issue,positive,positive,positive,positive,positive,positive
389146781,"Hmm, that score should be average CV score used in optimization process. I
agree that it is a little confusing in export codes. Thank you for
reporting this issue.I will make a PR for refining export function.
On Mon, May 14, 2018 at 1:45 PM scolemann <notifications@github.com> wrote:

> Hi,
>
> I've noticed that the score in the
> pipeline_optimizer.score(testing_features, testing_target)), which I print
> upon completion, is different than the score in the
> tpot_exported_pipeline.py where it says ""# Score on the training set was x""
>
> It seems the number that I print from pipeline_optimizer is always lower
> than the one listed in the file. I expected them to be the same. Can you
> explain the difference?
>
> Thanks!
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/EpistasisLab/tpot/issues/691>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AUG7Kl0-y7cEgYelNiDekE75J2OmNEgWks5tycKbgaJpZM4T-Obw>
> .
>
",score average score used optimization process agree little export thank make refining export function mon may wrote hi score print upon completion different score score training set number print always lower one listed file explain difference thanks thread reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
388982088,"Hmm, stderr showed `config_dict='TPOT sparse'`, which is different with your examples. 

I think the issue can be fixed if the last line of codes is `tpot.fit(df.drop('Survived', axis=1).values, df.Survived).values`. tpot.fit() just takes np.ndarray as inputs for now.",sparse different think issue fixed last line,issue,negative,positive,neutral,neutral,positive,positive
388979762,"Hi, we are working on other issues in TPOT now, but we like better parallel computing way from dask or spark. [This line of code](https://github.com/EpistasisLab/tpot/blob/master/tpot/base.py#L1170) is about using joblib for parallel computing, which should be a good start. Please feel free to submit a PR for replacing joblib by dask/spark.",hi working like better parallel way spark line code parallel good start please feel free submit,issue,positive,positive,positive,positive,positive,positive
388903584,"Hi guys!
I have tried the solution showed [here](https://github.com/EpistasisLab/tpot/pull/560#issuecomment-385716353) with [titanic dataset](https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/ff414a1bcfcba32481e4d4e8db578e55872a2ca1/titanic.csv). I believe that with this PR and the right config TPOT should apply one hot encoding automatically on my data, but I have got the following error:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-60-4ca640247df0> in <module>()
     14 tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, config_dict='TPOT sparse')
     15 
---> 16 tpot.fit(df.drop(['Name', 'Ticket', 'Survived', 'Cabin', 'PassengerId', 'Age', 'Fare'], axis=1).as_matrix(), df.Survived.as_matrix())

~/.virtualenvs/playground/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    540         """"""
    541 
--> 542         features, target = self._check_dataset(features, target)
    543 
    544         # Randomly collect a subsample of training samples for pipeline optimization process.

~/.virtualenvs/playground/lib/python3.6/site-packages/tpot/base.py in _check_dataset(self, features, target)
   1020                 )
   1021         else:
-> 1022             if np.any(np.isnan(features)):
   1023                 self._imputed = True
   1024                 features = self._impute_values(features)

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
```
I have tried the following code:

```
import numpy as np
import pandas as pd

from tpot import TPOTClassifier
from tpot.config import classifier_config_dict

url = ""https://gist.githubusercontent.com/michhar/2dfd2de0d4f8727f873422c5d959fff5/raw/ff414a1bcfcba32481e4d4e8db578e55872a2ca1/titanic.csv""

df = pd.read_csv(url, sep=None, engine='python')

classifier_config_dict['tpot.builtins.CategoricalSelector'] = {
    'threshold': [10],
    'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],
    'sparse': [False]
}

classifier_config_dict['tpot.builtins.ContinuousSelector'] = {
    'threshold': [10],
    'svd_solver': ['randomized'],
    'iterated_power': range(1, 11)
}

tpot = TPOTClassifier(
    generations=5,
    population_size=50,
    verbosity=2,
    config_dict=classifier_config_dict
)

tpot.fit(df.drop('Survived', axis=1), df.Survived)
```
I'm using Ubuntu 18.04, Python 3.6.5 and the following libs:
```
Package            Version  
------------------ ---------    
numpy              1.14.3   
pandas             0.22.0     
pip                10.0.1   
scikit-learn       0.19.1   
scipy              1.1.0       
TPOT               0.9.3      
xgboost            0.71  
```
Sorry if this is not the right place to post this!",hi tried solution titanic believe right apply one hot automatically data got following error recent call last module sparse fit self target target target randomly collect subsample training pipeline optimization process self target else true input could safely according casting rule safe tried following code import import import import false range python following package version pip sorry right place post,issue,positive,positive,neutral,neutral,positive,positive
387166026,"Weixuanfu thank you for your prompt answer.

You may want to add this explanation to the documents. Also, here is something to add to what I am sure is a large ""to do"" list: use [Graphviz](http://www.graphviz.org) to print out a tree structure image of the best pipeline. This would make it easier for the user to understand the data flow in the pipeline.
",thank prompt answer may want add explanation also something add sure large list use print tree structure image best pipeline would make easier user understand data flow pipeline,issue,positive,positive,positive,positive,positive,positive
387075517,"For Question 1. The steps are:
1. raw attributes -> RidgeCV -> 1st prediction
2. raw attributes + 1st predictions-> PolynomialFeatures -> 1st transformed attributes --> LassoLarsCV -> 2nd predictions
3. 1st transformed attributes + 2nd predictions ->  XGBRegressor -> 3rd predictions
4. 1st transformed attributes + 2nd predictions + 3rd predictions -> ExtraTreesRegressor -> final predictions

For Question 2.

For now, TPOT does not provide this options. But: 


> 
> One of my dev branch of TPOT called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `simple_pipeline`, which can disable both `StackingEstimator` and `CombineDFs` if `simple_pipeline=True` (e.g. `TPOTClassifier(simple_pipeline=True)`). But it is noted that this dev branch is not fully tested yet. If you want to try TPOT without `StackingEstimator` and `FeatureUnion`, you may install this branch in your test environment via the command below:
> 
> ```
> pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
> ```

Please check #152 for more details. We are working on a more advanced pipeline configuration option. ",question raw st prediction raw st st st st final question provide one dev branch option disable noted dev branch fully tested yet want try without may install branch test environment via command pip install upgrade please check working advanced pipeline configuration option,issue,positive,negative,neutral,neutral,negative,negative
386660353,Well there are [various ways](https://stackoverflow.com/questions/1947904/how-can-i-pickle-a-nested-class-in-python/11493777#11493777) to pickle dynamic classes. Have you considered using [dill](https://github.com/uqfoundation/dill) instead of pickle though? Dill can serialize dynamically generated classes out of the box.,well various way pickle dynamic class considered dill instead pickle though dill serialize dynamically class box,issue,positive,neutral,neutral,neutral,neutral,neutral
386405960,"@rhiever Yeah, just saw that. Thanks. By the way, you guys are doing some awesome work here. Keep up the good work.",yeah saw thanks way awesome work keep good work,issue,positive,positive,positive,positive,positive,positive
386405118,"Thank you for putting that great kernel together, @TheBrownViking20! I answered your question directly on the kernel.",thank great kernel together question directly kernel,issue,positive,positive,positive,positive,positive,positive
386357203,Thank you for the notebook of using TPOT. TPOT only uses genetic programming for optimization.,thank notebook genetic optimization,issue,positive,neutral,neutral,neutral,neutral,neutral
385978642,"I know this issue now. You just need simply rename the file `code/scripts/python/tpot.py`, which cause conflicts when importing tpot.",know issue need simply rename file cause,issue,negative,neutral,neutral,neutral,neutral,neutral
385900138,"Traceback (most recent call last):
  File ""tpot_1.py"", line 6, in <module>
    from tpot.builtins import StackingEstimator
  File ""code/scripts/python/tpot.py"", line 6, in <module>
    from tpot import TPOTClassifier
ImportError: cannot import name 'TPOTClassifier'",recent call last file line module import file line module import import name,issue,negative,neutral,neutral,neutral,neutral,neutral
385899546,You could try to customize operators within TPOT with LightBGM’s scikit-learn API. Please check [the link](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for more details.,could try within please check link,issue,negative,neutral,neutral,neutral,neutral,neutral
385895503,Can you please provide more details information? Like stdout/stderr?,please provide information like,issue,positive,neutral,neutral,neutral,neutral,neutral
385716353,"@mattspinelli 

I will merge the branch to TPOT dev branch soon. You could install the tweaked version of TPOT via the commend below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/EpistasisLab/tpot.git@development
```

Below is a demo for using these two new transformer:

```
from tpot import TPOTClassifier
from tpot.config import classifier_config_dict
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
# add 2 operators into the classifier_config_dict
classifier_config_dict['tpot.builtins.CategoricalSelector'] = {
    'threshold': [10],
    'minimum_fraction': [0.05, 0.1, 0.15, 0.2, 0.25],
    'sparse': [False]
}

classifier_config_dict['tpot.builtins.ContinuousSelector'] = {
    'threshold': [10],
    'svd_solver': ['randomized'],
    'iterated_power': range(1, 11)
}

# load data
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, config_dict=classifier_config_dict)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```

[More info for using `config_dict`](https://epistasislab.github.io/tpot/using/#customizing-tpots-operators-and-parameters)
",merge branch dev branch soon could install version via commend pip install upgrade two new transformer import import import import import add false range load data iris print,issue,negative,negative,negative,negative,negative,negative
385676911,Sounds like a python environment issue. Please make sure the python scripts are running with python in jupyter notebook ,like python environment issue please make sure python running python notebook,issue,positive,positive,positive,positive,positive,positive
385535973,"For now, I think a better way is just to use confg_dict parameter to include them. I will provide a demo for it.",think better way use parameter include provide,issue,negative,positive,positive,positive,positive,positive
385533463,"@weixuanfu 

Reading this PR closer it seems there was hesitation to include it because a benchmark study hasn't been done to access if this helps improve performance overall and talk. If there was a flag to to make the categorical/continuous selector optional (default not enabled) then this probably would be the least invasive way to incorporate this feature. I don't know if this optional flag would be the ""deeper change"" @rhiever envisioned.",reading closer hesitation include study done access improve performance overall talk flag make selector optional default probably would least invasive way incorporate feature know optional flag would change,issue,negative,negative,negative,negative,negative,negative
385532743,"OK, I will fix the conflicts soon and then merge it into dev branch",fix soon merge dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
385529459,What is the status on this PR considering it is now 6 months old? Is something holding this up from being merged? I could benefit from this as I have a dataset with mixed categorical basically continuous data.,status considering old something holding could benefit mixed categorical basically continuous data,issue,negative,positive,neutral,neutral,positive,positive
384404768,"You can choose a pipeline off the Pareto front (accessed via `tpot_obj.pareto_front_fitted_pipelines_`, see the [TPOT API](https://epistasislab.github.io/tpot/api/)) that has lower complexity than the highest-accuracy pipeline. TPOT uses multi-objective optimization to maximize the tradeoffs between predictive ability and pipeline complexity.

I believe @weixuanfu is also working on a primitive grammar for TPOT that would support this issue.",choose pipeline front via see lower complexity pipeline optimization maximize predictive ability pipeline complexity believe also working primitive grammar would support issue,issue,positive,neutral,neutral,neutral,neutral,neutral
382072276,"after updating the scikit learn when i was import  i am getting the error like it
from sklearn.model_selection import cross_val_score

ImportError: cannot import name 'comb'",learn import getting error like import import name,issue,negative,neutral,neutral,neutral,neutral,neutral
381391879,"Oh, I see. Thank you very much !",oh see thank much,issue,negative,positive,positive,positive,positive,positive
381390781,"Related issue #425 #612

Please check TPOT API, the default scoring function is accuracy for TPOTClassifier, which is in range from 0 to 1. But it is neg_mean_squared_error for TPOTRegressor that stands for negated value of mean squared error. ",related issue please check default scoring function accuracy range value mean squared error,issue,negative,negative,negative,negative,negative,negative
381389751,"Thank you. I checked the two same issues, but I have not clearly known about the Tpot Regressor. The value of cv socre should be  negative ? Could you tell me where I can find the cv score of Tpot Regressor ？ ",thank checked two clearly known regressor value negative could tell find score regressor,issue,negative,negative,neutral,neutral,negative,negative
380997097,how to determine the winning individual? the scores on every value of that dict are all over the place. Can verbosity help?? More insights are needed in the docs. Will dive into the code now.,determine winning individual every value place verbosity help dive code,issue,positive,positive,positive,positive,positive,positive
380415148,I think the error message is clear and you may need reshape your input X. And also I think this should be a issue in scikit-learn’s repo. Unless please provide codes about tpot to reproduce it.,think error message clear may need reshape input also think issue unless please provide reproduce,issue,negative,positive,positive,positive,positive,positive
380351433,"PLS HELP ME WITH THIS ERRORS

Traceback (most recent call last):
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\app.py"", line 1982, in
 wsgi_app
    response = self.full_dispatch_request()
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\app.py"", line 1614, in
 full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\app.py"", line 1517, in
 handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\_compat.py"", line 33,
in reraise
    raise value
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\app.py"", line 1612, in
 full_dispatch_request
    rv = self.dispatch_request()
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\flask\app.py"", line 1598, in
 dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""app.py"", line 192, in cancerClassifier
    prediction=model.predict(X_test)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py"",
 line 538, in predict
    proba = self.predict_proba(X)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py"",
 line 578, in predict_proba
    X = self._validate_X_predict(X)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\ensemble\forest.py"",
 line 357, in _validate_X_predict
    return self.estimators_[0]._validate_X_predict(X, check_input=True)
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\tree\tree.py"", line
373, in _validate_X_predict
    X = check_array(X, dtype=DTYPE, accept_sparse=""csr"")
  File ""C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\utils\validation.py""
, line 441, in check_array
    ""if it contains a single sample."".format(array))
ValueError: Expected 2D array, got 1D array instead:
array=[15.2 52.  28.  20.   5. ].
Reshape your data either using array.reshape(-1, 1) if your data has a single fe
ature or array.reshape(1, -1) if it contains a single sample.
",help recent call last file line response file line file line reraise file line reraise raise value file line file line return file line file line predict file line file line return file line file line single sample array array got array instead reshape data either data single fe single sample,issue,positive,negative,neutral,neutral,negative,negative
380065306,"hmm, I just found `from sklearn.cross_validation import KFold` in codes. I think it should be updated to `from sklearn.model_selection import KFold`",found import think import,issue,negative,neutral,neutral,neutral,neutral,neutral
380010245,"As one of the maintainers of Auto-sklearn I'm also asked this several times. However, there are two papers arguing against such a feature:

* http://proceedings.mlr.press/v27/luxburg12a/luxburg12a.pdf
* https://lirias.kuleuven.be/bitstream/123456789/504712/1/automl_camera.pdf

I'd be really interested to learn how you overcome these problems.",one also several time however two feature really interested learn overcome,issue,negative,positive,positive,positive,positive,positive
379886434,I close this issue. Please feel free to re-open the issue if you have any more questions,close issue please feel free issue,issue,positive,positive,positive,positive,positive,positive
379886018,I close this issue since I merged the fix to TPOT 0.9.3. Please feel free to reopen this issue if you have any questions. ,close issue since fix please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
379828227,Thank you for reporting this bug. I will push a fix to dev branch soon.,thank bug push fix dev branch soon,issue,negative,neutral,neutral,neutral,neutral,neutral
379801418,">>> from sklearn.cross_validation import KFold
C:\Users\DELLPC\Anaconda3\lib\site-packages\sklearn\cross_validation.py:41: Depr
ecationWarning: This module was deprecated in version 0.18 in favor of the model
_selection module into which all the refactored classes and functions are moved.
 Also note that the interface of the new CV iterators are different from that of
 this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)",import module version favor model module class also note interface new different module module removed module removed,issue,negative,positive,neutral,neutral,positive,positive
379267277,This looks like a really interesting project. I someone i willing to mentor me i can take it up.,like really interesting project someone willing mentor take,issue,positive,positive,positive,positive,positive,positive
379092243,"Actually the answer is to look at your
    exported_pipeline.scoring_function
which, at least in my case, turns out to be 'neg_mean_squared_error', so your result is MSE=6.42%",actually answer look least case turn result,issue,negative,negative,negative,negative,negative,negative
378759385,"@rhiever 
> TPOT's imputation step isn't stored in the internal pipelines, but it should apply imputation when you call any functions like fit, predict, score, etc.

Here's the code inside `TPOTBase(BaseEstimator).predict`

```
features = features.astype(np.float64)

if np.any(np.isnan(features)):
    self._imputed = True
    features = self._impute_values(features)
else:
    self._imputed = False
return self.fitted_pipeline_.predict(features)
```

So yeah, it will use the `self._fitted_imputer` when predicting because of the `self._impute_values` call.

But now here's the code inside `TPOTBase(BaseEstimator).score`:

```
score = SCORERS[self.scoring_function](
            self.fitted_pipeline_,
            testing_features.astype(np.float64),
            testing_target.astype(np.float64)
)
```

It simply forwards the `testing_features` without using the `self._fitted_imputer`, so unlike I expected, it won't do imputation automatically when using the `score` method.

Should I open a new issue? (And perhaps a PR...)",imputation step internal apply imputation call like fit predict score code inside true else false return yeah use call code inside score simply forward without unlike wo imputation automatically score method open new issue perhaps,issue,positive,positive,neutral,neutral,positive,positive
377549070,Did this issue open by mistake? I close it now and please feel free to reopen it with some descriptions.  ,issue open mistake close please feel free reopen,issue,negative,positive,positive,positive,positive,positive
376998693,Looking forward to this in TPOT. I think it will clean up the interface quite a bit.,looking forward think clean interface quite bit,issue,negative,positive,positive,positive,positive,positive
376627497,"It's nasty. Due to it's unpredictable nature, it makes you slowly distrust every piece of software on the system. Then I removed the module that I disliked most and now. Suddenly 
everything is stable.",nasty due unpredictable nature slowly distrust every piece system removed module suddenly everything stable,issue,negative,negative,negative,negative,negative,negative
376626082,Glad you were able to get to the bottom of the issue! Guess that's about as low-level as an issue can go.,glad able get bottom issue guess issue go,issue,negative,positive,positive,positive,positive,positive
376601328,"I have found the reason for the initial problem, the segfaults:
My RAM has defects.
```
$ sudo memtester 12G
FAILURE: 0xee6eaae5e9499bb3 != 0xee6ebae5e9499bb3 at offset 0x44ae4678.
```
This issue can be closed.",found reason initial problem ram failure offset issue closed,issue,negative,negative,negative,negative,negative,negative
376316125,"I think the issue about multit-threads in `htop` maybe due to the [`threading` backend](https://github.com/scikit-learn/scikit-learn/blob/a24c8b464d094d2c468a16ea9f8bf8d42d949f84/sklearn/ensemble/forest.py#L177-L180) used by some ensemble-based algorithms. Also I found it happened with XGboost, but I am not sure why for now. You could use the code to reproduce the issue. But I noticed the cpu% in `htop` was 100% even with multithreads since n_jobs=1.
```
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split, TimeSeriesSplit, cross_val_score
from sklearn.ensemble import GradientBoostingClassifier

digits = load_digits()
cv = TimeSeriesSplit(n_splits=20)
gbc = GradientBoostingClassifier(learning_rate=0.5, max_depth=6, max_features=0.1, min_samples_leaf=1, min_samples_split=13, n_estimators=100, subsample=0.60)

for _ in range(1000):
    cv_scores = cross_val_score(gbc, digits.data, digits.target.astype('bool'), cv=cv)
    print(cv_scores)
```",think issue maybe due used also found sure could use code reproduce issue even since import import import range print,issue,negative,positive,positive,positive,positive,positive
376267306,"This is interesting. How did you count the threads? Here's what I came up with:
```
python ./test2.py 
# wait a few secs
ps hH p $(pgrep -f test2.py)
67341 pts/3    Rl+    3:04 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:22 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:20 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:22 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:20 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:21 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:20 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:20 /usr/bin/python ./test2.py
67341 pts/3    Rl+    0:16 /usr/bin/python ./test2.py
67341 pts/3    Sl+    0:00 /usr/bin/python ./test2.py
67341 pts/3    Sl+    0:00 /usr/bin/python ./test2.py
```

To setup up a fresh install I did this:
```
conda config --remove channels conda-forge
conda clean --all
conda update conda python pip
conda create --name test python=3
source activate test
(test) pip install --no-cache-dir tpot
```",interesting count came python wait setup fresh install remove clean update python pip create name test source activate test test pip install,issue,positive,positive,positive,positive,positive,positive
376172298,I close this issue. Please feel free to re-open the issue (or comment further) if you have any more questions,close issue please feel free issue comment,issue,positive,positive,positive,positive,positive,positive
376016569,"I couldn't reproduce the issue on my MacBook with TPOT v0.9.2 that's available through `pip`. For reference, here are the package versions I'm running:

```
$ python -c ""import numpy; print('numpy %s' % numpy.__version__)""
numpy 1.12.1
$ python -c ""import scipy; print('scipy %s' % scipy.__version__)""
scipy 1.0.0
$ python -c ""import sklearn; print('sklearn %s' % sklearn.__version__)""
sklearn 0.19.1
$ python -c ""import deap; print('deap %s' % deap.__version__)""
deap 1.2
$ python -c ""import xgboost; print('xgboost %s ' % xgboost.__version__)""
xgboost 0.7.post3 
$ python -c ""import update_checker; print('update_checker %s ' % update_checker.__version__)""
update_checker 0.16 
$ python -c ""import tqdm; print('tqdm %s' % tqdm.__version__)""
tqdm 4.19.5
$ python -c ""import pandas; print('pandas %s' % pandas.__version__)""
pandas 0.22.0
```",could reproduce issue available pip reference package running python import print python import print python import print python import print python import print post python import print python import print python import print,issue,negative,positive,positive,positive,positive,positive
375983788,"I’ll have to take a look when I get back to the office. Thank you for the
minimal working example.
",take look get back office thank minimal working example,issue,negative,negative,neutral,neutral,negative,negative
375977104,"I built a minimal working example that shows the problem:
```
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split, TimeSeriesSplit

digits = load_digits()
cv = TimeSeriesSplit(n_splits=20)
tpot = TPOTClassifier(
        generations=1000, population_size=100, offspring_size=100, cv=cv, verbosity=2, n_jobs=1,
        random_state=42, subsample=1, scoring='f1',
        periodic_checkpoint_folder='test_checkpoints')

tpot.fit(digits.data, digits.target.astype('bool'))
```
Now, when it runs 1 minute or so, multiple processes/or threads? are spawned.
![htop2](https://user-images.githubusercontent.com/13658554/37876554-67581694-304e-11e8-841a-abd2956804c6.png)
",built minimal working example problem import import import minute multiple,issue,negative,negative,neutral,neutral,negative,negative
375975329,"![htop](https://user-images.githubusercontent.com/13658554/37876320-d21a4bae-304a-11e8-8557-fc3b3d7f3f20.png)
But how can this be explained? Command is
> model = TPOTClassifier(
>            generations=1000, population_size=100, offspring_size=100, cv=cv - cv[0][0][0], verbosity=2,  n_jobs=1,
>            random_state=42, subsample=1, scoring='f1',
>            periodic_checkpoint_folder=pathjoin(data_path, 'checkpoints'))

It's only using multiple cores for short time periods and it starts after ~2hours of optimization. ",command model multiple short time optimization,issue,negative,neutral,neutral,neutral,neutral,neutral
375974636,"None of the operators in TPOT should be spawning new processes, especially
when n_jobs=1. We’ve been careful to make sure of that.",none spawning new especially careful make sure,issue,positive,positive,positive,positive,positive,positive
375961311,"After it ran 1.5 hours, it's using all 16 cores for a short period even though I specified n_jobs=1. 
Although it seems stable up until now.

Could it be, that TPOT is spawning several jobs when n_jobs>1 and some estimators used by TPOT are spawning several jobs on their own (no matter if n_jobs=1)? I got trouble with this scenario some time ago too. 
I'm using default config dict at the moment.

Also, I made some memory measurements during a segfault:
> while true; do free >> memory.log; sleep 0.5; done
> grep Speicher memory.log |awk '{print $4}'|sort |head -n1
> 10000444

Either the peak was shorter than .5 seconds or memory usage is not a problem.

I also tried a non-conda python Version, since I read about segfaults in conda's python. Seems that python is not the problem here, it crashes too.

Another observation I made is that the other programs become unstable too when TPOT is run with n_jobs>1: Firefox (sometimes only tabs crash, sometimes the whole thing), conda (segfaults when creating environments), ...",ran short period even though although stable could spawning several used spawning several matter got trouble scenario time ago default moment also made memory true free sleep done print either peak shorter memory usage problem also tried python version since read python python problem another observation made become unstable run sometimes crash sometimes whole thing,issue,negative,positive,positive,positive,positive,positive
375892785,"Nope:
> Generation 1 - Current best internal CV score: 0.514192598512397                                                                                                                                                   
> Optimization Progress:   0%|▎                                                                                                                                       | 203/100100 [33:13<911:00:21, 32.83s/pipeline]
> Speicherzugriffsfehler (Speicherabzug geschrieben)

Also, I noticed that it used all available cores, even though n_jobs=4 was specified.",nope generation current best internal score optimization progress also used available even though,issue,negative,positive,positive,positive,positive,positive
375761834,16GB RAM maybe not enough to handle n_jobs=-1. Could you please also try n_jobs=4 instead?,ram maybe enough handle could please also try instead,issue,negative,neutral,neutral,neutral,neutral,neutral
375761619,"By the way, I think this line in the output above is significant:
> Error in '.../miniconda3/bin/python': corrupted double-linked list: 0x00007fff2443b060",way think line output significant error corrupted list,issue,negative,positive,positive,positive,positive,positive
375760935,RAM size is 16GB and nCPUs=16 (hyperthreading). I will check the dev branch out and see if it fixes the issue. ,ram size check dev branch see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
375741938,I am not sure why this error showed up. What are the RAMs size and the number of CPU? I just limited chunk-size when n_jobs = -1 or very large number in #677 (I will merge it dev branch soon). Maybe the dev branch is more stable for n_jobs=-1.,sure error size number limited large number merge dev branch soon maybe dev branch stable,issue,negative,positive,positive,positive,positive,positive
374690052,While I'm looking at it... This is a pretty mighty stack. Could it have something to do with the maximum recursion depth like in https://stackoverflow.com/q/10035541?,looking pretty mighty stack could something maximum recursion depth like,issue,positive,positive,positive,positive,positive,positive
374243209,We had a lot discussions about this. Please check the related issue #503 and PR #506.,lot please check related issue,issue,negative,neutral,neutral,neutral,neutral,neutral
374238037,"Thanks, that explains the second point. Why should they have a primitive in common in order to be eligible for crossover?",thanks second point primitive common order eligible crossover,issue,negative,negative,neutral,neutral,negative,negative
374229257,"Yes, that is exactly how crossover operator in GP algorithm in TPOT. We tweaked codes based on [varOr in deap](http://deap.readthedocs.io/en/master/api/algo.html?highlight=varOr#deap.algorithms.varOr). `varOr` should only keep one child from a crossover since the percentage of offspring due to crossover should be equal to the probability of mating two individuals.",yes exactly crossover operator algorithm based keep one child crossover since percentage offspring due crossover equal probability mating two,issue,negative,positive,neutral,neutral,positive,positive
374187322,"@weixuanfu going trough the crossover operator i see two things:
- Two pipelines need a shares primitive in order to be eligible for crossover
- Only one of the two resulting pipelines in a crossover is kept as offspring.
Could you briefly shed some light on these choices?
(PS: hope you don't mind that i abuse this issue a bit for some short questions, please let me know if you think this should be done otherwise :))",going trough crossover operator see two two need primitive order eligible crossover one two resulting crossover kept offspring could briefly shed light hope mind abuse issue bit short please let know think done otherwise,issue,negative,positive,positive,positive,positive,positive
373708692,"Related issue #425 #612 

Please check [TPOT API](https://github.com/EpistasisLab/tpot/issues/612), the scoring function is `accuracy` for TPOTClassifier, which is in range from 0 to 1. But it is `neg_mean_squared_error` for TPOTRegressor that stands for negated value of mean squared error. Please check [this link](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values) for more details",related issue please check scoring function accuracy range value mean squared error please check link,issue,positive,negative,negative,negative,negative,negative
373067039,"Sorry, we don't any literature so far about the effects of this change in TPOT. But definitely, the mu + lambda algorithm is more advanced algorithm. @rhiever ",sorry literature far effect change definitely mu lambda algorithm advanced algorithm,issue,positive,neutral,neutral,neutral,neutral,neutral
373056364,"Sorry for reopening, but is there any literature on the effects of this change on the performance of TPOT yet available?",sorry literature effect change performance yet available,issue,negative,negative,neutral,neutral,negative,negative
373055934,Thanks! Will close this now. Might reopen if i have further questions,thanks close might reopen,issue,negative,positive,positive,positive,positive,positive
373047289,"Yes, the selection way in the paper is included in TPOT version 0.3-0.6 but we changed it to the  [mu + lambda evolutionary algorithm](http://deap.readthedocs.io/en/master/api/algo.html#deap.algorithms.eaMuPlusLambda) in TPOT 0.7 which was documented in the [release log](https://github.com/EpistasisLab/tpot/releases)",yes selection way paper included version mu lambda evolutionary algorithm release log,issue,negative,neutral,neutral,neutral,neutral,neutral
369571459,"I don’t see detailed description about the issue, so I think it might be open by mistake. Please feel free to provide more details and reopen this issue.",see detailed description issue think might open mistake please feel free provide reopen issue,issue,negative,positive,positive,positive,positive,positive
369274764,"Yes, it works! Thanks for such a quick and valid solution! cheers!",yes work thanks quick valid solution,issue,positive,positive,positive,positive,positive,positive
369273176,"It seems the issue is about importing deap package. I think maybe this issue is only for Windows OS as we discussed before in this [deap issue](https://github.com/DEAP/deap/issues/240) . 

You could try to install deap 1.0.2.post2 instead via the command `pip install deap==1.0.2.post2`",issue package think maybe issue o issue could try install post instead via command pip install post,issue,negative,neutral,neutral,neutral,neutral,neutral
368458038,I see it freezing after ~3 generations with and without forkserver for different scorers. A workaround seems to be setting backend='threading' as default kwarg for Parallel in sklearn/externals/joblib/parallel.py,see freezing without different setting default parallel,issue,negative,neutral,neutral,neutral,neutral,neutral
367613266,"Interesting, thank you for the explanation. Overall I found TPOT to be very useful, well done!",interesting thank explanation overall found useful well done,issue,positive,positive,positive,positive,positive,positive
367102967,"The custom data preprocessing procedure needs to be added to the exported code in this case. Follow the steps under the ""Make predictions on the submission data"" section to see how the data from `titanic_test.csv` is preprocessed before being passed to TPOT. You should also be able to take the code from that section and use it to preprocess the data then use it in the exported pipeline.",custom data procedure need added code case follow make submission data section see data also able take code section use data use pipeline,issue,negative,positive,positive,positive,positive,positive
367090862,I have already went through it. Can you tell me which file exactly should I give to the tpot_titanic_pipeline.py ?,already went tell file exactly give,issue,negative,positive,positive,positive,positive,positive
367087613,"Do you mean [this titanic-train.csv](https://github.com/EpistasisLab/tpot/blob/master/tutorials/data/titanic_train.csv)? If so, please refer to [this notebook](https://github.com/EpistasisLab/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb) for an example of how to preprocess that dataset so it can be used with TPOT.",mean please refer notebook example used,issue,negative,negative,negative,negative,negative,negative
367085988,"The data set is titanic-train.csv 
After exporting the python file and changing "" PATH/TO/DATA/FILE "" with the path to titanic-train.csv file, I have this error:
Error: ValueError: could not convert string to float",data set python file path file error error could convert string float,issue,negative,neutral,neutral,neutral,neutral,neutral
367075154,"Ah, 5 generations isn't very much time for TPOT to really refine the pipelines - at that point the GA has only gone through 5 rounds of selection. That's good to hear that pipelines from later in the run didn't retain this artifact.

The reason why TPOT doesn't immediately get rid of pipelines like this is because this artifact is potentially useful for building more complex pipelines later in the optimization process. Either of those `FunctionTransformer`s can be replaced with another pipeline operation in subsequent generations, and that could potentially be useful for improving prediction performance. As such, our pipeline regularization process doesn't penalize pipelines that make two copies of the features like this because it technically doesn't ""hurt"" the pipeline.

We've discussed other approaches to pipeline regularization (#207) that would probably weed out pipelines like you showed above, bu we haven't gotten to implementing those ideas yet.",ah much time really refine point ga gone selection good hear later run retain artifact reason immediately get rid like artifact potentially useful building complex later optimization process either another pipeline operation subsequent could potentially useful improving prediction performance pipeline regularization process penalize make two like technically hurt pipeline pipeline regularization would probably weed like bu gotten yet,issue,positive,positive,positive,positive,positive,positive
367072208,"@bassmaamn, what does your dataset look like? Does it have string data in it?",look like string data,issue,negative,neutral,neutral,neutral,neutral,neutral
366976467,Could you please provide more details about this error?,could please provide error,issue,negative,neutral,neutral,neutral,neutral,neutral
366932068,"I since deleted this example but I got another one:

```python
# Score on the training set was:0.522222222222
exported_pipeline = make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    StandardScaler(),
    MaxAbsScaler(),
    StackingEstimator(estimator=LinearSVC(C=10.0, dual=False, loss=""squared_hinge"", penalty=""l2"", tol=0.01)),
    RandomForestClassifier(bootstrap=True, criterion=""gini"", max_features=0.5, min_samples_leaf=8, min_samples_split=18, n_estimators=100)
)
```

Here is the tpot classifier that I configured:

```python
model = tpot.TPOTClassifier(
    cv=LeaveOneGroupOut(),
    scoring=experiment.build_scorer(),
    periodic_checkpoint_folder=files.create_abspath('models/multi_pca_usine_lcdv'),
    max_time_mins=11 * 60,
    max_eval_time_mins=10,
    n_jobs=10,
    verbosity=2
)
```

So population is 100. Not sure about the number of generations at this point.. I guess at least 5 since there are 5 exported pipelines in the output folder before this one.

The optimizer ran for ~6 hours before reaching this intermediate result (better pipelines obtained later in the same run did not contain such artifacts). ",since example got another one python score training set copy copy classifier python model population sure number point guess least since output folder one ran reaching intermediate result better later run contain,issue,positive,positive,positive,positive,positive,positive
366912543,"When I run it I get this Error: ValueError: could not convert string to float
Any idea please ?",run get error could convert string float idea please,issue,negative,neutral,neutral,neutral,neutral,neutral
366755669,That's interesting. How long did you run TPOT (population & generations) when it gave you this solution?,interesting long run population gave solution,issue,positive,positive,positive,positive,positive,positive
366743839,"This can lead to weird pipelines though. Here is what I got

```python
# Score on the training set was:0.333968253968
exported_pipeline = make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    RandomForestClassifier(bootstrap=""false"", criterion=""gini"", max_features=0.15, min_samples_leaf=10, min_samples_split=4, n_estimators=100)
)
```

In this case I doubt that the `FunctionTransformer(copy)` is useful. I guess adding copies is roughly equivalent to tweaking the `max_features` parameter of the random forest.

Context:
- train dataset of ~30 000 sample x 15 features 
- tpot v 0.9.2",lead weird though got python score training set copy copy false case doubt copy useful guess roughly equivalent parameter random forest context train sample,issue,negative,negative,negative,negative,negative,negative
365377312,"We don't have direct support for this functionality, but it could technically be feasible through the use of the `warm_start` parameter. Some quick code:

```Python
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=test_size, random_state=seed
)

tpot = TPOTRegressor(generations=1, population_size=20, verbosity=2, warm_start=True)
for _ in range(50):
    tpot.fit(x_train, y_train)
    y_predicted = tpot.predict(scaler_x.transform(x_test))
    print('me: ', mean_absolute_error(y_test, y_predicted))
    print('mse: ', mean_squared_error(y_test, y_predicted))

# score at the end of training
y_predicted = tpot.predict(scaler_x.transform(x_test))
print('me: ', mean_absolute_error(y_test, y_predicted))
print('mse: ', mean_squared_error(y_test, y_predicted))
```",direct support functionality could technically feasible use parameter quick code python range print print score end training print print,issue,negative,positive,positive,positive,positive,positive
365337761,"Would these memory mapping and zero-copy approaches help with parallelism here?
https://arrow.apache.org/docs/python/memory.html

https://github.com/maartenbreddels/vaex
",would memory help parallelism,issue,negative,neutral,neutral,neutral,neutral,neutral
365278767,"I think you could change the line `clf_sigmoid = CalibratedClassifierCV(tpot, cv=2, method='sigmoid')` to `clf_sigmoid = CalibratedClassifierCV(tpot.fitted_pipeline_, cv=2, method='sigmoid')`. The attribute `fitted_pipeline_` is scikit-learn Pipeline object which has this `classes_` atrribute.",think could change line attribute pipeline object,issue,negative,neutral,neutral,neutral,neutral,neutral
364638829,Did you ever get to the bottom of what is going on? I also find the same thing which looks suspicious to me.,ever get bottom going also find thing suspicious,issue,negative,neutral,neutral,neutral,neutral,neutral
363139007,"The issue is about using `f1_score` scorer in multi-class data, please check [this link](http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values). You need specify the right f1 score function name for multi-class data except using `f1`. For example:

```Python
from sklearn.datasets import make_classification
from tpot import TPOTClassifier

X, y = make_classification(n_samples=200, n_features=100,
                           n_informative=20, n_redundant=10,
                           n_classes=3, random_state=42)
tpot = TPOTClassifier(generations=10, population_size=20, verbosity=20, scoring='f1_macro')
tpot.fit(X, y)
```

",issue scorer data please check link need specify right score function name data except example python import import,issue,negative,positive,positive,positive,positive,positive
363134169,"Heym I found this [old related issue](https://github.com/scikit-learn/scikit-learn/issues/5115) in sklearn, which seems it is still unsolved yet. I will track this one.",found old related issue still unsolved yet track one,issue,negative,positive,neutral,neutral,positive,positive
363131114,That's very kind of you. Could you link the issue? Would like to track.,kind could link issue would like track,issue,positive,positive,positive,positive,positive,positive
363130521,"Yep, I reproduced this issue with codes below:

```
import multiprocessing

if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
    from sklearn import linear_model, metrics
    from sklearn.model_selection import GridSearchCV
    import numpy as np
    np.random.seed(42)
    X_train = np.random.random((1000,10))
    y_train = np.random.random(1000)

    def RMSLE(p,a):
        return np.sqrt(np.mean( (np.log(p+1) - np.log(a+1))**2 ))

    rmsle_score = metrics.make_scorer(RMSLE,greater_is_better=False)
    parameters = {'fit_intercept':(True, False), 'normalize':[True, False]}
    regr = linear_model.LinearRegression()
    reg1 = GridSearchCV(regr, parameters, verbose=2,scoring=rmsle_score,n_jobs=-1)
    reg1.fit(X_train, y_train)
```
I will report this issue to scikit-learn's repo",yep issue import import metric import import return true false true false reg report issue,issue,positive,negative,neutral,neutral,negative,negative
363127656,Using the customer scorer in GridSearchCV with `forkserver` seems to make everything freeze. No exception is raised,customer scorer make everything freeze exception raised,issue,negative,neutral,neutral,neutral,neutral,neutral
363126373,"Hmm, weird. It seems that this sklearn's scorer has this pickable issue with `forkserver`. ",weird scorer pickable issue,issue,negative,negative,negative,negative,negative,negative
363124810,It works when standard fork is used for multihreading. Using latest version of TPOT,work standard fork used latest version,issue,negative,positive,positive,positive,positive,positive
363124475,What is the version of TPOT? I think TPOT somehow cannot recognized the customized scoring function. I agree that it maybe related to #645. What will happen if n_jobs=1?,version think somehow scoring function agree maybe related happen,issue,negative,neutral,neutral,neutral,neutral,neutral
362897297,"PS: Option 2. is [what sklearn does](http://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics) when given a cross validation split generator, that is evaluated at each iteration. ",option given cross validation split generator iteration,issue,negative,neutral,neutral,neutral,neutral,neutral
362896846,"Currently I'm seeing the same behaviour as CSNoyes, especially for time series problems. 
As we can only give one sample split to TPOT at the beginning of the optimization process, the pipeline seems to overfit this special split during fitting, even if there are many splits. 
In the end, this results in a good score reported by TPOT, but when evaluating the best pipeline on other splits, the performance is much worse. I think a simple reproducing example could be built, if you wanted, I could give it a try.

There may be two options to overcome this: 
1. Changing the random_state at each iteration as suggested by CSNoyes
2. Randomizing splits at each iteration. 

To me, option 1 seems more easy. As an additional option to TPOT, it wouldn't hurt the overall performance of TPOT. ",currently seeing behaviour especially time series give one sample split beginning optimization process pipeline overfit special split fitting even many end good score best pipeline performance much worse think simple example could built could give try may two overcome iteration iteration option easy additional option would hurt overall performance,issue,positive,positive,positive,positive,positive,positive
362209224,"Hello @weixuanfu ,
Sorry for asking it again but, do you know more or less when your ""noCDF_noStacking"" branch will be merged with the master branch? It fits perfectly in our work, but now it's a lot of commits behind the master branch.
Thank you very much :) .",hello sorry know le branch master branch perfectly work lot behind master branch thank much,issue,positive,positive,neutral,neutral,positive,positive
361245483,"The warning message is from importing the old version of xgboost (0.6) in TPOT and this warning message won't effect optimization of TPOT. If you want to avoild this warning message, the latest version  of xgboost (0.7) need to be installed in the environment. But the latest version cannot be installed via conda in Windows. You may the [official installation guide](http://xgboost.readthedocs.io/en/latest/build.html) for building xgboost in Windows. ",warning message old version warning message wo effect optimization want warning message latest version need environment latest version via may official installation guide building,issue,negative,positive,positive,positive,positive,positive
361092140,"FWIW I'm enthusiastic about this pairing.  From my perspective as a Dask developer TPOT's workload is a nice example of something that is clearly parallelizable, but requires more sophistication than your typical big data system (or at least, this is based on a guess of what I think your workloads look like).  This plays nicely to Dask's strengths.

I suspect that the efficient parallelism would provide some convenient speedups (especially if you're currently spending time repeatedly shipping data off with the multiprocessing module), but that you might actually find more value in the visual diagnostics, parallel profiling, etc. that come along for free.",enthusiastic perspective developer nice example something clearly sophistication typical big data system least based guess think look like nicely suspect efficient parallelism would provide convenient especially currently spending time repeatedly shipping data module might actually find value visual diagnostics parallel come along free,issue,positive,positive,positive,positive,positive,positive
361091154,"> With dask, is there any way to prevent constant duplication of a dataset in memory? As it is, it's fairly easy to parallelize TPOT with the python multiprocessing module, but that requires cloning the entire environment with pickle through some IPC. This is a problem for large data-sets, as the memory copying takes up a lot of time, and in the end you need a shit-ton of RAM.

I suspect that Dask's memory management heuristics will handle this without significant issue.

> Intermediate states of the graph are not cached between generations. The solution for this is trickier. Maybe it is possible to build a graph for all generations at the same time, but that would imply to reimplement DEAP as a dask graph and just the idea makes my head spin.

Note that with the newer scheduler you can evolve a computation on the fly, so you don't need to decide everything ahead of time.  The right API to look at for this is probably the [concurrent.futures](http://dask.pydata.org/en/latest/futures.html) API.  You can submit many tasks, wait for a few of them to come back, and then based on those results submit more that are more likely to be helpful.  Here is a talk at the last SciPy conference that talks about some of the more real-time features of the dask schedulers: https://www.youtube.com/watch?v=ZxNPVTHQwGo",way prevent constant duplication memory fairly easy parallelize python module entire environment pickle problem large memory lot time end need ram suspect memory management handle without significant issue intermediate graph solution maybe possible build graph time would imply graph idea head spin note evolve computation fly need decide everything ahead time right look probably submit many wait come back based submit likely helpful talk last conference,issue,negative,positive,positive,positive,positive,positive
360602576,"Just thought I'd chime in and say I was able to get a custom estimator working based on advice here: https://github.com/EpistasisLab/tpot/issues/407

Has anyone managed to get a keras model working as an estimator? That will likely be my next project. Also, I'd be curious to hear about other custom estimators (I'm focused on regression, myself) that don't come ""in the box"" yet. Thanks!",thought chime say able get custom estimator working based advice anyone get model working estimator likely next project also curious hear custom regression come box yet thanks,issue,positive,positive,positive,positive,positive,positive
360537436,"Thanks guys, I will take a look at those and see if I can make it work. Really appreciate the suggestions.",thanks take look see make work really appreciate,issue,positive,positive,positive,positive,positive,positive
360233834,"You can also use this hacky method to enable certain estimators:

```
from sklearn.base import RegressorMixin
from glmnet import ElasticNet
from catboost import CatBoostRegressor
from tpot.config.regressor import regressor_config_dict

CatBoostRegressor.__bases__ += (RegressorMixin,)
ElasticNet.__bases__ += (RegressorMixin,)

config_dict = regressor_config_dict
config_dict['catboost.CatBoostRegressor'] = {
    'logging_level': ['Silent'],
    'thread_count': [8]}

config_dict['glmnet.ElasticNet'] = {
    # Elastic net L1 vs L2
    'alpha': [0, 0.25, 0.5, 0.75, 1],
    'n_jobs': [8]
}
```",also use hacky method enable certain import import import import elastic net,issue,negative,positive,positive,positive,positive,positive
360200565,"yes, the method is still the same to implement additional estimators in 0.9.2.

I think these links is out of date. Please check a related issue #602 and I just add a comment with updated links.",yes method still implement additional think link date please check related issue add comment link,issue,positive,neutral,neutral,neutral,neutral,neutral
360182026,"Hi guys - I know this issue is closed, but is the method still the same to implement additional estimators in 0.9.2? I'm somewhat experienced with out-of-the-box sklearn, but relatively new to the class structures involved and found the xgboost example above a little bit confusing. Is line 351 in the ""Build sklearn-based class"" uniquely relevant, or is that entire file something that would need to be built for each custom estimator? Forgive my ignorance. For what it's worth, I happen to be interested in adding almost the exact same set of estimators as the original poster in this topic - are there any plans to add additional estimators to the base TPOT configuration, or a simplified ""extender"" interface?

Also, I should mention that TPOT is simply awesome, and as a first-time participant here, I'd just like to say thanks for building this great library!",hi know issue closed method still implement additional somewhat experienced relatively new class involved found example little bit line build class uniquely relevant entire file something would need built custom estimator forgive ignorance worth happen interested almost exact set original poster topic add additional base configuration simplified extender interface also mention simply awesome participant like say thanks building great library,issue,positive,positive,positive,positive,positive,positive
360158154,"I found a few examples online ([example1](https://gist.github.com/ktrnka/81c8a7b79cb05c577aab) and [example 2](https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/keras/_impl/keras/wrappers/scikit_learn.py)) and a [related tutorial](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/) .

In general, you need:
1. build a new estimator class with scikit-learn API;
2. then edit the `config_dict` and let TPOT found the customized estimator ([example](https://github.com/EpistasisLab/tpot/blob/master/tpot/config/classifier.py#L164-L167) for tpot built-in operators). The new class should be found by `import` in your python code. For example, if the class is named tmp in foo.py, and the key's name in the configuration dictionary need to be 'foo.tmp'.",found example example related tutorial general need build new estimator class edit let found estimator example new class found import python code example class key name configuration dictionary need,issue,negative,positive,neutral,neutral,positive,positive
360139376,"Perfect, thanks for your swift investigation. Anything i can do on this issue? Otherwise, i think we can close it :)",perfect thanks swift investigation anything issue otherwise think close,issue,positive,positive,positive,positive,positive,positive
360018207,I would like to add my own keras models to the stack of possible estimators the algorithm can fit into the pipeline. Any idea how I might create a new class for this project to pick from?,would like add stack possible algorithm fit pipeline idea might create new class project pick,issue,positive,positive,positive,positive,positive,positive
359932735,"I just updated `numpy` from 1.12.1 to 1.13.3 in the python 3.5.3 env and now `str(np.arange(0.0, 1.01, 0.05)[-2])` is `0.95`. So I think updating numpy via `conda update numpy` should solve this issue. ",python think via update solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
359844967,"The pipeline is indeed generated on another environment (3.5.3), which is the cloud environment i use to run the optimization (I pickle the evaluated individuals dict to investigate on my local machine). This seems some kind of floating point precision issue. Would it be somehow possible to set a package wide precision of some sorts? ",pipeline indeed another environment cloud environment use run optimization pickle investigate local machine kind floating point precision issue would somehow possible set package wide precision,issue,positive,positive,positive,positive,positive,positive
359841046,"`str(np.arange(0.0, 1.01, 0.05)[-2])` (which is used by TPOT in this [line](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/base.py#L436)) is `0.95`  in python 3.6.2 but `0.9500000000000001` in my python 3.5.3 environment. ",used line python python environment,issue,negative,neutral,neutral,neutral,neutral,neutral
359836851,"Hmm, right, I just build a Python 3.6.2 environment and this issue can be reproduced. Also using `XGBClassifier__subsample=0.95` indeed works, but I don't how the pipeline generated from TPOT has `XGBClassifier__subsample=0.9500000000000001` with python 3.6.2. Is this pipeline generated in another python environment? And another weird thing is `np.arange(0.05, 1.01, 0.05)[-2]` in python3.6.2 is `0.95000000000000007` instead.",right build python environment issue also indeed work pipeline python pipeline another python environment another weird thing python instead,issue,negative,negative,negative,negative,negative,negative
359818649,Could the reason be that i am running python 3.6.2?,could reason running python,issue,negative,neutral,neutral,neutral,neutral,neutral
359808375,"Hmm, somehow it works in my environment. Check the ipython log below:


```
Python 3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13)
Type 'copyright', 'credits' or 'license' for more information
IPython 6.0.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: from tpot import TPOTClassifier

In [2]: from deap import creator

In [3]: tpot_str = 'XGBClassifier(RobustScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegr
   ...: ession__dual=False, LogisticRegression__penalty=l1)), XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth
   ...: =5, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier_
   ...: _subsample=0.9500000000000001)'

In [4]: tpot = TPOTClassifier()

In [5]: deap_ind = creator.Individual.from_string(tpot_str, tpot._pset)

In [6]: deap_ind
Out[6]:
[<deap.gp.Primitive at 0x7eff18b639f8>,
 <deap.gp.Primitive at 0x7eff18b636d8>,
 <deap.gp.Primitive at 0x7eff18b63138>,
 <deap.gp.Terminal at 0x7eff18b5ba20>,
 <deap.gp.Terminal at 0x7eff18b80120>,
 <deap.gp.Terminal at 0x7eff18b803f0>,
 <deap.gp.Terminal at 0x7eff18b80438>,
 <deap.gp.Terminal at 0x7eff18b8bdc8>,
 <deap.gp.Terminal at 0x7eff18b8bfc0>,
 <deap.gp.Terminal at 0x7eff184e5318>,
 <deap.gp.Terminal at 0x7eff184e5750>,
 <deap.gp.Terminal at 0x7eff184e5798>,
 <deap.gp.Terminal at 0x7eff184e5cf0>]

In [7]: print(deap_ind)
XGBClassifier(RobustScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=False, LogisticRegression__penalty=l1)), XGBClassifier__learning_rate=0.1, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9500000000000001)
``` ",somehow work environment check log python custom default mar type information enhanced interactive python type help import import creator print,issue,negative,neutral,neutral,neutral,neutral,neutral
359707524,"Maybe some kind of 'pipeline similarity score'  could help in this case. perhaps something similar to the Levenshtein distance for sequences [link](https://en.wikipedia.org/wiki/Levenshtein_distance). I think this may also be useful for other issues. I had some in mind earlier, but can't recall them right now. ",maybe kind similarity score could help case perhaps something similar distance link think may also useful mind ca recall right,issue,positive,positive,positive,positive,positive,positive
359529755,Thank you. It seems a useful feature for scalability. Unfortunately I don't know enough about the python backend to provide any insights. Thanks for pointing out the source of the constraint. It is something I was not aware of.,thank useful feature unfortunately know enough python provide thanks pointing source constraint something aware,issue,positive,positive,neutral,neutral,positive,positive
359528029,"Hey, the issue is solved. This is the terminal output for FYI

Collecting tpot
Collecting deap>=1.0 (from tpot)
Requirement already satisfied: scikit-learn>=0.18.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tpot)
Requirement already satisfied: numpy>=1.12.1 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tpot)
Collecting update-checker>=0.16 (from tpot)
  Using cached update_checker-0.16-py2.py3-none-any.whl
Requirement already satisfied: tqdm>=4.11.2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tpot)
Requirement already satisfied: scipy>=0.19.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from tpot)
Collecting pandas>=0.20.2 (from tpot)
  Using cached pandas-0.22.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl
Collecting stopit>=1.1.1 (from tpot)
Requirement already satisfied: requests>=2.3.0 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from update-checker>=0.16->tpot)
Requirement already satisfied: python-dateutil>=2 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandas>=0.20.2->tpot)
Requirement already satisfied: pytz>=2011k in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from pandas>=0.20.2->tpot)
Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (from python-dateutil>=2->pandas>=0.20.2->tpot)
Installing collected packages: deap, update-checker, pandas, stopit, tpot
  Found existing installation: pandas 0.19.2
    Uninstalling pandas-0.19.2:
      Successfully uninstalled pandas-0.19.2
Successfully installed deap-1.2.2 pandas-0.22.0 stopit-1.1.1 tpot-0.9.2 update-checker-0.16",hey issue terminal output requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied six collected found installation successfully uninstalled successfully,issue,positive,positive,positive,positive,positive,positive
359498144,This is just a example and I think it should rarely happen. But mutations or crossover sometimes can randomly generate this case.,example think rarely happen crossover sometimes randomly generate case,issue,negative,negative,neutral,neutral,negative,negative
359497101,Aren't such cases (eg 10x pca + RF) be avoided by the genetic algorithms mutation scheme in the first place?,genetic mutation scheme first place,issue,negative,positive,positive,positive,positive,positive
359493913,"We had a few similar issues related to making tpot object pickleable. I think we need put this issue in the roadmap of tpot development now. 

I tried to find some workarounds to make tpot object pickleable, but they all failed. The main issue is that the dynamically generated classes (check `operator_utils.py`) are not pickleable.

I will keep working on this one. Any idea or contribution is welcome!",similar related making object think need put issue development tried find make object main issue dynamically class check keep working one idea contribution welcome,issue,positive,positive,positive,positive,positive,positive
359492011,"For now, you could try to use [`TPOT light`](http://epistasislab.github.io/tpot/using/#built-in-tpot-configurations) configuration to avoid this case. About disabling time-consuming operators during optimization based on a number of timeouts, I think it is a good idea. But it is not easy to tell which operator in a pipeline are the most time-consuming during optimization. For example, if there are 10 pipelines including both PCA and RandomForestClassifier  in a generation, and all of them cannot complete evaluation in a time limit. It is possible to disable both operators if I set the maximum of timeout times as 10. Based on my experience, we should disable `RandomForestClassifier ` which is usually more time-consuming and keep PCA in the config. I think we need find a better way to disable operators during optimization. ",could try use light configuration avoid case optimization based number think good idea easy tell operator pipeline optimization example generation complete evaluation time limit possible disable set maximum time based experience disable usually keep think need find better way disable optimization,issue,positive,positive,positive,positive,positive,positive
359461242,"I see, thank you. I think a work-around for persistent state saving is then setting the time limits very high and running the session inside a virtual machine. Is making the tpot object pickleable in the roadmap of development?",see thank think persistent state saving setting time high running session inside virtual machine making object development,issue,negative,positive,positive,positive,positive,positive
359438423,"I think this is because the iris benchmark is a simple problem and `train_test_split` could randomly generate a perfect training split, which make the average CV scores on training set is 1 but not for testing set. 

Please set different `random_state` (like 42) in `train_test_split` and `TPOTClassifier` and you should get different results.

",think iris simple problem could randomly generate perfect training split make average training set testing set please set different like get different,issue,positive,positive,neutral,neutral,positive,positive
359436473,"`warm_start` in [TPOT API](http://epistasislab.github.io/tpot/api/) maybe what you need. But it need keep the Python Interactive Console alive since so far tpot object is not pickleable. 

",maybe need need keep python interactive console alive since far object,issue,negative,positive,neutral,neutral,positive,positive
359402052,"There is a solution in the [link](https://python-forum.io/Thread-Modules-issue-pip3-download-modules-only-to-pyhton3-5-2-not-the-latest-3-6-1)

Please try `python3 -m pip install tpot`",solution link please try python pip install,issue,positive,neutral,neutral,neutral,neutral,neutral
359399548,I am still not sure what happened to python3/pip3 in your environment. Could you please provide more details about versions of pip3 and python3. Here is a [link](https://www.quora.com/What-is-difference-between-pip-and-pip3) that may help. I highly suggest use `conda` for managing your Python environments,still sure environment could please provide pip python link may help highly suggest use python,issue,positive,positive,positive,positive,positive,positive
359333485,"/usr/local/bin/python
/usr/local/bin/python
/usr/bin/python


/usr/local/bin/pip
/usr/local/bin/pip

/usr/local/bin/python3
/usr/local/bin/python3

/usr/local/bin/pip3
/usr/local/bin/pip3

Thats the exact output for the commands above given above for python and python3 both. Its weird that it works for python2 but not python3

All other libraries like tensorflow, keras etc.. have no issues so far.",thats exact output given python python weird work python python like far,issue,negative,negative,neutral,neutral,negative,negative
359264590,"try 
```
which -a python
which -a pip
```
The first entry of the return is the path that is called if you type python/pip in your terminal (assuming you use Mac) 
Then check if your pip put tpot in the right directory. The right directory is referred to in your python under $sys.path. You can check your sys.path this way:

```
import sys
print(""\n"".join(sys.path))
```


Also check that your .bash_profile (or .zshrc as in my case) contains
```
export PYTHONPATH=$PYTHONPATH:/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages
```
while the path after `$PYTHONPATH:` should be where tpot is. To find your tpot, start each installed python and check each sys.path returns ending with site-packages for tpot. 

If it still doesn't work try to append your tpot path to $sys.path of your preferred python.",try python pip first entry return path type terminal assuming use mac check pip put right directory right directory python check way import print also check case export path find start python check ending still work try append path preferred python,issue,negative,positive,positive,positive,positive,positive
359085093,"Hmm, weird. Could you plz let me know the pathes of your `python` and `pip` in your environment via the commands below:
```
which python
which pip
# or which pip3
``` ",weird could let know python pip environment via python pip pip,issue,negative,negative,negative,negative,negative,negative
358687895,@bartdp1 thank you. I think the naive way is the best way for now - I'm going to close this out. Thank you for the reconfirmation.,thank think naive way best way going close thank reconfirmation,issue,positive,positive,positive,positive,positive,positive
358582325,"@pliablepixels possibly the most naive way to do this is to simply generate a random feature matrix (e.g. random combinations what you refer to as A,B,C,D, given the possible values for ABCD), predict using your fitted pipeline, and keep all samples for which the predicted value is 1. 

Any other way would require knowledge of the decision making process within the pipeline, which would be difficult for the pipeline which was selected in your case.

Another options, somewhat unrelated to TPOT, would be to use a simpler model, for which the decision making process is easier to investigate. See for example: http://scikit-learn.org/stable/modules/tree.html#classification",possibly naive way simply generate random feature matrix random refer given possible predict fitted pipeline keep value way would require knowledge decision making process within pipeline would difficult pipeline selected case another somewhat unrelated would use simpler model decision making process easier investigate see example,issue,negative,negative,negative,negative,negative,negative
358418220,"Hmm, for now TPOT cannot stack estimators for ensembling models. We have a idea about TPOTensemble #479 but it is very computational expensive. I suppose that you may want use the good pipelines in `tpot_object._pareto_front` since the final optimized pipeline was got from the pareto front (check [codes](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/base.py#L624-L630)) and you could optimized pipeline in pareto front to scikit-learn pipeline with [this one line of code](https://github.com/EpistasisLab/tpot/blob/v0.9/tpot/base.py#L677) (also it's the way to generate the final pipeline `tpot_object.fitted_pipeline_` within TPOT).",stack idea computational expensive suppose may want use good since final pipeline got front check could pipeline front pipeline one line code also way generate final pipeline within,issue,negative,positive,neutral,neutral,positive,positive
358245136,"I think Jonathan is right and there is data leakage. I actually ran into this problem myself when implementing something similar. I decided to ignore this for now for several reasons:

1. Others found it had little impact on overfitting (e.g.  from a top kaggler's [blogpost](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/)):
> This is leakage and in theory S could deduce information about the target values from the meta features in a way that would cause it to overfit the training data and not generalize well to out-of-bag samples. However, you have to work hard to conjure up an example where this leakage is significant enough to cause the stacked model to overfit. In practice, everyone ignores this theoretical hole (and frankly I think most people are unaware it even exists!).

2. If you want to avoid it, the evaluation process requires (potentially a lot) more computation time. Consider training a StackingEstimator with K-Fold you propose, in this case the estimator will generate out-of-sample predictions for each row in your data (so without data leakage), and the rest of the pipeline can then use this to make further predictions. This means instead of training one model, you trained K models. You need to do this for each fold in the cross validation of your pipeline (by default, 5). 
Then, consider adding another StackingEstimator, SE A, before your first StackingEstimator, SE B (i.e. the pipeline is now (SE A)->(SE B)->(final estimator)). SE A needs to provide predictions for SE B, for each fold SE B uses to create predictions. SE A can't simply be trained in K folds on all of the data, because then it would leak some data for SE B (i.e. if SE B makes predictions on a certain fold, SE A can not have been trained on any data of that fold). This means that to provide SE B with leak-free predictions, it needs to be trained K times for each time SE A is trained. So we see an exponential growth in the number of models to be trained to create a leak-free estimate (i.e. K^S where K is the number of folds you use per stacking estimator, and S is the number of sequential estimators). Especially in an evolutionary algorithm where we want to evaluate hundreds to tens of thousands (or more) pipelines, this is something to consider.

3. While the stacking estimator is trained with this leak, the evaluation of the entire pipeline will still be on out-of-sample rows. This means the StackingEstimator's final model will not actually be trained on data of that fold. The leakage might introduce a bias to use more of the StackingEstimator's predictions, but if they turn out not to generalize well, they will soon enough be dropped.

All in all, in theory, I do think you should avoid this issue. In practice, I am not convinced the benefits will outweigh the costs.

Please let me know if you see any issues with the statements above :)",think right data leakage actually ran problem something similar decided ignore several found little impact top leakage theory could deduce information target meta way would cause overfit training data generalize well however work hard conjure example leakage significant enough cause model overfit practice everyone theoretical hole frankly think people unaware even want avoid evaluation process potentially lot computation time consider training propose case estimator generate row data without data leakage rest pipeline use make instead training one model trained need fold cross validation pipeline default consider another se first se pipeline se se final estimator se need provide se fold se create se ca simply trained data would leak data se se certain fold se trained data fold provide se need trained time time se trained see exponential growth number trained create estimate number use per estimator number sequential especially evolutionary algorithm want evaluate something consider estimator trained leak evaluation entire pipeline still final model actually trained data fold leakage might introduce bias use turn generalize well soon enough theory think avoid issue practice convinced outweigh please let know see,issue,positive,positive,neutral,neutral,positive,positive
358067580,Here are a few old discussions (#214 #360 and #457) about stacking features in TPOT. `StackingEstimator` in TPOT is a transformer to use results of some estimator as synthetic feature(s) so that the final `X_transformed` could be n+1 or n+2 (here n is the number of features). I think `StackingEstimator` may not use final estimator since it could be a intermediate step between two other transformers in a pipeline. @rhiever Any idea?,old transformer use estimator synthetic feature final could number think may use final estimator since could intermediate step two pipeline idea,issue,negative,positive,neutral,neutral,positive,positive
357697866,What is your population size? I think the reason should be that the search space is limited with that configuration dictionary.,population size think reason search space limited configuration dictionary,issue,negative,negative,neutral,neutral,negative,negative
357695047,"I restricted the max eval time of each pipeline using `max_eval_time_mins=` and the XGBRegressor function seems to pop up now sometimes, although TPOT is still favoring GradientBoostingRegressor. So there isn't any bug here but it's an interesting observation as I thought XGBoost has been shown to outperform the naive version of gradient boosting both in time and performance. Thank you for the help.",restricted time pipeline function pop sometimes although still favoring bug interesting observation thought shown outperform naive version gradient time performance thank help,issue,positive,positive,neutral,neutral,positive,positive
357363031,"My apologies for the confusing phrasing. Yes, I am referring to being able to generate ""new"" positive classification items (They don't exist).

A concrete example: (the example is made up)

I've been given a dataset that has 10,000 rows of 'car color, car model, state, driver age' (X) and a classification of each row (Y) that is ""hasAccident"" which is 1 or 0.

Using TPOT, I came up with a great classifier.

Now I'd like to be able to generate _new_ combinations of 'car color, car model, state, driver age' that might result in hasAccident=1.  The question therefore is ""generate a new tuple that will have a high chance if accident=1"" as opposed to ""given a new tuple, tell me what is the likelihood of having an accident""

Thank you.
",phrasing yes able generate new positive classification exist concrete example example made given color car model state driver age classification row came great classifier like able generate color car model state driver age might result question therefore generate new high chance opposed given new tell likelihood accident thank,issue,positive,positive,positive,positive,positive,positive
357360503,"Good to hear that TPOT works well for you. I am a little confused about your question here. Could you please explain more what is ""new values of A, B, C, D"". Is it new dataset? If so, you may need refit TPOT on new dataset for best pipeline. I think there is no best pipeline for all the data.",good hear work well little confused question could please explain new new may need refit new best pipeline think best pipeline data,issue,positive,positive,positive,positive,positive,positive
357358098,"`pipeline_optimizer.evaluated_individuals_` it is a python dictionary.

BTW, we could just use `pipeline_optimizer.fit(x_train2,y_train2)` instead of `model = pipeline_optimizer.fit(x_train2,y_train2)`",python dictionary could use instead model,issue,negative,neutral,neutral,neutral,neutral,neutral
357353462,"Thank you. Here is how I'm setting up the process after importing all the packages and loading the data set.

```
pipeline_optimizer = TPOTRegressor(generations=50, population_size=15, cv=5,n_jobs=-1,
       random_state=42, verbosity=3,max_eval_time_mins=2000,scoring='neg_mean_absolute_error')
model = pipeline_optimizer.fit(x_train2,y_train2)
```

How would I grab the `evaluated_individuals_` from the pipeline_optimizer.fit()?",thank setting process loading data set model would grab,issue,negative,neutral,neutral,neutral,neutral,neutral
357345161,"Hmm, it is interesting. Please check [TPOT API for regression](http://rhiever.github.io/tpot/api/#regression). You may check the `evaluated_individuals_` attribute to find out if xgboost regression has been evaluated in the optimization process.",interesting please check regression may check attribute find regression optimization process,issue,positive,positive,positive,positive,positive,positive
357029615,"Argh, how incredibly stupid of me. I did not see the word 'Classifier' and was using it blindly. Thank you for your attention and sorry for wasting your time!",incredibly stupid see word blindly thank attention sorry wasting time,issue,negative,negative,negative,negative,negative,negative
357029299,Please try TPOTRegressor instead of TPOTClassifier for regression problems. Let me know if it will solve this issue.,please try instead regression let know solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
357024832,"No it's a regression problem (I'm trying to create a model to predict values of Y given fields of X). Y values can be negative or positive.

X: (999, 15)
Y: (999, 1)

(I'm currently reading 1000 records as a test)
",regression problem trying create model predict given negative positive currently reading test,issue,negative,negative,neutral,neutral,negative,negative
356713567,"Hmm, it is weird. It should be the same since `pipeline_optimizer.score` just use `pipeline_optimizer.fitted_pipeline_` to estimate score as shows [in this part of codes](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L828-L854). Could you please provide a demo to reproduce this issue?",weird since use estimate score part could please provide reproduce issue,issue,negative,negative,negative,negative,negative,negative
356326050,"I have another look on this issue. I think this issue is related to whether the customized scorer is pickable in parallel computing using joblib.

I can reproduce this issue using GridSearchCV from sklearn instead of using tpot (examples below). It seems that scorer is not pickable somehow.

```
from sklearn import linear_model, metrics
from sklearn.model_selection import GridSearchCV
import numpy as np
np.random.seed(42)
X_train = np.random.random((1000,10))
y_train = np.random.random(1000)

def RMSLE(p,a):
    return np.sqrt(np.mean( (np.log(p+1) - np.log(a+1))**2 ))

rmsle_score = metrics.make_scorer(RMSLE,greater_is_better=False)
parameters = {'fit_intercept':(True, False), 'normalize':[True, False]}
regr = linear_model.LinearRegression()
reg1 = GridSearchCV(regr, parameters, verbose=2,scoring=rmsle_score,n_jobs=-1)
reg1.fit(X_train, y_train)
```

Maybe it is a issue in sklearn's repo.",another look issue think issue related whether scorer pickable parallel reproduce issue instead scorer pickable somehow import metric import import return true false true false reg maybe issue,issue,positive,negative,neutral,neutral,negative,negative
355851545,"Groovy. Come `0.9.2`, we'll run the tests in the packaging step on `conda-forge`!",groovy come run step,issue,negative,neutral,neutral,neutral,neutral,neutral
355774073,"Thanks @bollwyvl! Looks like the tests were timing out. Hopefully not a regular issue, otherwise we'll have to dig into the specific tests that are timing out.",thanks like timing hopefully regular issue otherwise dig specific timing,issue,positive,positive,neutral,neutral,positive,positive
355708576,"Hmm, I am not with my laptop now. Please try to push a null comment (e.g. “clean codes”) to restart the test. ",please try push null comment clean restart test,issue,positive,positive,positive,positive,positive,positive
355706263,"Eek... Looks like some Network weirdness for various package managers. I'd
try a restart of the job?

Probably possible to do some more caching...

On Fri, Jan 5, 2018, 18:20 Weixuan Fu <notifications@github.com> wrote:

> Thank you! It is weird that 2 ci tests on Linux failed.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/649#issuecomment-355691975>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AACxRA1gbqAxj9ZLLJ9Mi49PZAUq0SnAks5tHq4-gaJpZM4RU90m>
> .
>
",like network weirdness various package try restart job probably possible fu wrote thank weird thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
355431521,"No problem, I think it is a good issue that we need fix for updated version of TPOT.",problem think good issue need fix version,issue,negative,positive,positive,positive,positive,positive
355430459,"Please check the issue #645 . I think it is a notebook-related issue. You may not use n_jobs > 1 with customized scoring functions using current version (0.9.1) of TPOT in Jupyter notebook. Also I fixed another bug in the new scoring API and merged into dev branch. I will work on this issue and release a patch soon.
  
  ",please check issue think issue may use scoring current version notebook also fixed another bug new scoring dev branch work issue release patch soon,issue,negative,positive,neutral,neutral,positive,positive
355414986,Thank you once again for bearing with me. Appreciate it. I am closing this issue now,thank bearing appreciate issue,issue,positive,neutral,neutral,neutral,neutral,neutral
355361240,"The alternative way is just a work around for these indexes in your case. The best way is to reindex the training set to 0-7949 in this case.
  ",alternative way work around case best way training set case,issue,positive,positive,positive,positive,positive,positive
355356988,"But using the .fit() function we are training the whole dataset, including the validation set. Wouldn't that be fallacious?",function training whole validation set would fallacious,issue,negative,positive,positive,positive,positive,positive
355330073,"Yes, TPOT functionality evaluates the best regressor based on the train/test split given by iter_cv",yes functionality best regressor based split given,issue,positive,positive,positive,positive,positive,positive
355329621,"That is a good idea. I tried doing it and it works fine.
However, I have one concern. Wouldn't it be fallacious to choose the classifier by training it on the whole dataset rather than dividing it into part train and part validation set? Or does the TPOT functionality evaluates the best regressor based on the train/test split given by iter_cv? ",good idea tried work fine however one concern would fallacious choose classifier training whole rather dividing part train part validation set functionality best regressor based split given,issue,positive,positive,positive,positive,positive,positive
355295961,"OK, I understand the issue here now. I think reindexing `train_cv_df` pandas dataframe and `iter_cv` may help since so far tpot.fit() do not fully support the index of pandas dataframe as input for cv. We will add this support later.

Also, since your cv is specified in the `iter_cv` and all indexes in the list are matched to the training subset from `final_df`, you can use `tpot.fit(final_df.iloc[:,final_df.columns != 'count'].values, final_df.iloc[:, 6].values)` instead so that CV in tpot still use samples in training set for pipeline evaluation. But after tpot.fit(), you need refit `tpot.fitted_pipeline_` with your training set with command `tpot.fitted_pipeline_.fit(X_train, Y_train)`.",understand issue think may help since far fully support index input add support later also since list training subset use instead still use training set pipeline evaluation need refit training set command,issue,positive,positive,neutral,neutral,positive,positive
355237001,"My task is to predict the geolocation. so the training data label is geolocation(Longitude,Latitude). I think custom TPO configuration can not work for me.  ",task predict training data label longitude latitude think custom configuration work,issue,negative,neutral,neutral,neutral,neutral,neutral
355196017,"Number of rows in X_train is 7950 but the index values are different and it falls within the max range of the iter_cv which is 10777. Also, the max value of iter_cv i.e 10777 is correct because that is the size of the final_df dataframe
![image](https://user-images.githubusercontent.com/31832306/34549929-a4a32fd8-f0da-11e7-9f53-aad6fbdcc777.png)

If you take the example of the Boston dataset that you have provided, the no of rows in x_train and test are 379,127 respectively and they encompass subset of index values based on the split. Also, the length of the values of the train/validation indices are smaller than these values because 75% has been allocated to train and the remaining to test. 
![image](https://user-images.githubusercontent.com/31832306/34550211-f75a5a24-f0dc-11e7-870c-c8f8ba167d0d.png). 


 




  ",number index different within range also value correct size image take example boston provided test respectively encompass subset index based split also length index smaller train test image,issue,positive,neutral,neutral,neutral,neutral,neutral
355192386,"In your code `tpot.fit(X_train, Y_train)`, X_train with 7950 rows was fed to tpot but the maximum value of iter_cv is 10777. So I still feel that it is a index mismatch issue. I think fitting on the whole final_df with iter_cv will works",code fed maximum value still feel index mismatch issue think fitting whole work,issue,negative,positive,positive,positive,positive,positive
355187664,"I have updated the code and now there is no discrepancy between the indices of iter_cv and train_cv_df and test_cv_df.

`train_cv_df=pd.DataFrame(columns=final_df.columns)
test_cv_df=pd.DataFrame(columns=final_df.columns)
iter_cv=[]

tr_temp=[]
val_temp=[]

for idx,row in final_df.iterrows():
    #print (pd.DataFrame(row).T)
    
    try:
        if (final_df.at[idx,'month']!=final_df.at[idx-1,'month']):
            
                    
            train_cv_df=train_cv_df.append(final_df.ix[tr_temp])
    
            test_cv_df=test_cv_df.append(final_df.ix[val_temp])
            
            iter_cv.append((np.array(tr_temp),np.array(val_temp)))
            
            tr_temp=[]
            val_temp=[]
            
        elif (idx==final_df.shape[0]-1):
            print (idx)
            
            if (final_df.at[idx,'day']) in list(range(1,15)):
          
                tr_temp.append(idx)
            else:
                val_temp.append(idx)
            train_cv_df=train_cv_df.append(final_df.ix[tr_temp])
    
            test_cv_df=test_cv_df.append(final_df.ix[val_temp])
            iter_cv.append((np.array(tr_temp),np.array(val_temp)))
            
    except Exception as e:
        print (e)
        
        
    if (final_df.at[idx,'day']) in list(range(1,15)):
        #print ('oye')
        #train_cv_df.at[idx]
        #train_cv_df=train_cv_df.append(pd.DataFrame(row),ignore_index=False)
        #train_cv_df=pd.concat([train_cv_df,pd.DataFrame(row)],axis=0)
        tr_temp.append(idx)
        
    else:
        #test_cv_df=test_cv_df.append(pd.DataFrame(row),ignore_index=False)
        #test_cv_df=pd.concat([test_cv_df,pd.DataFrame(row)],axis=0)
        #val_+str(i).append(idx)
        val_temp.append(idx)

    `
After deleting null values, the X_train rows come to 7950 and Y_test rows sum upto 2827 which sum upto the maximum value of iter_cv and final_df.shape which is 10777. The problem still persists. I am pretty sure that there is no mismatch in the index values ",code discrepancy index row print row try print list range else except exception print list range print row row else row row null come sum sum maximum value problem still pretty sure mismatch index,issue,positive,positive,positive,positive,positive,positive
355143521,"Yes I dropped the NAs too, forgot to mention that in the code. I think will have to check the value of the indices because the total values in the final_df dataframe is 10777 which do not match with the 10320 and get back to you. Anyway, thank you so much for your help, appreciate it ",yes forgot mention code think check value index total match get back anyway thank much help appreciate,issue,positive,positive,neutral,neutral,positive,positive
355139283,"I checked the codes and dataset. I found that there are many NAs in `train_cv_df` and `test_cv_df` so I drop these nans and then put them into TPOTRegressor. `iter_cv` is 23 splits and the largest index is 10320 but the total row number in the train_X (after dropping these nans) is 7950. I think the largest index is out of limit. But if you used the first 15 splits of `iter_cv` (`tpot = TPOTRegressor(generations=10, population_size=50, verbosity=3,n_jobs=-1,cv=iter_cv[:15])`) in which no index is out of limit, then TPOT will works fine.",checked found many drop put index total row number dropping think index limit used first index limit work fine,issue,negative,positive,positive,positive,positive,positive
355128636,"Hmm, now I think it is a notebook-related issue and it also related to the scoring API for customized scoring function. I will look into it and refine the API. Thank you for report this issue here. ",think issue also related scoring scoring function look refine thank report issue,issue,negative,neutral,neutral,neutral,neutral,neutral
355116208,"Hmm, it is weird. Could you upload tsv or csv file for `final_df` here? I don't think I can reproduce this issue with some benchmarks.",weird could file think reproduce issue,issue,negative,negative,negative,negative,negative,negative
355113082,"The iter_cv works fine with the cross_val_score
![image](https://user-images.githubusercontent.com/31832306/34537402-494547b4-f096-11e7-84f4-843e6c590d01.png)

Demo example:

'final_df' dataframe contains the pre-processed values as shown in the below figure
![image](https://user-images.githubusercontent.com/31832306/34537483-972bb2ec-f096-11e7-8456-84c0bd6e2f30.png)

I have written a function that creates an iterable object (iter_cv) by looping through the dataframe and creating train-test splits such as train data includes all days with 1-14 and validation set includes the remaining part. Also, during the loop I have also created train_cv_df and test_cv_df which are dataframes that store the actual values of the split. This will be later fed to the TPOTRegressor

`for idx,row in final_df.iterrows():
    #print (pd.DataFrame(row).T)
    
    try:
        if (final_df.at[idx,'month']!=final_df.at[idx-1,'month']):
            
                    
            train_cv_df=train_cv_df.append(final_df.ix[tr_temp])
    
            test_cv_df=test_cv_df.append(final_df.ix[val_temp])
            
            iter_cv.append((np.array(tr_temp),np.array(val_temp)))
            
            tr_temp=[]
            val_temp=[]
            
    except Exception as e:
        print (e)
        
    if (final_df.at[idx,'day']) in list(range(1,15)):
        #print ('oye')
        #train_cv_df.at[idx]
        #train_cv_df=train_cv_df.append(pd.DataFrame(row),ignore_index=False)
        #train_cv_df=pd.concat([train_cv_df,pd.DataFrame(row)],axis=0)
        tr_temp.append(idx)
        
    else:
        #test_cv_df=test_cv_df.append(pd.DataFrame(row),ignore_index=False)
        #test_cv_df=pd.concat([test_cv_df,pd.DataFrame(row)],axis=0)
        #val_+str(i).append(idx)
        val_temp.append(idx)

    

print (iter_cv)`

The train/validation set are then converted to numerical format

`X_train=train_cv_df.iloc[:,train_cv_df.columns!='count'].values

Y_train=train_cv_df.iloc[:,6].values


X_test=test_cv_df.iloc[:,test_cv_df.columns!='count'].values

Y_test=test_cv_df.iloc[:,6].values`

FInally, the TPOT function is used to run the AutoML

`
from tpot import TPOTRegressor


tpot = TPOTRegressor(generations=10, population_size=50, verbosity=2,n_jobs=-1,cv=iter_cv)
print ('aaaaaaaaaaaaaaaaa')
tpot.fit(X_train, Y_train)
print ('bbbbbbbbbbbbbbbb')
print(tpot.score(X_test, Y_test))
print ('ccccccccccccccccccc')
tpot.export('tpot_bike_rental.py')`

This throws the following error:
![image](https://user-images.githubusercontent.com/31832306/34537766-9c3e82d6-f097-11e7-84e2-189cdafb3ba7.png)

When the cv parameter is used without the iter_cv, i.e a numerical value, the code runs fine. I have also used the iter_cv iterable with cross_val_score and GridSearchCV and it works fine for both of them. 








",work fine image example shown figure image written function iterable object looping train data day validation set part also loop also store actual split later fed row print row try except exception print list range print row row else row row print set converted numerical format finally function used run import print print print print following error image parameter used without numerical value code fine also used iterable work fine,issue,negative,positive,positive,positive,positive,positive
355096257,"Could you try to use the `iter_cv` with the [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) instead?

TPOT should not change the input cv and just pass it to `cross_val_score`. 

Or could you please provide a minimum demo with a example dataset here to let us reproduce this issue?",could try use instead change input pas could please provide minimum example let u reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
355038465,"I have created my own custom validation set, unlike the one that has been mentioned by you which uses train_test_split.

`train_cv_df=pd.DataFrame(index=final_df.index,columns=final_df.columns)
test_cv_df=pd.DataFrame(index=final_df.index,columns=final_df.columns)
iter_cv=[]

tr_temp=[]
val_temp=[]

for idx,row in final_df.iterrows():
    #print (pd.DataFrame(row).T)
    
    try:
        if (final_df.at[idx,'month']!=final_df.at[idx-1,'month']):
            
            
            iter_cv.append((np.array(tr_temp),np.array(val_temp)))
            
            tr_temp=[]
            val_temp=[]
            
    except Exception as e:
        print (e)
        
    if (final_df.at[idx,'day']) in list(range(1,15)):
        #print ('oye')
        train_cv_df=train_cv_df.append(pd.DataFrame(row).T,ignore_index=True)
        #train_cv_df=pd.concat([train_cv_df,pd.DataFrame(row)],axis=0)
        tr_temp.append(idx)
    else:
        test_cv_df=test_cv_df.append(pd.DataFrame(row).T,ignore_index=True)
        #test_cv_df=pd.concat([test_cv_df,pd.DataFrame(row)],axis=0)
        #val_+str(i).append(idx)
        val_temp.append(idx)`

X_train=train_cv_df.iloc[:,train_cv_df.columns!='count'].values

Y_train=train_cv_df.iloc[:,6].values


X_test=test_cv_df.iloc[:,test_cv_df.columns!='count'].values

Y_test=test_cv_df.iloc[:,6].values`


The iter_cv is an iterbale which is a list of tuples containing indices of train and validation/test set.
This iterable when used with the GridSearchCV method works fine, but when used with the TPOTRegressor method, throws the aforementioned error while running the fit method. 
Also, when replacing cv with a numeric constant, the program runs fine
  ",custom validation set unlike one row print row try except exception print list range print row row else row row list index train set iterable used method work fine used method error running fit method also constant program fine,issue,negative,positive,positive,positive,positive,positive
355028343,"I tested TPOTRegressor with KFold iterator but did not reproduce this issue. Please check the test codes below. Could you please provide more details about this issue?

```
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split, KFold

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25)
cv = KFold(n_splits=3)
cv_iter = list(cv.split(X_train, y_train))
tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, cv=cv_iter)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```
",tested reproduce issue please check test could please provide issue import import import housing list print,issue,positive,neutral,neutral,neutral,neutral,neutral
354806084,"I rechecked the issue. I think there is a bug in new scoring api. Check PR #626

Try reinstall TPOT with this fix via the command below
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@scoring_api_bug
```

I think your original codes without reseting start mode in `multiprocessing` will work in jupyter. I tested it in my MacOS",issue think bug new scoring check try reinstall fix via command pip install upgrade think original without start mode work tested,issue,negative,positive,positive,positive,positive,positive
354802061,"I should add that I executed the same code in a `test.py` and it seems to work **sometimes** (not freezing).


```python
import multiprocessing
import numpy as np
import tpot
import sklearn

if __name__ == '__main__':
	X_train = np.random.random((1000,10))
	y_train = np.random.random(1000)+10

	def RMSLE(p,a):
	    return np.sqrt(np.mean( (np.log(p+1) - np.log(a+1))**2 ))

	rmsle_score = sklearn.metrics.make_scorer(RMSLE,greater_is_better=False)

	reg1 = tpot.TPOTRegressor(verbosity=2, 
	                          n_jobs=-1, 
	                          scoring= rmsle_score, 
	                          cv=10, 
	                          max_time_mins=2)


	reg1.fit(X_train, y_train)
```",add executed code work sometimes freezing python import import import import return reg,issue,negative,neutral,neutral,neutral,neutral,neutral
354800413,"Hello,

After inserting (at the first cell of the notebook)
```
import multiprocessing
multiprocessing.set_start_method('forkserver')
```
Then I get quite a big error that ends up with:

```
During handling of the above exception, another exception occurred:

RuntimeError                              Traceback (most recent call last)
<ipython-input-8-84c7f7a33512> in <module>()
----> 1 reg1.fit(X_train, y_train[target[0]])

~/anaconda3/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    660                     # raise the exception if it's our last attempt
    661                     if attempt == (attempts - 1):
--> 662                         raise e
    663             return self
    664 

~/anaconda3/lib/python3.6/site-packages/tpot/base.py in fit(self, features, target, sample_weight, groups)
    651                         self._pbar.close()
    652 
--> 653                     self._update_top_pipeline()
    654                     self._summary_of_best_pipeline(features, target)
    655                     # Delete the temporary cache before exiting

~/anaconda3/lib/python3.6/site-packages/tpot/base.py in _update_top_pipeline(self)
    725             # If user passes CTRL+C in initial generation, self._pareto_front (halloffame) shoule be not updated yet.
    726             # need raise RuntimeError because no pipeline has been optimized
--> 727             raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
    728 
    729     def _summary_of_best_pipeline(self, features, target):

RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
```

can it be an error ""notebook related""?",hello first cell notebook import get quite big error handling exception another exception recent call last module target fit self target raise exception last attempt attempt raise return self fit self target target delete temporary cache self user initial generation yet need raise pipeline raise pipeline yet please call fit first self target pipeline yet please call fit first error notebook related,issue,positive,positive,positive,positive,positive,positive
354798068,[This line](https://github.com/rhiever/tpot/blob/v0.9/tpot/base.py#L455) is about the fit() function in tpot. You may tweak a customized fit() function from there.,line fit function may tweak fit function,issue,positive,positive,positive,positive,positive,positive
354797019,Please check [the alternative solution about this issue](http://rhiever.github.io/tpot/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux),please check alternative solution issue,issue,positive,neutral,neutral,neutral,neutral,neutral
354489718,">one could imagine said benchmark study using such a configuration, along with metafeatures, etc. to burn even more coal and identify correlation between datasets and approaches.

Exactly! This is the main idea behind meta-learning. I made an early attempt to do this in TPOT (paper [here](https://arxiv.org/abs/1607.08878)), but that approach didn't work well. I would encourage all work toward that goal.",one could imagine said study configuration along burn even coal identify correlation exactly main idea behind made early attempt paper approach work well would encourage work toward goal,issue,positive,negative,neutral,neutral,negative,negative
354475461,"@rhiever thanks for the thorough guidance and patience!

To the selection end, indeed: that sounds like a tough problem. One thing I liked about, say, the `category_encoders` approach is that explicitly allows for passing in columns to encode, though as suggested #602, arbitrary callables aren't supported... having a callable that accepts the data itself is getting pretty murky indeed...

Again, as I am starting out and my goal is ""learn about the landscape"", my reaction in terms of possible pipeline members is ""more is better."" Of course, if the goal is ""generate a good pipeline before the end of the universe,"" every additional possible pipeline member expands the search space even more. 

To the learning end, perhaps an annotated, discovery-focused ""kitchen sink"" could be useful, which encouraged the contribution of new configuration members.

Indeed, melting my brain further, one could imagine said benchmark study using such a configuration, along with [metafeatures](https://github.com/rhiever/sklearn-benchmarks), etc. to burn _even more_ coal and identify correlation between datasets and approaches.",thanks thorough guidance patience selection end indeed like tough problem one thing say approach explicitly passing encode though arbitrary callable data getting pretty murky indeed starting goal learn landscape reaction possible pipeline better course goal generate good pipeline end universe every additional possible pipeline member search space even learning end perhaps kitchen sink could useful contribution new configuration indeed melting brain one could imagine said study configuration along burn coal identify correlation,issue,positive,positive,positive,positive,positive,positive
354383006,"Hi @bollwyvl, thanks for your interest in TPOT and this PR. This PR has been stagnant for a while because it entailed some bigger changes to TPOT than we originally suspected. Before this PR can really be merged, we need to make sure that it *at least* doesn't make TPOT perform worse, which entails a sizable benchmark study.

The core idea of this PR is to encourage TPOT to treat categorical and continuous features differently, as TPOT has a tendency to apply preprocessing operations wholesale to the entire dataset, when some preprocessing operations weren't designed with such an application in mind. As such, this PR entails a deeper change to TPOT than adding new preprocessing operators: we need to devise a mechanism for TPOT to optionally *only* pull continuous or categorical features from a dataset without causing significant performance degradation.

I think there are many integrations of TPOT and scikit-learn-contrib packages that could be interesting. We prepackage a few different flavors of TPOT as you noted (normal, light, sparse, etc.) but ultimately wanted to leave the heavy customization of TPOT to users via the [custom config interface](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) in TPOT. We would be interested in integrating more operators into the prepackaged TPOT configurations if they show consistent promise for improving TPOT's performance on a variety of problems. That would again entail a sizable benchmark study, either within or outside of TPOT.",hi thanks interest stagnant bigger originally suspected really need make sure least make perform worse sizable study core idea encourage treat categorical continuous differently tendency apply wholesale entire designed application mind change new need devise mechanism optionally pull continuous categorical without causing significant performance degradation think many could interesting different noted normal light sparse ultimately leave heavy via custom interface would interested show consistent promise improving performance variety would entail sizable study either within outside,issue,positive,positive,positive,positive,positive,positive
354208063,"Please see [these related issues](https://github.com/rhiever/tpot/issues?utf8=%E2%9C%93&q=is%3Aissue+multi+label) and let us know if they don't address your question. TPOT doesn't support multi label classification out of the box, but it might be possible to create a [custom TPOT configuration](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) to accomplish that.",please see related let u know address question support label classification box might possible create custom configuration accomplish,issue,positive,neutral,neutral,neutral,neutral,neutral
354162153,"Hi folks, love the work.

I've been mainly focused on the build side of these things to support other developers, but am now scaling up my own understanding of the sklearniverse beyond ""will the tests run"". I like tpot because it already wraps a lot of cool things, which i can try out and play against each other.

Open PRs are always interesting, so I've been digging into this one. While I haven't tried it out locally, this one looks good, as I've certainly seen datasets that mix continuous and categorical data! 

A quick thing I notice is that some of the categorical encoders (plus a whole raft more) are already implemented over in [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding), which is already over on [conda-forge](https://github.com/conda-forge/category_encoders-feedstock).

Some questions:
- what would be the disadvantages of using `category_encoders`, besides reduced builtin functionality?
- assuming that making something ""tpot-ready"" is ""just"" adding appropriate bounds to parameters a la `regressor.py` and `classifier.py`, could these be advertised in `entry_points`, either on `tpot` or on the upstream say:
```python
# setup.py
setup(
  entry_points=[
     ""tpot.classifier"": [
         ""category_encoders"": ""tpot.config.extras.category_encoders:classifiers""
     ]
  ]

# tpot/config/extras/category_encoders.py
def classifiers():
    return {
        'category_encoders.one_hot.OneHotEncoder': {
          ...
        }
 },
```
  - could the metadata about the config flavors (sparse, lite, etc) be captured inside the config entry vs. as whole separate sets 
  - Aside from a runtime dependency on `setuptools` (or the lighter-weight `entrypoints`), what are the disadvantages of adopting `entry_points` for discovery?
- Are there any heuristics for whether a classifier/regressor is ""interesting"" to tpot? would any of the other [scikit-learn-contrib](https://github.com/scikit-learn-contrib) things match those criteria?

Looking forward to digging into this more!",hi love work mainly build side support scaling understanding beyond run like already lot cool try play open always interesting digging one tried locally one good certainly seen mix continuous categorical data quick thing notice categorical plus whole raft already already would besides reduced functionality assuming making something appropriate la could either upstream say python setup return could sparse lite inside entry whole separate aside dependency discovery whether interesting would match criterion looking forward digging,issue,positive,positive,positive,positive,positive,positive
354024986,"`tpot.builtins.OneHotEncoder` was adopted from auto-sklearn some time ago. It's treated as just another preprocessing operator and is integrated into the [sparse config](https://github.com/rhiever/tpot/blob/5516d7ef501c39efb59c2dd1ec7ba100cbe540c4/tpot/config/classifier_sparse.py#L29). This OHE implementation considers all columns with <= 10 unique values to be categorical (that threshold value is a tunable parameter), and all other columns to be continuous. It will apply OHE only to categorical columns.

It also handles missing data and treats it as a another level in the categorical features.",adopted time ago another operator sparse implementation unique categorical threshold value tunable parameter continuous apply categorical also missing data another level categorical,issue,negative,positive,neutral,neutral,positive,positive
354023003,"The TPOT project appears to contain a `tpot.builtins.OneHotEncoder` transformation. However, I couldn't find any information about by whom/where/how it is incorporated into the pipeline - am expecting that TPOT contains some ""detect categorical columns""-logic, and does it all for end users.

Suppose my input dataset contains an integer column (eg. obtained by applying `LabelEncoder` to a string column) that needs to be ""interpreted"" as categorical not continuous. How can I order TPOT to apply this `tpot.builtins.OneHotEncoder` transformation to it? Or, more generally, how can I order TPOT to split the ""mixed"" original dataset into ""clean"" categorical and continuous sub-datasets for feature engineering work?",project contain transformation however could find information incorporated pipeline detect categorical end suppose input integer column string column need categorical continuous order apply transformation generally order split mixed original clean categorical continuous feature engineering work,issue,positive,positive,positive,positive,positive,positive
354017686,"👍 on LabelEncoder!

wrt the imputer: I agree, and it's worth filing an issue to rework how TPOT handles missing data. I don't believe it's as simple as injecting an Imputer object at the beginning of every pipeline because TPOT can have ""splits"" in the pipeline that work on separate copies of the original dataset (as seen [here](https://github.com/rhiever/tpot/blob/master/images/tpot-pipeline-example.png)).",imputer agree worth filing issue rework missing data believe simple imputer object beginning every pipeline pipeline work separate original seen,issue,positive,positive,positive,positive,positive,positive
354017064,"For categorical we have CategoricalEncoder in master so you don't need LabelEncoder.

I don't understand what the question is. How to convert an sklearn pipeline to PMML?
It looks like ``make_pmml_pipeline`` does that, but ``estimator.fitted_pipeline_`` doesn't contain all the steps, and so converting it to pmml is not the same as converting ``estimator`` to PMML.

I think @vruusmann is saying that if TPOT put all steps into  ``estimator.fitted_pipeline_``  he could just use ``make_pmml_pipeline``, but since it doesn't, you'd need a custom PMML exporter for ``TPOTClassifier``.",categorical master need understand question convert pipeline like contain converting converting estimator think saying put could use since need custom exporter,issue,negative,neutral,neutral,neutral,neutral,neutral
354016378,"Good question. What do you do with regular sklearn pipelines? My first thoughts are:

* Use a LabelEncoder to convert string columns to corresponding numerical columns before passing them to the pipeline.

* Use a fixed imputation strategy (i.e., Imputer with the 'median' strategy) on datasets with missing data before passing them to the pipeline.

The above recommendations are in line with his sklearn works: sklearn assumes that the data is complete (i.e., no missingness) and numerically encoded. It leaves the handling of datasets with incomplete or string values to the user.

Maybe @amueller would have some useful comments here, as this issue goes beyond just TPOT; it applies to all sklearn pipelines.",good question regular first use convert string corresponding numerical passing pipeline use fixed imputation strategy imputer strategy missing data passing pipeline line work data complete numerically leaf handling incomplete string user maybe would useful issue go beyond,issue,positive,positive,positive,positive,positive,positive
353989361,"I'm fetching the value of `fitted_pipeline_` in order to send it to the PMML conversion utility:
```Python
from sklearn2pmml import make_pmml_pipeline, sklearn2pmml

estimator = TPOTClassifier(...)
estimator.fit(X, y)
pmml_pipeline = make_pmml_pipeline(estimator.fitted_pipeline_)

sklearn2pmml(pmml_pipeline, ""Pipeline.pmml"")
```

Everything seems to work fine (ie. TPOT and (J)PMML predictions are 100% the same) when the dataset contains numeric features:
https://github.com/jpmml/jpmml-sklearn/blob/master/src/test/resources/extensions.py

I still need to figure out a solution for the following two problem areas:
- How to prepare categorical features? Ideally, I'd like to be able to pass string columns (ie. categorical features) directly to the `TPOTEstimator.fit(X, y)` method.
- How to prepare missing values (both continuous and categorical features)? I can see from the above comments that TPOT estimator classes perform value imputation internally. However, it would be necessary to expose the state of fitted imputers to end users somehow (so that the PMML conversion utility could write down the corresponding logic) - why not insert them into the first position of the `fitted_pipeline_` attribute?",fetching value order send conversion utility python import estimator everything work fine ie still need figure solution following two problem prepare categorical ideally like able pas string ie categorical directly method prepare missing continuous categorical see estimator class perform value imputation internally however would necessary expose state fitted end somehow conversion utility could write corresponding logic insert first position attribute,issue,positive,positive,positive,positive,positive,positive
353979918,"We didn't intend for the `fitted_pipeline_` property to be interacted with in that way, although I can see why that's an issue. The alternative solution is to inject an Imputer at the beginning of every pipeline when they're internally generated, but we'll need to add extra logic that covers pipeline splits too.

cc @weixuanfu ",intend property way although see issue alternative solution inject imputer beginning every pipeline internally need add extra logic pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
353807035,"At the moment there is a situation where `TPOTRegressor.predict(X)` and `TPOTRegressor.fitted_pipeline_.predict(X)` exhibit different behaviour - the former is able to make a prediction, whereas the latter fails with an error (if the testing dataset contains missing values). Intuitively, they should behave the same.

Or am I missing some important TPOT design consideration?",moment situation exhibit different behaviour former able make prediction whereas latter error testing missing intuitively behave missing important design consideration,issue,negative,positive,neutral,neutral,positive,positive
353806216,"TPOT's imputation step isn't stored in the internal pipelines, but it should apply imputation when you call any functions like fit, predict, score, etc. Also when you export the pipeline, it should export the pipeline with the Imputer at the beginning.",imputation step internal apply imputation call like fit predict score also export pipeline export pipeline imputer beginning,issue,positive,positive,positive,positive,positive,positive
353790832,"I'm experimenting with TPOT 0.9.1, and contrary to what is claimed in the above comment, the `fitted_pipeline_` doesn't seem to contain any imputer instances.

My training dataset contains missing values, so during training TPOT reports the following:
```
Imputing missing values in feature set
```

However, when I do `print(repr(regressor.fitted_pipeline_))`, then there's no `Imputer` step anywhere to be seen. Also, the fitted pipeline fails to score a testing dataset, which contains missing values:
```
Best pipeline: DecisionTreeRegressor(RidgeCV(RobustScaler(input_matrix)), max_depth=5, min_samples_leaf=12, min_samples_split=7)
Pipeline(memory=None,
     steps=[('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,
       with_scaling=True)), ('stackingestimator', StackingEstimator(estimator=RidgeCV(alphas=(0.1, 1.0, 10.0), cv=None, fit_intercept=True, gcv_mode=None,
    normalize=False, scoring=None, store_cv_val...lit=7, min_weight_fraction_leaf=0.0,
           presort=False, random_state=None, splitter='best'))])
```",contrary comment seem contain imputer training missing training following missing feature set however print imputer step anywhere seen also fitted pipeline score testing missing best pipeline pipeline,issue,negative,positive,neutral,neutral,positive,positive
353607441,"Hmm, it seems that there are some interesting changes in this closed PR, like using Spark. Please feel free to open a issue or another PR with some description about these changes if you want to contribute to TPOT. Also, please rebase the branch to dev branch. Thank you!",interesting closed like spark please feel free open issue another description want contribute also please rebase branch dev branch thank,issue,positive,positive,positive,positive,positive,positive
352541085,"Oh,right, thank you for the fix! Merged!",oh right thank fix,issue,negative,positive,positive,positive,positive,positive
352504634,"Hello @weixuanfu thanks for the reply, @rhiever have opened this issue https://github.com/rhiever/tpot/issues/629 that maybe could be oriented in this direction as well?
I mean, sort of final and middle explanation about what is happening.",hello thanks reply issue maybe could direction well mean sort final middle explanation happening,issue,positive,negative,neutral,neutral,negative,negative
352438383,"Hi, thank you for the interesting idea here. So far, TPOT does not have a API to export the features selected in the best pipeline. We used [feature selectors](https://github.com/rhiever/tpot/blob/master/tpot/config/classifier.py#L169-L208) from scikit-learn and put them into scikit-learn [Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) objects. I think you could access the selected features within the best Pipeline object. Please check the example in the [link for scikit-learn Pipeline](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) (see codes after ""# getting the selected features chosen by anova_filter"")",hi thank interesting idea far export selected best pipeline used feature put pipeline think could access selected within best pipeline object please check example link pipeline see getting selected chosen,issue,positive,positive,positive,positive,positive,positive
351079875,Thank you for sharing the solutions here. I just recalled that the issue should be from the latest version of deap. The deap in conda is not the latest one. ,thank issue latest version latest one,issue,negative,positive,positive,positive,positive,positive
351073773,"And another thing about using 'xgboost'  that I figured out was that people must use python 64 bit. Otherwise, they will never ever be able to use xgboost although they think they've installed perfectly. I used to use python 32 bit installed with anaconda and had spend a lot of time till I figured out that I had to use python 64 bits to use xgboost. ",another thing figured people must use python bit otherwise never ever able use although think perfectly used use python bit anaconda spend lot time till figured use python use,issue,positive,positive,positive,positive,positive,positive
351072277,"Oh, hey, I found the solution and what the problem was. 
Actually, aside from the above two error messages, there was one more, which was 

/usr/local/lib/python2.7/dist-packages/deap/tools/_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow.
  ""module. Expect this to be very slow."", ImportWarning)

- And I thought it was not a big deal, but it caused the following error messages mentioned above in the previous writing.

- That deap lib related error messaged occured coz I installed that with command 'conda install deap' <- which turned out to have caused errors. 
And the solution was to install deap with command 'conda install -c conda-forge deap' (ref. https://anaconda.org/conda-forge/deap). It was the right way to install deap and since then, the error deap -related message no longer showed and neither did both of the error messaged mentioned here above.


#####################################
Issue discussion as to ""_hypervolume/pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow. "" 
- https://github.com/DEAP/deap/issues/240


Now without any error messages, TPOT perfectly runs well!!",oh hey found solution problem actually aside two error one falling back python version hypervolume module expect slow module expect slow thought big deal following error previous writing related error coz command install turned solution install command install ref right way install since error message longer neither error issue discussion falling back python version hypervolume module expect without error perfectly well,issue,negative,positive,neutral,neutral,positive,positive
351070298,"Cool, thanks. I will test the new branch. ",cool thanks test new branch,issue,positive,positive,positive,positive,positive,positive
351005589,"I did not expect a PR, because when I did the last commit it wasn't clear how Layered TPOT would be integrated. I made sure all unit tests pass locally now.",expect last commit clear layered would made sure unit pas locally,issue,positive,positive,positive,positive,positive,positive
350779558,"@PGijsbers We agree that we can put the Layered TPOT into TPOT as a separated branch. After some more tests, we eventually will merge it into master branch.  ",agree put layered branch eventually merge master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
350778445,"Hmm, I am not sure where these error messages came from. It seems something wrong with environment settings or missing module. More detailed stdout or stderr will be helpful. Could you rebuild a conda environment (check [this link](https://conda.io/docs/user-guide/tasks/manage-environments.html)) and install TPOT there via [the guide](http://rhiever.github.io/tpot/installing/). ",sure error came something wrong environment missing module detailed helpful could rebuild environment check link install via guide,issue,negative,positive,neutral,neutral,positive,positive
350548349,"oh, thanks a lot! :) I've been running a lot of examples so far haha and every example's CV score converged to 0..! It's such good news!  Once again, thanks a lot and have a great day :)!! ",oh thanks lot running lot far every example score good news thanks lot great day,issue,positive,positive,positive,positive,positive,positive
350545907,"It should be 0 for `TPOTRegressor`. The default scoring function in `TPOTRegressor` is `neg_mean_squared_error `, and 0 means 0 MSE. Please check the [TPOT API for Regression](http://rhiever.github.io/tpot/api/#regression) for more details",default scoring function please check regression,issue,negative,neutral,neutral,neutral,neutral,neutral
350020316,"@bartdp1 The `combineDFs` in TPOT is to build pipeline in tree structure by merging data frame the prediction by other classifiers or  transformed features by transformers and then pass the data frame to next step.  But since pipelines is randomly generated in the beginning so this kind of combination can be generated. 

We had some discussions in the issue #152. I think in a future version of TPOT, there are two new parameters to disable the `CombineDFs` and  `StackingEstimator` respectively. For now, you could try one of my dev branch called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `simple_pipeline`, which can disable both `StackingEstimator` and `CombineDFs` if `simple_pipeline=True` (e.g. `TPOTClassifier(simple_pipeline=True)`). But it is noted that this dev branch is not fully tested yet. You may install this branch in your test environment via the command below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
```",build pipeline tree structure data frame prediction pas data frame next step since randomly beginning kind combination issue think future version two new disable respectively could try one dev branch option disable noted dev branch fully tested yet may install branch test environment via command pip install upgrade,issue,positive,positive,neutral,neutral,positive,positive
349985090,"We updated the API since version 0.8.
Try ‘tpot_obj.evaluated_individuals_’ instead.


On Dec 7, 2017, at 9:08 AM, yan <notifications@github.com<mailto:notifications@github.com>> wrote:

evaluated_individuals
",since version try instead yan wrote,issue,negative,neutral,neutral,neutral,neutral,neutral
349976618,"Hi, thanks for your answer.
It seems like your code doesn't work now.
when I run `tpot_obj._evaluated_individuals`, error is `'TPOTClassifier' object has no attribute '_evaluated_individuals'`

",hi thanks answer like code work run error object attribute,issue,negative,positive,positive,positive,positive,positive
349748942,"@saddy001 Could you please let me know how to use `BalancedBaggingClassifier`? Especially, if `xgboost `is to be selected and tuned as the base estimator? Thanks ",could please let know use especially selected tuned base estimator thanks,issue,positive,negative,negative,negative,negative,negative
349351447,"Yes, you need edit the location of `PATH/TO/FILE`.",yes need edit location,issue,negative,neutral,neutral,neutral,neutral,neutral
349315977,@esanchezSavvyds It is good to know that it works for you. That branch is one of my test branches to test the performance of using the simple linear pipelines in TPOT vs. the tree-based ones. We need more tests and discussions before deciding to merge this branch to master branch. ,good know work branch one test test performance simple linear need merge branch master branch,issue,negative,positive,positive,positive,positive,positive
349272923,"Hello @weixuanfu ,
We have been using your ""noCDF_noStacking"" feature during this time and we didn´t have any problem. We would like to know if you have planned to finish this branch and deploy it to the master branch.

Thank you very much. ",hello feature time problem would like know finish branch deploy master branch thank much,issue,negative,positive,positive,positive,positive,positive
348248298,">My experience is that mutations and crossover, on average, have a negative effect.

I find it easy to believe that would be the case. But I do not think that changes to a very good pipeline (i.e. from the top layer), would on average yield a bad pipeline (even if it becomes worse, it won't be ""bad"" compared to the random ones in the first layer). Introducing this pipeline to the first layer might impede the ability for the first layer to find _truly new_ promising pipelines.  Again, I have no actual data to support (or disprove) this.

Perhaps we could also consider a middle ground where they are inserted into a lower layer, but not the lowest. This allows for new individuals to be formed without competition from more matured pipelines. Only in e.g. the second layer onwards would they compete with more mature pipelines. Alternatively, you would let those individuals created from mutation/crossover in higher layers only compete amongst themselves (in that case you kind of get an age-layered population idea).

If you do revert the mutated/crossed children over to the first layer as it exists now, it becomes a bit fuzzy to me as to how it would actually work. Currently, in every layer the EA is executed for a certain amount of generations. However, if crossover and mutation does not affect the layer it happens in (because children go back to the first layer), what part of the EA does remain in higher layers? Do you just wait until a certain amount of individuals are transferred from lower layers get passed on to the higher layer and then perform a selection process? Does each layer generate an equal amount of children to be put into the first layer?
",experience crossover average negative effect find easy believe would case think good pipeline top layer would average yield bad pipeline even becomes worse wo bad random first layer pipeline first layer might impede ability first layer find promising actual data support disprove perhaps could also consider middle ground inserted lower layer new formed without competition second layer onwards would compete mature alternatively would let higher compete amongst case kind get population idea revert first layer becomes bit fuzzy would actually work currently every layer ea executed certain amount however crossover mutation affect layer go back first layer part ea remain higher wait certain amount transferred lower get higher layer perform selection process layer generate equal amount put first layer,issue,positive,positive,neutral,neutral,positive,positive
348219975,"My experience is that mutations and crossover, on average, have a negative effect. That is to say that when mutating from a ""good"" solution, there are more ""bad"" steps than ""good"" ones. That's why it's important to start mutated/crossed over solutions back at ground zero, because more likely than not they are worse than their parents.

BTW: The same is true in nature.",experience crossover average negative effect say good solution bad good important start back ground zero likely worse true nature,issue,negative,positive,neutral,neutral,positive,positive
348219107,">About the new individuals in higher layer, maybe mutation/crossover could randomly generate a complex but worse pipeline,

That is definitely a possibility, but I think generally speaking the pipeline will still be pretty good (even if slightly worse), at least when mutation is applied. I believe this because generally speaking only one hyperparameter gets changed (or preprocessing step added), and quite often the effect is not very drastic (e.g. the other hyperparameters and the choice of learning algorithm together have a much higher impact). Generally speaking, not all hyperparameters matter, and those that do might only matter when tweaked in a specific range. Moreover, because the change in performance is probably very marginal, you can't assess if the pipeline is better or worse by evaluating on a small sample.

 Of course, we would have to collect data on this to make an educated decision.",new higher layer maybe could randomly generate complex worse pipeline definitely possibility think generally speaking pipeline still pretty good even slightly worse least mutation applied believe generally speaking one step added quite often effect drastic choice learning algorithm together much higher impact generally speaking matter might matter specific range moreover change performance probably marginal ca ass pipeline better worse small sample course would collect data make educated decision,issue,negative,negative,neutral,neutral,negative,negative
348197249,"@PGijsbers as you mentioned above, I agree that the layered TPOT should be turned on with a parameter in TPOTBase class.

About the new individuals in higher layer, maybe mutation/crossover could randomly generate a complex but worse pipeline, I think put it back to layer 1 could save some time to git rid of it in the lower layer.
",agree layered turned parameter class new higher layer maybe could randomly generate complex worse pipeline think put back layer could save time git rid lower layer,issue,negative,negative,negative,negative,negative,negative
348118294,"@rhiever In general I think that's probably a good approach. I have to look at how feasible this is wrt code-reuse. Layered TPOT's evaluated_individuals dict keeps track of them not just by their string representation, but also what layer(s) they were evaluated on. Unfortunately this is used in many functions meaning I would have the overwrite them in the child class (which makes it harder to update functions from just the TPOTBase class).

@weixuanfu 1. We chose to evaluate new individuals in the layer in which they are created because typically the higher layers are already more optimized. This might mean that a new individual from a higher layer may completely dominate the individuals in a lower layer, starving the diversity.
2. In this context, you can see this as a GA being applied at each layer - each having their own population. Thus `offspring_size` would be the number of offspring produced by the population of each layer. ",general think probably good approach look feasible layered track string representation also layer unfortunately used many meaning would overwrite child class harder update class chose evaluate new layer typically higher already might mean new individual higher layer may completely dominate lower layer diversity context see ga applied layer population thus would number offspring produced population layer,issue,negative,positive,positive,positive,positive,positive
348010380,"@PGijsbers Hi, Pieter, we had 2 questions about Layered TPOT:

1. For new individuals generated in higher layered, should them start over the evaluation from layer 1  to the high layer? For example, if individual A is generated in layer 3, should it just be evaluated in this layer or go back to layer 1?

2. Since new individuals can be generated within layers and the number of layers is different among generations, it means that the number of offsprings would vary in different generations. I think it make the `offspring_size` parameter lose its meaning. Maybe we need refine its definition, right?",hi layered new higher layered start evaluation layer high layer example individual layer layer go back layer since new within number different among number would vary different think make parameter lose meaning maybe need refine definition right,issue,negative,positive,positive,positive,positive,positive
347905179,I close this issue since there is no further comment in a while. Please feel free to re-open the issue (or comment further) if you have any more questions.,close issue since comment please feel free issue comment,issue,positive,positive,positive,positive,positive,positive
347904922,I close this issue since the PR is merged to dev branch. Please feel free to re-open the issue (or comment further) if you have any more questions,close issue since dev branch please feel free issue comment,issue,positive,positive,positive,positive,positive,positive
347904270,I closed this issue since there is no answer in a while. Please feel free to re-open the issue (or comment further) if you have any more questions,closed issue since answer please feel free issue comment,issue,positive,positive,positive,positive,positive,positive
347903108,"The simplest way to merge this work into TPOT would be to add a new TPOT class, such as `TPOTLayeredClassifier` etc. that inherits from `TPOTClassifier` etc.

It could eventually be merged to override the base TPOT class when it's been thoroughly tested and vetted.",way merge work would add new class could eventually override base class thoroughly tested,issue,negative,negative,negative,negative,negative,negative
347902202,Please go ahead and merge when it passes the unit tests.,please go ahead merge unit,issue,negative,neutral,neutral,neutral,neutral,neutral
347901761,I closed this issue since the fix was merged to dev branch.,closed issue since fix dev branch,issue,negative,negative,neutral,neutral,negative,negative
347895023,"Yes, after we've experimented more with regression now, I think we need to remove the `abs` from the `score` function and simply allow the user to deal with the negative values as they need to.",yes experimented regression think need remove score function simply allow user deal negative need,issue,negative,negative,negative,negative,negative,negative
347268167,"Oh Okay
It looks very complicated. I will leave it and just continue with the Version of xgboost I have at the Moment.

Nonetheless Thank you so much

Best 
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Monday, November 27, 2017 1:08 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

The latest version of xgboost cannot be installed with pip. You need rebuild it from source codes. Check the link http://xgboost.readthedocs.io/en/latest/build.html for more details. It is not easy to install xgboost in Windows, so I think you need file a issue in xgboost repo for better solution.
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",oh complicated leave continue version moment nonetheless thank much best jeff sent mail fu sent author subject install anaconda latest version pip need rebuild source check link easy install think need file issue better solution thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
347163014,"The latest version of xgboost cannot be installed with pip. You need rebuild it from source codes. Check the link http://xgboost.readthedocs.io/en/latest/build.html for more details. It is not easy to install xgboost in Windows, so I think you need file a issue in xgboost repo for better solution.",latest version pip need rebuild source check link easy install think need file issue better solution,issue,positive,positive,positive,positive,positive,positive
347114515,"Appreciate the response.

I have py-xgboost in myenv and when I try to uninstall it first it says „Cannot uninstall requirement py-xgboost, not installed“

When I tried to upgrade directly I got the following error…
(myenv) C:\Users\ijeff>pip install --upgrade xgboost
Collecting xgboost
  Using cached xgboost-0.6a2.tar.gz
No files/directories in C:\Users\ijeff\AppData\Local\Temp\pip-build-kq_wznta\xgboost\pip-egg-info (from PKG-INFO)

Conda list showed 
py-xgboost                0.60            py35np112h24854b6_0

What do you suggest?

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Monday, November 27, 2017 12:21 AM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

No problem. I think this warning is from the old version of xgboost. You may need rebuild xgboost from source codes to update xgboost.
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",appreciate response try first requirement tried upgrade directly got following pip install upgrade list suggest best jeff sent mail fu sent author subject install anaconda problem think warning old version may need rebuild source update thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
347047470,No problem. I think this warning is from the old version of xgboost. You may need rebuild xgboost from source codes to update xgboost.,problem think warning old version may need rebuild source update,issue,negative,positive,neutral,neutral,positive,positive
347046414,"Yessssssss!!!!!
Finally it is fixed.
Thank you soooooooo much.

I just wanted to ask you why I got this warning message although I have sklearn 0.19.1 installed…
I use   from sklearn import model_selection. I have not ued cross_validation at all in my Code.

C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20. ""This module will be removed in 0.20."", DeprecationWarning)

Any suggestions?
Once again Thank you for your help and appreciate your patience and support.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 11:21 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Try conda install pywin32
Please check https://rhiever.github.io/tpot/installing/ for more details.
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",finally fixed thank much ask got warning message although use import code module version favor module class also note interface new different module module removed module removed thank help appreciate patience support best jeff sent mail fu sent author subject install anaconda try install please check thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
347043350,"Try `conda install pywin32`
Please check https://rhiever.github.io/tpot/installing/ for more details.





",try install please check,issue,negative,neutral,neutral,neutral,neutral,neutral
347042148,"I got the following error after successful installation of tpot 0.9.1

# Import required libraries
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
import pandas as pd 
import numpy as np

C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\deap\tools\_hypervolume\pyhv.py:33: ImportWarning: Falling back to the python version of hypervolume module. Expect this to be very slow. ""module. Expect this to be very slow."", ImportWarning)

ImportError Traceback (most recent call last) <ipython-input-1-6e78dc33889f> in <module>() 1 # Import required libraries ----> 2 from tpot import TPOTClassifier 3 from sklearn.model_selection import train_test_split 4 import pandas as pd 5 import numpy as np ~\Anaconda3\envs\myenv\lib\site-packages\tpot\__init__.py in <module>() 25 26 from ._version import __version__ ---> 27 from .tpot import TPOTClassifier, TPOTRegressor 28 from .driver import main ~\Anaconda3\envs\myenv\lib\site-packages\tpot\tpot.py in <module>() 24 """""" 25 ---> 26 from .base import TPOTBase 27 from .config.classifier import classifier_config_dict 28 from .config.regressor import regressor_config_dict ~\Anaconda3\envs\myenv\lib\site-packages\tpot\base.py in <module>() 77 # https://github.com/ContinuumIO/anaconda-issues/issues/905 78 if sys.platform.startswith('win'): ---> 79 import win32api 80 81 try: ImportError: No module named 'win32api'

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 10:56 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Try the command below:
conda uninstall tpot
# or pip uninstall tpot
pip install --upgrade tpot 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",got following error successful installation import import import import import falling back python version hypervolume module expect slow module expect slow recent call last module import import import import import module import import import main module import import import module import try module sent mail fu sent author subject install anaconda try command pip pip install upgrade thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
347041680,"Try the command below:

```
conda uninstall tpot
# or pip uninstall tpot
pip install --upgrade tpot 
```",try command pip pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
347035716,"So what do you suggest?
How to upgrade the tpot?

Sent from Mail for Windows 10

From: Eyad Sibai
Sent: Sunday, November 26, 2017 9:24 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

the tpot version in conda-forge channel is old
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",suggest upgrade sent mail sent author subject install anaconda version channel old thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
347004584,"Hello,

I created a new environment and now tpot works. Great! Thank you very much for your Support.
But I get the following errors…Unfortunately the tpot cannot be updated for whatever reason.

I have Python 3.6.3 in root and Python3.5.4 in myenv where tpot and other related modules are. 
As a result it is not performing at Optimum Levels.
What do you suggest?

Version 0.6.4 of tpot is outdated. Version 0.9.1 was released Friday November 10, 2017.
GP Progress:   1%|▋                                                              | 3/300 [00:01<05:42,  1.15s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  10%|██████▏                                                       | 30/300 [00:18<04:05,  1.10pipeline/s]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  15%|█████████▌                                                    | 46/300 [00:27<02:36,  1.62pipeline/s]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
                                                                                                                       
Generation 1 - Current best internal CV score: 9.097548191320314
GP Progress:  26%|████████████████                                              | 78/300 [01:27<05:51,  1.58s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  28%|█████████████████▎                                            | 84/300 [01:42<08:08,  2.26s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  29%|██████████████████▏                                           | 88/300 [01:56<09:46,  2.77s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
                                                                                                                       
Generation 2 - Current best internal CV score: 9.097548191320314
GP Progress:  39%|███████████████████████▉                                     | 118/300 [02:57<04:43,  1.56s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  46%|████████████████████████████                                 | 138/300 [03:41<04:33,  1.69s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  47%|████████████████████████████▍                                | 140/300 [03:44<03:47,  1.42s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
                                                                                                                       
Generation 3 - Current best internal CV score: 8.883995323472124
                                                                                                                       
Generation 4 - Current best internal CV score: 8.846148613723344
GP Progress:  75%|█████████████████████████████████████████████▉               | 226/300 [06:47<04:51,  3.93s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
GP Progress:  79%|████████████████████████████████████████████████▏            | 237/300 [07:09<02:14,  2.14s/pipeline]C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\utils\deprecation.py:58: DeprecationWarning: Class RandomizedPCA is deprecated; RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver='randomized') instead. The new implementation DOES NOT store whiten ``components_``. Apply transform to get them.
  warnings.warn(msg, category=DeprecationWarning)
                                                                                                                       
Generation 5 - Current best internal CV score: 8.846148613723344
                                                                                                                       

Best pipeline: ExtraTreesRegressor(input_matrix, 0.67000000000000004)
C:\Users\ijeff\Anaconda3\envs\myenv\lib\site-packages\sklearn\metrics\scorer.py:100: DeprecationWarning: Scoring method mean_squared_error was renamed to neg_mean_squared_error in version 0.18 and will be removed in 0.20.
  sample_weight=sample_weight)
14.2713730312

A prompt response would be appreciated

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 6:31 AM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Building a new running environment won’t affect your files’ path and Python3.4 is not necessary. Please check the link I sent over before for more details. 

For deap installation, maybe you need submit another issue in deap’s GitHub repo for a better solution. I am not sure why your root environment is messed up with deap in the old anaconda environment. 

Weixuan 

From my iPhone 

On Nov 26, 2017, at 12:23 AM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote: 

Thank you for the prompt response. 
After I tried the force reinstall deap I got the following error 

Collecting deap==1.0.2.post2 
Exception: 
Traceback (most recent call last): 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main 
status = self.run(options, args) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run 
wb.build(autobuilding=True) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build 
self.requirement_set.prepare_files(self.finder) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files 
ignore_dependencies=self.ignore_dependencies)) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file 
require_hashes 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link 
self.link<http://self.link> = finder.find_requirement(self, upgrade) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement 
all_candidates = self.find_all_candidates(req.name) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates 
for page in self._get_pages(url_locations, project_name): 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages 
page = self._get_page(location) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page 
return HTMLPage.get_page(link, session=self.session) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page 
inst = cls(resp.content, resp.url, resp.headers) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__ 
namespaceHTMLElements=False, 
TypeError: parse() got an unexpected keyword argument 'transport_encoding' 

A question regarding rebuilding a new fresh environment: 
- How do I access the files for example TPOT, deap,…which would be in the new environment? 
- I mean, while using Jupyter notebook do I have to import the files using a special command as these files are not in the root environment. 
- Also what if I am going to use modules which are in the root environment and tpot in the new environment ? 
- Do I install python3.4 in the new environment? And then install deap and tpot ? 
Awaiting your response. 

Best, 
Jeff 

Sent from Mail for Windows 10 

From: Weixuan Fu 
Sent: Sunday, November 26, 2017 12:23 AM 
To: rhiever/tpot 
Cc: ijeffking; Author 
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633) 

Sorry, I cannot access my PC right now. Just two quick possible solutions: 

How about force reinstall deap? 

pip install --force-reinstall deap==1.0.2.post2 

Or rebuilding a new fresh environment in conda should works. 

Check this link for more details: https://conda.io/docs/user-guide/tasks/manage-environments.html 

From my iPhone 

On Nov 25, 2017, at 6:17 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com>> wrote: 

Hello, 

I just observed that I have 2 folders of deap 

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0 

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages 

One of them seems to be the extra that could be the cause of the problem but I do not know which one 

Your help is solicited and would be highly appreciated. 

Best, 
Jeff 

Sent from Mail for Windows 10 

From: Weixuan Fu 
Sent: Saturday, November 25, 2017 11:10 PM 
To: rhiever/tpot 
Cc: ijeffking; Author 
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633) 

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version? 

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it. 

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com><mailto:notifications@github.com>> wrote: 


UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

Context of the issue 

[provide more detailed introduction to the issue itself and why it is relevant] 

[the remaining entries are only necessary if you are reporting a bug] 

Process to reproduce the issue 

[ordered list the process to finding and recreating the issue, example below] 

1. User creates TPOT instance 
2. User calls TPOT fit() function with training data 
3. TPOT crashes with a KeyError after 5 generations 

Expected result 

[describe what you would expect to have resulted from this process] 

Current result 

[describe what you currently experience from this process, and thereby explain the bug] 

Possible fix 

[not necessary, but suggest fixes or reasons for the bug] 

name of issue screenshot 

[if relevant, include a screenshot] 

— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>. 

— 
You are receiving this because you authored the thread. 
Reply to this email directly, view it on GitHub, or mute the thread. 


— 
You are receiving this because you commented. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346972547>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7Ki7Q9haF1du9AoaQYGaj37auyXj3ks5s6J_ugaJpZM4Qqh9x>. 

— 
You are receiving this because you authored the thread. 
Reply to this email directly, view it on GitHub, or mute the thread. 


— 
You are receiving this because you commented. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346985360>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KhC_xXhmMHMS7tEfKQkMW73RLfchks5s6PXQgaJpZM4Qqh9x>. 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello new environment work great thank much support get following whatever reason python root python related result optimum suggest version outdated version progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get generation current best internal score progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get generation current best internal score progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get generation current best internal score generation current best internal score progress class removed use instead new implementation store whiten apply transform get progress class removed use instead new implementation store whiten apply transform get generation current best internal score best pipeline scoring method version removed prompt response would best jeff sent mail fu sent author subject install anaconda building new running environment affect path python necessary please check link sent installation maybe need submit another issue better solution sure root environment old anaconda environment wrote thank prompt response tried force reinstall got following error post exception recent call last file line main status file line run file line build file line file line file line self upgrade file line file line page file line page location file line return link file line file line parse got unexpected argument question regarding new fresh environment access example would new environment mean notebook import special command root environment also going use root environment new environment install python new environment install response best jeff sent mail fu sent author subject install anaconda sorry access right two quick possible force reinstall pip install post new fresh environment work check link wrote hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346986787,"Hello,

I created a new environment myenv and installed tpot, deap and other dependencies.

But while running the tpot Code I got back the following error:
ModuleNotFoundError Traceback (most recent call last) <ipython-input-3-3cdf56f70941> in <module>() ----> 1 from tpot import TPOTClassifier 2 from sklearn.datasets import load_digits 3 from sklearn.model_selection import train_test_split 4 5 digits = load_digits() ModuleNotFoundError: No module named 'tpot'

Do I have to specify anything ? 

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 6:31 AM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Building a new running environment won’t affect your files’ path and Python3.4 is not necessary. Please check the link I sent over before for more details. 

For deap installation, maybe you need submit another issue in deap’s GitHub repo for a better solution. I am not sure why your root environment is messed up with deap in the old anaconda environment. 

Weixuan 

From my iPhone 

On Nov 26, 2017, at 12:23 AM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote: 

Thank you for the prompt response. 
After I tried the force reinstall deap I got the following error 

Collecting deap==1.0.2.post2 
Exception: 
Traceback (most recent call last): 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main 
status = self.run(options, args) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run 
wb.build(autobuilding=True) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build 
self.requirement_set.prepare_files(self.finder) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files 
ignore_dependencies=self.ignore_dependencies)) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file 
require_hashes 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link 
self.link<http://self.link> = finder.find_requirement(self, upgrade) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement 
all_candidates = self.find_all_candidates(req.name) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates 
for page in self._get_pages(url_locations, project_name): 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages 
page = self._get_page(location) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page 
return HTMLPage.get_page(link, session=self.session) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page 
inst = cls(resp.content, resp.url, resp.headers) 
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__ 
namespaceHTMLElements=False, 
TypeError: parse() got an unexpected keyword argument 'transport_encoding' 

A question regarding rebuilding a new fresh environment: 
- How do I access the files for example TPOT, deap,…which would be in the new environment? 
- I mean, while using Jupyter notebook do I have to import the files using a special command as these files are not in the root environment. 
- Also what if I am going to use modules which are in the root environment and tpot in the new environment ? 
- Do I install python3.4 in the new environment? And then install deap and tpot ? 
Awaiting your response. 

Best, 
Jeff 

Sent from Mail for Windows 10 

From: Weixuan Fu 
Sent: Sunday, November 26, 2017 12:23 AM 
To: rhiever/tpot 
Cc: ijeffking; Author 
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633) 

Sorry, I cannot access my PC right now. Just two quick possible solutions: 

How about force reinstall deap? 

pip install --force-reinstall deap==1.0.2.post2 

Or rebuilding a new fresh environment in conda should works. 

Check this link for more details: https://conda.io/docs/user-guide/tasks/manage-environments.html 

From my iPhone 

On Nov 25, 2017, at 6:17 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com>> wrote: 

Hello, 

I just observed that I have 2 folders of deap 

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0 

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages 

One of them seems to be the extra that could be the cause of the problem but I do not know which one 

Your help is solicited and would be highly appreciated. 

Best, 
Jeff 

Sent from Mail for Windows 10 

From: Weixuan Fu 
Sent: Saturday, November 25, 2017 11:10 PM 
To: rhiever/tpot 
Cc: ijeffking; Author 
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633) 

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version? 

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it. 

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com><mailto:notifications@github.com>> wrote: 


UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

Context of the issue 

[provide more detailed introduction to the issue itself and why it is relevant] 

[the remaining entries are only necessary if you are reporting a bug] 

Process to reproduce the issue 

[ordered list the process to finding and recreating the issue, example below] 

1. User creates TPOT instance 
2. User calls TPOT fit() function with training data 
3. TPOT crashes with a KeyError after 5 generations 

Expected result 

[describe what you would expect to have resulted from this process] 

Current result 

[describe what you currently experience from this process, and thereby explain the bug] 

Possible fix 

[not necessary, but suggest fixes or reasons for the bug] 

name of issue screenshot 

[if relevant, include a screenshot] 

— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>. 

— 
You are receiving this because you authored the thread. 
Reply to this email directly, view it on GitHub, or mute the thread. 


— 
You are receiving this because you commented. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346972547>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7Ki7Q9haF1du9AoaQYGaj37auyXj3ks5s6J_ugaJpZM4Qqh9x>. 

— 
You are receiving this because you authored the thread. 
Reply to this email directly, view it on GitHub, or mute the thread. 


— 
You are receiving this because you commented. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346985360>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KhC_xXhmMHMS7tEfKQkMW73RLfchks5s6PXQgaJpZM4Qqh9x>. 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello new environment running code got back following error recent call last module import import import module specify anything best jeff sent mail fu sent author subject install anaconda building new running environment affect path python necessary please check link sent installation maybe need submit another issue better solution sure root environment old anaconda environment wrote thank prompt response tried force reinstall got following error post exception recent call last file line main status file line run file line build file line file line file line self upgrade file line file line page file line page location file line return link file line file line parse got unexpected argument question regarding new fresh environment access example would new environment mean notebook import special command root environment also going use root environment new environment install python new environment install response best jeff sent mail fu sent author subject install anaconda sorry access right two quick possible force reinstall pip install post new fresh environment work check link wrote hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346985607,"Building a new running environment won’t affect your files’ path and Python3.4 is not necessary. Please check the link I sent over before for more details.

For deap installation, maybe you need submit another issue in deap’s GitHub repo for a better solution. I am not sure why your root environment is messed up with deap in the old anaconda environment.

Weixuan

From my iPhone

On Nov 26, 2017, at 12:23 AM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote:

Thank you for the prompt response.
After I tried the force reinstall deap I got the following error

Collecting deap==1.0.2.post2
Exception:
Traceback (most recent call last):
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
status = self.run(options, args)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
wb.build(autobuilding=True)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
self.requirement_set.prepare_files(self.finder)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
ignore_dependencies=self.ignore_dependencies))
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
require_hashes
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
self.link<http://self.link> = finder.find_requirement(self, upgrade)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
all_candidates = self.find_all_candidates(req.name)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
for page in self._get_pages(url_locations, project_name):
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
page = self._get_page(location)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
return HTMLPage.get_page(link, session=self.session)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
inst = cls(resp.content, resp.url, resp.headers)
File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'

A question regarding rebuilding a new fresh environment:
- How do I access the files for example TPOT, deap,…which would be in the new environment?
- I mean, while using Jupyter notebook do I have to import the files using a special command as these files are not in the root environment.
- Also what if I am going to use modules which are in the root environment and tpot in the new environment ?
- Do I install python3.4 in the new environment? And then install deap and tpot ?
Awaiting your response.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 12:23 AM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Sorry, I cannot access my PC right now. Just two quick possible solutions:

How about force reinstall deap?

pip install --force-reinstall deap==1.0.2.post2

Or rebuilding a new fresh environment in conda should works.

Check this link for more details: https://conda.io/docs/user-guide/tasks/manage-environments.html

From my iPhone

On Nov 25, 2017, at 6:17 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com>> wrote:

Hello,

I just observed that I have 2 folders of deap

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages

One of them seems to be the extra that could be the cause of the problem but I do not know which one

Your help is solicited and would be highly appreciated.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Saturday, November 25, 2017 11:10 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version?

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it.

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com><mailto:notifications@github.com>> wrote:


UnsatisfiableError: The following specifications were found to be in conflict:

* python 3.6*
* tpot -> deap >=1.0 -> python 3.4*
Use ""conda info "" to see the dependencies for each package.

Context of the issue

[provide more detailed introduction to the issue itself and why it is relevant]

[the remaining entries are only necessary if you are reporting a bug]

Process to reproduce the issue

[ordered list the process to finding and recreating the issue, example below]

1. User creates TPOT instance
2. User calls TPOT fit() function with training data
3. TPOT crashes with a KeyError after 5 generations

Expected result

[describe what you would expect to have resulted from this process]

Current result

[describe what you currently experience from this process, and thereby explain the bug]

Possible fix

[not necessary, but suggest fixes or reasons for the bug]

name of issue screenshot

[if relevant, include a screenshot]

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.


—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346972547>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7Ki7Q9haF1du9AoaQYGaj37auyXj3ks5s6J_ugaJpZM4Qqh9x>.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.


—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346985360>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KhC_xXhmMHMS7tEfKQkMW73RLfchks5s6PXQgaJpZM4Qqh9x>.
",building new running environment affect path python necessary please check link sent installation maybe need submit another issue better solution sure root environment old anaconda environment wrote thank prompt response tried force reinstall got following error post exception recent call last file line main status file line run file line build file line file line file line self upgrade file line file line page file line page location file line return link file line file line parse got unexpected argument question regarding new fresh environment access example would new environment mean notebook import special command root environment also going use root environment new environment install python new environment install response best jeff sent mail fu sent author subject install anaconda sorry access right two quick possible force reinstall pip install post new fresh environment work check link wrote hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346985360,"Thank you for the prompt response.
After I tried the force reinstall deap I got the following error

Collecting deap==1.0.2.post2
Exception:
Traceback (most recent call last):
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\Users\ijeff\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'

A question regarding rebuilding a new fresh environment:
- How do I access the files for example TPOT, deap,…which would be in the new environment?
- I mean, while using Jupyter notebook do I have to import the files using a special command as these files are not in the root environment.
- Also what if I am going to use modules which are in the root environment and tpot in the new environment ? 
- Do I install python3.4 in the new environment? And then install deap and tpot ?
Awaiting your response.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Sunday, November 26, 2017 12:23 AM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Sorry, I cannot access my PC right now. Just two quick possible solutions: 

How about force reinstall deap? 

pip install --force-reinstall deap==1.0.2.post2 

Or rebuilding a new fresh environment in conda should works. 

Check this link for more details: https://conda.io/docs/user-guide/tasks/manage-environments.html 

From my iPhone 

On Nov 25, 2017, at 6:17 PM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote: 

Hello, 

I just observed that I have 2 folders of deap 

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0 

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages 

One of them seems to be the extra that could be the cause of the problem but I do not know which one 

Your help is solicited and would be highly appreciated. 

Best, 
Jeff 

Sent from Mail for Windows 10 

From: Weixuan Fu 
Sent: Saturday, November 25, 2017 11:10 PM 
To: rhiever/tpot 
Cc: ijeffking; Author 
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633) 

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version? 

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it. 

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com>> wrote: 


UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

Context of the issue 

[provide more detailed introduction to the issue itself and why it is relevant] 

[the remaining entries are only necessary if you are reporting a bug] 

Process to reproduce the issue 

[ordered list the process to finding and recreating the issue, example below] 

1. User creates TPOT instance 
2. User calls TPOT fit() function with training data 
3. TPOT crashes with a KeyError after 5 generations 

Expected result 

[describe what you would expect to have resulted from this process] 

Current result 

[describe what you currently experience from this process, and thereby explain the bug] 

Possible fix 

[not necessary, but suggest fixes or reasons for the bug] 

name of issue screenshot 

[if relevant, include a screenshot] 

— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>. 

— 
You are receiving this because you authored the thread. 
Reply to this email directly, view it on GitHub, or mute the thread. 


— 
You are receiving this because you commented. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346972547>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7Ki7Q9haF1du9AoaQYGaj37auyXj3ks5s6J_ugaJpZM4Qqh9x>. 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",thank prompt response tried force reinstall got following error post exception recent call last file line main status file line run file line build file line file line file line self upgrade file line file line page file line page location file line return link file line file line parse got unexpected argument question regarding new fresh environment access example would new environment mean notebook import special command root environment also going use root environment new environment install python new environment install response best jeff sent mail fu sent author subject install anaconda sorry access right two quick possible force reinstall pip install post new fresh environment work check link wrote hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346972831,"Sorry, I cannot access my PC right now. Just two quick possible solutions:

How about force reinstall deap?

pip install --force-reinstall deap==1.0.2.post2

Or rebuilding a new fresh environment in conda should works.

Check this link for more details: https://conda.io/docs/user-guide/tasks/manage-environments.html

From my iPhone

On Nov 25, 2017, at 6:17 PM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote:

Hello,

I just observed that I have 2 folders of deap

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages

One of them seems to be the extra that could be the cause of the problem but I do not know which one

Your help is solicited and would be highly appreciated.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Saturday, November 25, 2017 11:10 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version?

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it.

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com><mailto:notifications@github.com>> wrote:


UnsatisfiableError: The following specifications were found to be in conflict:

* python 3.6*
* tpot -> deap >=1.0 -> python 3.4*
Use ""conda info "" to see the dependencies for each package.

Context of the issue

[provide more detailed introduction to the issue itself and why it is relevant]

[the remaining entries are only necessary if you are reporting a bug]

Process to reproduce the issue

[ordered list the process to finding and recreating the issue, example below]

1. User creates TPOT instance
2. User calls TPOT fit() function with training data
3. TPOT crashes with a KeyError after 5 generations

Expected result

[describe what you would expect to have resulted from this process]

Current result

[describe what you currently experience from this process, and thereby explain the bug]

Possible fix

[not necessary, but suggest fixes or reasons for the bug]

name of issue screenshot

[if relevant, include a screenshot]

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>.

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.


—
You are receiving this because you commented.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633#issuecomment-346972547>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7Ki7Q9haF1du9AoaQYGaj37auyXj3ks5s6J_ugaJpZM4Qqh9x>.
",sorry access right two quick possible force reinstall pip install post new fresh environment work check link wrote hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346972547,"Hello,

I just observed that I have 2 folders of deap

C:\Users\ijeff\Anaconda3\pkgs\ deap-1.0.2.post2-py34_0

C:\Users\ijeff\Anaconda3\pkgs\deap-1.0.2.post2-py34_0\Lib\site-packages

One of them seems to be the extra that could be the cause of the problem but I do not know which one

Your help is solicited and would be highly appreciated.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Saturday, November 25, 2017 11:10 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version? 

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it. 

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote: 


UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

Context of the issue 

[provide more detailed introduction to the issue itself and why it is relevant] 

[the remaining entries are only necessary if you are reporting a bug] 

Process to reproduce the issue 

[ordered list the process to finding and recreating the issue, example below] 

1. User creates TPOT instance 
2. User calls TPOT fit() function with training data 
3. TPOT crashes with a KeyError after 5 generations 

Expected result 

[describe what you would expect to have resulted from this process] 

Current result 

[describe what you currently experience from this process, and thereby explain the bug] 

Possible fix 

[not necessary, but suggest fixes or reasons for the bug] 

name of issue screenshot 

[if relevant, include a screenshot] 

— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>. 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello one extra could cause problem know one help solicited would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346972038,"Hello,

Appreciate the prompt Response.

I am using Windows 10.

I had Anaconda installed on my PC and had been using TPOT without any Problems.I had followed the Installation iinstructions.
Yesterday I had been Learning CNN with Keras and then the Problem started. I downloaded tensorflow and since then various issues cropped up.

So I decided to uninstall the old Anaconda and reinstall the new Version of Anaconda Version 1.6.9
Then I installed all the libraries required for Data Science like scikit-learn, keras, Theano,tensorflow,…
BUT  I just cannot get to install tpot 
In the Anaconda the files are available but everytime I try to install them I get the Unsatisfiableerror. Tpot 0.6.4 and deap 1.0.2 are available but cannot be installed.
UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

What do you suggest ?
You have said uninstall deap. But when I do uninstall it says PackageNOTFound error. Package missing from the environment. 

I really liked using TPOT. A prompt response would be highly appreciated.

Best,
Jeff

Sent from Mail for Windows 10

From: Weixuan Fu
Sent: Saturday, November 25, 2017 11:10 PM
To: rhiever/tpot
Cc: ijeffking; Author
Subject: Re: [rhiever/tpot] Cannot install TPOT after installing Anaconda5.0.1 (#633)

Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version? 

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it. 

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote: 


UnsatisfiableError: The following specifications were found to be in conflict: 

* python 3.6* 
* tpot -> deap >=1.0 -> python 3.4* 
Use ""conda info "" to see the dependencies for each package. 

Context of the issue 

[provide more detailed introduction to the issue itself and why it is relevant] 

[the remaining entries are only necessary if you are reporting a bug] 

Process to reproduce the issue 

[ordered list the process to finding and recreating the issue, example below] 

1. User creates TPOT instance 
2. User calls TPOT fit() function with training data 
3. TPOT crashes with a KeyError after 5 generations 

Expected result 

[describe what you would expect to have resulted from this process] 

Current result 

[describe what you currently experience from this process, and thereby explain the bug] 

Possible fix 

[not necessary, but suggest fixes or reasons for the bug] 

name of issue screenshot 

[if relevant, include a screenshot] 

— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>. 

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello appreciate prompt response anaconda without installation yesterday learning problem since various decided old anaconda reinstall new version anaconda version data science like get install anaconda available try install get available following found conflict python python use see package suggest said error package missing environment really prompt response would highly best jeff sent mail fu sent author subject install anaconda could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
346969473,"Could you please provide more details about your running environment, like versions of TPOT and all the dependencies, OS and python version?

My guess about the issue is related to deap. Maybe you need uninstall deap and then reinstall it.

On Nov 25, 2017, at 2:31 PM, ijeffking <notifications@github.com<mailto:notifications@github.com>> wrote:


UnsatisfiableError: The following specifications were found to be in conflict:

  *   python 3.6*
  *   tpot -> deap >=1.0 -> python 3.4*
Use ""conda info "" to see the dependencies for each package.

Context of the issue

[provide more detailed introduction to the issue itself and why it is relevant]

[the remaining entries are only necessary if you are reporting a bug]

Process to reproduce the issue

[ordered list the process to finding and recreating the issue, example below]

  1.  User creates TPOT instance
  2.  User calls TPOT fit() function with training data
  3.  TPOT crashes with a KeyError after 5 generations

Expected result

[describe what you would expect to have resulted from this process]

Current result

[describe what you currently experience from this process, and thereby explain the bug]

Possible fix

[not necessary, but suggest fixes or reasons for the bug]

name of issue screenshot

[if relevant, include a screenshot]

—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/rhiever/tpot/issues/633>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AUG7KlitS0S-kJOJcFaH0_kMM-L5WMnnks5s6GsngaJpZM4Qqh9x>.
",could please provide running environment like o python version guess issue related maybe need reinstall wrote following found conflict python python use see package context issue provide detailed introduction issue relevant necessary bug process reproduce issue ordered list process finding issue example user instance user fit function training data result describe would expect process current result describe currently experience process thereby explain bug possible fix necessary suggest bug name issue relevant include thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346959649,"UnsatisfiableError: The following specifications were found to be in conflict:
  - python 3.6*
  - tpot -> deap >=1.0 -> python 3.4*
Use ""conda info <package>"" to see the dependencies for each package.

I get the above error while trying to install tpot using Anaconda",following found conflict python python use package see package get error trying install anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
346866777,"@PGijsbers thank you for proposing these good ideas and thoughtful implementation. I just had a quick look on your paper, and it is very interesting. I think we would need some tests and discussions next week about your proposal before deciding whether it should be merged to TPOT. Nice works!",thank good thoughtful implementation quick look paper interesting think would need next week proposal whether nice work,issue,positive,positive,positive,positive,positive,positive
346849973,"Maybe we need reopen #425 to refine the stdout @rhiever. We had seen a few related questions already, like #612.",maybe need reopen refine seen related already like,issue,negative,neutral,neutral,neutral,neutral,neutral
346848687,"@weixuanfu  So is 122.641597476  the absolute value of the r2 score?  If so, does it make sense to output this as it doesn't seem informative. I mean -1 and 1 are very different r2 scores afaict. ",absolute value score make sense output seem informative mean different,issue,negative,negative,neutral,neutral,negative,negative
346848037,I think this issue is that the r2 score in scikit-learn can be negative and even less than -1 for very bad predictions. TPOT internally should maximize the r2 score but the stdout is the absolute score (I think it is mentioned in one issue before).,think issue score negative even le bad internally maximize score absolute score think one issue,issue,negative,negative,negative,negative,negative,negative
346846768,"@rspadim  I don't fully understand sorry. The full code is at https://bpaste.net/show/cf0b0f75657f .  I think you always want to maximize the r2 score but the max possible should be 1.0.  I set the scoring function in the line

tpot = TPOTRegressor(scoring=""r2"", generations=2, population_size=50, verbosity=2, n_jobs=-1)

The mathematical formula for r2 is at http://scikit-learn.org/stable/modules/model_evaluation.html#r2-score .",fully understand sorry full code think always want maximize score possible set scoring function line mathematical formula,issue,negative,negative,neutral,neutral,negative,negative
346845494,"check if you are not using negative log loss, probably yes and you ar doing a maximization instead of minimization",check negative log loss probably yes ar maximization instead minimization,issue,negative,negative,negative,negative,negative,negative
346346803,Thank you for the feedback. Please test the PR if you want to use the latest version of Deap. I think it will work.,thank feedback please test want use latest version think work,issue,positive,positive,positive,positive,positive,positive
346345825,"Downgrading to 1.0.2.post2 fixed also fixed this issue :)
Do you still want me to try your PR in my environment, or should that be OK anyway?",post fixed also fixed issue still want try environment anyway,issue,negative,positive,neutral,neutral,positive,positive
346333753,@PG-TUe please check the related issue #623. Downgrade your deal version to 1.0.2 should avoid these errors. I also made a patch in #624 .,please check related issue downgrade deal version avoid also made patch,issue,negative,neutral,neutral,neutral,neutral,neutral
346332944,@PG-TUe I think the commits in this PR would fix this issue. Could you please merge/add/install them into your environment and have a try.,think would fix issue could please environment try,issue,negative,neutral,neutral,neutral,neutral,neutral
346294718,"Hi,

I just started working at a new workspace, after configuring and downloading the required software to develop TPOT, I ran into issues with the unit tests.

Specifically, it failed two mutNodeReplacement tests because Primitives are not hashable. Is this one of the issues that you found, or should those unit tests be working already (on the latest development branch)?

Currently running Python 3.6.3 on Windows 10.

Full test output for those two failed tests:

======================================================================                                                  

ERROR: Assert that mutNodeReplacement() returns the correct type of mutation node in a fixed pipeline.  
   
Traceback (most recent call last):                                                                                        
File ""...\Continuum\anaconda3\lib\site-packages\nose\case.py"", line 197, in runTest            self.test(*self.arg)                                                                                                  
File ""...\tpot\tests\tpot_tests.py"", line 1760, in test_mutNodeReplacement                             
      diff_prims = list(set(new_prims_list).symmetric_difference(old_prims_list))     
                                   TypeError: unhashable type: 'Primitive'     
                                                                                                                                                                                                    ======================================================================                                              
    ERROR: Assert that mutNodeReplacement() returns the correct type of mutation node in a complex pipeline.           
                                          Traceback (most recent call last):              
                                                                          File ""...\Continuum\anaconda3\lib\site-packages\nose\case.py"", line 197, in runTest            self.test(*self.arg)  
                                                                                                File ""...\tpot\tests\tpot_tests.py"", line 1798, in test_mutNodeReplacement_2                  
                diff_prims = list(set(new_prims_list).symmetric_difference(old_prims_list))          
                               TypeError: unhashable type: 'Primitive' ",hi working new develop ran unit specifically two one found unit working already latest development branch currently running python full test output two error assert correct type mutation node fixed pipeline recent call last file line file line list set type error assert correct type mutation node complex pipeline recent call last file line file line list set type,issue,negative,positive,neutral,neutral,positive,positive
346182368,"Thanks for the tips. I am quite familiar with the frustrations in multiprocessing in python. I've been running the jobs with the `n_jobs=-1` option on 2-8 cores, depending on the training input. I think I understand your `2.` point; it can process in parallel if the cores are on the same node.   ",thanks quite familiar python running option depending training input think understand point process parallel node,issue,negative,positive,positive,positive,positive,positive
346172730,"Can we provide pre-defined  a list
of estimator to be used during fitting ?
Thanks






On Nov 22, 2017, at 2:34, bartdp1 <notifications@github.com> wrote:

Just to be sure we are discussing the same things, I think that it is good to first distinguish two types of overfitting. Apologies if the discussion is a bit extensive.

The first type is overfitting on a model level. A model which is too flexible/complex will not only 'attempt' to explain the signal in the training data, but also the randomness. Think of a decision tree model which grows a terminal leaf for each observation; perfect fit on the training set, but very poor generalization performance.
This type of ‘overfitting’ is leveraged by fitting models of different levels of complexity (often controlled by regularization parameters), and for all of these models, calculate an estimate for the generalization performance, often by cross validation. The model with the lowest cross validation estimate is then picked.

The problem here is that the cross validation error is only an estimate of the actual generalization error of the model. By picking the model that minimizes the cross validation error, we cannot trust the cross validation estimate as a good estimate of the actual generalization performance of the model, since it could just as well be the ‘winner’ model since it’s cross validation estimate is just more optimistic than that of its competitors. This fact has been mentioned numerous times in prominent literature on the topic. This is why the final model quality should always be assessed on an external validation set (or using nested-cv)

Algorithms like TPOT, that do an intelligent search over the model space, in some sense perform a directed search towards the best models, based on CV. However, given the above discussion, the estimate for these models could just as well be biased. Moreover, models with a lower cv-error could just as well have a better generalization performance. By taking this ‘direction’ in the search we could just as well be looking for models that have an even more optimistic cv-error, without improving the actual generalization performance. In some sense, we are searching for models that perform well on the out-of-sample cv folds, and thus are overfitting the CV-error.

TPOT might suffer from the last type of overfitting. I agree that the pareto optimization helps to alleviate this problem. However, I do not see why stimulating the choice for feature selectors/constructors does this. It could just as well be picked since some of the selected/constructed features help a lot in predicting the out-of-sample folds used for the cross validation estimate. I did some runs of TPOT on simulated data, after which I refitted all the evaluated pipelines on the entire training set and evaluated on a large test set. For most of these simulations it was indeed the case that most improvements of the best model made by TPOT where only an improvement in CV-score and not in score on the test set. Setup was 1000 observations and 35 variables generated from some difficult process, to keep the simulations workable.

Personally, I was thinking about penalizing CV-error of the top-performing models in each generation, since we expect the estimate for these models to be overly optimistic. This would give good performing models with a less optimistic CV-estimate a better chance to evolve. As I am writing this, I am even considering the idea to stop the top models to traverse to the next generation at all.
It would be great to have some discussion on this. FYI, I am currently writing my Master’s thesis on a particular prediction problem. Since I like TPOT a lot, it would be very nice to dedicate a part of the thesis to (possible) improvements :).

―
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.

",provide list estimator used fitting thanks wrote sure think good first distinguish two discussion bit extensive first type model level model explain signal training data also randomness think decision tree model terminal leaf observation perfect fit training set poor generalization performance type fitting different complexity often regularization calculate estimate generalization performance often cross validation model cross validation estimate picked problem cross validation error estimate actual generalization error model model cross validation error trust cross validation estimate good estimate actual generalization performance model since could well winner model since cross validation estimate optimistic fact numerous time prominent literature topic final model quality always assessed external validation set like intelligent search model space sense perform directed search towards best based however given discussion estimate could well moreover lower could well better generalization performance taking direction search could well looking even optimistic without improving actual generalization performance sense searching perform well thus might suffer last type agree optimization alleviate problem however see choice feature could well picked since help lot used cross validation estimate data entire training set large test set indeed case best model made improvement score test set setup difficult process keep workable personally thinking generation since expect estimate overly optimistic would give good le optimistic better chance evolve writing even considering idea stop top traverse next generation would great discussion currently writing master thesis particular prediction problem since like lot would nice dedicate part thesis possible reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
346110851,"Do you have limit setting of CPU in your server? If not, maybe using `n_jobs=4` or `n_jobs=8` will help since the limit is walltime not CPU time. Two thing need to be notices for this setting:

1. There is a potential freezing issue in Linux-based environment, and there is a work around mentioned in our doc (check this [link](http://rhiever.github.io/tpot/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux))

2. Please Make sure your server scheduler assign only one compute code to this tpot job since tpot cannot use CPUs in multiple compute node so far.",limit setting server maybe help since limit time two thing need setting potential freezing issue environment work around doc check link please make sure server assign one compute code job since use multiple compute node far,issue,positive,positive,positive,positive,positive,positive
346107715,Please keep us updated on this issue if you find any alternative solutions.,please keep u issue find alternative,issue,negative,neutral,neutral,neutral,neutral,neutral
346105643,"Thanks for the quick reply. I'll try out pickling if I run into the walltime issue. Unfortunately serialization is not supported for the server I use. You may close this issue if you like. Thanks again!
",thanks quick reply try run issue unfortunately serialization server use may close issue like thanks,issue,positive,positive,neutral,neutral,positive,positive
346102461,"Just to be sure we are discussing the same things, I think that it is good to first distinguish two types of overfitting. Apologies if the discussion is a bit extensive.

The first type is overfitting on a model level. A model which is too flexible/complex will not only 'attempt' to explain the signal in the training data, but also the randomness. Think of a decision tree model which grows a terminal leaf for each observation; perfect fit on the training set, but very poor generalization performance.
This type of ‘overfitting’ is leveraged by fitting models of different levels of complexity (often controlled by regularization parameters), and for all of these models, calculate an estimate for the generalization performance, often by cross validation. The model with the lowest cross validation estimate is then picked.

The problem here is that the cross validation error is only an estimate of the actual generalization error of the model. By picking the model that minimizes the cross validation error, we cannot trust the cross validation estimate as a good estimate of the actual generalization performance of the model, since it could just as well be the ‘winner’ model since it’s cross validation estimate is just more optimistic than that of its competitors. This fact has been mentioned numerous times in prominent literature on the topic. This is why the final model quality should always be assessed on an external validation set (or using nested-cv) 

Algorithms like TPOT, that do an intelligent search over the model space, in some sense perform a directed search towards the best models, based on CV. However, given the above discussion, the estimate for these models could just as well be biased. Moreover,  models with a lower cv-error could just as well have a better generalization performance. By taking this ‘direction’ in the search we could just as well be looking for models that have an even more optimistic cv-error, without improving the actual generalization performance. In some sense, we are searching for models that perform well on the out-of-sample cv folds, and thus are overfitting the CV-error.

TPOT might suffer from the last type of overfitting. I agree that the pareto optimization helps to alleviate this problem. However, I do not see why stimulating the choice for feature selectors/constructors does this. It could just as well be picked since some of the selected/constructed features help a lot in predicting the out-of-sample folds used for the cross validation estimate. I did some runs of TPOT on simulated data, after which I refitted all the evaluated pipelines on the entire training set and evaluated on a large test set. For most of these simulations it was indeed the case that most improvements of the best model made by TPOT where only an improvement in CV-score and not in score on the test set. Setup was 1000 observations and 35 variables generated from some difficult process, to keep the simulations workable.

Personally, I was thinking about penalizing CV-error of the top-performing models in each generation, since we expect the estimate for these models to be overly optimistic. This would give good performing models with a less optimistic CV-estimate a better chance to evolve. As I am writing this, I am even considering the idea to stop the top models to traverse to the next generation at all.
It would be great to have some discussion on this. FYI, I am currently writing my Master’s thesis on a particular prediction problem. Since I like TPOT a lot, it would be very nice to dedicate a part of the thesis to (possible) improvements :). 



",sure think good first distinguish two discussion bit extensive first type model level model explain signal training data also randomness think decision tree model terminal leaf observation perfect fit training set poor generalization performance type fitting different complexity often regularization calculate estimate generalization performance often cross validation model cross validation estimate picked problem cross validation error estimate actual generalization error model model cross validation error trust cross validation estimate good estimate actual generalization performance model since could well winner model since cross validation estimate optimistic fact numerous time prominent literature topic final model quality always assessed external validation set like intelligent search model space sense perform directed search towards best based however given discussion estimate could well moreover lower could well better generalization performance taking direction search could well looking even optimistic without improving actual generalization performance sense searching perform well thus might suffer last type agree optimization alleviate problem however see choice feature could well picked since help lot used cross validation estimate data entire training set large test set indeed case best model made improvement score test set setup difficult process keep workable personally thinking generation since expect estimate overly optimistic would give good le optimistic better chance evolve writing even considering idea stop top traverse next generation would great discussion currently writing master thesis particular prediction problem since like lot would nice dedicate part thesis possible,issue,positive,positive,positive,positive,positive,positive
346045518,">How does the pareto optimization actually work? If i understand correctly, in the end TPOT always provides the pipeline with the best internal CV-score which has been evaluated during the optimization right? The user could choose to pick a pipeline with less components from the pareto boundary, but he should do so manually, if i am correct.

That's correct. This Pareto front concept is also used to eliminate ""poor-performing"" pipelines at the end of every GP generation as well: All pipelines in the current generation are ranked by their ""dominance,"" i.e., top-ranking pipelines must outperform all other pipelines on at least one of the multi-objective criteria (in our case, predictive performance or complexity). Second-rank pipelines must outperform all other pipelines except the top-ranking ones on at least one of the multi-objective criteria. And so on. You can look up the NSGA2 algorithm if you'd like to dig into the specific algorithm more.

We recently added an early stopping feature for TPOT, and I think that fits into GA regularization as described above. As others have pointed out, many algorithms in TPOT already have per-model regularization. I think the key is to find out what kind of per-pipeline regularization is needed to prevent TPOT from selecting pipelines with an overly-optimistic estimate of generalization performance. I still think that encouraging TPOT to perform feature construction and selection is the most promising way to accomplish that goal, as one of the biggest causes of overfitting is providing too many features to ML algorithms.",optimization actually work understand correctly end always pipeline best internal optimization right user could choose pick pipeline le boundary manually correct correct front concept also used eliminate end every generation well current generation ranked dominance must outperform least one criterion case predictive performance complexity must outperform except least one criterion look algorithm like dig specific algorithm recently added early stopping feature think ga regularization pointed many already regularization think key find kind regularization prevent estimate generalization performance still think encouraging perform feature construction selection promising way accomplish goal one biggest providing many,issue,positive,positive,positive,positive,positive,positive
345987958,"Little bump here, since i wanna do some research into this.
Namely, i have the idea that TPOT tends to select pipelines for which the CV-error is rather a rather optimistic estimate of the true test-error, and that this effect becomes greater as more generations of optimization are performed.

How does the pareto optimization actually work? If i understand correctly, in the end TPOT always provides the pipeline with the best internal CV-score which has been evaluated during the optimization right? The user could choose to pick a pipeline with less components from the pareto boundary, but he should do so manually, if i am correct.
",little bump since wan na research namely idea select rather rather optimistic estimate true effect becomes greater optimization optimization actually work understand correctly end always pipeline best internal optimization right user could choose pick pipeline le boundary manually correct,issue,positive,positive,positive,positive,positive,positive
345803014,"You would have to [pickle](https://docs.python.org/3/library/pickle.html) the TPOT object (in your code, the `pipeline_optimizer` variable) in this particular case. However, I believe there are some outstanding issues with pickling the TPOT object (per #608) that still need to be addressed. `warm_start` was designed more for interactive use, though once the pickle issue is resolved, it could also be used to support a use case like yours.

Does the server you're using support serialization of the entire job state, such that the entire job can be re-queued once the 2 day walltime limit is reached?",would pickle object code variable particular case however believe outstanding object per still need designed interactive use though pickle issue resolved could also used support use case like server support serialization entire job state entire job day limit,issue,positive,positive,positive,positive,positive,positive
345545019,"For huge datasets, like half million of samples with thousands of features, I suggest to try ”TPOT light” configuration firstly with setting `config_dict=""TPOT light""`.",huge like half million suggest try light configuration firstly setting light,issue,positive,positive,positive,positive,positive,positive
345544417,"yep, the dataset is huge... So, there is no way to use it for the big data?",yep huge way use big data,issue,positive,positive,positive,positive,positive,positive
345544295,It seems that the trainning dataset are very huge or the first pipeline was too complex to cause the timeout warning. Increasing max_eval_time_mins may let the pipeline finish evaluation with this warning message or set verbosity = 2 to mute this warning message. ,huge first pipeline complex cause warning increasing may let pipeline finish evaluation warning message set verbosity mute warning message,issue,negative,positive,positive,positive,positive,positive
345305397,"Thank you for solving this so promptly.

I will use the currently of TPOT for now but will also test the new
installation later. I'll let you know how these go.

-Rob

---------------------------------------------
Robert B. Wexler
PhD Candidate in Chemistry
University of Pennsylvania, 2018
rwexler@sas.upenn.edu
(215) 801-8741
---------------------------------------------

On Fri, Nov 17, 2017 at 11:56 AM, Weixuan Fu <notifications@github.com>
wrote:

> Hi @rwexler <https://github.com/rwexler>
>
> I found that it should be caused by a bug when introducing the new scoring
> API in TPOT 0.9.1 (related issue #579
> <https://github.com/rhiever/tpot/issues/579>, PR #594
> <https://github.com/rhiever/tpot/pull/594> #607
> <https://github.com/rhiever/tpot/pull/607>).
>
> I just made a PR #626 <https://github.com/rhiever/tpot/pull/626> for
> fixing this issue. You may try the fixed branch by install TPOT:
>
> pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@scoring_api_bug
>
> And then you need change GiniScore(y_actual, y_pred): to GiniScore(y_true,
> y_pred): in your codes.
>
> Or, you can still use the current TPOT 0.9.1 and add these codes below to
> your scripts:
>
> from sklearn.metrics.scorer import make_scorer
> GiniScore_scorer = make_scorer(GiniScore, greater_is_better=True)
>
> tpot = TPOTClassifier(generations = 10, population_size = 20, verbosity = 2, scoring = GiniScore_scorer, random_state = 42, max_eval_time_mins=1)
>
> Sorry about the inconvenience.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/625#issuecomment-345300492>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AFf9tkk69HxTIBPbdpCvobTzJBSSA5dBks5s3brWgaJpZM4QiCRk>
> .
>
",thank promptly use currently also test new installation later let know go candidate chemistry university fu wrote hi found bug new scoring related issue made fixing issue may try fixed branch install pip install upgrade need change still use current add import verbosity scoring sorry inconvenience reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
345300492,"Hi @rwexler 

I found that it should be caused by a bug when introducing the new scoring API in TPOT 0.9.1 (related issue #579, PR #594 #607).

I just made a PR #626 for fixing this issue. You may try the fixed branch by install TPOT:
```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@scoring_api_bug
```
And then you need change `GiniScore(y_actual, y_pred):` to `GiniScore(y_true, y_pred):` in your codes.

Or, you can still use the current TPOT 0.9.1 and add these codes below to your scripts:
```
from sklearn.metrics.scorer import make_scorer
GiniScore_scorer = make_scorer(GiniScore, greater_is_better=True)

tpot = TPOTClassifier(generations = 10, population_size = 20, verbosity = 2, scoring = GiniScore_scorer, random_state = 42)
```
Sorry about the inconvenience. 

",hi found bug new scoring related issue made fixing issue may try fixed branch install pip install upgrade need change still use current add import verbosity scoring sorry inconvenience,issue,negative,negative,neutral,neutral,negative,negative
345261586,"Hi Weixuan,

Thanks for your reply.

The size of the training and testing data is 0.115852544 GB and 0.172006681
GB, respectively. Do you think that data sets of these sizes would be too
large for TPOT?

I'm not sure if free memory is the issue given the size of the data set and
the fact that I am using two nodes of a supercomputer each with 64 GB of
memory.

In the calculation that completed, I did not set random_state. In the
calculation that failed, however, I used `random_state = 42`. I have
attached a copy of the script that failed.

Thanks again for your help!

---------------------------------------------
Robert B. Wexler
PhD Candidate in Chemistry
University of Pennsylvania, 2018
rwexler@sas.upenn.edu
(215) 801-8741
---------------------------------------------

On Fri, Nov 17, 2017 at 9:31 AM, Weixuan Fu <notifications@github.com>
wrote:

> Hmm, it seems that no valid pipeline (or pipeline is too time-consuming to
> get a valid fitness score) is generated in the initial generation, which is
> very weird. I have a quick look on the dataset in Kaggle, maybe each
> pipeline need more resources to evaluate since the dataset is very huge.
> Please make sure there is enough free memory and maybe reset
> max_eval_time_mins to 10 or increase population_size to 50 or higher
> would let TPOT randomly generate a valid pipeline in initial generation.
>
> But without a fixed random_state, we could not reproduce the issue in our
> environment. If these possible solutions do not help, could you please
> provide codes with random_state parameter, versions of TPOT and its
> dependencies, python version and system information for let us to reproduce
> the issue?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/625#issuecomment-345258758>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AFf9tieo2-M3lYiWzqLc5n3qjWCmhd9fks5s3ZitgaJpZM4QiCRk>
> .
>
",hi thanks reply size training testing data respectively think data size would large sure free memory issue given size data set fact two memory calculation set calculation however used attached copy script thanks help candidate chemistry university fu wrote valid pipeline pipeline get valid fitness score initial generation weird quick look maybe pipeline need evaluate since huge please make sure enough free memory maybe reset increase higher would let randomly generate valid pipeline initial generation without fixed could reproduce issue environment possible help could please provide parameter python version system information let u reproduce issue thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
345258758,"Hmm, it seems that no valid pipeline (or pipeline is too time-consuming to get a valid fitness score) is generated in the initial generation, which is very weird. I have a quick look on the dataset in Kaggle, maybe each pipeline need more resources to evaluate since the dataset is very huge.  Please make sure there is enough free memory and maybe reset `max_eval_time_mins` to 10 or increase `population_size` to 50 or higher would let TPOT randomly generate a valid pipeline in initial generation. 

But without a fixed `random_state`, we could not reproduce the issue in our environment. If these possible solutions do not help, could you please provide codes with `random_state` parameter, versions of TPOT and its dependencies, python version and system information for let us to reproduce the issue?",valid pipeline pipeline get valid fitness score initial generation weird quick look maybe pipeline need evaluate since huge please make sure enough free memory maybe reset increase higher would let randomly generate valid pipeline initial generation without fixed could reproduce issue environment possible help could please provide parameter python version system information let u reproduce issue,issue,positive,positive,neutral,neutral,positive,positive
344653392,I think the last 4 commits in #622 fixed the issue.,think last fixed issue,issue,negative,positive,neutral,neutral,positive,positive
344386311,"Another issue related to `RuntimeWarning`, I submitted [a issue in deap's repo](https://github.com/DEAP/deap/issues/243) and will keep tracking this issue too.",another issue related issue keep issue,issue,negative,neutral,neutral,neutral,neutral,neutral
344352849,Another weird thing that import some deap object would raise `Hypervolume warning`. I submitted my observation to [this deap issue](https://github.com/DEAP/deap/issues/240) and will keep tracking this issue.,another weird thing import object would raise hypervolume warning observation issue keep issue,issue,negative,negative,negative,negative,negative,negative
344351241,It is weird that this compatibility issues only happened in python 3 but not in python 2.,weird compatibility python python,issue,negative,negative,negative,negative,negative,negative
344342333,"@korcky Hi, I just pushed #622 for this general solution. You may install that branch for testing with your customized model via the command below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@refine_op_util
```",hi general solution may install branch testing model via command pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
343834907,"That's will be great:
""more general solution should be let TPOT to tell whether `sklearn.metrics.r2_score` is a callable function or sklearn's `BaseEsitmator`""",great general solution let tell whether callable function,issue,positive,positive,positive,positive,positive,positive
343737363,"Actually I'm mainly using [Negative predictive value](https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values#Negative_predictive_value) which gives results between 0 and 1 and it works as you designed.
I was just experimenting and found this behavior. Your explanation is reasonable and its OK for me ;)
Thanks for hint and reasons. 
I believe we can close this thread now.",actually mainly negative predictive value work designed found behavior explanation reasonable thanks hint believe close thread,issue,negative,positive,neutral,neutral,positive,positive
343737086,I was thinking about accessing feature matrix X used in fold.,thinking feature matrix used fold,issue,negative,neutral,neutral,neutral,neutral,neutral
343735155,"I think we should keep the automatic tpot spirit when dealing with categorical and text features and assume the input data is a mixture of numerical/categorical and text features. We should automatically infer the column type and treat properly trying different pipelines.  In other words, it should be transparent to the user whether or not a column is textual or not. What do you think?",think keep automatic spirit dealing categorical text assume input data mixture text automatically infer column type treat properly trying different transparent user whether column textual think,issue,positive,neutral,neutral,neutral,neutral,neutral
343482530,Should be fixed on the dev branch now. Release coming soon.,fixed dev branch release coming soon,issue,negative,positive,neutral,neutral,positive,positive
343482390,Merged into dev branch now. Release coming soon.,dev branch release coming soon,issue,negative,neutral,neutral,neutral,neutral,neutral
343101370,"Sorry, I wanted to know whether I can use tpot for binary logistic. I have written logistic regression by mistake.",sorry know whether use binary logistic written logistic regression mistake,issue,negative,negative,negative,negative,negative,negative
342964083,"Yep, I'm using custom ML algorithms.
Right now TPOT's fit (if gets `cv=5`) receive from me: 
  * `X`: `ndarray` with shape `(5, 6)`
  * `y`: `ndarray` with shape `(5,)`
And, if I understand correctly, when fit method used my data it fits on 4 items and predicts 5th (that's what exactly what i want right now, because predict method of my model always return only one value). And looks like it works for me right now.",yep custom right fit receive shape shape understand correctly fit method used data th exactly want right predict method model always return one value like work right,issue,positive,positive,positive,positive,positive,positive
342957981,@korcky You are right. The demo is just a hacky way. I think a more general solution should be let TPOT to tell whether `'sklearn.metrics.r2_score'` is a callable function or sklearn's `BaseEsitmator` instead of checking the parameter name. @rhiever ,right hacky way think general solution let tell whether callable function instead parameter name,issue,negative,positive,positive,positive,positive,positive
342957411,"I think the challenge here is that because the parameter you're fixing is a *function*, it requires this special setup in the config dictionary. It is easy enough to fix any numerical, string, etc. parameter as you did in your original comment.",think challenge parameter fixing function special setup dictionary easy enough fix numerical string parameter original comment,issue,positive,positive,positive,positive,positive,positive
342953849,"I guess this solution doesn't support custom parameters names (something besides `score_func`), because if not changing `b` parameter name to `score_func ` I will have same exception? But what if I will have more than one parameter that I want to fix? What if this parameters not a score function?",guess solution support custom something besides parameter name exception one parameter want fix score function,issue,positive,neutral,neutral,neutral,neutral,neutral
342927974,Can you please provide us with more details on the shape of your data? Are you using custom ML algorithms as well?,please provide u shape data custom well,issue,positive,neutral,neutral,neutral,neutral,neutral
342927101,"Can you please clarify what you mean by this?

>I just want to know whether we can use tpot for logistic regression?

Logistic regression is a classification method and is included in the `TPOTClassifier` default configuration.",please clarify mean want know whether use logistic regression logistic regression classification method included default configuration,issue,negative,negative,negative,negative,negative,negative
342926826,@mlapierre I closed this PR since the PR already merged to dev branch! Thank you!,closed since already dev branch thank,issue,negative,negative,neutral,neutral,negative,negative
342900290,"Do you mean using the GP optimization in TPOT for tuning parameters of logistic regression? If so there is hacky way for it:

One of my dev branch of TPOT called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `fixed_length`, which can disable `CombineDFs` if `fixed_length` is not None (e.g. `TPOTClassifier(fixed_length=1)`) and generate pipeline with fixed number of operators (e.g. `fixed_length=1` means only 1 operator). 

**Note: this dev branch is not fully tested yet**. 

You may install this branch in your test environment via the command below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
```

Then, you need use [a custom configuration](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) with only one operator in this dictionary as the demo below:

```python
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

tpot_config = {
    'sklearn.linear_model.LogisticRegression': {
        'penalty': [""l1"", ""l2""],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'dual': [True, False]
    }
}

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, 
                                 fixed_length=1, config_dict=tpot_config)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```
",mean optimization tuning logistic regression hacky way one dev branch option disable none generate pipeline fixed number operator note dev branch fully tested yet may install branch test environment via command pip install upgrade need use custom configuration one operator dictionary python import import import true false print,issue,positive,negative,neutral,neutral,negative,negative
342887776,"How about using the `config_dict` as below?

**You need change `b` parameter to `score_func` though.**

```
config_dict = {
    'tpot_test.Test': {
        'a': np.arange(-1, 1, 0.1),
        'score_func': {
            'sklearn.metrics.r2_score': None
        }
    }
}

```",need change parameter though none,issue,negative,neutral,neutral,neutral,neutral,neutral
342854007,"Oh, yeah, I forgot to update TPOT. It's fix problem with value parameter (now DEFAULT disappeared), but when using method or class exception appear:

## Code

```python
import numpy as np
from sklearn.base import RegressorMixin
from sklearn.metrics import r2_score
from tpot import TPOTRegressor


class Test(RegressorMixin):
    def __init__(self, a=None, b=None):
        print(a, b)
        self.a = a
        self.b = b

    def fit(self, X, y):
        return

    def predict(self, X):
        if self.b is r2_score:
            return self.a * X + 1
        else:
            return self.a * X

    def score(self, X, y, sample_weight=None):
        return r2_score(y_true=y, y_pred=self.predict(X))

    def get_params(self, deep=None):
        return {'a': self.a, 'b': self.b}


config_dict = {
    'tpot_test.Test': {
        'a': np.arange(-1, 1, 0.1),
        'b': [r2_score]
    }
}

tpot = TPOTRegressor(generations=5, population_size=5, cv=2, verbosity=1,
                     config_dict=config_dict)

tpot.fit(np.arange(0, 10, 1).reshape(-1, 1), (np.arange(0, 5, 0.5) + 1))
```

## Exception

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 586, in fit
    per_generation_function=self._check_periodic_pipeline
  File ""/usr/local/lib/python3.5/site-packages/tpot/gp_deap.py"", line 219, in eaMuPlusLambda
    per_generation_function()
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 848, in _check_periodic_pipeline
    self._update_top_pipeline()
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 632, in _update_top_pipeline
    raise RuntimeError('There was an error in the TPOT optimization '
RuntimeError: There was an error in the TPOT optimization process. This could be because the data
 was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object.
 Please make sure you passed the data to TPOT correctly.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""tpot_test.py"", line 37, in <module>
    config_dict=config_dict)
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 265, in __init__
    ArgBaseClass=ARGType
  File ""/usr/local/lib/python3.5/site-packages/tpot/operator_utils.py"", line 138, in TPOTOperatorClassFactory
    import_str, op_str, op_obj = source_decode(opsourse)
  File ""/usr/local/lib/python3.5/site-packages/tpot/operator_utils.py"", line 70, in source_decode
    exec('from {} import {}'.format(import_str, op_str))
  File ""<string>"", line 1, in <module>
  File ""/Users/korcky/Documents/cindicator/cindicator-ai/tpot_test.py"", line 39, in <module>
    tpot.fit(np.arange(0, 10, 1).reshape(-1, 1), (np.arange(0, 5, 0.5) + 1))
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 616, in fit
    raise e
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 609, in fit
    self._update_top_pipeline()
  File ""/usr/local/lib/python3.5/site-packages/tpot/base.py"", line 632, in _update_top_pipeline
    raise RuntimeError('There was an error in the TPOT optimization '
RuntimeError: There was an error in the TPOT optimization process. This could be because the data 
 was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object.
 Please make sure you passed the data to TPOT correctly.
```
But works fine if change `config_dict` to:
```
config_dict = {
    'tpot_test.Test': {
        'a': np.arange(-1, 1, 0.1),
        'b': [1]
    }
}
```",oh yeah forgot update fix problem value parameter default method class exception appear code python import import import import class test self print fit self return predict self return else return score self return self return exception recent call last file line fit file line file line file line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly handling exception another exception recent call last file line module file line file line file line import file string line module file line module file line fit raise file line fit file line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly work fine change,issue,positive,positive,positive,positive,positive,positive
342846767,We may need better way to replace `_pre_test` decorator.,may need better way replace decorator,issue,negative,positive,positive,positive,positive,positive
342846579,"Hmm, `_pre_test` decorator can not be turned off in current version of TPOT. We added this decorator for preventing TPOT from generating invalid parameter combination by evaluating each pipeline on a small test dataset. ",decorator turned current version added decorator generating invalid parameter combination pipeline small test,issue,negative,negative,negative,negative,negative,negative
342844688,Is this the latest version of TPOT (v0.9)? I think I removed the DEFAULT in this [commit](https://github.com/rhiever/tpot/commit/5d6b73184c02600a85476856b9e58be6fd85d5de).,latest version think removed default commit,issue,negative,positive,positive,positive,positive,positive
342514880,"@weixuanfu we have been testing your feature and we think it works correctly, at least  it works correctly for  us. We have been able to export every pipeline without any problem (using an appropiate configuration dictionary as you have said).  ",testing feature think work correctly least work correctly u able export every pipeline without problem configuration dictionary said,issue,negative,positive,neutral,neutral,positive,positive
342492384,"@esanchezSavvyds you may want to use a `configuration dictionary` for excluding `ZeroCount` and `XGBClassififer` from [the default dictionary](https://github.com/rhiever/tpot/blob/master/tpot/config/classifier.py) and pass it to `config_dict` parameter in TPOT, in order to avoid these operators that PMML do not supported so far. Please check the example in this [link](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters)",may want use configuration dictionary excluding default dictionary pas parameter order avoid far please check example link,issue,negative,positive,neutral,neutral,positive,positive
342492283,"@esanchezSavvyds You should list all ""problematic"" TPOT estimator and transformer types here: https://github.com/jpmml/jpmml-sklearn/issues/54",list problematic estimator transformer,issue,negative,neutral,neutral,neutral,neutral,neutral
342490103,"Hi @weixuanfu , first of all, thank you very much for your effort. We are testing this feature and we have found that ZeroCount transformation is neither supported. This is the error:

java.lang.IllegalArgumentException: The transformer object (Python class tpot.builtins.zero_count.ZeroCount) is not a Transformer or is not a supported Transformer subclass",hi first thank much effort testing feature found transformation neither error transformer object python class transformer transformer subclass,issue,positive,positive,positive,positive,positive,positive
342359766,"You may set `n_jobs` larger than 1, e.g. `n_jobs=8` for speeding up. But please use python>3.4 to avoid freezing issue mentioned in this [link](https://rhiever.github.io/tpot/using/#crashfreeze-issue-with-n_jobs-1-under-osx-or-linux).",may set speeding please use python avoid freezing issue link,issue,negative,neutral,neutral,neutral,neutral,neutral
342357135,"other question，：
i train 5000 ，it  work so slow,  mem use is ok ,how can i make it  faster ?
[root@hardyserver ~]# free -g
             total       used       free     shared    buffers     cached
Mem:            31         16         14          0          0          5
-/+ buffers/cache:         11         20
Swap:           15          1         13

>>> tpot.fit(X_train, y_train)
Optimization Progress:   3%|█████                                                                                                                                                                   | 2/66 [06:12<2:29:46, 140.41s/pipeline",train work slow mem use make faster root free total used free mem swap optimization progress,issue,positive,positive,positive,positive,positive,positive
342217916,"@esanchezSavvyds One of my dev branch of TPOT called [noCDF_noStacking](https://github.com/weixuanfu/tpot/tree/noCDF_noStacking) has a option named `simple_pipeline`, which can disable both `StackingEstimator` and `CombineDFs` if `simple_pipeline=True` (e.g. `TPOTClassifier(simple_pipeline=True)`). But it is noted that this dev branch is not fully tested yet. If you want to try TPOT without `StackingEstimator` and `FeatureUnion`, you may install this branch in your test environment via the command below:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@noCDF_noStacking
```
",one dev branch option disable noted dev branch fully tested yet want try without may install branch test environment via command pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
342180654,"@vruusmann I think the problem is not creating a compatible config directory, which I agree it has to be done. The problem is that, as far as I can understand, StackingEstimator cannot be disabled in tpot and it's not supported by the sklearn2pmml.",think problem compatible directory agree done problem far understand disabled,issue,negative,negative,neutral,neutral,negative,negative
342174915,"@esanchezSavvyds For the pickable issue, you may try to use ['copy.copy'](https://docs.python.org/2/library/copy.html) instead of the lambda function used in old version of TPOT. 

```
from copy import copy
exported_pipeline = make_pipeline(
    make_union(
        make_union(VotingClassifier([('branch',
            DecisionTreeClassifier(criterion=""gini"", max_depth=8, min_samples_leaf=5, min_samples_split=5)
        )]), FunctionTransformer(copy)),
        SelectKBest(score_func=f_classif, k=20)
    ),
    KNeighborsClassifier(n_neighbors=10, p=1, weights=""uniform"")
)
```",pickable issue may try use instead lambda function used old version copy import copy copy uniform,issue,negative,positive,neutral,neutral,positive,positive
342172373,"@esanchezSavvyds You're using the `FeatureUnion` transformation type in a context which requires an estimator type. Specifically, feature union cannot be the last step of a pipeline.",transformation type context estimator type specifically feature union last step pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
342171740,@rhiever @weixuanfu Here's a possible workflow for automatically generating PMML-compatible TPOT configuration dictionaries: https://github.com/jpmml/jpmml-sklearn/issues/55,possible automatically generating configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
342169947,"@vruusmann I am using the last version of the java command line application and this versions:

- python:  3.6.2
- sklearn:  0.18.2
- sklearn.externals.joblib: 0.10.3
- pandas:  0.21.0
- sklearn_pandas:  1.6.0
- sklearn2pmml:  0.26.0

And we are getting that FeatureUnion is not supported with this error:

Failed to convert
java.lang.IllegalArgumentException: The estimator object (Python class sklearn.pipeline.FeatureUnion) is not an Estimator or is not a supported Estimator subclass
        at sklearn.EstimatorUtil$1.apply(EstimatorUtil.java:90)
        at sklearn.EstimatorUtil$1.apply(EstimatorUtil.java:78)
        at sklearn.EstimatorUtil.asEstimator(EstimatorUtil.java:42)
        at sklearn2pmml.PMMLPipeline.getEstimator(PMMLPipeline.java:216)
        at sklearn2pmml.PMMLPipeline.encodePMML(PMMLPipeline.java:73)
        at org.jpmml.sklearn.Main.run(Main.java:144)
        at org.jpmml.sklearn.Main.main(Main.java:93)
Caused by: java.lang.ClassCastException: sklearn.pipeline.FeatureUnion cannot be cast to sklearn.Estimator
        at sklearn.EstimatorUtil$1.apply(EstimatorUtil.java:88)",last version command line application python getting error convert estimator object python class estimator estimator subclass cast,issue,negative,neutral,neutral,neutral,neutral,neutral
342159829,"Yes, that was my thinking as well, @vruusmann. The issue is that stacking and pipeline splitting are not currently configurable in TPOT configuration dictionaries; they are always on by default. Hence my suggestion to add options to turn them off.

I agree that it would be wise for `SkLearn2PMML/JPMML-SkLearn` to maintain a TPOT configuration dictionary that is 100% PMML compliant. We could document the location of that configuration dictionary in the TPOT docs and point to the corresponding `SkLearn2PMML/JPMML-SkLearn` docs page.",yes thinking well issue pipeline splitting currently configuration always default hence suggestion add turn agree would wise maintain configuration dictionary compliant could document location configuration dictionary point corresponding page,issue,positive,positive,positive,positive,positive,positive
342156284,"First, the `FeatureUnion` meta-transformation has been supported for over a year or so. I'm afraid that @esanchezSavvyds is simply using an outdated `sklearn2pmml` package version.

Second, I intend to catch up with TPOT-specific estimator types sometimes in December. You can track my progress by subscribing to this issue: https://github.com/jpmml/jpmml-sklearn/issues/54

As a workaround, it might be worthwhile to define a PMML-specific configuration dictionary (argument `config_dict = ""TPOT pmml""` to TPOT estimator types), which restricts the use of estimator and transformer types that are currently not convertible to PMML. However, this configuration dictionary should be maintained by the SkLearn2PMML/JPMML-SkLearn projects, because it should not be TPOT's concern long-term?",first year afraid simply outdated package version second intend catch estimator sometimes track progress issue might define configuration dictionary argument estimator use estimator transformer currently convertible however configuration dictionary concern,issue,negative,negative,negative,negative,negative,negative
342153209,"That's too bad. Perhaps we can add an option (or series of flags) to disable features such as stacking and pipeline splitting (i.e., CombineDFs). Disabling those features should then make the TPOT pipelines PMML compliant.",bad perhaps add option series disable pipeline splitting make compliant,issue,negative,negative,negative,negative,negative,negative
342152634,"@mlapierre, we'd be happy to review and merge a PR for this enhancement to the periodic checkpoint folder feature.",happy review merge enhancement periodic folder feature,issue,positive,positive,positive,positive,positive,positive
342136502,@weixuanfu @rasbt I have tried to export that pipeline to PMML. I have removed the FunctionTransformer because I can't pickle it. The issue I'm having when I try it is that FeatureUnion is not supported. The specification of the JAVA API I'm using to export to PMML says VotingClassifier is supported. Here you have it [https://github.com/jpmml/jpmml-sklearn](url). Thank you all,tried export pipeline removed ca pickle issue try specification export thank,issue,negative,neutral,neutral,neutral,neutral,neutral
342127159,"A couple things:

1) It looks like your `y` is continuous, i.e., this is a regression problem. You should use `TPOTRegressor` in place of `TPOTClassifier`.

2) You have very few training instances---only 5, it seems. I recommend collecting enough data until you have at least 100 training instances before applying any form of machine learning.",couple like continuous regression problem use place training recommend enough data least training form machine learning,issue,negative,negative,negative,negative,negative,negative
341779392,"Old version (< v0.7.3) of TPOT used `VotingClassifier` but it caused the issue #457 for stacking regressor in `TPOTRegressor` so we added to `StackingEstimator` for solving this issue. 

@esanchezSavvyds could you please try to export the pipeline below to PMML?

```
exported_pipeline = make_pipeline(
    make_union(
        make_union(VotingClassifier([('branch',
            DecisionTreeClassifier(criterion=""gini"", max_depth=8, min_samples_leaf=5, min_samples_split=5)
        )]), FunctionTransformer(lambda X: X)),
        SelectKBest(score_func=f_classif, k=20)
    ),
    KNeighborsClassifier(n_neighbors=10, p=1, weights=""uniform"")
)
```",old version used issue regressor added issue could please try export pipeline lambda uniform,issue,negative,positive,neutral,neutral,positive,positive
341775253,"I am happy about PRs to the StackingClassifier & StackingCVClassifier fixing this; however, it sounds like it's rather due to PMML? In that case, I am curious, does the `VotingClassifier` from scikit-learn work? I implemented the `VotingClassifier` quite similarly (however, it does not have the level-2/meta estimator).",happy fixing however like rather due case curious work quite similarly however estimator,issue,positive,positive,positive,positive,positive,positive
341773886,"I have a quick test on car training dataset via CLI mode of TPOT using the command below:
```
tpot .\car-Training0.csv -target class -is "","" -g 5 -p 30 -v 2

TPOT settings:
CHECKPOINT_FOLDER   =     None
CONFIG_FILE         =     None
CROSSOVER_RATE      =     0.1
EARLY_STOP          =     None
GENERATIONS         =     5
INPUT_FILE          =     .\car-Training0.csv
INPUT_SEPARATOR     =     ,
MAX_EVAL_MINS       =     5
MAX_TIME_MINS       =     None
MUTATION_RATE       =     0.9
NUM_CV_FOLDS        =     5
NUM_JOBS            =     1
OFFSPRING_SIZE      =     30
OUTPUT_FILE         =
POPULATION_SIZE     =     30
RANDOM_STATE        =     None
SCORING_FN          =     accuracy
SUBSAMPLE           =     1.0
TARGET_NAME         =     class
TPOT_MODE           =     classification
VERBOSITY           =     2

Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.
Generation 1 - Current best internal CV score: 0.9802534630120838
Generation 2 - Current best internal CV score: 0.9828470380194518
Generation 3 - Current best internal CV score: 0.9828470380194518
Generation 4 - Current best internal CV score: 0.9845711759504864
Generation 5 - Current best internal CV score: 0.9845711759504864

Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.5, max_depth=4, max_features=0.95, min_samples_leaf=8, min_samples_split=11, n_estimators=100, subsample=0.85)

Training score: 0.9845711759504864
Holdout score: 0.9845758354755784
```
TPOT works fine. 

I suspected that the issue maybe related to the way of formatting `y`. Please try the demo below for formatting dataset. 

```
import pandas as pd
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep=',', dtype=np.float64)
features = tpot_data.drop('class', axis=1).values
classes = tpot_data['class'].values
```



",quick test car training via mode command class none none none none none accuracy subsample class classification verbosity warning available used generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline training score holdout score work fine suspected issue maybe related way please try import class,issue,positive,positive,positive,positive,positive,positive
341771754,"I'm facing this problem only when the dataset has more than two classes. When the problem is from binary classification, this doesn't happen.",facing problem two class problem binary classification happen,issue,negative,neutral,neutral,neutral,neutral,neutral
341769845,"These are the datasets:

(car) https://www.dropbox.com/sh/o8l4uyy9mlyepsp/AABz6jlkAs1EGYccQ_Kp43Lra?dl=0

(vehicle) https://www.dropbox.com/sh/zt0xgxk8kersqun/AACv_CISMObJfMLLRsCp07Lra?dl=0


For the dataset 'car', I used the One Hot Encoder to create the new features (nominal to binary) and the Label Encoder to the define the class. 

For the dataset 'vehicle', I only used the Label Encoder, because the features were already numerical.",car vehicle used one hot create new nominal binary label define class used label already numerical,issue,negative,positive,positive,positive,positive,positive
341765950,Could you please share the codes/dataset for reproducing this issue?,could please share issue,issue,positive,neutral,neutral,neutral,neutral,neutral
341694304,"It's probable that the StackingEstimator is not supported in PMML, as it's a custom class outside of scikit-learn that's implemented by @rasbt. PMML would need to be extended to support estimator stacking.

You can look at alternatives to the best-scoring pipeline in the `pareto_front_fitted_pipelines_` attribute of TPOT, accessed with `tpot_object.pareto_front_fitted_pipelines_`. That attribute should contain multiple possible solution pipelines for your problem, ranging from more complex and high-scoring to less complex and slightly-lower-scoring. Perhaps one of the less complex pipelines won't have the StackingEstimator.",probable custom class outside would need extended support estimator look pipeline attribute attribute contain multiple possible solution problem ranging complex le complex perhaps one le complex wo,issue,negative,negative,negative,negative,negative,negative
341693288,@weixuanfu Problems have returned. I'm having problems at exporting the pipeline to PMML when tpot generates a model using StackingEstimator as it says it's not a supported transformation. What can I do? Is there a possibility to not use it? Thank you,returned pipeline model transformation possibility use thank,issue,negative,neutral,neutral,neutral,neutral,neutral
341680473,"Oh, that is indeed a strange output. We do [use the absolute value](https://github.com/rhiever/tpot/blob/89d3ee6b4a48c6d411761ec13121006ac5ee59be/tpot/base.py#L782) when reporting the score of the best TPOT pipeline at each generation. We wrote TPOT assuming that the metrics being used within TPOT won't have a meaningful positive *and* negative value.

Judging from your other issue (#611), I am assuming that your metric is the amount of profit/loss (in terms of money) that the algorithm would have caused for you based on its performance on all of the training records. I believe that is an atypical metric design in ML. Typically we use metrics that quantify the amount of mismatch between the predicted value and the true value of the target.

That said, I think your metric can be supported in the current TPOT framework. Of course, your metric still works on the optimization side because TPOT will maximize any metric that doesn't have 'loss' or 'error' in the name. However, I think your metric could be better supported within TPOT if you devise a function of the metric that results in negative values being > 0 and positives values > all possible negative values. For example, your metric can return `profits + 1` if `profits >= 0`, and return `1.0 / abs(profits)` if `profits < 0`. That function would cause the metric to return an ever smaller value with more losses (i.e., negative profits) and an ever increasing value with more profits.",oh indeed strange output use absolute value score best pipeline generation wrote assuming metric used within wo meaningful positive negative value issue assuming metric amount money algorithm would based performance training believe atypical metric design typically use metric quantify amount mismatch value true value target said think metric current framework course metric still work optimization side maximize metric name however think metric could better within devise function metric negative possible negative example metric return return function would cause metric return ever smaller value negative ever increasing value,issue,positive,positive,neutral,neutral,positive,positive
341677613,"When you say that it needs access to the entire array, do you mean that it needs access to the feature matrix `X` as well? Or do you mean that you don't want to perform k-fold CV and instead evaluate each pipeline only once on the entire training set?",say need access entire array mean need access feature matrix well mean want perform instead evaluate pipeline entire training set,issue,negative,negative,negative,negative,negative,negative
341671465,"@esanchezSavvyds Cool, good to know it solved the issue. No need to apologize.",cool good know issue need apologize,issue,positive,positive,positive,positive,positive,positive
341644989,@weixuanfu I apologize for not seeing that earlier. That's what I need. Thank you very much for your job.,apologize seeing need thank much job,issue,negative,positive,positive,positive,positive,positive
341468835,@esanchezSavvyds Please check the [TPOT API](https://rhiever.github.io/tpot/api/). I think the `fitted_pipeline_` attribute (e.g. `tpot_object.fitted_pipeline_`) is what you need.,please check think attribute need,issue,negative,neutral,neutral,neutral,neutral,neutral
341464101,"Thank you for your fast answer @weixuanfu . My aim is to automatize the model generation with tpot and export it to PMML automatically. For that reason, I need to access to the scikit-learn pipeline at execution time in my code. Is that possible or the only possibility is to access to it through that file manually?",thank fast answer aim automatize model generation export automatically reason need access pipeline execution time code possible possibility access file manually,issue,negative,positive,neutral,neutral,positive,positive
341460900,"@esanchezSavvyds `read_csv` function is for reading input dataset, you may need change the file path string 'PATH/TO/DATA/FILE' in that function to the dataset path and also need change `COLUMN_SEPARATOR` based on the dataset.

You may find a line in the .py file that starts with `exported_pipeline`, which is the scikit-learn pipeline. For example:

```
exported_pipeline = make_pipeline(
    SelectPercentile(score_func=f_classif, percentile=65),
    DecisionTreeClassifier(criterion=""gini"", max_depth=7, min_samples_leaf=4, min_samples_split=18)
)
```",function reading input may need change file path string function path also need change based may find line file pipeline example,issue,negative,neutral,neutral,neutral,neutral,neutral
341458380,@rhiever Thank you very much for your time. I'm new to this world. How can I do that? With the export function I create .py file. In that file there is a read_csv function which I don´t understand what does. How can I get the scikit-learn pipeline from that file?,thank much time new world export function create file file function understand get pipeline file,issue,positive,positive,positive,positive,positive,positive
341444594,"If you use TPOT's `export` function, you can export the code to a scikit-learn pipeline. From there, whatever process you use to convert scikit-learn pipelines to PMML xml should work fine (as long as it supports the scikit-learn Pipeline object).",use export function export code pipeline whatever process use convert work fine long pipeline object,issue,negative,positive,positive,positive,positive,positive
341443715,"Hi @rhiever @vruusmann @jln-ho  , I'm trying to export my tpot model to a PMML xml using jpmml-sklearn but I'm getting crazy to do it. Is there any option to do it? If not, how can I export my tpot model to use it later in Java? Thank you all, I'm looking forward to your response.",hi trying export model getting crazy option export model use later thank looking forward response,issue,negative,negative,negative,negative,negative,negative
341421022,"@woodrujm , if you instead use `pickle.dump(tpot.fitted_pipeline_,xx)` it works as you might have intended.",instead use work might intended,issue,negative,neutral,neutral,neutral,neutral,neutral
341170270,I close this issue since there is no comment these weeks. Please feel free to re-open the issue if you have any more questions.,close issue since comment please feel free issue,issue,positive,positive,positive,positive,positive,positive
341141104,"In general, dask leaves scheduler choice up to the user. It provides good
defaults dask collections (multiprocessing for dask.bag, threaded for
dask.array and dask.dataframe), and makes it easy to switch between
schedulers.

On Fri, Oct 27, 2017 at 8:33 AM, QuantumDamage <notifications@github.com>
wrote:

> @TomAugspurger <https://github.com/tomaugspurger>
> Maybe if there is a way to predict memory allocation we could swap
> scheduler to better fit on available resources? Kind of heuristics which
> will take available cpus, threads, ram and swap as parameters?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/304#issuecomment-339972505>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ABQHIofrrPQDXzA4136-FiamHElToo9Cks5swdukgaJpZM4KqMt4>
> .
>
",general leaf choice user good threaded easy switch wrote maybe way predict memory allocation could swap better fit available kind take available ram swap reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
340819753,"Okay, thanks for the reply. TPOT has been great so far!",thanks reply great far,issue,positive,positive,positive,positive,positive,positive
340816557,"I think the issue is related to #520 about pickling TPOT object. I think, for now, entire TPOT object is not pickleable due to the attribute lookup issue. You should be able to pickle these attributes in [TPOT API](https://rhiever.github.io/tpot/api/). We may work on this picleable issue later. ",think issue related object think entire object due attribute issue able pickle may work issue later,issue,negative,positive,neutral,neutral,positive,positive
340795501,"I get this error when trying to pickle the tpot model:

with open('tpot_.pkl','wb') as xx:
    pickle.dump(tpot,xx)

""""""PicklingError: Can't pickle <class 'tpot.operator_utils.XGBClassifier__learning_rate'>: attribute lookup XGBClassifier__learning_rate on tpot.operator_utils failed""""""

",get error trying pickle model open ca pickle class attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
340165186,"> Looks like we have several merge conflicts to sort out now.

@rhiever, did you mean in the static content (as I've specified, my latest pull request does not contain the updated rendered content), or otherwise?",like several merge sort mean static content latest pull request contain content otherwise,issue,negative,positive,positive,positive,positive,positive
339972505,"@TomAugspurger 
Maybe if there is a way to predict memory allocation we could swap scheduler to better fit on available resources? Kind of heuristics which will take available cpus, threads, ram and swap as parameters?",maybe way predict memory allocation could swap better fit available kind take available ram swap,issue,positive,positive,positive,positive,positive,positive
339952220,"> With dask, is there any way to prevent constant duplication of a dataset in memory?

Dask lets you swap out the scheduler (threaded, multiprocessing, distributed) easily. The threaded scheduler will avoid the need to clone the datasets multiple times, but for the best performance the algorithms should release the GIL.

I spent a bit of time a few weeks ago looking into this, but I'm not too familiar with tpot. The joblib-based parallelization added in the dev branch seems to do pretty well for coarse-grained parallelism like ""fit these 8 models in parallel"".

If you're able to build the entire graph ahead of time, and if there's redundant computations in multiple branch, dask will be able to avoid those redundant computations.

If anyone more familiar with tpot is interested in prototyping something, I'd be happy to support from the dask side. Otherwise, I'll try to take another look in a few weeks.",way prevent constant duplication memory swap threaded distributed easily threaded avoid need clone multiple time best performance release spent bit time ago looking familiar parallelization added dev branch pretty well parallelism like fit parallel able build entire graph ahead time redundant multiple branch able avoid redundant anyone familiar interested something happy support side otherwise try take another look,issue,positive,positive,positive,positive,positive,positive
339922578,"The more I'm watching Dask videos (for example https://www.youtube.com/watch?v=RA_2qdipVng) i believe that this could make much sense. 
Was there any approaches to test tpot with dask?",watching example believe could make much sense test,issue,negative,positive,positive,positive,positive,positive
339760226,@mikister Agree. I just updated the PR. Thank you for catching this issue and suggestion.,agree thank catching issue suggestion,issue,positive,positive,positive,positive,positive,positive
339756590,"@weixuanfu 
You should be able to install it from pypi with python 2.x but not with python 3.x, here you must use conda, or you can install pywin32 from a wheel.",able install python python must use install wheel,issue,negative,positive,positive,positive,positive,positive
339680635,"@Damian89 I just submitted the PR #607 for this fix. You may try to install the branch with the commend below for a test:

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@scoring_api
```",fix may try install branch commend test pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
339667874,"OK, I think this is because the scoring function is not a instance of [`_BaseScorer`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/scorer.py#L46), which can be easily built with [`make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)

I think I need refine [this line](https://github.com/rhiever/tpot/blob/development/tpot/base.py#L352) for letting TPOT to tell whether it is a scoring function with estimator as first argument instead. ",think scoring function instance easily built think need refine line tell whether scoring function estimator first argument instead,issue,negative,positive,positive,positive,positive,positive
339650005,"Hmm...
I'm running the current 0.9.1 (installed via pip using developement branch) and have the following code:

```
tpot = TPOTClassifier(
    generations=5,
    population_size=4,
    verbosity=2,
    n_jobs=-1,
    random_state=42,
    periodic_checkpoint_folder=workfolder + ""top_classifiers"",
    scoring=my_scorer
)
```

my_scorer is defined as:

```
def my_scorer(clf,X,y):
   ... code
   return a_float_value
```

This is the way it works flawless with GridSearchCV:

```
model = RandomizedSearchCV(
    ensemble.GradientBoostingClassifier(random_state=42),
    param_distributions=param_dist,
    scoring=my_scorer,
    n_jobs=-1,
)
```

Seems not to work with tpot, all I get is the following error:

> /usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
>   ""This module will be removed in 0.20."", DeprecationWarning)
> /home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py:362: DeprecationWarning: Scoring function <function my_scorer at 0x7f63f608eea0> looks like it is a metric function rather than a scikit-learn scorer. This scoring type was deprecated in version TPOT 0.9.1 and will be removed in version 0.11. Please update your custom scoring function.
>   'Please update your custom scoring function.'.format(scoring), DeprecationWarning)
> Traceback (most recent call last):
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 617, in fit
>     per_generation_function=self._check_periodic_pipeline
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/gp_deap.py"", line 245, in eaMuPlusLambda
>     per_generation_function()
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 904, in _check_periodic_pipeline
>     self._update_top_pipeline()
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 696, in _update_top_pipeline
>     raise RuntimeError('There was an error in the TPOT optimization '
> RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/modelselect-tpot.py"", line 206, in <module>
>     tpot.fit(X_train, y_train)
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 649, in fit
>     raise e
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 640, in fit
>     self._update_top_pipeline()
>   File ""/home/damian/Projekte (Python)/NMR-Projekt/src/dev/tpot/base.py"", line 696, in _update_top_pipeline
>     raise RuntimeError('There was an error in the TPOT optimization '
> RuntimeError: There was an error in the TPOT optimization process. This could be because the data was not formatted properly, or because data for a regression problem was provided to the TPOTClassifier object. Please make sure you passed the data to TPOT correctly.
> 
> Process finished with exit code 1


Do you have any ideas?",running current via pip branch following code defined code return way work flawless model work get following error module version favor module class also note interface new different module module removed module removed python scoring function function like metric function rather scorer scoring type version removed version please update custom scoring function update custom scoring function scoring recent call last file python line fit file python line file python line file python line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly handling exception another exception recent call last file python line module file python line fit raise file python line fit file python line raise error optimization error optimization process could data properly data regression problem provided object please make sure data correctly process finished exit code,issue,positive,positive,positive,positive,positive,positive
339631635,"Thanks, I think windows users may need use conda to install it instead of pypi.",thanks think may need use install instead,issue,negative,positive,positive,positive,positive,positive
339451164,Could you please try `conda install -c anaconda pywin32`?,could please try install anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
339052719,Thank you for reporting this issue. I think it should be a bug. I will working on fixing it and add an unit test for this issue. @rhiever ,thank issue think bug working fixing add unit test issue,issue,negative,neutral,neutral,neutral,neutral,neutral
338661901,"Hmm, a good idea. It seems Keras has useful [wrappers for the Scikit-Learn API](https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api). We need figure out a way to pass callable function to `build_fn` of wrapped classes.",good idea useful need figure way pas callable function wrapped class,issue,positive,positive,positive,positive,positive,positive
338489081,"@weixuanfu excellent!
I'll see If i can give it a go when it's merged, and test it!
I still have to figure out how to install packages from github on my anaconda",excellent see give go test still figure install anaconda,issue,positive,positive,positive,positive,positive,positive
338487482,@HarkDev please check the PR #560. I will rebase this PR on version 0.9. ,please check rebase version,issue,negative,neutral,neutral,neutral,neutral,neutral
338487187,Hey there. I would like to take a crack at this feature. ,hey would like take crack feature,issue,negative,neutral,neutral,neutral,neutral,neutral
337614334,Looks like we have several merge conflicts to sort out now.,like several merge sort,issue,negative,neutral,neutral,neutral,neutral,neutral
337432222,"OK, I've made all the changes and everything's passed. I think that's all you need from my end, but let me know if there's another step. Apologies for the latency - I mostly on get to work on this first thing in the morning or last thing at night, while the rest of the house is asleep.",made everything think need end let know another step latency mostly get work first thing morning last thing night rest house asleep,issue,negative,positive,positive,positive,positive,positive
337390687,"Just a heads-up from me. I still intend to submit a PR regarding the time-series example request, and hopefully (subject to time availability) you'll have the opportunity to review it and provide feedback soon enough.

I like to do things in a ground-up manner and, naturally, I've made sure to attempt to familiarize myself with the project and, specifically, the documentation part, before embarking on contributing with an example.

As part of my acclimation, I've issued #600.",still intend submit regarding example request hopefully subject time availability opportunity review provide feedback soon enough like manner naturally made sure attempt familiarize project specifically documentation part example part acclimation,issue,positive,positive,positive,positive,positive,positive
337226083,"@gaochen219, please feel free to drop a link to the SO/CrossValidated thread if you ended up posting it there. In the meantime, I will close this issue.",please feel free drop link thread ended posting close issue,issue,positive,positive,positive,positive,positive,positive
337225886,"I made some minor suggestions for wording. Once those are sorted out, I think we're good to merge.",made minor wording sorted think good merge,issue,negative,positive,positive,positive,positive,positive
337202055,"@saddy001: We do indeed use CV to get an estimate of generalization performance. AFAIK, I don't know of any paper that explicitly explores the issue of random seeds and generalizability. However, it makes inherent sense that an algorithm's performance shouldn't heavily depend on its random seed. Random seeds are designed for reproducibility and aren't intended to be another ""knob"" (i.e., parameter) to tune in the algorithm.",indeed use get estimate generalization performance know paper explicitly issue random however inherent sense algorithm performance heavily depend random seed random designed reproducibility intended another knob parameter tune algorithm,issue,negative,negative,negative,negative,negative,negative
337069006,"OK, will do! I also read the scikit-learn contributor documentation, which suggest submitting a [WIP] PR immediately, but since this is so simple I'll just wait until I've made all the changes",also read contributor documentation suggest immediately since simple wait made,issue,negative,neutral,neutral,neutral,neutral,neutral
336901383,"In terms of *where* to add this note in the docs, I think we should add it to:

- [ ] `base.py` [fit function](https://github.com/rhiever/tpot/blob/911ece728b77f4ef5cb6192f5002060916ea332f/tpot/base.py#L455) docstrings
- [ ] The [API docs](https://github.com/rhiever/tpot/blob/master/docs_sources/api.md) for the `fit` function (both TPOTClassifier and TPOTRegressor)
- [ ] The [Using docs](https://github.com/rhiever/tpot/blob/master/docs_sources/using.md) under the `TPOT with code` section

When you send the PR, please send it to the `master` branch. This is a rare exception where we will merge the PR directly to the `master` branch because it is a documentation change that we can roll out immediately.",add note think add fit function fit function code section send please send master branch rare exception merge directly master branch documentation change roll immediately,issue,positive,positive,positive,positive,positive,positive
336899669,"@matthewwritter, we'd be happy for you to contribute a PR for this issue. I will change the issue status to indicate that you've taken on this PR. Please check out the [contributor's documentation](http://rhiever.github.io/tpot/contributing/) (linked by @weixuanfu above), which provides details on how to contribute to our project. Feel free to ask any questions you have here.",happy contribute issue change issue status indicate taken please check contributor documentation linked contribute project feel free ask,issue,positive,positive,positive,positive,positive,positive
336700582,"To get a first insight one could include non-linear preprocessors, run TPOT for 2-3 standard datasets and look into the best pipelines, if any of those preprocessors were included. ",get first insight one could include run standard look best included,issue,positive,positive,positive,positive,positive,positive
336648059,"> I think the general consensus from the ML community is that if an algorithm/pipeline's performance is highly dependent on its random seed, then it is not a very good nor generalizable algorithm/pipeline.

Do you have any links for further reading? I think it is a very interesting question if an estimator, that has proven good during CV with a certain random state is not good at generalization. Because I thought we use CV to achieve exactly this.",think general consensus community performance highly dependent random seed good generalizable link reading think interesting question estimator proven good certain random state good generalization thought use achieve exactly,issue,positive,positive,positive,positive,positive,positive
336635151,"Yep, it is similar, check this [link](http://rhiever.github.io/tpot/contributing/) for more details.",yep similar check link,issue,negative,neutral,neutral,neutral,neutral,neutral
336635064,"One way to find good pipelines faster at the beginning may be to sample sane default parameter settings with a higher probability. http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization gives a good intro here. One may use a gaussian distribution with the median set to default parameter settings. 
For example if SVC parameter ""degree""s default is 3, the TPOT default config could look like this:
```
'sklearn.svm.LinearSVC': {
    'degree': scipy.stats.norm(3),
},
```",one way find good faster beginning may sample sane default parameter higher probability good one may use distribution median set default parameter example parameter degree default default could look like,issue,positive,positive,positive,positive,positive,positive
336633769,"sklearn has a nice introductory section about how to contribute. I think it is similar for TPOT:
http://scikit-learn.org/stable/developers/contributing.html#contributing-code
",nice introductory section contribute think similar,issue,negative,positive,positive,positive,positive,positive
336630988,"This new function was already merged to TPOT 0.9, so I closed this issue.",new function already closed issue,issue,negative,positive,neutral,neutral,positive,positive
336630821,@matthewwritter thank you. Please submit your contribution via a PR based on development branch.,thank please submit contribution via based development branch,issue,positive,neutral,neutral,neutral,neutral,neutral
336629636,"I'm a TPOT user, though new to contributing to OSS. This seems like a manageable way to dip my toe in. Could I give it a shot?",user though new like manageable way dip toe could give shot,issue,negative,positive,positive,positive,positive,positive
336228476,"@Damian89, check the [development branch of TPOT](https://github.com/rhiever/tpot/tree/development). It now has support for a scoring function as you described.",check development branch support scoring function,issue,negative,neutral,neutral,neutral,neutral,neutral
336212891,"Thank you @rhiever .You're right, I put it under TPOT-specific by mistake.",thank right put mistake,issue,negative,positive,positive,positive,positive,positive
336210916,"This sounds like a good question for StackOverflow and/or [CrossValidated](https://stats.stackexchange.com/), as it's not necessarily TPOT-specific. It's difficult to provide specific advice as we don't know the precise details of your problem, but your approach sounds like a reasonable step toward solving the problem.",like good question necessarily difficult provide specific advice know precise problem approach like reasonable step toward problem,issue,negative,positive,positive,positive,positive,positive
336209270,@weixuanfu Thanks again for your help. I realized that the score error may come from the data bias (a ton of 0 in the Y). Therefore I have another question here: https://github.com/rhiever/tpot/issues/598. Would you help me take a look at it? Thanks!,thanks help score error may come data bias ton therefore another question would help take look thanks,issue,positive,positive,positive,positive,positive,positive
336127739,"When you say that you want to ignore but not delete those rows, do you mean you just want to exclude them from the analysis? If so, you can subset your dataset as follows:

```Python
X = X[y != 0.]
y = y[y != 0.]

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    train_size=0.75, test_size=0.25)
tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_y.py')
```",say want ignore delete mean want exclude analysis subset python print,issue,negative,negative,negative,negative,negative,negative
335930942,"@rhiever @weixuanfu. I think i got all your feedback integrated now, please let me know if there are some more modifications i can make :).",think got feedback please let know make,issue,negative,neutral,neutral,neutral,neutral,neutral
335922732,"The default CV score in TPOT was based on [mean_squared_error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error). I cannot tell whether it is reasonable without knowing details of your data, like sample size and how bias the dataset is. You may try to tune `subsample` parameter in TPOT to solve this overfitting issue. ",default score based tell whether reasonable without knowing data like sample size bias may try tune subsample parameter solve issue,issue,negative,positive,positive,positive,positive,positive
335921088,"Thank you! It is a regression. Any suggestion on how to solve the overfitting issue? Thanks!

Another questioon, regression again, best internal CV score is bigger than 1 (1.8, 2.5....) - is it reasonable? Thanks a lot!",thank regression suggestion solve issue thanks another regression best internal score bigger reasonable thanks lot,issue,positive,positive,positive,positive,positive,positive
335919337,"Hmm, for regression problem, R^2 score may be negative (it need not actually be the square of a quantity R). Check this [link](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score) for more details. Maybe there is a issue about overfitting in training dataset. ",regression problem score may negative need actually square quantity check link maybe issue training,issue,negative,negative,negative,negative,negative,negative
335896337,"@bartdp1 you may try the git commands below for updating these expensive unit tests to fix this issue.

```
git cherry-pick 68e42f2136ec43200964d355c5c68288af85e7f5
git cherry-pick 9b428d73e5724ecb61bebc40f69b8628ecc2474a
```",may try git expensive unit fix issue git git,issue,negative,negative,negative,negative,negative,negative
335895005,"@rhiever @weixuanfu. Seems like the PR times out on some unit test. Found issue #588, which adresses the same problem. How can i best proceed with integrating the fix? Copying it and comitting it to the PR? Thanks in advance",like time unit test found issue problem best proceed fix thanks advance,issue,positive,positive,positive,positive,positive,positive
335889209,Update: update 2 time-consuming unit tests for passing CI in python 2.7 (issue #588 ),update update unit passing python issue,issue,negative,neutral,neutral,neutral,neutral,neutral
335661816,"Another useful way is to detect categorical types on pandas dataframes. Of course, this should go along with a good documentation about the behaviour.

Also, there could be a parameter indicating column indices or names of categorical values features.",another useful way detect categorical course go along good documentation behaviour also could parameter column index categorical,issue,positive,positive,positive,positive,positive,positive
335503563,Closing this issue. Please feel free to re-open or file a new issue if you have any further questions or comments.,issue please feel free file new issue,issue,positive,positive,positive,positive,positive,positive
335463851,"I found ""Segmentation fault: 11"" in the log, so I thought it might be a memory issue. But I could not reproduce this issue in my Macbook Air with only 8Gb memory. Maybe [building a new python environment with conda](https://conda.io/docs/user-guide/tasks/manage-environments.html) will help. If not,  could you please let us know more details of the environment with the codes below?
```
python -c ""import numpy; print('numpy %s' % numpy.__version__)""
python -c ""import scipy; print('scipy %s' % scipy.__version__)""
python -c ""import sklearn; print('sklearn %s' % sklearn.__version__)""
python -c ""import deap; print('deap %s' % deap.__version__)""
python -c ""import xgboost; print('xgboost %s ' % xgboost.__version__)""
python -c ""import update_checker; print('update_checker %s ' % update_checker.__version__)""
python -c ""import tqdm; print('tqdm %s' % tqdm.__version__)""
python -c ""import pandas; print('pandas %s' % pandas.__version__)""
```
",found segmentation fault log thought might memory issue could reproduce issue air memory maybe building new python environment help could please let u know environment python import print python import print python import print python import print python import print python import print python import print python import print,issue,negative,positive,positive,positive,positive,positive
335186570,Thanks so much! That fixes it indeed (I just changed the scoring function to return pearson_r**2).  My next challenge is to get the score above 0.15 .,thanks much indeed scoring function return next challenge get score,issue,negative,positive,positive,positive,positive,positive
335185120,"Great, we have much more unit tests in version 0.9 than 0.8. Maybe they need more resources than before and also I think we need refine some unit tests and make them not use too much resources. ",great much unit version maybe need also think need refine unit make use much,issue,positive,positive,positive,positive,positive,positive
335179793,"I think the issue here is the following:

>Define loss function to be between 0 and 1 where 0 is the best and 1 is the worst for optimisation.

Currently, TPOT assumes that any custom scoring function is to be maximized (i.e., 1 is best and 0 is worst) *unless* it has `loss` or `error` in the name. Thus, I would simply keep everything the same but change `return 1-pearson_r**2` to `return pearson_r**2`.

As @weixuanfu mentioned, we have some `scoring` API changes in the works, but this issue can be resolved in the latest release without any TPOT code changes.",think issue following define loss function best worst currently custom scoring function best worst unless loss error name thus would simply keep everything change return return scoring work issue resolved latest release without code,issue,negative,positive,neutral,neutral,positive,positive
335176381,"We are working on a new API for scoring function in TPOT related to the issue #579. For now, could you please try to put 'loss' or 'error' into the scoring function's name to make `greater_is_better` is False in `make_scorer` function (from [this line](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L308)).",working new scoring function related issue could please try put scoring function name make false function line,issue,negative,negative,neutral,neutral,negative,negative
335165659,The 0.9 release added sparse matrix support via the `TPOT sparse` configuration.,release added sparse matrix support via sparse configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
335164303,"It looks like this could be supported if we changed how TPOT takes custom functions as input. Currently we assume that all functions being passed to `scoring` will have only `y_true` and `y_pred` as the parameters: [link](https://github.com/rhiever/tpot/blob/ac7f18cd3f33cc4b5e8318132a2ed22bd4d31122/tpot/base.py#L306)

but that whole functionality passes the user-provided scoring function to the `make_scorer` function, which makes a function with the signature `my_scorer(clf, X, y)`.

Thus, we should change the processing of the `scoring` input to allow a function with the signature `my_scorer(clf, X, y)` as well. I'll change this issue to an enhancement issue and endeavor to have this change out in the next release.

Sorry for the long time to response. I didn't get a notification about this issue for some reason.",like could custom input currently assume scoring link whole functionality scoring function function function signature thus change scoring input allow function signature well change issue enhancement issue endeavor change next release sorry long time response get notification issue reason,issue,positive,negative,neutral,neutral,negative,negative
335161477,Fair enough. I've filed #589 so we have it on record that we should add that note for the next release. Sorry for the misunderstanding and thank you for the feedback! Please feel free to re-open this issue if you have further comments or questions.,fair enough record add note next release sorry misunderstanding thank feedback please feel free issue,issue,positive,positive,positive,positive,positive,positive
335014857,"Well I'm not using a single split in production but only for testing purposes for the same reasons. Although it would be more comfortable to have a parameter for re-fitting, I'm fine with TPOT handling it this way. All I wanted to say was that the user should be informed about how TPOT handles the data in the fit-function and that the documentation may be the right place for this. ",well single split production testing although would comfortable parameter fine handling way say user informed data documentation may right place,issue,positive,positive,positive,positive,positive,positive
335006840,"Oh, I see what's going on. We coded TPOT with the assumption that only training data would be passed to `fit`, and that the holdout data would only be seen by TPOT through the `predict` and `score` functions.

We hadn't considered this use case because using such a setup is highly prone to overfitting in our experience. We found early on that using a single train/test split during the optimization process is overfitting-prone, and thus adopted a k-fold CV procedure instead, which helped quite a bit.

Off the top of my head, I'm not sure of an elegant way to handle this use case. It would require some hard-coded logic to handle the single train/test split use case.",oh see going assumption training data would fit holdout data would seen predict score considered use case setup highly prone experience found early single split optimization process thus adopted procedure instead quite bit top head sure elegant way handle use case would require logic handle single split use case,issue,positive,positive,positive,positive,positive,positive
335005872,"Thanks, I agree that this is a standard procedure, but whether the model is re-fit after CV depends on the implementation. For example sklearn has a parameter ""refit"" in GridSearchCV() that controls this behavior. ",thanks agree standard procedure whether model implementation example parameter refit behavior,issue,positive,positive,neutral,neutral,positive,positive
335005209,It is standard procedure in ML to explicitly make a training set that is non-overlapping with the holdout set. We provide [a few examples](http://rhiever.github.io/tpot/examples/) in the docs that use the recommended `train_test_split` function to accomplish that. Hope that helps!,standard procedure explicitly make training set holdout set provide use function accomplish hope,issue,positive,neutral,neutral,neutral,neutral,neutral
335001102,"I see, this is what I meant with my comment. I think it would be helpful to mention this in the docs to TPOT.fit().",see meant comment think would helpful mention,issue,negative,neutral,neutral,neutral,neutral,neutral
334951029,@hristog thank you. Please submit a PR for this example.,thank please submit example,issue,positive,neutral,neutral,neutral,neutral,neutral
334948241,"Hi @rhiever. I would like to work on this, please.",hi would like work please,issue,positive,neutral,neutral,neutral,neutral,neutral
334927038,"I still did not figure out why this freezing happened in CI. I cannot reproduce it in my environments with python 2.7 in macOS, Linux and Windows. It is very odd.

 ",still figure freezing reproduce python odd,issue,negative,negative,negative,negative,negative,negative
334867312,Failed for Python 2.7 in Linux too but freezing in a different unit test. (check [this](https://travis-ci.org/rhiever/tpot/jobs/284386435)). I will look into this.,python freezing different unit test check look,issue,negative,neutral,neutral,neutral,neutral,neutral
334850625,"I see. This is a pretty smart way to track lineage and related stats about each individual. I suppose since we don't really allow the same pipeline to be generated via crossover/mutation once it's been previously evaluated (barring the situation where a crossover/mutation can't generate an entirely new individual within 10 attempts), there's really no need to maintain pointers from every offspring to its parents.

Well done and thank you for the PR, @bartdp1. I'll allow @weixuanfu some time to review this PR as well. I've added my comments.",see pretty smart way track lineage related individual suppose since really allow pipeline via previously barring situation ca generate entirely new individual within really need maintain every offspring well done thank allow time review well added,issue,positive,positive,neutral,neutral,positive,positive
334582237,"Update:
Added `memory` parameter in TPOTbase
Added 6 unit tests for this function
Also added `memory` option into CLI
Updated docs in both `api.md` and `using.md`",update added memory parameter added unit function also added memory option,issue,negative,neutral,neutral,neutral,neutral,neutral
334481348,I will make an attempt and report on the progress :).,make attempt report progress,issue,negative,neutral,neutral,neutral,neutral,neutral
334480063,"I'd be happy to review a PR to the `development` branch with the proposed changes, and we can evaluate the overhead from there.

One note on implementation: Probably better to change the values stored in `evaluated_individuals_` from a tuple to a dictionary. That change will make it easier to query specific values from the `evaluated_individuals_` dictionary.",happy review development branch evaluate overhead one note implementation probably better change dictionary change make easier query specific dictionary,issue,positive,positive,positive,positive,positive,positive
334476502,"@rhiever Thanks for your comment. I think it would only be necessary to keep the string representation of the direct predecessor (or predecessors in case of a mating operation) for each individuals. Users can then choose to build the complete 'family tree' themselves after running TPOT using the direct predecessor links. As for the statistics regarding mutation and mating operations, these could be increased incrementally when then the operations occur (e.g. taking the current number of mutations and adding 1). Personally, i don't think that this should significantly increase the computation/memory required to run the algorithm, as opposed to the current creation of   'evaluated_individuals_'.",thanks comment think would necessary keep string representation direct predecessor case mating operation choose build complete tree running direct predecessor link statistic regarding mutation mating could occur taking current number personally think significantly increase run algorithm opposed current creation,issue,positive,positive,neutral,neutral,positive,positive
334473828,"Thank you for your interest in TPOT, @bartdp1. This is an interesting feature that could be useful to TPOT power users and researchers. My fear for this feature is that it would have considerable overhead for the non-research version of TPOT, both in terms of memory/computation and in terms of code integration.

For example, to track the list of predecessors, we would need to store an additional object in TPOT that tracks the ancestry of every individual, which can grow quite large. Then to store the ancestry for one individual, we would need to traverse down that ancestry tree and store a list of strings for every evaluated individual.

If this feature is proposed for a research version of TPOT for a research project, I don't think I would have these reservations.

Perhaps @weixuanfu can add some thoughts about the feature?",thank interest interesting feature could useful power fear feature would considerable overhead version code integration example track list would need store additional object ancestry every individual grow quite large store ancestry one individual would need traverse ancestry tree store list every individual feature research version research project think would perhaps add feature,issue,positive,positive,positive,positive,positive,positive
334472864,"Alright, thank you :) good to know for sure problems have been fixed",alright thank good know sure fixed,issue,positive,positive,positive,positive,positive,positive
334471574,"@PG-TUe Thank you for these findings. I agree that RECIPE paper claims are true for old version of TPOT. I think the invalid hyperparameter combinations was fixed in the verison 0.5 (e.g. [Logistic Regression](https://github.com/rhiever/tpot/blob/v0.5/tpot/operators/classifiers/logistic_regression.py#L52-L53) ) even without `pre_test` decorator but not in the [version 0.4](https://github.com/rhiever/tpot/tree/0.4).
Also the root of GP tree as you noticed was added since version 0.5 ([here](https://github.com/rhiever/tpot/blob/v0.5/tpot/tpot.py#L158-L166)).

",thank agree recipe paper true old version think invalid fixed logistic regression even without decorator version also root tree added since version,issue,positive,positive,positive,positive,positive,positive
334463870,Closed until we get this prototype working.,closed get prototype working,issue,negative,negative,neutral,neutral,negative,negative
334455228,"Even if the speedup for regular TPOT is fairly minor with this `memory` option, I figure it doesn't hurt to add it as a new parameter.

In this PR, let's make sure to:

* Update the docstrings
* Update the docs (both API and a new `Using` section on how to use the new `memory` option)
* Add a few unit tests",even regular fairly minor memory option figure hurt add new parameter let make sure update update new section use new memory option add unit,issue,negative,positive,positive,positive,positive,positive
334452392,"Looking at your code, there seems to be a small issue in it. It looks like you provide the entire dataset for training:

```Python
model.fit(train_matrix, target)
```

then validate on a subset of that dataset:

```Python
pred = model.predict(train_matrix[valid_split])
print(f'valid mean precision score: {prc(target[valid_split], pred, average=""macro""):.2f}')
```

After choosing the best pipeline(s), TPOT trains the best pipeline(s) on the training data that was provided to `fit`. Thus, if you've provided the entire dataset to TPOT in `fit`, the best pipeline has already trained on a validation data as well.

You will need to subset `train_matrix` and `target` in the `fit` call above such that you are passing only the training subset to TPOT.",looking code small issue like provide entire training python target validate subset python print mean precision score target macro choosing best pipeline best pipeline training data provided fit thus provided entire fit best pipeline already trained validation data well need subset target fit call passing training subset,issue,positive,positive,positive,positive,positive,positive
334451119,"I filed an issue suggesting to add such an example (#584). We're currently seeking contributors for the issue.

Will close this issue for now. Please feel free to re-open the issue if you have more questions.",issue suggesting add example currently seeking issue close issue please feel free issue,issue,positive,positive,positive,positive,positive,positive
334450633,I don't think this is mentioned in the sklearn docs. We implemented this feature ourselves within the existing sklearn pipeline framework.,think feature within pipeline framework,issue,negative,neutral,neutral,neutral,neutral,neutral
334381675,"PS: I assume, that TPOT doesn't re-fit on the whole set (train and valid) after CV. If it does, that would explain the differing scores. ",assume whole set train valid would explain,issue,negative,positive,positive,positive,positive,positive
334348042,Ok thanks. I will try that. Would it be possible to add a time series example in the documentation. Because afterall Time Series prediction does make use of Regressions,thanks try would possible add time series example documentation time series prediction make use,issue,negative,positive,neutral,neutral,positive,positive
334208898,"Nice, a feedback classifier. Is it somewhere mentioned in the sklearn docs, I couldn't find anything about this?",nice feedback classifier somewhere could find anything,issue,negative,positive,positive,positive,positive,positive
334166568,"When you say ""generated pipeline,"" do you mean the pipeline code generated via the `export` function?

Otherwise if by ""generated pipeline"" you mean the ""best"" pipeline discovered during the optimization process, then you can simply use the `score` function and provide it with the test data and labels.

>And also how to handle Time series using TPOT ( any time series examples making use of TPOT available?)

TPOT would handle time series the same way that scikit-learn does. You can use [TimeSeriesSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) for the `cv` parameter.",say pipeline mean pipeline code via export function otherwise pipeline mean best pipeline discovered optimization process simply use score function provide test data also handle time series time series making use available would handle time series way use parameter,issue,positive,positive,positive,positive,positive,positive
334165331,"The `FunctionTransformer(copy)` object allows for a basic form of stacking when a classifier is present in the middle of a pipeline. `FunctionTransformer(copy)` makes a copy of the entire dataset, and that is merged with the predictions of a classifier on that dataset.",copy object basic form classifier present middle pipeline copy copy entire classifier,issue,negative,neutral,neutral,neutral,neutral,neutral
334134106,"Thanks @weixuanfu for the speedy response.

If I add the 
```
import multiprocessing
multiprocessing.set_start_method('forkserver')
```
lines it seems to be working. Otherwise it utilises all cores to 100%. After some time the CPU consumption per core drops to zero with no observable progress. Memory pressure does not see to be a problem.

Using python 3.6.0 in the virtualenv on Mac Os Sierra.

Please let me know if you need further information.",thanks speedy response add import working otherwise time consumption per core zero observable progress memory pressure see problem python mac o sierra please let know need information,issue,negative,positive,positive,positive,positive,positive
334125827,"@jaksmid did 0.9 version freezes with `forkserver` start methods. Or maybe it is be a memory issue with a large number of `n_jobs`. Could you please provide more details about the issue in your environment? 

The reason why I closed that PR is that it did not save computation time with n_jobs > 1 in my tests.",version start maybe memory issue large number could please provide issue environment reason closed save computation time,issue,positive,positive,neutral,neutral,positive,positive
334122793,"The PR branch worked from me for the big dataset ( approx 600 MB). The 0.8/0.9 branch freezes.
Could we reinitiate discussion for the permanent fix? It seems the PR was closed without merging.",branch worked big branch could reinitiate discussion permanent fix closed without,issue,negative,negative,neutral,neutral,negative,negative
333604510,Awesome. Let's get this implemented on the dev branch and get to testing it within TPOT. Maybe we can release it as a minor release this week.,awesome let get dev branch get testing within maybe release minor release week,issue,positive,positive,positive,positive,positive,positive
333579898,"I can give you one of my current examples (written today, working perfectly with keras and sklearns GridSearchCV:

```Python
grid = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    n_jobs=-1,
    cv=ShuffleSplit(n_splits=1,test_size=1, random_state=42),
    scoring=my_scorer`
)
```

There's my custom function ""my_scorer"":

```Python
def my_scorer(clf, X, y):
    y_predicted_probability = clf.predict_proba(X_validation)
    # Calculation with other (global) data
    # Check probability against known y_validation, calculating logloss...
    return some_new_float
```

As you can see in my case I dont need X andy from the cross validation dataset, I have my own X_validation which currently is a global array. Thats fine.
What I need TPOT to do is to use the current classifier/model and test it against my own global validation set (calculating some metrics and also doing some heavywork writing and comparing files created using the current clf and based on this comparision results the clf is weighted)

Maybe theres another way to accomplish my goal, I didnt test croll_val_score but it looks like it also possible to use a custom callable with the following params:
scorer(estimator, X, y)
See: [http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html](url)",give one current written today working perfectly python grid custom function python calculation global data check probability known calculating return see case dont need cross validation currently global array thats fine need use current test global validation set calculating metric also writing current based weighted maybe there another way accomplish goal didnt test like also possible use custom callable following scorer estimator see,issue,positive,positive,positive,positive,positive,positive
333572460,"Relief works too!
```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.decomposition import PCA, NMF
from sklearn.feature_selection import SelectKBest, chi2, RFE
from skrebate import ReliefF
from sklearn.ensemble import ExtraTreesClassifier

from tempfile import mkdtemp
from shutil import rmtree
from sklearn.externals.joblib import Memory

# Create a temporary folder to store the transformers of the pipeline
cachedir = mkdtemp()
memory = Memory(cachedir=cachedir, verbose=10)

cached_pipe = Pipeline([('reduce_dim', ReliefF()),
                        ('classify', LinearSVC())],
                       memory=memory)

N_FEATURES_OPTIONS = [2, 4, 8]
C_OPTIONS = [1, 5, 10, 100, 1000]
param_grid = [
    {
        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,
        'classify__C': C_OPTIONS
    },
]

# This time, a cached pipeline will be used within the grid search
grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid, verbose=3)
digits = load_digits()
grid.fit(digits.data, digits.target)


cached_pipe = Pipeline([('reduce_dim', ReliefF()),
                        ('classify', ExtraTreesClassifier(n_estimators=100))],
                       memory=memory)

N_FEATURES_OPTIONS = [2, 4, 8]
param_grid = [
    {
        'reduce_dim__n_features_to_select': N_FEATURES_OPTIONS,
        'classify__max_depth': [1, 3, 5, 10]
    },
]

# This time, a cached pipeline will be used within the grid search
print(""A cached pipeline will be used within the grid search"")
grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid, verbose=3)
digits = load_digits()
grid.fit(digits.data, digits.target)



# Delete the temporary cache before exiting
rmtree(cachedir)
```",relief work import import import import import pipeline import import import chi import import import import import memory create temporary folder store pipeline memory memory pipeline time pipeline used within grid search grid pipeline time pipeline used within grid search print pipeline used within grid search grid delete temporary cache,issue,positive,neutral,neutral,neutral,neutral,neutral
333568821,"The demo below is for testing both stacking and combine op:

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_digits
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.svm import LinearSVC
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import FunctionTransformer
from tempfile import mkdtemp
from shutil import rmtree
from sklearn.externals.joblib import Memory
from copy import copy
from tpot.builtins import StackingEstimator

# Create a temporary folder to store the transformers of the pipeline
cachedir = mkdtemp()
memory = Memory(cachedir=cachedir, verbose=10)

cached_pipe = make_pipeline(
    make_union(
        StackingEstimator(estimator=ExtraTreesClassifier(n_estimators=500)),
        FunctionTransformer(copy)
    ),
    LinearSVC(),
    memory=memory
)


N_FEATURES_OPTIONS = [2, 4, 8]
C_OPTIONS = [1, 5, 10, 100, 1000]
param_grid = [
    {
        'featureunion__stackingestimator__estimator__max_features': N_FEATURES_OPTIONS,
        'linearsvc__C': C_OPTIONS
    },
]

# This time, a cached pipeline will be used within the grid search
grid = GridSearchCV(cached_pipe, cv=3, n_jobs=1, param_grid=param_grid, verbose=2)
digits = load_digits()
grid.fit(digits.data, digits.target)

cached_pipe_2 = make_pipeline(
    make_union(
        StackingEstimator(estimator=ExtraTreesClassifier(n_estimators=500)),
        FunctionTransformer(copy)
    ),
    ExtraTreesClassifier(n_estimators=100),
    memory=memory
)

param_grid = [
    {
        'featureunion__stackingestimator__estimator__max_features': N_FEATURES_OPTIONS,
        'extratreesclassifier__max_depth': [1, 3, 5, 10]
    },
]

# This time, a cached pipeline will be used within the grid search
print(""A cached pipeline will be used within the grid search"")
grid = GridSearchCV(cached_pipe_2, cv=3, n_jobs=1, param_grid=param_grid, verbose=2)
digits = load_digits()
grid.fit(digits.data, digits.target)



# Delete the temporary cache before exiting
rmtree(cachedir)
```",testing combine import import import import import import import import chi import import import import import memory copy import copy import create temporary folder store pipeline memory memory copy time pipeline used within grid search grid copy time pipeline used within grid search print pipeline used within grid search grid delete temporary cache,issue,negative,neutral,neutral,neutral,neutral,neutral
333543642,"Hi @Damian89, can you please provide a code example of what this would look like if you were to pass such as a custom scoring function directly to sklearn's `cross_val_score` function? Essentially, we pass the contents of TPOT's `scoring` parameter directly to the `cross_val_score` function. Thus if it's possible to pass such a function to `cross_val_score`, then it's possible to pass it to TPOT's `scoring` parameter.",hi please provide code example would look like pas custom scoring function directly function essentially pas content scoring parameter directly function thus possible pas function possible pas scoring parameter,issue,positive,positive,neutral,neutral,positive,positive
333541701,"Need to test if pipeline caching works with:

- [x] stacking
- [x] combine op
- [x] relief algorithms.",need test pipeline work combine relief,issue,negative,neutral,neutral,neutral,neutral,neutral
332182959,"For clarifying the issues, I close this PR and will submit two separate PRs soon.",close submit two separate soon,issue,negative,neutral,neutral,neutral,neutral,neutral
331175709,"As TPOT is based on scikit-learn, it supports large-scale ML about as well as scikit-learn does (i.e., not great). My recommendation is to look into ML packages based on TensorFlow and/or that have GPU support to scale ML to that size of data.",based well great recommendation look based support scale size data,issue,positive,positive,positive,positive,positive,positive
331076664,"It might be extremely slow to run TPOT in such a big dataset. For the case, I highly suggest to:

1. Set`n_jobs=1` for TPOT object and make sure there are enough RAM to avoid memory issue;
2. Use [`TPOT light`](http://rhiever.github.io/tpot/using/#built-in-tpot-configurations) configuration;
3. Apply [MDR](https://en.m.wikipedia.org/wiki/Multifactor_dimensionality_reduction) or other features selection/dimension reduction algorithms for reducing feature numbers before using TPOT.

@rhiever ",might extremely slow run big case highly suggest set object make sure enough ram avoid memory issue use light configuration apply reduction reducing feature,issue,negative,positive,positive,positive,positive,positive
330954959,"Thanks! Increasing the population size will be pretty important for TPOT performance; if I had to choose, I would take 100 population and 10 generations instead of 10 pop/100 gen. Of course, 100/100 would be even better.",thanks increasing population size pretty important performance choose would take population instead course would even better,issue,positive,positive,positive,positive,positive,positive
330953685,"My code is above. 100 generations, 10 population size, but not a huge training set. 1500 observations for training, 500 for validation.",code population size huge training set training validation,issue,negative,positive,positive,positive,positive,positive
330945600,"That's possible. It's also possible that TPOT didn't have enough iterations to prune down the pipeline to get rid of the superfluous ZeroCount operators. For example, if TPOT was run only for 5 generations or so.",possible also possible enough prune pipeline get rid superfluous example run,issue,negative,neutral,neutral,neutral,neutral,neutral
330873417,"Hi. I did try removing the ZeroCount sequentially. There test metrics are a bit noisy, but I do not see any discernable improvement in performance with each additional ZeroCount(). I think it may be getting lucky with the noise at each step, tricking it into adding ZeroCount preprocessors.
",hi try removing sequentially test metric bit noisy see improvement performance additional think may getting lucky noise step tricking,issue,positive,positive,positive,positive,positive,positive
330591357,"Hmm, thank you for finding this bug. We will fix it soon. ",thank finding bug fix soon,issue,negative,neutral,neutral,neutral,neutral,neutral
330197445,"Multiprocessing simply doesn't work in Windows with Python, so we had to drop support for it.",simply work python drop support,issue,negative,neutral,neutral,neutral,neutral,neutral
330024708,"I have the exact same issue, running on Windows. Even with the below params running on the tiny Titanic dataset (100's of rows), the optimizer simply never makes progress. 

`model = tp.TPOTClassifier(generations=1,
    population_size=1,
    cv=5,
    verbosity=2,
    n_jobs=8,
    config_dict=config_dict)`

`Optimization Progress:   0%|          | 0/2 [00:00<?, ?pipeline/s]`

That said, CPU usage is around 100% and python processes are constantly getting spun up and torn down, but no progress is made. `n_jobs=1` works as expected (< 15 sec).

Have any of the devs tried multiprocessing on a Windows machine? I suspect it just doesn't work.

```
multiprocessing.cpu_count() == 12

            platform : win-64
       conda version : 4.3.25
    conda is private : False
   conda-env version : 4.3.25
 conda-build version : 3.0.14
      python version : 3.5.4.final.0
    requests version : 2.13.0
                TPOT : 0.8.3
               numpy : 1.12.1
               scipy : 0.19.1
        scikit-learn : 0.19.0
                deap : 1.0.2
```",exact issue running even running tiny titanic simply never progress model optimization progress said usage around python constantly getting spun torn progress made work sec tried machine suspect work platform version private false version version python version final version,issue,negative,negative,neutral,neutral,negative,negative
329596684,Support for sparse matrices would be great!,support sparse matrix would great,issue,positive,positive,positive,positive,positive,positive
329581516,"@dah33, have you tried sequentially moving the ZeroCount operators to see how it affects the prediction accuracy? The optimization procedure has a pressure to maximize prediction accuracy while minimizing the number of operators in the pipeline, so the ZeroCount operators should only be there if they improved the 10-fold CV score.

How many generations and what population size did you use for this run?",dah tried sequentially moving see prediction accuracy optimization procedure pressure maximize prediction accuracy number pipeline score many population size use run,issue,negative,positive,positive,positive,positive,positive
328905232,You're welcome. Good to know that you get good result. ,welcome good know get good result,issue,positive,positive,positive,positive,positive,positive
328871060,"Awesome... Thank you very much.
Both methods worked even without using train_test_split but as:

X_train, X_test, y_train, y_test = np.array(df_X.iloc[0:800].values),  np.array(df_X.iloc[800:1150].values), np.array((df_Y.iloc[0:800].values).ravel()), np.array((df_Y.iloc[800:1150].values).ravel())
 
Thank you so much. The results are really promising",awesome thank much worked even without thank much really promising,issue,positive,positive,positive,positive,positive,positive
328847646,Could you please also add another class called `TPOTCluster` besides `TPOTClassifier` and `TPOTRegessor` for clustering problem?,could please also add another class besides clustering problem,issue,negative,neutral,neutral,neutral,neutral,neutral
328846238,Thank you for reporting this issue. I think we need add a rule in TPOT grammar to collapse repeated preprocessor operators in a pipeline. @rhiever ,thank issue think need add rule grammar collapse repeated pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
328845040,"Below are my codes for reproducing the results:

- Training by TPOTClassifier
```
import numpy as np
import pandas as pd
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

df_X=pd.read_csv('X.csv', index_col=False)
df_Y=pd.read_csv('Y.csv', index_col=False)

X_train, X_test, y_train, y_test = train_test_split(np.array(df_X.values), np.array(df_Y.values).ravel(),
                                                    train_size=800/1150, test_size=350/1150, random_state=42)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring= 'roc_auc', random_state=42, config_dict='TPOT light')
tpot.fit(X_train, y_train)
tpot.export('tpot_dorothea_auc_test.py')

print(tpot.score(X_train, y_train))
```
- Best pipeline: Best pipeline: BernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=True)

- Codes for reproducing the result:
```
import numpy as np
import pandas as pd
from sklearn.naive_bayes import BernoulliNB
from sklearn.metrics import roc_auc_score
from sklearn.metrics import SCORERS
from sklearn.model_selection import train_test_split
exported_pipeline = BernoulliNB(alpha=0.1, fit_prior=True)

df_X=pd.read_csv('X.csv', index_col=False)
df_Y=pd.read_csv('Y.csv', index_col=False)

# train_test_split can generate the same train/test split with the same random state (42)
X_train, X_test, y_train, y_test = train_test_split(np.array(df_X.values), np.array(df_Y.values).ravel(),
                                                    train_size=800/1150, test_size=350/1150, random_state=42)

# method 1
exported_pipeline.fit(X_train, y_train)
y_scores = exported_pipeline.predict_proba(X_train)

print('Method 1',roc_auc_score(y_train, y_scores[:,1]))

# method 2
score_export = SCORERS['roc_auc'](
            exported_pipeline,
            X_train.astype(np.float64),
            y_train.astype(np.float64)
        )

print('Method 2', score_export)

```

Two key points for  reproducing the result:
1. [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) splits arrays or matrices into **random** train and test subsets

2. [roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) only takes target scores (`y_score`) instead of target prediction (`y_pred`) 
",training import import import import import light print best pipeline best pipeline result import import import import import import generate split random state method print method print two key result matrix random train test target instead target prediction,issue,positive,positive,positive,positive,positive,positive
328825945,"@weixuanfu
My apologies sir,
Please add a row at top in Y.csv for column label ""class"".
Once again... thanks a lot",sir please add row top column label class thanks lot,issue,positive,positive,positive,positive,positive,positive
328687799,"@weixuanfu
Here is the challenge: My Optimizing code contains (random_state=42):
tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, scoring= 'roc_auc', **random_state=42**, config_dict='TPOT light')
but the the tpot.export ( 'tpot_dorothea_auc.py' ) has a pipeline consisting of KNeighborsClassifier and BernoulliNB.
Neither of which accept parameter like **random_state**
Attached are two data files files **X.csv** & **Y.csv** (First 800 rows are meant to be for training and last 350 rows are for testing but for train_test_split we could use all 1150 rows)
My code script is **For_Weixuan.py** and the exported script is **tpot_dorothea_auc.py**
All are zipped in **together.zip**

[together.zip](https://github.com/rhiever/tpot/files/1294315/together.zip)


See what best you can do. Thanks-a-million !!
",challenge code light pipeline neither accept parameter like attached two data first meant training last testing could use code script script see best,issue,negative,positive,positive,positive,positive,positive
328672294,"@shirishr ignore the last comment. If you cannot reproduce the results, could you please let me know more details about the datasets and codes? I think another possible reason is about using `train_test_split` in exported code:

```
training_features, testing_features, training_target, testing_target = \
    train_test_split(features, tpot_data['class'], random_state=42)
```

If you use all the data for training TPOTClassifier and the exported codes cannot reproduce the results because of using a subset of dataset.",ignore last comment reproduce could please let know think another possible reason code use data training reproduce subset,issue,negative,neutral,neutral,neutral,neutral,neutral
328665803,"@weixuanfu 
If CV score: 0.9894097222222221 is an average of cross-validation scores from cross_val_score for the scoring function auc_roc as set in the TPOTClassifier...it is even more impressive.
I will run the tpot.export ( 'tpot_dorothea_auc.py' ) with random state =42. Let's see...fingers crossed 😄 ",score average scoring function set even impressive run random state let see crossed,issue,positive,positive,positive,positive,positive,positive
328650223,"Please check the issue #513, I think the issue is about no random seed in exported pipeline. 

During the 5 generations of training & validation (testing), TPOT was reporting average of cross-validation scores from [`cross_val_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score) and the scoring function in `cross_val_score` is `auc_roc` as the scoring setting in `TPOTClassifier`
",please check issue think issue random seed pipeline training validation testing average scoring function scoring setting,issue,negative,negative,negative,negative,negative,negative
328102719,Could you please rebase this PR on dev branch and add a few unit tests about it?,could please rebase dev branch add unit,issue,negative,neutral,neutral,neutral,neutral,neutral
327849027,"Yes sir. I am sorry

El 7 sept. 2017 10:57 a.m., ""Weixuan Fu"" <notifications@github.com>
escribió:

> ? Did this issue create by mistake?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/561#issuecomment-327843950>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AQpzgCcSaOY0fF20s23qn-rhIVXA-bK0ks5sgBJugaJpZM4PP-A_>
> .
>
",yes sir sorry el fu issue create mistake thread reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
327256347,"@rhiever I saw that, but I think maybe the [sklearn link](http://scikit-learn.org/stable/modules/model_evaluation.html) could be a better reference for TPOT documentation about scoring function",saw think maybe link could better reference documentation scoring function,issue,negative,positive,positive,positive,positive,positive
327253378,"You can pass in ['roc_auc'](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) to the `scoring` parameter in `TPOTClassifier` for using AUC. More metrics can be found in this [link](http://scikit-learn.org/stable/modules/model_evaluation.html). I will update [TPOT documentation](https://rhiever.github.io/tpot/using/#scoring-functions) for adding more details about metrics.  
",pas scoring parameter metric found link update documentation metric,issue,negative,neutral,neutral,neutral,neutral,neutral
327245986,"There are two ways to accomplish this task. You can do what @weixuanfu suggested and set TPOT's `n_jobs` parameter, which will parallelize TPOT's pipeline evaluation process. You can also make your own [custom TPOT configuration](https://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) to set all of the `n_jobs` parameters of the individual algorithms. However, I recommend using TPOT's parallelization rather than parallelizing the individual algorithms.",two way accomplish task set parameter parallelize pipeline evaluation process also make custom configuration set individual however recommend parallelization rather individual,issue,positive,neutral,neutral,neutral,neutral,neutral
327237777,Please check [TPOT API](https://rhiever.github.io/tpot/api/). TPOT allows parallel computing during optimization process with `n_jobs` parameter. ,please check parallel optimization process parameter,issue,positive,neutral,neutral,neutral,neutral,neutral
326664556,"It would be nice to eventually incorporate Hyperband as well:

https://github.com/zygmuntz/hyperband
https://people.eecs.berkeley.edu/~kjamieson/hyperband.html",would nice eventually incorporate well,issue,positive,positive,positive,positive,positive,positive
326320328,"Great! To answer that question, it must follow the input/output scheme of the other scikit-learn CV operators. Assuming you want to define your own predefined splits, you can use scikit-learn's [PredefinedSplit](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.PredefinedSplit.html#sklearn.model_selection.PredefinedSplit).",great answer question must follow scheme assuming want define use,issue,positive,positive,positive,positive,positive,positive
326318529,"Yep, got results with LOO, thanks!

Then back to my initial question about custom generator: what format should I use?",yep got loo thanks back initial question custom generator format use,issue,positive,positive,neutral,neutral,positive,positive
326318325,"Okay, the next thing to check is the shape of the data. What is the output of the following?

```Python
print(X_train.shape, y_train.shape)
```

Below is a working example of TPOT applied to the iris multiclass problem with LeaveOneOut cv and `my_score` scoring function.

```Python
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, LeaveOneOut
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

def my_score(y_true, y_pred):
    res = 0
    cl_num = 3
    for cl in range(0, 3):
        ind = y_true == cl
        if len(ind) == 0:
            cl_num -= 1
            continue
        res += (y_true[ind] == y_pred[ind]).sum() / ind.sum()
    return res / cl_num

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2,
                      cv=LeaveOneOut(), scoring=my_score)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```",next thing check shape data output following python print working example applied iris problem scoring function python import import import import iris range continue return print,issue,negative,neutral,neutral,neutral,neutral,neutral
326313950,"I think I spotted a bug in `my_score`: If `0`, `1`, or `2` aren't in `y_true`, then the next line will cause the function to return `nan`. For example, call the following:

```Python
def my_score(y_true, y_pred):
    res = 0
    for cl in range(0, 3):
        ind = y_true == cl
        res += (y_true[ind] == y_pred[ind]).sum() / ind.sum()
    return res / 3

print(my_score(np.array([1,2,3]), np.array([1,2,3])))
```

If you expect classes 0, 1, and 2 to always appear in the labels, it would be a good idea to put a check in the scoring function to make sure they're all there. If it's meant to be a genetic multi-class function, then [TPOT's balanced accuracy function](https://github.com/rhiever/tpot/blob/1b012a9ae5722ba31869ee556054e0f219424885/tpot/metrics.py#L45:L62) can make a good template for a scoring function that works with arbitrary class labels.",think spotted bug next line cause function return nan example call following python range return print expect class always appear would good idea put check scoring function make sure meant genetic function balanced accuracy function make good template scoring function work arbitrary class,issue,positive,positive,positive,positive,positive,positive
326309339,"Here's my function (seems right to me):

```
def my_score(y_true, y_pred):
    res = 0
    for cl in range(0, 3):
        ind = y_true == cl
        res += (y_true[ind] == y_pred[ind]).sum() / ind.sum()
    return res / 3
```",function right range return,issue,negative,positive,positive,positive,positive,positive
326305874,"The error message you're seeing is a generic error message, likely because the `cross_val_score` function used inside TPOT is throwing errors. Looking at the code snippet you shared, the first thing to check is the format of the scoring function `my_score`. Does it follow the format shown in the example from the [TPOT scores section](http://rhiever.github.io/tpot/using/#scoring-functions)?

```Python
def my_custom_accuracy(y_true, y_pred):
    return float(sum(y_pred == y_true)) / len(y_true)
```

Essentially, the function must take `y_true` (array of numbers; the true labels for the data) and `y_pred` (array of numbers; the predicted labels from the pipeline) and return a single float value.",error message seeing generic error message likely function used inside throwing looking code snippet first thing check format scoring function follow format shown example section python return float sum essentially function must take array true data array pipeline return single float value,issue,negative,positive,neutral,neutral,positive,positive
326095461,"@mattvan83 

`pareto_front_fitted_pipelines_` attribute is a python dictionary containing the all pipelines on the TPOT Pareto front, where the key is the string representation of the pipeline and the value is the corresponding pipeline fitted on the entire training dataset. The TPOT Pareto front provides a trade-off between pipeline complexity (i.e., the number of steps in the pipeline) and the predictive performance of the pipeline. 

In the example in the comment above, the key is:
```
'LogisticRegression(DecisionTreeClassifier(input_matrix,
DecisionTreeClassifier__criterion=gini,
DecisionTreeClassifier__max_depth=10,
DecisionTreeClassifier__min_samples_leaf=18,
DecisionTreeClassifier__min_samples_split=3), LogisticRegression__C=10.0,
LogisticRegression__dual=DEFAULT, LogisticRegression__penalty=l1)'
``` 
and its value is:
```
Pipeline(steps=[('stackingestimator',
StackingEstimator(estimator=DecisionTreeClassifier(class_weight=None,
criterion='gini', max_depth=10,            max_features=None,
max_leaf_nodes=None,            min_impurity_split=1e-07,
min_samples_leaf=18,            min_samples_split=3,
min_weight_fraction_leaf=...ty='l1', random_state=None, solver='liblinear',
tol=0.0001,          verbose=0, warm_start=False))])
```
You may use the pipeline object directly for subsequent analysis as a [scikit-learn Pipeline object](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) .

However, the `print` function does not print all details in the pipeline if the pipeline is very complex. For better presentation of the pipeline, you could try the demo codes below:

```
from tpot import TPOTClassifier
from tpot.export_utils import expr_to_tree, generate_pipeline_code
from deap import creator
tpot_obj = TPOTClassifier()
# a demo key of your_TPOT_object.pareto_front_fitted_pipelines_
pipeline_string = (
      'LogisticRegression(DecisionTreeClassifier(input_matrix,'
      'DecisionTreeClassifier__criterion=gini,'
      'DecisionTreeClassifier__max_depth=10,'
      'DecisionTreeClassifier__min_samples_leaf=18,'
      'DecisionTreeClassifier__min_samples_split=3), LogisticRegression__C=10.0,'
      'LogisticRegression__dual=False, LogisticRegression__penalty=l1)'
)
# a deap object
optimized_pipeline = creator.Individual.from_string(pipeline_string, tpot_obj._pset)
# A string for scikit-learn Pipeline
sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(optimized_pipeline, tpot_obj._pset), tpot_obj.operators)
# scikit-learn Pipeline object
sklearn_pipeline_obj = eval(sklearn_pipeline_str, tpot_obj.operators_context)  

print(sklearn_pipeline_str)
``` 
And you also find the `StackingEstimator` transformer in the Pipeline object for stacking prediction from `DecisionTreeClassifier` in the first step of pipeline, and second step is `LogisticRegression`",attribute python dictionary front key string representation pipeline value corresponding pipeline fitted entire training front pipeline complexity number pipeline predictive performance pipeline example comment key value pipeline may use pipeline object directly subsequent analysis pipeline object however print function print pipeline pipeline complex better presentation pipeline could try import import import creator key object string pipeline pipeline object print also find transformer pipeline object prediction first step pipeline second step,issue,positive,positive,neutral,neutral,positive,positive
326081466,"@rhiever please find below the pareto_front_fitted_pipelines_ attribute of
the TPOT object with ""TPOT_light"" configuration:













*{'DecisionTreeClassifier(input_matrix,
DecisionTreeClassifier__criterion=gini,
DecisionTreeClassifier__max_depth=3,
DecisionTreeClassifier__min_samples_leaf=11,
DecisionTreeClassifier__min_samples_split=15)':
Pipeline(steps=[('decisiontreeclassifier',
DecisionTreeClassifier(class_weight=None, criterion='gini',
max_depth=3,            max_features=None, max_leaf_nodes=None,
min_impurity_split=1e-07, min_samples_leaf=11,
min_samples_split=15, min_weight_fraction_leaf=0.0,
presort=False, random_state=None, splitter='best'))]),
'LogisticRegression(DecisionTreeClassifier(input_matrix,
DecisionTreeClassifier__criterion=gini,
DecisionTreeClassifier__max_depth=10,
DecisionTreeClassifier__min_samples_leaf=18,
DecisionTreeClassifier__min_samples_split=3), LogisticRegression__C=10.0,
LogisticRegression__dual=DEFAULT, LogisticRegression__penalty=l1)':
Pipeline(steps=[('stackingestimator',
StackingEstimator(estimator=DecisionTreeClassifier(class_weight=None,
criterion='gini', max_depth=10,            max_features=None,
max_leaf_nodes=None,            min_impurity_split=1e-07,
min_samples_leaf=18,            min_samples_split=3,
min_weight_fraction_leaf=...ty='l1', random_state=None, solver='liblinear',
tol=0.0001,          verbose=0, warm_start=False))]),
'LogisticRegression(VarianceThreshold(DecisionTreeClassifier(input_matrix,
DecisionTreeClassifier__criterion=gini,
DecisionTreeClassifier__max_depth=10,
DecisionTreeClassifier__min_samples_leaf=18,
DecisionTreeClassifier__min_samples_split=3),
VarianceThreshold__threshold=0.7), LogisticRegression__C=10.0,
LogisticRegression__dual=DEFAULT, LogisticRegression__penalty=l1)':
Pipeline(steps=[('stackingestimator',
StackingEstimator(estimator=DecisionTreeClassifier(class_weight=None,
criterion='gini', max_depth=10,            max_features=None,
max_leaf_nodes=None,            min_impurity_split=1e-07,
min_samples_leaf=18,            min_samples_split=3,
min_weight_fraction_leaf=...ty='l1', random_state=None, solver='liblinear',
tol=0.0001,          verbose=0, warm_start=False))])}*

Could you help me understand in it where should I:
1) find the best-discovered pipelines that have a trade-off between
maximizing your performance metric and minimizing the number of steps in
the pipeline ?
2) find a better-performing pipeline with (for example) many preprocessing
steps before a classifier ?

Could you explain me the significance of for example
""*LogisticRegression(DecisionTreeClassifier(input_matrix,
DecisionTreeClassifier__criterion=gini,
DecisionTreeClassifier__max_depth=10,
DecisionTreeClassifier__min_samples_leaf=18,
DecisionTreeClassifier__min_samples_split=3), LogisticRegression__C=10.0,
LogisticRegression__dual=DEFAULT, LogisticRegression__penalty=l1)'"" *where
*LogisticRegression* and *DecisionTreeClassifier* seems to be imbricated ?

Many thanks in advance !

Best regards,
Matthieu


2017-08-29 16:34 GMT+02:00 Randy Olson <notifications@github.com>:

> Oh, I see. It's not possible to avoid that in the current version of TPOT,
> @mattvan83 <https://github.com/mattvan83>. However, you can access the
> pareto_front_fitted_pipelines_ attribute of the TPOT object to see the
> entire Pareto front of pipelines for the problem. In our case, the Pareto
> front shows the best-discovered pipelines that have a trade-off between
> maximizing your performance metric and minimizing the number of steps in
> the pipeline. Thus, you should see a pipeline with only one classifier in
> it, but you should also see a better-performing pipeline with (for example)
> many preprocessing steps before a classifier.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/553#issuecomment-325683776>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AEapKX7yHlzvi2MMSIhg5i-XlB2DuceCks5sdCF7gaJpZM4PE6X5>
> .
>
",please find attribute object configuration pipeline pipeline pipeline could help understand find performance metric number pipeline find pipeline example many classifier could explain significance example imbricated many thanks advance best randy oh see possible avoid current version however access attribute object see entire front problem case front performance metric number pipeline thus see pipeline one classifier also see pipeline example many classifier reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
326007480,"After thinking about it some more, an outer CV loop can already be obtained for TPOT by passing TPOT as the `clf` in the sklearn `cross_val_score` function. Essentially achieves the same as steps 1-4 in my comment above, although I don't think we would be able to inspect the best pipelines at the end as wisely suggested by @crawles.",thinking outer loop already passing function essentially comment although think would able inspect best end wisely,issue,positive,positive,positive,positive,positive,positive
326005081,">'nthread': [1]

This particular line is telling XGBoost not to use its built-in multithreading because it causes issues with TPOT. Until the next major XGBoost release, the default behavior of XGBoost was to use multithreading.",particular line telling use next major release default behavior use,issue,negative,positive,neutral,neutral,positive,positive
325994725,"@rhiever that's exactly how I would envision it. It would be adding an outer cv loop over TPOT.

Something tricky about the approach is in step 4, there could very possibly be k different best pipelines chosen from each run of TPOT. This may be nice though because it would give you a sense of the stability of the chosen model– e.g., if in fact k different model/hyperparameters are chosen and very different it would tell you TPOT hasn't converged. 

You could return all the unique models from the outer cv or if there are less than k models chosen you could do a majority vote as the best model.

And agreed, it would be much more computationally expensive.

",exactly would envision would outer loop something tricky approach step could possibly different best chosen run may nice though would give sense stability chosen fact different chosen different would tell could return unique outer le chosen could majority vote best model agreed would much expensive,issue,positive,positive,positive,positive,positive,positive
325968444,"Thank you for introducing this algorithm. We need check whether this algorithm is suitable before putting it into default configuration in future version of TPOT. @rhiever 

For now, you can use a [custom TPOT configuration](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) to include this algorithms that TPOT optimizes over. You may find some good examples in our [built-in configuration](http://rhiever.github.io/tpot/using/#built-in-tpot-configurations). Below is a demo:

```
import numpy as np
from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25)

tpot_config = {
    'sklearn.preprocessing.Normalizer': {
        'norm': ['l1', 'l2', 'max']
    },

    'pyearth.Earth': {
        'max_degree': [1,2,3],
        'endspan_alpha': np.arange(0.0, 1.01, 0.05)
    }
}

tpot = TPOTRegressor(generations=5, population_size=30, verbosity=2, config_dict=tpot_config)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```",thank algorithm need check whether algorithm suitable default configuration future version use custom configuration include may find good configuration import import import import housing print,issue,positive,positive,positive,positive,positive,positive
325814980,"Got same error message ( colsample_bytree=1 is too small that no feature can be included)
But I was running TPOTClassifier(generations=10, population_size=50, verbosity=2, n_jobs=4) and then noticed that xgboost configuration calls out
    'nthread': [1]
Is this relevant?
Am continuing testing by excluding the Xgboost. Will keep you posted.
",got error message small feature included running configuration relevant testing excluding keep posted,issue,negative,positive,neutral,neutral,positive,positive
325747151,"How would nested CV even be implemented in TPOT? After thinking about it for a bit, to implement nested CV we would need to do the following:

1) initially split the training data into k training folds

2) start up *k* GP populations, where each population corresponds to a training fold group

3) TPOT runs like regular on each training fold group, using the 10-fold CV score on the training fold to select the best pipeline(s)

4) after all *k* populations are finished evolving, use the holdout fold for each population to report the ""true"" best pipeline score to pick the overall best pipeline

It sounds that implementing nested CV in TPOT would be convoluted and much more expensive than the current TPOT process.",would even thinking bit implement would need following initially split training data training start population training fold group like regular training fold group score training fold select best pipeline finished use holdout fold population report true best pipeline score pick overall best pipeline would convoluted much expensive current process,issue,positive,positive,positive,positive,positive,positive
325745402,"Regardless of its effects on bias, it should probably be an optional feature.",regardless effect bias probably optional feature,issue,negative,neutral,neutral,neutral,neutral,neutral
325683776,"Oh, I see. It's not possible to avoid that in the current version of TPOT, @mattvan83. However, you can access the `pareto_front_fitted_pipelines_` attribute of the TPOT object to see the entire Pareto front of pipelines for the problem. In our case, the Pareto front shows the best-discovered pipelines that have a trade-off between maximizing your performance metric and minimizing the number of steps in the pipeline. Thus, you should see a pipeline with only one classifier in it, but you should also see a better-performing pipeline with (for example) many preprocessing steps before a classifier.",oh see possible avoid current version however access attribute object see entire front problem case front performance metric number pipeline thus see pipeline one classifier also see pipeline example many classifier,issue,negative,positive,positive,positive,positive,positive
325682767,"Thank you for raising this issue, @crawles. We have been considering nested CV since the early inception of TPOT, however as you mentioned, the computational cost is much higher. For that reason, we've avoided nested CV because AutoML tools are already slow by using CV.

Perhaps a formal evaluation of TPOT with and without nested CV is in order. It could be good to see if nested CV would lead to different pipelines, or if it doesn't matter at all in AutoML. Thoughts, @weixuanfu @teaearlgraycold?",thank raising issue considering since early inception however computational cost much higher reason already slow perhaps formal evaluation without order could good see would lead different matter,issue,negative,positive,positive,positive,positive,positive
325505564,Unfortunately I haven't invested time in this yet since I don't think it's ready yet. But again tpot needs to remove multiprocessing support if haven't done so. I will try revisit this once Experiment API in TensorFlow is landed in core (currently in `contrib.learn.estimators`). The `LearnRunner` and the `tune()` method will make parameter tuning easier. ,unfortunately time yet since think ready yet need remove support done try revisit experiment landed core currently tune method make parameter tuning easier,issue,positive,negative,neutral,neutral,negative,negative
325503634,"I think we still need grammar for this case, like the number of classifier/regressor = 1 in the configurable grammar. I will look into it.",think still need grammar case like number grammar look,issue,negative,neutral,neutral,neutral,neutral,neutral
325494460,"Am still just getting going with TF and kind of fairly quickly run into the issue of Parameter tuning and model selection (activation, learning rate, regularization and the like..)  Started looking for patterns to follow, and came here hoping... :-) I'm not smart enough to grapple with it but would be happy to try and help once some direction is established. It does seem that the TF. Estimator class is getting the main features and love from Google.. I'm currently working with TF1.3 (and with GPU)",still getting going kind fairly quickly run issue parameter tuning model selection activation learning rate regularization like looking follow came smart enough grapple would happy try help direction established seem estimator class getting main love currently working,issue,positive,positive,positive,positive,positive,positive
325480156,"@rhiever yes I want to avoid stacking of multiple ML algorithms.

Le 28 août 2017 9:26 PM, ""Randy Olson"" <notifications@github.com> a écrit :

> @mattvan83 <https://github.com/mattvan83>, do you mean you want to avoid
> stacking of ML algorithms? For example, a pipeline that performs
> StandardScaler --> LogisticRegression --> SVM?
>
> Or do you simply mean you want to avoid algorithms that are ensemble
> models, such as Random Forest and Gradient Tree Boosting?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/553#issuecomment-325453827>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AEapKX-k5zQ_DLkeQVmecV6FfiaNyYwHks5scxR8gaJpZM4PE6X5>
> .
>
",yes want avoid multiple randy mean want avoid example pipeline simply mean want avoid ensemble random forest gradient tree reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
325453827,"@mattvan83, do you mean you want to avoid stacking of ML algorithms? For example, a pipeline that performs StandardScaler --> LogisticRegression --> SVM?

Or do you simply mean you want to avoid algorithms that are ensemble models, such as Random Forest and Gradient Tree Boosting?",mean want avoid example pipeline simply mean want avoid ensemble random forest gradient tree,issue,negative,negative,negative,negative,negative,negative
325450213,StackingEstimator is meta-transformer for adding predictions and/or class probabilities as synthetic feature(s) (see #278),class synthetic feature see,issue,negative,neutral,neutral,neutral,neutral,neutral
325449880,"@rhiever No I don’t want to suppress preprocessing or normalization but only combination of ML algorithms.

> Le 28 août 2017 à 21:09, Weixuan Fu <notifications@github.com> a écrit :
> 
> @mattvan83 <https://github.com/mattvan83> do you mean that ""ensemble models"" is the pipeline?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/553#issuecomment-325449290>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEapKaiMd-E3_jMjg9js_z6oOQakqZqoks5scxBogaJpZM4PE6X5>.
> 

",want suppress normalization combination fu mean ensemble pipeline reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
325449591,"@Weixuan What StackingEstimator correspond to if ensemble ML algorithms have been deactivated ? 


> Le 28 août 2017 à 21:05, Weixuan Fu <notifications@github.com> a écrit :
> 
> @rhiever <https://github.com/rhiever> Custom configuration is indeed a good option for one algorithm for one TPOT fit(). But the builtin StackingEstimator sometime can build some complex pipeline with even one algorithm.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/553#issuecomment-325448139>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEapKbpR7YQx5-REOEjwtSrigPjLkocvks5scw96gaJpZM4PE6X5>.
> 

",correspond ensemble fu custom configuration indeed good option one algorithm one fit sometime build complex pipeline even one algorithm reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
325448139,@rhiever Custom configuration is indeed a good option for specifying one algorithm in one TPOT fit(). But the builtin `StackingEstimator` sometime can build some complex pipelines with even one algorithm.,custom configuration indeed good option one algorithm one fit sometime build complex even one algorithm,issue,positive,positive,positive,positive,positive,positive
325447809,"Hi,

Thank you for this answer. However, using a custom TPOT configuration needs to specify all ML algorithms wanted to be tested. Wouldn’t it be an option that tests all « simple »  ML algorithms existing ?

TPOT light avoids complex ensemble models, but I would like to simply avoid ensemble models.

> Le 28 août 2017 à 21:00, Randy Olson <notifications@github.com> a écrit :
> 
> Hi @mattvan83 <https://github.com/mattvan83>,
> 
> You can use a custom TPOT configuration <x-msg://8/rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters> to constrain the ML algorithms that TPOT optimizes over. The TPOT light <http://rhiever.github.io/tpot/using/#built-in-tpot-configurations> configuration is a built-in configuration that avoids complex ensemble models, so that is another option.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/553#issuecomment-325446718>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEapKd6IAgOqDDBY8d-G4sxNLPOrJ6hyks5scw5MgaJpZM4PE6X5>.
> 

",hi thank answer however custom configuration need specify tested option simple light complex ensemble would like simply avoid ensemble randy hi use custom configuration constrain light configuration configuration complex ensemble another option reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
325446718,"Hi @mattvan83,

You can use a [custom TPOT configuration](rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) to constrain the ML algorithms that TPOT optimizes over. The [TPOT light](http://rhiever.github.io/tpot/using/#built-in-tpot-configurations) configuration is a built-in configuration that avoids complex ensemble models, so that is another option.",hi use custom configuration constrain light configuration configuration complex ensemble another option,issue,negative,positive,neutral,neutral,positive,positive
325445541,"Hi Matthieu,

Thank you for interesting question. 

We are thinking about using a configurable grammar (see #523) to specify pipeline structure in future version of TPOT.  With the grammar, we can easily specify simple estimator or pipeline (compound estimators).
",hi thank interesting question thinking grammar see specify pipeline structure future version grammar easily specify simple estimator pipeline compound,issue,positive,positive,positive,positive,positive,positive
325442201,"Hi,

Many thanks for this explanation !

Best regards,
Matthieu

> Le 28 août 2017 à 20:40, Randy Olson <notifications@github.com> a écrit :
> 
> Hi Matthieu,
> 
> If you would like to use a different cross-validation strategy than the default (StratifiedKFold), then you can pass the cross-validation object directly to the cv parameter. In your case, it would look something like:
> 
> from tpot import TPOTClassifier
> from sklearn.model_selection import LeaveOneOut, train_test_split
> import numpy as np
> 
> f = open(""SVM.csv"")
> data = np.loadtxt(f)
> X = data[:, 2:]  # select columns 1 through end
> y = data[:, 0]
> X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)
> 
> tpot = TPOTClassifier(generations=100, population_size=100, cv=LeaveOneOut(), verbosity=2, n_jobs=8)
> tpot.fit(X_train, y_train)
> print(tpot.score(X_test, y_test))
> tpot.export('tpot_pipeline_'  + str(index) + '.py')
> The same applies to all other cross-validation strategies in sklearn as well (e.g., ShuffleSplit).
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/551#issuecomment-325441334>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AEapKZwyxU0P9kW737XwEjZ_1kftM8dGks5scwmlgaJpZM4PDy1z>.
> 

",hi many thanks explanation best randy hi would like use different strategy default pas object directly parameter case would look something like import import import open data data select end data print index well thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
325441334,"Hi Matthieu,

If you would like to use a different cross-validation strategy than the default (StratifiedKFold), then you can pass the cross-validation object directly to the `cv` parameter. In your case, it would look something like:

```Python
from tpot import TPOTClassifier
from sklearn.model_selection import LeaveOneOut, train_test_split
import numpy as np

f = open(""SVM.csv"")
data = np.loadtxt(f)
X = data[:, 2:]  # select columns 1 through end
y = data[:, 0]
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=100, population_size=100, cv=LeaveOneOut(), verbosity=2, n_jobs=8)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_pipeline_'  + str(index) + '.py')
```

The same applies to all other cross-validation strategies in sklearn as well (e.g., ShuffleSplit). This is because we pass whatever is in the TPOT `cv` parameter directly to sklearn's `cross_val_score` `cv` parameter.",hi would like use different strategy default pas object directly parameter case would look something like python import import import open data data select end data print index well pas whatever parameter directly parameter,issue,positive,positive,neutral,neutral,positive,positive
325387978,"No, the TPOT version is something else entirely. It's macro-average accuracy, not macro-average recall.",version something else entirely accuracy recall,issue,negative,neutral,neutral,neutral,neutral,neutral
325387634,"OK, so the TPOT version is exactly `sklearn.metrics.recall_score(y_true, y_pred, average='macro')`, and the AutoML score adjusts this by https://github.com/ch-imad/AutoMl_Challenge/blob/2353ec0/Starting_kit/scoring_program/libscores.py#L210, right?",version exactly score right,issue,negative,positive,positive,positive,positive,positive
325381385,Interesting. Maybe it was a memory issue (not enough memory to run all jobs)?,interesting maybe memory issue enough memory run,issue,negative,positive,positive,positive,positive,positive
325353509,"Everything ran successfully after setting `n_jbos` to half the # of processors and converting all the data to numpy arrays before feeding it into `fit`.

`forkserver` was not necessary.",everything ran successfully setting half converting data feeding fit necessary,issue,positive,positive,positive,positive,positive,positive
325130332,Another strategy would be to use a randomized forest and use the importance weights as the fitness.,another strategy would use forest use importance fitness,issue,positive,neutral,neutral,neutral,neutral,neutral
325066788,You can inspect all of the pipelines that were successfully evaluated during the run with the `evaluated_individuals_` property of your TPOT object. Does XGBoost show up in any of the pipelines?,inspect successfully run property object show,issue,negative,positive,positive,positive,positive,positive
325024167,"No, you shouldn't be concerned about that output. That's TPOT pre-testing pipelines and throwing out bad pipelines before fully evaluating them. I generally recommend using `verbosity=2`, as `verbosity=3` is going to have a ton of extra output that probably won't be useful for you (except for the Pareto front scores, maybe).",concerned output throwing bad fully generally recommend going ton extra output probably wo useful except front maybe,issue,negative,negative,neutral,neutral,negative,negative
325023498,"With the `timeout_pipe` branch, it ran successfully, resulting in `RandomForestRegressor` as the best. But then I ran the data using XGBoost, and I get better results.

Did all the XGBoost runs fail in the `pipeline_optimizer`. How would I know if they didn't work.",branch ran successfully resulting best ran data get better fail would know work,issue,positive,positive,positive,positive,positive,positive
325022708,"I'm running xgboost version 0.6, and I'm getting a bunch of error message.

```
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
    train_df.ix[:, train_df.columns != 'logerror'],
    train_df['logerror'],
    train_size  = 0.75,
    test_size   = 0.25
)

pipeline_optimizer = tpot.TPOTRegressor(
    n_jobs        = -1,
    max_time_mins = 60 * 1,
    warm_start    = True,
    verbosity     = 100
)

pipeline_optimizer.fit(X_train.values, y_train.values)
pipeline_optimizer.export('tpot_exported_pipeline.py')
print(pipeline_optimizer.score(X_test, y_test))
```

```
28 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument.
_pre_test decorator: _generate: num_test=2 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=3 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98
_pre_test decorator: _generate: num_test=0 X contains negative values.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.
_pre_test decorator: _generate: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances.
_pre_test decorator: _generate: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 precomputed was provided as affinity. Ward can only work with euclidean distances.
```

Should I be alarmed?",running version getting bunch error message true verbosity print decorator got unexpected argument decorator unsupported set combination decorator provided affinity ward work decorator got unexpected argument decorator unsupported set combination decorator got unexpected argument decorator unsupported set combination decorator got unexpected argument decorator got unexpected argument decorator automatic alpha grid generation please supply grid providing estimator appropriate argument decorator got unexpected argument decorator got unexpected argument decorator decorator negative decorator got unexpected argument decorator found array feature minimum decorator found array feature minimum decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator provided affinity ward work decorator decorator unsupported set combination decorator provided affinity ward work alarmed,issue,negative,positive,positive,positive,positive,positive
324999335,"Hi @jonathanng,

AFAIK, scikit-learn pipelines do not work with transformers that change the number of instances (i.e., rows) in the dataset, because the target labels (i.e., `y`) would also need to be transformed as well. There would need to be a custom scikit-learn pipeline implementation that works with datasets of varying sample sizes.",hi work change number target would also need well would need custom pipeline implementation work sample size,issue,negative,neutral,neutral,neutral,neutral,neutral
324746229,"How about using `forkserver` start mode in current master branch? 

The timeout_pipe branch is a compromising solution, and I tested it had some issues in Windows OS and also the performance of `n_jobs` was not as good as current master branch. We need refine this solution before merging if `forkserver` does not work or we need find a better solution to solve this fork issue.",start mode current master branch branch compromising solution tested o also performance good current master branch need refine solution work need find better solution solve fork issue,issue,positive,positive,positive,positive,positive,positive
324734507,"I have the same issue.
```
> devtools::session_info()
Session info --------------------------------------------------------------------------------------
 setting  value                       
 version  R version 3.4.0 (2017-04-21)
 system   x86_64, mingw32             
 ui       RStudio (1.0.153)           
 language (EN)                        
 collate  French_France.1252          
 tz       Europe/Paris                
 date     2017-08-24                  

Packages ------------------------------------------------------------------------------------------
 package       * version    date       source                             
 acepack         1.4.1      2016-10-29 CRAN (R 3.4.1)                     
 backports       1.1.0      2017-05-22 CRAN (R 3.4.0)                     
 base          * 3.4.0      2017-04-21 local                              
 base64enc       0.1-3      2015-07-28 CRAN (R 3.4.0)                     
 bigmemory       4.5.19     2016-03-28 CRAN (R 3.4.1)                     
 bigmemory.sri   0.1.3      2014-08-18 CRAN (R 3.4.0)                     
 bigsnpr       * 0.1.0.9001 2017-08-24 local                              
 bigstatsr     * 0.1.0.9002 2017-08-24 local                              
 checkmate       1.8.3      2017-07-03 CRAN (R 3.4.1)                     
 cluster         2.0.6      2017-03-10 CRAN (R 3.4.0)                     
 codetools       0.2-15     2016-10-05 CRAN (R 3.4.0)                     
 colorspace      1.3-2      2016-12-14 CRAN (R 3.4.0)                     
 compiler        3.4.0      2017-04-21 local                              
 crayon          1.3.2.9000 2017-07-22 Github (gaborcsardi/crayon@750190f)
 data.table      1.10.4     2017-02-01 CRAN (R 3.4.0)                     
 datasets      * 3.4.0      2017-04-21 local                              
 devtools        1.13.3     2017-08-02 CRAN (R 3.4.1)                     
 digest          0.6.12     2017-01-27 CRAN (R 3.4.0)                     
 foreach       * 1.4.3      2015-10-13 CRAN (R 3.4.0)                     
 foreign         0.8-67     2016-09-13 CRAN (R 3.4.0)                     
 Formula       * 1.2-2      2017-07-10 CRAN (R 3.4.1)                     
 ggplot2       * 2.2.1.9000 2017-07-23 Github (hadley/ggplot2@331977e)    
 graphics      * 3.4.0      2017-04-21 local                              
 grDevices     * 3.4.0      2017-04-21 local                              
 grid            3.4.0      2017-04-21 local                              
 gridExtra       2.2.1      2016-02-29 CRAN (R 3.4.0)                     
 gtable          0.2.0      2016-02-26 CRAN (R 3.4.0)                     
 Hmisc         * 4.0-3      2017-05-02 CRAN (R 3.4.1)                     
 htmlTable       1.9        2017-01-26 CRAN (R 3.4.1)                     
 htmltools       0.3.6      2017-04-28 CRAN (R 3.4.0)                     
 htmlwidgets     0.9        2017-07-10 CRAN (R 3.4.1)                     
 iterators       1.0.8      2015-10-13 CRAN (R 3.4.0)                     
 knitr           1.17       2017-08-10 CRAN (R 3.4.1)                     
 lattice       * 0.20-35    2017-03-25 CRAN (R 3.4.0)                     
 latticeExtra    0.6-28     2016-02-09 CRAN (R 3.4.1)                     
 lazyeval        0.2.0      2016-06-12 CRAN (R 3.4.0)                     
 magrittr      * 1.5        2014-11-22 CRAN (R 3.4.0)                     
 Matrix        * 1.2-9      2017-03-14 CRAN (R 3.4.0)                     
 memoise         1.1.0      2017-04-21 CRAN (R 3.4.0)                     
 methods       * 3.4.0      2017-04-21 local                              
 munsell         0.4.3      2016-02-13 CRAN (R 3.4.0)                     
 nnet            7.3-12     2016-02-02 CRAN (R 3.4.0)                     
 parallel        3.4.0      2017-04-21 local                              
 plyr            1.8.4      2016-06-08 CRAN (R 3.4.0)                     
 R6              2.2.2      2017-06-17 CRAN (R 3.4.1)                     
 RColorBrewer    1.1-2      2014-12-07 CRAN (R 3.4.0)                     
 Rcpp            0.12.12    2017-07-15 CRAN (R 3.4.1)                     
 rlang           0.1.2      2017-08-09 CRAN (R 3.4.1)                     
 rpart           4.1-11     2017-03-13 CRAN (R 3.4.0)                     
 rstudioapi      0.6        2016-06-27 CRAN (R 3.4.0)                     
 scales          0.4.1.9002 2017-07-23 Github (hadley/scales@6db7b6f)     
 splines         3.4.0      2017-04-21 local                              
 stats         * 3.4.0      2017-04-21 local                              
 stringi         1.1.5      2017-04-07 CRAN (R 3.4.0)                     
 stringr         1.2.0      2017-02-18 CRAN (R 3.4.0)                     
 survival      * 2.41-3     2017-04-04 CRAN (R 3.4.0)                     
 testthat      * 1.0.2      2016-04-23 CRAN (R 3.4.0)                     
 tibble          1.3.4      2017-08-22 CRAN (R 3.4.0)                     
 tools           3.4.0      2017-04-21 local                              
 utils         * 3.4.0      2017-04-21 local                              
 withr           2.0.0      2017-07-28 CRAN (R 3.4.1)                     
 xgboost         0.6-4      2017-01-05 CRAN (R 3.4.0)  
```",issue session setting value version version system language en collate date package version date source cran cran base local cran cran cran local local checkmate cran cluster cran cran cran compiler local crayon cran local cran digest cran cran foreign cran formula cran graphic local local grid local cran cran cran cran cran cran cran cran lattice cran cran cran cran matrix cran cran local cran cran parallel local cran cran cran cran cran cran cran scale local local cran cran survival cran cran cran local local cran cran,issue,negative,negative,neutral,neutral,negative,negative
323838366,@jonathanng oh you are right. It is not necessary in a jupyter notebook.,oh right necessary notebook,issue,negative,positive,positive,positive,positive,positive
323836182,"This is all within a Jupyter notebook, so I don't think: `if __name__ == '__main__':` is necessary, right?",within notebook think necessary right,issue,negative,positive,positive,positive,positive,positive
323835732,"I met this error before. You need restart the terminal and then start with:
```
import multiprocessing
if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
    # Note: need move import sklearn into main unless a RuntimeError (RuntimeError: context has already been set) will raise
```",met error need restart terminal start import note need move import main unless context already set raise,issue,negative,positive,positive,positive,positive,positive
323835375,"I'm using Python 3.6.1. I tried both n_jobs=1 and n_jobs=-1. Both gave me the error.

Also, when I run:
```
import multiprocessing
multiprocessing.set_start_method('forkserver')
```
I get the following error:
> RuntimeError: context has already been set

I'm trying your `timeout_pipe` branch now. Will you let you know the results.
",python tried gave error also run import get following error context already set trying branch let know,issue,negative,neutral,neutral,neutral,neutral,neutral
323834302,"jonathanng Could you please let me know whether setting n_jobs=1 or using `forkserver` start mode in python 3.4+ in `TPOTRegressor` can solve this issue? Below is a demo for `TPOTClassifier`.

```
import multiprocessing
if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
    # Note: need move import sklearn into main unless a RuntimeError (RuntimeError: context has already been set) will raise
    from sklearn.datasets import make_classification
    from tpot import TPOTClassifier
    # make a huge dataset
    X, y = make_classification(n_samples=50000, n_features=200,
                                        n_informative=20, n_redundant=20,
                                        n_classes=5, random_state=42)
    
    # max_eval_time_mins=0.04 means 2 seconds limits for evaluating a single pipeline 
    # working in python 3.4+ in Linux OS and MacOS
    tpot = TPOTClassifier(generations=5, population_size=50, offspring_size=100, random_state=42, n_jobs=2, max_eval_time_mins=0.04, verbosity=3) 
    tpot.fit(X, y)
```",could please let know whether setting start mode python solve issue import note need move import main unless context already set raise import import make huge single pipeline working python o,issue,positive,positive,positive,positive,positive,positive
323338663,The reason is that feature-selection step in a pipeline can exclude all features before running xgboost. We need better control on feature numbers within pipeline.,reason step pipeline exclude running need better control feature within pipeline,issue,negative,positive,positive,positive,positive,positive
323294154,"Do we know why this issue occurs ? it will be helpful to know why ""colsample_bytree=1 is too small that no
feature can be included"" occurs.",know issue helpful know small feature included,issue,negative,negative,negative,negative,negative,negative
323164572,"@rhiever sounds like a good idea. you could use it with any method that admits some kind of feature score, e.g. lasso, random forests, etc.. and perhaps even with stacking if stacking can be made to score the models it uses in its ensemble. ",like good idea could use method kind feature score lasso random perhaps even made score ensemble,issue,positive,positive,positive,positive,positive,positive
323135776,"Oh, sorry for the misunderstanding. We are working on #523 and #462 (One hot encoder can deal with sparse matrix) to add this support in the future version of TPOT.  ",oh sorry misunderstanding working one hot deal sparse matrix add support future version,issue,negative,negative,neutral,neutral,negative,negative
323134888,"yes, that works. I meant if it was possible to use TPOT with a sparse matrix.",yes work meant possible use sparse matrix,issue,negative,neutral,neutral,neutral,neutral,neutral
323131082,I had the same problem. This was caused by using a sparse matrix as input. np.isnan() does not work on sparse matrices. Is there a way around this other than converting to dense?,problem sparse matrix input work sparse matrix way around converting dense,issue,negative,neutral,neutral,neutral,neutral,neutral
322867882,"Sorry, the current CLI mode in TPOT do not have this option. You may need write python scripts for this purpose.",sorry current mode option may need write python purpose,issue,negative,negative,negative,negative,negative,negative
322509637,"It seems if you are using pipelines than imbalanced-learn comes with it's own implementation,[imblearn.pipeline.Pipeline](http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.pipeline.Pipeline.html#imblearn.pipeline.Pipeline) which has a bunch of extra functions to do with transforming and sampling. Looks to be to do with supporting having a different number of examples through a pipeline, rather than just different features. Probably only makes sense for them to be at the start of the pipeline too, and unsure how to enforce that.
",come implementation bunch extra transforming sampling supporting different number pipeline rather different probably sense start pipeline unsure enforce,issue,negative,positive,neutral,neutral,positive,positive
322507566,"I tried to use `config_dict` for incorporating `imblalanced-learn` with the codes below:

```
#  with imbalanced-learn-0.3.0.dev0
from sklearn.datasets import load_iris
from imblearn.datasets import make_imbalance
from sklearn.model_selection import train_test_split
from tpot import TPOTClassifier
ratio = {0: 10, 1: 20, 2: 30}
iris = load_iris()
X, y = make_imbalance(iris.data, iris.target, ratio=ratio)


tpot_config = {

    'sklearn.naive_bayes.BernoulliNB': {
        'alpha': [1e-3, 1e-2, 1e-1, 1., 10., 100.],
        'fit_prior': [True, False]
    },

    'imblearn.under_sampling.RandomUnderSampler': {
        'ratio': ['minority', 'majority', 'all'],
        'replacement': [True, False]
    }
    
}

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=3,
                      config_dict=tpot_config, random_state=42)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
```

However, I got a lot error messages like:
```
All intermediate steps should be transformers and implement fit and transform. 
'RandomUnderSampler(random_state=None, ratio='minority', replacement=True, return_indices=False)'
(type <class 'imblearn.under_sampling.prototype_selection.random_under_sampler.RandomUnderSampler'>) doesn't
```

Maybe we need wrap the `imblalanced-learn` object as subclass of [sklearn.base.TransformerMixin](http://scikit-learn.org/stable/data_transforms.html) and add implementation of `transform`.

",tried use dev import import import import ratio iris true false true false print however got lot error like intermediate implement fit transform type class maybe need wrap object subclass add implementation transform,issue,positive,positive,neutral,neutral,positive,positive
322435052,I think it may be related to the issue #508. Please try to set n_jobs=1 with python 2.7 or try use `forkserver` start mode in python 3.4+ (see the demo codes in issue #508),think may related issue please try set python try use start mode python see issue,issue,negative,neutral,neutral,neutral,neutral,neutral
322233775,"Yes, you may need exclude xgboost from [built-in configuration](https://github.com/rhiever/tpot/blob/master/tpot/config/classifier.py) to avoid the error. Or you could try to run the codes with default configuration in Linux or macOS where the error message would still be printed out but TPOT optimization process would not be stopped.",yes may need exclude configuration avoid error could try run default configuration error message would still printed optimization process would stopped,issue,negative,neutral,neutral,neutral,neutral,neutral
322226946,"@weixuanfu : thanks for detailed and prompt explanation. 

Just wanted to clarify the practical steps on my end.
1. Since no model is generated by TPOT on my premise due to the xgboost-driven issue referred, shall I simply exclude xgboost in the customized configuration?
2. Otherwise, shall I try to tweak xgboost settings in the customized configuration to find some setup where it does not crash and allows for a model to be generated by TPOT eventually?

Thank you in advance.",thanks detailed prompt explanation clarify practical end since model premise due issue shall simply exclude configuration otherwise shall try tweak configuration find setup crash model eventually thank advance,issue,negative,positive,positive,positive,positive,positive
322174527,"The answer to the first question is using the customized configuration of operators and their parameters.
 Please check the [link](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters) for more details.

The second question of this issue is related to the issue #449 in tpot repo and [the issue 2349](https://github.com/dmlc/xgboost/issues/2349) in xgboost repo. In general, the logging cannot be disabled  with `silent=True` in xgboost API. ",answer first question configuration please check link second question issue related issue issue general logging disabled,issue,negative,positive,neutral,neutral,positive,positive
321341331,That's more-or-less what we've seen across 100s of datasets. Glad you were able to replicate it.,seen across glad able replicate,issue,negative,positive,positive,positive,positive,positive
321332008,"Thanks @rhiever. Understood about the custom configuration dictionary. I was surprised that RF with 500 trees beat TPOT result. Understand it's not automatically optimized out of the box. Thinking about it more, I used all default TPOT setup (population size and generations). I see in the docs you recommend AutoML is often designed to run for a long time, so impatience could have been it :) 

I ran a little experiment on MNIST. There is an average and median increase after 100 trees. The average score improvement is marginal. 

```
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
n_trees = [10,25,50,75,100,200,300,400,500,600,700,800,900,1000]
scores = []
for n in n_trees:
    cl = RandomForestClassifier(n_estimators=n)
    score = cross_val_score(cl, digits.data, digits.target, cv=10)
    scores.append(score)
```

```
plt.figure(figsize=(12,8))
scores = np.array(scores)
scores_mean = scores.mean(axis=1)
scores_median = np.median(scores2, axis=1)
scores_min = scores.min(axis=1)
scores_max = scores.max(axis=1)
plt.plot(n_trees, scores_mean, label='mean score')
plt.fill_between(n_trees, scores_min, scores_max, alpha=0.5)
plt.plot(n_trees, scores_median, label='median score')
plt.vlines(100, 0.94, 0.96, label='100 estimators')
plt.xlabel('n_estimators')
plt.ylabel('accuracy')
plt.legend()
plt.xlim(0,500)
```
![mnist](https://user-images.githubusercontent.com/5091145/29135776-4468b4ca-7d09-11e7-9692-16593faef46f.png)

",thanks understood custom configuration dictionary beat result understand automatically box thinking used default setup population size see recommend often designed run long time impatience could ran little experiment average median increase average score improvement marginal import import import score score score score,issue,positive,negative,neutral,neutral,negative,negative
321188759,"I noticed it works if you give a list of instantiated estimators, like 
```
'estimator' : [  ensemble.RandomForestClassifier(class_weight=""balanced"", n_estimators=501),
                linear_model.LogisticRegression(class_weight=""balanced"", penalty=""l2""),
                linear_model.LogisticRegression(class_weight=""balanced"", penalty=""l1""),
                linear_model.ElasticNet() ]
```
However those estimators won't be optimized",work give list like balanced balanced balanced however wo,issue,negative,neutral,neutral,neutral,neutral,neutral
320978171,"I think this issue is related to #507. We are working on a configurable grammar in #523 to add the support for text classification. For now, you may try to transform text to numeric matrix using [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), [TFIDFVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) and [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) before using TPOTClassifier for your problem. ",think issue related working grammar add support text classification may try transform text matrix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
320975636,"Thank you of this interesting idea. For now, TPOT does not allow multiple estimators in `estimator` parameter in `SeletFromModel` since this parameter only takes one estimator. By default, TPOTClassifier uses `ExtraTreesClassifier` for the estimator as shown [here](https://github.com/rhiever/tpot/blob/master/tpot/config/classifier.py#L188-L197). We may add this support into TPOT later",thank interesting idea allow multiple estimator parameter since parameter one estimator default estimator shown may add support later,issue,positive,positive,positive,positive,positive,positive
320519000,@weixuanfu n_jobs =1 worked for windows. I am also running it on a linux box with with n_jobs=20. It seems to be working on linux.,worked also running box working,issue,negative,neutral,neutral,neutral,neutral,neutral
320462019,"Oh, I just found that. Maybe decreasing n_jobs to 1 would help for Windows. Or you could try the latest dev branch where has better timeout control. Please let me know and inform me more environment infos (Tpot and its deps' versions) if both possible solutions do not work. I need double-check it.",oh found maybe decreasing would help could try latest dev branch better control please let know inform environment possible work need,issue,positive,positive,positive,positive,positive,positive
320454417,"@weixuanfu Thank you for the prompt reply. I should have mentioned that I am using Windows. Sorry about that. 'fork server' doesn't work on windows. How can I make it work for Windows? I have changed n_jobs =1, and even that doesn't seem to work. Thanks again!",thank prompt reply sorry server work make work even seem work thanks,issue,positive,negative,negative,negative,negative,negative
320443586,"I think this is related to #508. Please try to run TPOT like this demo below:

```
import multiprocessing
if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
    # Note: need move import sklearn into main unless a RuntimeError (RuntimeError: context has already been set) will raise
    from sklearn.datasets import make_classification
    from tpot import TPOTClassifier
    # your TPOT codes
    pipeline_optimizer = TPOTClassifier()
    pipeline_optimizer = TPOTClassifier(generations=5, population_size=20, cv=5, random_state=0, 
                                                              verbosity=2,n_jobs = 10)
    X_train = np.nan_to_num(X_train)
    pipeline_optimizer.fit(X_train, dataY_train)
```
Please let me know if this way solve this issue.",think related please try run like import note need move import main unless context already set raise import import please let know way solve issue,issue,positive,positive,neutral,neutral,positive,positive
320322538,">Did you find that having less than ten unique values is a good predictor of categorical features?

That's a rule of thumb that we've often used in the lab, but we haven't evaluated it experimentally. Usually we make that a parameter for the user.",find le ten unique good predictor categorical rule thumb often used lab experimentally usually make parameter user,issue,negative,positive,positive,positive,positive,positive
320306517,Update: We added this option in the PR #541 and it should be merged later.,update added option later,issue,negative,neutral,neutral,neutral,neutral,neutral
320305914,Closing this issue for now. Please feel free to re-open if you have any more questions or comments.,issue please feel free,issue,positive,positive,positive,positive,positive,positive
320010513,"```from tpot import TPOTClassifier
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split, GroupKFold


mnist_data = load_digits()
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(mnist_data.data.astype(np.float64), mnist_data.target.astype(np.float64), random_state=42)

means = np.mean(training_features, axis=1)
groups = means >= np.median(means)

tpot_obj = TPOTClassifier(
    random_state=42,
    population_size=2,
    offspring_size=4,
    generations=1,
    verbosity=2,
    config_dict='TPOT light',
    cv = GroupKFold(n_splits=2),
)
tpot_obj.fit(training_features, training_classes, groups=groups)
```

Please check the demo for using `GroupKFold`",import import import import light please check,issue,negative,positive,positive,positive,positive,positive
319990808,"@drorhilman after some discussions with @rhiever, we will add this option to stop process if the score does not improve after N generations. The option will be added into next version of TPOT and of course, it is turned off by default. ",add option stop process score improve option added next version course turned default,issue,negative,neutral,neutral,neutral,neutral,neutral
319872358,"The implementation is in line with the implementation of scikit-learn, and the categorical features need to be specified by the user. Did you find that having less than ten unique values is a good predictor of categorical features?",implementation line implementation categorical need user find le ten unique good predictor categorical,issue,negative,positive,positive,positive,positive,positive
319764542,"That seems to be right, @weixuanfu. I've changed the default behavior of OHE to be in-line with @mfeurer's OHE (i.e., treating all features as categorical features) until we figure out what's going on with the auto_select function. OHE seems to be working fine now.",right default behavior treating categorical figure going function working fine,issue,negative,positive,positive,positive,positive,positive
319732716,"Just have a quick look on the `OneHotEncoder` codes in TPOT. I think [_auto_select_categorical_features](https://github.com/rhiever/tpot/blob/development/tpot/builtins/one_hot_encoder.py#L45-L75) causes `ValueError: X has different shape than during fitting. Expected 2, got 3` in this issue. Because `KFold` provides different train/test indices to split data but `_auto_select_categorical_features` only uses one threshold (less than 10 unique values are considered categorical) to check categorical features, it cause the inconsistent number of categorical features among train/test indices. `OneHotEncoder` in Auto-sklearn just considers all features are categorical by default. ",quick look think different shape fitting got issue different index split data one threshold le unique considered categorical check categorical cause inconsistent number categorical among index categorical default,issue,negative,positive,positive,positive,positive,positive
319709126,"From our internal tests, we've found that the RandomForest has small improvements when adding beyond 100 trees. You can use a [custom configuration dictionary](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters), though, if you want TPOT to explore a larger number of trees.",internal found small beyond use custom configuration dictionary though want explore number,issue,negative,negative,negative,negative,negative,negative
319707535,"@teaearlgraycold, can you look into this please? Could any of the changes you made introduce this bug?",look please could made introduce bug,issue,negative,neutral,neutral,neutral,neutral,neutral
319689488,"If the option just check whether the best pipeline is still improving in GP, this option maybe misused with small number of population size due to limited number of new better individuals per generation. I think we still need a threshold value if we add this option. ",option check whether best pipeline still improving option maybe small number population size due limited number new better per generation think still need threshold value add option,issue,positive,positive,positive,positive,positive,positive
319561198,"Yes. I think that it should have a notion of ""progress"" - check whether you are still improving or not, and if not then stop. ",yes think notion progress check whether still improving stop,issue,positive,neutral,neutral,neutral,neutral,neutral
319079429,"Hmm, I think we might add this stop option once the accuracy of classification reaches a very high score, like 0.999. What do you think of this idea? @rhiever 

But I think it is not simply to use the option in most of scoring functions in regression tasks. ",think might add stop option accuracy classification high score like think idea think simply use option scoring regression,issue,negative,positive,neutral,neutral,positive,positive
318882909,"I cannot reproduce the error in Auto-sklearn. This piece of code works all fine (tried >10 times):
```
from sklearn.linear_model import LinearRegression
from sklearn.datasets import load_boston
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score, KFold
from autosklearn.pipeline.implementations.OneHotEncoder import OneHotEncoder

boston = load_boston()

clf = make_pipeline(OneHotEncoder(sparse=False, minimum_fraction=0.05), LinearRegression())
cross_val_score(clf, boston.data, boston.target, cv=KFold(n_splits=10, shuffle=True))
```
Does this work for you?

I'm not sure if the latest fix from March 28 would make a difference, but it seems that you guys didn't pull that. Another possibility is that you introduced a bug when refactoring the class.",reproduce error piece code work fine tried time import import import import import boston work sure latest fix march would make difference pull another possibility bug class,issue,negative,positive,positive,positive,positive,positive
318207374,"Oops, I just closed by mistake. Cellphone screen is too small for typing. Sorry.",closed mistake screen small sorry,issue,negative,negative,negative,negative,negative,negative
318206606,"For solving this issue, we are working on the related issue #529 for adding a grammar configuration for supporting text classification.",issue working related issue grammar configuration supporting text classification,issue,negative,positive,positive,positive,positive,positive
318205923,"Oh, sorry for the misunderstanding before. I think you need to reformat the output table to 1-D array, like nameing these classes as 1-19 in the array.",oh sorry misunderstanding think need output table array like class array,issue,negative,negative,negative,negative,negative,negative
318180318,"Is there a workaround for this we can use in the interim?

I tried simply adding the tpot classifier to a pipeline:

pipeline_optimizer = pipeline = Pipeline([('vect', CountVectorizer()),
                     ('tfidf', TfidfTransformer()),
                    ('clf', TPOTClassifier(generations=5, population_size=20, verbosity=2))])

But this returns an error 'ufunc 'isnan' not supported for the input types' which I saw addressed elsewhere in the issues list. It seems to be an issue with numpy. I saw a suggestion to use toarray(), so ended up with something like this:

vectorizer = CountVectorizer()
tfidf_transformer = TfidfTransformer()  
trainVectorized = vectorizer.fit_transform(X_train)
testVectorized = vectorizer.fit_transform(X_test)
finalTrain = tfidf_transformer.fit_transform(trainVectorized).toarray()
finalTest = tfidf_transformer.fit_transform(testVectorized).toarray()
   
But this seems to produce input data that tpot doesn't care for. I get a different error message: 'Input data is not in a valid format.' I'm sure I'm doing something wrong. 

Any other ideas what I might try to get text classification working today (though I look forward to the tighter integration proposed here)?",use interim tried simply classifier pipeline pipeline pipeline error input saw elsewhere list issue saw suggestion use ended something like produce input data care get different error message data valid format sure something wrong might try get text classification working today though look forward integration,issue,negative,neutral,neutral,neutral,neutral,neutral
318170918,"No its not a multi label problem. So I need to feed one set of examples and their corresponding class? because I have 19 classes in my problem, I need to do this for each set of examples? I am not quite sure how this works",label problem need feed one set corresponding class class problem need set quite sure work,issue,negative,positive,positive,positive,positive,positive
318163939,"Could you please let me know more details about your case? For example, system environment  (OS, python version, TPOT version) ? Also, did you reproduce the issue with IRIS or MINST example in the [link](http://rhiever.github.io/tpot/examples/) ?",could please let know case example system environment o python version version also reproduce issue iris example link,issue,negative,neutral,neutral,neutral,neutral,neutral
318162902,"Is this a multilabel problem as mentioned in #529?

If not, I suggest to use one label out of output to one `tpot.fit()`.  You just need select one column out of output as `y` in fit(). ",problem suggest use one label output one need select one column output fit,issue,negative,positive,positive,positive,positive,positive
318095845,"It's definitely not a 1D array, because this is how I stored my data in matlab and now I have no idea how to format it to work for tpot.

I basically have a 2D array as an input in which each column corresponds to an example and another 2D array as the output with each column corresponding to a label for the corresponding data in the input array.

so outputnn[:,1] corresponds to input[:,1]

This is my output in matlab:
![image](https://user-images.githubusercontent.com/1936480/28630474-9f961d22-722a-11e7-8a7e-5f2fa62f2657.png)

and this is my input:

![image](https://user-images.githubusercontent.com/1936480/28630498-b0ca0086-722a-11e7-9e26-0c1e3c23a47d.png)


and each column is an example/label. Can you suggest any work around for this?",definitely array data idea format work basically array input column example another array output column corresponding label corresponding data input array input output image input image column suggest work around,issue,negative,neutral,neutral,neutral,neutral,neutral
318092346,I think maybe `outputnn` is not a 1-D array. Could please change the shape of `outputnn`?,think maybe array could please change shape,issue,negative,neutral,neutral,neutral,neutral,neutral
318090688,"Thanks! it was actually a list and I have just converted to numpy array, however I am now getting the following error

![image](https://user-images.githubusercontent.com/1936480/28629395-94d85588-7227-11e7-8f9d-3699aaf4790c.png)

I am getting the following error now, which has to do with the shape of my data. 

my input data is definitely 2D array with each column being an example and its dimension is (85 rows ,20163 columns), while my output data is also a 2D array with each column being a label and has a dimension of (19,20163). How should format my data?",thanks actually list converted array however getting following error image getting following error shape data input data definitely array column example dimension output data also array column label dimension format data,issue,negative,positive,neutral,neutral,positive,positive
318078744,It seem that `input_nn` is a list instead a numpy array based on the error message. Could you please print out `type(input_nn)` for checking the type of `input_nn`?,seem list instead array based error message could please print type type,issue,negative,neutral,neutral,neutral,neutral,neutral
318077744,"@dclambert Thank you for reporting this issue.

I just posted a PR #538 for it and it is based on dev branch. You may try the commands below to reinstall TPOT with the patch

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@f_regression
```

Also I made another branch with the patch based on Master branch (I did not post PR for it for avoiding conflicts). Try the commands below for installing the branch. 

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu/tpot.git@f_regression_patch
```",thank issue posted based dev branch may try reinstall patch pip install upgrade also made another branch patch based master branch post try branch pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
318070328,"![capture](https://user-images.githubusercontent.com/1936480/28626507-70eaa49e-721f-11e7-9914-4d99029e520d.JPG)

This is the error I am getting

I am trying to run tpot using jupyter ",capture error getting trying run,issue,negative,neutral,neutral,neutral,neutral,neutral
318068647,"@amir-abdi Could you please provide more details about the problem, like codes for reproducing the issue? Or could you please try the dev branch. It may give you some error message. Please try the command for installing dev branch

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/rhiever/tpot.git@development
```

",could please provide problem like issue could please try dev branch may give error message please try command dev branch pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
318067535,"Hmm, I think it is a bug. I will submit a PR for fixing it. ",think bug submit fixing,issue,negative,neutral,neutral,neutral,neutral,neutral
318066962,"Could you please provide more details about this issue, like detail stderr or codes for reproducing the issue?",could please provide issue like detail issue,issue,positive,neutral,neutral,neutral,neutral,neutral
317839130,"Thank you for the suggestion. I agree that `n_estimators` is a crucial parameter. But, increasing `n_estimator` to 500 may largely increase computational time of pipeline evaluation (check #384  for more details). We need more evaluation before increasing `n_estimators` in default `config_dict`",thank suggestion agree crucial parameter increasing may largely increase computational time pipeline evaluation check need evaluation increasing default,issue,positive,positive,positive,positive,positive,positive
317837021,Thank you for the feedback. We already add `pandas` as default csv reader in the PR #519 that was merged to dev branch.,thank feedback already add default reader dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
317241204,"I have tested speed of numpy vs pandas loader:

numpy
real	1m17.487s
user	1m14.974s
sys	0m4.410s

pandas
real	0m9.028s
user	0m8.662s
sys	0m4.043s

Code:
~~~
import numpy as np
import pandas as pd

def load_train_data_np():
	train_data = np.genfromtxt('MNIST/train.csv', delimiter=',', skip_header=1)
	X_train= train_data[:,1:]
	y_train= train_data[:,0]
	
	print ('X_train.shape', X_train.shape)
	print ('y_train.shape', y_train.shape)
	
	return X_train, y_train

def load_test_data_np():
	test_data = np.genfromtxt('MNIST/test.csv', delimiter=',', skip_header=1)
	X_test= test_data
	
	print ('X_test.shape', X_test.shape)
	
	
def load_train_data_pd():
	train_data = pd.read_csv('MNIST/train.csv', delimiter=',', skiprows=1).as_matrix()
	X_train= train_data[:,1:]
	y_train= train_data[:,0]
	
	print ('X_train.shape', X_train.shape)
	print ('y_train.shape', y_train.shape)
	
	return X_train, y_train

def load_test_data_pd():
	test_data = pd.read_csv('MNIST/test.csv', delimiter=',', skiprows=1).as_matrix()
	X_test= test_data
	
	print ('X_test.shape', X_test.shape)

def test_np_loader():
	load_train_data_np()
	load_test_data_np()
	
def test_pd_loader():
	load_train_data_pd()
	load_test_data_pd()
~~~
",tested speed loader real user real user code import import print print return print print print return print,issue,negative,positive,positive,positive,positive,positive
317071664,"@kegl the one in that toolkit is ""adjusted for chance"" though, and the one in TPOT is not. So that toolkit does macro average recall but adjusted for chance.",one chance though one macro average recall chance,issue,negative,negative,negative,negative,negative,negative
317068662,"@kegl are you doing binary classification? Then it's pretty clear and using the macro average should be fine. If it's multi-class, it's a bit less clear.",binary classification pretty clear macro average fine bit le clear,issue,positive,positive,positive,positive,positive,positive
317067760,"@kegl I would have hoped you could tell us ;) There is multiple definitions of balanced accuracy, and one of them is ``recall_score(..., average='macro')`` and another is something different, see https://github.com/scikit-learn/scikit-learn/pull/8066/
It looks like https://github.com/ch-imad/AutoMl_Challenge/blob/2353ec0/Starting_kit/scoring_program/libscores.py#L187 implements  ``recall_score(..., average='macro')``  see [@jnothman's comment](https://github.com/scikit-learn/scikit-learn/issues/6747#issuecomment-227920492). Whoever told you to use this metric should have given you a paper reference or use a more specific name ;)",would hoped could tell u multiple balanced accuracy one another something different see like see comment whoever told use metric given paper reference use specific name,issue,positive,neutral,neutral,neutral,neutral,neutral
317064938,"I went through this thread and the related sklearn thread and it's not clear to me what the consensus is. Somebody asked me to used balanced accuracy from [here](http://www.causality.inf.ethz.ch/AutoML/Starting_kit.zip), `scoring_program/libscores.py`, line 187. Should I clean this up or I can use recall_score(..., average='macro') from sklearn?",went thread related thread clear consensus somebody used balanced accuracy line clean use,issue,positive,positive,positive,positive,positive,positive
316742288,"That would be very unfortunate. @rhiever could you provide me a pickle/csv of the data which produces the error? 

Edit - nevermind, I'll try the minimal working example once I find some time to do so.",would unfortunate could provide data error edit try minimal working example find time,issue,negative,negative,negative,negative,negative,negative
316717229,"@dnuffer @CSNoyes 

Below is the demo for using `forkserver` to solving this issue in Linux and MacOS. I am still thinking whether we should put this solution into codes. It seems that it is not easy to use `forkserver` with joblib in interactive mode. Maybe we should provide a friendly warning message about this and/or tghe solution in the demo below as [the Q&A in scikit-learn](http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux). Please let me know if the issue still exists with the demo below in your environment. Thanks. 

```
import multiprocessing
if __name__ == '__main__':
    multiprocessing.set_start_method('forkserver')
    # Note: need move import sklearn into main unless a RuntimeError (RuntimeError: context has already been set) will raise
    from sklearn.datasets import make_classification
    from tpot import TPOTClassifier
    # make a huge dataset
    X, y = make_classification(n_samples=50000, n_features=200,
                                        n_informative=20, n_redundant=20,
                                        n_classes=5, random_state=42)
    
    # max_eval_time_mins=0.04 means 2 seconds limits for evaluating a single pipeline 
    # working in python 3.4+ in Linux OS and MacOS
    tpot = TPOTClassifier(generations=5, population_size=50, offspring_size=100, random_state=42, n_jobs=2, max_eval_time_mins=0.04, verbosity=3) 
    tpot.fit(X, y)
```",issue still thinking whether put solution easy use interactive mode maybe provide friendly warning message solution please let know issue still environment thanks import note need move import main unless context already set raise import import make huge single pipeline working python o,issue,positive,positive,positive,positive,positive,positive
316298394,"It sounds like this issue could be driven more by the data split moreso than the random state of the algorithms. KNNs, for example, aren't stochastic algorithms and don't use the random state.

Are you shuffling your dataset beforehand? Are you performing stratified splits before passing the data to TPOT?",like issue could driven data split random state example stochastic use random state shuffling beforehand stratified passing data,issue,negative,negative,negative,negative,negative,negative
316296187,"Seems possible (probable?) the bug is present in both softwares, then.",possible probable bug present,issue,negative,neutral,neutral,neutral,neutral,neutral
316185502,I've added a table of related projects. Any other information I should provide?,added table related information provide,issue,negative,neutral,neutral,neutral,neutral,neutral
316177718,"Well I may have pulled the code some time in May.

It looks like the last change to autosklearn's OHE implementation on master is from March - https://github.com/automl/auto-sklearn/commits/master/autosklearn/pipeline/implementations/OneHotEncoder.py.",well may code time may like last change implementation master march,issue,positive,neutral,neutral,neutral,neutral,neutral
316145513,"Hey, sorry for the radio silence. The RF was adversely affected, but not to a degree that would *usually* be significant (1.5% accuracy). However, there are many times when a lot of KNN's are tried in the initial population in which some random combination or parameters leads to a good accuracy. However, that result is truly due only to randomness, and fails completely with a different random_state.

The KNN example is a more insidious one, since if you have a small and noisy training set (let's say ~1000 samples), the mechanism of evolution in TPOT will carry that ""good"" KNN through and continue to mutate it as it's still the top genome; the rest of the mutations will build off it it in a way that makes it very difficult for the Praeto front to break away form it (at least this is what I observed empirically). Changing the random_state at each iteration and re-calculating the cross_val_score fixes this issue. The randomly good genomes fall away very quickly and it made my problem tractable. It also requires only a few lines changed in base.py. I would recommend adding it as an option for small, noisy datasets, or I can maintain a separate fork with these capabilities.

",hey sorry radio silence adversely affected degree would usually significant accuracy however many time lot tried initial population random combination good accuracy however result truly due randomness completely different example insidious one since small noisy training set let say mechanism evolution carry good continue mutate still top genome rest build way difficult front break away form least iteration issue randomly good fall away quickly made problem tractable also would recommend option small noisy maintain separate fork,issue,positive,positive,neutral,neutral,positive,positive
316015660,"@teaearlgraycold, think you can PR this early this week? Would be good to get this issue finished before we start anything larger.",think early week would good get issue finished start anything,issue,negative,positive,positive,positive,positive,positive
316009972,"Closing this issue for now. Please feel free to re-open if you have any more questions or comments.

We look forward to your suggestions for improvement, especially in the docs. :-)",issue please feel free look forward improvement especially,issue,positive,positive,positive,positive,positive,positive
316008767,OK. Please feel free to re-open the issue (or comment further) if you have any more questions about pickling TPOT.,please feel free issue comment,issue,positive,positive,positive,positive,positive,positive
315712317,"Dropping an idea here while it's on my mind: Maybe the original approach to TPOTEnsemble is not good because it requires too many expensive evaluations every generation. Perhaps a better approach would be similar to what @lacava does in FEW:

1) Take entire TPOT population and stack the outputs into a feature matrix
2) Fit a regularized (Lasso, preferably) linear model on the feature matrix
3) Use the linear model coefficients as the fitness of each pipeline

After the first generation, all pipelines with a 0 coefficient will be removed from the TPOT ensemble.

At generation 1 (and beyond), all pipelines in the new population will be added to the TPOT ensemble along with the surviving pipelines currently in the TPOT ensemble. Stack all of the outputs, fit a regularized linear model, and again use the coefficients as the fitness.

Maybe something we can collaborate on, @lacava?",dropping idea mind maybe original approach good many expensive every generation perhaps better approach would similar take entire population stack feature matrix fit lasso preferably linear model feature matrix use linear model fitness pipeline first generation coefficient removed ensemble generation beyond new population added ensemble along surviving currently ensemble stack fit linear model use fitness maybe something collaborate,issue,positive,positive,positive,positive,positive,positive
315544348,"TPOT doesn't currently support multi-label classification problems, only multi-class problems. TPOT performs a check at the beginning of a run that includes a check to ensure that the labels are a 1-D array, which is likely why you're receiving the error you quoted.

This question is related to #196.",currently support classification check beginning run check ensure array likely error question related,issue,negative,neutral,neutral,neutral,neutral,neutral
314793927,"Yes, the `_pre_test` is to check correctness of the parameter combinations in the operators within pipeline since these parameter combinations are randomly generated in these three functions. For example, dual formulation is only implemented for l2 penalty in `sklearn.linear_model.LogisticRegression`, but the unsupported combination (`dual=True, penalty='l1'` in this case) can be randomly generated and cause a invalid pipeline in optimization process. `_pre_test` was designed to prevent it. 

But I hope there is a better way to check the correctness of parameter combinations without using `fit()`",yes check correctness parameter within pipeline since parameter randomly three example dual formulation penalty unsupported combination case randomly cause invalid pipeline optimization process designed prevent hope better way check correctness parameter without fit,issue,positive,negative,neutral,neutral,negative,negative
314549950,"I tested the solution of `forkserver` when n_jobs > 1 and it at least solved the freezing issue when using TPOT on super large datasets. I put the demo in [a branch](https://github.com/weixuanfu2016/tpot/tree/mp_forkserver), which can only be tested in Linux and MacOS with python 3.4+",tested solution least freezing issue super large put branch tested python,issue,positive,positive,neutral,neutral,positive,positive
314541020,"@dnuffer @CSNoyes Thank you for feedbacks. 

@rhiever and I had second thoughts about this issue. We thought this issue might be related to [the start methods in multiprocessing](http://scikit-learn.org/stable/faq.html#why-do-i-sometime-get-a-crash-freeze-with-n-jobs-1-under-osx-or-linux). I also reproduced the freezing issue when n_jobs  >1 in MacOS and Linux but it seems everything is all right when n_jobs = 1. 

@dnuffer @CSNoyes Could you please let me know the sys environment, python version and n_jobs settings when this issue happened before? Thanks. ",thank second issue thought issue might related start also freezing issue everything right could please let know environment python version issue thanks,issue,positive,positive,positive,positive,positive,positive
314257890,Check your gcc version. I solved my problem like that,check version problem like,issue,negative,neutral,neutral,neutral,neutral,neutral
314192145,"The current version (0.8.3) of TPOT do not supported sparse matrix as input array and `np.isnan` is for 
imputation function in TPOT. In the future version of TPOT, we may add the support for sparse matrix since one hot encoder supports sparse matrix (related PR #462 has been merged under dev branch)

For now, please try to use `X.toarray()` to convert to a dense numpy array.
",current version sparse matrix input array imputation function future version may add support sparse matrix since one hot sparse matrix related dev branch please try use convert dense array,issue,positive,positive,neutral,neutral,positive,positive
314080867,"The DeprecationWarning should be from the old version of xgboost. You may need rebuild the latest xgboost from source codes and then install xgboost python module, for solving this problem.",old version may need rebuild latest source install python module problem,issue,negative,positive,positive,positive,positive,positive
314059691,"I have the same problem


C:\Python27\lib\site-packages\sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning",problem module version favor module class also note interface new different module module removed module removed,issue,negative,positive,neutral,neutral,positive,positive
313829985,"When did you port the code? I fixed an issue at some point between March and May. If the code is older than that, you may consider updating it? Also, could you please post a link? I can't find the code with the github search?",port code fixed issue point march may code older may consider also could please post link ca find code search,issue,negative,positive,positive,positive,positive,positive
313815168,"@rhiever Thanks for answer! I was just think about this, but also not sure if this function is fit on this project.",thanks answer think also sure function fit project,issue,positive,positive,positive,positive,positive,positive
313752782,"@rhiever :  For my specific case, your suggestion works perfectly, thank you. I defer the closure (or non-closure) of this issue to you.

Thanks!",specific case suggestion work perfectly thank defer closure issue thanks,issue,positive,positive,positive,positive,positive,positive
313749235,"I believe this is a known issue for XGBoost. It's historically been difficult to install XGBoost on Windows. I did hear recently that they plan to add better support for Windows via conda, though.",believe known issue historically difficult install hear recently plan add better support via though,issue,negative,neutral,neutral,neutral,neutral,neutral
313748929,"@tjvananne, it does indeed seem to be difficult to pickle (or even dill) the TPOT object because of how we're generating classes on-the-fly for TPOT's internal pipeline representation. We've had a long-standing issue to implement a serialization/checkpointing feature (#79), which I agree would be useful for some use cases, but no one has taken it on.

That said, the dev branch has a new feature that regularly outputs the best pipeline every (configurable) number of generations to a (configurable) directory. That's one step toward better serialization.",indeed seem difficult pickle even dill object generating class internal pipeline representation issue implement feature agree would useful use one taken said dev branch new feature regularly best pipeline every number directory one step toward better serialization,issue,positive,positive,positive,positive,positive,positive
313747478,"Sorry for the slow response on this issue. First to answer @ktran9891's question. If all you want to do is pickle the best fitted pipeline at the end of the run, instead of pickling the entire TPOT object, you can pickle just the best pipeline stored in the `fitted_pipeline_` attribute. That attribute stores the best fitted pipeline from the run, which is what TPOT uses to make the predictions etc when you call `predict`. See the [API docs](http://rhiever.github.io/tpot/api/#regression) for more info.",sorry slow response issue first answer question want pickle best fitted pipeline end run instead entire object pickle best pipeline attribute attribute best fitted pipeline run make call predict see,issue,positive,positive,positive,positive,positive,positive
313746607,"TPOT integrates directly with the scikit-learn API, so you could use a package like [scikit-plot](https://github.com/reiinakano/scikit-plot) and scikit-learn's plotting functions to generate reports of the results. I'm not sure if it's currently in the scope of TPOT to generate reports itself; even if we did develop something like that, it'd probably be best to develop it as a separate package that works with any scikit-learn estimator. (Unless, of course, you're speaking of reports about things specific to TPOT, such as the pipelines explored, etc.)",directly could use package like plotting generate sure currently scope generate even develop something like probably best develop separate package work estimator unless course speaking specific,issue,positive,positive,positive,positive,positive,positive
313209130,"@weixuanfu2016 Tested on OSX 10.12 and Ubuntu 14.04 with high dimensionality dataset (I think poly features was getting stuck), looks good so far. Will update if it creeps back in.",tested high dimensionality think poly getting stuck good far update back,issue,negative,positive,positive,positive,positive,positive
313158406,"I just posted a PR #522 and use the way in `hyperopt-sklearn` to kill child process. Could you try this branch and let us know if that corrects your issue using the command below? @dnuffer @CSNoyes 

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@timeout_pipe
```

@rhiever  For this way, I need use `threading` backend in joblib instead of `multiprocessing`. It maybe not as efficient as before in parallel computing.

One drewback is that CTRL+C only works in Linux and Mac but not in Windows. So I add a warning message about it.
",posted use way kill child process could try branch let u know issue command pip install upgrade way need use instead maybe efficient parallel one work mac add warning message,issue,negative,neutral,neutral,neutral,neutral,neutral
313109855,"Hmm, I am not sure what is wrong in code about `np.genfromtxt`. I tried `load_train_data` with the MNIST download from this [link](https://pjreddie.com/projects/mnist-in-csv/) and no error happened. 

Maybe your issue is related to #492 . Could you please try `pandas` module instead of `numpy` for reading input data? Below is a demo:

```
import pandas as pd

tpot_data = pd.read_csv('/home/andrewcz/tpot/tutorials/data/titanic_train.csv')
features = tpot_data.drop('class', axis=1).values
training_features, testing_features, training_classes, testing_classes = 
                        train_test_split(features, tpot_data['class'].values, random_state=42)
```",sure wrong code tried link error maybe issue related could please try module instead reading input data import,issue,negative,neutral,neutral,neutral,neutral,neutral
313039979,I can report similar issues. Using the development branch also does not fix it.,report similar development branch also fix,issue,negative,neutral,neutral,neutral,neutral,neutral
312680979,"@rhiever When I try conda update scikit-learn, I am getting this error

`PackageNotInstalledError: Package is not installed in prefix.
  prefix: /anaconda
  package name: scikit-learn

`",try update getting error package prefix prefix package name,issue,negative,neutral,neutral,neutral,neutral,neutral
312512926,"I tried the development branch, and it didn't fix the issue. I'm think it's probably because the stopit module is a pure python solution and so can only interrupt a thread once it runs some python code, and since the core of most ml training algorithms is written non-python code, stopit won't get a chance to interrupt the thread. 

I also tried my earlier suggestion, but it doesn't work because a pool of processes is used, and a process doesn't exit once an evaluation is complete, leaving the threads running.

I have successfully used the timeout feature in hyperopt-sklearn, and so I dug into how it works. 
This is the code: https://github.com/hyperopt/hyperopt-sklearn/blob/master/hpsklearn/estimator.py
The `trial_timeout` variable controls how long each trial is allowed to run. Then `fn_with_timeout()` is where the action happens. Each trial is run in a separate process (using multiprocessing.Process) using a Pipe for communicating the result at the end of `_cost_fn()`.  When a timeout happens the child process is terminated ensuring a certain and clean exit.",tried development branch fix issue think probably module pure python solution interrupt thread python code since core training written code wo get chance interrupt thread also tried suggestion work pool used process exit evaluation complete leaving running successfully used feature dug work code variable long trial run action trial run separate process pipe communicating result end child process certain clean exit,issue,positive,positive,positive,positive,positive,positive
312434904,"I was just about to ask this same question: 

The ""**warm start**"" feature is really awesome, but it would be significantly better if there were also some way to save/serialize  your current TPOT object as-is and then be able to load it back into memory later.

When I tried pickling my TPOT object that had already processed a few generations, I received this error:

    cannot serialize '_io.TextIOWrapper' object

I'm not sure which portion of the TPOT object is ""unpickle-able"", but this functionality would be very valuable.

The next thing I might try is:
1) run the fit method until I process a few generations
2) pickle the dictionary that holds the previously attempted pipelines and results (I forget the attribute name)
    - **(some process that removes the TPOT object from memory to mimic a computer shut-down)**
3) instantiate a new TPOT object and assign it the previously pickled dictionary object of results
4) call ""fit"" method on the newly created TPOT object that now has access to previous results

I might be overlooking a simpler solution, however.
",ask question warm start feature really awesome would significantly better also way current object able load back memory later tried object already received error serialize object sure portion object functionality would valuable next thing might try run fit method process pickle dictionary previously forget attribute name process object memory mimic computer new object assign previously dictionary object call fit method newly object access previous might simpler solution however,issue,positive,positive,positive,positive,positive,positive
312247154,"Hmm, not sure why the conda recipe of xgboost is not working in Windows. Maybe you need install xgboost following [official installation guide](http://xgboost.readthedocs.io/en/latest/build.html)",sure recipe working maybe need install following official installation guide,issue,negative,positive,positive,positive,positive,positive
312164633,"thanks a lot @weixuanfu2016 , i confirmed it is a xgboost error. after i uninstalled xgboost, everything works fine. then i reinstalled xgboost, the error still exists. my xgboost version:

C:\Users\tiefan>conda install py-xgboost
Fetching package metadata ...........
Solving package specifications: .

Package plan for installation in environment C:\Program Files\Anaconda3:

The following NEW packages will be INSTALLED:

    py-xgboost: 0.60-py35np112h24854b6_0

Proceed ([y]/n)? y",thanks lot confirmed error uninstalled everything work fine error still version install fetching package package package plan installation environment following new proceed,issue,negative,positive,positive,positive,positive,positive
311938580,"It seems a Xgboost import error. Could you try the command `from xgboost import XGBClassifier`? I think it may reproduce the error. If so, please uninstall or reinstall xgboost. If not, please let us know which version of xgboost is in your environment.",import error could try command import think may reproduce error please reinstall please let u know version environment,issue,negative,neutral,neutral,neutral,neutral,neutral
311872745,"Thanks @weixuanfu2016 that works perfectly. The key line here is 
`deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)`

Worth noting for others that the tpot object can be an unfit object of the same model (TPOTClassifier or TPOTRegressor) used for tpot._pset.

I'll try to find some time this weekend to work on the documentation if that is something you guys would find helpful. I've discovered a lot of the functionality through trial and error, I think that there's room for improvement in that department.",thanks work perfectly key line worth object unfit object model used try find time weekend work documentation something would find helpful discovered lot functionality trial error think room improvement department,issue,positive,positive,positive,positive,positive,positive
311857684,Note: just edit some codes on the comment above because of messing up `tpot_obj` and `tpot`,note edit comment messing,issue,negative,neutral,neutral,neutral,neutral,neutral
311857382,"For now, we do not have built-in help function to covert string to sklearn pipeline. Please check the demo below. It should be helpful for building a function for this purpose.

```python
import numpy as np
from deap import creator
from sklearn.model_selection import cross_val_score
from tpot.export_utils import generate_pipeline_code

# print part of pipeline dictionary
print(dict(list(tpot.evaluated_individuals_.items())[0:2]))
# print a pipeline and its values
pipeline_str = list(tpot.evaluated_individuals_.keys())[0]
print(pipeline_str)
print(tpot.evaluated_individuals_[pipeline_str])
for pipeline_string in sorted(tpot.evaluated_individuals_.keys()):
    # convert pipeline string to scikit-learn pipeline object
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot._pset)
    sklearn_pipeline = tpot._toolbox.compile(expr=deap_pipeline)
    # print sklearn pipeline string
    sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot._pset), tpot.operators)
    print(sklearn_pipeline_str)
    # Fix random state when the operator allows  (optional) just for get consistent CV score 
    tpot._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)

    try:
        cv_scores = cross_val_score(sklearn_pipeline, training_features, training_target, cv=5, scoring='accuracy', verbose=0)
        mean_cv_scores = np.mean(cv_scores)
    except Exception as e:
        print(e)
        mean_cv_scores = -float('inf')
    print(mean_cv_scores)",help function covert string pipeline please check helpful building function purpose python import import creator import import print part pipeline dictionary print list print pipeline list print print sorted convert pipeline string pipeline object print pipeline string print fix random state operator optional get consistent score try except exception print print,issue,positive,negative,negative,negative,negative,negative
311777819,"The reason we evaluate all TPOT pipelines with a specific `random_state` is so the pipeline will give a consistent score every time they are evaluated. If pipelines gave different scores every time they were evaluated, then it would be problematic for us to rank pipelines because, as you said, some pipelines can just get ""lucky"" with a particular seed. Also, having pipelines with a fixed seed is advantageous because we can use a cache whenever we encounter the same pipeline in the future, which means we can save time by avoiding re-evaluations of pipelines.

I think the general consensus from the ML community is that if an algorithm/pipeline's performance is highly dependent on its random seed, then it is not a very good nor generalizable algorithm/pipeline. As such, we felt comfortable fixing every TPOT pipeline's seed and rely on k-fold CV to properly assess the generalization performance of every pipeline.

I'm actually quite surprised that the pipeline with a RF you mentioned in #513 works dramatically differently with different `random_state` values for the RF. I didn't think RFs were affected that much by their `random_state`, especially with >=100 trees. Can you verify that it wasn't the cross-validation procedure that was causing inconsistencies in the pipeline performance?",reason evaluate specific pipeline give consistent score every time gave different every time would problematic u rank said get lucky particular seed also fixed seed advantageous use cache whenever encounter pipeline future save time think general consensus community performance highly dependent random seed good generalizable felt comfortable fixing every pipeline seed rely properly ass generalization performance every pipeline actually quite pipeline work dramatically differently different think affected much especially verify procedure causing pipeline performance,issue,positive,positive,neutral,neutral,positive,positive
311669039,"@rhiever Do you like this idea? I think a piratical way for adding random seed is to add `random_state` into built-in configuration of operators, like `'random_state': [42, 84, 100,...]` for operators with this parameters. Or we can add this function into `self._add_terminals()`",like idea think piratical way random seed add configuration like add function,issue,positive,negative,negative,negative,negative,negative
311524747,"Fantastic, thank you. A suggestion: for problems where the data is extremely noisy, it might be good to make TPOT reseed the random weights of each model at each generation, such that a model that is good in a very noisy dataset with only a certain weight (i.e. random chance that it did well) will be less likely to be selected as a top performing genome.

For other problems this might not be helpful but I think that an option to have the weights be reseeded/randomized at each call of the scoring function would make TPOT a better candidate for use in problems where the available data is small and quite noisy.",fantastic thank suggestion data extremely noisy might good make reseed random model generation model good noisy certain weight random chance well le likely selected top genome might helpful think option call scoring function would make better candidate use available data small quite noisy,issue,positive,positive,positive,positive,positive,positive
311518420,"I think this issue is about no `random_state` in export codes in TPOT output. Below is a small demo to reproduce the issue. You may also use the demo to add `random_state` to your pipelines for consistent CV score. (Note, the old API of xgboost use `seed` for random seeds.)

```
# coding: utf-8
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from tpot import TPOTClassifier
from tpot.export_utils import generate_pipeline_code, expr_to_tree

data = pd.read_csv(""data.csv"", index_col=False)
features = data.drop(""target"", axis=1).as_matrix()[:,1:]
targets = np.array(data[""target""])
cv = TimeSeriesSplit(n_splits=30)

tpot = TPOTClassifier(generations=2, population_size=10, offspring_size=10, verbosity=2, cv=cv, random_state=42)
tpot.fit(features, targets)
# print out pipeline
print(tpot.fitted_pipeline_)
sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(tpot._optimized_pipeline, tpot._pset), tpot.operators)
print(sklearn_pipeline_str)
sklearn_pipeline = eval(sklearn_pipeline_str, tpot.operators_context)
print(sklearn_pipeline) # should be the same as tpot.fitted_pipeline_
scores = cross_val_score(sklearn_pipeline, features, targets, cv=cv, n_jobs=-1)
print(""Accuracy without random_state:"", scores.mean())
tpot._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)
if 'XGB' in sklearn_pipeline_str: # for old API in XGBClassifier 
    tpot._set_param_recursive(sklearn_pipeline.steps, 'seed', 42)
scores = cross_val_score(sklearn_pipeline, features, targets, cv=cv, n_jobs=-1)
print(""Accuracy with random_state:"", scores.mean())
```",think issue export output small reproduce issue may also use add consistent score note old use seed random import import import import import data target data target print pipeline print print print print accuracy without old print accuracy,issue,negative,negative,neutral,neutral,negative,negative
311459878,"Yes, that should be exactly correct. Pareto-front selection, in addition to the fact that besides the offspring the previous generation is always considered during selection, should handle that case.",yes exactly correct selection addition fact besides offspring previous generation always considered selection handle case,issue,negative,positive,neutral,neutral,positive,positive
311455847,"I discussed the above issue with some folks in the lab and it may be less of a concern for TPOT because it uses the mu+lambda algorithm. In the example I used above, the 3-primitive pipeline would not be replaced on the Pareto front by the 4-primitive variants because it is less complex. So the selection procedure that eliminates half of the population would still keep the 3-primitive pipeline around, as long as there isn't another 3-primitive pipeline that outperforms it.",issue lab may le concern algorithm example used pipeline would front le complex selection procedure half population would still keep pipeline around long another pipeline,issue,negative,negative,negative,negative,negative,negative
311440328,"I made some (mostly) minor comments about the PR. Otherwise, this PR LGTM. 👍 

Here's a broader issue that came to mind while reviewing this PR: Could forcing the GP to always produce unexplored individuals from mutation/crossover hamper the GP algorithm?

For example, let's say the GP finds a nice pipeline with 3 primitives that performs well. It may make some variants of that pipeline---say, a couple offspring that add 1 primitive to the pipeline each, so these will be 4-primitive offspring.

However, what if the best pipeline is actually a version of the 3-primitive pipeline with 1 of the primitives removed? Mutation/crossover won't be able to change those pipelines back to the original 3-primitive pipeline, so the GP will be challenged to find a way to jump to the ideal 2-primitive pipeline from the 4-primitive offspring (or from another individual).

Basically: It's probable that having copies of already-evaluated pipelines in the population---especially good pipelines---can be useful as material for mutations and crossover to work on. I think before we merge this PR (and after we settle the code), we'll have to do some thorough comparisons between TPOT with and without this change to make sure that it doesn't hamper TPOT's abilities to explore the pipeline space.",made mostly minor otherwise issue came mind could forcing always produce unexplored hamper algorithm example let say nice pipeline well may make pipeline couple offspring add primitive pipeline offspring however best pipeline actually version pipeline removed wo able change back original pipeline find way jump ideal pipeline offspring another individual basically probable population good useful material crossover work think merge settle code thorough without change make sure hamper explore pipeline space,issue,positive,positive,positive,positive,positive,positive
311403555,">Or would you for example then maybe allow an extra generations value (none or so) to indicate max_time_mins should be the only stopping criteria?

That seems like a good idea to support both use cases.",would example maybe allow extra value none indicate stopping criterion like good idea support use,issue,positive,positive,positive,positive,positive,positive
311402961,"This sklearn issue seems relevant to our conversations here: https://github.com/scikit-learn/scikit-learn/pull/9012

Maybe a better way to accomplish what we want in the sklearn Pipeline architecture.",issue relevant maybe better way accomplish want pipeline architecture,issue,positive,positive,positive,positive,positive,positive
311396981,"Is text classification such a fundamentally different problem type that it requires a new TPOT class? Once the text is converted to a bag-of-[words, ngrams, etc.] representation, we're working with a regular feature matrix again. Maybe it'll be a sparse matrix, but it's still a feature matrix.

Here's an example of using sklearn pipelines to CountVectorize (etc) specific columns in a dataset: [link](http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html)

Although that seems to rely on the data being passed as a dictionary, I think we could have TPOT recognize what columns are text and apply the vectorizers specifically to those columns. Maybe via wrapped versions of CountVectorizer etc?",text classification fundamentally different problem type new class text converted representation working regular feature matrix maybe sparse matrix still feature matrix example specific link although rely data dictionary think could recognize text apply specifically maybe via wrapped,issue,negative,positive,neutral,neutral,positive,positive
311388749,"@weixuanfu2016's PR that should fix this issue is merged on to the development branch. @dnuffer, can you try the dev branch and let us know if that corrects your issue?",fix issue development branch try dev branch let u know issue,issue,negative,neutral,neutral,neutral,neutral,neutral
311388243,"Seems reasonable to add the imputation step to the exported pipeline if missing data was in the training data. Perhaps we can store an internal boolean variable indicating whether missing data was in the training data, and add the Imputer to the exported pipeline if True?

cc @teaearlgraycold ",reasonable add imputation step pipeline missing data training data perhaps store internal variable whether missing data training data add imputer pipeline true,issue,negative,positive,neutral,neutral,positive,positive
311264501,"I think that the best solution here is to decompose `TPOTBase.fit()` into a series of smaller functions, then implement a separate fit function for each of `TPOTClassifier`, `TPOTRegressor` and a new `TPOTTextClassifier` - each using the small functions to reduce code duplication. That way there doesn't need to be a ton of code like:

```python
if self._text:
    # foo
else:
    # bar
```

Might need to do something similar for `_evaluate_individual()`.",think best solution decompose series smaller implement separate fit function new small reduce code duplication way need ton code like python foo else bar might need something similar,issue,positive,positive,positive,positive,positive,positive
311237601,"@rhiever, as far as I know, yes, HashingVectorizer takes input data the same way as the other two vectorizers. Even though TFIDF and Count vectorizers are more popular for text classification with ML, Hashing vectorizer is mostly used when the text corpus is large.",far know yes input data way two even though count popular text classification mostly used text corpus large,issue,positive,positive,positive,positive,positive,positive
311214347,"Hmm, the default imputer `Imputer(strategy=""median"", axis=1)` in TPOT is simple as a step of preprocessing if imputer is not specified. The current version of TPOT won't export the imputation step. Maybe we should add this step in exported codes as demo below? @rhiever 

```
from sklearn.preprocessing import Imputer
imputer=Imputer(strategy=""median"", axis=1)
imputer.fit(features)
imputed_features = imputer.transform(features)
```",default imputer imputer median simple step imputer current version wo export imputation step maybe add step import imputer median,issue,negative,neutral,neutral,neutral,neutral,neutral
311188654,"Okay, changes are made and unit tests added. I also merged what was needed after PR #510 was made (not much). Crossover now works between any two individuals that are distinct from each other but share at least one primitive.

Only thing that would need clarification is the desired behavior if:
* the population is of size 0 or 1.
* there are no shared primitives between any two individuals in the population.

The first should not happen, the second is very unlikely but could happen.
Edit: I am also not entirely sure why coverage for `gp_deap.py` has decreased, I can't tell which lines were covered before but now are not? The methods I worked in all are completely covered, though perhaps somehow old tests were affected?

Edit: FWIW I once again evaluated the amount of duplicates and cache hits with this new selection for crossover, and they seem to be the same at 2\~3% duplicates in a generation, 1\~2% cache hits (did not ran many trials, though).",made unit added also made much crossover work two distinct share least one primitive thing would need clarification desired behavior population size two population first happen second unlikely could happen edit also entirely sure coverage ca tell covered worked completely covered though perhaps somehow old affected edit amount cache new selection crossover seem generation cache ran many though,issue,positive,positive,positive,positive,positive,positive
311184972,"Agree. Maybe we need `fit_params` in fit() to organize`weight`, `groups`  and `text_input` parameters as scikit-learn does.",agree maybe need fit organize weight,issue,positive,positive,positive,positive,positive,positive
311181227,"That'd work, but would make for an even messier fit function

On Mon, Jun 26, 2017, 4:07 PM Randy Olson <notifications@github.com> wrote:

> @teaearlgraycold <https://github.com/teaearlgraycold>, taking your
> example, it could work like this:
>
> my_text = ""..."".split(""\n"")
> class_labels = [1, 0, 0, 1, 1, ..., 0, 0]# Assuming len(my_text) == len(class_labels)
>
> my_tpot = TPOTClassifier(..., config='TPOT text', ...)
> my_tpot.fit(my_text, class_labels)
>
> We would indeed need to change the dataset validation procedure when config='TPOT
> text'.
>
> @ziarahman <https://github.com/ziarahman>, I haven't used the
> HashingVectorizer. Does it take input data in the same format as the other
> two vectorizers?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/507#issuecomment-311167235>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ADISY7Tv-p6pgLz7b_TgLB_8jB4eJOAtks5sIA9vgaJpZM4OEWJL>
> .
>
",work would make even fit function mon randy wrote taking example could work like assuming text would indeed need change validation procedure text used take input data format two reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
311180461,"how about `fit(features (non-text data), text, target)`? CountVectorizer & TFIDFVectorizer can be used on text data only and then stacking them in pipeline as below:

```
pipeline = make_pipeline(
    StackingText(CountVectorizer(input_text), input_matrix),
    TPOTClassifier()
)
```

",fit data text target used text data pipeline pipeline,issue,negative,positive,positive,positive,positive,positive
311168577,Future note (also to @teaearlgraycold): Please don't rebuild the docs in future PRs. That's an easy way to get ugly merge conflicts.,future note also please rebuild future easy way get ugly merge,issue,negative,negative,neutral,neutral,negative,negative
311167235,"@teaearlgraycold, taking your example, it could work like this:

```python
my_text = ""..."".split(""\n"")
class_labels = [1, 0, 0, 1, 1, ..., 0, 0]
# Assuming len(my_text) == len(class_labels)

my_tpot = TPOTClassifier(..., config='TPOT text', ...)
my_tpot.fit(my_text, class_labels)
```

We would indeed need to change the dataset validation procedure when `config='TPOT text'`. I wonder how we could get it to support a mix of text *and* non-text data, though.

@ziarahman, I haven't used the HashingVectorizer. Does it take input data in the same format as the other two vectorizers?",taking example could work like python assuming text would indeed need change validation procedure text wonder could get support mix text data though used take input data format two,issue,positive,neutral,neutral,neutral,neutral,neutral
311146889,"Update: 

After more tests, freezing issue #436 and #422 are also fixed somehow in Windows OS (~6s/pipeline) using test codes below. But for Linux OS, it is ~22s/pipeline.

```
# coding: utf-8
from sklearn.datasets import make_classification
from tpot import TPOTClassifier
# make a huge dataset
X, y = make_classification(n_samples=50000, n_features=200,
                                    n_informative=20, n_redundant=20,
                                    n_classes=5, random_state=42)

# max_eval_time_mins=0.1 means 6 seconds limits for evaluating a single pipeline 
tpot = TPOTClassifier(generations=5, population_size=50, offspring_size=100, random_state=42, n_jobs=1, max_eval_time_mins=0.1, verbosity=3) 
tpot.fit(X, y)
```",update freezing issue also fixed somehow o test o import import make huge single pipeline,issue,negative,positive,positive,positive,positive,positive
310885209,"@weixuanfu2016, I like the idea of 'new text classification mode' .

@rhiever , thank you for opening this up for discussion. You mentioned CountVectorizer & TFIDFVectorizer. What about [HashingVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html#sklearn.feature_extraction.text.HashingVectorizer)?",like idea text classification mode thank opening discussion,issue,positive,neutral,neutral,neutral,neutral,neutral
310871648,Thank you for suggestion. I will look into it. One of my branches uses `stopit` module for timeout function. I need to check if it is better than daemon thread solution.,thank suggestion look one module function need check better daemon thread solution,issue,positive,positive,positive,positive,positive,positive
310861382,Maybe we need a new text classification mode rather than a config file for transforming text input by the two operators.,maybe need new text classification mode rather file transforming text input two,issue,negative,positive,positive,positive,positive,positive
310860639,"I checked it out. It was indeed a crash caused by a population consisting of only individuals with one primitive. What would be the desired behavior here? Especially when working with a small population it could be realistic that this comes up in practice.

I think I should be able to create a new selection procedure which always selects two individuals that can perform crossover, even if they have only one primitive (the same one). I started on it now, but probably won't be able to continue until monday. The simple approach to do this would be:
(1) select a random individual
(2) find all primitives in this individual
(3) find all other individuals in the population with at least one matching primitive\*
(4) select a random individual from those individuals
(5) perform crossover
This should be linear time in the total length of all pipelines, so I think that is quite manageable.
\* Technically should be one matching terminal, but the only terminal that exists that would match between individuals that do not share a primitive would be the `input_matrix` and only swapping those out does not create a new individual.

**Edit** 
Actually you would want to just construct all allowed pairs instead, because otherwise you might get unlucky with your first individual not matching with any others, while there are viable pairs. The naive way would be quadratic, but perhaps based on sorting one can do better? I am not sure. New procedure:
(1) for each individual in the population, see which primitives it has
(2) find all pairs of individuals which have at least one primitive in common, and are not the same individual
(4) select a random pair from those
(5) perform crossover
**End edit**

**Edit 2**
Got it done already, but I do not have the time to go through and test it right now. It does not crash any extra unit tests, but I would like to add a few for this to make sure it handles all cases well (coding late is not always a success ;) ). See the code [here](https://github.com/PG-TUe/tpot/blob/no_repeats/tpot/gp_deap.py#L41-L68).
**End edit**

That said, if there is a population in which no individual shares any primitives, this still would crash.
In practice this should probably not come up as (1) populations are quite big, so duplicate primitives should be there and (2) pipelines would probably always tend to use a similar set of algorithms that perform well for a certain dataset.
If we do find such an odd population, we can either (1) allow crossover anyway (switching input matrices, doing nothing) or (2) disable crossover and only allow mutation.
I prefer option two because is more likely to introduce individuals with shared primitives for the next generation (but again, this probably does not come up in practice).",checked indeed crash population one primitive would desired behavior especially working small population could realistic come practice think able create new selection procedure always two perform crossover even one primitive one probably wo able continue simple approach would select random individual find individual find population least one matching select random individual perform crossover linear time total length think quite manageable technically one matching terminal terminal would match share primitive would swapping create new individual edit actually would want construct instead otherwise might get unlucky first individual matching viable naive way would quadratic perhaps based one better sure new procedure individual population see find least one primitive common individual select random pair perform crossover end edit edit got done already time go test right crash extra unit would like add make sure well late always success see code end edit said population individual still would crash practice probably come quite big duplicate would probably always tend use similar set perform well certain find odd population either allow crossover anyway switching input matrix nothing disable crossover allow mutation prefer option two likely introduce next generation probably come practice,issue,positive,positive,neutral,neutral,positive,positive
310857545,"Okay, I understood that wrong then.
Does this take the ability away for a user to specify a certain duration (and not generations)?
Or would you for example then maybe allow an extra generations value (`none` or so) to indicate `max_time_mins` should be the only stopping criteria?",understood wrong take ability away user specify certain duration would example maybe allow extra value none indicate stopping criterion,issue,positive,negative,neutral,neutral,negative,negative
310852919,"How would the text be passed to `fit`/`transform`? Right now we only handle float values for features, but both of these operators expect blocks of text.

Without any major changes to TPOT you'd only be able to use either of these operators in an external pipeline:

```python
my_text = ""..."".split(""\n"")

pipeline = make_pipeline(
    CountVectorizer(),
    TPOTClassifier()
)
pipeline.fit(my_text, ...)
```",would text fit transform right handle float expect text without major able use either external pipeline python pipeline,issue,negative,positive,positive,positive,positive,positive
310843135,That's a strange one. You can drop a debugger at that line and run the specific unit test for that logical branch to see what's going on. A crash seems to be the logical explanation. I think our unit tests sometimes squash the errors (though they shouldn't).,strange one drop line run specific unit test logical branch see going crash logical explanation think unit sometimes squash though,issue,negative,positive,positive,positive,positive,positive
310843028,"In my proposed solution, if the user specifies `max_time_mins` *and* `generations`, then TPOT would quit either when `max_time_mins` is exceeded *or* `generations` iterations have passed. So TPOT could take less than `max_time_mins` if it completes `generations` iterations before that time limit, i.e., it won't continue until `max_time_mins` has elapsed as it currently does.",solution user would quit either could take le time limit wo continue currently,issue,negative,neutral,neutral,neutral,neutral,neutral
310833807,I also tested in a Win 7 virtual machine and the issue did not happen. Maybe it is just a conda environment issue. You could close it for now. ,also tested win virtual machine issue happen maybe environment issue could close,issue,positive,positive,positive,positive,positive,positive
310821170,"Thanks for the suggestion @weixuanfu2016 . 
Changing from Anaconda 4.3.x to 4.4.x did not solve the issue. 
I am able to install ```tpot``` outside the environment. That is sufficient for now. ",thanks suggestion anaconda solve issue able install outside environment sufficient,issue,positive,positive,positive,positive,positive,positive
310795721,">Perhaps what we can do is have max_time_mins not override generations. If the user only sets max_time_mins=30 and leaves the rest of the TPOT parameters as default, then TPOT will run for 100 generations of 100 population and only be interrupted if the process takes >=30 minutes.

And for user specified number of generations, combined with `max_time_mins`, just have the current behavior? In that case I would just leave it as is and have the behavior be consistent.

But you are probably right in that specifying by generations is probably not that important in practice (compared to specifying by time).",perhaps override user leaf rest default run population interrupted process user number combined current behavior case would leave behavior consistent probably right probably important practice time,issue,negative,positive,positive,positive,positive,positive
310768994,"Okay, I looked at the coverage, I am not sure why [line 85-93](https://coveralls.io/builds/12109785/source?filename=tpot%2Fgp_deap.py#L85) is not covered if 84 is (only reason would be a crash? but then this should follow from test results?).

I will add unit tests for mutate and mate operators.",coverage sure line covered reason would crash follow test add unit mutate mate,issue,negative,positive,positive,positive,positive,positive
310753233,This PR looks promising and I'm generally supportive of what it implements. I will try to review it early next week so we can get it merged.,promising generally supportive try review early next week get,issue,positive,positive,positive,positive,positive,positive
310752211,"You're indeed right that the `generations` and `max_time_mins` parameters are in conflict with one another. When we implemented `max_time_mins`, we purposely decided to have `max_time_mins` override `generations` because `max_time_mins` is a more practical way to tell TPOT how long it has to run its optimization procedure, whereas `generations` is more for users like ourselves who want to run fixed-evaluation-count experiments because we're comparing optimization methods.

I think it's technically possible to achieve your goal by setting `max_eval_time_mins` to (`max_time_mins` / (`population_size` x `generations`) ). If `max_eval_time_mins` is indeed killing the evaluations on time, then the run should take no longer than the desired maximum amount of time, and will still run for the desired number of generations.

If we can figure out a good way for these parameters to interact, I'm not opposed to tweaking it slightly. But I'd very much prefer to avoid adding another parameter.

Perhaps what we can do is have `max_time_mins` *not* override `generations`. If the user only sets `max_time_mins=30` and leaves the rest of the TPOT parameters as default, then TPOT will run for 100 generations of 100 population and only be interrupted if the process takes >=30 minutes.",indeed right conflict one another purposely decided override practical way tell long run optimization procedure whereas like want run optimization think technically possible achieve goal setting indeed killing time run take longer desired maximum amount time still run desired number figure good way interact opposed slightly much prefer avoid another parameter perhaps override user leaf rest default run population interrupted process,issue,positive,positive,positive,positive,positive,positive
310748050,"I noticed that the development branch no longer has `tests.py`, how should I submit a new test for my code?

Here is a code which covers the clean_pipeline_string function:
```
def test_clean_pipeline_string():
    """"""Assert that clean_pipeline_string correctly returns a string without parameter prefixes""""""

    with_prefix = 'BernoulliNB(input_matrix, BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=True)'
    without_prefix = 'BernoulliNB(input_matrix, alpha=1.0, fit_prior=True)'
    tpot_obj = TPOTClassifier()
    ind1 = creator.Individual.from_string(with_prefix)
    
    pretty_string = tpot_obj.clean_pipeline_string(ind1)
    assert pretty_string == without_prefix
```",development branch longer submit new test code code function assert correctly string without parameter assert,issue,negative,positive,positive,positive,positive,positive
310743006,the scores in `tpot.evaluated_individuals_` is average CV scores from [`cross_eval_score`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html). The output of `tpot.score` is fitness score of the best pipeline (see [these lines](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L666-L671)) @dartdog ,average output fitness score best pipeline see,issue,positive,positive,positive,positive,positive,positive
310735701,"Further,, (sorry)
when running the same example i get:

```Generation 7 - Current best internal CV score: 0.9904761904761905

Best pipeline: LinearSVC(BernoulliNB(input_matrix, BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), LinearSVC__C=DEFAULT, LinearSVC__dual=True, LinearSVC__loss=squared_hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.1)
0.947368421053```
yet none of the tpot.evaluated_individuals_ results show the same result numbers... and in fact this seems to be the ""best"" from that output:

```BernoulliNB(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=7, DecisionTreeClassifier__min_samples_leaf=2, DecisionTreeClassifier__min_samples_split=3), BernoulliNB__alpha=0.01, BernoulliNB__fit_prior=DEFAULT)```
``` 0      2
    1      0.962771```
",sorry running example get generation current best internal score best pipeline yet none show result fact best output,issue,positive,positive,positive,positive,positive,positive
310731042,"I just created a python 3.6.1 (Anaconda 4.4.0) in my Window OS (using GitBash on Windows 10) environment but the `pip install tpot` works (see the log below). Maybe it is a issue on Windows 7. Could you please also update your conda environment of pyhon 3.6.1 to check if this is a issue of anaconda version?
```
Weixuan@DESKTOP-KVOBB47 MINGW64 /d/Documents/GitHub
$ source activate py36
(py36)
Weixuan@DESKTOP-KVOBB47 MINGW64 /d/Documents/GitHub
$ conda install numpy scipy scikit-learn
Fetching package metadata ...........
Solving package specifications: .

# All requested packages already installed.
# packages in environment at C:\Users\Weixuan\Anaconda3\envs\py36:
#
numpy                     1.12.1                   py36_0
scikit-learn              0.18.1              np112py36_1
scipy                     0.19.0              np112py36_0
(py36)
Weixuan@DESKTOP-KVOBB47 MINGW64 /d/Documents/GitHub
$ pip install deap update_checker tqdm
Collecting deap
  Using cached deap-1.0.2.post2.tar.gz
Collecting update_checker
  Using cached update_checker-0.16-py2.py3-none-any.whl
Collecting tqdm
  Downloading tqdm-4.14.0-py2.py3-none-any.whl (46kB)
Requirement already satisfied: requests>=2.3.0 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from update_checker)
Building wheels for collected packages: deap
  Running setup.py bdist_wheel for deap: started
  Running setup.py bdist_wheel for deap: finished with status 'done'
  Stored in directory: C:\Users\Weixuan\AppData\Local\pip\Cache\wheels\c9\9c\cd\d52106f0148e675df35718c0efff2ecf03cc86d5bdcfb91db5
Successfully built deap
Installing collected packages: deap, update-checker, tqdm
Successfully installed deap-1.0.2 tqdm-4.14.0 update-checker-0.16
(py36)
Weixuan@DESKTOP-KVOBB47 MINGW64 /d/Documents/GitHub
$ pip install tpot
Collecting tpot
  Downloading TPOT-0.8.3.tar.gz (864kB)
Requirement already satisfied: numpy>=1.12.1 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: scipy>=0.19.0 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: scikit-learn>=0.18.1 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: deap>=1.0 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: update_checker>=0.16 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: tqdm>=4.11.2 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from tpot)
Requirement already satisfied: requests>=2.3.0 in c:\users\weixuan\anaconda3\envs\py36\lib\site-packages (from update_checker>=0.16->tpot)
Building wheels for collected packages: tpot
  Running setup.py bdist_wheel for tpot: started
  Running setup.py bdist_wheel for tpot: finished with status 'done'
  Stored in directory: C:\Users\Weixuan\AppData\Local\pip\Cache\wheels\c1\cd\89\4604b70f3d3adb5ddd4a6f32d1255a5a0e9affb241baf5ec6c
Successfully built tpot
Installing collected packages: tpot
Successfully installed tpot-0.8.3
```",python anaconda window o environment pip install work see log maybe issue could please also update environment check issue anaconda version source activate install fetching package package already environment pip install requirement already satisfied building collected running running finished status directory successfully built collected successfully pip install requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied building collected running running finished status directory successfully built collected successfully,issue,positive,positive,positive,positive,positive,positive
310728666,"@weixuanfu2016 The length of an individual is defined by the number of primitives *and* terminals (code you linked). 
You **can** have crossover given two individuals with only one Primitive, try this example where two one-primitive individuals perform crossover by swapping a terminal:

```
from tpot import TPOTClassifier
from deap import creator, gp
tpot = TPOTClassifier()

ind1 = creator.Individual.from_string('BernoulliNB(input_matrix, BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True)', tpot._pset)
ind2 = creator.Individual.from_string('BernoulliNB(input_matrix, BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=False)', tpot._pset)

new1, new2 = gp.cxOnePoint(ind1, ind2)
str(new1)
>>> BernoulliNB(input_matrix, BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=False)
```

So two individuals are eligble for crossover if they have any one terminal in common.
For TPOT, we want to select any two individuals from the population for cross-over as long as either (1)
they both have only one Primitive, but it is the same one (which means same Terminal-types\*) or (2) at least one of them has two Primitives.

I will go ahead and make a PR.

\* technically all one-primitives individuals have one terminal type in common regardless of primitive types, which is the terminal with `input_matrix`, but this is always the same terminal for one-primitive individuals, so crossover does not generate a new individual.",length individual defined number code linked crossover given two one primitive try example two perform crossover swapping terminal import import creator new new new two crossover one terminal common want select two population long either one primitive one least one two go ahead make technically one terminal type common regardless primitive terminal always terminal crossover generate new individual,issue,negative,negative,neutral,neutral,negative,negative
310726092,"I think it is a issue related to python 3.6 in Windows OS. (See the [related issue](https://github.com/pytest-dev/py/issues/103)). For now, please try to use conda install python 3.5 environment (check this [instruction](https://conda.io/docs/py2or3.html#create-a-python-3-5-environment)). We will try to make a patch to fix this issue.",think issue related python o see related issue please try use install python environment check instruction try make patch fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
310722320,Thank you for your suggestion. I agree with the scenario when `stop_at_first_criteria` is `True`. It is a little confused about the scenario when `stop_at_first_criteria` is `False`. I feel that the parameter `stop_at_first_criteria` is not very clear. @rhiever do you think that we should add this function?,thank suggestion agree scenario true little confused scenario false feel parameter clear think add function,issue,positive,negative,negative,negative,negative,negative
310715608,"@PG-TUe Thank you for the fixes.

For crossover, I think only one individual with at least 2 Primitive in a pair can make a crossover. We also need to change [crossover operator](https://github.com/PG-TUe/tpot/blob/0aab429d99b4af2090df5b429c54adb5b3ca5bd9/tpot/gp_deap.py#L230-L240) for this purpose. For example: 
```
PCA-RFC X LR --> PCA-LR + RFC
# Note: PCA-LR is new individual as output ind1 from modified cxOnePoint
```

Though crossover may not make big difference comparing with mutation.

I think you can make a PR for these fixes.



",thank crossover think one individual least primitive pair make crossover also need change crossover operator purpose example note new individual output though crossover may make big difference mutation think make,issue,negative,negative,neutral,neutral,negative,negative
310697355,"@dartdog Will do once the respective pull request is made.
@weixuanfu2016 Let me know if you think I should just send a PR for the fixes as I implemented them.",respective pull request made let know think send,issue,negative,neutral,neutral,neutral,neutral,neutral
310695500,"I am not sure what the best way to do this is.
The first thing that comes to mind is redesigning the parameters this way: replace `max_time_mins` with `duration_mins` and add `stop_at_first_criteria` (bool)

If either only  `duration_mins` or `generations` is specified, then it will run for X minutes or generations, respectively (regardless of `stop_at_first_criteria`).

When both  `duration_mins` are `generations` is specified and `stop_at_first_criteria` is `True`, then 
it will stop when `duration_mins` have elapsed, or  `generations` generations have been evaluated, whichever comes first.

When both  `duration_mins` are `generations` is specified and `stop_at_first_criteria` is `False`, then 
it will stop when `duration_mins` have elapsed *and*  `generations` generations have been evaluated.

Alternatively don't rename `max_time_mins` to `duration_mins`, so it will not break code.
The problem with `max_time_mins` is that the name would only be accurate with stop at first criterion, not stop last criterion.
",sure best way first thing come mind way replace add bool either run respectively regardless true stop whichever come first false stop alternatively rename break code problem name would accurate stop first criterion stop last criterion,issue,negative,positive,positive,positive,positive,positive
310688176,"feel free to close as to not pile things up
and FWIW I just stuck the results in a DF for somewhat pretty presentation..
hmm not rendering the header which has the model params..

   proc=pd.DataFrame(tpot.evaluated_individuals_)
   proc.head()
```BernoulliNB(BernoulliNB(input_matrix, BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=DEFAULT), BernoulliNB__alpha=0.01, BernoulliNB__fit_prior=DEFAULT)	BernoulliNB(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=20, DecisionTreeClassifier__min_samples_split=10), BernoulliNB__alpha=100.0, BernoulliNB__fit_prior=True)	BernoulliNB(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=7, DecisionTreeClassifier__min_samples_leaf=2, DecisionTreeClassifier__min_samples_split=3), BernoulliNB__alpha=0.01, BernoulliNB__fit_prior=DEFAULT)	BernoulliNB(ExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=DEFAULT, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.85, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True)	BernoulliNB(GaussianNB(input_matrix), BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=DEFAULT)	BernoulliNB(LogisticRegression(input_matrix, LogisticRegression__C=1.0, LogisticRegression__dual=False, LogisticRegression__penalty=DEFAULT), BernoulliNB__alpha=100.0, BernoulliNB__fit_prior=True)	BernoulliNB(LogisticRegression(input_matrix, LogisticRegression__C=DEFAULT, LogisticRegression__dual=DEFAULT, LogisticRegression__penalty=l1), BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=False)	BernoulliNB(Normalizer(input_matrix, Normalizer__norm=l2), BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=DEFAULT)	BernoulliNB(RobustScaler(input_matrix), BernoulliNB__alpha=1.0, BernoulliNB__fit_prior=False)	BernoulliNB(RobustScaler(input_matrix), BernoulliNB__alpha=100.0, BernoulliNB__fit_prior=DEFAULT)	...	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=3, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.25)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=18, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.9)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=5, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.35)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=6, XGBClassifier__min_child_weight=9, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.65)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=7, XGBClassifier__min_child_weight=20, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.5)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=9, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=DEFAULT, XGBClassifier__max_depth=1, XGBClassifier__min_child_weight=7, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.1)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=DEFAULT, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=17, XGBClassifier__n_estimators=DEFAULT, XGBClassifier__nthread=1, XGBClassifier__subsample=0.25)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=DEFAULT, XGBClassifier__max_depth=8, XGBClassifier__min_child_weight=2, XGBClassifier__n_estimators=DEFAULT, XGBClassifier__nthread=1, XGBClassifier__subsample=1.0)	XGBClassifier(input_matrix, XGBClassifier__learning_rate=DEFAULT, XGBClassifier__max_depth=DEFAULT, XGBClassifier__min_child_weight=19, XGBClassifier__n_estimators=100, XGBClassifier__nthread=1, XGBClassifier__subsample=0.45)
0	2.00000	2.000000	2.000000	2.0000	2.000000	2.00000	2.000000	2.00000	2.000000	2.000000	...	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.000000	1.00000
1	0.36621	0.750456	0.929903	0.7786	0.669433	0.36621	0.651647	0.36621	0.860926	0.851402	...	0.940114	0.725927	0.893864	0.894297	0.356686	0.946932	0.348353	0.285437	0.938599	0.36621
2 rows × 1410 columns```",feel free close pile stuck somewhat pretty presentation rendering header model normalizer,issue,positive,positive,positive,positive,positive,positive
310684510,"Okay, that makes sense.
This field has only been recently officially exposed, the dump you see is because an internal dictionary simply gets put out to the screen.
For people wanting to access the TPOT objects themselves, this dictionary is useful as is.
I think perhaps the best solution is to also present a nicer, better legible option for those who just want to immediately gain some insight into what type of pipelines have been examined and what their performance was like.

At this point I would like to defer from going into this aspect any further on this specific issue-thread.
However, issues #337 and #459 both talk about the visualization of TPOT results, so feel free to contribute to the conversation there.",sense field recently officially exposed dump see internal dictionary simply put screen people wanting access dictionary useful think perhaps best solution also present better legible option want immediately gain insight type performance like point would like defer going aspect specific however talk visualization feel free contribute conversation,issue,positive,positive,positive,positive,positive,positive
310678715,"actually the output of tpot.evaluated_individuals_ in a more attractive format? It is a bit dump like... 
so maybe a few embedded line breaks? I guess I'm just being too picky...
 
```{'LinearSVC(input_matrix, LinearSVC__C=5.0, LinearSVC__dual=DEFAULT, LinearSVC__loss=hinge, LinearSVC__penalty=DEFAULT, LinearSVC__tol=0.0001)': (1,
  0.94703557312252973),
 'GaussianNB(GaussianNB(input_matrix))': (2, 0.95569358178053831),
 'XGBClassifier(MinMaxScaler(input_matrix), XGBClassifier__learning_rate=1.0, XGBClassifier__max_depth=5, XGBClassifier__min_child_weight=6, XGBClassifier__n_estimators=DEFAULT, XGBClassifier__nthread=1, XGBClassifier__subsample=0.2)': (2,
  0.69819311123658956),
 'BernoulliNB(input_matrix, BernoulliNB__alpha=0.1, BernoulliNB__fit_prior=DEFAULT)': (1,
  0.35761340109166195),
 'GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.1, GradientBoostingClassifier__max_depth=4, GradientBoostingClassifier__max_features=0.3, GradientBoostingClassifier__min_samples_leaf=5, GradientBoostingClassifier__min_samples_split=16, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=1.0)': (1,
  0.9652173913043478),```",actually output attractive format bit dump like maybe line guess picky,issue,negative,positive,positive,positive,positive,positive
310672713,"@dartdog 
Hmm, I am not sure why you get this as output:
```
Optimization Progress: 9%|▊ | 182/2100 [00:23<03:39, 8.76pipeline/s]
Generation 1 - Current best internal CV score: 0.9739130434782609
Optimization Progress: 13%|█▎ | 272/2100 [00:41<05:09, 5.92pipeline/s]
Generation 2 - Current best internal CV score: 0.9739130434782609
Optimization Progress: 17%|█▋ | 361/2100 [01:07<04:52, 5.95pipeline/s]
Generation 3 - Current best internal CV score: 0.9739130434782609
Optimization Progress: 22%|██▏ | 457/2100 [01:35<04:25, 6.19pipeline/s]
Generation 4 - Current best internal CV score: 0.9826086956521738
Optimization Progress: 26%|██▌ | 551/2100 [02:12<04:59, 5.17pipeline/s]
Generation 5 - Current best internal CV score: 0.9826086956521738
Optimization Progress: 31%|███ | 647/2100 [02:55<03:55, 6.16pipeline/s]
.....
Optimization Progress: 81%|████████ | 1692/2100 [07:20<01:11, 5.70pipeline/s]
Generation 17 - Current best internal CV score: 0.9913043478260869
Optimization Progress: 85%|████████▌ | 1786/2100 [08:00<00:55, 5.68pipeline/s]
Generation 18 - Current best internal CV score: 0.9913043478260869
Optimization Progress: 90%|████████▉ | 1883/2100 [08:34<00:30, 7.02pipeline/s]
Generation 19 - Current best internal CV score: 0.9913043478260869
```

I got something like this:

```
Generation 1 - Current best internal CV score: 0.9739130434782609
Generation 2 - Current best internal CV score: 0.9739130434782609
Generation 3 - Current best internal CV score: 0.9739130434782609
Generation 4 - Current best internal CV score: 0.9826086956521738
Generation 5 - Current best internal CV score: 0.9826086956521738
.....
Generation 17 - Current best internal CV score: 0.9913043478260869
Generation 18 - Current best internal CV score: 0.9913043478260869
Generation 19 - Current best internal CV score: 0.9913043478260869
Optimization Progress: 90%|████████▉ | 1883/2100 [08:34<00:30, 7.02pipeline/s]
```

For the final line, we could 'prettify'

```
Best pipeline: DecisionTreeClassifier(FastICA(input_matrix, FastICA__tol=0.25), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=5)
0.842105263158
```
to
```
Best pipeline: DecisionTreeClassifier(FastICA(input_matrix, tol=0.25), criterion=entropy, max_depth=8, min_samples_leaf=19, min_samples_split=5)
0.842105263158
```

Anything else you had in mind?",sure get output optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score got something like generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score optimization progress final line could best pipeline best pipeline anything else mind,issue,positive,positive,positive,positive,positive,positive
310671602,tpot.evaluated_individuals_ looks quite helpful... at this point all I can think of is maybe a way to pretty the output  up?,quite helpful point think maybe way pretty output,issue,positive,positive,positive,positive,positive,positive
310667581,"@weixuanfu2016 I worked on the issue, my work can be found here: https://github.com/PG-TUe/tpot/tree/no_repeats
Specifically I changed:
* Crossover will keep trying to do new crossovers until one that has not been evaluated before has been found.
* Mutation will keep trying to do new crossovers until one that has not been evaluated before has been found.
* Mutation will no longer consider the `mutShrink` operator if there is only one primitive in the pipeline (and hence can not be shrunk).
* For longer pipelines, if all shrunk versions of it have already been considered, it will try to mutate by using a different operator instead.
* For crossover, only individuals with 2 or more primitives are considered. 

The reason I did not commit a pull request yet is because the last point is circumventing a problem, rather than fixing it.
Previously, it could be possible that two individuals with only one Primitive would be passed to crossover, which often meant no crossover could take place, and hence no new individual would be made.
Now, with both individuals needing at least two primitives, crossover can almost always take place.

While two individuals with each only one Primitive can have crossover, they must have the same typed terminals (in our case, this means that it would be the same base learner), in this case cross over will swap out a terminal (in our case, a parameter value).
In practice, often two different learners were matched, and `input_matrix` would be the only terminal that could be exchanged between the two individuals, causing two pipelines which were identical to their parents.
Of course, for any two individuals, no matter their structure, it can always be that all crossover combinations have been tried out before, in which case this solution still introduces an old individual.
The mismatch of 1-primitive-pipelines is now avoided, but this also means that crossover between those two (swapping terminals), is also no longer possible.

The problems of duplicates in a generation was largely removed by the fact that mutation and crossover now almost always provide a new individual, rather than one seen before.

I ran the same setup as yesterday (digits, 10 pop, 50 gens), though I cut it short after 20 generations, because they take much longer now, with all these new individuals ;)
Only one generation contained a duplicate, and there were no cache hits.
I ran an experiment on iris with 100 population and 36 generations (planned 100, but I accidentally had a `max_time_mins` set), and over those 36 generation, each generation had on average 1.8 duplicates and 0.3 cache hits.

I think that in practice, this solution is probably good enough (for now).",worked issue work found specifically crossover keep trying new one found mutation keep trying new one found mutation longer consider operator one primitive pipeline hence shrunk longer shrunk already considered try mutate different operator instead crossover considered reason commit pull request yet last point problem rather fixing previously could possible two one primitive would crossover often meant crossover could take place hence new individual would made needing least two crossover almost always take place two one primitive crossover must case would base learner case cross swap terminal case parameter value practice often two different would terminal could two causing two identical course two matter structure always crossover tried case solution still old individual mismatch also crossover two swapping also longer possible generation largely removed fact mutation crossover almost always provide new individual rather one seen ran setup yesterday pop gen though cut short take much longer new one generation duplicate cache ran experiment iris population accidentally set generation generation average cache think practice solution probably good enough,issue,positive,positive,neutral,neutral,positive,positive
310665444,"@dartdog 
You do not get the line about the new valid individuals because that is something @weixuanfu2016 added himself when trying to evaluate your issue, for his local version of TPOT only. 

> also would be very nice to have more insight as to what is going on internally? 

From the top of my head, I am not sure what is exposed in the most recent version of TPOT, but I think you can access results of all evaluated pipelines with `tpot.evaluated_individuals_`. What kind of extra information would be useful to you? Perhaps we can expose more of the process in a new release.

small tip: to show code blocks, instead of using a single quote, use three, i.e.:
\`\`\` 
my code can be 
many lines now 
\`\`\` 
gives 
```
my code can be
many lines now
```
",get line new valid something added trying evaluate issue local version also would nice insight going internally top head sure exposed recent version think access kind extra information would useful perhaps expose process new release small tip show code instead single quote use three code many code many,issue,positive,positive,positive,positive,positive,positive
310659760,"FYI my new results following the advice here, now shows progress, I do have xxgboost installed so that is why I don't get that message,, but wondering, why don't I get these lines?

    Number of new vaild pipelines in currect generation: 35
also would be very nice to have more insight as to what is going on internally? What is being tried?

    from tpot import TPOTClassifier
``` 
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=20, population_size=100, verbosity=2, random_state=2)
tpot.fit(X_train, y_train)

print(tpot.score(X_test, y_test))
tpot.export('tpot_iris_pipeline.py')
Optimization Progress:   9%|▊         | 182/2100 [00:23<03:39,  8.76pipeline/s]
Generation 1 - Current best internal CV score: 0.9739130434782609
Optimization Progress:  13%|█▎        | 272/2100 [00:41<05:09,  5.92pipeline/s]
Generation 2 - Current best internal CV score: 0.9739130434782609
Optimization Progress:  17%|█▋        | 361/2100 [01:07<04:52,  5.95pipeline/s]
Generation 3 - Current best internal CV score: 0.9739130434782609
Optimization Progress:  22%|██▏       | 457/2100 [01:35<04:25,  6.19pipeline/s]
Generation 4 - Current best internal CV score: 0.9826086956521738
Optimization Progress:  26%|██▌       | 551/2100 [02:12<04:59,  5.17pipeline/s]
Generation 5 - Current best internal CV score: 0.9826086956521738
Optimization Progress:  31%|███       | 647/2100 [02:55<03:55,  6.16pipeline/s]
.....
Optimization Progress:  81%|████████  | 1692/2100 [07:20<01:11,  5.70pipeline/s]
Generation 17 - Current best internal CV score: 0.9913043478260869
Optimization Progress:  85%|████████▌ | 1786/2100 [08:00<00:55,  5.68pipeline/s]
Generation 18 - Current best internal CV score: 0.9913043478260869
Optimization Progress:  90%|████████▉ | 1883/2100 [08:34<00:30,  7.02pipeline/s]
Generation 19 - Current best internal CV score: 0.9913043478260869
                                                                                
Generation 20 - Current best internal CV score: 0.9913043478260869

Best pipeline: DecisionTreeClassifier(FastICA(input_matrix, FastICA__tol=0.25), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=8, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=5)
0.842105263158 ```",new following advice progress get message wondering get number new generation also would nice insight going internally tried import import import import iris print optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score generation current best internal score best pipeline,issue,positive,positive,positive,positive,positive,positive
310632217,"@weixuanfu2016 I actually worked on this before I managed to check in here ^^;;
I'll put up a pull request with my changes when I round it up, then you can compare and see if you want to use some of it.
",actually worked check put pull request round compare see want use,issue,negative,negative,neutral,neutral,negative,negative
310541540,@PG-TUe I will work on this PR and test it since it is just some small changes.,work test since small,issue,negative,negative,negative,negative,negative,negative
310538090,"Thank you for the suggestion. I have a little concern that it might cause a infinite loop. Could you please make a PR for these suggested changes? We could test it later.

> On Jun 22, 2017, at 8:20 PM, PG-TUe <notifications@github.com> wrote:
> 
> I still think the suggested changes to _random_mutation_operator and _mate_operator, or alternatively _pre_test would be a good!
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/503#issuecomment-310536919>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AUG7KmY1_Rlfm4MWCLwWI_Wn2hmHHpnDks5sGwTbgaJpZM4OBlIm>.
> 

",thank suggestion little concern might cause infinite loop could please make could test later wrote still think alternatively would good reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
310536919,"I still think the suggested changes to  `_random_mutation_operator` and `_mate_operator`, or alternatively `_pre_test` would be a good!",still think alternatively would good,issue,negative,positive,positive,positive,positive,positive
310536486,"@weixuanfu2016  No, your idea of cache hits was correct. As stated just before you made your post (you probably missed it since you were typing yours), I made a mistake raising cache-hits significantly (specifically, I counted logged the  `population` instead of `offspring`, meaning I only looked at what remained after `selection`).

My new results are just in (with correct logging) and results are similar to yours. Here is a (quickly and badly made) bar chart [link](http://imgur.com/sy0OtUu). 

",idea cache correct stated made post probably since made mistake raising significantly specifically logged population instead offspring meaning selection new correct logging similar quickly badly made bar chart link,issue,negative,negative,neutral,neutral,negative,negative
310534251,"@rhiever that is a good idea. I will work on it.

@PG-TUe thank you for the notebook and idea. Maybe my understanding of `cache_hits` in your plots is not right. I made a quick [test branch](https://github.com/weixuanfu2016/tpot/tree/issue503) based on master branch in my forked repo to check how many unique new individuals in each generation (since [this line](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L863) will skip evaluated pipelines in earlier generations). The percentage of new individuals in later generations was around 30% in the test. (check the partial log below)

@dartdog check the partial log below, I saw that scores changed in the early generations but fixed in the later generations. I think this problem in the example is easy for getting a very good pipeline in the early generations. As mentioned above, lower population diversity in later populations is indeed a concern for slowing down optimization process. I will work on a solution to increase population diversity in later generations.  

```
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np

iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25, random_state=2)

tpot = TPOTClassifier(generations=100, population_size=100, verbosity=2, random_state=2)
tpot.fit(X_train, y_train)


Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.
Optimization Progress:   0%|                                                                                                    | 0/10100 [00:00<?, ?pipeline/s]
Number of new vaild pipelines in currect generation: 90
Optimization Progress:   1%|▊                                                                                        | 92/10100 [00:22<1:08:13,  2.44pipeline/s]
Number of new vaild pipelines in currect generation: 86
Generation 1 - Current best internal CV score: 0.9726896292113685                                                                                               
Optimization Progress:   2%|█▋                                                                                      | 188/10100 [00:48<1:37:21,  1.70pipeline/s]
Number of new vaild pipelines in currect generation: 77
Generation 2 - Current best internal CV score: 0.9726896292113685                                                                                               
Optimization Progress:   3%|██▍                                                                                     | 277/10100 [01:19<1:47:59,  1.52pipeline/s]
Number of new vaild pipelines in currect generation: 72
Generation 3 - Current best internal CV score: 0.9726896292113685                                                                                               
Optimization Progress:   4%|███▏                                                                                    | 366/10100 [01:46<2:13:02,  1.22pipeline/s]
Number of new vaild pipelines in currect generation: 74
Generation 4 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   5%|███▉                                                                                    | 457/10100 [02:19<2:13:45,  1.20pipeline/s]
Number of new vaild pipelines in currect generation: 67
Generation 5 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   5%|████▊                                                                                   | 547/10100 [02:50<4:00:17,  1.51s/pipeline]
Number of new vaild pipelines in currect generation: 71
Generation 6 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   6%|█████▌                                                                                  | 639/10100 [03:23<3:01:06,  1.15s/pipeline]
Number of new vaild pipelines in currect generation: 67
Generation 7 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   7%|██████▍                                                                                 | 733/10100 [03:51<2:14:14,  1.16pipeline/s]
Number of new vaild pipelines in currect generation: 66
Generation 8 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   8%|███████▏                                                                                | 824/10100 [04:16<2:08:42,  1.20pipeline/s]
Number of new vaild pipelines in currect generation: 61
Generation 9 - Current best internal CV score: 0.982213438735178                                                                                                
Optimization Progress:   9%|███████▉                                                                                | 914/10100 [04:43<2:29:08,  1.03pipeline/s]
Number of new vaild pipelines in currect generation: 59
Generation 10 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  10%|████████▋                                                                               | 996/10100 [05:00<1:34:22,  1.61pipeline/s]

...

Generation 21 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  20%|█████████████████▎                                                                     | 2011/10100 [11:11<2:27:03,  1.09s/pipeline]
Number of new vaild pipelines in currect generation: 58
Generation 22 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  21%|██████████████████▏                                                                    | 2106/10100 [11:52<3:47:06,  1.70s/pipeline]
Number of new vaild pipelines in currect generation: 53
Generation 23 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  22%|██████████████████▉                                                                    | 2202/10100 [12:48<2:17:50,  1.05s/pipeline]
Number of new vaild pipelines in currect generation: 46
Generation 24 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  23%|███████████████████▊                                                                   | 2296/10100 [13:25<1:36:54,  1.34pipeline/s]
Number of new vaild pipelines in currect generation: 55
Generation 25 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  24%|████████████████████▌                                                                  | 2384/10100 [13:58<1:49:03,  1.18pipeline/s]
Number of new vaild pipelines in currect generation: 51
Generation 26 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  25%|█████████████████████▍                                                                 | 2482/10100 [14:45<2:29:02,  1.17s/pipeline]
Number of new vaild pipelines in currect generation: 44
Generation 27 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  25%|██████████████████████▏                                                                | 2571/10100 [15:21<2:09:12,  1.03s/pipeline]
Number of new vaild pipelines in currect generation: 55
Generation 28 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  26%|██████████████████████▉                                                                | 2666/10100 [15:55<2:25:48,  1.18s/pipeline]
Number of new vaild pipelines in currect generation: 46
Generation 29 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  27%|███████████████████████▊                                                               | 2762/10100 [16:46<2:36:52,  1.28s/pipeline]
Number of new vaild pipelines in currect generation: 44
Generation 30 - Current best internal CV score: 0.982213438735178     

...

Generation 60 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  56%|████████████████████████████████████████████████▌                                      | 5631/10100 [36:31<1:26:53,  1.17s/pipeline]
Number of new vaild pipelines in currect generation: 27
Generation 61 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  57%|█████████████████████████████████████████████████▏                                     | 5716/10100 [37:15<2:04:30,  1.70s/pipeline]
Number of new vaild pipelines in currect generation: 35
Generation 62 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  57%|█████████████████████████████████████████████████▉                                     | 5804/10100 [37:44<1:13:17,  1.02s/pipeline]
Number of new vaild pipelines in currect generation: 35
Generation 63 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  58%|██████████████████████████████████████████████████▋                                    | 5890/10100 [38:21<1:40:59,  1.44s/pipeline]
Number of new vaild pipelines in currect generation: 30
Generation 64 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  59%|███████████████████████████████████████████████████▌                                   | 5986/10100 [39:04<1:23:10,  1.21s/pipeline]
Number of new vaild pipelines in currect generation: 33
Generation 65 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  60%|████████████████████████████████████████████████████▎                                  | 6074/10100 [39:48<1:28:08,  1.31s/pipeline]
Number of new vaild pipelines in currect generation: 36
Generation 66 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  61%|██████████████████████████████████████████████████████▎                                  | 6166/10100 [40:09<49:16,  1.33pipeline/s]
Number of new vaild pipelines in currect generation: 39
Generation 67 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  62%|█████████████████████████████████████████████████████▉                                 | 6262/10100 [40:56<1:22:02,  1.28s/pipeline]
Number of new vaild pipelines in currect generation: 33
Generation 68 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  63%|██████████████████████████████████████████████████████▋                                | 6352/10100 [41:34<1:20:02,  1.28s/pipeline]
Number of new vaild pipelines in currect generation: 24
Generation 69 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  64%|███████████████████████████████████████████████████████▍                               | 6442/10100 [42:08<1:12:56,  1.20s/pipeline]
Number of new vaild pipelines in currect generation: 30
Generation 70 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  65%|████████████████████████████████████████████████████████▏                              | 6530/10100 [42:56<1:10:04,  1.18s/pipeline]
Number of new vaild pipelines in currect generation: 37

...

Generation 90 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  83%|█████████████████████████████████████████████████████████████████████████▉               | 8390/10100 [54:50<25:52,  1.10pipeline/s]
Number of new vaild pipelines in currect generation: 29
Generation 91 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  84%|██████████████████████████████████████████████████████████████████████████▋              | 8481/10100 [55:06<22:49,  1.18pipeline/s]
Number of new vaild pipelines in currect generation: 39
Generation 92 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  85%|███████████████████████████████████████████████████████████████████████████▌             | 8573/10100 [55:32<32:06,  1.26s/pipeline]
Number of new vaild pipelines in currect generation: 34
Generation 93 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  86%|████████████████████████████████████████████████████████████████████████████▎            | 8664/10100 [56:02<27:22,  1.14s/pipeline]
Number of new vaild pipelines in currect generation: 28
Generation 94 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  87%|█████████████████████████████████████████████████████████████████████████████            | 8752/10100 [56:40<28:31,  1.27s/pipeline]
Number of new vaild pipelines in currect generation: 28
Generation 95 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  88%|█████████████████████████████████████████████████████████████████████████████▉           | 8840/10100 [57:12<24:24,  1.16s/pipeline]
Number of new vaild pipelines in currect generation: 24
Generation 96 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  88%|██████████████████████████████████████████████████████████████████████████████▋          | 8930/10100 [57:48<20:35,  1.06s/pipeline]
Number of new vaild pipelines in currect generation: 23
Generation 97 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  89%|███████████████████████████████████████████████████████████████████████████████▌         | 9022/10100 [58:22<18:36,  1.04s/pipeline]
Number of new vaild pipelines in currect generation: 29
Generation 98 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  90%|████████████████████████████████████████████████████████████████████████████████▎        | 9109/10100 [58:46<25:46,  1.56s/pipeline]
Number of new vaild pipelines in currect generation: 26
Generation 99 - Current best internal CV score: 0.982213438735178                                                                                               
Optimization Progress:  91%|█████████████████████████████████████████████████████████████████████████████████        | 9198/10100 [59:08<20:48,  1.38s/pipeline]
Number of new vaild pipelines in currect generation: 26
Generation 100 - Current best internal CV score: 0.982213438735178                                                                                              
                                                                                                                                                                
Best pipeline: GaussianNB(RBFSampler(input_matrix, RBFSampler__gamma=0.55))


                                                                                          
``` ",good idea work thank notebook idea maybe understanding right made quick test branch based master branch forked check many unique new generation since line skip percentage new later around test check partial log check partial log saw early fixed later think problem example easy getting good pipeline early lower population diversity later indeed concern optimization process work solution increase population diversity later import import import import iris warning available used optimization progress number new generation optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score optimization progress number new generation generation current best internal score best pipeline,issue,positive,positive,positive,positive,positive,positive
310533937,"Oops! It turns out the problem does exist, but is not as severe as stated in the notebook.
I made a mistake on when to log the population, over representing the amount of duplicates and cache hits. I am currently rerunning the experiment, so far it seems that there are still a fair number of cache hits and duplicates, but not nearly as many (maybe 10~40% instead of near 100%).

In addition to changing where the logging happens, I now also have a direct duplicate/cache hit counter directly where they happen in the code in `evaluate_individuals` to verify.",turn problem exist severe stated notebook made mistake log population amount cache currently experiment far still fair number cache nearly many maybe instead near addition logging also direct hit counter directly happen code verify,issue,negative,positive,positive,positive,positive,positive
310529558,"@dartdog Good to hear! Indeed, the initial population seems to be generated fairly diverse :) so that would align with getting better results by increasing population size (even in the 1st generation).

@rhiever I think you can do this in the `_random_mutation_operator` and `_mate_operator` functions by rewriting them from
```
@_pre_test
def _mate_operator(self, ind1, ind2):
    return cxOnePoint(ind1, ind2)

@_pre_test
def _random_mutation_operator(self, individual):
    mutation_techniques = [
        partial(gp.mutInsert, pset=self._pset),
        partial(mutNodeReplacement, pset=self._pset),
        partial(gp.mutShrink)
    ]
    return np.random.choice(mutation_techniques)(individual)
```
to
```
@_pre_test
def _mate_operator(self, ind1, ind2):
    offspring = cxOnePoint(ind1, ind2)
    while offspring in self.evaluated_individuals_:
        offspring = cxOnePoint(ind1, ind2)
    return offspring

@_pre_test
def _random_mutation_operator(self, individual):
   mutation_techniques = [
       partial(gp.mutInsert, pset=self._pset),
       partial(mutNodeReplacement, pset=self._pset),
       partial(gp.mutShrink)
   ]
   mutator = np.random.choice(mutation_techniques)
   offspring = mutator(individual)
   while offspring in self.evaluated_individuals_:
       offspring = mutator(individual)
   return offspring
```
Of course you probably want to use a for-loop defining some amount of max tries instead to avoid infinite loops.

However, I would still try and see if the behavior is caused by a bug. Considering 30% of individuals will have a random insert mutation each generation (on average), it seems very weird to me that can get 99% cache hits consistently.

Edit: As stated below, bug was with logging, actual numbers rise much more slowly.",good hear indeed initial population fairly diverse would align getting better increasing population size even st generation think self return self individual partial partial partial return individual self offspring offspring offspring return offspring self individual partial partial partial offspring individual offspring offspring individual return offspring course probably want use amount instead avoid infinite however would still try see behavior bug considering random insert mutation generation average weird get cache consistently edit stated bug logging actual rise much slowly,issue,negative,positive,neutral,neutral,positive,positive
310520265,FWIW I upped the pop size to 150 on the iris example and 1st gen at .990909090909091 with no changes beyond that in subsequent generations was looking for something to really show the power that I assume is here!,pop size iris example st gen beyond subsequent looking something really show power assume,issue,negative,positive,neutral,neutral,positive,positive
310517136,"Wow, that's a lot of duplicates! I wonder if we can change the mutation/xover functions to only produce individuals not in the cache using the pre_check function, @weixuanfu2016?",wow lot wonder change produce cache function,issue,positive,positive,neutral,neutral,positive,positive
310506631,"Actually, while working on a new extension to TPOT and analyzing my results, I found the same issue to be true (in both the 'vanilla' 0.8.1 version of TPOT and my extension).
There are a significant amount of duplicates being produced both within the same generation and cross generations.
I have not yet tried to figure out the cause.
I made a very brief write-up [here](https://github.com/PG-TUe/MyNotepad/blob/master/TPOT/TPOT_Repeating_pipelines.ipynb).

**EDIT:** I made a mistake on when to log the population, over representing the amount of duplicates and cache hits. This mistake is currently present in the above notebook.",actually working new extension found issue true version extension significant amount produced within generation cross yet tried figure cause made brief edit made mistake log population amount cache mistake currently present notebook,issue,negative,positive,positive,positive,positive,positive
310472097,"For inspiration, check out **AutoNet** by auto-sklearn ([Paper](http://proceedings.mlr.press/v64/mendoza_towards_2016.html), [Source Code](https://github.com/automl/auto-sklearn/pull/223)) ",inspiration check paper source code,issue,positive,neutral,neutral,neutral,neutral,neutral
310390039,"Right. Does it work better if you set `population_size=100` and `generations=100`? The examples are meant to run fast, but they're not ideal parameter settings. The defaults (100 pop and 100 gen) are better.",right work better set meant run fast ideal parameter pop gen better,issue,positive,positive,positive,positive,positive,positive
310236066,I think the population size of 50 and generation number of 5 in examples may be too small to show evolving. ,think population size generation number may small show,issue,negative,negative,negative,negative,negative,negative
310156928,Checkpointing of the best pipeline from every generation has been added to the dev branch and will be included in the next release.,best pipeline every generation added dev branch included next release,issue,positive,positive,positive,positive,positive,positive
309583357,"thanks, i will also report  on XGBoost repo. I don't use conda so I will be not abble to try your solution.
Thanks for your answer.",thanks also report use try solution thanks answer,issue,positive,positive,positive,positive,positive,positive
309578024,"This is probably a better question to ask on the [XGBoost repo](https://github.com/dmlc/xgboost/issues). But my first thought is you should try installing XGBoost via Anaconda: `conda install py-xgboost`

If that doesn't work, I suggest filing an issue on the XGBoost repo.",probably better question ask first thought try via anaconda install work suggest filing issue,issue,negative,positive,positive,positive,positive,positive
309569131,"Hi guys,

I am trying to install xgboost on my mac (10.12.5) and after several attempts I did not succeed. I follow different tuto trying to modifi gcc variable by adding gcc-6 but I have the same problem. 

So what I did:

1. git clone --recursive https://github.com/dmlc/xgboost
2. cd xgboost; cp make/minimum.mk ./config.mk; make -j4

I have the following error:

```
/usr/local/bin/g++ -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/learner.o src/learner.cc >build/learner.d
/usr/local/bin/g++ -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/logging.o src/logging.cc >build/logging.d
/usr/local/bin/g++ -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/c_api/c_api.o src/c_api/c_api.cc >build/c_api/c_api.d
/usr/local/bin/g++ -std=c++11 -Wall -Wno-unknown-pragmas -Iinclude   -Idmlc-core/include -Irabit/include -I/include -O3 -funroll-loops -msse2 -fPIC -DDISABLE_OPENMP -MM -MT build/c_api/c_api_error.o src/c_api/c_api_error.cc >build/c_api/c_api_error.d
/bin/sh: /usr/local/bin/g++: No such file or directory
/bin/sh: /usr/local/bin/g++: No such file or directory
make: *** [build/learner.o] Error 127
make: *** Waiting for unfinished jobs....
/bin/sh: /usr/local/bin/g++: No such file or directory
make: *** [build/logging.o] Error 127
make: *** [build/c_api/c_api.o] Error 127
/bin/sh: /usr/local/bin/g++: No such file or directory
make: *** [build/c_api/c_api_error.o] Error 127
```
My gcc version is:

```
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 8.1.0 (clang-802.0.42)
Target: x86_64-apple-darwin16.6.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```

I really need your help. For the moment I use ```pip install xgboost``` to use the library but I know it's not the right way to use this library. 
Thank you for your help 

ps: sorry if my english is not ok, i am not fluent",hi trying install mac several succeed follow different trying variable problem git clone recursive make following error file directory file directory make error make waiting unfinished file directory make error make error file directory make error version apple version target thread model really need help moment use pip install use library know right way use library thank help sorry fluent,issue,negative,negative,neutral,neutral,negative,negative
309471265,"> Are there are ensemble methods (preferably with a sklearn-like interface) that we should be aware of?

Not sure if you should, but Sebastian has own Stacker here https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor/",ensemble preferably interface aware sure stacker,issue,negative,positive,positive,positive,positive,positive
309276286,"I am being taken away from my working computer, they are saying im fired and i cant touch it anymore :1st_place_medal: ",taken away working computer saying fired cant touch,issue,negative,neutral,neutral,neutral,neutral,neutral
309268729,"Yey!
Even on the coverage front I look good!

On Sun, Jun 18, 2017 at 12:44 PM, Coveralls <notifications@github.com>
wrote:

> [image: Coverage Status] <https://:/builds/12019579>
>
> Coverage increased (+6.5%) to 94.81% when pulling *81045ab
> <https://github.com/rhiever/tpot/commit/81045ab98cbf366307181c158fcece3a0304936b>
> on kuratsak:tpot_features* into *2b0c29c
> <https://github.com/rhiever/tpot/commit/2b0c29cc1020602f4a89bfa50f0f83386a6db133>
> on rhiever:development*.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/498#issuecomment-309267115>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7PQm8VDoocIX-JW78M1CLgZkGqtyks5sFPF6gaJpZM4N7Ikm>
> .
>



-- 
Cheers,
Dani K.
",even coverage front look good sun coverall wrote image coverage status coverage development reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
309262414,"Can someone please check and rerun travis build? it seems the build failed because of external sources of trouble..

both py36 builds failed with errors related to:
Error: HTTPError: 522 against the url https://repo.continuum.io/pkgs/....

I checked of course locally: both py3 and py2 tests pass

Update:
pushed documentation changes, tests passed.
As I suspected the problem was external.

Everything looks good now :raised_hands: ",someone please check rerun travis build build external trouble related error checked course locally pas update documentation suspected problem external everything good,issue,negative,positive,neutral,neutral,positive,positive
309260794,"Done, fixed all comments and remerged development into my branch.
Sadly this is my last day at work, so next time I can fix anything would be
in about a month (mid-july).

I fixed all your comments as you asked, I'm hoping we can merge this now.

On Thu, Jun 15, 2017 at 11:56 PM, Randy Olson <notifications@github.com>
wrote:

> It's merged into the dev branch now.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/498#issuecomment-308863490>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7H7AyKmJ1yn7yBYCfWVzy_NHjUBCks5sEZqNgaJpZM4N7Ikm>
> .
>



-- 
Cheers,
Dani K.
",done fixed development branch sadly last day work next time fix anything would month fixed merge randy wrote dev branch reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
309065637,"Update: after discussions with @rhiever, we decide to remove `DEFAULT` value in terminal since it is confusing as default in TPOT. And user can easily use `config_dict` parameter to add default value back based on scikit-learn online documentation. ",update decide remove default value terminal since default user easily use parameter add default value back based documentation,issue,positive,positive,positive,positive,positive,positive
308978218,"Score is still negative (in your code) but fit() end properly, score in `print(tpot.score(X_test, y_test))` (and `Current best internal CV score`) correct, and with `verbosity=3` no lines like this:
```
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=2 'Model1'
...
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model8'
```
I think that now all works. Thank you!",score still negative code fit end properly score print current best internal score correct like decorator decorator decorator decorator think work thank,issue,positive,positive,positive,positive,positive,positive
308917494,"I figured out that the `KeyError` was due to a bug about `DEFAULT` value in these parameters of these stacked estimators in Ensemble. I just made a PR to fix this bug. 

Before this PR is merged to dev branch or master branch. You may try my branch for testing if it works in your Ensemble method. The command below may help you to install the branch. Please let me know if this PR fix your issue.

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@issue499
```

",figured due bug default value ensemble made fix bug dev branch master branch may try branch testing work ensemble method command may help install branch please let know fix issue pip install upgrade,issue,positive,negative,negative,negative,negative,negative
308868274,"Negative score and, again, can't find one of the models:
```
1 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model1'
...
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model8'
make_pipeline(
    Ensemble(...)
)
-23845.7731237
make_pipeline(
    StackingEstimator(estimator=Ensemble(...)),
    Ensemble(...)
)
-23910.755966
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-61-5e259a95c5bc> in <module>()
      5 for deap_pipeline in pop:
      6     # convert pipeline string to scikit-learn pipeline object
----> 7     sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
      8     # print sklearn pipeline string
      9     sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot_obj._pset), tpot_obj.operators)

/usr/local/lib/python3.5/dist-packages/tpot/base.py in _compile_to_sklearn(self, expr)
    779         sklearn_pipeline: sklearn.pipeline.Pipeline
    780         """"""
--> 781         sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr, self._pset), self.operators)
    782         return eval(sklearn_pipeline, self.operators_context)
    783 

/usr/local/lib/python3.5/dist-packages/tpot/export_utils.py in generate_pipeline_code(pipeline_tree, operators)
    254 
    255     """"""
--> 256     steps = _process_operator(pipeline_tree, operators)
    257     pipeline_text = ""make_pipeline(\n{STEPS}\n)"".format(STEPS=_indent("",\n"".join(steps), 4))
    258     return pipeline_text

/usr/local/lib/python3.5/dist-packages/tpot/export_utils.py in _process_operator(operator, operators, depth)
    297 
    298         if input_name != 'input_matrix':
--> 299             steps.extend(_process_operator(input_name, operators, depth + 1))
    300 
    301         # If the step is an estimator and is not the last step then we must

/usr/local/lib/python3.5/dist-packages/tpot/export_utils.py in _process_operator(operator, operators, depth)
    306             steps.append(
    307                 ""StackingEstimator(estimator={})"".
--> 308                 format(tpot_op.export(*args))
    309             )
    310         else:

/usr/local/lib/python3.5/dist-packages/tpot/operator_utils.py in export(cls, *args)
    252                         arg_value = dep_op_str
    253                     else:
--> 254                         arg_value = ""{}({})"".format(dep_op_str, "", "".join(dep_op_arguments[dep_op_str]))
    255                     tmp_op_args.append(""{}={}"".format(dep_op_pname, arg_value))
    256             op_arguments = tmp_op_args + op_arguments

KeyError: 'Model8'
```",negative score ca find one decorator decorator decorator decorator decorator ensemble ensemble recent call last module pop convert pipeline string pipeline object print pipeline string self return return operator depth depth step estimator last step must operator depth format else export else,issue,negative,negative,neutral,neutral,negative,negative
308866613,"Oh, just notice the scoring function for regression should be `neg_mean_squared_error `. I just updated codes in the comments above
cv_scores = cross_val_score(sklearn_pipeline, pretest_X_reg, pretest_y_reg, cv=5, scoring='neg_mean_squared_error', verbose=0)",oh notice scoring function regression,issue,negative,neutral,neutral,neutral,neutral,neutral
308865486,"Sorry, I changed `training_features` and `training_target` to `pretest_X_reg` and `pretest_y_reg` in the codes. Could you please try it again?",sorry could please try,issue,negative,negative,negative,negative,negative,negative
308864337,"Got this (`...` - just a lot of parameters):
```
_pre_test decorator: _generate: num_test=0 'Model10'
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model10'
_pre_test decorator: _generate: num_test=2 'Model10'
_pre_test decorator: _generate: num_test=3 'Model1'
_pre_test decorator: _generate: num_test=4 'Model9'
_pre_test decorator: _generate: num_test=5 'Model8'
_pre_test decorator: _generate: num_test=6 'Model8'
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model10'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=3 'Model8'
_pre_test decorator: _generate: num_test=0 'Model1'
_pre_test decorator: _generate: num_test=1 'Model7'
_pre_test decorator: _generate: num_test=2 'Model1'
_pre_test decorator: _generate: num_test=3 'Model1'
_pre_test decorator: _generate: num_test=4 'Model1'
_pre_test decorator: _generate: num_test=5 'Model7'
_pre_test decorator: _generate: num_test=6 'Model9'
_pre_test decorator: _generate: num_test=0 'Model1'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model3'
_pre_test decorator: _generate: num_test=3 'Model8'
_pre_test decorator: _generate: num_test=4 'Model9'
_pre_test decorator: _generate: num_test=5 'Model1'
_pre_test decorator: _generate: num_test=6 'Model8'
_pre_test decorator: _generate: num_test=7 'Model8'
_pre_test decorator: _generate: num_test=8 'Model8'
_pre_test decorator: _generate: num_test=9 'Model9'
_pre_test decorator: _generate: num_test=0 'Model7'
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=3 'Model10'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=3 'Model7'
_pre_test decorator: _generate: num_test=4 'Model1'
_pre_test decorator: _generate: num_test=5 'Model1'
_pre_test decorator: _generate: num_test=6 'Model1'
_pre_test decorator: _generate: num_test=7 'Model8'
_pre_test decorator: _generate: num_test=8 'Model1'
_pre_test decorator: _generate: num_test=9 'Model1'
make_pipeline(
    Ensemble(...)
)
name 'training_features' is not defined
-inf
make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    Ensemble(...)
)
name 'training_features' is not defined
-inf
make_pipeline(
    Ensemble(...)
)
name 'training_features' is not defined
-inf
make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    Ensemble(...)
)
name 'training_features' is not defined
-inf
make_pipeline(
    Ensemble(...)
)
name 'training_features' is not defined
-inf
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-42-de3937e51b24> in <module>()
      2 for deap_pipeline in pop:
      3     # convert pipeline string to scikit-learn pipeline object
----> 4     sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
      5     # print sklearn pipeline string
      6     sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot_obj._pset), tpot_obj.operators)

/usr/local/lib/python3.5/dist-packages/tpot/base.py in _compile_to_sklearn(self, expr)
    779         sklearn_pipeline: sklearn.pipeline.Pipeline
    780         """"""
--> 781         sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr, self._pset), self.operators)
    782         return eval(sklearn_pipeline, self.operators_context)
    783 

/usr/local/lib/python3.5/dist-packages/tpot/export_utils.py in generate_pipeline_code(pipeline_tree, operators)
    254 
    255     """"""
--> 256     steps = _process_operator(pipeline_tree, operators)
    257     pipeline_text = ""make_pipeline(\n{STEPS}\n)"".format(STEPS=_indent("",\n"".join(steps), 4))
    258     return pipeline_text

/usr/local/lib/python3.5/dist-packages/tpot/export_utils.py in _process_operator(operator, operators, depth)
    309             )
    310         else:
--> 311             steps.append(tpot_op.export(*args))
    312     return steps
    313 

/usr/local/lib/python3.5/dist-packages/tpot/operator_utils.py in export(cls, *args)
    252                         arg_value = dep_op_str
    253                     else:
--> 254                         arg_value = ""{}({})"".format(dep_op_str, "", "".join(dep_op_arguments[dep_op_str]))
    255                     tmp_op_args.append(""{}={}"".format(dep_op_pname, arg_value))
    256             op_arguments = tmp_op_args + op_arguments

KeyError: 'Model9'
```",got lot decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator ensemble name defined copy copy ensemble name defined ensemble name defined copy copy ensemble name defined ensemble name defined recent call last module pop convert pipeline string pipeline object print pipeline string self return return operator depth else return export else,issue,negative,neutral,neutral,neutral,neutral,neutral
308861882,"Is there a branch? I'll merge on Sunday, it's my final work day so last
chance to finish this PR.
I will try to address all the comments then..

On Thu, 15 Jun 2017, 23:14 Randy Olson, <notifications@github.com> wrote:

> We merged some PRs today that re-organized the unit tests. I think it's
> just a matter of moving the unit tests into the appropriate files in the
> tests directory.
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/498#issuecomment-308852841>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7LZw7zagiCrviFUdk7pOfLhYSqz0ks5sEZAFgaJpZM4N7Ikm>
> .
>
",branch merge final work day last chance finish try address randy wrote today unit think matter moving unit appropriate directory reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
308859860,"Hmm, it means that no valid pipeline can be randomly generated in the founder population (generation 0) or no pipeline has been a valid CV score

I changed that codes a little bit as below to check if these pipelines are generated and valid:

```
from sklearn.datasets import make_classification
from tpot import TPOTRegressor
pretest_X_reg, pretest_y_reg = make_regression(n_samples=50, n_features=10, random_state=42)
tpot_obj = TPOTRegressor(generations=1, population_size=10, verbosity=3,
                     n_jobs=-1, cv=5, config_dict=tpot_config)
pop = tpot_obj._toolbox.population(n=10)
for deap_pipeline in pop:
    # convert pipeline string to scikit-learn pipeline object
    sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
    # print sklearn pipeline string
    sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(deap_pipeline, tpot_obj._pset), tpot_obj.operators)
    print(sklearn_pipeline_str)
    # Fix random state when the operator allows  (optional) just for get consistent CV score 
    tpot_obj._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)

    try:
        cv_scores = cross_val_score(sklearn_pipeline, pretest_X_reg, pretest_y_reg, cv=5, scoring='neg_mean_squared_error', verbose=0)
        mean_cv_scores = np.mean(cv_scores)
    except Exception as e:
        print(e)
        mean_cv_scores = -float('inf')
    print(mean_cv_scores)
```
",valid pipeline randomly founder population generation pipeline valid score little bit check valid import import pop pop convert pipeline string pipeline object print pipeline string print fix random state operator optional get consistent score try except exception print print,issue,negative,negative,negative,negative,negative,negative
308852841,We merged some PRs today that re-organized the unit tests. I think it's just a matter of moving the unit tests into the appropriate files in the `tests` directory.,today unit think matter moving unit appropriate directory,issue,negative,positive,positive,positive,positive,positive
308837638,"Hmm, it seems that `_pre_test decorator` printed out too many error messages. But I don't understand why these error messages were ""Model*"". Also, it seems that some pipelines passed `_pre_test` and should be passed to evaluation step to get CV scores. But no stdout showed up?

Please try the codes below to checkout which steps cause the error.
```
# 1 generation is OK
tpot_obj = TPOTRegressor(generations=1, population_size=10, verbosity=3,
                     n_jobs=-1, cv=5, config_dict=tpot_config)
tpot_obj.fit(X_train, y_train)


import numpy as np
from deap import creator
from sklearn.model_selection import cross_val_score
from tpot.export_utils import generate_pipeline_code

# print part of pipeline dictionary
print(dict(list(tpot.evaluated_individuals_.items())[0:2]))
# print a pipeline and its values
pipeline_str = list(tpot.evaluated_individuals_.keys())[0]
print(pipeline_str)
print(tpot.evaluated_individuals_[pipeline_str])
for pipeline_string in sorted(tpot_obj.evaluated_individuals_.keys()):
    # convert pipeline string to scikit-learn pipeline object
    deap_pipeline = creator.Individual.from_string(pipeline_string, tpot_obj._pset)
    sklearn_pipeline = tpot_obj._toolbox.compile(expr=deap_pipeline)
    # print sklearn pipeline string
    sklearn_pipeline_str = generate_pipeline_code(expr_to_tree(individual, tpot_obj._pset), tpot_obj.operators)
    print(sklearn_pipeline_str)
    # Fix random state when the operator allows  (optional) just for get consistent CV score 
    tpot_obj._set_param_recursive(sklearn_pipeline.steps, 'random_state', 42)

    try:
        cv_scores = cross_val_score(sklearn_pipeline, training_features, training_target, cv=5, scoring='accuracy', verbose=0)
        mean_cv_scores = np.mean(cv_scores)
    except Exception as e:
        print(e)
        mean_cv_scores = -float('inf')
    print(mean_cv_scores)
",decorator printed many error understand error model also evaluation step get please try cause error generation import import creator import import print part pipeline dictionary print list print pipeline list print print sorted convert pipeline string pipeline object print pipeline string individual print fix random state operator optional get consistent score try except exception print print,issue,negative,positive,neutral,neutral,positive,positive
308829983,"Yea, I added some ifs on verbosity that another comment requested.
Rebase? I based it on the development branch.
Is that not the most updated one?

On Thu, 15 Jun 2017, 21:31 Randy Olson, <notifications@github.com> wrote:

> Overall, I'm 👍 on these changes, pending the rebase and requested
> changes.
>
> Beyond the several requests for adding to the docstrings, please also
> update the API documentation <http://rhiever.github.io/tpot/api/> and
> other documentation where relevant.
>
> Any ideas why the coverage decreased by 1.4%?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/498#issuecomment-308821985>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7HLeMFFWHlseEVgDAdoFFDZVTPfnks5sEXHcgaJpZM4N7Ikm>
> .
>
",yea added verbosity another comment rebase based development branch one randy wrote overall pending rebase beyond several please also update documentation documentation relevant coverage thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
308829940,"A nice way to use such a nice thing. With `verbosity=3`:
```
tpot = TPOTRegressor(generations=5, population_size=10, verbosity=3,
                     n_jobs=-1, cv=5, config_dict=tpot_config)
tpot.fit(X_train, y_train)
```
Output:
```
1 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 'Model1'
_pre_test decorator: _generate: num_test=1 'Model1'
_pre_test decorator: _generate: num_test=2 'Model7'
_pre_test decorator: _generate: num_test=3 'Model1'
_pre_test decorator: _generate: num_test=4 'Model8'
_pre_test decorator: _generate: num_test=5 'Model8'
_pre_test decorator: _generate: num_test=6 'Model8'
_pre_test decorator: _generate: num_test=7 'Model7'
_pre_test decorator: _generate: num_test=8 'Model7'
_pre_test decorator: _generate: num_test=9 'Model9'
_pre_test decorator: _generate: num_test=0 'Model9'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=0 'Model10'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model7'
_pre_test decorator: _generate: num_test=3 'Model10'
_pre_test decorator: _generate: num_test=4 'Model9'
_pre_test decorator: _generate: num_test=5 'Model7'
_pre_test decorator: _generate: num_test=6 'Model8'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model1'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=3 'Model8'
_pre_test decorator: _generate: num_test=4 'Model8'
_pre_test decorator: _generate: num_test=5 'Model1'
_pre_test decorator: _generate: num_test=6 'Model7'
_pre_test decorator: _generate: num_test=7 'Model9'
_pre_test decorator: _generate: num_test=8 'Model8'
_pre_test decorator: _generate: num_test=9 'Model8'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model9'
_pre_test decorator: _generate: num_test=3 'Model1'
_pre_test decorator: _generate: num_test=4 'Model1'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=0 'Model8'
_pre_test decorator: _generate: num_test=1 'Model1'
_pre_test decorator: _generate: num_test=2 'Model7'
_pre_test decorator: _generate: num_test=3 'Model10'
_pre_test decorator: _generate: num_test=0 'Model1'
_pre_test decorator: _generate: num_test=1 'Model8'
_pre_test decorator: _generate: num_test=2 'Model8'
_pre_test decorator: _generate: num_test=3 'Model1'
_pre_test decorator: _generate: num_test=4 'Model1'
_pre_test decorator: _generate: num_test=5 'Model1'
_pre_test decorator: _generate: num_test=6 'Model9'
_pre_test decorator: _generate: num_test=7 'Model9'
_pre_test decorator: _generate: num_test=8 'Model1'
_pre_test decorator: _generate: num_test=9 'Model9'
```",nice way use nice thing output decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator decorator,issue,positive,positive,positive,positive,positive,positive
308821985,"Overall, I'm 👍 on these changes, pending the rebase and requested changes.

Beyond the several requests for adding to the docstrings, please also update the [API documentation](http://rhiever.github.io/tpot/api/) and other documentation where relevant.

I'm also concerned that the coverage decreased by 1.4%.

Thank you for the PR, @kuratsak.",overall pending rebase beyond several please also update documentation documentation relevant also concerned coverage thank,issue,positive,positive,positive,positive,positive,positive
308781137,"@weixuanfu2016, we can do that separately in another PR. Merging this one before any other big changes happen and it needs to be rebased for the 20th time. :-)",separately another one big happen need th time,issue,negative,neutral,neutral,neutral,neutral,neutral
308747154,"A nice way to use TPOT. `config_dict` allows this kind of nested estimator as shown in our default [`config_dict`](https://github.com/rhiever/tpot/blob/master/tpot/config/regressor.py#L177-L185)  and we do not limit max number of parameters. Could you please use `verbosity=3` in `tpot = TPOTRegressor(..., config_dict=config_dict)` to check more detailed error message? ",nice way use kind estimator shown default limit number could please use check detailed error message,issue,positive,positive,positive,positive,positive,positive
308742841,The PR looks very nice. Only one thing about unit tests: OneHotEncoder has a separated code file `one_hot_encoder_tests.py` for the unit tests of this built-in operator. Should we also make separated files of unit tests for the other built-in operators (`StackingEstimator ` and `ZeroCount`)? Or just combine unit tests for all built-in operators into a single file?,nice one thing unit code file unit operator also make unit combine unit single file,issue,negative,positive,positive,positive,positive,positive
308715328,"Hi Approvers!

This is my second attempt after splitting the pull request into a bug fix one that was approved already.

I am finishing a job and starting another, So I won't be able to fix code until at least July probably..
I have thoroughly tested and used these features in my research in the current job, I really hope we can approve this.

I am available until sunday for any fixes to be made in this pull request, otherwise it will have to wait and I really hope we don't have to.

",hi second attempt splitting pull request bug fix one already finishing job starting another wo able fix code least probably thoroughly tested used research current job really hope approve available made pull request otherwise wait really hope,issue,positive,positive,positive,positive,positive,positive
308574208,"@simonzcaiman, this is certainly something we should discuss now before we move forward with actual implementation of TPOTEnsemble. It seems like a good idea to allow different ensemble methods, but I only know of the ones in VotingClassifier from sklearn. Are there are ensemble methods (preferably with a sklearn-like interface) that we should be aware of?",certainly something discus move forward actual implementation like good idea allow different ensemble know ensemble preferably interface aware,issue,positive,positive,positive,positive,positive,positive
308551858,"Ensemble of pipelines would be a great improvement for TPOT! 
Will it be better if there is a stacking model selection as well? For example, if one does not want to use a VotingClassifier as the stacking model, can he also use another TPOT pipeline optimization to choose the best stacking model? ",ensemble would great improvement better model selection well example one want use model also use another pipeline optimization choose best model,issue,positive,positive,positive,positive,positive,positive
308462236,Thanks @tnlin! I'll be honest: I'm not a fan of the awesome lists. :-) So I'll probably just keep it to a section of the docs for TPOT.,thanks honest fan awesome probably keep section,issue,positive,positive,positive,positive,positive,positive
308461057,Fix is merged into dev branch. Thanks again all.,fix dev branch thanks,issue,negative,positive,positive,positive,positive,positive
308338037,"Maybe should create an awesome list?
Here is another one call [automl](https://github.com/ClimbsRocks/auto_ml)",maybe create awesome list another one call,issue,positive,positive,positive,positive,positive,positive
308248011,Nice! This PR was already merged to master branch of xgboost. I tested it and the API issue is fixed!,nice already master branch tested issue fixed,issue,negative,positive,positive,positive,positive,positive
308175470,Thanks @wxchan! Glad to see this fixed in future versions of XGBoost.,thanks glad see fixed future,issue,positive,positive,positive,positive,positive,positive
308108953,"If you import TPOT as a core part of the software you release, then yes, our understanding of the LGPL is that you must relicense Xcessiv to LGPL as well. An exception to this is if you're using the code exported from TPOT, though, which does not have the LGPL license applied to it.

Including TPOT in your `setup.py` should not entail that requirement. It sounds like you found a good workaround for it. :-)",import core part release yes understanding must relicense well exception code though license applied entail requirement like found good,issue,positive,positive,positive,positive,positive,positive
308083096,"I actually realized that because of how Xcessiv is built, I wouldn't actually need to do `import TPOT`.

I'd just be using Python's duck typing and assume I'm working with a TPOT object. It's not even a hack or a workaround, as for my particular use case, it's literally the straightforward way to do things.

Of course, now my question is if I could include TPOT in my setup.py as a dependency. Would still work if not, but then users would have to install TPOT separately.",actually built would actually need import python duck assume working object even hack particular use case literally straightforward way course question could include dependency would still work would install separately,issue,negative,positive,positive,positive,positive,positive
307953608,"I just made a commit in xgboost https://github.com/dmlc/xgboost/pull/2378, I think may solve your issue here.",made commit think may solve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
307891099,I think you need relicense Xcessiv for importing TPOT based on the [link](https://www.apache.org/legal/resolved.html#category-x).,think need relicense based link,issue,negative,neutral,neutral,neutral,neutral,neutral
307585256,Hello all! Is there any progress on this topic? I am very interested.,hello progress topic interested,issue,positive,positive,positive,positive,positive,positive
307533768,"Maybe, I need test it after coming back from vocation. 

""If you want to run XGBoost process in parallel using the fork backend for joblib/multiprocessing, you must build XGBoost without support for OpenMP by make no_omp=1""

Above is noted In this link:
https://github.com/dmlc/xgboost/blob/master/python-package/README.rst
",maybe need test coming back vocation want run process parallel fork must build without support make noted link,issue,negative,neutral,neutral,neutral,neutral,neutral
307511911,"@martinzc: you are right. I believe this is because that, while both methods are trying to achieve the same thing (prediction accuracy per class that is insensitive to data-set imbalance), they are going about it differently. Adding safety to the original function instead is preferred if the goal is to keep the function output consistent.",right believe trying achieve thing prediction accuracy per class insensitive imbalance going differently safety original function instead preferred goal keep function output consistent,issue,negative,positive,positive,positive,positive,positive
307510989,"Can we add this condition check?
if float(sum((y_true == this_class))):
    this_class_sensitivity = 0
else:
    this_class_sensitivity = \
            float(sum((y_pred == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))
The reason is that if a predicted class is not in the test case, then it means the proportion of positives that are correctly identified as this class is zero.",add condition check float sum else float sum float sum reason class test case proportion correctly class zero,issue,negative,neutral,neutral,neutral,neutral,neutral
307505768,"The suggested fix is not completely right. After I implemented it and tested on some simple test cases, it does not yield the same results. The tests are followed:

Result on [0, 1, 2, 3] and [0, 1, 2, 3]:
Original:
1.0
Fixed:
1.0
Result on [0, 1, 2, 3] and [1, 1, 2, 3]:
Original:
0.833333333333
Fixed:
0.75
Result on [0, 1, 2, 3] and [1, 2, 3, 4]:
Original:
ZeroDivisionError: float division by zero
Fixed:
0.0
",fix completely right tested simple test yield result original fixed result original fixed result original float division zero fixed,issue,positive,positive,positive,positive,positive,positive
307437001,"Great, please feel free to reopen the issue if you have any other questions!",great please feel free reopen issue,issue,positive,positive,positive,positive,positive,positive
307436837,"@weixuanfu2016, do you think this could be happening because we still use joblib even when `n_jobs=1`?",think could happening still use even,issue,negative,neutral,neutral,neutral,neutral,neutral
307353340,"@rhiever getting some unexpected behavior with XGBoost.

Setting XGBoost as the only estimator in `tpot_config`, and applying XGBoost to a training set of approximately 180,000 observations with ~600 features (mostly one hot encoded categorical variables) - based on the [Allstate Claims Severity](https://www.kaggle.com/c/allstate-claims-severity) Kaggle challenge - it's taking multiple hours for pipeline evaluations to start and during that time no processing is being done in the GPU.

Here are the variable definitions I'm using:

```
tpot_config = {
'xgboost.XGBRegressor':
    {
        'n_estimators': [400],
        'max_depth':[10, 12],
        'learning_rate': [0.01, 0.03],
        'subsample': [0.75, 1],
        'min_child_weight': [1,3],
        'n_jobs': [1],
        'updater': ['grow_gpu_hist']
    }
}
```

```
tpot = TPOTRegressor(generations=5, population_size=50, verbosity=3, config_dict=tpot_config,
                     scoring=evalerror, cv=2, max_eval_time_mins=15, n_jobs=1)
```

Given that XGBoost is the only allowed operator, I was expecting to see processing being shifted to the GPU almost immediately, but perhaps I'm missing something about how TPOT is configuring the pipelines. ",getting unexpected behavior setting estimator training set approximately mostly one hot categorical based severity challenge taking multiple pipeline start time done variable given operator see almost immediately perhaps missing something,issue,negative,positive,positive,positive,positive,positive
307255499,"yer, from my experience pandas data frames are more reliable than numpy arrays.
more of a focused product.
Thankyou for your help, i am now going to try the tool with the numerai dataset.
Many thanks,
Best,
Andrew
",yer experience data reliable product help going try tool many thanks best,issue,positive,positive,positive,positive,positive,positive
307199431,"Awesome. I look forward to the next release, then!",awesome look forward next release,issue,positive,positive,positive,positive,positive,positive
307067464,"Check this out: https://github.com/scikit-learn/scikit-learn/pull/8960

In the next release, scikit-learn is probably going to get an implementation of stacking classifier, so TPOT might be able to search stacked ensembles the same way it searches pipelines.",check next release probably going get implementation classifier might able search way,issue,negative,positive,positive,positive,positive,positive
306979597,"@rhiever happy to volunteer, but let me perform some testing to make sure there aren't any negative interactions with `n_jobs` or other parameters for large data sets. For example when using `updater='grow_gpu_hist'` trees are limited to a depth of 15 so if the maximum depth is not constrained in a `tpot_config` parameter XGBoost will terminate mid-run.

 ",happy volunteer let perform testing make sure negative large data example limited depth maximum depth constrained parameter terminate,issue,negative,positive,positive,positive,positive,positive
306829066,"@rhiever I think we could go back to using pandas. If we use TFlearn in the future version of TPOT, the [`tflearn.data_utils.load_csv`](http://tflearn.org/data_utils/#load_csv) can be a good alternative.",think could go back use future version good alternative,issue,negative,positive,positive,positive,positive,positive
306807036,"In the meantime, @AIAdventures, you can change that code to use pandas:

```Python
import pandas as pd

tpot_data = pd.read_csv('/home/andrewcz/tpot/tutorials/data/titanic_train.csv')
features = tpot_data.drop('class', axis=1).values
training_features, testing_features, training_classes, testing_classes = 
                        train_test_split(features, tpot_data['class'].values, random_state=42)
```",change code use python import,issue,negative,neutral,neutral,neutral,neutral,neutral
306806517,"I see the problem now. We're using numpy's `recfromcsv` to read the file in, and telling is that the delimiter is a comma `,`. The problem arises when we have strings containing names in the data, as the names have commas in them. Thus, `recfromcsv` thinks there are 13 columns when there are in fact 12.

`pandas.read_csv` is smart enough to handle this situation, but apparently `recfromcsv` isn't. @weixuanfu2016 / @teaearlgraycold, maybe we should go back to using pandas to read the files in again? I don't think that pandas is that heavy of a dependency, and apparently the numpy data file reading functions are pretty inflexible.",see problem read file telling delimiter comma problem data thus fact smart enough handle situation apparently maybe go back read think heavy dependency apparently data file reading pretty inflexible,issue,negative,negative,neutral,neutral,negative,negative
306654604,"# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = np.recfromcsv('/home/andrewcz/tpot/tutorials/data/titanic_train.csv', delimiter=',', dtype=np.float64)
features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(features, tpot_data['class'], random_state=42)",note make sure class data file,issue,negative,positive,positive,positive,positive,positive
306653481,"Yer, i am using the data in the example.
I might be that my numpy is just not up to date?
Will update numpy and re run through the example.",yer data example might date update run example,issue,negative,neutral,neutral,neutral,neutral,neutral
306597395,"It does indeed look like it's an issue reading the dataset. Specifically, numpy's `np.recfromcsv` function is detecting that there are 12 columns in the Titanic dataset (correct) but thinks there are 13 columns in several of the rows. Are you working on a copy of the Titanic dataset directly from our tutorials directory?",indeed look like issue reading specifically function titanic correct several working copy titanic directly directory,issue,negative,positive,neutral,neutral,positive,positive
306454561,"Cheers Randy, the above is the full error.
I want to try tpot on the numerai dataset.
Many thanks,
best,
Andrew",randy full error want try many thanks best,issue,positive,positive,positive,positive,positive,positive
306454324,"---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-24-74e286f9729e> in <module>()
      6 
      7 # NOTE: Make sure that the class is labeled 'class' in the data file
----> 8 tpot_data = np.recfromcsv('/home/andrewcz/tpot/tutorials/data/titanic_train.csv', delimiter=',', dtype=np.float64)
      9 features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
     10 training_features, testing_features, training_classes, testing_classes =     train_test_split(features, tpot_data['class'], random_state=42)

/home/andrewcz/miniconda3/lib/python3.5/site-packages/numpy/lib/npyio.py in recfromcsv(fname, **kwargs)
   2044     kwargs.setdefault(""delimiter"", "","")
   2045     kwargs.setdefault(""dtype"", None)
-> 2046     output = genfromtxt(fname, **kwargs)
   2047 
   2048     usemask = kwargs.get(""usemask"", False)

/home/andrewcz/miniconda3/lib/python3.5/site-packages/numpy/lib/npyio.py in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)
   1826             # Raise an exception ?
   1827             if invalid_raise:
-> 1828                 raise ValueError(errmsg)
   1829             # Issue a warning ?
   1830             else:

ValueError: Some errors were detected !
    Line 2 (got 13 columns instead of 12)
...
    Line 892 (got 13 columns instead of 12)",recent call last module note make sure class data file delimiter none output false delimiter unpack loose raise exception raise issue warning else line got instead line got instead,issue,negative,positive,neutral,neutral,positive,positive
306298241,"No, it uses about 15gb of 64. 
I've decreased number of features to 20 and it runs smoothly.
Most of my features has a lot of 0 and 1, it would be nice to have a factorization machine as operator.",number smoothly lot would nice factorization machine operator,issue,negative,positive,positive,positive,positive,positive
306289059,"That makes sense. When you fit PolynomialFeatures, does it use up all of your system's memory?",sense fit use system memory,issue,negative,positive,positive,positive,positive,positive
306288388,"I've checked them and they all seems to work (a few pipelines opitmized) with preprocessors disabled.
I suspect that polynomial features are cause of problem (initial dataset has about 300 feautere so, with polynomials we got additional 4500 feauteres, btw. we have only 4209 datpoints, so this could be also a culprit). 
With polynomials commented, even n_jobs=-1 doesn't seem to be a problem.
```
    'sklearn.preprocessing.PolynomialFeatures': {
        'degree': [2],
        'include_bias': [False],
        'interaction_only': [False]
    },

```
Btw. warnings about max-depth genarates Adaboost.

",checked work disabled suspect polynomial cause problem initial got additional could also culprit even seem problem false false,issue,negative,negative,negative,negative,negative,negative
306269920,"OK. So it seems that certain (or just one) sklearn operator freezes on your dataset, and they must be operators that aren't in the [TPOT light configuration](https://github.com/rhiever/tpot/blob/master/tpot/config/regressor_light.py). Need to see what operators are in the [default config](https://github.com/rhiever/tpot/blob/master/tpot/config/regressor.py) The first culprits I suspect are:

* ExtraTreesRegressor
* GradientBoostingRegressor
* AdaBoostRegressor
* RandomForestRegressor
* XGBRegressor

Do all of those work on your dataset?",certain one operator must light configuration need see default first suspect work,issue,negative,positive,positive,positive,positive,positive
306265112,"Yes, it works with 'TPOT light' config. 
I've noticed mistake on my side, not relavant to this issue: `y_predict.to_csv('tpot1_prediction.csv')` , it should be `np.savetxt(""tpot1_prediction.csv"", y_predict, delimiter="","")`",yes work light mistake side issue,issue,negative,positive,positive,positive,positive,positive
306249738,"Thank you for posting your code and data. I'm able to reproduce your issue on my end as well. Can you please confirm that the following code---with TPOT using the TPOT light configuration---works on your end?

```Python
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tpot import TPOTRegressor
import tpot
print(tpot.__version__)

train = pd.read_csv('train_A.csv')
test = pd.read_csv('test_A.csv')
y_train = train['y'].values
X_train = train.drop('y', axis=1)
X_train = X_train.values
X_test = test.values

tpot = TPOTRegressor(generations=5, population_size=20, n_jobs=1, verbosity=3, 
                     max_time_mins=20, max_eval_time_mins=3, random_state=42,
                     config_dict='TPOT light')

tpot.fit(X_train, y_train)
y_predict = tpot.predict(X_test)
# this line should raise an error because y_predict doesn't have a to_csv function
y_predict.to_csv('tpot1_prediction.csv') 
#print(tpot.score(X_test, y_test))

print(tpot.score(X_train, y_train))
tpot.export('tpot_merc_pipeline.py')

```",thank posting code data able reproduce issue end well please confirm following code light configuration end python import import import import import print train test train light line raise error function print print,issue,negative,positive,positive,positive,positive,positive
306246490,"This does indeed seem to be possible in cases where the classifier predicts classes that are not in the test set. Probably doesn't happen very often :-) but worth addressing nonetheless.

I'm open to a PR making a change like the one you suggested. Whoever submits the PR, please include several example snippets proving that the outputs are the same in both versions of the function.",indeed seem possible classifier class test set probably happen often worth nonetheless open making change like one whoever please include several example proving function,issue,positive,positive,neutral,neutral,positive,positive
306243828,"With xgboost removed, one cpu 100% - more than 20 minutes without either break or pipeline optimization:
```
runfile('/home/mglowacki/Desktop/Mercedes/tpot_regression_merc.py', wdir='/home/mglowacki/Desktop/Mercedes')
Reloaded modules: __mp_main__
0.8.3
Warning: xgboost.XGBRegressor is not available and will not be used by TPOT.
27 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False

Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]

```


```Python
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from tpot import TPOTRegressor
import tpot
print(tpot.__version__)

train = pd.read_csv('train_A.csv')
test = pd.read_csv('test_A.csv')
y_train = train['y'].values
X_train = train.drop('y', axis=1)
X_train = X_train.values
X_test = test.values

tpot = TPOTRegressor(generations=5, population_size=20, n_jobs=1, verbosity=3, 
                     max_time_mins=20, max_eval_time_mins=3, random_state=42)

tpot.fit(X_train, y_train)
y_predict = tpot.predict(X_test)
y_predict.to_csv('tpot1_prediction.csv')
#print(tpot.score(X_test, y_test))

print(tpot.score(X_train, y_train))
tpot.export('tpot_merc_pipeline.py')

```

My data set:
[my_dataset.tar.gz](https://github.com/rhiever/tpot/files/1052608/my_dataset.tar.gz)
",removed one without either break pipeline optimization warning available used decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator unsupported set combination decorator unsupported set combination optimization progress python import import import import import print train test train print print data set,issue,negative,positive,positive,positive,positive,positive
306235236,"- Ubuntu 16.04
- xgboost 0.6 - I've installed and build it directly from github 
- n_jobs=1
I'll check it with xgboost uninstalled. ",build directly check uninstalled,issue,negative,positive,neutral,neutral,positive,positive
306216387,Can you please copy and paste the full stack trace from the `ValueError`? It looks like it's having issues reading the Titanic training data.,please copy paste full stack trace like reading titanic training data,issue,positive,positive,positive,positive,positive,positive
306215789,"What's your OS? Also, what's your xgboost version?

```Python
import xgboost
print(xgboost.__version__)
```
I assume this is still with `n_jobs=1`?

Have you tried running without xgboost installed? That could help narrow down whether it's xgboost causing the issue or not.",o also version python import print assume still tried running without could help narrow whether causing issue,issue,negative,negative,negative,negative,negative,negative
306204512,"Good to know. This sounds like a good topic to write up in the TPOT docs related to the [TPOT custom configuration interface](http://rhiever.github.io/tpot/using/#customizing-tpots-operators-and-parameters). Would you be willing to volunteer to draft that write-up, @ap307? :-)",good know like good topic write related custom configuration interface would willing volunteer draft,issue,positive,positive,positive,positive,positive,positive
306050107,"In XGBoost you just have to able to pass the parameters `updater='grow_gpu_hist'` or `updater='grow_gpu'`.

I'm not sure what the interaction with n_jobs is as I've only started testing out the GPU functionality, but so far am able to run on GPU while specifying  `n_jobs=-1`.

Tested it out - you can pass that parameter explicitly in TPOT (together with `n_jobs=-1`) and it doesn't appear to throw of an error and it does appear to be drawing on the GPU.",able pas sure interaction testing functionality far able run tested pas parameter explicitly together appear throw error appear drawing,issue,negative,positive,positive,positive,positive,positive
306049128,"With newest xgboost, I've got error as below. 
I've increased max_time_mins=20, max_eval_time_mins=3 but error appears after 1-2 minutes. So, probably this is a problem with xgboost.
Subsequntly I've updated tpot to 0.8.3 but in this case it hangs up after a few pipelines.
```
unfile('/home/mglowacki/Desktop/Mercedes/tpot_regression_merc.py', wdir='/home/mglowacki/Desktop/Mercedes')
Reloaded modules: xgboost.rabit, xgboost.plotting, xgboost.training, xgboost.callback, xgboost.libpath, xgboost, xgboost.core, xgboost.sklearn, xgboost.compat
Version 0.8.2 of tpot is outdated. Version 0.8.3 was released 1 day ago.
28 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.

Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]
                                                                   
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python3/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 699, in runfile
    execfile(filename, namespace)
  File ""/usr/lib/python3/dist-packages/spyderlib/widgets/externalshell/sitecustomize.py"", line 88, in execfile
    exec(compile(open(filename, 'rb').read(), filename, 'exec'), namespace)
  File ""/home/mglowacki/Desktop/Mercedes/tpot_regression_merc.py"", line 24, in <module>
    y_predict = tpot.predict(X_test)
  File ""/usr/local/lib/python3.5/dist-packages/tpot/base.py"", line 616, in predict
    raise RuntimeError('A pipeline has not yet been optimized. Please call fit() first.')
RuntimeError: A pipeline has not yet been optimized. Please call fit() first.
>>> 

```",got error error probably problem case unfile version outdated version day ago decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator unsupported set combination decorator found array feature minimum optimization progress recent call last file line module file line file line compile open file line module file line predict raise pipeline yet please call fit first pipeline yet please call fit first,issue,negative,positive,positive,positive,positive,positive
306047795,"Yes, I've xgboost installed. I've updated xgboost today, I'll check if this solves/mitigate problem.",yes today check problem,issue,negative,neutral,neutral,neutral,neutral,neutral
306046234,"I'm not sure what it would take to add XGBoost GPU support to TPOT. I'd imagine it entails that you can't set `n_jobs!=1`, but are there any additional contraints?",sure would take add support imagine ca set additional,issue,positive,positive,positive,positive,positive,positive
306046105,Sometimes threads freeze and won't even respond to being interrupted. Do you have xgboost installed by chance? That's a common culprit.,sometimes freeze wo even respond interrupted chance common culprit,issue,negative,negative,negative,negative,negative,negative
305985147,"Great, this is working now (on a source install of xgboost v0.6 with GPU support).

Any plans to enable use of XGboost's GPU processing support through TPOT? That would allow for dramatically faster hyperparameter optimization.",great working source install support enable use support would allow dramatically faster optimization,issue,positive,positive,positive,positive,positive,positive
305979235,"Result is slightly different. CPU is still occupied as before, but a few pipelines were optimized before 'freeze'. 
Note, that limits on `max_time_mins=2, max_eval_time_mins=1` are ignored or treated with big margin.

```
Version 0.8.2 of tpot is outdated. Version 0.8.3 was released 12 minutes ago.
/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
28 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.

Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]
Optimization Progress:   5%|▌         | 1/20 [04:23<1:23:27, 263.56s/pipeline]
          
Skipped pipeline #4 due to time out. Continuing to the next pipeline.

          
Optimization Progress:  20%|██        | 4/20 [04:23<1:10:17, 263.56s/pipeline]
Optimization Progress:  25%|██▌       | 5/20 [05:37<47:30, 190.03s/pipeline]  
          
Skipped pipeline #8 due to time out. Continuing to the next pipeline.

          
Optimization Progress:  40%|████      | 8/20 [05:37<38:00, 190.03s/pipeline]
Optimization Progress:  45%|████▌     | 9/20 [06:07<24:47, 135.25s/pipeline]
```",result slightly different still note big margin version outdated version ago module version favor module class also note interface new different module module removed module removed decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator unsupported set combination decorator found array feature minimum optimization progress optimization progress pipeline due time next pipeline optimization progress optimization progress pipeline due time next pipeline optimization progress optimization progress,issue,positive,negative,neutral,neutral,negative,negative
305976718,"TPOT has been a little finnicky with XGBoost, but the latest release should work with XGBoost 0.6a2. I just committed a patch that should add support in for 0.6. There's a weird issue where XGBoost stores both a `seed` and `random_state` parameter, both of which do the same thing, and XGBoost throws an error if they're not equal. Good times... thankfully they're fixing it soon.

BTW: You can install XGBoost on Anaconda now: `conda install py-xgboost`",little latest release work patch add support weird issue seed parameter thing error equal good time thankfully fixing soon install anaconda install,issue,positive,positive,positive,positive,positive,positive
305972121,"OK, I think the issue is resolved. Unfortunately v0.6a2 is not available on WIndows 10 as far as I can see at this stage.",think issue resolved unfortunately available far see stage,issue,negative,positive,positive,positive,positive,positive
305955928,"I have very similar issue regardless pandas/numpy data (the same messages with verbosity=3). 
For boston regression example training is running, but I have a dataset when it hangs at first pipeline...

```
/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
28 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=0 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=1 __init__() got an unexpected keyword argument 'max_depth'
_pre_test decorator: _generate: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.

Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]

```

```
import numpy as np
import pandas as pd
from tpot import TPOTRegressor

train = pd.read_csv('train_A.csv')
test = pd.read_csv('test_A.csv')
y_train = train['y'].values
X_train = train.drop('y', axis=1)
X_train = X_train.values
X_test = test.values
tpot = TPOTRegressor(generations=5, population_size=20, n_jobs=-1, verbosity=3, 
                     max_time_mins=2, max_eval_time_mins=1, random_state=42)

tpot.fit(X_train, y_train)
y_predict = tpot.predict(X_test)
y_predict.to_csv('tpot1_prediction.csv')
#print(tpot.score(X_test, y_test))
print(tpot.score(X_train, y_train))
tpot.export('tpot_merc_pipeline.py')


```
[my_dataset.tar.gz](https://github.com/rhiever/tpot/files/1049425/my_dataset.tar.gz)


)",similar issue regardless data boston regression example training running first pipeline module version favor module class also note interface new different module module removed module removed decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator got unexpected argument decorator unsupported set combination decorator found array feature minimum optimization progress import import import train test train print print,issue,negative,positive,neutral,neutral,positive,positive
305949459,Got the Boston housing price example to work after uninstalling xgboost v0.6. Appears that it's the interaction with xgboost that is causing the issue? Had installed it using pip install xgboost.,got boston housing price example work interaction causing issue pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
305946032,Running Python 3.6.1 on Windows 10. Appreciate the help.,running python appreciate help,issue,positive,neutral,neutral,neutral,neutral,neutral
305935944,Can you please provide more details on your Python environment and OS? That example is running fine on my machines.,please provide python environment o example running fine,issue,negative,positive,positive,positive,positive,positive
305928645,"Should be fine now. Not sure what's going on with the test coverage decrease though -- maybe it's somehow accounting for the fact that XGBoost related code is not being tested in AppVeyor (however, the travis tests are unchanged)",fine sure going test coverage decrease though maybe somehow accounting fact related code tested however travis unchanged,issue,negative,positive,positive,positive,positive,positive
305907522,"TPOT can indeed run without XGBoost, and I don't think any of the unit tests should be affected by not having it.",indeed run without think unit affected,issue,negative,neutral,neutral,neutral,neutral,neutral
305904684,"@terrytangyuan sorry I didn't maintain the windows binary for XGBoost: there is no official windows binary support for now. We have the plan that, I talked with MXNet's windows binary maintainer for reusing his expertise on making an XGboost windows binary in PyPI (plus a variant with GPU tree building), but it may take a little bit more time. Let me check with him about the progress. 

Meanwhile, please follow https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en to build XGBoost in windows.",sorry maintain binary official binary support plan binary maintainer making binary plus variant tree building may take little bit time let check progress meanwhile please follow build,issue,positive,negative,negative,negative,negative,negative
305904017,"Does TPOT currently run without having XGBoost installed? In this case, I'd suggest to remove it from the Windows tests ",currently run without case suggest remove,issue,negative,neutral,neutral,neutral,neutral,neutral
305903363,"I don't use Windows either. @phunterlau maintains the Windows binaries, @phunterlau could you share some instruction/tricks here? ",use either could share,issue,negative,neutral,neutral,neutral,neutral,neutral
305902885,"Last I checked, XGBoost was practically impossible to install on Windows. But that's been a long time because I don't use Windows. Maybe @terrytangyuan can comment?",last checked practically impossible install long time use maybe comment,issue,negative,negative,negative,negative,negative,negative
305874067,"The setup seems to work in general:

![screen shot 2017-06-02 at 2 27 31 pm](https://cloud.githubusercontent.com/assets/5618407/26739400/a75a124e-479f-11e7-8066-9dea58ec431a.png)

There seems to be an issue with XGBoost and pip though. Do you know if XGBoost even supported by Windows? Or is there some trick for pip-installing it on Windows?
",setup work general screen shot issue pip though know even trick,issue,negative,positive,neutral,neutral,positive,positive
305874012,"Ohhh sweet, it IS integrated into GitHub! Looks like we're [failing](https://ci.appveyor.com/project/rhiever/tpot/build/1.0.1) on AppVeyor though because of the XGBoost install. Not surprised there.",sweet like failing though install,issue,negative,positive,positive,positive,positive,positive
305782614,Maybe it'd be a good idea (in a separate PR) to add a unit test importing all of those built-in import statements in the export module?,maybe good idea separate add unit test import export module,issue,negative,positive,positive,positive,positive,positive
305782342,Whoops! Guess that one got overlooked in @teaearlgraycold's reorg PR. This fix will probably roll out on to master/pip later today.,whoop guess one got fix probably roll later today,issue,negative,neutral,neutral,neutral,neutral,neutral
305761405,PR for the patch is posted! It maybe merged to master branch later. You may try my fixed branch just for now. ,patch posted maybe master branch later may try fixed branch,issue,negative,positive,neutral,neutral,positive,positive
305759983,"Hmm, you are right! It need to be updated since we moved these built-in operators to the `builtins ` folders. I will make a patch soon!",right need since make patch soon,issue,negative,positive,positive,positive,positive,positive
305678869,"True. But let's just wait to see if any other bug reports pop up in the next few days. For example, I've had TPOT freezing on me a lot when I run it from the CLI with `n_jobs!=1`, even with smaller datasets. Multiprocessing has me worried that it's causing too many freezing issues in TPOT.",true let wait see bug pop next day example freezing lot run even smaller worried causing many freezing,issue,negative,positive,positive,positive,positive,positive
305672795,"Hmm, nice catch. Maybe it can be a patch for 0.8.0. I think the bug will cause issues when verbosity>=3 in command line mode.",nice catch maybe patch think bug cause verbosity command line mode,issue,negative,positive,positive,positive,positive,positive
305667021,"I made a hacky demo of the TPOTEnsemble idea in [this commit](https://github.com/rhiever/tpot/commit/21a56ec082eb568c6ed6dcc1b804e11a2f474542).

It seemed to work fine in my tests, although it gets much, much slower as the generations pass because, e.g., by generation 100 every pipeline is being evaluated in a VotingClassifier with 99 other pipelines. The only reasonable solution seems to be to store the predictions of each ""best"" pipeline from every generation, and manually ensemble those predictions with the new predictions from the pipelines in the current generation.

Of course, there will be no way around storing the entire pipeline list in a VotingClassifier for new predictions in the TPOT `predict` and `score` functions. But at least the above solution will save evaluating the same list of pipelines over and over again.",made hacky idea commit work fine although much much pas generation every pipeline reasonable solution store best pipeline every generation manually ensemble new current generation course way around entire pipeline list new predict score least solution save list,issue,positive,positive,positive,positive,positive,positive
305623724,One hot encoder is not added since #462 was not merged yet,one hot added since yet,issue,negative,positive,positive,positive,positive,positive
305613102,Fantastic! This tools is quickly becoming an integral part of the 'experimentation' phase of my projects. Thanks for all the hard work.,fantastic quickly becoming integral part phase thanks hard work,issue,positive,positive,positive,positive,positive,positive
305612753,"My pleasure. BTW, next version of TPOT will have imputation function to deal with NA. #253 ",pleasure next version imputation function deal na,issue,positive,neutral,neutral,neutral,neutral,neutral
305612199,"Done and dusted, I had NA's in my X_train. Did a fillna and it's golden. Thanks for helping me walk through it. ",done na golden thanks helping walk,issue,positive,positive,positive,positive,positive,positive
305611312,"Please try this codes below in your data and Let me know what happens.
```
from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth=5)
clf = clf.fit(X_train, y_train)",please try data let know import,issue,negative,neutral,neutral,neutral,neutral,neutral
305610477,"Oh sorry, I did both and neither worked.",oh sorry neither worked,issue,negative,negative,negative,negative,negative,negative
305609761,I tried both with and without as_matrix() and both gave the same error. ,tried without gave error,issue,negative,neutral,neutral,neutral,neutral,neutral
305609389,"From the error message `tpot.fit(X_train.as_matrix(), y_train.as_matrix())`, I think you might add `as_matrix()` in the codes. Could you try the codes again without `as_matrix()`?",error message think might add could try without,issue,negative,neutral,neutral,neutral,neutral,neutral
304929698,This feature will be added in the upcoming 0.8 release. Thanks again @zoso95.,feature added upcoming release thanks,issue,negative,positive,positive,positive,positive,positive
304377690,"Thanks guys. using cv=2 will solve the puzzle. 
if one decides to use "" roc_auc"" score as the scoring function for TPOTClassifier object, 
the internal CV will return ""inf"", when there is an exception occurred in the scoring function. 
I wrote a sample code that can reproduce the error. It is because the roc_auc can only handle binary class labels. It will raise error for multiclasses. 
However, tpot currently is not passing any error message. Maybe passing the error message from sklearn function?
 

import sklearn.metrics.roc_auc_score as roc
metric.roc_auc_score([1,-1,1],[1,-1,1])",thanks solve puzzle one use score scoring function object internal return exception scoring function wrote sample code reproduce error handle binary class raise error however currently passing error message maybe passing error message function import roc,issue,negative,positive,neutral,neutral,positive,positive
304324476,"Yes, sorry.  Just saw that.  Any updates on getting this compatibility issue fixed?  

I'll remove seed on my xgboost to see what happens.",yes sorry saw getting compatibility issue fixed remove seed see,issue,negative,negative,negative,negative,negative,negative
304317852,@rhiever No I don't think that's feasible. I am thinking it would just be good to support existing TF Estimators (other people from Google and the community will open-source more estimators) in a similar fashion to supporting sklearn estimators. All the distributed logics are inside TF Estimators instead of in TPOT since those are hard to implement and maintain. TPOT would focus on providing genetic programming to both sklearn and TF Estimators. ,think feasible thinking would good support people community similar fashion supporting distributed inside instead since hard implement maintain would focus providing genetic,issue,positive,positive,positive,positive,positive,positive
304316228,"@terrytangyuan: Do I understand right that, to support the models, transformers, etc. in scikit-learn, we would need to rewrite all of the algorithms in a tflearn-like format?",understand right support would need rewrite format,issue,negative,positive,positive,positive,positive,positive
304315613,"@weixuanfu2016 is correct. If you are using `cv=5`, you must have at least 5 training samples for each class.",correct must least training class,issue,negative,negative,negative,negative,negative,negative
304152862,"@weixuanfu2016 Yes that's exactly the example canned estimators I was referring to that has sklearn-like interface. They will be moved to TensorFlow core soon but it will take some time. The only stable ones can be found [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator/canned). For each estimator, you can set their configuration that contains all the distributed logics, e.g. master, workers, parameter servers. However, this might not be something @rhiever talked about. I need to look into the code a little bit more. ",yes exactly example canned interface core soon take time stable found estimator set configuration distributed master parameter however might something need look code little bit,issue,positive,positive,neutral,neutral,positive,positive
304122547,"Hmm, I think the reason is the sample size is too small to use `cv=5` in TPOT. Please check the cv parameter in [`cross_eval_score` in scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)

The codes below can reproduce this issue.

```
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
clf = DecisionTreeClassifier(max_depth=5)
y=np.array([ 1., 1., 0., -1., 0.])
x=np.array([[-1.1491834 , -1.77252632, 0.72741207],
[-0.69059757, -1.59468535, 0.13948231],
[ 0.06890373, 0.43569532, 0.21672055],
[ 1.77999225, 0.23434428, 0.49602475],
[ 1.09255898, 1.56640373, -0.47355987]])
# pass with warning
cv_scores_1 = cross_val_score(clf, x, y, cv=2)
print(cv_scores_1)
# fail here
cv_scores_2 = cross_val_score(clf, x, y, cv=5)
```",think reason sample size small use please check parameter reproduce issue import import import pas warning print fail,issue,negative,negative,negative,negative,negative,negative
304116455,"@terrytangyuan I think `tf.contrib.learn.BaseEstimator` is worthy to look 
 into. https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/learn/python/learn/estimators/estimator.py 

I agree to drop `multiprocessing` in TPOT and use `TensorFlow` but many scikit-learn's estimator using `multiprocessing` backend in `joblib`, especially for those `ensemble` method. (check [this example](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/ensemble/forest.py#L323)). It is computationally expensive part within some operators. So apply parallel computing in TPOT only may not speed up a lot since TPOT need evaluate these scikit-learn pipelines. I think the ideal way is to warp some scikit-learn operators by TensorFlow like [this SVM](https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/learn/python/learn/estimators/svm.py), which can speed up scikit-learn's end",think worthy look agree drop use many estimator especially ensemble method check example expensive part within apply parallel may speed lot since need evaluate think ideal way warp like speed end,issue,positive,positive,positive,positive,positive,positive
304099846,"👍 Excited to collaborate on this issue, @terrytangyuan!

Note that the part we're thinking of using TF for is the pipeline evaluations, which happens [here](https://github.com/rhiever/tpot/blob/7b1eb275b7f00a581fc37575b92f0ef8cee16ee8/tpot/base.py#L756:L757). Essentially, every iteration of TPOT runs the sklearn `cross_val_score` function on valid sklearn Pipelines, which can contain one or many sklearn estimators and transformers.

We would definitely drop multiprocessing support (which has had many bugs!) if we could run sklearn on TF.",excited collaborate issue note part thinking pipeline essentially every iteration function valid contain one many would definitely drop support many could run,issue,positive,positive,positive,positive,positive,positive
304097570,There has been a lot of changes to TF Estimator (many of them are now in core so they are more stable now) so I am not sure if it's still working with sklearn's pipeline. I'll take a look at the codebase for TPOT soon and hopefully propose a better solution (my last comment assumes no knowledge in TPOT but hopefully could help move this discussion along from TF perspective).,lot estimator many core stable sure still working pipeline take look soon hopefully propose better solution last comment knowledge hopefully could help move discussion along perspective,issue,positive,positive,positive,positive,positive,positive
304095198,I came across similar thought that I shared with @rhiever today. I am not familiar with TPOT at all but my initial/naive idea would be dropping TPOT's multiprocessing and use TensorFlow's [Estimator API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator) that provides simple interface similar to sklearn. Then allow TPOT to specify the [RunConfig class](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/run_config.py) for Estimator that has all the specifications for distributed training. ,came across similar thought today familiar idea would dropping use estimator simple interface similar allow specify class estimator distributed training,issue,negative,positive,neutral,neutral,positive,positive
304068897,"@fferroni Could you please install [version 0.6a2 of xgboost](https://pypi.python.org/pypi/xgboost/)? The version 0.7.5 of TPOT works with this version of xgboost in Pypi. 

If you need build xgboost 0.6a2 from source codes, you may find the source codes in the Pypi webpage.",could please install version version work version need build source may find source,issue,negative,neutral,neutral,neutral,neutral,neutral
304044864,Thank you for suggestion. It is already fixed in dev branch on [this line](https://github.com/rhiever/tpot/blob/development/tpot/base.py#L123),thank suggestion already fixed dev branch line,issue,negative,positive,neutral,neutral,positive,positive
304044387,"Wow. That's great!
One more suggestion on the minor typo error in the documentation of the ""base.py""
In the available options of ""scoring"" argument, there is an option as 'balanced accuracy', where it should really be 'balanced_accuracy'
Thanks again for the great work here :)",wow great one suggestion minor typo error documentation available scoring argument option accuracy really thanks great work,issue,positive,positive,positive,positive,positive,positive
304034903,"I made a PR #467 to fix this issue based on dev branch.
 The changes are simple as shows in the [link](https://github.com/rhiever/tpot/pull/467/files).

@rhiever should we make a patch for master branch?
",made fix issue based dev branch simple link make patch master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
304013292,Looks like this needs to be rebased with your latest repo reorganization PR.,like need latest reorganization,issue,negative,positive,positive,positive,positive,positive
304012598,"Funny you say that---we just added that to the docs on the dev branch, so it will be out in the next release. :-)

Glad the issue was sorted out.",funny say added dev branch next release glad issue sorted,issue,positive,positive,positive,positive,positive,positive
304011116,"I found the reason. It is from [this source code](https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L165) in xgboost. If the `seed` is not set, it would be assigned the default value 0 of `random_state` .",found reason source code seed set would assigned default value,issue,negative,neutral,neutral,neutral,neutral,neutral
304009997,The code snippet I sent is the new error... :/,code snippet sent new error,issue,negative,positive,positive,positive,positive,positive
304008845,"Hi,

The code you suggest doesn't work. I get the same error.

```
RuntimeError: Cannot clone object XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.01, max_delta_step=0,
       max_depth=3, min_child_weight=5, missing=None, n_estimators=100,
       n_jobs=1, nthread=1, objective='binary:logistic', random_state=42,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,
       subsample=0.8500000000000001), as the constructor does not seem to set parameter seed
```

Doesn't seem to change seed=0 to seed=None (which I think is what you want to do?)",hi code suggest work get error clone object logistic constructor seem set parameter seed seem change think want,issue,negative,neutral,neutral,neutral,neutral,neutral
304007817,"Thank you for the quick test. The warning message is normal because the `nthread` and `seed` are deprecated and replaced by `n_jobs` and `random_state` in the latest version.

I will submit a PR to fix this compatibility issue with the latest version of xgboost. Meanwhile you can try to use the codes below to use xgboost in TPOT 0.7.5.

```
from tpot.config_classifier import classifier_config_dict
from tpot import TPOTClassifier
import numpy as np
xgbclf_correct = {
        'n_estimators': [100],
        'max_depth': range(1, 11),
        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],
        'subsample': np.arange(0.05, 1.01, 0.05),
        'min_child_weight': range(1, 21),
        'n_jobs': [1], 
        'seed': [None]
    }
classifier_config_dict['xgboost.XGBClassifier'] =  xgbclf_correct

clf = TPOTClassifier(verbosity=2, config_dict=classifier_config_dict)
clf.fit(X.loc[train_ind], Y.loc[train_ind])

```
Please let me know how the codes works.
",thank quick test warning message normal seed latest version submit fix compatibility issue latest version meanwhile try use use import import import range range none please let know work,issue,negative,positive,positive,positive,positive,positive
304003070,"The code you wrote in your comment doesn't seem to raise any errors. Only a warning:

```
/Users/xxx/anaconda/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/sklearn.py:171: DeprecationWarning: The nthread parameter is deprecated as of version .6.Please use n_jobs instead.nthread is deprecated.
  'nthread is deprecated.', DeprecationWarning)
```",code wrote comment seem raise warning parameter version please use,issue,negative,neutral,neutral,neutral,neutral,neutral
304002597,"I suspected the latest version of XGBoost did something abnormal since it put both `seed` and `random_state` into parameter list. Could you please try the codes below? Please let me know if the `RuntimeError` still happens . I will also test it after I installed this version of XGBoost in my environment.

```
from xgboost import XGBClassifier
from sklearn.base import clone
clf = XGBClassifier(base_score=0.5, colsample_bylevel=1,
                    colsample_bytree=1, gamma=0, learning_rate=0.001, max_delta_step=0,
                    max_depth=3, min_child_weight=9, missing=None, n_estimators=100,
                    nthread=1, objective='binary:logistic',
                    reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, random_state=42, silent=True,
                    subsample=0.45)

clf_clone  = clone(clf)
```",suspected latest version something abnormal since put seed parameter list could please try please let know still also test version environment import import clone logistic clone,issue,negative,positive,positive,positive,positive,positive
303847198,"
[![Coverage Status](https://coveralls.io/builds/11676719/badge)](https://coveralls.io/builds/11676719)

Coverage remained the same at 87.129% when pulling **3f9d185ddf5c1565b8efd0aa6dadf6dfae6b5e6a on weixuanfu2016:doc** into **305701c6c3dd280f2d1ac6ebec6fa8548e08800e on rhiever:development**.
",coverage status coverage doc development,issue,negative,neutral,neutral,neutral,neutral,neutral
303827785,"Hi, 

First of all, thanks for the awesome project. It just got into it two days ago. I'm loving it a lot. 
The problem is solved as I restart my python terminal. I took a look at the source code and I see that the cv input argument is passing directly to sklearn cross_val_score function. 
No problem with this. Just would like to make a suggestion by include these possible cv objects in the documentations in the base.py script. 
",hi first thanks awesome project got two day ago loving lot problem restart python terminal took look source code see input argument passing directly function problem would like make suggestion include possible script,issue,positive,positive,positive,positive,positive,positive
303823058,"Hi @simonzcaiman, can you please explain why you think TPOT is still using K-fold with random shuffling from this example? We pass the object given to the `cv` parameter directly to the `cross_val_score` function, so it should work just the same.",hi please explain think still random shuffling example pas object given parameter directly function work,issue,negative,negative,negative,negative,negative,negative
303726451,"Hi guys, 
I try to create a time series split object from sklearn.model_selection and pass it into the tpot classifier input argument as cv. It seems like internal cv is still K-folds with random shuffling. 
I'm dealing with time series classification problem. This might create a look ahead bias. 
Is there a way to change the tpot internal cv method? 

Tpotregressor has ""num_cv_folds"", but TpotClassifier does not. 

Here is my code: 
tscv = TimeSeriesSplit(n_splits=10)
tpot = TPOTClassifier(generations=4, population_size=40, verbosity=2,cv=tscv)",hi try create time series split object pas classifier input argument like internal still random shuffling dealing time series classification problem might create look ahead bias way change internal method code,issue,negative,negative,negative,negative,negative,negative
303519897,I also added the patches in master branch (0.7.5) in this PR,also added master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
303510430,Fixed in the latest dev branch.,fixed latest dev branch,issue,negative,positive,positive,positive,positive,positive
303480274,"Hi @chivalry123, did that change work for you?",hi chivalry change work,issue,negative,neutral,neutral,neutral,neutral,neutral
303478375,Looks like something broke with the rest of the merges. :-(,like something broke rest,issue,negative,neutral,neutral,neutral,neutral,neutral
303478071,"Worked out some merge conflicts. @zoso95, can you please add some unit tests to the PR that test and demonstrate the intended functionality?",worked merge please add unit test demonstrate intended functionality,issue,negative,neutral,neutral,neutral,neutral,neutral
303475128,I think I'll close this PR because this seems like a roundabout way to fix a problem with XGBoost. We may want stdout in the future.,think close like roundabout way fix problem may want future,issue,negative,neutral,neutral,neutral,neutral,neutral
303473910,"
[![Coverage Status](https://coveralls.io/builds/11653523/badge)](https://coveralls.io/builds/11653523)

Coverage increased (+0.2%) to 87.035% when pulling **261b0bd74476f9120a835ea54b6c83c8d64665d4 on teaearlgraycold:imputer** into **991fb0fc9ad95efd1b224bffbb7690ce894208e4 on rhiever:development**.
",coverage status coverage imputer development,issue,negative,neutral,neutral,neutral,neutral,neutral
303372409,"Thank you very much for the responses, I'll test this as soon as possible.",thank much test soon possible,issue,negative,positive,neutral,neutral,positive,positive
303213804,"Hi @axelroy,

If you want to access the best model from the TPOT run, you can access it with the `tpot. _fitted_pipeline` property at the end of the run. If you run TPOT at the highest verbosity (3), you can also access the entire Pareto front of best pipelines with the `tpot._pareto_front_fitted_pipelines` property. Note that both of these properties are only assigned at the end of a TPOT run.

In terms of presenting feature importances, those are limited to specific models. In the case of the paper you linked, those were decision trees and random forests, so I was displaying tree-based feature importances. If TPOT discovers a pipeline for you that uses a decision tree or other tree-based method as the final classifier, for example, then you could access those feature importances with the following code:

```python
# The first index to -1 gets the last step in the pipeline
# The second index to 1 gets the actual classifier object
tpot._fitted_pipeline.steps[-1][1].feature_importances_
```

which is an array of feature importances that you can then match with the feature names. The same applies with linear models, except you'd access the `coef_` property instead.",hi want access best model run access property end run run highest verbosity also access entire front best property note assigned end run feature limited specific case paper linked decision random feature pipeline decision tree method final classifier example could access feature following code python first index last step pipeline second index actual classifier object array feature match feature linear except access property instead,issue,positive,positive,positive,positive,positive,positive
303182413,@rasbt Thank you for sharing the way of CV in `StackingCVClassifier`. But I think the pipelines in TPOT stack only one estimator as a step to add synthetic feature(s) into transformed X. So I reimplement the Stacking class based on the codes in your tools and simplified it as a transformer by removing meta-estimator. I put the transformer into the PR #460 ,thank way think stack one estimator step add synthetic feature class based simplified transformer removing put transformer,issue,negative,neutral,neutral,neutral,neutral,neutral
303172852,I close this old PR since the PR #460 covered new function in this PR already,close old since covered new function already,issue,negative,positive,positive,positive,positive,positive
303139586,"For the first question, I think this is right way to use it for current version and next version (0.8). You can also find the example in the unit test, [test_evaluated_individuals](https://github.com/rhiever/tpot/blob/development/tests.py#L643-L666), if the way is updated in future versions.

For the second one. for now TPOT cannot provide the ranking of features' importance as Figure 5 in the paper. The importance of features on page 12 was [estimated using Random Forest method](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#giniimp). 
",first question think right way use current version next version also find example unit test way future second one provide importance figure paper importance page random forest method,issue,positive,positive,neutral,neutral,positive,positive
303093197,"Fixed all conflicts + additional bugs other people added (unit tests broke,
is that a thing?)
Hoping I can add the features I'm using for a while now, once you accept
that PR request I'll open one for some of them hopefully...

I'm getting tired of fixing conflicts so many times :(
-- 
Cheers,
Dani K.
",fixed additional people added unit broke thing add accept request open one hopefully getting tired fixing many time,issue,negative,positive,neutral,neutral,positive,positive
303090671,"Fixed all requested comments.
Fixed some additional bugs.
Fixed unit tests which were broken again by someone else (is that a standard occurence?)...

@rhiever 
Please accept my PR ^_^",fixed fixed additional fixed unit broken someone else standard please accept,issue,negative,negative,neutral,neutral,negative,negative
302882761,"Cool! 
> TPOT should be handling the CV, so no need to use the CV versions IMO.

I think it's a bit different in this context from what TPOT is doing with CV. Here, it's an internal way of doing CV between the 1st & 2nd level classifiers (not a wrapper around the classifier). Basically, the 2nd level classifier gets different features from the 1st level classifier here:

> In the standard stacking procedure, the first-level classifiers are fit to the same training set that is used prepare the inputs for the second-level classifier, which may lead to overfitting. The StackingCVClassifier, however, uses the concept of cross-validation: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level classifier; in each round, the first-level classifiers are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level classifier. After the training of the StackingCVClassifier, the first-level classifiers are fit to the entire dataset

![stacking_cv_algorithm](https://cloud.githubusercontent.com/assets/5618407/26277457/d5da24d2-3d55-11e7-90cb-b9d892131a50.png)
",cool handling need use think bit different context internal way st level wrapper around classifier basically level classifier different st level classifier standard procedure fit training set used prepare classifier may lead however concept split successive used fit first level classifier round applied subset used model fitting iteration resulting provided input data classifier training fit entire,issue,positive,positive,positive,positive,positive,positive
302877806,"I'm 👍 on porting in the stacking classifier/regressor into an `external` directory.

TPOT should be handling the CV, so no need to use the CV versions IMO.",external directory handling need use,issue,negative,neutral,neutral,neutral,neutral,neutral
302875561,"The code for the StackingRegressor/Classifier is indeed super simple. You could either port it or just put it under an `tpot/external` path like they do in scikit-learn with joblib, [for example](https://github.com/scikit-learn/scikit-learn/tree/master/sklearn/externals) to avoid additional external dependencies; mlxtend is under an MIT license, which should be compatible with GNU/GPL.

Regarding the StackingClassifier, I would maybe recommend to also (or instead) consider the [StackingCVClassifier](http://rasbt.github.io/mlxtend/user_guide/classifier/StackingCVClassifier/), which is a bit more robust when it comes to overfitting :)",code indeed super simple could either port put path like example avoid additional external license compatible regarding would maybe recommend also instead consider bit robust come,issue,positive,positive,positive,positive,positive,positive
302867754,Sounds good and thank you for the nice tool @rasbt ,good thank nice tool,issue,positive,positive,positive,positive,positive,positive
302867680,Will have to test and verify when I get back.,test verify get back,issue,negative,neutral,neutral,neutral,neutral,neutral
302867631,"Rather than adding a new dependency (even if it's a nice one, @rasbt ;-) ), could we port the underlying code for those stacking models to TPOT? I think it's realistically only something like [this](https://github.com/rasbt/mlxtend/blob/5d9938626fe98ff26ce89fbd0ae0557619864630/mlxtend/regressor/stacking_regression.py#L134).",rather new dependency even nice one could port underlying code think realistically something like,issue,positive,positive,positive,positive,positive,positive
302867577,"Cool, I'll try to PR whatever I can..

On Sat, 20 May 2017, 13:56 Randy Olson, <notifications@github.com> wrote:

> I'm hoping for a release early this upcoming week. Life may get in the way
> and change that :-) but that's the plan right now.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/432#issuecomment-302865985>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7L5ZAQ7kDIO-uhL5pYaos6sHkBErks5r7sbJgaJpZM4NMiJi>
> .
>
",cool try whatever sat may randy wrote release early upcoming week life may get way change plan right thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
302867379,"@rhiever I found two replacements for stacking classifiers or regressors. Check the links below:

[StackingClassifier](https://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier)
[StackingRegressor](https://rasbt.github.io/mlxtend/user_guide/regressor/StackingRegressor)
",found two check link,issue,negative,neutral,neutral,neutral,neutral,neutral
302867005,"I think we don't need stdout when verboisty==2, since process bar uses stderr instead of stdout in this case.",think need since process bar instead case,issue,negative,neutral,neutral,neutral,neutral,neutral
302866962,"@rhiever  the outputs from the `VotingClassifier` are continuous when running with regressors, but not the right prediction of y. It is very weird. I think `VotingClassifier` should raise warning message when running with regressor.",continuous running right prediction weird think raise warning message running regressor,issue,negative,negative,negative,negative,negative,negative
302866302,"As @weixuanfu2016 mentioned, we use the VotingClassifier to [stack](https://www.quora.com/What-is-stacking-in-machine-learning) intermediate classifiers in TPOT pipelines. As @weixuanfu2016 pointed out, VotingClassifier doesn't seem to work quite as expected with regression models.

Regardless, TPOT seems to have found that stacking a couple regression models' predictions improves the predictive performance of your model, even if the outputs from the VotingClassifier aren't continuous. You can validate this claim by comparing your original pipeline's score to this one:

```python
exported_pipeline = make_pipeline(
    VarianceThreshold(threshold=0.7000000000000001),
    ExtraTreesRegressor(max_features=0.05, min_samples_leaf=20, min_samples_split=3, n_estimators=100)
)
```

We'll have to look into this issue deeper. I'll file this as a bug report for now. Thank you.",use stack intermediate pointed seem work quite regression regardless found couple regression predictive performance model even continuous validate claim original pipeline score one python look issue file bug report thank,issue,positive,positive,positive,positive,positive,positive
302865985,"I'm hoping for a release early this upcoming week. Life may get in the way and change that :-) but that's the plan right now.

We can always target the next group of features for 0.9, or even a minor 0.8 release if they're not major changes. I suspect they will entail some detailed discussions.",release early upcoming week life may get way change plan right always target next group even minor release major suspect entail detailed,issue,negative,positive,positive,positive,positive,positive
302865926,"Try changing your code to

```python
tpot = TPOTRegressor(generations=5, population_size=200, verbosity=3,n_jobs=16,)
tpot.fit(x_pca_all_imputed[:len_training].values, y_train_log.values)
```

Importantly, adding the `.values` should convert the pandas DataFrames to NumPy matrices. You can learn more about the data representation in scikit-learn (and TPOT) [here](https://github.com/amueller/scipy-2017-sklearn/blob/master/notebooks/03.Data_Representation_for_Machine_Learning.ipynb).",try code python importantly convert matrix learn data representation,issue,negative,positive,positive,positive,positive,positive
302853581,"How soon is the next release? Will I make it adding my actual features into
it? As soon as this PR is merged I was planning to open a PR on each of
those..

On Sat, 20 May 2017, 00:24 Dan Koretsky, <dankoretsky@gmail.com> wrote:

> Cool, I was hoping to finish it early next week.
>
> On Fri, 19 May 2017, 23:58 Randy Olson, <notifications@github.com> wrote:
>
>> ETA on the updates? I plan to review and start merging again for a
>> probable 0.8 release starting next Monday.
>>
>> —
>> You are receiving this because you authored the thread.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/rhiever/tpot/pull/432#issuecomment-302809869>, or mute
>> the thread
>> <https://github.com/notifications/unsubscribe-auth/AMzW7PXAq2H9yhBDp2xVMBrwvuPk5jEEks5r7gKOgaJpZM4NMiJi>
>> .
>>
>
",soon next release make actual soon open sat may dan wrote cool finish early next week may randy wrote eta plan review start probable release starting next thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
302841910,"Thank you for letting us know this issue. I use the codes below to check if `VotingClassifier` also works for regressor. Sadly, it did not work as expected. We may need avoid `VotingClassifier` in `TPOTRegressor ` or make a `VotingRegessor` in TPOT.
```
import numpy as np
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.datasets import make_regression, make_classification
from sklearn.metrics import r2_score, accuracy_score
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from copy import copy

print('Test Regression')

reg1 = LinearRegression()

X, y = make_regression(n_samples=50, n_features=5, random_state=0)

ereg1 = make_pipeline(make_union(VotingClassifier(estimators=[
        ('lr', LinearRegression())], voting='hard'), FunctionTransformer(copy)))

ereg1.fit(X, y)
reg1.fit(X,y)
print('Regressor out of pipeline', r2_score(y, reg1.predict(X)))
print('Regressor in pipeline',r2_score(y, ereg1.transform(X)[:,0]))
print('If the synthetic feature is the predict y in regressor: ', np.allclose(reg1.predict(X), ereg1.transform(X)[:,0]))

print('Test Classifcation')
clf1 = LogisticRegression()
X, y = make_classification(n_samples=50, n_features=5, random_state=0)
eclf1 = make_pipeline(make_union(VotingClassifier(estimators=[
        ('lr', LogisticRegression())], voting='hard'), FunctionTransformer(copy)))

eclf1.fit(X, y)
clf1.fit(X,y)
print('Classifier out of pipeline', accuracy_score(y, clf1.predict(X)))
print('Classifier in pipeline', accuracy_score(y, eclf1.transform(X)[:,0]))
print('If the synthetic feature is the predict y in classifier: ', np.allclose(clf1.predict(X), eclf1.transform(X)[:,0]))

```

Output:
```
Test Regression
Regressor out of pipeline 1.0
Regressor in pipeline 0.188336862313
If the synthetic feature is the predict y in regressor:  False
Test Classifcation
Classifier out of pipeline 0.98
Classifier in pipeline 0.98
If the synthetic feature is the predict y in classifier:  True
```
",thank u know issue use check also work regressor sadly work may need avoid make import import import import import import import copy import copy print regression reg copy print pipeline print pipeline print synthetic feature predict regressor print copy print pipeline print pipeline print synthetic feature predict classifier output test regression regressor pipeline regressor pipeline synthetic feature predict regressor false test classifier pipeline classifier pipeline synthetic feature predict classifier true,issue,negative,negative,negative,negative,negative,negative
302839318,"The purpose of the`make_union(VotingClassifier(.... FunctionTransformer...))` in the codes is that if one step in a pipeline is an estimator and is not the last step then we must add its guess as a synthetic feature. 

Like you mentioned, `VotingClassifier ` is not designed for regressor but somehow it could pass the prediction as a synthetic feature to next steps. I think we need run some tests before deciding if TPOT should avoid using `VotingClassifier`in regression program.

The issue is related to #214",purpose one step pipeline estimator last step must add guess synthetic feature like designed regressor somehow could pas prediction synthetic feature next think need run avoid regression program issue related,issue,negative,neutral,neutral,neutral,neutral,neutral
302814726,"Cool, I was hoping to finish it early next week.

On Fri, 19 May 2017, 23:58 Randy Olson, <notifications@github.com> wrote:

> ETA on the updates? I plan to review and start merging again for a
> probable 0.8 release starting next Monday.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/432#issuecomment-302809869>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7PXAq2H9yhBDp2xVMBrwvuPk5jEEks5r7gKOgaJpZM4NMiJi>
> .
>
",cool finish early next week may randy wrote eta plan review start probable release starting next thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
302810445,"Hi @slcott, a small update. Sorry we didn't merge this PR back when it was ready. It looks like we will be reworking this PR to fit into the most recent TPOT framework. We tried to rebase this PR to do so, but it would've taken more work than just starting from scratch. Thank you for your contribution nonetheless. We will be sure to credit you in the rebase.",hi small update sorry merge back ready like fit recent framework tried rebase would taken work starting scratch thank contribution nonetheless sure credit rebase,issue,positive,positive,neutral,neutral,positive,positive
302809869,ETA on the updates? I plan to review and start merging again for a probable 0.8 release starting next Monday.,eta plan review start probable release starting next,issue,negative,neutral,neutral,neutral,neutral,neutral
302809353,Will review when I get back to the office next week.,review get back office next week,issue,negative,neutral,neutral,neutral,neutral,neutral
302798900,"@weixuanfu2016 Thanks for the suggestion. I'll try to familiarize myself with the project structure before making any potential contributions. I'll do my best to read and absorb the documentation on the Github Page, but if the two of you have any other recommendations for the best route to become familiar with TPOT's code base, I welcome the suggestions.

TensorFlow 1.2rc0 was just ""announced"" on twitter this morning by some of the devs. Based on past releases, that would suggest 1.2.0 will be releases by June, perhaps as early as next week. There are no breaking changes between 1.1 and 1.2 that look relevant to TPOT. More interestingly, though, is that 1.2 is the first version that will include `tf.contrib.data`, which is a tool meant specifically for pipelining datasets by introducing a `Dataset` and `Iterator` abstraction to TensorFlow. `tf.contrib` libraries aren't officially supported, but the most used ones are frequently integrated into the main library, and this looks to be one that'll follow that path. I think sticking 1.1 for now is good for the initial changes, but the TPOT team should keep an eye on TensorFlow's Dataset API for potential uses.",thanks suggestion try familiarize project structure making potential best read absorb documentation page two best route become familiar code base welcome twitter morning based past would suggest june perhaps early next week breaking look relevant interestingly though first version include tool meant specifically abstraction officially used frequently main library one follow path think sticking good initial team keep eye potential,issue,positive,positive,positive,positive,positive,positive
302715119,Thanks a bunch for the speedy fix!,thanks bunch speedy fix,issue,negative,positive,positive,positive,positive,positive
302714448,Fixed and rolled out in the latest TPOT release.,fixed rolled latest release,issue,negative,positive,positive,positive,positive,positive
302714065,Fixed and rolled out in TPOT 0.7.5. Thank you for the bug report.,fixed rolled thank bug report,issue,negative,positive,neutral,neutral,positive,positive
302710631,"I just posted a PR for fixing this issue. It is not merged to master branch now. If you need fix the tpot in your local environment for running some urgent jobs. Please try the command below to reinstall tpot from the fixed branch.

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@issue454
```


",posted fixing issue master branch need fix local environment running urgent please try command reinstall fixed branch pip install upgrade,issue,negative,positive,neutral,neutral,positive,positive
302701230,"Oh, right, this is a bug. I will submit a PR soon to fix it. ",oh right bug submit soon fix,issue,negative,positive,positive,positive,positive,positive
302699435,I changed the base to the development branch. Looks like there are several conflicts that need to be resolved now.,base development branch like several need resolved,issue,negative,negative,negative,negative,negative,negative
302698016,"Looks good. Could you please merge it to dev branch instead of master branch? 

May also need update description of cv parameter in TPOTbase class in`base.py` and `driver.py`",good could please merge dev branch instead master branch may also need update description parameter class,issue,positive,positive,positive,positive,positive,positive
302694719,"I'm 👍 on using the latest version of TF. What's the ETA for 1.2?

I'm thinking TF integration would come out in TPOT 0.9, as we're ramping up to roll out all of the latest features in a 0.8 release soon.",latest version eta thinking integration would come ramping roll latest release soon,issue,negative,positive,positive,positive,positive,positive
302587901,"I ran into this issue recently. Sklearn let's you pass in custom cross validation into cross_val_score directly. For example
```
gkf = GroupKFold(n_splits=5)
X = df[""numerical stuff""]
group = df[""some category""]
 scores = cross_val_score(model, X,Y, cv=gkf, groups=group)
```

The only thing to note here is you also need to pass in groups in addition to the CV object. 

",ran issue recently let pas custom cross validation directly example numerical stuff group category model thing note also need pas addition object,issue,negative,positive,neutral,neutral,positive,positive
302564393,"Do you mean there is a way to warp these scikit-learn operators in both config files? If so, please check `operator_utils.py`.

I am thinking about replacing the joblib in `base.py` since they have issues when dealing with large dataset. (#436)

Maybe we should support the current stable version 1.1.",mean way warp please check thinking since dealing large maybe support current stable version,issue,positive,negative,neutral,neutral,negative,negative
302558827,"I think this is a feasible change to make - all the info really needed is what files need to have tensorflow analogs inserted. Right now it looks like `config_regression.py` and `config_classifier.py` are the files that needs to have the relevant tf models added. I haven't used tpot before, but I should be able to make the changes relatively easily if the first post is edited to include a checklist and milestones, so that I make sure I'm understanding correctly what all.

It would also be a good idea to decide what version of TensorFlow to use as a base since (as I think someone mentioned on twitter) the API has undergone a lot of changes since its initial release. Things have stabilized a bit now that it's past 1.0.0, but it's still possible some breaking changes might be introduced in the future. Right now the stable version is 1.1.0, but there's already a release candidate for 1.2. Which should we be aiming to guarantee support for?",think feasible change make really need inserted right like need relevant added used able make relatively easily first post include make sure understanding correctly would also good idea decide version use base since think someone twitter lot since initial release bit past still possible breaking might future right stable version already release candidate aiming guarantee support,issue,positive,positive,positive,positive,positive,positive
302532591,"Thank you for these good ideas.

It is hard to tell whether the feature-selection step would remove all features before running the pipeline, and it also depends on data. We will refine parameters in selectors (#423) to prevent this issue.

In my codes above, I tried XGBoosts built-in error `XGBoostError` in their python wrapper but it still printed out the error message even the program is still running. I think `std::ostringstream` in xgboost C++ source codes printed out error in stdout. It is very strange.",thank good hard tell whether step would remove running pipeline also data refine prevent issue tried error python wrapper still printed error message even program still running think source printed error strange,issue,negative,positive,positive,positive,positive,positive
302528595,"I think the 800-fold speedup may use GPU.

The example seems not use TensorFlow to speed up the whole pipeline but only for the `learn.DNNClassifier` step. 

I think the Distributed Computing in TensorFlow is very promising. Here are some links about it:

http://ischlag.github.io/2016/06/12/async-distributed-tensorflow/ 
http://learningtensorflow.com/lesson11/",think may use example use speed whole pipeline step think distributed promising link,issue,negative,positive,positive,positive,positive,positive
302247092,"Ah that makes sense, good catch!

Would it be acceptable to just suppress any pipelines that don't meet certain conditions (in this case, not passing in any features due to no features meeting the feature-selection threshold) so they don't get scored / crossed over with other pipelines?

I see what you're saying though, it would probably be best to try to use XGBoosts built-in error checking from a maintainability perspective, right?",ah sense good catch would acceptable suppress meet certain case passing due meeting threshold get scored crossed see saying though would probably best try use error perspective right,issue,positive,positive,positive,positive,positive,positive
302231055,"From this part of [source code](https://github.com/dmlc/dmlc-core/blob/master/include/dmlc/logging.h#L235-L247) in xgboost, it seems that the error message is printed out by `std::ostringstream`. I am not sure if python can catch this message. ",part source code error message printed sure python catch message,issue,negative,positive,positive,positive,positive,positive
302214110,"```
from xgboost.core import XGBoostError
import warnings
for i in range(2000):
    try:
    # cv scores
        with warnings.catch_warnings():
            warnings.simplefilter('ignore')
            cvscores = cross_val_score(test_pipeline, x_train, y_train, cv=5, scoring='accuracy', verbose=0)
    except XGBoostError:
        print(""Wrong"")

```
Somehow, the error message still showed up even I used the codes above to catch XGBoostError. But program did not crash after runing this bad pipeine 2000 times.",import import range try except print wrong somehow error message still even used catch program crash bad time,issue,negative,negative,negative,negative,negative,negative
302209358,"I found the reason of the issue. It is due to pipeline 32 in generation 0 (see below) when using the demo in the issue. The first step is feature selection. But sadly no feature passed the threshold in the first step so that no feature is available for `XGBClassifier` in second steps. 

For solving this issue, I will submit a PR to catch this error message to prevent TPOT from crashing.

```
# pipeline 32
make_pipeline(
    SelectFromModel(estimator=ExtraTreesClassifier(max_features=0.2), threshold=0.30000000000000004),
    XGBClassifier(learning_rate=0.1, max_depth=1, min_child_weight=13, nthread=1, subsample=0.6000000000000001)
)
```

For reproducing the error message without running TPOT, please try the codes below:

```
print(""importing modules..."")
import pandas as pd
import numpy as np
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.ensemble import VotingClassifier, ExtraTreesClassifier
from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.feature_selection import SelectFromModel
from xgboost import XGBClassifier
from random import randint
from copy import copy
from tpot import TPOTClassifier

# I wanted the label data to be a bit imbalanced
print(""creating fake data..."")
np.random.seed(1776)
df = pd.DataFrame(np.random.randn(8000,11), columns=list(""ABCDEFGHIJK""))
label = np.array([randint(1,11) for mynumber in range(0, 8000)])
label[label <= 9] = 0
label[label >= 10] = 1
print(label)
df['label'] = label


# extract labels and drop them from the DataFrame
y = df['label'].values
colsToDrop = ['label']
xdf = df.drop(colsToDrop, axis=1)


x_train, x_test, y_train, y_test = train_test_split(xdf, y, train_size=0.7, test_size=0.3, random_state=1776)

# make a test pipeline
""""""test_pipeline = make_pipeline(
    make_union(VotingClassifier([(""est"", DecisionTreeClassifier(criterion=""gini"", max_depth=10, min_samples_leaf=8, min_samples_split=13))]), FunctionTransformer(copy)),
    XGBClassifier(learning_rate=0.01, max_depth=2, min_child_weight=9, nthread=1, subsample=0.1)
)""""""

test_pipeline = make_pipeline(
    SelectFromModel(estimator=ExtraTreesClassifier(max_features=0.2), threshold=0.30000000000000004),
    XGBClassifier(learning_rate=0.1, max_depth=1, min_child_weight=13, nthread=1, subsample=0.6000000000000001)
    )

# # Fix random state when the operator allows  (optional) just for get consistent CV score in TPOT
tpot = TPOTClassifier()
tpot._set_param_recursive(test_pipeline.steps, 'random_state', 42)

# cv scores
cvscores = cross_val_score(test_pipeline, x_train, y_train, cv=5, scoring='accuracy', verbose=0)


```",found reason issue due pipeline generation see issue first step feature selection sadly feature threshold first step feature available second issue submit catch error message prevent pipeline error message without running please try print import import import import import import import import random import copy import copy import label data bit print fake data label range label label label label print label label extract drop make test pipeline copy fix random state operator optional get consistent score,issue,negative,negative,neutral,neutral,negative,negative
301932082,"Note: this PR is just a patch and should be merged into master batch. This compatibility issue is fixed in dev batch. So for merging with dev batch later, the change in [7cea3bf](https://github.com/rhiever/tpot/pull/451/commits/7cea3bf8d1059fe15ef3e7fe50c748f65bfe0a2e) can be deleted but unit test in this PR is helpful.",note patch master batch compatibility issue fixed dev batch dev batch later change unit test helpful,issue,negative,positive,neutral,neutral,positive,positive
301926803,"Or you can try this dictionary in dev branch.
```
tpot_mdr_classifier_config_dict = {

    # Classifiers

    'sklearn.linear_model.LogisticRegression': {
        'penalty': [""l1"", ""l2""],
        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],
        'dual': [True, False]
    },

    # Feature constructors

    'mdr.MDR': {
        'tie_break': [0, 1],
        'default_label': [0, 1]
    },

    # Feature Selectors

    'skrebate.ReliefF': {
        'n_features_to_select': range(1, 6),
        'n_neighbors': [2, 10, 50, 100, 250, 500]
    },

    'skrebate.SURF': {
        'n_features_to_select': range(1, 6)
    },

    'skrebate.SURFstar': {
        'n_features_to_select': range(1, 6)
    },

    'skrebate.MultiSURF': {
        'n_features_to_select': range(1, 6)
    }

}

```",try dictionary dev branch true false feature feature range range range range,issue,negative,negative,neutral,neutral,negative,negative
301926220,"Thank you for letting us know this issue. The issue is caused by a recent update in [scikit-MDR](https://github.com/EpistasisLab/scikit-mdr/commit/6046ac963000de6ba90d2c0194a2a7446ea4b755#diff-102b1487e5185df93069d2af9ffce457) where MDR was not setting as a subclass of `ClassifierMixin` .

I will submit a PR soon to fix this compatibility issue in TPOT. 

For solving the issue temperately, please try the codes below to use TPOT MDR.

```
tpot_mdr_classifier_config_dict = {

    # Classifiers

    'mdr.MDRClassifier': {
        'tie_break': [0, 1],
        'default_label': [0, 1]
    },

    # Feature Selectors

    'skrebate.ReliefF': {
        'n_features_to_select': range(1, 6),
        'n_neighbors': [2, 10, 50, 100, 250, 500]
    },

    'skrebate.SURF': {
        'n_features_to_select': range(1, 6)
    },

    'skrebate.SURFstar': {
        'n_features_to_select': range(1, 6)
    },

    'skrebate.MultiSURF': {
        'n_features_to_select': range(1, 6)
    }

}

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=3, config_dict=tpot_mdr_classifier_config_dict)

``` ",thank u know issue issue recent update setting subclass submit soon fix compatibility issue issue temperately please try use feature range range range range,issue,positive,neutral,neutral,neutral,neutral,neutral
301918299,"Absolutely! Thanks for your response!

That reminds me, it's also probably worth mentioning that I ran into a few `Invalid pipeline encountered. Skipping its evaluation.` messages when using `verbosity=3` in the `TPOTClassifier()` constructor.

Thank you!",absolutely thanks response also probably worth ran invalid pipeline skipping constructor thank,issue,positive,positive,positive,positive,positive,positive
301915094,"Thank you for these detailed information for this issue.

I don't think the issue was related to `generation` since the error message showed up in the initial generation which had only randomly generated pipelines. I suspected it might be due to [`_pre_test` decorator](https://github.com/rhiever/tpot/blob/master/tpot/decorators.py#L35) because it would test pipeline with a small dataset to make sure it is a valid pipeline. Some invalid pipelines including XGBClassifier operator might cause this issue in `_per_test`. I will also run more tests to find the reason.
",thank detailed information issue think issue related generation since error message initial generation randomly suspected might due decorator would test pipeline small make sure valid pipeline invalid operator might cause issue also run find reason,issue,negative,positive,neutral,neutral,positive,positive
301909587,"I should have been a bit more explicit for anyone else following along. In addition to the message in the console, I also receive the windows message that ""python.exe has stopped working"" and then the program crashes.

![image](https://cloud.githubusercontent.com/assets/7726537/26125853/110e6866-3a49-11e7-93fd-b2464b0271eb.png)

Also, I'm not sure if ""Pipeline"" is a 1-to-1 with ""generation"" (haven't dug into too much of the source yet), but it did require me to set the ""generations"" parameter of the `TPOTClassifier` object sufficiently high enough to receive the error. So maybe it isn't one of the XGBClassifiers in the first 26 pipelines, but rather one of the pipelines later on? 

I might be misunderstanding the relationships between pipelines and generations though.

In the script above for example, if I set the `generations` and `population_size` both to **32**, then I get this error message, but if I lower those parameters each to **30**, then there is no error. That seems to be where the threshold is for reproducing this issue.

**Note:**
(Nevermind, I just tested it with 30 as the value for both `generations` and `population_size` and it received the same error message, but not until it was 23% done with the optimization)

The tpot object was able to fit with no errors when `generations` and `population_size` were both set to 26.

Going to try and investigate this more tonight.
",bit explicit anyone else following along addition message console also receive message stopped working program image also sure pipeline generation dug much source yet require set parameter object sufficiently high enough receive error maybe one first rather one later might misunderstanding though script example set get error message lower error threshold issue note tested value received error message done optimization object able fit set going try investigate tonight,issue,negative,positive,positive,positive,positive,positive
301895285,"Hmm, the warning message was indeed reproduced in my environment. But it is very weird that this `colsample_bytree` parameter is not in our [operator dictionary](https://github.com/rhiever/tpot/blob/master/tpot/config_classifier.py#L103-L110) so TPOT wound not tune this parameter and keep it as default value of 1.

I also checked the first 26 pipelines in TPOT optimization process. Only two pipelines below used XGBClassifier. I tested both of them and both worked without warning message. Very weird.

```
Pipeline(steps=[('xgbclassifier', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,
       min_child_weight=1, missing=None, n_estimators=100, nthread=1,
       objective='binary:logistic', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True,
       subsample=0.6500000000000001))])

Pipeline(steps=[('xgbclassifier', XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.5, max_delta_step=0, max_depth=4,
       min_child_weight=7, missing=None, n_estimators=100, nthread=1,
       objective='binary:logistic', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True, subsample=0.8))])
```

",warning message indeed environment weird parameter operator dictionary wound tune parameter keep default value also checked first optimization process two used tested worked without warning message weird pipeline logistic pipeline logistic,issue,negative,negative,negative,negative,negative,negative
301861721,It seems that no valid pipeline was randomly generated. Could you please let me know which version of scikit-MDR? I think `TPOT MDR` should work with version 0.4.3 and 0.4.4.,valid pipeline randomly could please let know version think work version,issue,negative,negative,negative,negative,negative,negative
301630536,Please check my comments in #337. A demo there may help you to get intermediate results.,please check may help get intermediate,issue,positive,neutral,neutral,neutral,neutral,neutral
301569532,It's okay. But I do not have permission to delete it. You can close the issue by using the `Close Issue` bottom below comment box on this webpage.  ,permission delete close issue close issue bottom comment box,issue,negative,neutral,neutral,neutral,neutral,neutral
301568695,Yes it was a mistake. no idea that i was created. sorry! please delete it. ,yes mistake idea sorry please delete,issue,negative,negative,negative,negative,negative,negative
301566794,??? Did you submit the issue by mistake? It is just the template for submitting issue.,submit issue mistake template issue,issue,negative,neutral,neutral,neutral,neutral,neutral
301108732,"
[![Coverage Status](https://coveralls.io/builds/11495814/badge)](https://coveralls.io/builds/11495814)

Changes Unknown when pulling **0c71438a838e524fbf825fe68016aa853b08138f on weixuanfu2016:revert-441-subsample_param** into ** on rhiever:development**.
",coverage status unknown revert development,issue,negative,negative,neutral,neutral,negative,negative
301108469,"Awesome, will do first thing when I'm I'm the office.

On Fri, 12 May 2017, 18:24 Randy Olson, <notifications@github.com> wrote:

> Yep, I think the few issues that folks raised are all that needs fixing
> prior to the merge.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/432#issuecomment-301107104>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7HSmAggXSkmB3vMtvTwb8n8pKKQoks5r5HmTgaJpZM4NMiJi>
> .
>
",awesome first thing office may randy wrote yep think raised need fixing prior merge thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
301107104,"Yep, I think the few issues that folks raised are all that needs fixing prior to the merge.",yep think raised need fixing prior merge,issue,negative,neutral,neutral,neutral,neutral,neutral
301106443,"...Miss a commit, Merge too quickly",miss commit merge quickly,issue,negative,positive,positive,positive,positive,positive
301105887,"I'll fix them, are the comments here the only questions?

On Fri, 12 May 2017, 18:13 Randy Olson, <notifications@github.com> wrote:

> Looks like this PR has merge conflicts now too.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/432#issuecomment-301103984>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7KccHwdVlk1Vhvb_j32KOqyZPpdeks5r5HblgaJpZM4NMiJi>
> .
>
",fix may randy wrote like merge thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
301015185,"great! that's what I was looking for...
 thank you for your response...",great looking thank response,issue,positive,positive,positive,positive,positive,positive
300799383,Update: just fix a small bug in the demo above.,update fix small bug,issue,negative,negative,negative,negative,negative,negative
300798544,"Hi, Do you mean to check the modeling algorithms / pipelines were evaluated in TPOT optimization process?

You can easily found these pipelines in the dictionary  `tpot_obj._evaluated_individuals` (`tpot_obj` is a TPOT object) based on the [source codes(https://github.com/rhiever/tpot/blob/master/tpot/base.py#L259-260). The keys of dictionary are pipeline string and the values are the count of operators and pipelines' CV scores. 

To be more clear, below it is a small demo:

```
from tpot import TPOTClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
import numpy as np
from deap import creator
from sklearn.model_selection import cross_val_score

# Iris flower classification
iris = load_iris()
X_train, X_test, y_train, y_test = train_test_split(iris.data.astype(np.float64),
    iris.target.astype(np.float64), train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
# print part of pipeline dictionary
print(dict(list(tpot._evaluated_individuals.items())[0:2]))
# print a pipeline and its values
pipeline_str = list(tpot._evaluated_individuals.keys())[0]
print(pipeline_str)
print(tpot._evaluated_individuals[pipeline_str])
# convert pipeline string to scikit-learn pipeline object
optimized_pipeline = creator.Individual.from_string(pipeline_str, tpot._pset) # deap object
fitted_pipeline = tpot._toolbox.compile(expr=optimized_pipeline ) # scikit-learn pipeline object
# print scikit-learn pipeline object
print(fitted_pipeline)
# Fix random state when the operator allows  (optional) just for get consistent CV score 
tpot._set_param_recursive(fitted_pipeline.steps, 'random_state', 42)
# CV scores from scikit-learn
scores = cross_val_score(fitted_pipeline, X_train, y_train, cv=5, scoring='accuracy', verbose=0)
print(np.mean(scores))
print(tpot._evaluated_individuals[pipeline_str][1])
```",hi mean check modeling optimization process easily found dictionary object based source dictionary pipeline string count clear small import import import import import creator import iris flower classification iris print print part pipeline dictionary print list print pipeline list print print convert pipeline string pipeline object object pipeline object print pipeline object print fix random state operator optional get consistent score print print,issue,positive,negative,neutral,neutral,negative,negative
300745507,"Hello

just adding my question to this as I thought this question was closest to mine (let me know otherwise, I can raise a separate question),

I've just discovered TPOT this week and having fun learning it so far! 

In the description on http://rhiever.github.io/tpot/  ,its mentioned, 
""TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.""

How can I find out which modeling algorithms were considered in the solution? 

Thanks!

",hello question thought question mine let know otherwise raise separate question discovered week fun learning far description tedious part machine learning intelligently exploring possible find best one data find modeling considered solution thanks,issue,positive,positive,positive,positive,positive,positive
299304823,"@mrocklin No apology needed. I am very happy to find better solution. Thanks again for these good suggestions. Right now, the main issue is that the freezing with a large dataset seems from the scikit-learn and its joblib as mentioned here and #436 , which is very hard to solve until sklearn fixed it.  We used a thread-based timeout decorator for trying to kill these freezing jobs but somehow it is not works very well. The decorator can kill the freezing jobs but not under the right time limit. I suspect it is due to GIL. I will look into the example to check if it change solve our issues, Let me know if you have any idea about it.  ",apology happy find better solution thanks good right main issue freezing large hard solve fixed used decorator trying kill freezing somehow work well decorator kill freezing right time limit suspect due look example check change solve let know idea,issue,positive,positive,positive,positive,positive,positive
299284994,"Maybe, but this isn't really something that's specific to TPOT. It's a general Anaconda/pip issue.",maybe really something specific general issue,issue,negative,positive,neutral,neutral,positive,positive
299278906,Looks good. Should we add a upgrading section in docs about installation? @rhiever ,good add section installation,issue,negative,positive,positive,positive,positive,positive
299277379,"I don't recommend running `pip install --upgrade <package>` with Anaconda. It might cause pip to mess up some of the Anaconda packages. A better way to upgrade is to run:

`pip uninstall tpot -y; pip install tpot --no-cache`",recommend running pip install upgrade package anaconda might cause pip mess anaconda better way upgrade run pip pip install,issue,negative,positive,positive,positive,positive,positive
299227458,"The pywin command needs to be updated in the tpot installation page as I was getting error 
  
 No matching distribution found",command need installation page getting error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
299221778,Please try `pip install tpot --upgrade`,please try pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
299073451,"Also, apologies for going off topic, but if you all ever wanted to dive more deeply into Dask you might consider the asynchronous concurrent.futures interface, notably [submit](http://distributed.readthedocs.io/en/latest/api.html#distributed.client.Client.submit) and [as_completed](http://distributed.readthedocs.io/en/latest/api.html#distributed.client.as_completed).  Here is an example of a somewhat-related library that uses this interface well to solve a somewhat-related computational problem: https://github.com/eriknw/dask-patternsearch .

Sorry for the intrusion.  I ran across this PR while searching github.",also going topic ever dive deeply might consider asynchronous interface notably submit example library interface well solve computational problem sorry intrusion ran across searching,issue,negative,neutral,neutral,neutral,neutral,neutral
298981329,"
![training_sample_size_on_percentage_decrease_in_accuracy_based_on_pmlb](https://cloud.githubusercontent.com/assets/21084970/25672938/a34ef4b6-3003-11e7-8548-8307bc118899.png)

The subsample option were tested on >160 benchmarks in [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks/tree/master/pmlb). Subsample ratios were set from 0.1 to 0.9 with step = 0.1. The percentage changes in accuracy with partial training datasets were calculated based on accuracy with full dataset in each benchmark.  The scatter plot above shows the relationship between training sample size and percentage decreases in accuarcy (the results with training sample size > 5000 are excluded for clearer presentation in the scatter plot). It seems that once train sample size is less then ~1500, the changes in accuracy is very random and could be higher than 30%.  Based on this issue, I think I will add a friendly warning message for using `subsample` option once the training sample size is less then 1500 just in case that the option is used by mistake.

",subsample option tested subsample set step percentage accuracy partial training calculated based accuracy full scatter plot relationship training sample size percentage training sample size clearer presentation scatter plot train sample size le accuracy random could higher based issue think add friendly warning message subsample option training sample size le case option used mistake,issue,negative,positive,neutral,neutral,positive,positive
298878858,"Yes, pathos was removed 3 weeks ago.

> On May 3, 2017, at 1:56 AM, Dani K <notifications@github.com> wrote:
> 
> Version 0.7 didn't freeze once over dozens of runs I did. Some a week long.
> Was it pathos then?
> 
> On Wed, 3 May 2017, 01:24 Weixuan, <notifications@github.com> wrote:
> 
> > BTW, you could also check a branch named pathos in my repo for a demo of
> > using pathos in tpot. But I tested it, freezing happened.
> >
> > —
> > You are receiving this because you authored the thread.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/rhiever/tpot/issues/436#issuecomment-298778678>, or mute
> > the thread
> > <https://github.com/notifications/unsubscribe-auth/AMzW7GN-Y1LumQf7xOVbtODiMzEsdggrks5r160FgaJpZM4NMmAI>
> > .
> >
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",yes pathos removed ago may wrote version freeze week long pathos wed may wrote could also check branch pathos pathos tested freezing thread reply directly view mute thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
298828745,"Version 0.7 didn't freeze once over dozens of runs I did. Some a week long.
Was it pathos then?

On Wed, 3 May 2017, 01:24 Weixuan, <notifications@github.com> wrote:

> BTW, you could also check a branch named pathos in my repo for a demo of
> using pathos in tpot. But I tested it, freezing happened.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298778678>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7GN-Y1LumQf7xOVbtODiMzEsdggrks5r160FgaJpZM4NMmAI>
> .
>
",version freeze week long pathos wed may wrote could also check branch pathos pathos tested freezing thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
298778678,"BTW, you could also check a branch named `pathos` in my repo for a demo of using pathos in tpot. But I tested it, freezing happened.",could also check branch pathos pathos tested freezing,issue,negative,neutral,neutral,neutral,neutral,neutral
298769822,"Thanks, I can just pull the branch, i made a soft link in my env directly
to the tpot source :)
We're talking about weixuanfu2016/tpot.git + joblib_timeout branch?

On Wed, May 3, 2017 at 12:37 AM, Weixuan <notifications@github.com> wrote:

> Great, a little bit more details about pathos. It uses dill instead of
> cPickle for pickling as joblib does. Maybe it is the key for this issue.
> The dask in #440 <https://github.com/rhiever/tpot/pull/440> uses
> cloudpickle, which seems works well in my tests. You may also try this
> branch on your dataset. (commands below for reinstalling the TPOT branch in
> your environment)
>
> pip install dask[complete]
> pip install stopit
> pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@joblib_timeout
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298768991>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7FAj2sDRnNKsAStfJbstJ7uk_Z3cks5r16IbgaJpZM4NMmAI>
> .
>



-- 
Cheers,
Dani K.
",thanks pull branch made soft link directly source talking branch wed may wrote great little bit pathos dill instead maybe key issue work well may also try branch branch environment pip install complete pip install pip install upgrade thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
298768991,"Great, a little bit more details about `pathos`. It uses `dill` instead of `cPickle` for pickling as joblib does. Maybe it is the key for this issue. The `dask` in #440 uses `cloudpickle`, which seems works well in my tests. You may also try this branch on your dataset. (commands below for reinstalling the TPOT branch in your environment)

```
pip install dask[complete]
pip install stopit
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@joblib_timeout

```",great little bit pathos dill instead maybe key issue work well may also try branch branch environment pip install complete pip install pip install upgrade,issue,positive,positive,positive,positive,positive,positive
298766978,"Makes sense, I'm still not sure what happened after the hang, I expected it
to just result in one timed out pipeline, I'll probably have to investigate
why it ruined the whole tpot process..

On Wed, 3 May 2017, 00:17 Weixuan, <notifications@github.com> wrote:

> Another change is about timeout function. In 0.7, we used signal-based
> timeout instead of thread-based timeout function. thread-based is more
> safe for multiprocessing
> <https://github.com/glenfant/stopit#comparing-thread-based-and-signal-based-timeout-control>
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298763938>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7GVyz-_N-7W8yXnAggtB2a3Svozaks5r151OgaJpZM4NMmAI>
> .
>
",sense still sure result one timed pipeline probably investigate ruined whole process wed may wrote another change function used instead function safe thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
298763938,"Another change is about timeout function. In 0.7, we used signal-based timeout instead of thread-based timeout function. thread-based is [more safe for multiprocessing](https://github.com/glenfant/stopit#comparing-thread-based-and-signal-based-timeout-control)",another change function used instead function safe,issue,negative,positive,positive,positive,positive,positive
298763244,"Interesting, perhaps I'll try using pathos to see if it fixes my problem.
Specifically, I used tpot lots of times in that version with xgboost (on
centos x64 though) and had no trouble at all.

On Wed, 3 May 2017, 00:07 Weixuan, <notifications@github.com> wrote:

> In 0.7, we used pathos <https://pypi.python.org/pypi/pathos/0.2.0> for
> multiprocessing. But it has some unknown issue working with xgboost and
> does not support Windows. So we switched to joblib in scikit-learn in 0.7.1.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298761507>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7Gb3ZQPJHouhIbVQVXH8ICRCnD5wks5r15sOgaJpZM4NMmAI>
> .
>
",interesting perhaps try pathos see problem specifically used lot time version though trouble wed may wrote used pathos unknown issue working support switched thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
298761507,"In 0.7, we used [pathos](https://pypi.python.org/pypi/pathos/0.2.0) for multiprocessing. But it has some unknown issue working with xgboost and does not support Windows. So we switched to joblib in scikit-learn in 0.7.1.",used pathos unknown issue working support switched,issue,negative,negative,neutral,neutral,negative,negative
298754590,"I don't mind delays, that's reasonable. But infinite hang and subsequent
failure is the problem. I'm taking leaving tpot running a weekend and
coming back to an empty failed process..
This didn't happen in 0.7, I wonder what changed..

On Tue, 2 May 2017, 23:34 Weixuan, <notifications@github.com> wrote:

> The freezing issue may come from scikit-learn and joblib, so the freezing
> still happened even without joblib in TPOT since scikit-learn use this own
> joblib in multiple functions and classes. In the PR #440
> <https://github.com/rhiever/tpot/pull/440>, I used the dask instead, I
> found the freezing time is less. Also, I tried to use dask to wrap these
> sklearn objects to use dask for multiprocessing, but dask do not allow this
> nested threading.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298752997>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7HoSnqW7ADvaz5-h6PF7HwH_dIWuks5r15NvgaJpZM4NMmAI>
> .
>
",mind reasonable infinite subsequent failure problem taking leaving running weekend coming back empty process happen wonder tue may wrote freezing issue may come freezing still even without since use multiple class used instead found freezing time le also tried use wrap use allow thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
298752997,"The freezing issue may come from scikit-learn and joblib, so the freezing still happened even without joblib in TPOT since scikit-learn use this own joblib in multiple functions and classes. In the PR #440, I used the dask instead, I found the freezing time is less. Also, I tried to use dask to wrap these sklearn objects to use dask for multiprocessing, but dask do not allow this nested threading. ",freezing issue may come freezing still even without since use multiple class used instead found freezing time le also tried use wrap use allow,issue,negative,neutral,neutral,neutral,neutral,neutral
298735617,"Although I set TPOT not use joblib when n_jobs=1, I found that many sklean's classes and functions used the built-in joblib and then freeze happened again.",although set use found many class used freeze,issue,negative,positive,positive,positive,positive,positive
298661005,"I think the best workaround (for now) is to make TPOT *not* use multiprocessing when `n_jobs=1`, and put warnings in the documentation that enabling multiprocessing (`n_jobs!=1`) may be slow and prone to freezing with very large datasets.

sklearn has this same problem, right? e.g. if you use `cross_val_score` with `n_jobs!=1` for a very large dataset, it will also be slow and/or freeze.",think best make use put documentation may slow prone freezing large problem right use large also slow freeze,issue,negative,positive,positive,positive,positive,positive
298635915,"I think I solved this issue for large dataset in the branch mentioned above. It is about memmaping of large arrays. I will post a PR soon.

Update: One of my tests shows that it still has issues with forking methods in multiprocessing. Need more workaround.   ",think issue large branch large post soon update one still need,issue,negative,positive,positive,positive,positive,positive
298489423,"I apologize, I'd left this open by mistake. Everything should be address in the above commit.",apologize left open mistake everything address commit,issue,negative,neutral,neutral,neutral,neutral,neutral
298442010,"Thank you for these details. After more tests, I think the issue is caused by [bad interaction of multiprocessing and third-party libraries](https://pythonhosted.org/joblib/parallel.html#bad-interaction-of-multiprocessing-and-third-party-libraries), and as you mentioned above, similar issues were reported in both joblib and scikit-learn's repos especially with large datasets. Unfortunately, so far I did not find a nice way to fix this issue with joblib in both python 2.X and python 3.X. I also tried the `dask` module in [one of my branches](https://github.com/weixuanfu2016/tpot/tree/joblib_timeout), which seems more stable than joblib. But I still found sometimes evaluation are freezed. You may try this branch with your datasets. I think we need more tests to solve this issue. I think this issue is also related to #422",thank think issue bad interaction similar especially large unfortunately far find nice way fix issue python python also tried module one stable still found sometimes evaluation may try branch think need solve issue think issue also related,issue,negative,positive,neutral,neutral,positive,positive
298348752,"I recommend using traceback module. As far as I know it's both 2 and 3
compliant and very useful to see traceback in many cases.
I intend to add traceback info in some places in tpot myself.

On Mon, 1 May 2017, 17:43 Weixuan, <notifications@github.com> wrote:

> I agree. I will submit a PR for removing these lines with with_traceback
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/435#issuecomment-298345606>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7OESsva0YBK2AeuunqXVNVNPIBYMks5r1e9_gaJpZM4NMlzj>
> .
>
",recommend module far know compliant useful see many intend add mon may wrote agree submit removing thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
298335512,"Hmm, you are right, it should be 1.0.2.post2, very strange. I will resubmit this PR to fix it soon",right post strange resubmit fix soon,issue,negative,positive,positive,positive,positive,positive
298314121,"Meanwhile, I've taken a look, I can't seem to understand when TPOT decides to fork and when it doesn't but it seems the hanging problems only occured on spawning forks - the posted above log was when it had njobs=1 by default, and for some reason still spawned a child process.

I suspect this has to do with joblib having weird behaviours in certain cases, the following search shows multiple stories that sound similar to this:

[https://www.google.com/search?q=skleanr+Parallel+joblib+hangs&oq=skleanr+Parallel+joblib+hangs&aqs=chrome..69i57j0.4739j0j1&sourceid=chrome&ie=UTF-8#q=sklearn+Parallel+joblib+hangs
](https://www.google.com/search?q=skleanr+Parallel+joblib+hangs&oq=skleanr+Parallel+joblib+hangs&aqs=chrome..69i57j0.4739j0j1&sourceid=chrome&ie=UTF-8#q=sklearn+Parallel+joblib+hangs
)

An Example:

[https://github.com/joblib/joblib/issues/125](https://github.com/joblib/joblib/issues/125)

Some talk about large matrices, some talk about certain problems with crashes/problems.
It might be that after a timeout on one pipeline, the next fork hangs (people talk about crash causing NEXT fork to hang).

I will investigate further when I have time.

Interestingly enough the timeout in TPOT doesn't seem to kill the spawned hanged processes even after the timeout hits (or maybe it does, but not in an orderly and ""move on"" fashion as we'd like).",meanwhile taken look ca seem understand fork hanging spawning posted log default reason still child process suspect weird certain following search multiple sound similar example talk large matrix talk certain might one pipeline next fork people talk crash causing next fork investigate time interestingly enough seem kill even maybe orderly move fashion like,issue,negative,positive,neutral,neutral,positive,positive
298313534,"Even the download link leads to:
https://pypi.python.org/packages/5b/d7/a49d3dd7aa8cbaf2b1ac8f4d6495824c886fea8b3dac4a73dc4df94cad76/deap-1.0.2.post2.tar.gz

Just letting you know. I just manualy changed to .post2 on my machine to
solve, if the problem is just for me I'll handle it :)

On Mon, May 1, 2017 at 12:55 PM, Dan Koretsky <dankoretsky@gmail.com> wrote:

> Please note for me at least (centos, linix x64 OS), this command:
> *pip install deap==1.0.2*
> fails, the 1.0.2 version out on pypi is identified in PIP as 1.0.2.post2
> Also, on the pypi page itself the green download button looks like this
> (looks like a hint that this is the version that's out):
> Download
> deap-1.0.2.post2.tar.gz
>
> So, I don't know if it's just on my machine, but the command
> *pip install deap==1.0.2.post2*
>
> Seems more generally working to me.
>
>
> On Sun, Apr 30, 2017 at 10:12 PM, Randy Olson <notifications@github.com>
> wrote:
>
>> DEAP 1.0.2 is out on PyPi: https://pypi.python.org/pypi/deap
>>
>> So any version of pip should pick it up.
>>
>> —
>> You are receiving this because you commented.
>> Reply to this email directly, view it on GitHub
>> <https://github.com/rhiever/tpot/pull/434#issuecomment-298250773>, or mute
>> the thread
>> <https://github.com/notifications/unsubscribe-auth/AMzW7MnT2U8WTman1ILoSgLcRJmIrJUNks5r1N0XgaJpZM4NMjAi>
>> .
>>
>
>
>
> --
> Cheers,
> Dani K.
>



-- 
Cheers,
Dani K.
",even link know machine solve problem handle mon may dan wrote please note least o command pip install version pip post also page green button like like hint version know machine command pip install post generally working sun randy wrote version pip pick reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
298313211,"Please note for me at least (centos, linix x64 OS), this command:
*pip install deap==1.0.2*
fails, the 1.0.2 version out on pypi is identified in PIP as 1.0.2.post2
Also, on the pypi page itself the green download button looks like this
(looks like a hint that this is the version that's out):
Download
deap-1.0.2.post2.tar.gz

So, I don't know if it's just on my machine, but the command
*pip install deap==1.0.2.post2*

Seems more generally working to me.


On Sun, Apr 30, 2017 at 10:12 PM, Randy Olson <notifications@github.com>
wrote:

> DEAP 1.0.2 is out on PyPi: https://pypi.python.org/pypi/deap
>
> So any version of pip should pick it up.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/434#issuecomment-298250773>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7MnT2U8WTman1ILoSgLcRJmIrJUNks5r1N0XgaJpZM4NMjAi>
> .
>



-- 
Cheers,
Dani K.
",please note least o command pip install version pip post also page green button like like hint version know machine command pip install post generally working sun randy wrote version pip pick reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
298253462,"The reason  I specifically wanted this except to be as global as possible
is for the retries of saving an optimised pipeline after days of work to
try his best to save it whatever the error. We can limit it to less, I
don't mind.

On Sun, 30 Apr 2017, 22:48 duboviy, <notifications@github.com> wrote:

> *@duboviy* requested changes on this pull request.
> ------------------------------
>
> In tpot/base.py
> <https://github.com/rhiever/tpot/pull/432#discussion_r114082609>:
>
> >
> -                        for pipeline in self._pareto_front.items:
> -                            self._pareto_front_fitted_pipelines[str(pipeline)] = self._toolbox.compile(expr=pipeline)
> -                            with warnings.catch_warnings():
> -                                warnings.simplefilter('ignore')
> -                                self._pareto_front_fitted_pipelines[str(pipeline)].fit(features, classes)
> +                    break
> +                except:
>
>
>    - To catch any exception use the except Exception, instead of except
>    BaseException or just except.
>    - Indicate the most specific exception type in except block.
>
> For example:
>
> # Badtry:
>     mapping[key]except Exception:
>     ...
> # Bettertry:
>     mapping[key]except KeyError:
>     ...
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/432#pullrequestreview-35524031>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7FJv65Tcwm3hA1x2j3GMrKhbOzlxks5r1OWagaJpZM4NMiJi>
> .
>
",reason specifically except global possible saving pipeline day work try best save whatever error limit le mind sun wrote pull request pipeline pipeline pipeline class break except catch exception use except exception instead except except indicate specific exception type except block example key except exception key except thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
298251457,"@weixuanfu2016 and @teaearlgraycold, please take a look sometime this week. Most of these changes look reasonable.",please take look sometime week look reasonable,issue,negative,positive,positive,positive,positive,positive
298251252,"Nope, I'll be back in the office on Wednesday, can try then.

On Sun, 30 Apr 2017, 22:19 Randy Olson, <notifications@github.com> wrote:

> Have you tried the TPOT light configuration
> <http://rhiever.github.io/tpot/using/#built-in-tpot-configurations> to
> see if that regularly freezes on you?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/436#issuecomment-298251177>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7M_67ZIa9s-erWs6kEiBNFb7mUSCks5r1N7agaJpZM4NMmAI>
> .
>
",nope back office try sun randy wrote tried light configuration see regularly thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
298251177,"Have you tried the [TPOT light configuration](http://rhiever.github.io/tpot/using/#built-in-tpot-configurations) to see if that regularly freezes on you? Some of the default TPOT operators are very expensive, especially for large datasets.",tried light configuration see regularly default expensive especially large,issue,negative,positive,neutral,neutral,positive,positive
298251022,"`with_traceback` is indeed a Python 3-only function. I don't think it's really necessary that we use it there, right @weixuanfu2016?",indeed python function think really necessary use right,issue,negative,positive,positive,positive,positive,positive
298240984,"Add a line: raise IndexError('blabla') In the location I wrote in the
issue. As far as I've checked, python 2 doesn't have the method
""with_traceback"" under the BaseException class as opposed to python 3..

On Sun, 30 Apr 2017, 18:57 Weixuan, <notifications@github.com> wrote:

> Which version of python 2? Could you please provide more details about
> running environment or/and codes for reproducing this issue?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/435#issuecomment-298240314>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7JkE19yC-YN8ldcmlPG3JiIJaR4nks5r1K9KgaJpZM4NMlzj>
> .
>
",add line raise location wrote issue far checked python method class opposed python sun wrote version python could please provide running environment issue thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
298240779,"I'll check that instruction, but I did install that way when I initially
installed the environment. Perhaps python 2 doesn't have?

On Sun, 30 Apr 2017, 18:50 Weixuan, <notifications@github.com> wrote:

> Hmm, my anaconda environment somehow has 1.0.2. Maybe most users will
> follow the installation instruction
> <http://rhiever.github.io/tpot/installing/> online for installing TPOT,
> which should not have this issue.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/434#issuecomment-298239896>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7C2Ysep7C8ue9Z_jYBLbzzZq-cdRks5r1K3FgaJpZM4NMjAi>
> .
>
",check instruction install way initially environment perhaps python sun wrote anaconda environment somehow maybe follow installation instruction issue reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
298240314,Which version of python 2? Could you please provide more details about running environment or/and codes for reproducing this issue?,version python could please provide running environment issue,issue,negative,neutral,neutral,neutral,neutral,neutral
298239896,"Hmm, my anaconda environment somehow has 1.0.2. Maybe most users will follow the [installation instruction](http://rhiever.github.io/tpot/installing/) online for installing TPOT, which should not have this issue.",anaconda environment somehow maybe follow installation instruction issue,issue,negative,neutral,neutral,neutral,neutral,neutral
298239627,Thank you for letting us know this issue and all those detailed info that was or will be posted here. I will also look into it tomorrow.,thank u know issue detailed posted also look tomorrow,issue,negative,positive,positive,positive,positive,positive
298229209,"one idea, when i put 1.0.2 in requirements.txt to test this, this command:

`pip install -r requirements.txt`

Failed. Might be worth to put in requirements.txt:

`deap==1.0.2.post2`

Which is a version pip did manage to find for me (and tests passed with this).

I'm not sure what the best fix here is, but it seems pip under anaconda doesn't have deap==1.0.2 in it.

EDIT:

BTW, I'm testing on:
Linux version 3.10.0-327.28.2.el7.x86_64 (builder@kbuilder.dev.centos.org) (gcc version 4.8.3 20140911 (Red Hat 4.8.3-9) (GCC) ) #1 SMP Wed Aug 3 11:11:39 UTC 2016

which is Centos if I understand correctly.",one idea put test command pip install might worth put post version pip manage find sure best fix pip anaconda edit testing version builder version red hat wed understand correctly,issue,positive,positive,positive,positive,positive,positive
298228986,"You are correct!

I ran:
pip install deap==1.0.2.post2

in both py2 and py3 anaconda ENVs I got, both just passed all the tests.
Awesome!

On Sun, Apr 30, 2017 at 2:49 PM, Weixuan <notifications@github.com> wrote:

> Thank you for let us know this issue. I reproduced the issue. I think the
> version of deap in requirements.txt cause this issue. I just posted a quick
> PR for this issue. Could you try nosetest again after reinstalling deap of
> version 1.0.2 (not version 1.0 in requirements.txt on current dev version.).
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/433#issuecomment-298227633>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7CivVRz1e4ZUF3QcFTSbMWuvGNOFks5r1HVEgaJpZM4NMiRI>
> .
>



-- 
Cheers,
Dani K.
",correct ran pip install post anaconda got awesome sun wrote thank let u know issue issue think version cause issue posted quick issue could try version version current dev thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
298227633,Thank you for let us know this issue. I reproduced the issue. I think the version of deap in requirements.txt cause this issue. I just posted a quick PR for this issue. Could you try nosetest again after reinstalling deap of version 1.0.2 (not version 1.0 in requirements.txt on current dev branch). ,thank let u know issue issue think version cause issue posted quick issue could try version version current dev branch,issue,negative,positive,positive,positive,positive,positive
298102157,Thanks to you all! It's so cool that you managed to fix the bug and release an update within 8 hours :),thanks cool fix bug release update within,issue,positive,positive,positive,positive,positive,positive
298075442,"I will separate all the bug fixes into one and each feature into a separate
one if it helps.

On Fri, 28 Apr 2017, 21:35 Randy Olson, <notifications@github.com> wrote:

> I agree with @teaearlgraycold <https://github.com/teaearlgraycold>'s
> suggestion. It would be easier to make a separate branch and PR for each
> feature so we can review and discuss them individually. This PR contains
> quite a few changes that would be difficult to review and process all
> together. Thanks again for sending these enhancements in!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/426#issuecomment-298074349>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7PPn8qblx1e9i6uQTPZqpdmdPORxks5r0jFhgaJpZM4NKlFn>
> .
>
",separate bug one feature separate one randy wrote agree suggestion would easier make separate branch feature review discus individually quite would difficult review process together thanks sending thread reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
298074349,I agree with @teaearlgraycold's suggestion. It would be easier to make a separate branch and PR for each feature so we can review and discuss them individually. This PR contains quite a few changes that would be difficult to review and process all together. Thanks again for sending these enhancements in!,agree suggestion would easier make separate branch feature review discus individually quite would difficult review process together thanks sending,issue,positive,negative,neutral,neutral,negative,negative
298073583,0.7.3 is now out with the patch. Thanks for raising the issue!,patch thanks raising issue,issue,negative,positive,positive,positive,positive,positive
298071444,Merged into master. Release going out soon.,master release going soon,issue,negative,neutral,neutral,neutral,neutral,neutral
298030831,"@weixuanfu2016 , the fix seems to work. You are awesome! Thanks a lot :D",fix work awesome thanks lot,issue,positive,positive,positive,positive,positive,positive
298028982,"@alfonsomhc if you want to use TPOT 0.7.2 with this patch in your environment today or this weekend. Please try the command below to reinstall tpot

```
pip install --upgrade --no-deps --force-reinstall git+https://github.com/weixuanfu2016/tpot.git@config_dict_patch
```",want use patch environment today weekend please try command reinstall pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
298023758,"@rhiever I make a patch #431  for master branch for this issue. Could you please merge it?

@alfonsomhc Thank you for let us know this issue. Sorry for the inconvenience. ",make patch master branch issue could please merge thank let u know issue sorry inconvenience,issue,negative,negative,negative,negative,negative,negative
298019978,"Opps, this is a bug for this function...it is fixed in dev branch but not merged yet...",bug function fixed dev branch yet,issue,negative,positive,neutral,neutral,positive,positive
298013412,"By the way, sending the config to an external file gives an error as well:
```
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

classifier_config_dict = 'test.conf'

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,
                      config_dict=classifier_config_dict)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_mnist_pipeline.py')
```

Where test.conf is a file that contains the dictionary definition. This produces this error:

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-3e350f442b73> in <module>()
     10 
     11 tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2,
---> 12                       config_dict=classifier_config_dict)
     13 tpot.fit(X_train, y_train)
     14 print(tpot.score(X_test, y_test))

/home/user/anaconda3/lib/python3.6/site-packages/tpot/base.py in __init__(self, generations, population_size, offspring_size, mutation_rate, crossover_rate, scoring, cv, n_jobs, max_time_mins, max_eval_time_mins, random_state, config_dict, warm_start, verbosity, disable_update_check)
    220         self.operators = []
    221         self.arguments = []
--> 222         for key in sorted(self.config_dict.keys()):
    223             op_class, arg_types = TPOTOperatorClassFactory(key, self.config_dict[key],
    224             BaseClass=Operator, ArgBaseClass=ARGType)

AttributeError: 'TPOTClassifier' object has no attribute 'config_dict'
```",way sending external file error well import import import print file dictionary definition error recent call last module print self scoring verbosity key sorted key key object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
297991923,"Hi @mfeurer, the datasets used in the RECIPE paper can be found on the following link:

https://github.com/RecipeML/Recipe/blob/master/datasets/NewDataset.zip

I uploaded then on the format used for our tests. If you need the .arff file of a dataset and you can't find in this link, you can just send me an email that i will prepare them and send it to you",hi used recipe paper found following link format used need file ca find link send prepare send,issue,negative,neutral,neutral,neutral,neutral,neutral
297975338,@alfonsomhc I just made a quick PR for restoring python dictionary support in TPOT but for now you still need use file path instead of a python dictionary unless you pull the commit to your local repo. Sorry for the inconvenience. ,made quick python dictionary support still need use file path instead python dictionary unless pull commit local sorry inconvenience,issue,negative,negative,neutral,neutral,negative,negative
297973910,"@rhiever I think we need restore the support for python dictionary in `config_dict ` parameter in interactive mode (shown in the TPOT example) as I removed this support when including TPOT light and TPOT MDR for this parameter. 

I will make a quick PR for this support.",think need restore support python dictionary parameter interactive mode shown example removed support light parameter make quick support,issue,positive,positive,positive,positive,positive,positive
297970884,"Oh, this is a mistake in our document and we need update the using of configuration dictionary  in version 0.7.2. From 0.7.2, this parameter do not support python dictionary anymore. Please write the python dictionary to a text file and use the path (string) of the file for this parameter.",oh mistake document need update configuration dictionary version parameter support python dictionary please write python dictionary text file use path string file parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
297847462,"I assumed it's less work this way. Most of them are relatively small. Of it
helps, I can open PRs on each..

On Thu, 27 Apr 2017, 23:37 Daniel, <notifications@github.com> wrote:

> I would recommend PRing most of these commits individually. Some of them
> (like using assert_equal) are useful and easy to review, but others may
> not be accepted or would take much longer to review.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/426#issuecomment-297832136>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMzW7KLwEU16csHtzabUzNduIxwcLq-kks5r0Px4gaJpZM4NKlFn>
> .
>
",assumed le work way relatively small open wrote would recommend individually like useful easy review may accepted would take much longer review thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
297843983,"Yes, I wouldn't consider the RECIPE paper to be a definitive comparison by any means. It's simply one step toward making a fairer comparison between the underlying methodologies. I think your point 1) is especially poignant: we need better benchmarks. [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks) and [OpenML](https://openml.org) are good steps toward that end at least.",yes would consider recipe paper definitive comparison simply one step toward making fairer comparison underlying think point especially poignant need better good toward end least,issue,positive,positive,positive,positive,positive,positive
297842673,"You can change the target branch by clicking the ""Edit"" button on the top right of the PR.",change target branch edit button top right,issue,negative,positive,positive,positive,positive,positive
297832136,"I would recommend PRing most of these commits individually. Some of them (like using `assert_equal`) are useful and easy to review, but others may not be accepted or would take much longer to review.",would recommend individually like useful easy review may accepted would take much longer review,issue,positive,positive,positive,positive,positive,positive
297819070,"
[![Coverage Status](https://coveralls.io/builds/11275840/badge)](https://coveralls.io/builds/11275840)

Coverage decreased (-0.3%) to 77.035% when pulling **73106762ad0c6fc4279835314076b7e808a77fbb on teaearlgraycold:imputer** into **27d60c4b89cbf34545bf82f74653bea6d46777d3 on rhiever:master**.
",coverage status coverage imputer master,issue,negative,neutral,neutral,neutral,neutral,neutral
297805182,"Saw python 3 broke, I have a local python 2 environment only, will fix the python 3 stuff in the close days.


Edit: Seems it was just xrange, I didn't miss py3 by far :dancer: ",saw python broke local python environment fix python stuff close day edit miss far dancer,issue,negative,positive,neutral,neutral,positive,positive
297795911,"I will be back sunday, will fix anything that rises here of course.
Sorry if I'm not responsive during my weekend, I can't take my work machine home with me.",back fix anything course sorry responsive weekend ca take work machine home,issue,negative,negative,negative,negative,negative,negative
297760467,"Indeed a very interesting paper to read, and we definitively need to improve Auto-sklearn to work comparable to TPOT and RECIPE on those datasets :) @walterjgsp are the bioinformatics datasets available online in order to perform runs on them?

Basically, my two take-home messages from this paper with respect to a comparison are:

1. datasets need to be representative. Without claiming that the results in the paper are false/bad etc., I wonder what happens if the datasets become larger and whether it will still be possible to run up to 40 generations of 100 individuals.
2. there should be some evaluation of how these methods perform with respect to runtime and number of function evaluation in order to understand how many function evaluations/time is necessary for a method to perform well.",indeed interesting paper read definitively need improve work comparable recipe available order perform basically two paper respect comparison need representative without paper wonder become whether still possible run evaluation perform respect number function evaluation order understand many function necessary method perform well,issue,positive,positive,positive,positive,positive,positive
297529815,"After more tests, I suspected it might be a issue related to memory usage for multiprocessing with large data sets and some memory-consuming pipelines. As a quick test: what if you use the [TPOT light configuration on 0.7.2](http://rhiever.github.io/tpot/using/#built-in-tpot-configurations)",suspected might issue related memory usage large data quick test use light configuration,issue,negative,positive,positive,positive,positive,positive
297469279,You can also email the authors if you'd like a legal copy of the paper. :-),also like legal copy paper,issue,negative,positive,positive,positive,positive,positive
297435382,"I know academic publishing is like a business nowadays (I mean the publishers) but 25 euros for one paper is crazy :)

Man I miss educational access.

I am glad you have taken this up again @setuc. It'll be very interesting to see your results.",know academic like business nowadays mean one paper crazy man miss educational access glad taken interesting see,issue,positive,positive,neutral,neutral,positive,positive
297432928,@rhiever Good read.....i guess this is more motivations for me to finish the test work....and the inclusion of Bayesian methodologies will be just perfect :). I didnt look at the code until now...a ton of improvement and more elegant. Good stuff. ,good read guess finish test work inclusion perfect didnt look code ton improvement elegant good stuff,issue,positive,positive,positive,positive,positive,positive
297429889,"In the meantime, there has been one research paper published that compares TPOT, auto-sklearn, and a variant of TPOT using grammar-based GP on a handful of problems: https://link.springer.com/chapter/10.1007/978-3-319-55696-3_16

Overall TPOT performed best on a majority of the problems, but most of the differences were fairly small (1-2% accuracy difference). The authors attributed TPOT's success to a smaller search space, which has been a focus on TPOT development---culling operators and parameters that are consistently not useful (or less useful than other operators).",one research paper variant handful overall best majority fairly small accuracy difference success smaller search space focus development consistently useful le useful,issue,positive,positive,positive,positive,positive,positive
297379837,"As i am starting to gear up, one of the things i noticed that not all of the optimizer tune the whole pipeline. for the sake of comparison, I am going to disable the feature extractions across all of them. We will just look at the HP space across various algorithms or within an algorithm. ",starting gear one tune whole pipeline sake comparison going disable feature across look space across various within algorithm,issue,negative,positive,neutral,neutral,positive,positive
297168481,"Are the selection criteria the same?

**Edit:**

Reading the source it seems like there shouldn't be any difference. So I'd be in favor of dropping `SelectKBest` since we already have `SelectPercentile`.",selection criterion edit reading source like difference favor dropping since already,issue,positive,neutral,neutral,neutral,neutral,neutral
297164970,PR looks good to merge. Any outstanding issue before I merge it?,good merge outstanding issue merge,issue,positive,positive,positive,positive,positive,positive
297161867,"Right. Currently when processing scoring functions, we assume that any scoring functions with ""loss"" or ""error"" in the name should be minimized. So we set the `greater_is_better` parameter in the `make_scorer` function to `False`: https://github.com/rhiever/tpot/blob/7970e73788ac8d834bb3dbd9b28b66a0d3343504/tpot/base.py#L258

That means the scoring function returns the negative of the score, so any optimizers (grid search, etc.) using `cross_val_score` can still assume that ""greater is better"" for the score.

So I suppose the `abs` in the `score` function offsets the negative of the score that is applied when `greater_is_better==False`.

As far as I can tell, none of the functions supported by TPOT (https://github.com/rhiever/tpot/blob/7970e73788ac8d834bb3dbd9b28b66a0d3343504/tpot/base.py#L123) return actual negative values. So perhaps we're fine with leaving the `abs` in the TPOT `score` function.",right currently scoring assume scoring loss error name set parameter function false scoring function negative score grid search still assume greater better score suppose score function negative score applied far tell none return actual negative perhaps fine leaving score function,issue,negative,positive,neutral,neutral,positive,positive
297161110,"Hold on, ignore that commit. I'm going to remove something and rebase.",hold ignore commit going remove something rebase,issue,negative,neutral,neutral,neutral,neutral,neutral
297158830,"Since we no longer have code that can preprocess arguments, anything with a parameter that's randomly assigned and has some constraints based on the dataset should be removed/changed.

Maybe we could PR sklearn's `SelectKBest` to add support for floats in the range `[0.0, 1.0]`, where that range represents the proportion of features. In the meantime I agree it should be removed.",since longer code anything parameter randomly assigned based maybe could add support range range proportion agree removed,issue,positive,negative,negative,negative,negative,negative
297157886,"For now, the score for time-consuming or failed pipeline is set to be `-float(inf)`. `abs` should not works for all the scoring in TPOTClassifier. Maybe we should add some exception for these specific regression metrics which return negative values.",score pipeline set work scoring maybe add exception specific regression metric return negative,issue,negative,negative,negative,negative,negative,negative
297155910,"I think so. I checked source codes of both operators. As mentioned above, both operators do the same thing. Let's remove it.",think checked source thing let remove,issue,negative,neutral,neutral,neutral,neutral,neutral
297119331,As a quick test: What if you use the TPOT light configuration? http://rhiever.github.io/tpot/using/#built-in-tpot-configurations,quick test use light configuration,issue,negative,positive,positive,positive,positive,positive
297054239,"hey, guys, sorry to disappear for a while...got tied up with some personal work. I will restart this work in a couple of weeks and start regular postings. ",hey sorry disappear got tied personal work restart work couple start regular,issue,negative,negative,negative,negative,negative,negative
296963281,"I cannot share my own data with you. I'm sorry for inconvenience.
I tried to reproduce the issue in MNIST example. If I use the code below (the differences with MNIST example are generations and population_size were changed into default values, and n_jobs=-1 was inserted), everythings worked well. 

from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, train_size=0.75, test_size=0.25)
tpot = TPOTClassifier(verbosity=2, n_jobs=-1)
tpot.fit(X_train, y_train)

In addition, I increased the feature size by 36 times (X_train: 1347 x 64 -> 1347 x 2304) using np.concatenate (X_train x 36), since the number of features is similar with my case.
In this trial, the idling happened between 4:33:24 and 5:53:54 for about 1.5 hours (as you can see, other steps are 5-10 minutes). During this, only 1 CPU was working and other resources (e.g. RAM) was not so working. But, after this idling, everything goes well again.
Is this issue reproducible? Are there something hard to be treated in parallel during each generation? Maybe due to my local resources??

Optimization Progress:   2%|▏         | 200/10100 [21:21<298:06:29, 108.40s/pipeline]Generation 1 - Current best internal CV score: 0.9666839080558811
Optimization Progress:   3%|▎         | 300/10100 [31:38<177:30:15, 65.21s/pipeline]Generation 2 - Current best internal CV score: 0.9666839080558811
Optimization Progress:   4%|▍         | 400/10100 [39:21<101:26:42, 37.65s/pipeline]Generation 3 - Current best internal CV score: 0.9822110227694297
Optimization Progress:   5%|▍         | 500/10100 [51:15<69:25:47, 26.04s/pipeline]Generation 4 - Current best internal CV score: 0.9822110227694297
Optimization Progress:   6%|▌         | 600/10100 [1:01:48<52:05:06, 19.74s/pipeline]Generation 5 - Current best internal CV score: 0.9822110227694297
Optimization Progress:   7%|▋         | 700/10100 [1:08:13<33:54:45, 12.99s/pipeline]Generation 6 - Current best internal CV score: 0.9822110227694297
Optimization Progress:   8%|▊         | 800/10100 [1:18:38<34:16:31, 13.27s/pipeline]Generation 7 - Current best internal CV score: 0.9822110227694297
Optimization Progress:   9%|▉         | 900/10100 [1:27:39<28:10:51, 11.03s/pipeline]Generation 8 - Current best internal CV score: 0.9837096564596672
Optimization Progress:  10%|▉         | 1000/10100 [1:38:46<34:29:15, 13.64s/pipeline]Generation 9 - Current best internal CV score: 0.9837096564596672
Optimization Progress:  11%|█         | 1100/10100 [1:48:44<29:43:31, 11.89s/pipeline]Generation 10 - Current best internal CV score: 0.9837096564596672
Optimization Progress:  12%|█▏        | 1200/10100 [1:57:25<25:13:44, 10.20s/pipeline]Generation 11 - Current best internal CV score: 0.9837096564596672
Optimization Progress:  13%|█▎        | 1300/10100 [2:04:02<20:57:00,  8.57s/pipeline]Generation 12 - Current best internal CV score: 0.984447663839741
Optimization Progress:  14%|█▍        | 1400/10100 [2:09:34<15:43:57,  6.51s/pipeline]Generation 13 - Current best internal CV score: 0.984447663839741
Optimization Progress:  15%|█▍        | 1500/10100 [2:41:56<38:32:18, 16.13s/pipeline]Generation 14 - Current best internal CV score: 0.984447663839741
Optimization Progress:  16%|█▌        | 1600/10100 [2:46:53<23:19:49,  9.88s/pipeline]Generation 15 - Current best internal CV score: 0.984447663839741
Optimization Progress:  17%|█▋        | 1700/10100 [3:01:11<22:13:03,  9.52s/pipeline]Generation 16 - Current best internal CV score: 0.984447663839741
Optimization Progress:  18%|█▊        | 1800/10100 [3:04:03<12:54:42,  5.60s/pipeline]Generation 17 - Current best internal CV score: 0.984447663839741
Optimization Progress:  19%|█▉        | 1900/10100 [3:09:35<9:44:45,  4.28s/pipeline]Generation 18 - Current best internal CV score: 0.984447663839741
Optimization Progress:  20%|█▉        | 2000/10100 [3:32:25<24:48:26, 11.03s/pipeline]Generation 19 - Current best internal CV score: 0.984447663839741
Optimization Progress:  21%|██        | 2100/10100 [3:43:46<19:49:51,  8.92s/pipeline]Generation 20 - Current best internal CV score: 0.984447663839741
Optimization Progress:  22%|██▏       | 2200/10100 [3:45:47<10:58:05,  5.00s/pipeline]Generation 21 - Current best internal CV score: 0.984447663839741
Optimization Progress:  23%|██▎       | 2300/10100 [3:48:45<7:26:33,  3.44s/pipeline]Generation 22 - Current best internal CV score: 0.984447663839741
Optimization Progress:  24%|██▍       | 2400/10100 [3:59:05<9:54:56,  4.64s/pipeline]Generation 23 - Current best internal CV score: 0.984447663839741
Optimization Progress:  25%|██▍       | 2500/10100 [4:05:01<8:22:48,  3.97s/pipeline]Generation 24 - Current best internal CV score: 0.984447663839741
Optimization Progress:  26%|██▌       | 2600/10100 [4:18:08<11:59:03,  5.75s/pipeline]Generation 25 - Current best internal CV score: 0.984447663839741
Optimization Progress:  27%|██▋       | 2700/10100 [4:28:49<12:17:02,  5.98s/pipeline]Generation 26 - Current best internal CV score: 0.9844476638397411
Optimization Progress:  28%|██▊       | 2800/10100 [4:33:24<8:28:08,  4.18s/pipeline]Generation 27 - Current best internal CV score: 0.985182716555299
Optimization Progress:  29%|██▊       | 2900/10100 [5:53:54<48:41:05, 24.34s/pipeline]Generation 28 - Current best internal CV score: 0.985182716555299
Optimization Progress:  30%|██▉       | 3000/10100 [6:01:47<28:02:24, 14.22s/pipeline]Generation 29 - Current best internal CV score: 0.985182716555299
Optimization Progress:  31%|███       | 3100/10100 [6:16:29<21:35:34, 11.10s/pipeline]Generation 30 - Current best internal CV score: 0.985182716555299
Optimization Progress:  32%|███▏      | 3200/10100 [6:25:28<15:46:54,  8.23s/pipeline]Generation 31 - Current best internal CV score: 0.985182716555299
Optimization Progress:  33%|███▎      | 3300/10100 [6:31:13<10:40:25,  5.65s/pipeline]Generation 32 - Current best internal CV score: 0.985182716555299",share data sorry inconvenience tried reproduce issue example use code example default inserted worked well import import import addition feature size time since number similar case trial see working ram working everything go well issue reproducible something hard parallel generation maybe due local optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score optimization progress generation current best internal score,issue,positive,positive,positive,positive,positive,positive
296857610,"Hmm, it should not be 3 hours. Could you please share your test codes and dataset (if available) with us? I would like to run more tests for this issue.",could please share test available u would like run issue,issue,positive,positive,positive,positive,positive,positive
296850852,"Thank you very much for your quick response.
Now I set max_eval_time_mins =5 (default values). Like the case above, idling is sometimes about 3 hours. During that, only 1 thread is working. Although I'm not sure how multiprocessing steps are organized in TPOT, do you mean that many pipelines are calculated sequentially in one thread and the time for [about 5minutes * the number of calculated piplines] are consuming? Is there any way to assign multiprocessing resource again to thouse almost stucked pipelines?
If not, I have to decrease max_eval_time_mins. I think that it is practical, but it will not lead to fundamental resolution.",thank much quick response set default like case sometimes thread working although sure organized mean many calculated sequentially one thread time number calculated consuming way assign resource thouse almost decrease think practical lead fundamental resolution,issue,positive,positive,positive,positive,positive,positive
296839692,"I have set chunk size of multiprocessing is equal to 4 X n_jobs for updating process bar after finishing  a chunk. So it is possible that a pipeline which takes a lot of time in one chunk. I think we may need better way to update process bar during multiprocessing. For now, you may try to set the max evaluation time of a single pipeline via `max_eval_time_mins` parameters (5 minutes by defaults) to skip these time-consuming (or stucked) pipelines.",set chunk size equal process bar finishing chunk possible pipeline lot time one chunk think may need better way update process bar may try set evaluation time single pipeline via skip,issue,negative,positive,positive,positive,positive,positive
296836402,"Alright. @rhiever I was running few experiments to figure out this issue for a better description. The summary of findings:

1) If I run with n_jobs=2 and much fewer features with `TPOTClassifier(verbosity=3, n_jobs=2, random_state=1, cv=4, generations=5, population_size=10, scoring='f1_micro')` then I'm able to get results.

2) If I run with n_jobs=10 and more features with `TPOTClassifier(verbosity=3, n_jobs=10, random_state=1, cv=4, generations=10, population_size=50, scoring='f1_micro')` then still after few days I'm not getting to any results. I ran the dataset with normal classifiers and maximum time for training and testing was 60 mins using sklearn while some classifiers were done in less than 5-10 mins. This is my log after couple of days:
```
Optimization Progress:   0%|          | 0/550 [00:00<?, ?pipeline/s](70409, 22197)                                                  
(70409,)                                                                                                                            
Version 0.7.0 of tpot is outdated. Version 0.7.2 was released 9 hours ago.                                                          
30 operators have been imported by TPOT.                                                                                            
start learning models...                                                                                                            
_pre_test decorator: _generate: num_test=0 k should be >=0, <= n_features; got 76.Use k='all' to return all features.               
_pre_test decorator: _generate: num_test=1 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' ar
e not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True                                           
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97                 
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 k should be >=0, <= n_features; got 67.Use k='all' to return all features.               
Invalid pipeline encountered. Skipping its evaluation.                                                                              
Optimization Progress:   0%|          | 0/550 [00:00<?, ?pipeline/s]
```",alright running figure issue better description summary run much able get run still day getting ran normal maximum time training testing done le log couple day optimization progress version outdated version ago start learning decorator got return decorator input must decorator unsupported set combination ar decorator input must decorator input must decorator decorator input must decorator got return invalid pipeline skipping evaluation optimization progress,issue,positive,positive,positive,positive,positive,positive
296802745,"Oops. the imputation function was mistakenly left in. I'll remove that and
update the PR.
",imputation function mistakenly left remove update,issue,negative,neutral,neutral,neutral,neutral,neutral
296792532,"This will take a while to review, but let's try to get this merged ASAP so it doesn't slow down other development efforts. @weixuanfu2016, can you please take a look in the next day or two to see if you find any issues? I will plan to do so as well.",take review let try get slow development please take look next day two see find plan well,issue,positive,negative,negative,negative,negative,negative
296791684,Great to hear. Good luck with installing XGBoost on OS X. It can be a big pain.,great hear good luck o big pain,issue,positive,positive,positive,positive,positive,positive
296366622,"Good to know that the issue is fixed. 

BTW, the instruction in this [link](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_on_Mac_OSX?lang=en) may help you install xgboost on MacOS.",good know issue fixed instruction link may help install,issue,positive,positive,positive,positive,positive,positive
296349687,"It works perfectly now, thank you.  

Now I just have to work out how to install xgboost on OS X.",work perfectly thank work install o,issue,positive,positive,positive,positive,positive,positive
296297367,"TPOT does work for Python 2.7, as Python 2.7 is included in our continuous integration testing. I suspect that your latest installation of Python 3.5 fixed your environment issues.

Glad to hear everything is sorted out and working for you!",work python python included continuous integration testing suspect latest installation python fixed environment glad hear everything sorted working,issue,negative,positive,positive,positive,positive,positive
296263374,"ok, I updated python version and environment to python3.5. It works fine there. I guess it doesnt work for python 2.7",python version environment python work fine guess doesnt work python,issue,negative,positive,positive,positive,positive,positive
295740952,"Hi @lesshaste, we're eager to hear how the latest release has helped with your run-time issues. It seems to be working quite well on our end with the latest multiprocessing changes. All of the changes that @weixuanfu2016 mentioned are now in the latest `pip` release.",hi eager hear latest release working quite well end latest latest pip release,issue,positive,positive,positive,positive,positive,positive
295740651,"Hi @subratac,

Has the latest release of TPOT helped with your run-time issue? We've changed to a better multiprocessing scheme in the latest release.",hi latest release issue better scheme latest release,issue,negative,positive,positive,positive,positive,positive
295737035,"What if you run the command ```!pip install tpot``` within one of the IPython Notebook cells? Can you import TPOT then? I suspect what @weixuanfu2016 suspects, which is that you have multiple Python installations on your computer.",run command pip install within one notebook import suspect multiple python computer,issue,negative,neutral,neutral,neutral,neutral,neutral
295522632,"Oh, I just noticed that there were two versions of python in the screenshot on the comment above. One is 2.7.12 and the other one is 2.7.13. It seems that you have two different environments but one of them did not have tpot module.

For better managing environment, I recommend to use conda to set/create a new environment. Check the [link](https://conda.io/docs/py2or3.html#install-a-different-version-of-python)  ",oh two python comment one one two different one module better environment recommend use new environment check link,issue,positive,positive,positive,positive,positive,positive
295520099,"Not allowing me to upload a Jupyter notebook. The error is ""No Module Named tpot"".

Here is the screenshot:

![image](https://cloud.githubusercontent.com/assets/19595496/25208412/673dcd6a-2529-11e7-80cf-e391d9f7bedd.png)

The python version is 2.7, anaconda 2.5.

TPOT version is 0.7.1
",notebook error module image python version anaconda version,issue,negative,neutral,neutral,neutral,neutral,neutral
295516138,"Oh, I think I may know the reason for this issue.

Please try `import tpot` instead of `import TPOT` in the Jupyter Notebook",oh think may know reason issue please try import instead import notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
295514857,"Could you please let me know more details about your Anaconda environment, like which version of python? 

Could you please try the commands below in DOS in Windows or Terminal in macOS/Linux and then let me know what shows up in stdout?
```
python --version
python -c ""import tpot; print('tpot %s' % tpot.__version__)""
```",could please let know anaconda environment like version python could please try do terminal let know python version python import print,issue,positive,neutral,neutral,neutral,neutral,neutral
294839210,"1. I have seen projects that use a file called ""optional-requirements.txt""
for that purpose.

2. It is possible to add code that does that:
http://stackoverflow.com/a/14399775/72099
 But I am not sure if it is the right thing to do:
https://caremad.io/posts/2013/07/setup-vs-requirement/
http://www.cdotson.com/2015/08/dont-use-parse_requirements-in-your-code/",seen use file purpose possible add code sure right thing,issue,negative,positive,positive,positive,positive,positive
294562840,"@weixuanfu2016, can you please rework this PR slightly by adding the ""TPOT MDR"" option to `config_dict` as well? See #418 for the merged config dict.",please rework slightly option well see,issue,negative,negative,negative,negative,negative,negative
294514027,"Interesting. I have a couple more questions:

1) Is it possible to specify optional dependencies within the `requirements.txt`?

2) Could this PR also add code to `setup.py` that parses `requirements.txt` to build the dependency list, to avoid duplication of information?",interesting couple possible specify optional within could also add code build dependency list avoid duplication information,issue,negative,positive,positive,positive,positive,positive
294512831,"I sorted out the merge conflict in `setup.py`.

@weixuanfu2016, please keep this PR in mind for #417.",sorted merge conflict please keep mind,issue,negative,neutral,neutral,neutral,neutral,neutral
294439105,"This requirements file is useful in order to preserve a working
environment, including dependencies.
In addition, installing the environment with  setup.py
<https://github.com/rhiever/tpot/blob/master/setup.py#L37> also install the
current project as a dependency.
Although it seem like a duplication, it is common practice to have both. As
seen here:
https://github.com/explosion/spaCy/blob/master/requirements.txt
https://github.com/explosion/spaCy/blob/master/setup.py#240

On 14 April 2017 at 20:08, Randy Olson <notifications@github.com> wrote:

> The requirements are processed in setup.py
> <https://github.com/rhiever/tpot/blob/master/setup.py#L37>; what's the
> advantage of having a separate requirements.txt?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/pull/416#issuecomment-294192899>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ABpqzrhqyBvXA_wEv3_UnzQTJ6g-H7Nwks5rv6f6gaJpZM4M9wJC>
> .
>
",file useful order preserve working environment addition environment also install current project dependency although seem like duplication common practice seen randy wrote advantage separate thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
294243048,"I merged the lite dictionary into this PR, since it is related to PR. ",lite dictionary since related,issue,negative,neutral,neutral,neutral,neutral,neutral
294193717,"Great work. It looks like we should use all the operators up to (and including) PCA (but excluding PolynomialFeatures) for the TPOT lite classification dictionary. And up through (and including) Binarizer (but excluding PolynomialFeatures) for TPOT lite regression. @weixuanfu2016, can you please PR a demo of these configurations so we can test them out on the dev branch?",great work like use excluding lite classification dictionary excluding lite regression please test dev branch,issue,positive,positive,positive,positive,positive,positive
294167975,"![regressor_used_time_on_pmlb_n_esitmater_100](https://cloud.githubusercontent.com/assets/21084970/25046465/9b734b24-20ff-11e7-93c9-151b44c07855.png)

Note: this figure above are generated using 89 benchmarks from openML. Just for a feeling about run time of these operators. More tests are running over thousands of benchmarks from openML, which should take a while to finish. I will update the figure after all tests are done.

For testing operators in TPOTRegressor, I used regression benchmarks from openML instead. Like what I did with operators in TPOTClassifier. The run time for all operators is based on cross_val_score with cv=3. Note for regressors in sklearn.ensemble, I set n_estimators=100 and keep other parameters as default. For other regressors, default settings was applied. For preprocessors and selectors, a pipeline was build with LinearRegression as second step for cross_val_score. The boxplot is made by filtered out the run time > 50 seconds for clear comparison.
",note figure feeling run time running take finish update figure done testing used regression instead like run time based note set keep default default applied pipeline build second step made run time clear comparison,issue,positive,positive,neutral,neutral,positive,positive
294152836,"![classifier_used_time_on_pmlb_n_esitmater_100](https://cloud.githubusercontent.com/assets/21084970/25044250/a2bec9dc-20f3-11e7-8ef3-a6089f8b2054.png)

Updated test using n_estimators=100 instead of 500. `SelectFromModel` takes longer time with `n_estimators=100` in XTC, but it is still slightly faster than XTC only in this run. Maybe it is because the random seed is not fixed in this test.
",test instead longer time still slightly faster run maybe random seed fixed test,issue,negative,negative,negative,negative,negative,negative
294041306,"A few requests:

1) Please send the PR to the development branch instead of the master branch.

2) Please make the TPOT-MDR config a separate config file

3) Re-work the dependencies to refer only to the package names: skrebate, scikit-mdr. You don't need to specify the classes within the packages.",please send development branch instead master please make separate file refer package need specify class within,issue,positive,neutral,neutral,neutral,neutral,neutral
294004846,"Oh, I did not see that. I will test it again with setting `n_estimators=100` instead of 500 in all `sklearn.ensemble` operators, including the estimater in `SelectFromModel `",oh see test setting instead,issue,negative,neutral,neutral,neutral,neutral,neutral
294004390,"Oh, I see that we already made the change I mentioned above on the dev branch: https://github.com/rhiever/tpot/blob/development/tpot/config_classifier.py#L207",oh see already made change dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
294004069,"I see. We should change the XTC to have at least 100 estimators in `SelectFromModel`, and in that case, it's probably too slow for TPOT lite.",see change least case probably slow lite,issue,negative,negative,negative,negative,negative,negative
294003311,"Yep, `ExtraTreesClassifier` in `SelectFromModel` used default parameters (`n_estimators=10`), since we did not specify the `n_estimators` in [classifier dictionary in TPOT](https://github.com/rhiever/tpot/blob/master/tpot/config_classifier.py#L201-L204)

I think if setting `n_estimators=500` in `SelectFromModel` will take much more time than `ExtraTreesClassifier` operator",yep used default since specify classifier dictionary think setting take much time operator,issue,negative,positive,positive,positive,positive,positive
294001895,"Thanks @weixuanfu2016 --- this is very useful. Perhaps as an initial attempt, TPOT lite should include all the operators starting from the bottom up to (and including) `SelectFromModel`, excluding `PolynomialFeatures` (as PF causes exponential feature growth)?

It's somewhat surprising that `SelectFromModel` is faster than `ExtraTreesClassifier`, as `SelectFromModel` uses the XTC right? Is it using fewer trees?",thanks useful perhaps initial attempt lite include starting bottom excluding exponential feature growth somewhat surprising faster right,issue,positive,positive,positive,positive,positive,positive
293933376,"![classifier_used_time_on_pmlb](https://cloud.githubusercontent.com/assets/21084970/25011989/13614244-203d-11e7-9e6c-1ee30aade726.png)
Here is a test for getting a general feeling about run time of operators in `TPOTClassifer` using all benchmarks in [pmlb](https://github.com/EpistasisLab/penn-ml-benchmarks). The run time for all operators is based on `cross_val_score` with `cv=3`. Note for classifiers in `sklearn.ensemble`, I set `n_estimators=500` and keep other parameters as default. For other classifiers, default settings was applied. For preprocessors and selectors, a pipeline was build with `GaussianNB` as second step for `cross_val_score`. The boxplot is made by filtered out the run time > 30 seconds for clear comparison.",test getting general feeling run time run time based note set keep default default applied pipeline build second step made run time clear comparison,issue,negative,positive,neutral,neutral,positive,positive
293907347,You may just build the xgboost but not install it as a python module. Roll down a little bit in the Installation Guide and you can find the command for installing xgboost as a python module (check this [link](http://xgboost.readthedocs.io/en/latest/build.html#python-package-installation)),may build install python module roll little bit installation guide find command python module check link,issue,negative,negative,negative,negative,negative,negative
293906282,"@weixuanfu2016 Thank you for your answer but it's still not working. 

I did `pip uninstall xgboost` and then follow Installation Guide. 

`git clone --recursive https://github.com/dmlc/xgboost
cd xgboost; make -j4`

I should forgot something because now I have:
No module named 'xgboost' when I try to `from xgboost import XGBRegressor`
",thank answer still working pip follow installation guide git clone recursive make forgot something module try import,issue,negative,neutral,neutral,neutral,neutral,neutral
293898812,"@JeremieGuez The issue may be caused by `pip install xgboost` since the version in pypi is not up-to-date (check this [issue](https://github.com/dmlc/xgboost/issues/1007#issuecomment-207170436)). For avoiding this warning message, you need reinstall xgboost via compiling it with source codes in its [GitHub repo](https://github.com/dmlc/xgboost) (check the [Installation Guide](http://xgboost.readthedocs.io/en/latest/build.html)).",issue may pip install since version check issue warning message need reinstall via source check installation guide,issue,negative,neutral,neutral,neutral,neutral,neutral
293885755,"Hi guys,

I have same problem, when I try to import xgboost: 
`import xgboost as xgb`

I have this warning:
DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)

- scikit-learn == 0.18.1
- xgboost == 0.6

Any advice ?? help? 

",hi problem try import import warning module version favor module class also note interface new different module module removed module removed advice help,issue,negative,positive,neutral,neutral,positive,positive
293224589,"Hmm, could you please try the dev version of tpot in development branch? It is noted that the dev version has not been not fully tested yet but it used `joblib` to replace `pathos` module for parallel computing related to the `n_jobs` parameter. I am curious whether the dev is better for large dataset.

Please try the commands below for installing the dev version.

```
pip uninstall tpot
pip install git+https://github.com/rhiever/tpot.git@development
```",could please try dev version development branch noted dev version fully tested yet used replace pathos module parallel related parameter curious whether dev better large please try dev version pip pip install,issue,positive,positive,positive,positive,positive,positive
293155834,"Same error with n_jobs =2
```
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regressi
on' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True                               
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 k should be >=0, <= n_features; got 52.Use k='all' to return all features.               
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' ar
e not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True                                           
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' ar
e not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True                                           
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 max_features must be in (0, n_features]                                                  
_pre_test decorator: _generate: num_test=1 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Input X must be non-negative                                                             
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not sup
ported, Parameters: penalty='l1', loss='hinge', dual=True                                                                           
_pre_test decorator: _generate: num_test=0 precomputed was provided as affinity. Ward can only work with euclidean distances.       
Invalid pipeline encountered. Skipping its evaluation.                                                                              
Traceback (most recent call last):                                                                                                  
  File ""training_tpot_debug.py"", line 177, in <module>                                           
    tpot.fit(X_train, y_train)                                                                                                      
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/tpot/base.py"", line 413, in fit                         
    verbose=self.verbosity, max_time_mins=self.max_time_mins)                                                                       
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/tpot/gp_deap.py"", line 138, in eaMuPlusLambda           
    fitnesses = toolbox.evaluate(invalid_ind)                                                                                       
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/tpot/base.py"", line 747, in _evaluate_individuals       
    resulting_score_list = [-float('inf') if x == 'Timeout' else x for x in list(res_imap)]                                         
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/multiprocess/pool.py"", line 698, in next                
    raise value                                                                                                                     
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/multiprocess/pool.py"", line 385, in _handle_tasks       
    put(task)                                                                                                                       
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/multiprocess/connection.py"", line 209, in send          
    self._send_bytes(ForkingPickler.dumps(obj))                                                                                     
  File ""~/anaconda3/envs/tpot/lib/python3.4/site-packages/multiprocess/connection.py"", line 402, in _send_bytes   
    header = struct.pack(""!i"", n)                                                                                                   
struct.error: 'i' format requires -2147483648 <= number <= 2147483647
```",error decorator unsupported set combination decorator input must decorator got return decorator unsupported set combination ar decorator input must decorator unsupported set combination ar decorator input must decorator input must decorator must decorator input must decorator input must decorator unsupported set combination sup ported decorator provided affinity ward work invalid pipeline skipping evaluation recent call last file line module file line fit file line file line else list file line next raise value file line put task file line send file line header format number,issue,negative,positive,neutral,neutral,positive,positive
293151864,"I think the problem is memory since when I decreased the sample size to 1000 then it could run with no error. However, the machine has 200gb available memory. The number of examples is around 70000, with around 20-25k features concatenated together with np.hstack. ",think problem memory since sample size could run error however machine available memory number around around together,issue,negative,positive,positive,positive,positive,positive
293145749,"Hmm, I have not seen this error message before. Could you please let me know more details about the environment and datasets, like the total numbers of samples and features and how many GB memory your computer has?

I suspect the error is caused by lack of memory when pickling objects in multiprocessing. Could you please also try to use lower number of `n_jobs`, like `n_jobs=2` for a test? ",seen error message could please let know environment like total many memory computer suspect error lack memory could please also try use lower number like test,issue,negative,positive,positive,positive,positive,positive
292651333,This bug seems to be fixed on the dev branch. Will roll it out with the next release.,bug fixed dev branch roll next release,issue,negative,positive,neutral,neutral,positive,positive
292548858,"Hi @sashml, were @weixuanfu2016's examples helpful for getting you started with a custom configuration in TPOT?",hi helpful getting custom configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
291998055,I may not be able to test it for a couple of weeks as I won't have access to the OS X machine. My apologies.,may able test couple wo access o machine,issue,negative,positive,positive,positive,positive,positive
291976994,Hi @lesshaste --- just wanted to check if the recent fixes on the dev branch helped on your end. We're still doing internal testing and everything seems fine in our tests.,hi check recent dev branch end still internal testing everything fine,issue,negative,positive,positive,positive,positive,positive
291592779,We'll make sure to get those fixes out on pip as soon as we wrap a couple other patches up.,make sure get pip soon wrap couple,issue,negative,positive,positive,positive,positive,positive
291582245,"It definitely did ! Thanks a lot for help.

",definitely thanks lot help,issue,positive,positive,positive,positive,positive,positive
291572998,"It is the same issue as #395. I think the current master branch already fixed this issue in python2.*. Please try the command below for reinstalling tpot. Let me know if the issue are fixed in your environment.

```
pip uninstall tpot
pip install git+https://github.com/rhiever/tpot.git
```",issue think current master branch already fixed issue please try command let know issue fixed environment pip pip install,issue,negative,positive,neutral,neutral,positive,positive
291243432,Merged just now. Sorry---was taking a break this weekend. :-),sorry taking break weekend,issue,negative,negative,negative,negative,negative,negative
290996771,"Sorry for the spam. Is there a timeline when this will be merged? I noticed it also removes pathos - and I have just been working all afternoon merging in those changes into my own branch ^^', I am not completely done yet - so I could put it on hold if the merge would happen soon?",sorry also pathos working afternoon branch completely done yet could put hold merge would happen soon,issue,negative,negative,negative,negative,negative,negative
290996632,Closing since an upcoming commit will fix this.,since upcoming commit fix,issue,negative,neutral,neutral,neutral,neutral,neutral
290993465,"Yes, a few unit tests in dev branch are not fixed yet after refining some operators's parameters in default operator dictionary. This [commit](https://github.com/weixuanfu2016/tpot/commit/544fdf3450c2a056db30f31f945712ef8382e8a9) (not merged yet) fixed these unit tests. ",yes unit dev branch fixed yet refining default operator dictionary commit yet fixed unit,issue,positive,positive,neutral,neutral,positive,positive
290933557,"Thank you for testing this new function. You install master branch in my repo. Try to the command below for switching to the branch with this new function. 

```
pip install git+https://github.com/weixuanfu2016/tpot.git@replace_pathos
```

Alternatively, for the tests, you could try to use `git` command to run these new codes in repo without installing into you system environment. Commands are list below and more information can be found in this [link](http://rhiever.github.io/tpot/contributing/).

```
git clone git@github.com:weixuanfu2016/tpot.git
cd tpot
# switch to the branch with new function in Windows OS
git checkout replace_pathos 
# Once some changes are saved locally, 
# you can use your tweaked version of TPOT by navigating to the project's base directory (the command of `cd tpot` above)
# just type `python` under the project's base directory into interactive mode
# for command mode use `python -m tpot.driver` instead
```




",thank testing new function install master branch try command switching branch new function pip install alternatively could try use git command run new without system environment list information found link git clone git switch branch new function o git saved locally use version project base directory command type python project base directory interactive mode command mode use python instead,issue,positive,negative,negative,negative,negative,negative
290930660,"I uninstalled tpot using pip on Anaconda: pip uninstall tpot
Then, went to your branch and did the following on my Windows Anaconda prompt:
pip install git+https://github.com/weixuanfu2016/tpot.git
Looks like it installed TPOT==0..7.0

However, when I run TPOT with n_jobs = -1, I get the following:

Warning: Parallelization is not currently supported in TPOT for Windows.  Setting n_jobs to 1 during the TPOT optimization process.

I must not be installing your latest version correctly. Could you please let me know where I am making the mistake? Thanks
",uninstalled pip anaconda pip went branch following anaconda prompt pip install like however run get following warning parallelization currently setting optimization process must latest version correctly could please let know making mistake thanks,issue,positive,positive,positive,positive,positive,positive
290833441,"Yes, below is a example in `xgboost`

[Import](https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/compat.py#L47-L63)

[Build sklearn-based class](https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py#L351)",yes example import build class,issue,negative,neutral,neutral,neutral,neutral,neutral
290832121,"So, it's mean I have following scenario as possible:

1. Build my own adapters per 3rd party algorithms
2. Pass new classes to TPOT

Thanks",mean following scenario possible build per party pas new class thanks,issue,positive,positive,neutral,neutral,positive,positive
290829778,"The format of dictionary is right but the operators in the dictionary are not inherited from [`sklearn.base.RegressorMixin`](http://scikit-learn.org/stable/modules/generated/sklearn.base.RegressorMixin.html) and TPOT cannot build pipeline without the `RegressorMixin` based operator as root of pipeline. 

The codes below can check if the operators are from `RegressorMixin`
```
from sklearn.base import RegressorMixin

from glmnet import ElasticNet
from mlxtend.regressor import LinearRegression

print('mlxtend.regressor.LinearRegression', issubclass(LinearRegression, RegressorMixin))
print('glmnet.ElasticNet', issubclass(ElasticNet, RegressorMixin)) 

# xgboost works
#from xgboost import XGBRegressor
#print('xgboostXGBRegressor.', issubclass(ElasticNet, XGBRegressor))
# sklearn.ensemble.ExtraTreesRegressor also works
from sklearn.ensemble import ExtraTreesRegressor
print('sklearn.ensemble.ExtraTreesRegressor', issubclass(ExtraTreesRegressor, RegressorMixin))

```

",format dictionary right dictionary build pipeline without based operator root pipeline check import import import print print work import print also work import print,issue,negative,positive,positive,positive,positive,positive
290811018,"New package in addition, `hyperas` for tuning keras models - https://github.com/maxpumperla/hyperas",new package addition tuning,issue,negative,positive,positive,positive,positive,positive
290799307,"
[![Coverage Status](https://coveralls.io/builds/10873747/badge)](https://coveralls.io/builds/10873747)

Changes Unknown when pulling **b43eff5ca1789333d49c700c229be683745889e6 on weixuanfu2016:replace_pathos** into ** on rhiever:development**.
",coverage status unknown development,issue,negative,negative,neutral,neutral,negative,negative
290797714,"Just FYI, I just add Parallelization function for Windows OS in the PR #406. But it has not been fully tested yet. If you want to speed up in the Windows OS, please try it in this git branch of my tpot repo.",add parallelization function o fully tested yet want speed o please try git branch,issue,negative,neutral,neutral,neutral,neutral,neutral
290797001,"I remove `pathos` dependency in the PR #406, which may solve the issue in built-in environment. Could you please test it in this git branch of my repo? ",remove pathos dependency may solve issue environment could please test git branch,issue,negative,neutral,neutral,neutral,neutral,neutral
290686842,"It is very inconsistent, sometimes running to completion and sometimes stalling right at the start at 1%.",inconsistent sometimes running completion sometimes stalling right start,issue,negative,positive,positive,positive,positive,positive
290606683,"Thanks for your notes. You are right, building a virtual Linux machine could be a good solution. I am afraid thought that I might screw up somewhere since I am not at all good with this! But, it seems like I have to learn how to do it. 
I am finding out (very quickly) that a lot of the functionalities are not available or don't work properly on Windows, but work perfectly on MacOS and Linux machines. I was using mlxtend to build stacked classifier and it would not take Xgboost as the meta classifier on my machine. It worked when other tried on their Mac and Linux machines. Maybe, because most of the developers are using Mac & Linux ...
Your help and suggestions much appreciated.

      From: Weixuan <notifications@github.com>
 To: rhiever/tpot <tpot@noreply.github.com> 
Cc: Subrata Chatterjee <chatterjee_sub@yahoo.com>; Author <author@noreply.github.com>
 Sent: Thursday, March 30, 2017 10:24 PM
 Subject: Re: [rhiever/tpot] Extremely slow (#404)
   
BTW, n_jobs parameter (default value = 1) can set the number of CPUs for evaluating pipelines in parallel during the TPOT optimization process. It can speed up a lot with using multiple cores, but sadly this function is not available in Windows OS now. For such a good machine you have, you may build a virtual machine running Linux for this jobs.—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.  

   ",thanks right building virtual machine could good solution afraid thought might screw somewhere since good like learn finding quickly lot available work properly work perfectly build classifier would take meta classifier machine worked tried mac maybe mac help much author author sent march subject extremely slow parameter default value set number parallel optimization process speed lot multiple sadly function available o good machine may build virtual machine running thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
290605807,"BTW, `n_jobs` parameter (default value = 1) can set the number of CPUs for evaluating pipelines in parallel during the TPOT optimization process. It can speed up a lot with using multiple cores, but sadly this function is not available in Windows OS now. For such a good machine you have, you may build a virtual machine running Linux for this job.   ",parameter default value set number parallel optimization process speed lot multiple sadly function available o good machine may build virtual machine running job,issue,positive,positive,positive,positive,positive,positive
290604744,"For Windows OS, I think the only way to speed up TPOT is to set a maximum evaluation time of a single pipeline with the `max_eval_time_mins` parameters in TPOT. The default `max_eval_time_mins` is 5 minutes. For the case in this issue, the average time per pipeline is around 77 seconds. You may try to set this parameter to `max_eval_time_mins=2` for skipping these time-consuming pipeline. This parameter is available in version > 0.6.6 of TPOT. Also, please try this parameter in python 3.5 environment for current TPOT 0.7. A patch for supporting python 2.7 in Windows was updated yesterday in GitHub but I am not sure the TPOT in PyPi is up to date.",o think way speed set maximum evaluation time single pipeline default case issue average time per pipeline around may try set parameter skipping pipeline parameter available version also please try parameter python environment current patch supporting python yesterday sure date,issue,positive,positive,positive,positive,positive,positive
290526182,"This was done in https://github.com/rhiever/tpot/commit/c36d6cefa8219c975c8aa8f4f602b0e2258848ed

Tests need fixing though.",done need fixing though,issue,negative,neutral,neutral,neutral,neutral,neutral
290505870,"My initial guess is that there is a clash between XGBoost's parallelization method (OpenMP, IIRC) and pathos/multiprocessing. But we'll need to dig into that more to find out.",initial guess clash parallelization method need dig find,issue,negative,neutral,neutral,neutral,neutral,neutral
290504287,"Deleting xgboost does seem to have done it. Thank you.

Any idea what the clash is between xgboost and pathos?",seem done thank idea clash pathos,issue,negative,neutral,neutral,neutral,neutral,neutral
290498996,"I can confirm the freezing issue on my system (Python 3 with Anaconda) when I install XGBoost. It does not happen when I uninstall XGBoost.

We're going to rework the multiprocessing support in TPOT a bit so TPOT doesn't use pathos at all when `n_jobs==1`. This should allow the use of XGBoost without issue, I hope. But until we sort out the problem that XGBoost has with pathos/multiprocessing, we will need to add a block to where users cannot use `n_jobs!=1` and XGBoost within TPOT.

Thank you for reporting the issue, @lesshaste.",confirm freezing issue system python anaconda install happen going rework support bit use pathos allow use without issue hope sort problem need add block use within thank issue,issue,negative,neutral,neutral,neutral,neutral,neutral
290476319,"Somehow, I fixed the issue on Ubuntu built-in python 2.7.12 by updating `pip` (`sudo pip install --upgrade pip`) and then removing xgboost (`sudo pip uninstall xgboost `). Could you please try it on your Ubuntu and let me know if it works? @lesshaste ",somehow fixed issue python pip pip install upgrade pip removing pip could please try let know work,issue,negative,positive,neutral,neutral,positive,positive
290475072,"That's right. Python 3 does not fix the problem for me. I thought it did but it just turns out the amount of progress you make varies for each run. 

I don't think I have anaconda installed at all. All the python modules are installed using pip.  Here is my python 3 environment.

Python 3.5.2
numpy 1.12.1
scipy 0.19.0
sklearn 0.18.1
deap 1.0
xgboost 0.6 
update_checker 0.16
tqdm 4.11.2
pathos 0.2.0


",right python fix problem thought turn amount progress make run think anaconda python pip python environment python pathos,issue,negative,positive,positive,positive,positive,positive
290473438,"Hmm, do you mean python 3 also did not work? Is the python 3 environment created by anaconda? ",mean python also work python environment anaconda,issue,negative,negative,negative,negative,negative,negative
290465043,"Python 2.7.12
numpy 1.12.1
scipy 0.19.0
sklearn 0.18.1
deap 1.0
xgboost 0.4 
update_checker 0.16
tqdm 4.11.2
pathos 0.2.0
multiprocessing 0.70a1

Using python 3 sadly does not resolve the issue.

----

My OS X system which shows the same problem has the following version numbers

Python 2.7.10
numpy 1.12.0
scipy 0.18.1
sklearn 0.18.1
deap 1.0
ImportError: No module named xgboost
update_checker 0.16
tqdm 4.11.2
pathos 0.2.0
multiprocessing 0.70a1


",python pathos python sadly resolve issue o system problem following version python module pathos,issue,negative,negative,negative,negative,negative,negative
290461180,"Okay. So it's a Python environment issue. @lesshaste, can you please run the following in your environment and let us know the versions that come out?

```
python --version
python -c ""import numpy; print('numpy %s' % numpy.__version__)""
python -c ""import scipy; print('scipy %s' % scipy.__version__)""
python -c ""import sklearn; print('sklearn %s' % sklearn.__version__)""
python -c ""import deap; print('deap %s' % deap.__version__)""
python -c ""import xgboost; print('xgboost %s ' % xgboost.__version__)""
python -c ""import update_checker; print('update_checker %s' % update_checker.__version__)""
python -c ""import tqdm; print('tqdm %s' % tqdm.__version__)""
python -c ""import pathos; print('pathos %s' % pathos.__version__)""
python -c ""import multiprocessing; print('multiprocessing %s' % multiprocessing.__version__)""
```",python environment issue please run following environment let u know come python version python import print python import print python import print python import print python import print python import print python import print python import pathos print python import print,issue,negative,neutral,neutral,neutral,neutral,neutral
290458549,It works fine in anaconda python 2.7,work fine anaconda python,issue,negative,positive,positive,positive,positive,positive
290457800,"Let's focus on finding a patch to make it work in this specific case. We've explored many different parallelization libraries and spent too much time on it. I want to move on.

If the issue turns out to be too complicated---and the issue is limited to Python 2---then we may just drop Python 2 support.",let focus finding patch make work specific case many different parallelization spent much time want move issue turn complicated issue limited python may drop python support,issue,negative,positive,neutral,neutral,positive,positive
290432955,"@rhiever Oh, working fine with Ubuntu built-in python instead of anaconda python? I met the same issue in built-in python with installing all other dependencies by `pip` but had no issue with anaconda python.

So I tried to disable `pathos` in TPOT and test in Ubuntu  built-in python, then it worked fine. So I suspect `pathos` for this issue in built-in environment.",oh working fine python instead anaconda python met issue python pip issue anaconda python tried disable pathos test python worked fine suspect pathos issue environment,issue,negative,positive,positive,positive,positive,positive
290429813,"@weixuanfu2016, why do you suspect that `pathos` is the problem? It's been working fine on my install.",suspect pathos problem working fine install,issue,negative,positive,positive,positive,positive,positive
290412275,"I tested the example in my Ubuntu 16.04.2 with built-in python 2.7.12. The issue was indeed reproduced. Then I disabled the `pathos` in source codes and the example worked fine then. 

Then I tried to use conda to install a new python 2.7 environment then the example worked fine with `pathos` module

I think that the reason of this issue may be related to the `pathos` module which possibly does not work well with other dependencies in the built-in environment. I am thinking about using an alternative module to replace `pathos` in TPOT.

",tested example python issue indeed disabled pathos source example worked fine tried use install new python environment example worked fine pathos module think reason issue may related pathos module possibly work well environment thinking alternative module replace pathos,issue,positive,positive,positive,positive,positive,positive
290389129,"python 2.7.12.  

I just tested python 3.5.2 and it works (apart from the odd `Invalid pipeline encountered. Skipping its evaluation.` message). 

 Is TPOT python 3 only?

----

I take it back. It seems to work intermittently. This is very strange.",python tested python work apart odd invalid pipeline skipping message python take back work intermittently strange,issue,negative,negative,neutral,neutral,negative,negative
290387829,Thank you for these information. I have tested it with Conda and python 3.6 and could not reproduce the issue. What is your python version? I will try to build the same environment on my ubuntu for more tests. ,thank information tested python could reproduce issue python version try build environment,issue,negative,neutral,neutral,neutral,neutral,neutral
290384266,I just tried it on a completely different machine (OS X) at work and it gives the same problem.    Maybe discuss it at https://gitter.im/rhiever/tpot ?,tried completely different machine o work problem maybe discus,issue,negative,neutral,neutral,neutral,neutral,neutral
290382448,"That's odd. Here is the information:

Ubuntu 16.04.2 LTS
tpot 0.7.0
scikit-learn 0.18.1

I am not using conda.

I ran `strace python boston.py` and at the point it is stalling it just prints out

`select(0, NULL, NULL, NULL, {0, 200000}) = 0 (Timeout)` repeatedly.",odd information ran python point stalling select null null null repeatedly,issue,negative,negative,negative,negative,negative,negative
290381786,"Hmm, I just tested the examples more than 10 times but could not reproduce the issue. Could you please provide more information about your running environment, like OS, TPOT version, conda version and these dependencies' version etc.? 
",tested time could reproduce issue could please provide information running environment like o version version version,issue,positive,neutral,neutral,neutral,neutral,neutral
290187962,This has been on my TODO list since the 0.7 release. Hope to update it soon!,list since release hope update soon,issue,negative,neutral,neutral,neutral,neutral,neutral
290043426,"thanks a lot. 
in the meantime I migrated part of the code to 3.5 and running tpot on that.",thanks lot part code running,issue,negative,positive,positive,positive,positive,positive
290029714,"Thank you for let us know this issue. Yes, the animated example need to be updated. Just FYI, please check the updated non-animated examples in this [link](http://rhiever.github.io/tpot/examples/)",thank let u know issue yes animated example need please check link,issue,positive,neutral,neutral,neutral,neutral,neutral
290027514,"It seems that _stop attribute is not available for some versions (like python2.*) of python in Windows. I will look into it and fix the bug.

Meanwhile, could you please create a python 3.5 environment with a command `conda create -n py35 python=3.5 anaconda` in your DOS ([More Info](https://conda.io/docs/py2or3.html#create-a-python-3-5-environment)) and then activate it by `activate py35` for solving the issue for now?",attribute available like python python look fix bug meanwhile could please create python environment command create anaconda do activate activate issue,issue,positive,positive,positive,positive,positive,positive
289422380,Please change the file name rather than tpot.py. Check the closed issue #372 for more details.,please change file name rather check closed issue,issue,negative,negative,neutral,neutral,negative,negative
289256205,"I have run some tests for this issue. These issue happened to both [this pmlb benchmark](https://github.com/EpistasisLab/penn-ml-benchmarks/raw/master/datasets/Hill_Valley_without_noise/Hill_Valley_without_noise.csv.gz) and `digits` benchmark with `n_jobs` or without `n_jobs` (disable `pathos.multiprocessing`). So I think this issue may not be related to `n_jobs`. 

I suspect it may be from the new mu + lambda algorithm. It will need more tests for confirming it. We also need a unit test for this issue.  ",run issue issue without disable think issue may related suspect may new mu lambda algorithm need confirming also need unit test issue,issue,negative,positive,neutral,neutral,positive,positive
289251421,"Hmm, I remember this issue was fixed once and confirmed by one user before. I will look into it",remember issue fixed confirmed one user look,issue,negative,positive,positive,positive,positive,positive
289064429,"Please re-open this issue and let us know if you find out the Python environment setting that seemed to cause TPOT 0.7 to freeze (or stop working). If it is a reproducible issue, then we will endeavor to patch it ASAP.",please issue let u know find python environment setting cause freeze stop working reproducible issue endeavor patch,issue,negative,neutral,neutral,neutral,neutral,neutral
288923621,"Great, good to know the issue is solved. It is my pleasure. Thank you for sharing the stdout here.",great good know issue pleasure thank,issue,positive,positive,positive,positive,positive,positive
288922323,"Thank you very much for your information.

If I try 2 examples with setting verbosity=3, it seems to stop after several steps.

Iris
DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
30 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True
_pre_test decorator: _generate: num_test=0 max_features must be in (0, n_features]
_pre_test decorator: _generate: num_test=1 max_features must be in (0, n_features]
_pre_test decorator: _generate: num_test=0 Input X must be non-negative
_pre_test decorator: _generate: num_test=1 Input X must be non-negative
_pre_test decorator: _generate: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.
_pre_test decorator: _generate: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True
_pre_test decorator: _generate: num_test=0 max_features must be in (0, n_features]
_pre_test decorator: _generate: num_test=0 Input X must be non-negative
_pre_test decorator: _generate: num_test=1 precomputed was provided as affinity. Ward can only work with euclidean distances.
_pre_test decorator: _generate: num_test=2 Input X must be non-negative
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True
_pre_test decorator: _generate: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.
_pre_test decorator: _generate: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67
Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]Invalid pipeline encountered. Skipping its evaluation.
Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:   4%|▍         | 12/300 [00:20<28:12,  5.88s/pipeline]

MNIST
DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)
30 operators have been imported by TPOT.
_pre_test decorator: _generate: num_test=0 max_features must be in (0, n_features]
_pre_test decorator: _generate: num_test=1 Input X must be non-negative
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True
_pre_test decorator: _generate: num_test=1 Input X must be non-negative
_pre_test decorator: _generate: num_test=0 Input X must be non-negative
_pre_test decorator: _generate: num_test=1 coef_ is only available when using a linear kernel
_pre_test decorator: _generate: num_test=0 Input X must be non-negative
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True
_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True
_pre_test decorator: _generate: num_test=0 precomputed was provided as affinity. Ward can only work with euclidean distances.
Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]_pre_test decorator: _generate: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False
_pre_test decorator: _generate: num_test=0 Input X must be non-negative
Invalid pipeline encountered. Skipping its evaluation.
Invalid pipeline encountered. Skipping its evaluation.
Optimization Progress:   5%|▌         | 16/300 [01:01<13:48,  2.92s/pipeline]

 However if I recreate another python3.5 environment, those things are completely solved. I'm not sure the reason why, but I'm really sorry to disturb you. I tried uninstall and reinstall TPOT several times, but I did not recreate python environment itself. I should do it by myself.

 Of course, several new features work very well.

 Thank you very much for your kind help.",thank much information try setting stop several iris module version favor module class also note interface new different module module removed module removed decorator found array feature minimum decorator unsupported set combination decorator must decorator must decorator input must decorator input must decorator found array feature minimum decorator unsupported set combination decorator must decorator input must decorator provided affinity ward work decorator input must decorator unsupported set combination decorator provided affinity ward work decorator optimization progress invalid pipeline skipping evaluation invalid pipeline skipping evaluation optimization progress module version favor module class also note interface new different module module removed module removed decorator must decorator input must decorator unsupported set combination decorator input must decorator input must decorator available linear kernel decorator input must decorator unsupported set combination decorator unsupported set combination decorator unsupported set combination decorator provided affinity ward work optimization progress decorator unsupported set combination decorator input must invalid pipeline skipping evaluation invalid pipeline skipping evaluation optimization progress however recreate another python environment completely sure reason really sorry disturb tried reinstall several time recreate python environment course several new work well thank much kind help,issue,positive,positive,positive,positive,positive,positive
288909087,"I tested both Iris and MNIST examples with TPOT 0.7.0 on my Ubuntu 16.04 environment with anaconda3. Both examples worked fine in my environment. I will try to use virtual machine later for checking whether this issue is OS-specific. 

Meanwhile, could you please try either of these two examples again **with setting `verbosity=3`**. Maybe error messages could show up. Or you may try to create a python3.5 environment using a simple command `conda create -n py35 python=3.5 anaconda` and then activate the new environment with a command `source activate py35` (check more [details](https://conda.io/docs/py2or3.html#use-a-different-version-of-python)). After that, you can install TPOT based on manual in the new python environment for testing TPOT 0.7. Please let me know if it can solve the issue for now.",tested iris environment anaconda worked fine environment try use virtual machine later whether issue meanwhile could please try either two setting maybe error could show may try create python environment simple command create anaconda activate new environment command source activate check install based manual new python environment testing please let know solve issue,issue,positive,positive,positive,positive,positive,positive
288765075,"Thank you all, after running for 100generations, it releases the Best pipeline: GradientBoostingClassifier(GaussianNB(FeatureAgglomeration(GradientBoostingClassifier(PCA(StandardScaler(CombineDFs(input_matrix, input_matrix)), 8), 19.0, 4.0), 24, 9)), 0.01, 28.0)
(None, 0.94935064935064939) with the same accuracy scale.",thank running best pipeline none accuracy scale,issue,positive,positive,positive,positive,positive,positive
288734819,"I bet we could hack this into TPOT for a quick test by passing TPOT's `cv` parameter a `StratifiedShuffleSplit` instead of KFold. If the `StratifiedShuffleSplit`'s test set is a small sample (e.g. 10%) of the full training data, then that would achieve what we're looking for here.",bet could hack quick test passing parameter instead test set small sample full training data would achieve looking,issue,negative,positive,positive,positive,positive,positive
288663458,Thank you so much weixuanfu201 and Rhiver. Both of your answers help me to understand more.,thank much help understand,issue,positive,positive,positive,positive,positive,positive
288516415,"Nope, this is not addressed in the 0.7 release since we do not check some specifical models/ models combination in optimization processes. The slow pipeline would have -inf fitness score due to timeout and it would be selected for next generation with updated NSGA-II selection operator in version 0.7 but these slow models and combination may be generated in later generations due to crossover and mutation. I think we may need an intelligent way to adapt the operator list or operator combinations during optimization process ",nope release since check specifical combination optimization slow pipeline would fitness score due would selected next generation selection operator version slow combination may later due crossover mutation think may need intelligent way adapt operator list operator optimization process,issue,positive,negative,neutral,neutral,negative,negative
288513179,"@weixuanfu2016, can you please confirm that this issue is addressed in the 0.7 release? IIRC models that failed to finish evaluating due to timeouts have their ""timeout score"" recorded in the lookup dictionary, and are thus overlooked on all future evaluations, right?",please confirm issue release finish due score dictionary thus future right,issue,negative,positive,neutral,neutral,positive,positive
288220351,"I suspect this happened because TPOT uses a different default scoring function than sklearn. TPOT uses balanced accuracy, whereas sklearn uses accuracy. In the upcoming 0.7 release, we have changed it so TPOT uses accuracy by default to avoid this kind of confusion.",suspect different default scoring function balanced accuracy whereas accuracy upcoming release accuracy default avoid kind confusion,issue,negative,positive,positive,positive,positive,positive
288220035,We're closing all questions that haven't been updated in a while. Please feel free to re-open this issue if your issue persists.,please feel free issue issue,issue,positive,positive,positive,positive,positive,positive
288212895,"Hmm, I agree. We need a friendly warning message for invalid input data",agree need friendly warning message invalid input data,issue,positive,positive,positive,positive,positive,positive
288211335,"While we're at it on this issue, maybe we can also catch when the user passes invalid data to TPOT at the beginning of `fit`.

e.g., we create a very simple sklearn pipeline (just a DecisionTree Classifier/Regressor) and try a simple fit with it on the data the user provides. If it fits without an exception (which would be caught in a try/except), then TPOT proceeds as normal. If it fails, then TPOT throws an exception stating that the data is not in proper shape.",issue maybe also catch user invalid data beginning fit create simple pipeline try simple fit data user without exception would caught proceeds normal exception data proper shape,issue,positive,positive,positive,positive,positive,positive
288210372,Do you mean typo in `scoring` parameter?,mean typo scoring parameter,issue,negative,negative,negative,negative,negative,negative
287802780,"We can live with a different filename :-)

Keep up the good work :+1: ",live different keep good work,issue,negative,positive,positive,positive,positive,positive
287801974,"Yes, it seems we may not be able to work around it. Will have to investigate further.",yes may able work around investigate,issue,negative,positive,positive,positive,positive,positive
287801558,"According to #374 it was an unfortunate / mistaken name selection for the exported file.  
I didn't realize it earlier.

Thank you @rhiever for your time and @teaearlgraycold for spotting it.",according unfortunate mistaken name selection file realize thank time spotting,issue,negative,negative,negative,negative,negative,negative
287797465,"This isn't a bug, just a classic mistake.

Python files are treated as modules just the same as system/user libraries. They share the same namespace, and a collision in the form of a file named `tpot.py` will cause the installed library to be overwritten (this can happen with other library, `json.py` would overwrite the json library).",bug classic mistake python share collision form file cause library happen library would overwrite library,issue,negative,positive,positive,positive,positive,positive
287794062,"Oh interesting. This looks like some strange sort of Python import error. `tpot.py` is one of the files in the TPOT project, and it seems to read your local `tpot.py` first in place of TPOT project's `tpot.py`. I'm going to file this as a separate bug report so we can figure out what to do here. In the meantime, yes, avoid having a file called `tpot.py` in your local working directory when using TPOT. Sorry for the strange bug!",oh interesting like strange sort python import error one project read local first place project going file separate bug report figure yes avoid file local working directory sorry strange bug,issue,negative,positive,neutral,neutral,positive,positive
287626970,"No, it happens again.
On this [notebook](https://github.com/YannisPap/Identify-Fraud-using-Machine-Learning/blob/master/Project0.5.ipynb), look at the last cell. I'm trying to import TPOTClassifier to make a validation of my results and it appears again.

So, it reads tpot.py, when I delete the file it runs fine.
Is this a kind of feature I use the wrong way?
I mean, why does it read this file during import?",notebook look last cell trying import make validation delete file fine kind feature use wrong way mean read file import,issue,negative,positive,neutral,neutral,positive,positive
287624430,"Actually I wasn't trying to run any code.
I opened a new notebook trying to import the library and the error was appearing.

What it is more strange is that today I tried it again and it worked just fine!

I don't know, probably something was wrong with my environment (I did some changes since then). In any case I cannot reproduce it so let's close it and if it happens again we can reopen it.

Taking advantage of the opportunity I want to thank you for this nice peace of software.
Keep up the good work.",actually trying run code new notebook trying import library error strange today tried worked fine know probably something wrong environment since case reproduce let close reopen taking advantage opportunity want thank nice peace keep good work,issue,positive,positive,positive,positive,positive,positive
287612527,I have the same issue. Please add this to documentation.,issue please add documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
287461213,"It looks like you're trying to run code that was exported by TPOT, is that right? That's what the traceback seems to indicate. If that's the case, you need to replace the string `'PATH/TO/DATA/FILE'` with the actual path to your data file. Same for the `'COLUMN_SEPARATOR'`, with the character that separates your columns in the data file.",like trying run code right indicate case need replace string actual path data file character separate data file,issue,negative,positive,positive,positive,positive,positive
287402459,"Actually it crashes during the import, so I don't have any code to send you.
I'm sending you the Traceback it throws.

It crashes when I'm trying import it in my code (tested it with Jupyter Notebook & Spyder) but I was able to run it as a command in the terminal.

```{python}
OSError                                   Traceback (most recent call last)
<ipython-input-1-b6ddd0209001> in <module>()
----> 1 from tpot import TPOTClassifier

/home/yannis/Projects/Identify-Fraud-using-Machine-Learning/tpot.py in <module>()
      8 
      9 # NOTE: Make sure that the class is labeled 'class' in the data file
---> 10 tpot_data = np.recfromcsv('PATH/TO/DATA/FILE', delimiter='COLUMN_SEPARATOR', dtype=np.float64)
     11 features = np.delete(tpot_data.view(np.float64).reshape(tpot_data.size, -1), tpot_data.dtype.names.index('class'), axis=1)
     12 training_features, testing_features, training_classes, testing_classes = \

/home/yannis/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py in recfromcsv(fname, **kwargs)
   2044     kwargs.setdefault(""delimiter"", "","")
   2045     kwargs.setdefault(""dtype"", None)
-> 2046     output = genfromtxt(fname, **kwargs)
   2047 
   2048     usemask = kwargs.get(""usemask"", False)

/home/yannis/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py in genfromtxt(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows)
   1510                 fhd = iter(np.lib._datasource.open(fname, 'rbU'))
   1511             else:
-> 1512                 fhd = iter(np.lib._datasource.open(fname, 'rb'))
   1513             own_fhd = True
   1514         else:

/home/yannis/anaconda3/lib/python3.6/site-packages/numpy/lib/_datasource.py in open(path, mode, destpath)
    149 
    150     ds = DataSource(destpath)
--> 151     return ds.open(path, mode)
    152 
    153 

/home/yannis/anaconda3/lib/python3.6/site-packages/numpy/lib/_datasource.py in open(self, path, mode)
    499             return _file_openers[ext](found, mode=mode)
    500         else:
--> 501             raise IOError(""%s not found."" % path)
    502 
    503 

OSError: PATH/TO/DATA/FILE not found.
```",actually import code send sending trying import code tested notebook able run command terminal python recent call last module import module note make sure class data file delimiter none output false delimiter unpack loose iter else iter true else open path mode return path mode open self path mode return found else raise found path found,issue,negative,positive,neutral,neutral,positive,positive
287385377,"Hmm, it is weird. `PATH/TO/DATA/FILE` should be in scripts exported by TPOT not for import. 

Could you please provide your codes or Jupyter Notebook for reproducing this issue?   ",weird import could please provide notebook issue,issue,negative,negative,negative,negative,negative,negative
287326456,No problem! And those are indeed the type of strings I meant. Closing this issue now as the discussion regarding stringifying individuals should now be more or less settled for now (at least there is a working version into the dev branch) and the original issue has also been solved through earlier reworks.,problem indeed type meant issue discussion regarding le settled least working version dev branch original issue also,issue,negative,positive,neutral,neutral,positive,positive
287177235,"Any update ? Is it possible to collaborate with us ? If yes, thank you to share your first tests.",update possible collaborate u yes thank share first,issue,positive,positive,positive,positive,positive,positive
287108802,"No need. I think I figure it out. Need more tests

Here is a demo for `from_string`:

```
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
from deap import creator
import numpy as np

# Set up the MNIST data set for testing
mnist_data = load_digits()
training_features, testing_features, training_classes, testing_classes = \
    train_test_split(mnist_data.data.astype(np.float64), mnist_data.target.astype(np.float64), random_state=42)


tpot_obj = TPOTClassifier()
pipeline_string= 'KNeighborsClassifier(input_matrix, KNeighborsClassifier__n_neighbors=10, KNeighborsClassifier__p=1,KNeighborsClassifier__weights=uniform)'
tpot_obj._optimized_pipeline = creator.Individual.from_string(pipeline_string, tpot_obj._pset)
tpot_obj._fitted_pipeline = tpot_obj._toolbox.compile(expr=tpot_obj._optimized_pipeline)
tpot_obj._fitted_pipeline.fit(training_features, training_classes)
score = tpot_obj.score(testing_features, testing_classes)

print(score)
```

Thank you again!",need think figure need import import import import creator import set data set testing score print score thank,issue,negative,neutral,neutral,neutral,neutral,neutral
287106579,Could you please provide an example about using `from_string` to build a deap individual object? ,could please provide example build individual object,issue,negative,neutral,neutral,neutral,neutral,neutral
287087146,"Alright, I looked into this, and a couple of observations:
* This bug exists in the current dev branch, it is not specific to my addition.
 The bug is caused because the [`_pre_test`](https://github.com/rhiever/tpot/blob/development/tpot/decorators.py#L125) wrapper expects a single return value from the function it wraps, as seen in the line 147: `expr = func(self, *args, **kwargs)`. However, the wrapping is also applied to the [mutate](https://github.com/rhiever/tpot/blob/development/tpot/base.py#L325) and [mate](https://github.com/rhiever/tpot/blob/development/tpot/base.py#L323) operators, both which return tuples (in the form of `ind1,` for the former, and `ind1, ind2` for the latter). Because of this during the `expr_to_tree` conversion in `_pre_test`, the line [` while len(stack[-1][1]) == stack[-1][0].arity:`](https://github.com/rhiever/tpot/blob/development/tpot/export_utils.py#L106) tries to get the arity property of the individual (the first element in the tuple), rather than the primitive (first element in the individual). This rises above exception. By creating separate wrapper functions, or by first analysing the return results to proper only forward the individual to `expr_to_tree` and not the tuple, this exception will no longer be raised ([example](https://github.com/PG-TUe/tpot/commit/b0cb35ca726b07947f7498155f8f80b0c69bcbf6)). 
* After the changes, instead, an exception will be raised that ""'Individual' object has no attribute 'operators'"". Because both the mutate and mate operators are wrapped inline with `_pre_test`, rather than through the @-syntax. It is not exactly clear to my why (for now, still reading up), but the `self` passed to `_pre_test` will be an individual, not the tpot object, when the decoration is done in-line. Adding `@_pre_test` to the `_random_mutation_operator` method for example, would be a solution to avoid said error with the mutate operator ([example](https://github.com/PG-TUe/tpot/commit/8abe2a40f4a7e220381870bc19d12acba73b3375)).

Lastly, for the matter at hand of making terminal names unique, there was a problem with the changes I suggested above. Giving names to terminals means they will only be used as symbolic links, in order to get the actual value, such as needed in [`prim_to_list`](https://github.com/rhiever/tpot/blob/development/tpot/export_utils.py#L96), we need to actually have access to the primitive set of which the terminals originate to access the context. This change is also relatively straightforward ([example](https://github.com/PG-TUe/tpot/commit/0a258e9408a03ee972005169a72ee2e4d4d19ec2)).

After all above changes, from what I can tell, the only exceptions thrown in pre-test are now actually because certain parameter settings are illegal - not because of coding errors.

I also worked on accepting missing values. The only changes needed are in [this](https://github.com/PG-TUe/tpot/commit/a9383208d47eda9bdade6e6340756c24c685ce31) commit. I am thinking of adding an option so that 'MISSING' will not be used during the generation of individuals (as this means the default values of scikit-learn will be used more often in the generation process if it is also represented by its explicit value).",alright couple bug current dev branch specific addition bug wrapper single return value function seen line self however wrapping also applied mutate mate return form former latter conversion line stack stack get property individual first element rather primitive first element individual exception separate wrapper first return proper forward individual exception longer raised example instead exception raised object attribute mutate mate wrapped rather exactly clear still reading self individual object decoration done method example would solution avoid said error mutate operator example lastly matter hand making terminal unique problem giving used symbolic link order get actual value need actually access primitive set originate access context change also relatively straightforward example tell thrown actually certain parameter illegal also worked missing commit thinking option used generation default used often generation process also explicit value,issue,positive,positive,neutral,neutral,positive,positive
286853925,"I only make 2 changes based on lastest dev branch in TPOT. 
1. Based on the codes below in your comment:
```
# Terminals
        for _type in self.arguments:
            for val in _type.values:
                terminal_name = _type.__name__ + ""="" + str(val)
                self._pset.addTerminal(val, _type, name=terminal_name)
```
2. For printing out error, change this [line](https://github.com/rhiever/tpot/blob/development/tpot/decorators.py#L155) in the `_pre_test` decorator to:
```
            except Exception as e:
                 print(e)
```

Or you can merge the PR #367 for fixing another bug in `_pre_test` into your dev branch for the test.

You could use the codes below to reproduce the issue:
```
from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.25, test_size=0.75)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, n_jobs = 2, random_state = 44)
tpot.fit(X_train, y_train)
```
",make based dev branch based comment printing error change line decorator except exception print merge fixing another bug dev branch test could use reproduce issue import import import,issue,negative,neutral,neutral,neutral,neutral,neutral
286849200,"Can you provide more details? An example of how I can reproduce the issue? I'm fairly sure I worked a little bit on this afterwards, but my memory is a bit fuzzy about what I changed exactly. Maybe I can see if I had added changes which fixed this (though it would have been accidental, as I do not believe I saw such errors come by).",provide example reproduce issue fairly sure worked little bit afterwards memory bit fuzzy exactly maybe see added fixed though would accidental believe saw come,issue,negative,positive,positive,positive,positive,positive
286833701,@PG-TUe I got a lot of stderr `'Individual' object has no attribute 'arity'` when testing the codes in the last comment.  I think it is because the connection between operator (Primitive) and its parameters (Terminal) is unlinked due to unique name instead of an unique class. Maybe modifying `from_string` function is better.,got lot object attribute testing last comment think connection operator primitive terminal unlinked due unique name instead unique class maybe function better,issue,negative,positive,positive,positive,positive,positive
286819265,"Hmm, I am not sure what happened. Below is my guesses:

* As you thought, the default scoring mechanism is different. It is called balanced accuracy which computes each class' accuracy on a per-class basis using a one-vs-rest encoding, then computes an unweighted average of the class accuracies. The default scoring function in `LinearSVC` is mean accuracy of self.predict(X) wrt. y. So if a few classes high classification error, it may cause this score very low. You may try add `scoring=""accuracy""` in TPOTClassifier for using the same scoring function as `LinearSVC` ;

* Maybe `LinearSVC` was not randomly generated in the founder population (generation 0) and not randomly generated by mutation in later generations. The default population size in TPOT is 100 and default mutation rate is 0.9. Maybe you could try to increase population size (with `population_size` parameter), like 500 or change a random seed with `random_state` parameter. (It is noted that current version of TPOT may not reproduce results with same random seed. This issue will be fixed in next version)

Please attach more details about your data (how many samples and features or raw data if available) and running environment. ",sure thought default scoring mechanism different balanced accuracy class accuracy basis unweighted average class default scoring function mean accuracy class high classification error may cause score low may try add accuracy scoring function maybe randomly founder population generation randomly mutation later default population size default mutation rate maybe could try increase population size parameter like change random seed parameter noted current version may reproduce random seed issue fixed next version please attach data many raw data available running environment,issue,positive,negative,neutral,neutral,negative,negative
286359455,"I agree, it looks good. I added this into #367. Thanks!",agree good added thanks,issue,positive,positive,positive,positive,positive,positive
286205018,"Thank you for this good idea. I think, for adding `toolbox.clone` , we could edit `_pre_test` in your PR #367 a little with `isinstance(ind, creator.Individual)` as below:

```
from deap import creator

def check_pipeline(self, *args, **kwargs):
    bad_pipeline = True
    num_test = 0 # number of tests
    while bad_pipeline and num_test < 10: # a pool for workable pipeline
--->        args = [self._toolbox.clone(arg) if isinstance(arg, creator.Individual) else arg for arg in args]
        try:
            with warnings.catch_warnings():
                warnings.simplefilter('ignore')
                expr = func(self, *args, **kwargs)


```",thank good idea think could edit little import creator self true number pool workable pipeline else try self,issue,positive,positive,positive,positive,positive,positive
285998248,"@weixuanfu2016

>This DeprecationWarning should show up due to from sklearn.cross_validation import train_test_split instead of from sklearn.model_selection import train_test_split. 

Thank you. Your suggestion helped. No more warning. I have the following version.

* scikit-learn==0.18.1
* tpot-0.6.8 
* xgboost-0.6a2  ",show due import instead import thank suggestion warning following version,issue,negative,negative,neutral,neutral,negative,negative
285484779,"PR #365 is made for this issue. It will rerun mutation and crossover 50 times at most to force them to generate a different child from its parents. But I still need keep `if ind_str != str(ind)` because:

1. as you mentioned, a classifier with no parameters cannot mutate.
2. For two parents which both are single-operator ""pipeline"", crossover does not work for generate a new individual. ",made issue rerun mutation crossover time force generate different child still need keep classifier mutate two pipeline crossover work generate new individual,issue,negative,positive,neutral,neutral,positive,positive
285470264,It was a [infinitely loop](https://github.com/weixuanfu2016/tpot/blob/point_mut/tpot/base.py#L713-714) in one of a old PR. But I removed this one in new dev batch due to this issue. I will submit another PR later this afternoon to solve this issue. Thank @mficek for catching it.,infinitely loop one old removed one new dev batch due issue submit another later afternoon solve issue thank catching,issue,positive,positive,positive,positive,positive,positive
285460998,"@weixuanfu2016 can confirm, but I believe we've put in checks to make sure that a mutation or crossover actually changes the individual. It retries something like 100x (instead of infinitely) to avoid the infinite loop situation you describe.",confirm believe put make sure mutation crossover actually individual something like instead infinitely avoid infinite loop situation describe,issue,negative,positive,positive,positive,positive,positive
285374275,"Yes, sorry, the string format of the TPOT solutions is a bit confusing because of the internal representation that our optimization tool uses.",yes sorry string format bit internal representation optimization tool,issue,positive,negative,negative,negative,negative,negative
285288777,"Thanks for explanation, guys, I missed that. I understood it might be useful to have a new feature as a result of some classifier, but I didn't see from pipeline printout that the outer classifier consumes [X,new_feature] and not only [new_feature].",thanks explanation understood might useful new feature result classifier see pipeline outer classifier,issue,positive,positive,positive,positive,positive,positive
285198833,"Hi rhiever,
Thanks for the response. Yes data is imbalanced. For external test I used accuracy_score and classification_report from scikit. I am reading the posts from #6747 and I can see a lot of information from the ticket. I will post more here if I have further questions. Thanks again.",hi thanks response yes data external test used reading see lot information ticket post thanks,issue,positive,positive,positive,positive,positive,positive
285175202,"Great to see a Python version of SMAC, @mfeurer!",great see python version,issue,positive,positive,positive,positive,positive,positive
285174374,"When engineering TPOT, we followed scikit-learn's model and assumed that the data provided to TPOT will be well-formed. Otherwise we would have to add many lines of code checking for the many ways that users can provide ill-formed data to TPOT. Instead, we chose to clearly document the data format that TPOT expects and throw a generic error when TPOT runs into an optimization error (which can result from many different problems, not just data formatting issues).

If the data format expected from TPOT isn't clear from our documentation, please let us know where we can improve the documentation.",engineering model assumed data provided otherwise would add many code many way provide data instead chose clearly document data format throw generic error optimization error result many different data data format clear documentation please let u know improve documentation,issue,positive,positive,positive,positive,positive,positive
285172879,"Yeah, with 500k rows it's likely crashing because your computer ran out of memory. Perhaps you can try running TPOT on a subset of the dataset.",yeah likely computer ran memory perhaps try running subset,issue,negative,neutral,neutral,neutral,neutral,neutral
285172708,"Yes, what @weixuanfu2016 said. The pipeline `ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, True, 'entropy', 0.10000000000000001, 13, 6), True, 'gini', 0.75, 17, 4)` does the following:

1) Fit all of the original features using an ExtraTreesClassifier
2) Take the predictions from that ExtraTreesClassifier and create a new feature using those predictions
3) Pass the original features plus the new ""predicted feature"" to the 2nd ExtraTreesClassifier and use its predictions as the final predictions of the pipeline

This process is called stacking classifiers, which is a fairly common tactic in machine learning.",yes said pipeline true true following fit original take create new feature pas original plus new feature use final pipeline process fairly common tactic machine learning,issue,positive,positive,positive,positive,positive,positive
285172372,You may need more RAM for running TPOT on this huge dataset. How many features in total?,may need ram running huge many total,issue,negative,positive,positive,positive,positive,positive
285172015,"You are right about the `op.root`. 

The expression is pipeline in TPOT which [chain estimators (including classifier/regressors and preprocessors)](http://scikit-learn.org/stable/modules/pipeline.html#pipeline). For example, in the pipeline `ExtraTreesClassifier(ExtraTreesClassifier(input_matrix, True, 'entropy', 0.10000000000000001, 13, 6), True, 'gini', 0.75, 17, 4)`, the ExtraTreesClassifier in second step would use the result from the first one to fit model. You may found similar method in Grid Search, like [this](http://www.codiply.com/blog/hyperparameter-grid-search-across-multiple-models-in-scikit-learn/).",right expression pipeline chain example pipeline true true second step would use result first one fit model may found similar method grid search like,issue,positive,positive,positive,positive,positive,positive
285171953,"Sorry for the slow reply on this issue -- I haven't been getting email notifications from GitHub.

One source of the discrepancy could be that different scoring metrics are being used. What metric did you use for your external test, and are you using the default scoring metric in TPOT? The default scoring metric in TPOT is balanced accuracy, which is a version of standard accuracy that accounts for class imbalance in your data. Is your data imbalanced?",sorry slow reply issue getting one source discrepancy could different scoring metric used metric use external test default scoring metric default scoring metric balanced accuracy version standard accuracy class imbalance data data,issue,negative,negative,negative,negative,negative,negative
285034691,"Hi Naoko, Were you able to figure out the issue last time? Thank you. ",hi able figure issue last time thank,issue,negative,positive,positive,positive,positive,positive
284828452,"Ok, I dived into the code and understood, that the preprocessors and classifiers are distinguished by what are they a subclass of. So far so good. 

My understanding is, that classifiers and regressors should appear ONLY in the root of the evaluation tree. I don't understand much, why we should have xgboost(xgboost(input_matrix, args), args) for example, it does not makes much sense to me. 

Following code from `def _setup_pset(self)` puzzles me: may I please ask why do we want rooted opperators to appear somewhere else in the tree?
```
        # Add all operators to the primitive set
        for op in self.operators:

            if op.root:
                # We need to add rooted primitives twice so that they can
                # return both an Output_DF (and thus be the root of the tree),
                # and return a np.ndarray so they can exist elsewhere in the tree.
                p_types = (op.parameter_types()[0], Output_DF)
                self._pset.addPrimitive(op, *p_types)

            self._pset.addPrimitive(op, *op.parameter_types())
```",code understood distinguished subclass far good understanding appear root evaluation tree understand much example much sense following code self may please ask want rooted appear somewhere else tree add primitive set need add rooted twice return thus root tree return exist elsewhere tree,issue,positive,positive,positive,positive,positive,positive
284477161,"Hmm, it is weird.

This DeprecationWarning should show up due to `from sklearn.cross_validation import train_test_split` instead of `from sklearn.model_selection import train_test_split`. Could you please let me know the version of scikit-learn, tpot and xgboost in your environment?",weird show due import instead import could please let know version environment,issue,negative,negative,negative,negative,negative,negative
284445406,"Hello, 
Thanks for the response but i have already updated verison of scikit-learn,

Requirement already up-to-date: scikit-learn in \appdata\local\continuum\anaconda3\lib\site-packages",hello thanks response already requirement already,issue,negative,positive,positive,positive,positive,positive
284442507,"I think updating your scikit-learn should solve the issue. Try the command below:

`pip install scikit-learn --upgrade`",think solve issue try command pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
284441436,"hi All,

I am facing issue while loading ""from sklearn.model_selection import train_test_split""

**Error:** 
sklearn\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  ""This module will be removed in 0.20."", DeprecationWarning)

Please let me know how to resolve it 
Python version: 3.5 Anaconda
",hi facing issue loading import error module version favor module class also note interface new different module module removed module removed please let know resolve python version anaconda,issue,positive,positive,neutral,neutral,positive,positive
284275464,"Just a follow up to that - could it be made more explicit that strings shouldn't passed to TPOT? i.e., if you pass strings to the fit or score function, an error message comes up? (Or in a dream world, an automated LabelEncoder runs?)

I successfully ran the tpot.fit command where y_train was a list of strings (class labels). However, it was only when I tried tpot.score did it break - it can't score with string classes. 

Side note: re-running the .fit after doing using LabelEncoder, I get better CV scores? So despite the fact TPOT runs with strings, it clearly doesn't work properly but there is no obvious message to the user about this. ",follow could made explicit pas fit score function error message come dream world successfully ran command list class however tried break ca score string class side note get better despite fact clearly work properly obvious message user,issue,positive,positive,positive,positive,positive,positive
284196340,Thank you for catching this bug. PR was just posted.,thank catching bug posted,issue,negative,positive,positive,positive,positive,positive
284111809,This happened to me as well. I tried 4 times running the tool but found that the generated pipeline is off from the final generation and score. I have a sparse data as well and doing text classification. Is there a way to troubleshoot this? Thanks.,well tried time running tool found pipeline final generation score sparse data well text classification way thanks,issue,positive,positive,neutral,neutral,positive,positive
282728809,You can setup a customized estimator dictionary in the current development branch in TPOT. Check the `operator_dict` parameter (`-operator` for command line) in this [manual](https://github.com/rhiever/tpot/blob/development/docs_sources/using.md) under dev ,setup estimator dictionary current development branch check parameter command line manual dev,issue,negative,neutral,neutral,neutral,neutral,neutral
282542234,"I looked more into this, and from what I can tell, we can just use the [from_string](https://github.com/DEAP/deap/blob/232ed17142da1c8bb6a39179018f8278b122aada/deap/gp.py#L104) method, given that we take care that our terminals have unique names. This can be done rather easily (I think), rather than having the terminals be named by a stringified version of their value, as is default, we can give them an explicit unique name by combining classifier, parameter name and value in the string (code is modification of [this](https://github.com/rhiever/tpot/blob/development/tpot/base.py#L303-L306)):
```
        # Terminals
        for _type in self.arguments:
            for val in _type.values:
                terminal_name = _type.__name__ + ""="" + str(val)
                self._pset.addTerminal(val, _type, name=terminal_name)
```

From what I tested so far, this makes sure that an individual can be reconstructed from its string. 
One caveat however, this still only works when all terminals are specified for each primitive, and those values exist. So I think I could modify the default `from_string` function of the PrimitiveTree to also allow for new values to be specified in the primitive tree and/or missing terminals. For this, I think we would indeed introduce a ""missing""/""default"" terminal for each parameter.",tell use method given take care unique done rather easily think rather version value default give explicit unique name combining classifier parameter name value string code modification tested far sure individual reconstructed string one caveat however still work primitive exist think could modify default function also allow new primitive tree missing think would indeed introduce missing default terminal parameter,issue,positive,positive,positive,positive,positive,positive
282390552,"Thank you. Yes, that makes sense, leaving it out is not the way to go, we have to have explicitly state missing (default) values.

One remark; the method currently does not take a stringified individual, but more something like ""LinearSVC(input_matrix, C=0.9, penalty=l1, dual=True)"" (a stringified individual looks differently). I will do a PR of this soon. I actually might be able to work out how to properly stringify an individual as well, so I want to give that a go first (that would be much nicer to work with). ",thank yes sense leaving way go explicitly state missing default one remark method currently take individual something like individual differently soon actually might able work properly individual well want give go first would much work,issue,positive,positive,neutral,neutral,positive,positive
282354538,"Thank you for the feedback. The `expr_to_tree` do not allow missing terminal because some terminal values may be shared among different terminals inside a operator. For example, `min_samples_split` and `min_samples_leaf` in [RandomForestClassifier](https://github.com/rhiever/tpot/blob/development/tpot/config_classifier.py#L66-L67) share terminal value from 2 to 20. So it is hard to tell which one is missing in the individual. I think maybe put something like `""""` or `""MISSING""` into the ordered individual list to specify which terminal is missing is nicer.

BTW, could you please make a PR for the nice `from_string` function (may be put into `export_utils.py`). I think it is very useful for TPOT. ",thank feedback allow missing terminal terminal may among different inside operator example share terminal value hard tell one missing individual think maybe put something like missing ordered individual list specify terminal missing could please make nice function may put think useful,issue,positive,negative,neutral,neutral,negative,negative
282347028,"Okay, so I have been experimenting for a bit, and I seem to be able to add my own terminals for missing values in the function itself (as an alternative to first declaring all needed terminals). Declaring which terminals are valid beforehand in a dictionary is good, but when you want more sophisticated search techniques, such as Bayesian Optimization, you want to be able to define parameter values on the fly.
I currently made this in a little bit of a hacky manner, but it works for now.

This left me still with the lack of possible default values for terminals. However, I noticed that if I do not provide a terminal for the operator, then it doesn't tend to mind, as long as I give a valid tree to `generate_pipeline_code`, it seemed to use scikit-learn defaults if no terminal is provided.

However, the [expr_to_tree](https://github.com/weixuanfu2016/tpot/blob/config_dict_rebase_dev/tpot/export_utils.py#L76) function currently doesn't really allow for missing terminals. I added the following lines of code to the bottom of this function, which will allow the topmost primitive to not be provided with its arity amount of terminals:
```
    if len(stack) != 0:
        prim, args = stack[0][0], stack[0][1]
        tree = prim_to_list(prim, args)

    return tree
```

This means that inner operators will not be able to have less terminals than their arity, but the main one does. For example, it will now make a proper tree out of the individual `['LinearSVC', 'ARG0', '0.9', 'False', 'squared_hinge', 'l1']`, eventhough primitive 'LinearSVC' has arity 6, and only 5 terminals are provided. Do you think it would be a good idea to make some kind of NotCare terminal, where it will be replaced with the default of scikit-learn values for the respective parameter when generating the actual code? This will allow you to explicitly not specify terminals, which is nicer, and also allows for inner operators to use default values.",bit seem able add missing function alternative first valid beforehand dictionary good want sophisticated search optimization want able define parameter fly currently made little bit hacky manner work left still lack possible default however provide terminal operator tend mind long give valid tree use terminal provided however function currently really allow missing added following code bottom function allow topmost primitive provided amount stack prim stack stack tree prim return tree inner able le main one example make proper tree individual primitive provided think would good idea make kind terminal default respective parameter generating actual code allow explicitly specify also inner use default,issue,positive,positive,positive,positive,positive,positive
282244178,"Yes, I tried @weixuanfu2016's version yesterday, it looks great! And now I see it's even in the development branch, sweet.",yes tried version yesterday great see even development branch sweet,issue,positive,positive,positive,positive,positive,positive
282232284,You could also try our latest implementation of [SMAC](https://github.com/automl/SMAC3) in python. Could simplify the setup.,could also try latest implementation python could simplify setup,issue,negative,positive,positive,positive,positive,positive
282196613,"Sure, I will be happy to share the results....Here is what i have planned. 

Here the optimizers that i will plan:
1. Hyperopt (http://github.com/hyperopt/hyperopt)
2. SMAC (http://www.cs.ubc.ca/labs/beta/Projects/SMAC/)
3. Spearmint (https://github.com/HIPS/Spearmint)
4. Osprey (https://github.com/msmbuilder/osprey)
5. MOE (https://github.com/Yelp/MOE)
6. TPOT (https://github.com/rhiever/tpot)
7. Optunity http://optunity.readthedocs.io/en/latest/)

If there are any other particular ones, do let me know. Currently I am building up some wrapper code, so that i dont have to keep configuring each of them for specific dataset. I will share the code or a jupyter notebook, so all can collaborate. 
I will keep posting my progress here. I think there is real need to understand the performances of these algorithms. Thanks for a wonderful question @TheodoreGalanos . Keep you guys posted. ",sure happy share plan spearmint osprey particular let know currently building wrapper code dont keep specific share code notebook collaborate keep posting progress think real need understand thanks wonderful question keep posted,issue,positive,positive,positive,positive,positive,positive
282188238,"Hi everyone,

First of all I am very glad that this conversation got interest from more knowledgable people than me. I haven't been able to follow up this in practice but I have been keeping an eye on the conversation.

@setuc I will just echo what the others have said, I would also love to see your results or even better the process of comparison. I think that can be extremely educational for both approaches, as well as their comparison.

Kind regards,
Theodore.",hi everyone first glad conversation got interest people able follow practice keeping eye conversation echo said would also love see even better process comparison think extremely educational well comparison kind,issue,positive,positive,positive,positive,positive,positive
282129896,"@setuc, I think this issue would be a good place to record your findings for folks following it here. If your analysis is changing quite a bit, maybe share a link to a Jupyter Notebook?",think issue would good place record following analysis quite bit maybe share link notebook,issue,positive,positive,positive,positive,positive,positive
282129671,"@leonfrench, looks like it's a known bug in scikit-learn: https://github.com/scikit-learn/scikit-learn/issues/7646

It's something they will have to fix on their end. We directly use their `cross_val_score` interface.",like known bug something fix end directly use interface,issue,negative,positive,neutral,neutral,positive,positive
282128818,"Check the [development branch](https://github.com/rhiever/tpot/tree/development) of TPOT. @weixuanfu2016 implemented multiprocessing for TPOT there, although I'll note that it's still not fully validated.",check development branch although note still fully,issue,negative,neutral,neutral,neutral,neutral,neutral
282127058,This should be fixed in the upcoming release (also see #350).,fixed upcoming release also see,issue,negative,positive,neutral,neutral,positive,positive
282080499,"Thank you for taking the time to answer those questions, it's very helpful! I will be working on it again tomorrow. I'll post back if I have any other remarks/questions :)",thank taking time answer helpful working tomorrow post back,issue,positive,neutral,neutral,neutral,neutral,neutral
282039474,"I am happy to answer all these good ideas in your comments. 
I found this `from_string` issue before and posted on one of our closed PR #343. I thought it is caused by a bug in [mapping](https://github.com/DEAP/deap/blob/232ed17142da1c8bb6a39179018f8278b122aada/deap/gp.py#L124) in `deap.gp`. 

_Do you think this is generally the correct approach, or can you think of a better way?_

I quickly checked the function you posed herein and thought it is a right way for building deap individual from string. Maybe you could make a PR for this. I feel it is very useful for unit tests or many future functions in TPOT

_Is any terminal declared as default? Eg. if I do not have information about the LinearSVC__tol setting, can I revert to a default terminal?_ 

Right now, we don't have default value for most terminals. But it is easy to set a customized operator dictionary. For example, The `PolynomialFeatures ` in default operator dictionary has three [default terminal](https://github.com/rhiever/tpot/blob/development/tpot/config_classifier.py#L143-L145 ). Then we can use `operator_dict` in TPOT (`-operator` for command line, input dictionary file). Please check this [manual](https://github.com/rhiever/tpot/blob/development/docs_sources/using.md) under dev for more details,

_And if I wish to use a value that is not declared in any terminal, should I just create one in a similar fashion to it being done in `TPOTBase._setup_pset`?_

You can also use a customized operator dictionary to add value that is not listed in default operator dictionary. These terminals are flexible for taking mixed type. For example: `max_features` in  [RandomForestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), you can set as `'max_features': list(np.arange(0, 1.01, 0.05))+['auto', None]` in the customized operator dictionary . Just make sure it is a 1-dimensional list. 
",happy answer good found issue posted one closed thought bug think generally correct approach think better way quickly checked function herein thought right way building individual string maybe could make feel useful unit many future terminal declared default information setting revert default terminal right default value easy set operator dictionary example default operator dictionary three default terminal use command line input dictionary file please check manual dev wish use value declared terminal create one similar fashion done also use operator dictionary add value listed default operator dictionary flexible taking mixed type example set list none operator dictionary make sure list,issue,positive,positive,positive,positive,positive,positive
282034918,@setuc hope you collaborate here so everyone can observe ;),hope collaborate everyone observe,issue,negative,neutral,neutral,neutral,neutral,neutral
282013326,"@weixuanfu2016,  I had a question for you; since you reworked some of how the operators work. I was unsure of how to contact you directly, so I am asking it here. I hope that's ok.
I am looking to create an individual given a specific pipeline. For now, the pipeline only consists of an algorithm with set parameters. Later, I will probably want to also specify more operators.
I tried to construct pipelines with their `from_string` method, but this doesn't work so well since it does not know how to interpret the terminals properly (it sees a float value which is used for multiple terminals, eg LinearSVC__C and also LogisticRegression__C - and confuses which to use).

Currently, I set out to create such a method manually, but I figured I might be missing some useful tricks already in TPOT that I just overlooked. I find myself using string parsing a lot.

The goal currently is provided the desired pipeline (right now provided as a row from a pandas dataframe, specifying in string the classifier and parameters), to create the respective individual.
```
def build_gp(row):
    expr = []
    classifiers = tpot_.pset.primitives[gp_types.Output_DF]
    candidates = [clf for clf in classifiers if clf.name == row.classifier]
    if len(candidates) == 0 or len(candidates)>1:
        print(""Should be exactly one candidate, found {}"".format(len(candidates)))
        raise
    clf_prim = candidates[0]
    expr.append(clf_prim)
    parameters = { params.split('=')[0]: params.split('=')[1] for params in row.parameters.split(',')}
    for arg in clf_prim.args:
        terms = tpot._pset.terminals[arg]
        if len(terms)==1:
            expr.append(terms[0])
        else:
            param_name = arg.__name__.split('__')[1]
            if param_name in parameters:
                param_value = parameters[param_name]
                if param_value in map(lambda t:t.name, terms):
                    term = terms[list(map(lambda t:t.name, terms)).index(param_value)]
                    expr.append(term)
                else:
                    print(""Could not find param value for {}"".format(arg.__name__))
            else:
                print(""Could not find listing for {}"".format(param_name))
            
        
    return creator.Individual(expr)
    
ind = build_gp(benchmark_data.loc()[1015848])
```
Do you think this is generally the correct approach, or can you think of a better way?
Is any terminal declared as default? Eg. if I do not have information about the LinearSVC__tol setting, can I revert to a default terminal? And if I wish to use a value that is not declared in any terminal, should I just create one in a similar fashion to it being done in `TPOTBase._setup_pset`?

I am sorry if this is a lot to ask, then please feel free to just say so, and I'll keep hacking stuff together on my own :)",question since reworked work unsure contact directly hope looking create individual given specific pipeline pipeline algorithm set later probably want also specify tried construct method work well since know interpret properly float value used multiple also use currently set create method manually figured might missing useful already find string lot goal currently provided desired pipeline right provided row string classifier create respective individual row print exactly one candidate found raise else map lambda term list map lambda term else print could find param value else print could find listing return think generally correct approach think better way terminal declared default information setting revert default terminal wish use value declared terminal create one similar fashion done sorry lot ask please feel free say keep hacking stuff together,issue,positive,positive,neutral,neutral,positive,positive
281990181,"@mficek The reason was pretty straightforward: I ran out of free time to run all the experiments that would empirically show one way or another whether this particular strategy of parallelization yielded any benefit :) With spring break coming up it's possible I may be able to come back to this, especially if I know someone is actively interested.",reason pretty straightforward ran free time run would show one way another whether particular strategy parallelization benefit spring break coming possible may able come back especially know someone actively interested,issue,positive,positive,positive,positive,positive,positive
281949252,"Hi guys, is there any progress on this PR? I'm playing now with tpot and paralellizing it on DEAP level would be great. May I ask why what was the reason that prevented you from merging magsol's PR into the dev branch? If I pick this up, some background knowledge could help a lot. Thanks!",hi progress level would great may ask reason dev branch pick background knowledge could help lot thanks,issue,positive,positive,positive,positive,positive,positive
281236970,I have started work on it....will discuss the findings some place...is there a better place to collaborate than just github? email perhaps,work discus place better place collaborate perhaps,issue,negative,positive,positive,positive,positive,positive
280969612,"Hi @dhananjayksharma, I am using the 

`from sknn.mlp import Regressor, Layer, Classifier`

for creating neural networks. I am pretty new on this stuff. So I wonder that deprecation going to effect to simple Neural Networks too? 

```
nn = Classifier(
              layers=[
              Layer(""Sigmoid"", units=1),
              Layer(""Softmax"")
              ],
              learning_rate=0.02,
              n_iter=500)
nn.fit(train_X, train_y)
    
results = nn.predict(test_X)
return results```",hi import regressor layer classifier neural pretty new stuff wonder deprecation going effect simple neural classifier layer sigmoid layer return,issue,negative,positive,positive,positive,positive,positive
280807121,"Hi, I'm having similar trouble - I'm trying to get TPOT to use GroupKFold (so crossvalidation splits don't break up data from single individuals). However, I can't set the groups variable for the fold object, so it seems to be using the same folds for every generation if I send it the split: 

```
gkf = GroupKFold(n_splits=4) #4 individuals left
groupFoldSplit = gkf.split(X_train, y_train, groups=Groups_train)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, num_cv_folds=groupFoldSplit)
tpot.fit(X_train, y_train)
```
I'm guessing I should send gkf and not groupFoldSplit, but it wouldn't know about the grouping. Any ideas? ",hi similar trouble trying get use break data single however ca set variable fold object every generation send split left guessing send would know grouping,issue,negative,negative,neutral,neutral,negative,negative
280679055,Thank you for the feedback! I think we may need add a exception for `xgboost` if no xgboost is installed.,thank feedback think may need add exception,issue,negative,neutral,neutral,neutral,neutral,neutral
280672957,"Okay, so. As far as I can tell, results are consistent now! (Well, they were with my other fixes too but this is much better)
One little thing though; the default operator config includes xgboost, while in the documentation it is still listed as optional. So I would personally prefer the default operator dict not to include xgboost (or perhaps better; just give a notice for the libraries which could not be imported, and continue).
Currently, if no xgboost is installed, with all default settings, TPOT crashes upon initialization trying to import xgboost.",far tell consistent well much better one little thing though default operator documentation still listed optional would personally prefer default operator include perhaps better give notice could continue currently default upon trying import,issue,positive,positive,positive,positive,positive,positive
280672301,"This specific problem is ""fixed"" by the rework considering this part is completely revamped, so I will close the issue.",specific problem fixed rework considering part completely close issue,issue,negative,positive,neutral,neutral,positive,positive
280445575,"Great! I would give it more time (i.e., larger population > 100 and more generations > 100) to see what it finds.",great would give time population see,issue,positive,positive,positive,positive,positive,positive
280340792,Great! I'll give it a go soon and post back with results.,great give go soon post back,issue,positive,positive,positive,positive,positive,positive
280338943,@PG-TUe Thank @rhiever for merging it with dev branch last night. Please let us know if the reproducibility issue still exist in this branch.,thank dev branch last night please let u know reproducibility issue still exist branch,issue,positive,neutral,neutral,neutral,neutral,neutral
280155989,"Any idea on when it will be merged with the dev branch? I am looking to extend TPOT, and while I can use my own version now (with a few alternations to make results reproducible), it's nicer to go with the solution that would be implemented later. Will save a headache in the end, hopefully.",idea dev branch looking extend use version make reproducible go solution would later save headache end hopefully,issue,positive,neutral,neutral,neutral,neutral,neutral
280154738,"Okay, at first glance seems a bit more robust, considering there are some sanity checks going on in the functions :) 
(which I suppose is especially necessary given that you are going to allow for users to specify lists of operators)",first glance bit robust considering sanity going suppose especially necessary given going allow specify,issue,positive,positive,positive,positive,positive,positive
280148488,"Nice find. We found the limitation for using operators folder thus we are working on the new version of TPOT and will discard the whole folder and use operator dictionary instead. About this function, we will replace it with [this one](https://github.com/weixuanfu2016/tpot/blob/config_dict_rebase_dev/tpot/export_utils.py#L23-46). It will check if there are duplicates  in operator dictionary.",nice find found limitation folder thus working new version discard whole folder use operator dictionary instead function replace one check operator dictionary,issue,negative,positive,positive,positive,positive,positive
280056057,"Further randomness observed is probably explained by issue #353.
Whenever the evaluation of an individual does not complete because of this bug, there is also one less call to np.random, resulting in the random number sequence being different- which means changes in mutation and mating.",randomness probably issue whenever evaluation individual complete bug also one le call resulting random number sequence mutation mating,issue,negative,negative,negative,negative,negative,negative
280052717,"Yes, we should update this information in `setup.py`. Thank you for filing the issue!",yes update information thank filing issue,issue,positive,neutral,neutral,neutral,neutral,neutral
279683077,"I have updated the github repository [https://github.com/AnnamalaiNagappan/tpot_share/blob/master/tpot_share.ipynb](https://github.com/AnnamalaiNagappan/tpot_share/blob/master/tpot_share.ipynb)

Find the result i got 

![predict_output](https://cloud.githubusercontent.com/assets/1428796/22927389/fa651b00-f2d6-11e6-87f3-9822d2cc6ad5.PNG)
",repository find result got,issue,negative,neutral,neutral,neutral,neutral,neutral
279653633,"It seems that it still wants to also commit the doc-changes. I am not sure how to fix this properly (still learning git). I can also commit the changes on a new branch which is based off of master, if this is a problem.",still also commit sure fix properly still learning git also commit new branch based master problem,issue,negative,positive,positive,positive,positive,positive
279646834,"I have actually found that after running a few hundred more experiments, it is still not actually always consistent, though it is generally much more consistent than it was before. I am also not sure if it is now always inconsistent, or only under certain environments. That considered, it still seems like an improvement regardless (I'd classify having unordered sets when the randomness should be determined by the choice of the index as a bug), so I will rebase the PR.",actually found running hundred still actually always consistent though generally much consistent also sure always inconsistent certain considered still like improvement regardless unordered randomness determined choice index bug rebase,issue,positive,positive,positive,positive,positive,positive
279493261,"Hmmm, that's not a very helpful error message from Python...

Was it perhaps using too much memory? How large is the dataset?",helpful error message python perhaps much memory large,issue,negative,positive,positive,positive,positive,positive
279488227,"Great find. We have some upcoming changes on the dev branch that will fix this issue, but I'd like to get this out as a hotfix on the current `master` branch release. Can you please rebase this PR and send it to the `master` branch so we can send out a hotfix for TPOT 0.6?",great find upcoming dev branch fix issue like get current master branch release please rebase send master branch send,issue,positive,positive,positive,positive,positive,positive
278683794,"Python 3.5.2
tpot 0.6.8
I don't get an error in the IDE(jupyter notebook) but a window pop up message which says python has stopped working.
",python get error ide notebook window pop message python stopped working,issue,negative,neutral,neutral,neutral,neutral,neutral
278674687,Great! Let us know how it works.,great let u know work,issue,positive,positive,positive,positive,positive,positive
278585666,Thank you very much. Now I am able to execute the code.,thank much able execute code,issue,negative,positive,positive,positive,positive,positive
278133125,"@rhiever @AnnamalaiNagappan Based on the Jupyther notebook, I think the issue is caused by `features = df['date_float'].values`, which export a 1-D array so that X-train and X_test are a 1-D array. But X for `TPOT.fit` need to be a 2-D matrix. Please change `features = df['date_float'].values` to:
```
features = df['date_float'].values.reshape((df.shape[0],1))
```",based notebook think issue export array array need matrix please change,issue,negative,neutral,neutral,neutral,neutral,neutral
278123009,"Try taking out the `pd.np.array` calls in this line:

```
X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features), pd.np.array(outcome).ra...
```

I suspect it's a data formatting issue still.",try taking line outcome suspect data issue still,issue,negative,neutral,neutral,neutral,neutral,neutral
278122479,What error message does it give you? What version of Python? And version of TPOT?,error message give version python version,issue,negative,neutral,neutral,neutral,neutral,neutral
277417932,"Have changed the line, no use

![code_block](https://cloud.githubusercontent.com/assets/1428796/22615662/a9384faa-eabf-11e6-9904-ec39fe19e581.PNG)


I have added the data file and python notebook to this link 
[https://github.com/AnnamalaiNagappan/tpot_share/blob/master/tpot_share.ipynb](https://github.com/AnnamalaiNagappan/tpot_share/blob/master/tpot_share.ipynb)
",line use added data file python notebook link,issue,negative,neutral,neutral,neutral,neutral,neutral
277373354,"Please try again after changing this line 
`X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features).ravel(), pd.np.array(outcome).ravel(),train_size=0.75, test_size=0.25)` 
to
 `X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features), pd.np.array(outcome).ravel(),train_size=0.75, test_size=0.25)`

Since X should be 2-D array but y is 1-D one.",please try line outcome outcome since array one,issue,negative,neutral,neutral,neutral,neutral,neutral
277273961,"I copy pasted the code, still getting the same error. and printed the type of data

```
from tpot import TPOTRegressor
from sklearn.model_selection import train_test_split
my_tpot = TPOTRegressor()
features = df['date_float'].values
outcome = df['metric'].values
X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features).ravel(), pd.np.array(outcome).ravel(),train_size=0.75, test_size=0.25)
print type(X_train)
tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
```

![error_format](https://cloud.githubusercontent.com/assets/1428796/22597435/96f2cf9c-ea55-11e6-84a3-2077ed41fa39.PNG)



",copy pasted code still getting error printed type data import import outcome outcome print type,issue,negative,neutral,neutral,neutral,neutral,neutral
277269547,"You have to convert the DataFrame to a numpy array:

```
from tpot import TPOTRegressor
from sklearn.model_selection import train_test_split
my_tpot = TPOTRegressor()
features = df['date_float'].values
outcome = df['metric'].values
X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features), pd.np.array(outcome).ravel(),train_size=0.75, test_size=0.25)
tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
```

then it should work.",convert array import import outcome outcome work,issue,negative,neutral,neutral,neutral,neutral,neutral
277224026,"Thank you for the reply.

This is my pandas dataframe 

![normal_df](https://cloud.githubusercontent.com/assets/1428796/22589176/0043d2ce-ea2f-11e6-9408-b08c9fbed6fe.PNG)

I converted the date column to float using the **[year] * 365 + [month] * 31 + [day]** and assigned it to **date_float** column.

![convert_df](https://cloud.githubusercontent.com/assets/1428796/22589235/4f4c6c64-ea2f-11e6-8ec9-e5513b236d42.PNG)

then used the following code 

```
from tpot import TPOTRegressor
from sklearn.model_selection import train_test_split
my_tpot = TPOTRegressor()
features = df['date_float']
outcome = df['metric']
X_train, X_test, y_train, y_test = train_test_split(pd.np.array(features), pd.np.array(outcome).ravel(),train_size=0.75, test_size=0.25)
tpot = TPOTRegressor(generations=5, population_size=20, verbosity=2)
tpot.fit(X_train, y_train)
```
where am getting the following error.

![error_format](https://cloud.githubusercontent.com/assets/1428796/22589346/db8e35c2-ea2f-11e6-9ee0-c06d30b82cb9.PNG)

Is there any problem in the way I passed the data ?
",thank reply converted date column float year month day assigned column used following code import import outcome outcome getting following error problem way data,issue,negative,neutral,neutral,neutral,neutral,neutral
277104169,"Yes, this should be feasible. If you convert the date into some numeric format, you can have a `TPOTRegressor` try to find a model that predicts `metric` as a function of `date_numeric`.

For example, you can convert `date` into `date_numeric` by counting the number of days since 0 AD:

```
date_numeric = [year] * 365 + [month] * 31 + [day]
```

(or maybe something less hacky than that -- there's Python libraries for doing similar things)

Then `date_numeric` is your feature and `metric` is your outcome. Create a `TPOTRegressor` and fit:

```
from tpot import TPOTRegressor

my_tpot = TPOTRegressor()
my_tpot.fit(features, outcome)
print(my_tpot.score(features))
```

etc.

You'd want to do some sort of [time series split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html) before passing the data to TPOT, of course. Probably also want to use TimeSeriesSplit in TPOT's cv parameter, as discussed [here](https://github.com/rhiever/tpot/issues/342). Basically the same way you would approach a time series problem in scikit-learn.",yes feasible convert date format try find model metric function example convert date counting number day since ad year month day maybe something le hacky python similar feature metric outcome create fit import outcome print want sort time series split passing data course probably also want use parameter basically way would approach time series problem,issue,positive,positive,positive,positive,positive,positive
276423086,All of the [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks) data sets are a good starting point. Most of them are small and should run pretty quickly.,data good starting point small run pretty quickly,issue,positive,positive,positive,positive,positive,positive
276422533,"All data passed to TPOT needs to be in standard scikit-learn vectorized format with all values as numbers. One potential issue with the data set snippet you posted is that the class labels are strings, not numbers. I recommend using a LabelEncoder to encode them into corresponding numbers.",data need standard format one potential issue data set snippet posted class recommend encode corresponding,issue,negative,neutral,neutral,neutral,neutral,neutral
276421611,Yes. You perform the train/test split before providing the training data to TPOT. Another example of that can be seen [here](http://rhiever.github.io/tpot/examples/MNIST_Example/).,yes perform split providing training data another example seen,issue,negative,neutral,neutral,neutral,neutral,neutral
276421293,Cool -- let's go over this in-person tomorrow.,cool let go tomorrow,issue,negative,positive,positive,positive,positive,positive
276418369,"nice, i think i'm a bit confuse, the train/test is just a ""unseen"" data, it's not a train/test data to fit() or cv, right?",nice think bit confuse unseen data data fit right,issue,positive,positive,positive,positive,positive,positive
276302892,"We have done some work internally around this. Would be happy to  do this on the public datasets. 

@rhiever Do you have suggestions on which ones to run? or what are some good test sets?",done work internally around would happy public run good test,issue,positive,positive,positive,positive,positive,positive
276147866,"You should pass numpy arrays to TPOT, just as you would pass data to any scikit-learn estimator. [Here](https://github.com/rhiever/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb) is an example that reads a data set into a pandas DataFrame, cleans it, then passes the data as numpy arrays to TPOT.

I also recommend performing a train/test split, as you described, so you can validate that the model TPOT builds does indeed generalize to unseen data. TPOT uses k-fold CV internally to determine pipeline scores, so the accuracy estimates on the training data *should* be close to the validation accuracy.

Another advantage of performing a scikit-learn train/test split is that it shuffles your data, which can be quite important for some data sets.",pas would pas data estimator example data set data also recommend split validate model indeed generalize unseen data internally determine pipeline accuracy training data close validation accuracy another advantage split data quite important data,issue,positive,positive,positive,positive,positive,positive
276147009,"In the upcoming 0.7 release, we'll be including the option to fully customize what TPOT optimizes over (estimators, selectors, parameters, parameter ranges). I think that will help with this situation.",upcoming release option fully parameter think help situation,issue,negative,neutral,neutral,neutral,neutral,neutral
275865907,"hum... I was testing fit with full dataframe (+8GB) and got one error (my computer have 16GB of ram + 10GB of swap, running linux), and all python3 process halted (checked with htop)

I pressed CONTROL+C and (I don't know why the process wasn't killed) process continue (maybe just one thread was 'canceled' with CONTROL+C?), output of terminal:

TPOTRegressor(crossover_rate=0.05, disable_update_check=False, generations=5,
       max_eval_time_mins=20, max_time_mins=None, mutation_rate=0.9,
       num_cv_folds=TimeSeriesSplit(n_splits=20), population_size=20,
       random_state=None, scoring=None, verbosity=4)
Optimization Progress:   0%|                                                                                                      | 0/120 [00:00<?, ?pipeline/s]Exception in thread Thread-41:
Traceback (most recent call last):
OSError: [Errno 12] Cannot allocate memory

^CProcess ForkPoolWorker-28:
Traceback (most recent call last):
KeyboardInterrupt
Optimization Progress:   1%|▊                                                                                         | 1/120 [12:17<24:21:57, 737.12s/pipeline]


should I post this as an issue (bug)? I liked that ^C didn't killed my process, but when an ""OSError 12"" occur the proccess stop, and wait a user CONTROL+C to continue, could we include a option to ""on error 12 abort current pipeline"" (something more ""automatic"")?",hum testing fit full got one error computer ram swap running python process checked know process process continue maybe one thread output terminal optimization progress exception thread recent call last allocate memory recent call last optimization progress post issue bug process occur stop wait user continue could include option error abort current pipeline something automatic,issue,positive,positive,positive,positive,positive,positive
275851573,"hi, it's relevant... i tested an xgboost at kaggle with 10 vs 100 n_estimators and the diference was from 100 position to 10 position at kaggle rank

could some parameters be ""optional"", for example...
if we use TPOTRegressor( n_jobs = -1, n_estimators= 100)
could the TPOTRegressor accept it and when some model (xgboost for example) know this parameter just use it? i see many tree using n_jobs and n_estimators, but some linear regressors no

maybe optional parameters could be interesting... or fixed, no problem, the point is expose more parameters instead of fixed values

could we reopen this issue?
if possible (i'm new to python) one example (commit) adding a new parameter to TPOTRegressor + xgboost parameter, could help allot increasing the complexity/number of parameters that we could use with TPOT",hi relevant tested position position rank could optional example use could accept model example know parameter use see many tree linear maybe optional could interesting fixed problem point expose instead fixed could reopen issue possible new python one example commit new parameter parameter could help allot increasing could use,issue,positive,positive,positive,positive,positive,positive
275850926,"hum nice! :)
but how about fit() and predict() functions? should i pass the ""full dataframe"" or slice it as X_train/X_test/y_train/y_test? 
i'm trying the 'default example':

tpot.fit(X_train, y_train)
tpot.score(X_test, y_test)

should be nice include this in docs :) i could help (tell me how and i help without problems :) )

",hum nice fit predict pas full slice trying example nice include could help tell help without,issue,positive,positive,positive,positive,positive,positive
275767566,"Yes. Pass the `TimeSeriesSplit` object to the `num_cv_folds` parameter of TPOTRegressor. Even though the parameter is called `num_cv_folds`, it is passed directly to the internal calls to `cross_val_score` as the `cv` parameter, like this:

`cross_val_score(features, labels, ..., cv=self.num_cv_folds)`",yes pas object parameter even though parameter directly internal parameter like,issue,positive,neutral,neutral,neutral,neutral,neutral
275729316,"Try adding `pip install pathos` [here](https://github.com/rhiever/tpot/blob/master/ci/.travis_install.sh#L54). Travis is complaining about the import, presumably because your lib's not being installed.

Also [this](https://github.com/weixuanfu2016/tpot/blob/4a6211fe35163aeff15217f53bc0d91a4d5ae0a1/tpot_test_multi_process.py 
) probably shouldn't exist at the top directory level. It should probably be added to `.gitignore`",try pip install pathos travis import presumably also probably exist top directory level probably added,issue,negative,positive,positive,positive,positive,positive
272707302,"Hello @rhiever, I am using sklearn 0.18 on Ubuntu 16.04:

sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.   ""This module will be removed in 0.20."", DeprecationWarning)

",hello module version favor module class also note interface new different module module removed module removed,issue,negative,positive,neutral,neutral,positive,positive
271851750,"I think it is because the Travis build does not have the pathos module.

> On Jan 11, 2017, at 5:54 AM, fferroni <notifications@github.com> wrote:
> 
> Hi,
> Thanks! Looks good. The Travis build fails though?
> Best,
> Francesco
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub, or mute the thread.
> 
",think travis build pathos module wrote hi thanks good travis build though best reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
271838843,"Hi,
Thanks! Looks good. The Travis build fails though?
Best,
Francesco",hi thanks good travis build though best,issue,positive,positive,positive,positive,positive,positive
271670646,I tried to use both SCOOP and multiprocessing to parallelize TPOT but both did not work due to some functions (like lambda func) are not pickleable. So I used `pathos` to parallelize TPOT and it use `dill` instead of `pickle` for function serialization (check [this](http://matthewrocklin.com/blog/work/2013/12/05/Parallelism-and-Serialization) for more details) . Check this PR #338 for a demo for parallelizing TPOT.,tried use scoop parallelize work due like lambda used pathos parallelize use dill instead pickle function serialization check check,issue,negative,negative,negative,negative,negative,negative
271236158,"@rhiever Such comparison would indeed be valuable, but I don't think I'll be able to do anything in this direction before April. A potential step into this direction might be our groups effort to create a hyperparameter optimization benchmark set, into which I'm about to include auto-sklearn (see [PR](https://github.com/automl/HPOlib2/pull/17)). Then, we'd only need to run the different optimization algorithms (BO vs GP) on that problem.",comparison would indeed valuable think able anything direction potential step direction might effort create optimization set include see need run different optimization bo problem,issue,positive,positive,positive,positive,positive,positive
270706767,"I expect a large improvement of tpot's honor and hype after this :muscle: 
glad to help, im using this amazing tool myself!",expect large improvement honor muscle glad help amazing tool,issue,positive,positive,positive,positive,positive,positive
270290803,"Thanks Brian,

Appreciate it. I'll test this out. Been using Gridsearch so far within scikit learn but the search space gets so large very quickly.",thanks appreciate test far within learn search space large quickly,issue,positive,positive,positive,positive,positive,positive
270266025,These are just the defaults.. for some problems I leave this range. In particular n_estimators must sometimes be very large.,leave range particular must sometimes large,issue,negative,positive,positive,positive,positive,positive
270260881,"Hi @brianmingus 

Could I ask the parameters and ranges you are searching for XGBRegressor?",hi could ask searching,issue,negative,neutral,neutral,neutral,neutral,neutral
270186434,"Anecdotally, my custom RandomizedSearchCV pipeline search over XGBRegressor is quickly discovering much more performant models than tpot.",custom pipeline search quickly much performant,issue,negative,positive,positive,positive,positive,positive
270185081,I notice that every once in a while tpot is using all the cores. I suspect this is xgboost. I recommend getting rid of as many single-core algorithms as possible - tpot performance should improve as you can afford to search a bigger space.,notice every suspect recommend getting rid many possible performance improve afford search bigger space,issue,negative,positive,positive,positive,positive,positive
270183037,"@rhiever A good way to do this would be to focus on algorithms that have parallel implementations, such as xgboost and sklearn SGD, setting n_jobs=-1. Looking at your GECCO '16 paper, I can make the following suggestions:

- Decomposition: Instead of RandomizedPCA, use sklearn.decomposition.SparsePCA and sklearn.linear_model.SGDClassifier. The latter may seem somewhat strange, but by creating an autoencoder with SGD you implement PCA.

- Models: Set n_jobs=-1 for sklearn.ensemble.RandomForestClassifier. Do not use sklearn.ensemble.GradientBoostingClassifier, instead just use xgboost which is massively parallel. Do not use SVM - instead use SGD with n_jobs=-1.

etc. etc.

",good way would focus parallel setting looking paper make following decomposition instead use latter may seem somewhat strange implement set use instead use massively parallel use instead use,issue,negative,positive,positive,positive,positive,positive
270170976,"`pywin32` is included in anaconda installer and installed as [default](https://docs.continuum.io/anaconda/pkg-docs). But if the python is download form official website, the `pywin32` need to be installed separately. Using this module is to prevent TPOT from crashing due to KeyboardInterrupt when working with scipy, [a old issue in scipy](http://stackoverflow.com/questions/15457786/ctrl-c-crashes-python-after-importing-scipy-stats). We may need add this dependency in TPOT document for Windows supports.",included anaconda installer default python form official need separately module prevent due working old issue may need add dependency document,issue,negative,negative,neutral,neutral,negative,negative
270156107,Can you please provide the code and data that you used to reproduce this issue?,please provide code data used reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
270155751,"I think such a comparison would indeed be valuable. Another important aspect to control for in this kind of comparison is the ML operators and parameters that you optimize over using GenProg and BO. We've made several attempts to compare auto-sklearn and TPOT internally in the past, but keep running into issues where we use different operators and different parameters (likely due to historical

@mfeurer, maybe you'd be interested in doing a larger comparison? We have a large, curated repo of supervised learning data sets at [PMLB](https://github.com/EpistasisLab/penn-ml-benchmarks) and if we could align TPOT and auto-sklearn to have the same operators---even if it's a simpler subset of operators for the purposes of the experiment---that could make an interesting comparison between GenProg and BO.",think comparison would indeed valuable another important aspect control kind comparison optimize bo made several compare internally past keep running use different different likely due historical maybe interested comparison large learning data could align simpler subset experiment could make interesting comparison bo,issue,positive,positive,positive,positive,positive,positive
270153655,"We plan to add it eventually, but we have many more high-priority issues to resolve before we touch this one. We are happy for you to start working on this issue and send over a PR if you're interested. Please let us know.",plan add eventually many resolve touch one happy start working issue send interested please let u know,issue,positive,positive,positive,positive,positive,positive
270151034,"@weixuanfu2016, have you been able to reproduce this? I don't have a Windows machine to check this on, unfortunately.",able reproduce machine check unfortunately,issue,negative,neutral,neutral,neutral,neutral,neutral
269770863,"I don't that spearmint is able to handle these spaces at all. My experience stems from an experiment in which we tried to run spearmint to optimize Auto-WEKA. You could try the code from [this group](https://www.ismll.uni-hildesheim.de/projekte/dfg_hylap_en.html), which is linked in one of their latest papers. They claim that their GP is able to handle such spaces.",spearmint able handle experience experiment tried run spearmint optimize could try code group linked one latest claim able handle,issue,negative,positive,positive,positive,positive,positive
269770368,"Hi Matthias,

This is exactly why I posted here in the first place! Thank you for all the input, it is really invaluable.

My main goal in this exercise is learning and not really proving a point. I'm a beginner in the field, looking to learn and use it in another field like so many others, so I'm sure I cannot and probably should not try and redo things that more adept ppl in ML have done.

I also thank you very much for the paper you linked I will go over it and other relevant literature before I start.

One last question if I may. I've already downloaded hyperopt and SMAC. Do you think the non-commercial version of spearmint would also be able to handle these spaces as well?

Once again, thanks for the input.

Kind regards,
Theodore.
",hi exactly posted first place thank input really invaluable main goal exercise learning really proving point beginner field looking learn use another field like many sure probably try redo adept done also thank much paper linked go relevant literature start one last question may already think version spearmint would also able handle well thanks input kind,issue,positive,positive,positive,positive,positive,positive
269767260,"One of the key points you should do is to not make up new benchmarks, but instead use the setup from previously published papers about the same topic. As one of the authors of [this paper](https://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf) I'm a little bit biased on what datasets to use, but I think that using either the ones used in the paper on Auto-WEKA, or the 13 datasets presented by us are a good starting point. You would of course also need to decide on a metric.

Next, you would have to make sure that both have a similar or equal search space, e.g. they search the same classifiers, preprocessors and their hyperparameters. 

Last, you need to find an implementation of Bayesian optimization which scales to such configurations spaces; to the best of my knowledge, these are only [hyperopt](https://github.com/hyperopt/hyperopt) and [SMAC](https://github.com/automl/SMAC3/).",one key make new instead use setup previously topic one paper little bit use think either used paper u good starting point would course also need decide metric next would make sure similar equal search space search last need find implementation optimization scale best knowledge,issue,positive,positive,positive,positive,positive,positive
269091890,"No, the python distribution being used was download from the official python website. Forgot to mention that I'm running python 3.5 64 bit. All other dependencies were download from [Unofficial Windows Binaries for Python Extension Packages](http://www.lfd.uci.edu/~gohlke/pythonlibs/) if available or simply by using pip.",python distribution used official python forgot mention running python bit unofficial python extension available simply pip,issue,negative,positive,positive,positive,positive,positive
268872565,"Feature selection is automated in TPOT pipelines in that TPOT decides what
type of feature selection to use and how many features to keep. Do you mean
using TPOT to automate feature selection only?


On Thu, Dec 22, 2016 at 1:51 PM Arita <notifications@github.com> wrote:

> Sure, the point : while this is clear which model seems best fit the data
> as well as the Hyper params,
>
>
> this is not clear which feature may have of importance....
>
>
> (Suppose there are 10000 features, but only 100 which gives 95% of the
> accuracy,...other improve the accuracy at very low marginal rate...).
>
>
> Feature selection is still 'manual'.
>
>
>
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/rhiever/tpot/issues/332#issuecomment-268864980>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/ABo7t_bmqnsG77JFGFfHrQftX4Eb0uICks5rKsaKgaJpZM4LSkVo>
> .
>
",feature selection type feature selection use many keep mean feature selection wrote sure point clear model best fit data well hyper clear feature may importance suppose accuracy improve accuracy low marginal rate feature selection still reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
268864980,"Sure, the point : while this is clear which model seems best fit the data as well as the Hyper params,
this is not clear which feature may have of importance....
(Suppose there are 10000 features, but only 100 which gives 95% of the accuracy,...other improve the accuracy at very low marginal rate...).

Feature reduction is still 'manual'.
",sure point clear model best fit data well hyper clear feature may importance suppose accuracy improve accuracy low marginal rate feature reduction still,issue,positive,positive,positive,positive,positive,positive
268821829,"TPOT has feature selectors built-in, including model-based feature selectors. It will add them to the pipeline if they improve accuracy.",feature feature add pipeline improve accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
268820130,"We're pursuing a different method to customizing the operators that TPOT optimizes over, so we likely won't merge this change.",different method likely wo merge change,issue,negative,neutral,neutral,neutral,neutral,neutral
268741783,"What is the status of this merge request?
A quick question... can an operator also be something like a preprocessing step?",status merge request quick question operator also something like step,issue,negative,positive,positive,positive,positive,positive
268730423,"Aditionnally, we need to deal with a large number of features (ncol= 1000, 5000, 10000).
Usually, we pre-process the data before feeding into TPOT, so we come up with ncol= 200 to accelerate the training.

It would be be useful to have feature reduction (automatic) models, although there is no unique solutions to this difficult problem.

For Classification, one can use Feature Importance from RF as one indicator.
PCA reduce dimensionnality (but necessary the number of input feature...).

Combining several methods would be of useful interest.
This method can be 'Orthogonal' to model selection and accelerate the model selection...





",need deal large number usually data feeding come accelerate training would useful feature reduction automatic although unique difficult problem classification one use feature importance one indicator reduce necessary number input feature combining several would useful interest method model selection accelerate model selection,issue,positive,positive,neutral,neutral,positive,positive
268699907,"I read the docs of ipyparall , it is just a distribute framework as my expect. If you want to do a big task efficiently, you must split it into pieces, then send to workers(ipyparall engine) which would excute tasks, but you must schedule tasks in right way first.  Runing multi tpot instance in multi-process/parallel is simple, but it is not much effcient.

",read distribute framework expect want big task efficiently must split send engine would must schedule right way first instance simple much,issue,negative,positive,positive,positive,positive,positive
268537146,"👍 

We are currently working on a feature for TPOT to allow users to fully customize the operators and operator parameters that TPOT optimizes over. That should handle your use case, although that's an up-and-coming feature.",currently working feature allow fully operator handle use case although feature,issue,negative,neutral,neutral,neutral,neutral,neutral
268509724,"You can check ipyparall documentation, pretty straightforward

On 21 Dec 2016, at 16:48, eromoe <notifications@github.com> wrote:

@arita37 What do you mean, any example ? Simply run multi tpot-instance would waste much calculation resource .The most easy and effcient way I can imagine is :
Split all combinations of model and params tpot generated into several groups and run parallel. (But tpot not support now) .

―
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.

",check documentation pretty straightforward wrote mean example simply run would waste much calculation resource easy way imagine split model several run parallel support reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
268458180,"@arita37 What do you mean, any example ? Simply run multi tpot-instance would waste much calculation resource .The most easy and effcient way I can imagine is : 
Split all combinations of model and params  generated by tpot  into several groups and run parallel. (But tpot not support now) .
",mean example simply run would waste much calculation resource easy way imagine split model several run parallel support,issue,negative,positive,neutral,neutral,positive,positive
268429220,"Think there Ipyparalle may solve the issue (add the possibility to work on remote server):
cf here: 
https://goo.gl/kgyJMX
",think may solve issue add possibility work remote server,issue,negative,negative,neutral,neutral,negative,negative
268280603,"Yep, I suspect that is the error as well.",yep suspect error well,issue,negative,neutral,neutral,neutral,neutral,neutral
268280041,"Yep, I am looking into it. The reason is somehow the `self._optimized_pipeline` is NoneType. And both unit tests used the fit() with population_size = 1 and generations = 1. My guess is that a random pipeline failed and then was assign a `-float(inf)` score  but this error was not caught. I will fix the bug soon.



> On Dec 20, 2016, at 10:45 AM, Randy Olson <notifications@github.com> wrote:
> 
> Latest commits seem to break. Something about a NoneType not being iterable.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/pull/319#issuecomment-268276592>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AUG7Kkxohxka6Y3ZJPef8SS_OGyzneshks5rJ_gYgaJpZM4K-Rfj>.
> 

",yep looking reason somehow unit used fit guess random pipeline assign score error caught fix bug soon randy wrote latest seem break something iterable thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
268276592,Latest commits seem to break. Something about a `NoneType` not being iterable.,latest seem break something iterable,issue,negative,positive,positive,positive,positive,positive
268064909,We exposed `sample_weights` in the latest version on the `development` branch. Hope that's useful in the next release!,exposed latest version development branch hope useful next release,issue,positive,positive,positive,positive,positive,positive
268064583,Should we count this issue as resolved since @weixuanfu2016 added the PR to the `development` branch to fix the solver issue?,count issue resolved since added development branch fix solver issue,issue,negative,neutral,neutral,neutral,neutral,neutral
268064391,"No. I put them together, but I the function calls were so similar to the classifier examples that I shelved it.

> On Dec 19, 2016, at 3:03 PM, Randy Olson <notifications@github.com> wrote:
> 
> Hi @slcott <https://github.com/slcott> -- are you still working on these notebooks?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/issues/257#issuecomment-268063701>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADdkYK0w3cV0yf_oGy5Ox68PJV2w4tTWks5rJuMMgaJpZM4J0B3R>.
> 

",put together function similar classifier randy wrote hi still working reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
268064203,This issue should be resolved in the upcoming release thanks to @weixuanfu2016's PR.,issue resolved upcoming release thanks,issue,positive,positive,positive,positive,positive,positive
268062404,"We looked into using the multiprocessing capabilities of DEAP, but ran into issues with pickling lambda functions and a few other tricks we use in TPOT. Maybe @weixuanfu2016 can provide full details.

In the meantime, I've merged the PR into the `development` branch that exposes `n_jobs` for the cross-validation procedure.",ran lambda use maybe provide full development branch procedure,issue,negative,positive,positive,positive,positive,positive
268062282,"OK, make sense.


> On Dec 19, 2016, at 2:48 PM, Randy Olson <notifications@github.com> wrote:
> 
> Can you please bring this PR up to date with the latest of the development branch?
> 
> Also, maybe we should move all of the GP-related functions to deap_gp.py.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub <https://github.com/rhiever/tpot/pull/319#issuecomment-268059971>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AUG7Ko39ll8YaYiRbEBNEm4avxPv1CJYks5rJt9wgaJpZM4K-Rfj>.
> 

",make sense randy wrote please bring date latest development branch also maybe move thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
268061955,This should be fixed in the upcoming release (currently in the `development` branch).,fixed upcoming release currently development branch,issue,negative,positive,neutral,neutral,positive,positive
268061020,"@sahilshah1194, please send this branch as a PR to the `development` branch. I had to un-merge it from the `master` branch and can't re-open this PR.",please send branch development branch master branch ca,issue,negative,neutral,neutral,neutral,neutral,neutral
268059971,"Can you please bring this PR up to date with the latest of the `development` branch?

Also, maybe we should move all of the GP-related functions to `gp_deap.py`.",please bring date latest development branch also maybe move,issue,negative,positive,positive,positive,positive,positive
268048157,Much appreciated! Can you please PR this to the `development` branch instead?,much please development branch instead,issue,negative,positive,positive,positive,positive,positive
267491678,"But multiple instances of TPOT may build many same pipeline and do much redundant calculation,  why this way is better?",multiple may build many pipeline much redundant calculation way better,issue,negative,positive,positive,positive,positive,positive
267373329,"Once you interrupt the TPOT process, it stores the best pipeline internally. If for some reason it's not closing gracefully after the interrupt, you can still access the best pipeline at `tpot._fitted_pipeline`.",interrupt process best pipeline internally reason gracefully interrupt still access best pipeline,issue,positive,positive,positive,positive,positive,positive
267372770,We've been looking at parallelization options but haven't found one that quite works for us yet. Perhaps the best option right now is to run multiple instances of TPOT (10+) to get a diversity of recommendations.,looking parallelization found one quite work u yet perhaps best option right run multiple get diversity,issue,positive,positive,positive,positive,positive,positive
267271201,"AFAIK with dask you build the computation graph, and *then* you *execute* it. The memory allocation, redundancy elimination and job distribution (in case of a cluster) is entirely Dask's problem.",build computation graph execute memory allocation redundancy elimination job distribution case cluster entirely problem,issue,negative,neutral,neutral,neutral,neutral,neutral
267175520,@rhiever I'd recommend specifying specific versions for each dependency both in setup.py and in the installation instructions (which would need to be updated for each release).,recommend specific dependency installation would need release,issue,negative,neutral,neutral,neutral,neutral,neutral
267159845,"With dask, is there any way to prevent constant duplication of a dataset in memory? As it is, it's fairly easy to parallelize TPOT with the python `multiprocessing` module, but that requires cloning the entire environment with `pickle` through some IPC. This is a problem for large data-sets, as the memory copying takes up a lot of time, and in the end you need a shit-ton of RAM.",way prevent constant duplication memory fairly easy parallelize python module entire environment pickle problem large memory lot time end need ram,issue,negative,positive,positive,positive,positive,positive
267093536,"> I know it's not ideal, but you could put that in the file name when you export. Or programmatically write out a README for each file that's exported.

- pipeline.py.json works (and maintains somewhat-structured data)
- wrapping the pipeline in a class with a ``__call__`` and a ``params`` attribute could be useful",know ideal could put file name export programmatically write file work data wrapping pipeline class attribute could useful,issue,positive,positive,positive,positive,positive,positive
267087568,"I know it's not ideal, but you could put that in the file name when you export. Or programmatically write out a README for each file that's exported.",know ideal could put file name export programmatically write file,issue,positive,positive,positive,positive,positive,positive
267086462,"> When you say parameters, do you mean TPOT parameters? e.g., mutation rate, pop size, etc.?

Yup",say mean mutation rate pop size,issue,negative,negative,negative,negative,negative,negative
267081964,"What version of `deap` are you using? Execute the following on your command line:

```python -c ""import deap; print(deap.__version__)""```",version execute following command line python import print,issue,negative,neutral,neutral,neutral,neutral,neutral
266992801,"... Use case: For historical purposes, I'd like to keep around generated pipelines with e.g. __00<n>.py as a suffix; as the codebase changes.

I suppose without the source or current commit rev the params are not as useful. ",use case historical like keep around suffix suppose without source current commit rev useful,issue,positive,positive,neutral,neutral,positive,positive
266690139,"Thanks for the quick reply,

I've tried the example code as it is (tried them all).  Got the same error.

I'm using python 3 (via anaconda) on windows,  I've installed all the relevant libraries.
",thanks quick reply tried example code tried got error python via anaconda relevant,issue,negative,positive,positive,positive,positive,positive
266533074,Can you please provide details on your machine and install? Are you simply running the code in the page you linked?,please provide machine install simply running code page linked,issue,negative,neutral,neutral,neutral,neutral,neutral
265004577,"@rhiever Thanks - I didn't realize that, but noticed in the docs that you do mention you can pass SKLearn CV objects. That should do it.",thanks realize mention pas,issue,negative,positive,positive,positive,positive,positive
264950819,"You should be able to do this through TPOT's `num_cv_folds` parameter. We pass that parameter directly to `cross_val_score`'s `cv` parameter.

Perhaps we should rename `num_cv_folds` to just `cv`, as sklearn does.",able parameter pas parameter directly parameter perhaps rename,issue,negative,positive,positive,positive,positive,positive
264210303,"Right now, the initial pipelines are randomly generated. [This line](https://github.com/rhiever/tpot/blob/21aad37930e2bc3cc40822abdeac5efaa1e809ce/tpot/base.py#L257) is used to create new random individuals, which uses [this](https://github.com/rhiever/tpot/blob/21aad37930e2bc3cc40822abdeac5efaa1e809ce/tpot/base.py#L256) generator. Those are the lines you would want to change if you wanted to change TPOT's initialization process.",right initial randomly line used create new random generator would want change change process,issue,negative,negative,negative,negative,negative,negative
263394214,That's very surprising. I bet we could find some examples where those findings don't hold.,surprising bet could find hold,issue,negative,positive,positive,positive,positive,positive
262965837,I suggest to use sag and always have dual on False.,suggest use sag always dual false,issue,negative,negative,negative,negative,negative,negative
262931586,This paper http://bit.ly/2gbuKey suggests that non-linear dimensionality reduction techniques fail to improve upon PCA in natural data sets; it actually has KernelPCA in the comparison. Since PCA is super fast compared to KernelPCA and other non-linear techniques I would vote against including non-linear stuff.,paper dimensionality reduction fail improve upon natural data actually comparison since super fast would vote stuff,issue,positive,positive,neutral,neutral,positive,positive
261310178,"Would not be better to use the [multiprocessing](https://github.com/DEAP/deap/blob/master/examples/ga/onemax_mp.py) capabilities of DEAP. i.e Each combination(preprocessor, algorithm, postprocessor, etc) is an individual in the population of different combinations in TPOT. Hence exploiting the DEAP's multiprocessing feature helps TPOT in parallelizing,  through running different individual in different cores?
",would better use combination algorithm individual population different hence feature running different individual different,issue,negative,positive,neutral,neutral,positive,positive
261235400,"@weixuanfu2016 thanks! it worked like a charm
(sorry for the late response)
",thanks worked like charm sorry late response,issue,positive,negative,negative,negative,negative,negative
261161664,"One unit test in test.py only tests one pipeline in TPOT, if the pipeline has a error (e.g `Found array with 0 feature(s) (shape=(116, 0)) while a minimum of 1 is required.`) then the unit test will fail. It is why the tests by Travis CI sometimes fail in a build but sometimes pass.

[Example 1](https://travis-ci.org/rhiever/tpot/builds/176604884)
[Example 2](https://travis-ci.org/rhiever/tpot/builds/176470237)

```
def test_fit():
    """"""Assert that the TPOT fit function provides an optimized pipeline""""""
    tpot_obj = TPOTClassifier(random_state=42, population_size=1, generations=1, verbosity=0)
    tpot_obj.fit(training_features, training_classes)

    assert isinstance(tpot_obj._optimized_pipeline, creator.Individual)
    assert tpot_obj._gp_generation == 0
    assert not (tpot_obj._start_datetime is None)
```
",one unit test one pipeline pipeline error found array feature minimum unit test fail travis sometimes fail build sometimes pas example example assert fit function pipeline assert assert assert none,issue,negative,negative,negative,negative,negative,negative
261154764,"@joseortiz3  #317  Here is a PR for adding sample weight into TPOT. You may want to try it on your datasets. Let us know if you have any questions or further comments. 
",sample weight may want try let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
261145396,"@rhiever I just looked into source codes of pipeline object of scikit-learn. the `fit_params` is not a name of argument in pipeline object, just kinda a variable name of a dictionary. However, in `cross_val_score`, `fit_params` is the name of the argument. Very strange. The codes below works:

```
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score

X, y = make_classification()

sample_weight = range(1, len(y) + 1) # made-up weights

clf_dt = make_pipeline(StandardScaler(), DecisionTreeClassifier())

fit_params_1={'decisiontreeclassifier__sample_weight': sample_weight}
clf_dt.fit(X=X, y=y, **fit_params_1)

cv_score = cross_val_score(clf_dt, X, y, fit_params = fit_params_1 )
print(cv_score)

```
",source pipeline object name argument pipeline object variable name dictionary however name argument strange work import import import import import import range print,issue,negative,negative,neutral,neutral,negative,negative
261093883,"Update: the list is changed to a dictionary which only store pipeline string and fitness stores to save the memory usage. I still keep this dictionary since not all the duplicate pipelines can be detected by parent-child comparison below
The `eaSimple` was replaced by adding built-in check duplicated pipeline by parent-child comparison. 
",update list dictionary store pipeline string fitness save memory usage still keep dictionary since duplicate comparison check pipeline comparison,issue,positive,neutral,neutral,neutral,neutral,neutral
260971586,"It sounds like it should be feasible to make a custom scoring metric for TPOT, e.g., you could take the weighted average if you know the class weights ahead of time:

``` python
def weighted_accuracy(y_true, y_pred):
    class_weights = {-1: 0.25, 0: 1., 1: 8.}
    all_classes = list(set(np.append(y_true, y_pred)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = \
            float(sum((y_pred == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))

        this_class_specificity = \
            float(sum((y_pred != this_class) & (y_true != this_class))) /\
            float(sum((y_true != this_class)))

        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        # Weight the accuracy score
        this_class_accuracy *= class_weights[this_class]
        all_class_accuracies.append(this_class_accuracy)

    return np.mean(all_class_accuracies)
```

then that function can simply be passed to TPOT's `scoring` parameter.

However, it may still make sense to integrate sample weights to affect the actual _training_ of the models rather than just the evaluation of them. @weixuanfu2016, sklearn Pipeline objects allow for `fit` calls to take a `fit_params`. Maybe we can make this an optional TPOT parameter as well, and only pass it to operators that allow the `sample_weights`parameter. I started typing up a quick demo but keep getting an odd error message:

``` python
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.datasets import make_classification

X, y = make_classification()

sample_weight = range(1, len(y) + 1) # made-up weights

clf_dt = make_pipeline(StandardScaler(), DecisionTreeClassifier())
clf_knn = make_pipeline(StandardScaler(), KNeighborsClassifier())

clf_dt.fit(X=X, y=y, fit_params={'decisiontreeclassifier__sample_weight': sample_weight})
```

```
sklearn/pipeline.py in _fit(self, X, y, **fit_params)
    225                                 if step is not None)
    226         for pname, pval in six.iteritems(fit_params):
--> 227             step, param = pname.split('__', 1)
    228             fit_params_steps[step][param] = pval
    229         Xt = X

ValueError: not enough values to unpack (expected 2, got 1)
```
",like feasible make custom scoring metric could take weighted average know class ahead time python list set float sum float sum float sum float sum weight accuracy score return function simply scoring parameter however may still make sense integrate sample affect actual rather evaluation pipeline allow fit take maybe make optional parameter well pas allow parameter quick keep getting odd error message python import import import import import range self step none step param step param enough unpack got,issue,positive,positive,neutral,neutral,positive,positive
260848988,"@rhiever I think I understand what you're saying, that if for instance I'm doing binary classification and I have 100 cases of class [0] and 1000000 cases of class [1] that getting the 100 [0]'s correct is just as important as getting the 10000000 [1]'s correct if I use `balanced accuracy metric`. However that is not what I'm trying to accomplish.

I have samples that I am trying to label (classify) as [-1,0,1]. Each sample has a true label [-1,0,1], and many features that I can use to try predict that label (about forty of them). However, some samples are much more important to get right than other samples. For instance, mislabelling one sample could be 100x worse than mislabelling another (which is why I was previously trying to make a loss function that depended on the xvalues/features but this will work too). Another example, two samples could be both class [1], but labelling the first a [1] could be _much_ more important than labelling the second a [1].
",think understand saying instance binary classification class class getting correct important getting correct use balanced accuracy metric however trying accomplish trying label sample true label many use try predict label forty however much important get right instance one sample could worse another previously trying make loss function work another example two could class first could important second,issue,positive,positive,positive,positive,positive,positive
260846717,"I put the custom scoring function into the SCORERS dictionary using my hacked `make_scorer`, and it worked fine. Unfortunately, the individual models (`xgboost`,etc..) will still only use the loss functions they are designed to support (such as least squares). I now know that many models work well only with certain loss functions, that passing my arbitrary scoring function as the loss function would probably (?) not work. It's still helpful to have the `xvals` scoring functionality however, and I will probably continue to use it. I can basically accomplish what I wanted to do by getting TPOT to use `sample_weights`.

```
#uses my custom subclass XValScorer due to my added argument needs_xvals
SCORERS['x_score'] = make_scorer(my_custom_func,needs_xvals = True)
#works fine
model = tpot.TPOTRegressor(generations = 6, population_size = 10, scoring='x_score', verbosity =3)
```
",put custom scoring function dictionary hacked worked fine unfortunately individual still use loss designed support least know many work well certain loss passing arbitrary scoring function loss function would probably work still helpful scoring functionality however probably continue use basically accomplish getting use custom subclass due added argument true work fine model verbosity,issue,positive,positive,positive,positive,positive,positive
260770289,"@joseortiz3, can you please explain your specific use case here? Are you trying to balance your scoring metric based on the distribution of data? By default, TPOT uses a [balanced accuracy metric](https://github.com/rhiever/tpot/blob/bd24d60375ea2083ecfddb069eee9b9ef259ecb5/tpot/metrics.py#L25), which computes the accuracy score on a per-class basis then averages them over all classes. This helps account for class imbalance in the sense that a very rare class would be equally important as an abundant class.
",please explain specific use case trying balance scoring metric based distribution data default balanced accuracy metric accuracy score basis class account class imbalance sense rare class would equally important abundant class,issue,negative,positive,positive,positive,positive,positive
260698837,"@weixuanfu2016 / @sahilshah1194, can you please verify that all of the classifiers and regressors that we use in TPOT indeed take `sample_weights` into their `fit` functions? If so, it would be a straightforward API change for TPOT to allow `sample_weights` as an optional parameter to the `fit` function, which would simply pass those weights (or `None` by default) to the sklearn pipeline `fit` function.
",please verify use indeed take fit would straightforward change allow optional parameter fit function would simply pas none default pipeline fit function,issue,positive,positive,positive,positive,positive,positive
260697819,"Can you please write a short working code example of how you would use this with the `cross_val_score` method? I'd imagine we could parameterize `cross_val_score`'s `cv` parameter to allow custom validation procedures. Currently we limit `cv` to only be integers, but it would be straightforward to allow it to take arbitrary validation procedures as well as integers. 
",please write short working code example would use method imagine could parameter allow custom validation currently limit would straightforward allow take arbitrary validation well,issue,positive,positive,neutral,neutral,positive,positive
260697184,"That bit of code is where we catch user interrupts and close TPOT gracefully. This checkpointing process would need to be inserted into the evolutionary algorithm itself.

I'd imagine we could do something similar to how we update TPOT's progress bar at the end of every generation. We can create a [decorator](https://github.com/rhiever/tpot/blob/21aad37930e2bc3cc40822abdeac5efaa1e809ce/tpot/base.py#L184) over the selection function, which can [checkpoint the current generation](https://github.com/rhiever/tpot/blob/master/tpot/decorators.py#L26) at the end of every optimization cycle.
",bit code catch user close gracefully process would need inserted evolutionary algorithm imagine could something similar update progress bar end every generation create decorator selection function current generation end every optimization cycle,issue,positive,neutral,neutral,neutral,neutral,neutral
260696178,"Yes, you should update to the latest version of scikit-learn. On your command line, enter:

```
pip install --upgrade scikit-learn
```

or

```
conda update scikit-learn
```

if you use the Anaconda distribution.
",yes update latest version command line enter pip install upgrade update use anaconda distribution,issue,negative,positive,positive,positive,positive,positive
260693892,"Can you please clarify: Where did you inject the scoring function? If you inject your scoring function into the `SCORING` dictionary and set TPOT's `scoring_function` to a string containing your scoring function, [as we do here](https://github.com/rhiever/tpot/blob/21aad37930e2bc3cc40822abdeac5efaa1e809ce/tpot/base.py#L184), TPOT should use that scoring function for the optimization process.
",please clarify inject scoring function inject scoring function scoring dictionary set string scoring function use scoring function optimization process,issue,positive,neutral,neutral,neutral,neutral,neutral
260693702,"Thank you for sharing this idea. I have some thoughts about it. Actually, it is related to a [old issue](https://github.com/scikit-learn/scikit-learn/issues/4632) in scikit-learn. The thing you may want to try is to wrap the cross_val_score from scikit-learn to including the sample weight. Here is a [demo](https://github.com/ndawe/scikit-learn/commit/3da7fb708e67dd27d7ef26b40d29447b7dc565d7).  It's a little hacky and not guaranteed to still work with the lastest version of scikit-learn. 
",thank idea actually related old issue thing may want try wrap sample weight little hacky still work version,issue,negative,negative,neutral,neutral,negative,negative
260664150,"The issue is caused by the `p_train[target_var]`, which should be a 1-D array but panda dataframe is 2D array-like data structure. Change the tpot.fit() codes as below will solve the input issue.

```
tpot.fit(pd.np.array(p_train[train_cols]), pd.np.array(p_train[target_var]).ravel())
```
",issue array panda data structure change solve input issue,issue,negative,neutral,neutral,neutral,neutral,neutral
260579723,"@weixuanfu2016 thanks for replying

I tried what you suggested and still got the same error. The program runs upto 100% but in all generations

I get 

```
Generation 100 - Current Pareto front scores:
5000    inf GradientBoostingClassifier(input_matrix, 0.93000000000000005, 0.92000000000000004)
```

the data is from the kaggle competition [ghouls, goblins and ghost](https://www.kaggle.com/c/ghouls-goblins-and-ghosts-boo)

and here is the [notebook](https://github.com/heaven00/ghost_busters/blob/master/Tpot%20Classifier.ipynb)
",thanks tried still got error program get generation current front data competition ghost notebook,issue,negative,positive,neutral,neutral,positive,positive
260485805,"Could you please let us know more details about the codes for inputting the dataset?

Could you please also try the codes below?

```
import numpy as np
...
tpot.fit(np.array(p_train[train_cols]), np.array(p_train[target_var]))
```
",could please let u know could please also try import,issue,positive,neutral,neutral,neutral,neutral,neutral
260378226,"Thanks... :)  @weixuanfu2016 problem solved.... after updating(sci kit) and reinstalling (tpot).

thanks once again.... :+1: 
",thanks problem kit thanks,issue,negative,positive,positive,positive,positive,positive
260352783,"The issue related to model_selection is commented in #314. Please update the scikit-learn to 0.18.

The operators issue is strange. Could  you please let us know the python codes or command lines to produce the issue?
",issue related please update issue strange could please let u know python command produce issue,issue,negative,negative,neutral,neutral,negative,negative
260101343,"Well, I did it, but it didn't do what I was naively hoping it would by adding a new subclass to sklearn.metrics.scorer. The custom scoring function works, but the models do not use it for optimization, so there is little point in using it.

I was [very naively] hoping that my custom scoring function would be used as the loss function for all the models included in TPOT. Yes, it's absurd, but I thought it.

Now I'm trying to see if TPOT might work with `sample_weights`. This sounds like the closest thing to solving my problem. I will make another issues thread, since this is unrelated to the current one. Feel free to close this, we found the answer to my original question.
",well naively would new subclass custom scoring function work use optimization little point naively custom scoring function would used loss function included yes absurd thought trying see might work like thing problem make another thread since unrelated current one feel free close found answer original question,issue,positive,negative,neutral,neutral,negative,negative
260045019,"Thanks for pointing this out! We may have to add some extra checks in the export functionality.
",thanks pointing may add extra export functionality,issue,negative,positive,neutral,neutral,positive,positive
260018266,"That's great, @manugarri! We haven't made any progress on this yet. At a high level, we would want to make it so the user can stop and restart TPOT at any point during the optimization process. This would mostly entail editing [base.py](https://github.com/rhiever/tpot/blob/master/tpot/base.py) in the `fit()` function to store the current state every generation of the optimization process.
",great made progress yet high level would want make user stop restart point optimization process would mostly entail fit function store current state every generation optimization process,issue,positive,positive,positive,positive,positive,positive
260013404,"It's possible if you're willing to edit the underlying TPOT code, but this functionality isn't currently built into TPOT. I can describe how to edit the TPOT code to accomplish this if you're interested.
",possible willing edit underlying code functionality currently built describe edit code accomplish interested,issue,positive,positive,positive,positive,positive,positive
259385912,"Thanks for answering! That's an interesting insight and could easily explain the problem. I checked but unfortunately my population was just 40 individuals (and 1000 generations).
",thanks interesting insight could easily explain problem checked unfortunately population,issue,negative,positive,positive,positive,positive,positive
259211300,"good point regarding linux - the python process just crashed on my macbook :(
",good point regarding python process,issue,negative,positive,positive,positive,positive,positive
259210717,"The default settings in TPOT on generation number and population size is `population_size=100, generations=100`. So, with generation 0, the number of run is 100*(100+1) = 10100. I think the dataset you used should be very huge and many pipeline were skiped due to a time limit for evaluating a single pipleline (`max_eval_time_mins = 5` in default) . 

To estimate the speed of simulation in the dataset,  I suggested reduce the generation number to ~10 and increase  `max_eval_time_mins` for your dataset by adding `generations=10, max_eval_time_mins = 10` into `TPOTClassifier`. Also I suggest this time-consuming process should run in a linux platform.   A strange bug related to #300 was just found in MacOS on Macbook Pro and we will fix it in next version of TPOT. The current version of TPOT is stable in Linux (should be also all right in Windows.)
",default generation number population size generation number run think used huge many pipeline due time limit single default estimate speed simulation reduce generation number increase also suggest process run platform strange bug related found pro fix next version current version stable also right,issue,positive,positive,positive,positive,positive,positive
259203552,"When I remove the option it seems to run longer:

```
2016-11-08 18:05:18,713 INFO -- MainProcess connectionpool.py:214 -- Starting new HTTP connection (1): update_checker.bryceboe.com
Optimization Progress:   0%|          | 7/10100 [10:51<201:40:59, 71.94s/pipeline]
Timeout during evaluation of pipeline #7. Skipping to the next pipeline.
Optimization Progress:   0%|          | 14/10100 [14:08<88:17:27, 31.51s/pipeline]
Timeout during evaluation of pipeline #14. Skipping to the next pipeline.
Optimization Progress:   0%|          | 16/10100 [17:45<182:17:02, 65.08s/pipeline]
Timeout during evaluation of pipeline #16. Skipping to the next pipeline.
Optimization Progress:   0%|          | 19/10100 [25:09<374:49:26, 133.85s/pipeline]
Timeout during evaluation of pipeline #19. Skipping to the next pipeline.
Optimization Progress:   0%|          | 22/10100 [25:23<136:50:45, 48.88s/pipeline]
```

I hope it works.
How long does such a simulation usually run? 10100 seems to be a lot.
",remove option run longer starting new connection optimization progress evaluation pipeline skipping next pipeline optimization progress evaluation pipeline skipping next pipeline optimization progress evaluation pipeline skipping next pipeline optimization progress evaluation pipeline skipping next pipeline optimization progress hope work long simulation usually run lot,issue,positive,negative,neutral,neutral,negative,negative
259164869,"Also, please test codes above again without `max_time_mins=10`. This parameters will override generation parameter and kill the fit() process in 10 minutes. If the process not get a best pipeline in the time limit, no fitted pipeline will be exported. I think it maybe the reason of this issue.

Update: the codes below may be used to reproduce the issue. I think we need add a friendly warning for using this parameters when running a time-consuming jobs. Sorry for the confusion. 

```
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification
from tpot import TPOTClassifier
X, y = make_classification(n_samples=200, n_features=100,
                                    n_informative=2, n_redundant=10,
                                    random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)

tpot = TPOTClassifier(verbosity=2, max_time_mins=1)
tpot.fit(X_train, y_train)
tpot.export('tpot_pipe.py')
print(tpot.score(X_test, y_test))

```
",also please test without override generation parameter kill fit process process get best pipeline time limit fitted pipeline think maybe reason issue update may used reproduce issue think need add friendly warning running sorry confusion import import import print,issue,negative,positive,positive,positive,positive,positive
259160846,"Increasing `n_features` (e.g `n_features=800`) will reproduce the issue more quickly. I tested it on 2 Macbook Pro on different version of MacOS. 
Update: after a few more tests, the issue also reproduced in Macbook Air. There seems to be a few conditions to reproduce the issue:
1. high feature numbers
2. LogisticRegression or BernoulliNB from scikit-learn (maybe more operators, need more tests)
3. using `resource. setrlimit` to set CPU time limit
4. macOS

@rhiever 
",increasing reproduce issue quickly tested pro different version update issue also air reproduce issue high feature maybe need resource set time limit,issue,negative,positive,positive,positive,positive,positive
259149275,"Could you please post the stdout after fit() function? The best pipeline should be printed out if fit() finished normally. Please also let us know  which platform this codes ran on. More details will help us find the bugs causing this issue.
",could please post fit function best pipeline printed fit finished normally please also let u know platform ran help u find causing issue,issue,positive,positive,positive,positive,positive,positive
259040620,"It showed there are more than 40k pipelines in TPOT pipeline optimization process in the stdout. What is the setting of population size and generation number? I think if the population size is more than 400 with limited number of operators, it is possible that many duplicated pipelines is generated in the creator (generation 0). I guess the initial evaluation may process all the individuals, including these duplicates, in generation 0.
",pipeline optimization process setting population size generation number think population size limited number possible many creator generation guess initial evaluation may process generation,issue,negative,positive,positive,positive,positive,positive
258933156,"Thank you for providing this log files. Finally, after a lot of tests, we found the bug. I already posted a hot patch #306 for dealing with this issue.

Please test this codes below, it should reproduce the issue randomly in macOS. Please let us know if it cannot be reproduced.

Some operators from scikit-learn seems not works with  resource.setrlimit in MacOS (from version 10.11.2 - 10.12.1). But there is not this kind of issue in Linux OS (I tested 100 instances with your dataset and codes in multiple Linux platforms). Before we merge the PR#306 with TPOT, please run the current version of TPOT in a Linux platform. Sorry for the inconvenient. 

```
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import make_classification
from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB
from resource import getrlimit, setrlimit, RLIMIT_CPU, getrusage, RUSAGE_SELF, RUSAGE_CHILDREN


X, y = make_classification(n_samples=200, n_features=80,
                                    n_informative=2, n_redundant=10,
                                    random_state=42)


classifor_list = {#""ExtraTreesClassifier"": ExtraTreesClassifier, # pass
                    ""LogisticRegression"": LogisticRegression, # fail
                    #""MultinomialNB"": MultinomialNB, # pass
                    #""GaussianNB"": GaussianNB, # pass
                    ""BernoulliNB"": BernoulliNB # fail
                }



for key in classifor_list.keys():
    clf = classifor_list[key]()
    for i in range(300):
        if i == 0:
            print(key)
        print(""test #"",i+1,'for',key)
        r = getrusage(RUSAGE_SELF)
        cpu_time = r.ru_utime + r.ru_stime
        r_child = getrusage(RUSAGE_CHILDREN)
        child_time = r_child.ru_utime + r_child.ru_stime
        current = getrlimit(RLIMIT_CPU)
        print('Main_cpu_time used: {} | child_cpu_time used: {}\n'.format(cpu_time, child_time))
        print('cpu_time_limit: {}'.format(current))
        try:
            setrlimit(RLIMIT_CPU, (cpu_time+300, current[1]))
            clf.fit(X,y)
            cv_scores = cross_val_score(clf, X, y)
        except Exception as e:
            print('some thing wrong',e)
        finally:
            setrlimit(RLIMIT_CPU, current)
```
",thank providing log finally lot found bug already posted hot patch dealing issue please test reproduce issue randomly please let u know work version kind issue o tested multiple merge please run current version platform sorry inconvenient import import import import import resource import pas fail pas pas fail key key range print key print test key current print used used print current try current except exception print thing wrong finally current,issue,negative,negative,negative,negative,negative,negative
258596292,"Thanks Randy for your fast reply! It seemed to be re-evaluating the same pipeline each time (that or the cached version took 3 minutes to load). But I changed a few lines of code (like removing Regressors, adding _print_ statements and setting n_jobs to **3** in _cross_val_score_) and thus I don't think it is representative of the current branch (maybe yes, maybe not). In any case I will keep an aye on this issue and if I detect it again in the _main_/_development_ branch I'll let you know with a PoC. 
",thanks randy fast reply pipeline time version took load code like removing setting thus think representative current branch maybe yes maybe case keep aye issue detect branch let know,issue,positive,positive,positive,positive,positive,positive
258505049,"This can currently be ""hacked"" into TPOT by modifying the [population instantiation procedure](https://github.com/rhiever/tpot/blob/master/tpot/base.py#L296), but it could be a nice feature to be able to do that through the regular TPOT interface. We'd have to put a lot of thought into how that would look. I'll file this as an enhancement request for now.
",currently hacked population procedure could nice feature able regular interface put lot thought would look file enhancement request,issue,negative,positive,positive,positive,positive,positive
258504027,"@QueenB, you would:
- Add the AdaBoostClassifier.py file in [this directory](https://github.com/rhiever/tpot/tree/master/tpot/operators/classifiers) (see the other classifiers for examples)
- Add the `import` statement to [this file](https://github.com/rhiever/tpot/blob/master/tpot/operators/classifiers/__init__.py)

And that's it. Note that we're working on a refactor of this interface to make it much easier to customize what algorithms and parameters TPOT optimizes over.
",would add file directory see add import statement file note working interface make much easier,issue,negative,positive,positive,positive,positive,positive
258503466,"We've been talking about adding a `n_jobs` parameter to TPOT for quite some time, which would basically do this. Perhaps we should just do that.
",talking parameter quite time would basically perhaps,issue,negative,neutral,neutral,neutral,neutral,neutral
258503236,"TPOT may maintain multiples of the same pipeline in the candidate solution population, but it only evaluates them once (and uses the cached result for the rest). Please let us know if the algorithm seems to be re-evaluating the pipelines multiple times.

As for _why_ TPOT does that: Sometimes it's useful in population-based algorithms to keep multiple copies of the most promising solution within the population. This gives the algorithm more copies of the solution to ""tinker"" with via mutation and crossover, rather than filling that population space with less-fit candidates.
",may maintain pipeline candidate solution population result rest please let u know algorithm multiple time sometimes useful keep multiple promising solution within population algorithm solution tinker via mutation crossover rather filling population space,issue,positive,positive,positive,positive,positive,positive
258502152,"@weixuanfu2016's solution is probably the ""best"" solution in sklearn terms. You can then pass arbitrary  `X` matrices to the function resulting from `make_scorer(custom_scorer, X_used = used_features)`, as `X_used` is a parameter of the scoring function and not a fixed value.
",solution probably best solution pas arbitrary matrix function resulting parameter scoring function fixed value,issue,positive,positive,positive,positive,positive,positive
258501604,"I agree. Can you file a separate issue and list the possible options?
",agree file separate issue list possible,issue,negative,neutral,neutral,neutral,neutral,neutral
258467405,"May I ask what is the current priority level of using distributed computing libraries (ideally DASK, that comes with caching) in tpot? I think that's vital for such a project to be usable in the real world and it should be orthogonal to the ""core"" branch development.

I think that if we manage to represent the whole population of pipelines in a huge dask graph it would be a good start. Then, caching of intermediate results (with the current _development_ branch I'm spending most of the computing time recalculating the same xgboost!), multicore and multi-server would be hand in hand.
Any chance of reopening this issue?
",may ask current priority level distributed ideally come think vital project usable real world orthogonal core branch development think manage represent whole population huge graph would good start intermediate current branch spending time multicore would hand hand chance issue,issue,positive,positive,positive,positive,positive,positive
258438175,"Indeed that would be more precise. I proposed to make _njobs == num_cv_folds_ since the default number of cv folds in **tpot** is 3, and most machines (used for machine learning) have more than 3 cores. Just to make @minimumnz feel better not having idle cores [1] ;-)

[1] https://github.com/rhiever/tpot/issues/177
",indeed would precise make since default number used machine learning make feel better idle,issue,negative,positive,positive,positive,positive,positive
258436741,"Thank you for sharing this nice tips. Based on the [User Guide](http://scikit-learn.org/0.17/modules/generated/sklearn.cross_validation.cross_val_score.html) of cross_val_score from  scikit-learn `njobs` parameter determines the CPU number to use in cross-validation while `cv` parameter determines the number of folds. Maybe adding `njobs` as another parameter in TPOT for paralleling the cross-validation with default of 1 since this way may use much more system resource. @rhiever 
",thank nice based user guide parameter number use parameter number maybe another parameter default since way may use much system resource,issue,positive,positive,positive,positive,positive,positive
258383167,"Not to hijack the thread, but i wanted to add AdaBoostClassifier into the model, what changes would be required to make in the code? besides the adding the classifier into the code. 
",hijack thread add model would make code besides classifier code,issue,negative,neutral,neutral,neutral,neutral,neutral
258238833,"@rhiever Thanks for that. So you don't think I'll have to hack new classes and functions into `sklearn.metrics.scorer`? I just want to clear this up, since from what I've seen, it looks like I need to hack sklearn more than tpot to get this to work.

It seems to me the way we add a scorer to TPOT's SCORERS dictionary is to do the following in `tpot.metrics` (or `sklearn.metrics.scorer`):

`SCORERS['custom_scorer_name'] = make_scorer(custom_scorer)`
(where `custom_scorer` is now `def custom_scorer(y_true, y_pred, x_used)`)

but make_scorer is defined in `sklearn.metrics.scorer`, and is a function that currently only has the insufficient arguments:
`def make_scorer(score_func, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs):`

It seems to me I would have to add another optional argument `needs_xvals=False` to make_scorer, that would make my scorer's class brand-new class: `_XvalScorer` (which would be adding another class to the three currently defined under `sklearn.metrics.scorer`: `ThresholdScorer`, `ProbaScorer`, and `PredictScorer`.) My new class would be able to call its `self._score_func` with the `self._score_func(y_true, y_pred, x_used)` signature.

Sounds like that's all I need to do? If so, I think I will try doing it soon.

@weixuanfu2016 I see what you did there: You used the `kwargs` argument of `make_scorer` to pass `X_used` to the scoring function through the scoring class. Didn't realize `X_used` would then become the `self._kwargs` member variable of `sklearn.metrics.scorer._BaseScorer` class, which would then make it available to `self._score_func` which is currently called by:

`self._score_func(y_true, y_pred, **self._kwargs)`

That's pretty neat (maybe it's considered best-practices in python?). That's nice, considering I don't have to hack anything. 

The following might be a misconception:
_The major problem with this is some array `X_used` will be permanently referenced by my `_BaseScorer` subclass. So I'm more inclined to hack sklearn (maybe it will be a supported feature someday)._
",thanks think hack new class want clear since seen like need hack get work way add scorer dictionary following defined function currently insufficient would add another optional argument would make scorer class class would another class three currently defined new class would able call signature like need think try soon see used argument pas scoring function scoring class realize would become member variable class would make available currently pretty neat maybe considered python nice considering hack anything following might misconception major problem array permanently subclass hack maybe feature someday,issue,positive,positive,positive,positive,positive,positive
258236010,"Here is a demo. Hope it could be helpful

```

# for editing tpot/metrics.py
from sklearn.metrics import make_scorer, SCORERS
import numpy as np

def custom_scorer(y_true, y_pred, X_used = None):
    """"""
    Parameters
    ----------
    y_true: numpy.ndarray {n_samples}
        True class labels
    y_pred: numpy.ndarray {n_samples}
        Predicted class labels by the estimator
    X_used: numpy.ndarray  {n_samples, n_features_used}
        A numpy matrix containing the training and used features for the
        `individual`'s evaluation

    Returns
    -------
    fitness: float
        Returns a float value indicating the `individual`'s accuracy
    """"""
    def custom_conseq_func(y_pred, X_used):
        """"""
        define the consequence of action
        return y_pred_consequences
        just a example:
        """"""
        if X_used:
            for i in X_used.shape[0]:
                if list(X_used[i,]).count(1) > 10:
                    y_pred[i] = 0
        return y_pred
    y_pred_consequences = custom_func(y_ture, X_used)
    all_classes = list(set(np.append(y_true, y_pred_consequences)))
    all_class_accuracies = []
    for this_class in all_classes:
        this_class_sensitivity = \
            float(sum((y_pred_consequences == this_class) & (y_true == this_class))) /\
            float(sum((y_true == this_class)))

        this_class_specificity = \
            float(sum((y_pred_consequences != this_class) & (y_true != this_class))) /\
            float(sum((y_true != this_class)))

        this_class_accuracy = (this_class_sensitivity + this_class_specificity) / 2.
        all_class_accuracies.append(this_class_accuracy)

    return np.mean(all_class_accuracies)

# register custom_scorer
# need also define the self.used_features to
# then you can use the custom_scorer with scoring='custom_scorer' in scripts of TPOT 
# X_used = used_features is a kwarg passing to custom_scorer function
# Note: !!used_features need to be a global variable if you don't want to change too much in base.py 
SCORERS['custom_scorer'] = make_scorer(custom_scorer, X_used = used_features)



```
",hope could helpful import import none true class class estimator matrix training used individual evaluation fitness float float value individual accuracy define consequence action return example list return list set float sum float sum float sum float sum return register need also define use passing function note need global variable want change much,issue,positive,positive,positive,positive,positive,positive
258160824,"[tpot_debug_for_issue300.tar.gz](https://github.com/rhiever/tpot/files/569232/tpot_debug_for_issue300.tar.gz)

Please download my tweaked version of TPOT based on 0.6.7. Your dataset and codes are also in the compressed file. You can use the codes below to get a debug log when running a instance. Please send over the log file once the issue reproduces in your platform. Thanks.

```
tar -zxvf tpot_debug_for_issue300.tar.gz 
cd tpot
python issue_300.py 
```
",please version based also compressed file use get log running instance please send log file issue platform thanks tar python,issue,positive,positive,positive,positive,positive,positive
258149659,"@weixuanfu2016, were you able to hack up a demo of this as we discussed yesterday?

@joseortiz3, there seems to be a way to hack something like this into TPOT/sklearn, but it's pretty non-standard. You need to hack your scoring function into `tpot.metrics.SCORERS`, then make your instance of TPOT use that scoring function (set `tpot_instance. scoring_name` to the name of your scoring function in the `tpot.metrics.SCORERS` dictionary).
",able hack yesterday way hack something like pretty need hack scoring function make instance use scoring function set name scoring function dictionary,issue,positive,positive,positive,positive,positive,positive
258062337,"I tried your test pipeline 4 times all passed without error.  It did increase memory usage but never to the point where free memory was an issue on my 16GB MBP.

I ran several more instances of TPOT tonight and it definitely seems to fail in this case with light memory usage < 150MB for the process (7GB free on the system).  

Any recommendation about how I might get tpot to output every pipeline it's evaluating when it's evaluating such that we can narrow it down to a potential issue like you suggested or other suggestions for profiling?
",tried test pipeline time without error increase memory usage never point free memory issue ran several tonight definitely fail case light memory usage process free system recommendation might get output every pipeline narrow potential issue like,issue,negative,positive,neutral,neutral,positive,positive
258044502,"Interesting idea. I think you need define this customized scoring function. You can start from `tpot.metrics` (metrics.py in tpot source codes, also see the codes below). Even though [`make_score` function from scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer) only use `score_func(y_true, y_pred, **kwargs)`, you may hack into the **kwargs part to add `x_used` (maybe defined it as a global variable for using it in the new scoring function). Also you can check these [scoring function](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/metrics/classification.py) in scikit-learn to build the customized one. 

Please also check the [TPOT document](https://rhiever.github.io/tpot/contributing/) for using this customized version of TPOT.

```
In [12]: import tpot.metrics

In [13]: tpot.metrics.SCORERS
Out[13]: 
{'accuracy': make_scorer(accuracy_score),
 'adjusted_rand_score': make_scorer(adjusted_rand_score),
 'average_precision': make_scorer(average_precision_score, needs_threshold=True),
 'balanced_accuracy': make_scorer(balanced_accuracy),
 ...
```
",interesting idea think need define scoring function start source also see even though function use may hack part add maybe defined global variable new scoring function also check scoring function build one please also check document version import,issue,positive,positive,positive,positive,positive,positive
257977407,"Very glad to see this issue! Just to add this has an additional (and I think free) win. It allows users to define a much more broad score function. Sklearn's [make_scorer ](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) takes a need_proba keyword, that only needs for the classifer(or pipeline) to have a predict_proba method. Simply implementing this on the TPOT classifier should open that up for free. But it's worth a test before announcing of course :) 

This would allow people to use things like [Normalized Discount Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain), which is useful for a range of problems. 
",glad see issue add additional think free win define much broad score function need pipeline method simply classifier open free worth test course would allow people use like discount cumulative gain useful range,issue,positive,positive,positive,positive,positive,positive
257934498,"![rfe_example](https://cloud.githubusercontent.com/assets/817331/19939314/c025036c-a0fe-11e6-874c-1aa95bcc6584.png)

Couldn't figure out how to add the labels in Excel for some reason: 
X-axis: Step value
Y-axis: Mean score
",could figure add excel reason step value mean score,issue,positive,negative,negative,negative,negative,negative
257916228,"After a few tests on your datasets, I think I may find the reason of this issue. It seems not related to version of TPOT you used. It is about memory and a particular pipeline. 

Try the codes below. it should reproduce the issue with both your datasets and simulation datasets.

We will try to limit memory usage of pipeline to avoid this issue in the future version of TPOT.

To avoid this issue in the current version of TPOT, please try to clean your memory in the platform (e.g quit some apps using a lot of memory, like Chrome) to get at least 2.5 Gb free RAM.

```
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.pipeline import make_pipeline, make_union
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import make_classification
import pandas as pd

X, y = make_classification(n_samples=200, n_features=800,
                                    n_informative=2, n_redundant=10,
                                    random_state=42)

avyDataAll = pd.read_csv('./filteredFeaturesForTPotV5.csv')
avyDataX = avyDataAll.ix[:, avyDataAll.columns != 'class'].values
avyDataY = avyDataAll.ix[:, avyDataAll.columns == 'class'].values

X_train, X_test, y_train, y_test = train_test_split(avyDataX, avyDataY,
train_size=0.75, test_size=0.25, stratify=avyDataY)



fa_make = make_pipeline(
    make_union(
        FunctionTransformer(lambda X: X),
        FunctionTransformer(lambda X: X)
    ),
    PolynomialFeatures(),
    LogisticRegression()
)

# !!!!!dataset in issue 300 need at least 2.5Gb Memory 
fa_make.fit(X_train,y_train[:,0])


# !!!!!simulation dataset need at least 4Gb Memory 
fa_make.fit(X,y)

```

Additional info about memory usage when runing the dataset in this issue:
- Sorry for the typos in the plots
- Note: my MBP sleep a while when running instance 1
  ![issue300_memory_profile_instance1](https://cloud.githubusercontent.com/assets/21084970/19937302/108c1726-a0f7-11e6-9d50-e7f08c37d336.png)
  ![issue300_memory_profile_instance2](https://cloud.githubusercontent.com/assets/21084970/19937303/108d6d56-a0f7-11e6-94fb-f73ff18f1618.png)
",think may find reason issue related version used memory particular pipeline try reproduce issue simulation try limit memory usage pipeline avoid issue future version avoid issue current version please try clean memory platform quit lot memory like chrome get least free ram import import import import import import import lambda lambda issue need least memory simulation need least memory additional memory usage issue sorry note sleep running instance,issue,negative,negative,neutral,neutral,negative,negative
257866002,"Is there any update on this? I would be willing to contribute (this seems like an easy implementation), if you gave me some pointer to where to look at in the code and the discussed approaches.
",update would willing contribute like easy implementation gave pointer look code,issue,positive,positive,positive,positive,positive,positive
257748035,"Tried again with random_state=99 and it still repros. 

(py35)Scotts-MacBook-Pro-2: scottcha$ python forecastTpot.py 
Optimization Progress:  17%|████████▉                                            | 67/400 [02:15<14:50,  2.67s/pipeline]Cputime limit exceeded: 24
",tried still python optimization progress limit,issue,positive,neutral,neutral,neutral,neutral,neutral
257682238,"Thank you for letting us know the issue. I tried the codes and dataset in my macbook pro with this latest OSX but the error cannot be reproduced. 

My first guess about this issue is that the CPU time limit failed to reset during pipeline evaluation somehow. But it is strange because if CPU time limit failed to reset then this expectation signal  should not be available either, since this `SIGXCPU` (Cputime limit exceeded: 24) should be replaced by `RuntimeError` during pipeline evaluation.

For better reproducing the issue, could you please try the codes again with setting a few random seeds (e.g. put `random_state=99` in the `TPOTClassifier` function in the codes)? If the error happen again, please let us know the update codes with these random seeds

Meanwhile, I can try more instances to reproduce the error. 
",thank u know issue tried pro latest error first guess issue time limit reset pipeline evaluation somehow strange time limit reset expectation signal available either since limit pipeline evaluation better issue could please try setting random put function error happen please let u know update random meanwhile try reproduce error,issue,negative,positive,neutral,neutral,positive,positive
255830524,"@MaxPowerWasTaken You bring up a good point about the per-model regularization, and maybe for the sake of discussion it makes sense that we spell out the different kinds of regularization that we might use in TPOT (please correct me if I'm misguided here): 
- **Per-model regularization.** Most of the estimator operators we have in TPOT will have some sort of the regularization parameters that ties into the scikit-learn model underneath. These are controlled by TPOT, and are not treated very different than the other model parameters.
- **Per-pipeline regularization.** The creation of the TPOT pipelines can be thought to have regularization parameters as well; how complex are the pipelines, and does having simpler pipelines result in better general performance?
- **Genetic algorithm regularization.** The DEAP GP algorithms that we're using for TPOT can probably be regularized too, to give us better performing pipelines. This might manifest as something like early-stopping, or perhaps changing the number of individuals per generation (I don't think controlling the number of individuals per generation would really help, but it's a useful example).

But to get back to it – maybe we _should_ be thinking more about how the per-model regularization affects the other levels of complexity. Are the less complex pipelines always using well-regularized models underneath?
",bring good point regularization maybe sake discussion sense spell different regularization might use please correct misguided regularization estimator sort regularization model underneath different model regularization creation thought regularization well complex simpler result better general performance genetic algorithm regularization probably give u better might manifest something like perhaps number per generation think number per generation would really help useful example get back maybe thinking regularization complexity le complex always underneath,issue,positive,positive,positive,positive,positive,positive
255799550,"Regularization parameters are already included in most scikit-learn models. So when TPOT is exploring the hyperparameter space of a classifier, it is already using regularization. 

The IRIS example in the TPOT docs (http://rhiever.github.io/tpot/examples/IRIS_Example/) show an exported pipeline using Logistic Regression including parameters ""C=.09"" and ""penalty='l2'"". So TPOT selected a regularized logistic regression classifier in that example.
",regularization already included exploring space classifier already regularization iris example show pipeline logistic regression selected logistic regression classifier example,issue,negative,neutral,neutral,neutral,neutral,neutral
255097235,"yes, Maybe, one can put into a repository the datasets with meta information on the dataset.
One can download and run it on our machines and put it back the results to the repository db.
So, you could get distributed calculation through the participants, improving the database of datasets.

After having enough results, one can analyze the results : 
     (Dataset_characteristics,  BestSet_Algo( top 5) )

It would accelerate TPOT by restricting the genetic search to smaller search space
if the datasets belongs to a pre-analyzed cluster.

Think datasets are homogenous when belongs to some categories (image, web data,...).

What do you think having a Gitter Chat for TPOT ?
(this project is really interesting for me..., glad to contribute).
",yes maybe one put repository meta information one run put back repository could get distributed calculation improving enough one analyze top would accelerate genetic search smaller search space cluster think homogenous image web data think chat project really interesting glad contribute,issue,positive,positive,positive,positive,positive,positive
255092567,"@sahilshah1194, can you please plot that data and post the chart here?
",please plot data post chart,issue,negative,neutral,neutral,neutral,neutral,neutral
255091993,"@arita37, that sounds related to what we've been thinking about in #59. Maybe we can use a metalearning method like that for seeding the TPOT population.
",related thinking maybe use method like population,issue,negative,neutral,neutral,neutral,neutral,neutral
254934942,"Xgboost already added compatibility for latest version of scikit-learn a few days ago. So user will not get deprecation warning in TPOT if they use the latest Xgboost (or don't have it) . To now, there is no compatibility issue for TPOT 0.6.6. Cheers. We can close the issue. 
",already added compatibility latest version day ago user get deprecation warning use latest compatibility issue close issue,issue,negative,positive,positive,positive,positive,positive
254929997,"I added a unit test but didn't know why it caused one test failed above. The nose tests were all right in all three of my local devices in three difficult OS (Windows, Linux and MacOS).
",added unit test know one test nose right three local three difficult o,issue,negative,negative,neutral,neutral,negative,negative
254929279,"I already reported [this issue](https://github.com/scikit-learn/scikit-learn/issues/7689) (marked as bugs now) to scikit-learn team,, please hold on the PR until they find why the memory overflow happened. 
",already issue marked team please hold find memory overflow,issue,negative,positive,neutral,neutral,positive,positive
254928367,"This PR maybe not compatible with #267 since arguments' type format changed in that PR
",maybe compatible since type format,issue,negative,neutral,neutral,neutral,neutral,neutral
254885363,"I set up these code to see the differences between SVC and ETC across differents steps and it looks like ETC performs at least as good as SVC for steps from 0.1-0.7. 

```
import time
import numpy as np
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split, cross_val_score
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import ExtraTreesClassifier

X, y = datasets.make_classification(n_samples=10000, n_features=4000,
                                    n_informative=2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90,
                                                    random_state=42)

steps = np.arange(0.1,0.7,0.1)

for step in np.nditer(steps):
    print('-------------------------------------------------')
    print('Beginning setup for step {}'.format(step))
    svc = SVC(kernel=""linear"", C=1)
    rfe = RFE(estimator=svc, step=step)

    etc = ExtraTreesClassifier()
    rfe2 = RFE(estimator=etc, step=step)

    gnb = GaussianNB()
    gnb2 = GaussianNB()
    sklearn_pipeline = make_pipeline(rfe,gnb)
    sklearn_pipeline2 = make_pipeline(rfe2,gnb2)

    time_start= time.time()
    print('Fitting RFE with SVC')
    sklearn_pipeline.fit(X_train, y_train) # freezing here
    cv_scores = cross_val_score( sklearn_pipeline, X_train, y_train, cv=3, scoring=""accuracy"")

    print('Fitting RFE with ETC')
    sklearn_pipeline2.fit(X_train, y_train)
    cv_scores2 = cross_val_score( sklearn_pipeline2, X_train, y_train, cv=3, scoring=""accuracy"") 
    time_end = time.time()
    #print('Simulation Dataset:',time_end-time_start)
    print('Step: {}'.format(step))
    print('SCV: {}'.format(cv_scores))
    print('ETC: {}'.format(cv_scores2))
```
",set code see across like least good import time import import import import import import import import import step print print setup step step linear print freezing accuracy print accuracy print print step print print,issue,negative,positive,positive,positive,positive,positive
254061823,"One possibility is to investigate a large number of various datasets, let's say 10,000, in a batch
and do the machine learning on the results. It would give some concrete insights by dataset nature.

What about using the Kaggle ones ?, it may require many ressources for a limited amount of time,
but one can split the training to the members.
",one possibility investigate large number various let say batch machine learning would give concrete nature may require many limited amount time one split training,issue,negative,positive,positive,positive,positive,positive
253644105,"Tested 10 times with ETC estimator and step = 0.1, no freezing happened and all tests finished in 30 seconds for fit(). For step = 0.5, time usage is less than 10 seconds. For step = 0.9, less than 8 seconds

Also, LogisticRegression will works as a estimator in RFE without CPU freezing.
",tested time estimator step freezing finished fit step time usage le step le also work estimator without freezing,issue,negative,positive,positive,positive,positive,positive
253580672,"I think we can assume that TPOT users will be using the latest version of scikit-learn.
",think assume latest version,issue,negative,positive,positive,positive,positive,positive
253580506,"@weixuanfu2016, can you please verify that RFE w/ the ETC estimator and `step = 0.1` doesn't freeze with the data set from #271?
",please verify estimator step freeze data set,issue,negative,neutral,neutral,neutral,neutral,neutral
253350187,"For RFE operator, argument `step` = 0.5 will speed up the process a lot, and no freezing happened with the simulated dataset with both SVC and ETC estimators. Also I tested step=0.1, 0.2, 0.3 and 0.4 with both estimators, and all tests passed without freezing. However I tested with the dataset from issue #271, CPU freezing happened with step = 0.5 when using SVC as estimators but no freezing happened when using ETC estimator. Maybe changing the estimator is the solution to avoid freezing.
",operator argument step speed process lot freezing also tested without freezing however tested issue freezing step freezing estimator maybe estimator solution avoid freezing,issue,negative,neutral,neutral,neutral,neutral,neutral
253279293,"sag solver is only available from version 0.17 of scikit-learn. Should we add separate options of these solver based on version of scikit-learn?
",sag solver available version add separate solver based version,issue,negative,positive,positive,positive,positive,positive
253272204,"Could using SVC as the estimator be the issue? We should see if RFE will no longer freezes with ETC as the estimator.
",could estimator issue see longer estimator,issue,negative,neutral,neutral,neutral,neutral,neutral
253271899,"Should we just always use the sag solver then?
",always use sag solver,issue,negative,neutral,neutral,neutral,neutral,neutral
253065569,"RFE operator will cause CPU freezing (low CPU usage, like < 20%) when evaluating pipeline or fitting with a large dataset of a large number of features (like the dataset in the issue #271 ). Use the codes below can reproduce the CPU freezing.

```
import time
from sklearn import datasets
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split, cross_val_score
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.pipeline import make_pipeline
X, y = datasets.make_classification(n_samples=10000, n_features=4000,
                                    n_informative=2, random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.90,
                                                    random_state=42)
svc = SVC(kernel=""linear"", C=1)
rfe = RFE(estimator=svc, step=1)
gnb = GaussianNB()
sklearn_pipeline = make_pipeline(rfe,gnb)

time_start= time.time()
sklearn_pipeline.fit(X_train, y_train) # freezing here
cv_scores = cross_val_score( sklearn_pipeline, X_train, y_train, cv=3, scoring=""accuracy"")
time_end = time.time()
print('Simulation Dataset:',time_start-time_end)

```
",operator cause freezing low usage like pipeline fitting large large number like issue use reproduce freezing import time import import import import import import import linear freezing accuracy print,issue,negative,positive,positive,positive,positive,positive
252726895,"I tested the dataset from the numer.ai website in my Linux platform and tracked the memory that the job used. The maximum memory it used was more than 2.5Gb (stdout below). So please use an instance with at least 3Gb free memory available to run TPOT on this dataset.

```

    Command being timed: ""tpot numerai_training_data.csv -is , -target target -o numerai_data_pipeline.py -g 5 -p 20 -cv 5 -s 42 -v 2 -maxeval 10""
    User time (seconds): 7212.62
    System time (seconds): 84.39
    Percent of CPU this job got: 113%
    Elapsed (wall clock) time (h:mm:ss or m:ss): 1:47:13
    Average shared text size (kbytes): 0
    Average unshared data size (kbytes): 0
    Average stack size (kbytes): 0
    Average total size (kbytes): 0
    Maximum resident set size (kbytes): 2545820
    Average resident set size (kbytes): 0
    Major (requiring I/O) page faults: 0
    Minor (reclaiming a frame) page faults: 4806497
    Voluntary context switches: 494145
    Involuntary context switches: 67974874
    Swaps: 0
    File system inputs: 0
    File system outputs: 64
    Socket messages sent: 0
    Socket messages received: 0
    Signals delivered: 0
    Page size (bytes): 4096
    Exit status: 0

```
",tested platform tracked memory job used maximum memory used please use instance least free memory available run command timed target user time system time percent job got wall clock time average text size average unshared data size average stack size average total size maximum resident set size average resident set size major page minor frame page voluntary context involuntary context file system file system socket sent socket received page size exit status,issue,negative,negative,neutral,neutral,negative,negative
252706154,"So there is a case when the calibrated classifier actually performs worse. I don't think that we should be the ones internally tracking the best way to predict probabilities. This is something the user should do. 

Thus, should we include a boolean flag inputted to predict_proba that allows the user to specify whether they want us to use the calibrated classifier or not?
",case classifier actually worse think internally best way predict something user thus include flag user specify whether want u use classifier,issue,negative,positive,positive,positive,positive,positive
252663774,"Were you able to solve this issue? Please reopen the issue if you need additional help.
",able solve issue please reopen issue need additional help,issue,positive,positive,positive,positive,positive,positive
252663736,"Please let us know if you have any additional questions/comments about this topic. Happy to reopen the issue and continue the conversation.
",please let u know additional topic happy reopen issue continue conversation,issue,positive,positive,positive,positive,positive,positive
252660565,"That warning is actually due to XGBoost importing the old version of `cross_validation`. Hopefully they fix it soon!
",warning actually due old version hopefully fix soon,issue,negative,negative,neutral,neutral,negative,negative
252498363,"I ran it on an Amazon Web Services EC2 t2.small (Linux) instance, which means it has 2GB of memory (See https://aws.amazon.com/ec2/instance-types/).
",ran web instance memory see,issue,negative,neutral,neutral,neutral,neutral,neutral
252497420,"I guess the issue was casued by out of stack or memory. For better understanding the issue, could you please let me know more details about the platform you used (e.g OS and maximum  memory size)?

You should not set a very large population size (like 200), which would cost much more computational resources than current one.
",guess issue stack memory better understanding issue could please let know platform used o maximum memory size set large population size like would cost much computational current one,issue,positive,positive,positive,positive,positive,positive
252491930,"I added the maxeval 10 option, 

tpot numerai_training_data.092916.csv -is , -target target -o numerai_data_pipeline.py  -g 5 -p 20 -cv 5 -s 42 -v 2 -maxeval 10

It ran for just over an hour, until it stopped - at 13%, outputting the word ""Killed""

TPOT settings:
CROSSOVER_RATE  =   0.05
GENERATIONS =   5
INPUT_FILE  =   numerai_training_data.092916.csv
INPUT_SEPARATOR =   ,
MAX_EVAL_MINS   =   10.0
MAX_TIME_MINS   =   None
MUTATION_RATE   =   0.9
NUM_CV_FOLDS    =   5
OUTPUT_FILE =   numerai_data_pipeline.py
POPULATION_SIZE =   20
RANDOM_STATE    =   42
SCORING_FN  =   balanced_accuracy
TARGET_NAME =   target
TPOT_MODE   =   classification
VERBOSITY   =   2

Timeout during evaluation of pipeline #1. Skipping to the next pipeline.  
Timeout during evaluation of pipeline #4. Skipping to the next pipeline.  
Timeout during evaluation of pipeline #5. Skipping to the next pipeline.  
Timeout during evaluation of pipeline #11. Skipping to the next pipeline.  
Timeout during evaluation of pipeline #12. Skipping to the next pipeline.  
Timeout during evaluation of pipeline #16. Skipping to the next pipeline.  
Optimization Progress:  13%|█████████████▋                                                                                         | 16/120 [1:12:21<9:18:51, 322.42s/pipeline]
Killed

I saw no files in the working directory - is there anywhere I can look for evidence? Any way to switch on logging?

I also am not sure about my option value for population size. I simply copied your value (20), but the total number of rows is 96321. Should I have set this value instead?

Regards

Colin Goldberg
",added option target ran hour stopped word none target classification verbosity evaluation pipeline skipping next pipeline evaluation pipeline skipping next pipeline evaluation pipeline skipping next pipeline evaluation pipeline skipping next pipeline evaluation pipeline skipping next pipeline evaluation pipeline skipping next pipeline optimization progress saw working directory anywhere look evidence way switch logging also sure option value population size simply copied value total number set value instead colin,issue,positive,positive,neutral,neutral,positive,positive
252425556,"Thank you for letting us know the question. The error is caused by no '-is' argument which let tpot know the character used to separate columns in the input file. For your case, the setting should be '-is ,'. After you need let TPOT know which column is your target with '-target' argument. Please check [the manual for command line](http://rhiever.github.io/tpot/using/#tpot-on-the-command-line) to customize other arguments for running TPOT (I think it may cause some confusions when reading the `tpot /path_to/data_file.csv` on the top of the webpage). The right example is listed below the argument table on that webpage and also for your case, a example is listed below herein. Please test the command in your case.

Example :

```
tpot numerai_training_data.092916.csv -is , -target target -o numerai_data_pipeline.py  -g 5 -p 20 -cv 5 -s 42 -v 2
## you can customize the setting (e.g. -o numerai_data_pipeline.py  -g 5 -p 20 -cv 5 -s 42 -v 2)
```

#Output before running pipeline evalutation 
TPOT settings:
CROSSOVER_RATE  =   0.05
GENERATIONS =   5
INPUT_FILE  =   numerai_training_data.csv
INPUT_SEPARATOR =   ,
MAX_EVAL_MINS   =   5
MAX_TIME_MINS   =   None
MUTATION_RATE   =   0.9
NUM_CV_FOLDS    =   5
OUTPUT_FILE =   numerai_data_pipeline.py
POPULATION_SIZE =   20
RANDOM_STATE    =   42
SCORING_FN  =   balanced_accuracy
TARGET_NAME =   target
TPOT_MODE   =   classification
VERBOSITY   =   2

BTW, I just found that your dataset was large so that it might be reasonable to customize the time limit of evaluating a pipeline with -maxeval (a new argument in the version 0.6.5, e.g. `-maxeval 10` means time limit is 10 minutes for evaluating a single pipeline). Note that the default time limit is 5 minutes for a single pipeline.
",thank u know question error argument let know character used separate input file case setting need let know column target argument please check manual command line running think may cause reading top right example listed argument table also case example listed herein please test command case example target setting output running pipeline none target classification verbosity found large might reasonable time limit pipeline new argument version time limit single pipeline note default time limit single pipeline,issue,negative,positive,positive,positive,positive,positive
252423136,"Thanks. This warning is normal (I think it should not affect the performance of running the current version of TPOT) because this old module will be removed in the coming version of scikit-learn. We are working on adding compatibility into TPOT for the version 0.20 of scikit-learn.
",thanks warning normal think affect performance running current version old module removed coming version working compatibility version,issue,negative,positive,positive,positive,positive,positive
252422423,"Thanks, that made a difference. I now get a deprecation warning running tpot.

/usr/local/lib64/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.

I hope this is useful.

Colin Goldberg
",thanks made difference get deprecation warning running module version favor module class also note interface new different module module removed hope useful colin,issue,positive,positive,positive,positive,positive,positive
252374692,"Thank you for letting us know this issue. I think it is related to the version of scikit learn. The module is included in the latest version (0.18) of scikit learn (it is related to this issue #284). Please update scikit learn with the command below and rerun TPOT with the latest version of scikit learn. 

```
conda update scikit-learn
```
",thank u know issue think related version learn module included latest version learn related issue please update learn command rerun latest version learn update,issue,positive,positive,positive,positive,positive,positive
252068836,"I fixed the `model_selection` references and `PCA` issues in [this commit](https://github.com/rhiever/tpot/commit/84c5e26b447251088826737612ccf0817ef43db2). Let me know if there remain any issues with sklearn compatibility.
",fixed commit let know remain compatibility,issue,negative,positive,neutral,neutral,positive,positive
251681171,"Sorry it's taken me forever to get to this. We will also need a unit test verifying that a pipeline with this imputer work -- both data sets with and without missing data (two separate tests).
",sorry taken forever get also need unit test pipeline imputer work data without missing data two separate,issue,negative,negative,negative,negative,negative,negative
251680597,"Please write unit tests that verify the functionality of this new method.
",please write unit verify functionality new method,issue,negative,positive,positive,positive,positive,positive
251551085,"It is not advised to use isotonic calibration with too few calibration samples (<<1000) since it tends to overfit. Use sigmoids (Platt’s calibration) in this case. Maybe we should set 'sigmoids' as default.

Also, calibrator still need fit even with cv = 'prefit' (why?) and the predict_proba is not exactly same as that from direct classier.

I found a [case](http://stackoverflow.com/questions/30285551/why-does-calibratedclassifiercv-underperform-a-direct-classifer) online. 
I revised a little their codes for testing isotonic method

Try the codes here:

```
from sklearn.datasets import make_classification
from sklearn import ensemble
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import log_loss
from sklearn import cross_validation

# Build a classification task using 3 informative features

X, y = make_classification(n_samples=1000,
                           n_features=100,
                           n_informative=30,
                           n_redundant=0,
                           n_repeated=0,
                           n_classes=9,
                           random_state=0,
                           shuffle=False)

skf = cross_validation.StratifiedShuffleSplit(y, 5)

for train, test in skf:

    X_train, X_test = X[train], X[test]
    y_train, y_test = y[train], y[test]

    clf = ensemble.GradientBoostingClassifier(n_estimators=50)
    clf.fit(X_train, y_train)
    probas = clf.predict_proba(X_test)
    clf_score = log_loss(y_test, probas)
    clf_cv1 = CalibratedClassifierCV(clf, cv=""prefit"", method='sigmoid')
    clf_cv1.fit(X_train, y_train)
    probas_cv1 = clf_cv1.predict_proba(X_test)
    cv_score1 = log_loss(y_test, probas_cv1)
    clf_cv2 = CalibratedClassifierCV(clf, cv=""prefit"", method='isotonic')
    clf_cv2.fit(X_train, y_train)
    probas_cv2 = clf_cv2.predict_proba(X_test)
    cv_score2 = log_loss(y_test, probas_cv2)
    print('calibrated score with method=\'sigmoid\' :', cv_score1)
    print('calibrated score with method=\'isotonic\' :',cv_score2)
    print('direct clf score:', clf_score)
```
",advised use isotonic calibration calibration since overfit use calibration case maybe set default also calibrator still need fit even exactly direct found case little testing isotonic method try import import ensemble import import import build classification task informative train test train test train test print score print score print score,issue,negative,positive,positive,positive,positive,positive
251480171,"got it, thanks for the quick reply
",got thanks quick reply,issue,negative,positive,positive,positive,positive,positive
251479891,"TPOT uses a stochastic pipeline optimization algorithm, so the results aren't guaranteed to be 100% the same each time you run it. This is why I typically recommend running multiple instances of TPOT to see what it finds.
",stochastic pipeline optimization algorithm time run typically recommend running multiple see,issue,positive,negative,neutral,neutral,negative,negative
251408173,"Please check the cv = ""prefit"" parameter in [CalibratedClassifierCV](http://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV). It shows that ""If “prefit” is passed, it is assumed that base_estimator has been fitted already and all data is used for calibration."" Is it what you are looking for?
",please check parameter assumed fitted already data used calibration looking,issue,negative,neutral,neutral,neutral,neutral,neutral
251395274,"@sahilshah1194 -- I think this will be another good one to take on. Shouldn't be too hard to switch over to sklearn's new `model_selection` module.
",think another good one take hard switch new module,issue,negative,positive,positive,positive,positive,positive
251123080,"Could be a nice convenience feature.
",could nice convenience feature,issue,negative,positive,positive,positive,positive,positive
250940868,"Yes, separate operations are a good idea. 
",yes separate good idea,issue,positive,positive,positive,positive,positive,positive
250936414,"```
    def predict_proba(self, features, classifier_type='isotonic'):
        """"""Uses the optimized pipeline to estimate the class probabilities for a feature set

        Parameters
        ----------
        features: array-like {n_samples, n_features}
            Feature matrix of the testing set
        classifier_type: string (default: 'isotonic')
            Type of classifier to be used when creating a CalibratedClassifier

        Returns
        -------
        array-like: {n_samples, n_classes}
            The class probabilities of the input samples

        """"""
        if not self._fitted_pipeline:
            raise ValueError('A pipeline has not yet been optimized. Please call fit() first.')
        else:
            if not(hasattr(self._fitted_pipeline, 'predict_proba')):
                #Would need to create calibrator object
                #Would need to re-fit data
            return self._fitted_pipeline.predict_proba(features.astype(np.float64))
```
",self pipeline estimate class feature set feature matrix testing set string default type classifier used class input raise pipeline yet please call fit first else would need create calibrator object would need data return,issue,positive,positive,positive,positive,positive,positive
250934282,"This makes sense. Although I'm not exactly sure what kind of calibrator to use. Should that be a parameter that they can pass in to predict_proba? Maybe give it a default value so that they can call it without a parameter. 

Also there becomes another problem. If I use the calibrator, I would need to call fit on the training data once again as per this example:

```
# Gaussian Naive-Bayes with no calibration
clf = GaussianNB()
clf.fit(X_train, y_train)  # GaussianNB itself does not support sample-weights
prob_pos_clf = clf.predict_proba(X_test)[:, 1]

# Gaussian Naive-Bayes with isotonic calibration
clf_isotonic = CalibratedClassifierCV(clf, cv=2, method='isotonic')
clf_isotonic.fit(X_train, y_train, sw_train)
prob_pos_isotonic = clf_isotonic.predict_proba(X_test)[:, 1]
```

predict_proba should not be calling the fit method - we should keep those as separate operations
",sense although exactly sure kind calibrator use parameter pas maybe give default value call without parameter also becomes another problem use calibrator would need call fit training data per example calibration support isotonic calibration calling fit method keep separate,issue,positive,positive,positive,positive,positive,positive
250916472,"I suggest to use ""if hasattr(clf, ""predict_proba""):"" to find out whether operator has the predict_proba object. Check this [Probability calibration](http://scikit-learn.org/stable/modules/calibration.html#probability-calibration). Maybe CalibratedClassifierCV can be helpful for Classifier. Not sure whether it is reasonable for regressor. 

Example codes:

```
import numpy as np
np.random.seed(0)

import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.svm import LinearSVC
from sklearn.calibration import calibration_curve
from sklearn.calibration import CalibratedClassifierCV

X, y = datasets.make_classification(n_samples=100000, n_features=20,
                                    n_informative=2, n_redundant=2)

train_samples = 100  # Samples used for training the models

X_train = X[:train_samples]
X_test = X[train_samples:]
y_train = y[:train_samples]
y_test = y[train_samples:]

svc = LinearSVC(C=1.0)
svc.fit(X_train, y_train)
print(svc.predict_proba(X_test)) ## AttributeError: 'LinearSVC' object has no attribute 'predict_proba'
clf = CalibratedClassifierCV(svc) 
clf.fit(X_train, y_train)
print(clf.predict_proba(X_test))  ## report probililty
```
",suggest use find whether operator object check probability calibration maybe helpful classifier sure whether reasonable regressor example import import import import import import used training print object attribute print report,issue,positive,positive,positive,positive,positive,positive
250850506,"Ok nevermind I see it now. Should I put this around a try statement and raise an exception? What do you think the best design is?
",see put around try statement raise exception think best design,issue,positive,positive,positive,positive,positive,positive
250844212,"import numpy as np
import matplotlib.pyplot as plt

from sklearn import ensemble
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error

boston = datasets.load_boston()
X, y = shuffle(boston.data, boston.target, random_state=13)
X = X.astype(np.float32)
offset = int(X.shape[0] \* 0.9)
X_train, y_train = X[:offset], y[:offset]
X_test, y_test = X[offset:], y[offset:]

params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,
          'learning_rate': 0.01, 'loss': 'ls'}
clf = ensemble.GradientBoostingRegressor(**params)

clf.fit(X_train, y_train)
mse = mean_squared_error(y_test, clf.predict(X_test))
print(""MSE: %.4f"" % mse)

print(clf.predict_proba(X_test))

Try the codes. Somehow I got this message

> > > print(""MSE: %.4f"" % mse)
> > > MSE: 6.7976
> > > 
> > > print(clf.predict_proba(X_test))
> > > Traceback (most recent call last):
> > >   File ""<stdin>"", line 1, in <module>
> > > AttributeError: 'GradientBoostingRegressor' object has no attribute 'predict_proba'
",import import import ensemble import import shuffle import boston shuffle offset offset offset offset offset print print try somehow got message print print recent call last file line module object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
250838420,"GradientBoostingRegressor does have a predict_proba. It seems like all of them do, from the conversation I linked and also additional verifications. 
",like conversation linked also additional,issue,negative,neutral,neutral,neutral,neutral,neutral
250811397,"Not all operators in scikit-learn has predict_proba (e.g 'GradientBoostingRegressor' object has no attribute 'predict_proba' ). Maybe you could add a exception for these operators.  
",object attribute maybe could add exception,issue,negative,neutral,neutral,neutral,neutral,neutral
250721049,"To the best of my knowledge they're using circle-ci to build the documentation. You can also find a simple example in the repository [scikit-learn-contrib](https://github.com/scikit-learn-contrib/project-template).
",best knowledge build documentation also find simple example repository,issue,positive,positive,positive,positive,positive,positive
250632928,"cookiecutter-pypackage defines a `[docs]` tox env:
- https://github.com/audreyr/cookiecutter-pypackage/blob/master/tox.ini

conda-docs now calls doctr to deploy from Travis-CI to GitHub Pages:
- https://github.com/conda/conda-docs/blob/master/.travis.yml
  - https://pypi.python.org/pypi/doctr (Python >= 3.5)
- [ ] Add tox.ini
  - https://tox.readthedocs.io/en/latest/
  - https://tox.readthedocs.io/en/latest/config.html
  - https://tox.readthedocs.io/en/latest/examples.html
  - [ ] tox.ini: Port or call https://github.com/rhiever/tpot/blob/master/ci/.travis_install.sh
  - [ ] tox.ini: Port or call https://github.com/rhiever/tpot/blob/master/ci/.travis_test.sh
- [ ] .travis.yml: call tox and then doctr
  - https://github.com/audreyr/cookiecutter-pypackage/blob/master/.travis.yml
",tox deploy python add port call port call call tox,issue,negative,neutral,neutral,neutral,neutral,neutral
250239962,"@mfeurer: We recently merged a PR with a basic implementation of this functionality for Windows, Linux, and Mac. You can see it [here](https://github.com/rhiever/tpot/blob/development/tpot/decorators.py#L75). We're still cleaning it up, but it's a good start.
",recently basic implementation functionality mac see still cleaning good start,issue,negative,positive,positive,positive,positive,positive
250197903,"I should say here that doing this actually breaks the `deap.Individual.from_string` function. I don't think it likes that there are multiple terminal sets that have the same terminal value (For example, both `IteratedPower` and `MaxDepth` contain the number `2`).

DEAP seems to expect that all terminal sets will be non-overlapping, so this PR might break other DEAP functionality as well.
",say actually function think multiple terminal terminal value example contain number expect terminal might break functionality well,issue,positive,neutral,neutral,neutral,neutral,neutral
249783256,"Just a hint, we use a softmax function in auto-sklearn to get probability estimates for the classifiers which don't support predict_proba, see [here](https://github.com/automl/auto-sklearn/blob/master/autosklearn/pipeline/implementations/util.py).
",hint use function get probability support see,issue,negative,neutral,neutral,neutral,neutral,neutral
249664594,"That sounds good, thanks for an excellent package!
",good thanks excellent package,issue,positive,positive,positive,positive,positive,positive
249663621,"We might have to drop the passive aggressive classifier for this issue then, which I'm not entirely opposed to. XGBoost seems to have `predict_prob()`.
",might drop passive aggressive classifier issue entirely opposed,issue,negative,neutral,neutral,neutral,neutral,neutral
249660431,"@rhiever , thanks for considering! I could find predict_proba for all the classifiers at https://github.com/rhiever/tpot/tree/master/tpot/operators/classifiers, except passive aggressive. Also, not sure about XG Boost
",thanks considering could find except passive aggressive also sure boost,issue,positive,positive,positive,positive,positive,positive
249644412,"@sahilshah1194, this might be a good issue to start with. Can you please look through all of the [classifier operators used in TPOT](https://github.com/rhiever/tpot/tree/master/tpot/operators/classifiers) in the sklearn docs to confirm that all of them have a `predict_prob()` method?
",might good issue start please look classifier used confirm method,issue,positive,positive,positive,positive,positive,positive
249643852,"Not currently, but this sounds like something we could add for the `TPOTClassifier` method. Can you confirm that all sklearn classifiers have a `predict_prob()` method? We would need to double-check that.
",currently like something could add method confirm method would need,issue,negative,neutral,neutral,neutral,neutral,neutral
249576651,"I agree with the one parameter idea. For the ease of use, if the parameter is set to specify classifiers/regressors, it can trigger a interface with a list of all the classifiers/regressors' names as well as their indexes and then let user to choose operators through their indexes . But I think it will be still not easy to use if the operators is more than 20+ in the future. Instead of interface, do you think a config file containing all these parameters (including ranges of params of operators if users want to specify them ) in _init_ is better? @rhiever 
",agree one parameter idea ease use parameter set specify trigger interface list well let user choose think still easy use future instead interface think file want specify better,issue,positive,positive,positive,positive,positive,positive
249178937,"It looks like you're trying to run the generated code, which has a placeholder `'PATH/TO/DATA/FILE'` for the filename. You'll have to switch that placeholder out for the real filename, or not run that bit of generated code (which is meant only to be an example anyway).
",like trying run code switch real run bit code meant example anyway,issue,negative,positive,positive,positive,positive,positive
248146885,"The linked example is clear and I think it is one of good solutions. But 
I think it may need many separate .py files for supporting many 
different system environments later, so I more like your suggestion 
about reading system variables.

On 9/19/2016 6:17 PM, Daniel wrote:

> Well, in the example I linked I think that just returning the function
> where you currently set the value of IMPLEMENTATION and removing the if
> statements before each implementation will be much cleaner. It might 
> not be
> clear that that is what I meant given it's just a small excerpt of 
> what the
> full decorators.py file would look like. But if you go the route of just
> reading system variables, then that should be cleaner still.
> 
> On Mon, Sep 19, 2016, 6:04 PM weixuanfu2016 notifications@github.com
> wrote:
> 
> > @teaearlgraycold https://github.com/teaearlgraycold Thanks for your
> > comments. I fixed the bug about sys.tracebacklimit and and now it is 
> > just
> > worked insides limitedTime function to prevent the program to 
> > produce too
> > much stdout. Indeed, the IMPLEMENTATION flag is a compromising 
> > solution to
> > use this timeout function in multiple platforms with different 
> > version of
> > Python. I think I need seek better solutions to deal with cross-platform
> > compatibility. Thanks for let me know the alternative way using sys. 
> > I will
> > start with this one.
> > 
> > —
> > You are receiving this because you were mentioned.
> > 
> > Reply to this email directly, view it on GitHub
> > https://github.com/rhiever/tpot/pull/274#issuecomment-248141655, 
> > or mute
> > the thread
> > 
> > https://github.com/notifications/unsubscribe-auth/ADISY_cINvGx_LLVfJ1heUzYWmRSekBmks5qrwbTgaJpZM4KA9kM
> > .
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub 
> https://github.com/rhiever/tpot/pull/274#issuecomment-248144709, or 
> mute the thread 
> https://github.com/notifications/unsubscribe-auth/AUG7KkNDsD4ci_F7h8-QhdaGMnZtTr1eks5qrwnxgaJpZM4KA9kM.

## 

---

Weixuan Fu
Bioinformatician
Bioinformatics Core, Institute for Biomedical Informatics, University of Pennsylvania
B101 Richards Building, 3700 Hamilton Walk, Philadelphia, PA 19104-6113
Email: weixuanf@mail.med.upenn.edu
",linked example clear think one good think may need many separate supporting many different system later like suggestion reading system wrote well example linked think function currently set value implementation removing implementation much cleaner might clear meant given small excerpt full file would look like go route reading system cleaner still mon wrote thanks fixed bug worked function prevent program produce much indeed implementation flag compromising solution use function multiple different version python think need seek better deal compatibility thanks let know alternative way start one reply directly view mute thread thread reply directly view mute thread fu core institute university building walk pa,issue,positive,positive,positive,positive,positive,positive
248144709,"Well, in the example I linked I think that just returning the function
where you currently set the value of IMPLEMENTATION and removing the if
statements before each implementation will be much cleaner. It might not be
clear that that is what I meant given it's just a small excerpt of what the
full decorators.py file would look like. But if you go the route of just
reading system variables, then that should be cleaner still.

On Mon, Sep 19, 2016, 6:04 PM weixuanfu2016 notifications@github.com
wrote:

> @teaearlgraycold https://github.com/teaearlgraycold Thanks for your
> comments. I fixed the bug about sys.tracebacklimit and and now it is just
> worked insides limitedTime function to prevent the program to produce too
> much stdout. Indeed, the IMPLEMENTATION flag is a compromising solution to
> use this timeout function in multiple platforms with different version of
> Python. I think I need seek better solutions to deal with cross-platform
> compatibility. Thanks for let me know the alternative way using sys. I will
> start with this one.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/274#issuecomment-248141655, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/ADISY_cINvGx_LLVfJ1heUzYWmRSekBmks5qrwbTgaJpZM4KA9kM
> .
",well example linked think function currently set value implementation removing implementation much cleaner might clear meant given small excerpt full file would look like go route reading system cleaner still mon wrote thanks fixed bug worked function prevent program produce much indeed implementation flag compromising solution use function multiple different version python think need seek better deal compatibility thanks let know alternative way start one reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
248141655,"@teaearlgraycold Thanks for your comments. I fixed the bug about sys.tracebacklimit and and now it is just worked insides limitedTime function to prevent the program to produce too much stdout. Indeed, the IMPLEMENTATION flag is a compromising solution to use this timeout function in multiple platforms with different version of Python. I think I need seek better solutions to deal with cross-platform compatibility. Thanks for let me know the alternative way using sys. I will start with this one. 
",thanks fixed bug worked function prevent program produce much indeed implementation flag compromising solution use function multiple different version python think need seek better deal compatibility thanks let know alternative way start one,issue,positive,positive,positive,positive,positive,positive
248131474,"I'd recommend you not use the IMPLEMENTATION variable as a flag for whether
you've found a successful implementation, and rather do something like this
for each function implementation:

https://gist.github.com/anonymous/47e6bed02f42871694c677d9931ee87f

Also, it may be preferable to keep each implementation in a seperate .py
file, and loop over each file, attempting to import the limitedTime
function from each one, and skipping to the next file if there is an import
error or some other exception.

Another nice alternative would be to find out what variables in the sys
module would tell you which implementation will work at runtime.

I'd recommend against setting sys.tracebacklimit = 0 permanently. IMO it
should be turned on and off around specific code.

On Mon, Sep 19, 2016 at 5:08 PM Coveralls notifications@github.com wrote:

> [image: Coverage Status] https://coveralls.io/builds/7957008
> 
> Coverage decreased (-3.06%) to 85.062% when pulling _a034568
> https://github.com/rhiever/tpot/commit/a0345683b1ccf3e2a0147eda8d51aaaf512c00a9
> on weixuanfu2016:timeout_func_ into _52c1182
> https://github.com/rhiever/tpot/commit/52c1182e9708426cc9e8e16686f948bdd74b1d1f
> on rhiever:development_.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/274#issuecomment-248127191, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/ADISY65RQKR0Sl0twwlCxVeNCotchouPks5qrvm9gaJpZM4KA9kM
> .
",recommend use implementation variable flag whether found successful implementation rather something like function implementation also may preferable keep implementation file loop file import function one skipping next file import error exception another nice alternative would find module would tell implementation work recommend setting permanently turned around specific code mon coverall wrote image coverage status coverage thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
247772671,"I will take a closer look at this after I get back from travels late next week. My first thought as to what's occurring is that TPOT is getting ""stuck"" on evaluating pipelines that take a long time on a data set with many features. This doesn't necessarily mean that TPOT will never finish running, but it may take a long time on data sets with thousands of records or features. Essentially, every TPOT ""tick"" is performing 3-fold cross-validation on the pipeline.

My best recommendation in the meantime is to run TPOT e.g. overnight and see if it moves past one of the pipelines it gets ""stuck"" on. If it's still sitting there on the same pipeline 8 hours later, then I will consider this a bug.

#249 is related to this issue. We're working on allowing users to place limits on how much time or memory each pipeline evaluation is allowed to take.
",take closer look get back late next week first thought getting stuck take long time data set many necessarily mean never finish running may take long time data essentially every tick pipeline best recommendation run overnight see past one stuck still sitting pipeline later consider bug related issue working place much time memory pipeline evaluation take,issue,negative,positive,neutral,neutral,positive,positive
247771738,"I think the Imputer should only be added to the primitive set if there are any missing values in the data set.
",think imputer added primitive set missing data set,issue,negative,negative,negative,negative,negative,negative
247771516,"Experimentally, we've found that (generally) `n_estimators=500` works well enough to get most of the gains that tree-based ensemble methods will achieve. Thus we fix `n_estimators` at a reasonably high value (considering both accuracy and time to build the model) and allow the learning rate to adapt around that. If you have evidence that this practice could be wrong, please let us know!
",experimentally found generally work well enough get gain ensemble achieve thus fix reasonably high value considering accuracy time build model allow learning rate adapt around evidence practice could wrong please let u know,issue,positive,negative,neutral,neutral,negative,negative
247426442,"Should the Imputer even be an option in the pipeline?  It seems like it would only be required when the fitting step beings if `np.isnan(features).any()`.
",imputer even option pipeline like would fitting step,issue,negative,positive,positive,positive,positive,positive
246829253,":+1:

2016-09-13 18:48 GMT+02:00 Randy Olson notifications@github.com:

> Closed #259 https://github.com/rhiever/tpot/issues/259.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/259#event-787856469, or mute the
> thread
> https://github.com/notifications/unsubscribe-auth/AAAB9M2ZInVKHQ0B6aW7B-0TGOyWkpwAks5qptP6gaJpZM4J0QEn
> .

## 

JJ
",randy closed thread reply directly view mute thread,issue,negative,neutral,neutral,neutral,neutral,neutral
246810564,"@rhiever @teaearlgraycold would the Imputer class handle the logic or would it go somewhere else?
",would imputer class handle logic would go somewhere else,issue,negative,neutral,neutral,neutral,neutral,neutral
246808577,"Unfortunately, same issue after upgrading to the latest version 

it takes about 3 minutes to get to 8/420, and then remains there for 45 minutes afterwards

![image](https://cloud.githubusercontent.com/assets/9681868/18489581/5fcc321e-79fe-11e6-8b91-f27d54d5574c.png)

i've made the code and dataset available here:

http://gleason.case.edu/webdata/tpot/
",unfortunately issue latest version get remains afterwards image made code available,issue,negative,positive,positive,positive,positive,positive
246746475,"The master branch has the latest stable release of TPOT, which is the same version that will install via pip. Hopefully the upgrade will fix your issues -- we made some major changes in 0.5 that should hopefully prevent TPOT from getting ""stuck.""
",master branch latest stable release version install via pip hopefully upgrade fix made major hopefully prevent getting stuck,issue,positive,positive,positive,positive,positive,positive
246742461,"in the instances where it works, the progress bar does move at a fairly
consistent pace.

in the failing ones, it will randomly get stuck at some percent (usually <
50%) and then never move.

i was a bit curious about the version numbers, i'm glad you mentioned it.

this github master branch doesn't contain the latest code?

because when i used ""pip install tpot"" and it pulled it from a distant
repo, it did in fact load a 0.6.x version, but installing directly from
github results in 0.4.x?

On Tue, Sep 13, 2016 at 6:30 PM, Randy Olson notifications@github.com
wrote:

> If you set verbosity=2 in the TPOT() instantiation call, does it look
> like it's getting stuck on one pipeline evaluation, or is it just taking a
> long time in general?
> 
> Also, I strongly recommend upgrading to the latest version of TPOT
> (0.6.4). That alone may fix your issue.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/271#issuecomment-246741170, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AJO7zAaUfr0zf4twYNhMFCfXjfXxuQKHks5qps-YgaJpZM4J7b9w
> .
",work progress bar move fairly consistent pace failing randomly get stuck percent usually never move bit curious version glad master branch contain latest code used pip install distant fact load version directly tue randy wrote set call look like getting stuck one pipeline evaluation taking long time general also strongly recommend latest version alone may fix issue thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
246741170,"If you set `verbosity=2` in the `TPOT()` instantiation call, does it look like it's getting stuck on one pipeline evaluation, or is it just taking a long time in general?

Also, I strongly recommend upgrading to the latest version of TPOT (0.6.4). That alone may fix your issue.
",set call look like getting stuck one pipeline evaluation taking long time general also strongly recommend latest version alone may fix issue,issue,negative,positive,positive,positive,positive,positive
246488586,"@rhiever are you sure that commit fixes this issue or was that a typo? 
",sure commit issue typo,issue,positive,positive,positive,positive,positive,positive
246365659,"It'd be ideal if it happened before passing it to FunctionTransformer, but this way works in the meantime. :-)
",ideal passing way work,issue,positive,positive,positive,positive,positive,positive
246238060,"Okay, cool~

Thanks for the quick turnaround!  Sorry it took me a couple days to update, I was airplaned for a bit @_@

Anyway!  I think I've addressed all your comments, `hof` is now `pareto_front` in all mentions, docs are updated (I think), tests are added (I think) and the one-liner is now a multi-liner.

The test only has a population of 1, though (is that a facile test?) because when I set it to 20 the unit test became realllllly slow.  It might be good for you to do a basic sanity check (I'm not sure what's necessarily sane in this world!) to make sure everything's fine.  Please look over the test at tests.py:199 as well to make sure my test logic is correct as well.

Let me know what you think!
",thanks quick turnaround sorry took couple day update bit anyway think think added think test population though facile test set unit test slow might good basic sanity check sure necessarily sane world make sure everything fine please look test well make sure test logic correct well let know think,issue,positive,positive,positive,positive,positive,positive
246121752,"A few things:
1. You'll need to add the Imputer to the preprocessors' `__init__.py` file so it is discovered by the operators module.
2. You should PR this onto the development branch.
3. You should add logic that makes it so the Imputer is always placed at the beginning of the pipeline if there are missing values in the input matrix. It probably doesn't ever need to be anywhere besides at the beginning of a pipeline, right?
",need add imputer file discovered module onto development branch add logic imputer always beginning pipeline missing input matrix probably ever need anywhere besides beginning pipeline right,issue,negative,positive,neutral,neutral,positive,positive
245982260,"well, yeah, obv. You said ""Before passing it to function transformer"" which seemed wrong to me, which is why I asked for an example ;)
",well yeah said passing function transformer wrong example,issue,negative,negative,negative,negative,negative,negative
245838273,"Hi, don't you rather want something like these:
- https://arxiv.org/abs/1605.07079
- https://scholar.google.com/citations?view_op=view_citation&hl=en&user=WrbJw1wAAAAJ&citation_for_view=WrbJw1wAAAAJ:d1gkVwhDpl0C

?

(Disclaimer: these are my colleagues)
",hi rather want something like disclaimer,issue,negative,neutral,neutral,neutral,neutral,neutral
245690885,"Cool!

I've thought a little about the long training time already. It looks like the research community has already been working on possible solutions. There's one that I think that is promising for the slowness issue.

I'm not too familiar with it yet, but there is a technique called [transfer learning](http://cs231n.github.io/transfer-learning/) that uses pre-trained models as a starting point. Caffe has something called a [model zoo](http://caffe.berkeleyvision.org/model_zoo.html) that offers models.

[This is a 2014 paper that explores this topic](http://arxiv.org/abs/1411.1792) and it looks like there's more [recent papers continuing to explore applications](https://www.google.com/search?q=transfer+learning+arxiv&oq=transfer+learning+arxiv)

After my quick literature search it looks like this technique may offer at least a partial solution to the slowness problem, though there were some caveats. Of course to build a system that exploits this would entail users downloading pre-trained models and then having the search algorithm fine tune them (perhaps ambitiously with an option to upload new models, thus offering a growing library of pre-trained models).
",cool thought little long training time already like research community already working possible one think promising issue familiar yet technique transfer learning starting point something model zoo paper topic like recent explore quick literature search like technique may offer least partial solution problem though course build system would entail search algorithm fine tune perhaps ambitiously option new thus offering growing library,issue,positive,positive,positive,positive,positive,positive
245668373,"Hi @matalab, did what I suggested resolve your issue? Do you see it is a major use case to disable XGBoost even if it's installed? Hopefully once we address #146 this will no longer be an issue.
",hi resolve issue see major use case disable even hopefully address longer issue,issue,positive,positive,neutral,neutral,positive,positive
245667841,"Great idea @slcott! We've explored this a bit with a summer student on a project called [DELFT](https://github.com/EpistasisLab/delft), but the project has stalled a bit since the student had to go back to school. I agree that it's quite promising to build a TPOT-like tool for designing deep neural networks, though. The primary challenge is that training deep neural networks on any reasonably sized benchmark data set takes a _very_ long time.
",great idea bit summer student project delft project bit since student go back school agree quite promising build tool designing deep neural though primary challenge training deep neural reasonably sized data set long time,issue,positive,positive,positive,positive,positive,positive
245667143,"@teaearlgraycold, can you please write up a quick patch to `master` for this?
",please write quick patch master,issue,negative,positive,positive,positive,positive,positive
245666826,"That's been my experience with t-SNE as well. @campbx, have you seen t-SNE used for non-visualization purposes?
",experience well seen used,issue,negative,neutral,neutral,neutral,neutral,neutral
245666346,"This code looks great! It's exactly what I had in mind for the issue. Thank you for coding it up.

> Also, I looked at driver.py (is this feature also meant to be accessible from the command line?) and thought I'd add something like this:

Warm start functionality unfortunately can't be made available to command-line users since the TPOT instance gets garbage collected as soon as the command-line call ends.

> The existing nosetests all seem to run fine, but I feel like I should add some others–I'm not 100% sure what to test in terms of the tpot lifecycle and when the stored variables should exist, so I'd appreciate some guidance on that.

I think we should make sure to update the unit tests at the very least. There's a test_init unit test that should check to make sure all variables assigned in init are properly assigned to a new TPOT instance (both default and user-provided params).

I think this PR should also add a new unit test that runs a short TPOTClassifier run (1 gen for 20 pop?) on the MNIST practice data set with `warm_start=True` and a fixed `random_state`, checks that the `_pop` and `hof` were assigned to something (e.g. `hof` should have at least one individual in it), runs another small run on that same TPOTClassifier instance (same data set, pop size, gens, different but fixed random state) and does the check again. That test will guarantee, at the very least, that the basic warm start functionality still works.

---

I also think we should consider renaming the hof variable throughout. Can you please change all references to `hof` to `pareto_front` throughout all files? (To make it easier: `hof` shouldn't be referenced in any of the operator files.)

Lastly, please make sure to update the docs (in this PR) with the new warm_start option.
",code great exactly mind issue thank also feature also meant accessible command line thought add something like warm start functionality unfortunately ca made available since instance garbage collected soon call seem run fine feel like add sure test exist appreciate guidance think make sure update unit least unit test check make sure assigned properly assigned new instance default think also add new unit test short run gen pop practice data set fixed assigned something least one individual another small run instance data set pop size gen different fixed random state check test guarantee least basic warm start functionality still work also think consider variable throughout please change throughout make easier operator lastly please make sure update new option,issue,positive,positive,positive,positive,positive,positive
245459218,"I have a little experience with T-SNE. I think T-SNE was primarily created as a visualization tool and it wasn't designed to be an alternative to PCA, for example like ICA is.
",little experience think primarily visualization tool designed alternative example like,issue,negative,positive,positive,positive,positive,positive
245457987,"+1 for github wiki. I think it would be easier to use for users like me to add edits too.
",think would easier use like add,issue,positive,neutral,neutral,neutral,neutral,neutral
245455953,"I think this is the problem on line 180 of `driver.py`: 

```
    input_data = np.recfromcsv(args.INPUT_FILE, delimiter=args.INPUT_SEPARATOR, dtype=np.float64)
```

And the fix

```
    input_data = np.recfromcsv(args.INPUT_FILE, delimiter=args.INPUT_SEPARATOR, dtype=np.float64, case_sensitive=True)
```

Background: http://docs.scipy.org/doc/numpy/user/basics.io.genfromtxt.html#validating-names

I submitted a PR: https://github.com/rhiever/tpot/pull/264
",think problem line fix background,issue,negative,neutral,neutral,neutral,neutral,neutral
245395106,"I think it's expected to achieve a training score of 1.0 if you run `fit` and `score` on the same training features and labels. Basically, you're training and testing on the same data in that case. If you run TPOT with `verbosity=2` (you can set this in the constructor when you create a new TPOT instance), then you will see the internal k-fold CV scores during the optimization process, which will probably be in the 0.8-0.9 range.

Another venue I would explore in the notebook is running TPOT for a short time period (as you have), and then running it for a longer time period (say 8 hours?) using the `max_time_mins` parameter. Will it find an even better pipeline for the data set?
",think achieve training score run fit score training basically training testing data case run set constructor create new instance see internal optimization process probably range another venue would explore notebook running short time period running longer time period say parameter find even better pipeline data set,issue,positive,positive,positive,positive,positive,positive
245373048,"I played around with the notebook. Trying to come up with ways to analyze the dataset. I'm wondering about overfit from tpot. The exported regressor has an r2 score of 1.0 when run on the training set. It still performs better on the test set than a simple DecisionTreeRegressor. Was thinking about adding a validation dataset. Maybe this is is overkill for a tutorial. Thoughts?

[Boston Housing Prices.ipynb.zip](https://github.com/rhiever/tpot/files/460012/Boston.Housing.Prices.ipynb.zip)
",around notebook trying come way analyze wondering overfit regressor score run training set still better test set simple thinking validation maybe tutorial boston housing,issue,negative,positive,positive,positive,positive,positive
245337213,"Awesome, you're right @absolutelyNoWarranty! The code below works:

``` python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.datasets import make_classification

features, targets = make_classification(n_classes=10, n_informative=20, n_features=100, n_samples=1000)

def identity(x):
    return x

def subset(x):
    return x[0]

clf = make_pipeline(make_union(make_pipeline(
            VotingClassifier([('rf1', RandomForestClassifier())], voting='soft'),
            FunctionTransformer(subset, validate=False)),
                               VotingClassifier([('rf1', RandomForestClassifier())], voting='hard'),
                               FunctionTransformer(identity)),
                    RandomForestClassifier())

clf.fit(features, targets)
print(clf.score(features, targets))
```

Extra proof:

``` python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.datasets import make_classification

features, targets = make_classification(n_classes=10, n_informative=20, n_features=100, n_samples=1000)

def subset(x):
    return x[0]

clf = make_pipeline(VotingClassifier([('rf1', RandomForestClassifier())], voting='soft'),
                    FunctionTransformer(subset, validate=False))

clf.fit(features, targets)
print(clf.transform(features).shape)
```

cc @amueller 
",awesome right code work python import import import import identity return subset return subset identity print extra proof python import import import import subset return subset print,issue,positive,positive,positive,positive,positive,positive
244614481,"Hi!  So I made a crack at this after v0.6 appeared to push up, here's what I've got so far:

https://github.com/kamalasaurus/tpot/commit/e8f47f997255250043946f55a445595bb54cdf49

I'm storing/not-rewriting pop and _hof in the case of a warm_start; I -think- that was the general plan.

I've only made additions to tpot/base.py .  The existing nosetests all seem to run fine, but I feel like I should add some others–I'm not 100% sure what to test in terms of the tpot lifecycle and when the stored variables should exist, so I'd appreciate some guidance on that.  Would it be easier to do in the form of a Pull Request?

Also, I looked at driver.py (is this feature also meant to be accessible from the command line?) and thought I'd add something like this:

``` python
    parser.add_argument('--warm-start', action='store', dest='WARM_START', default=False,
        choices=[True, False], type=bool, help='use previously generated models')
```

But I'm not entirely sure if that makes any sense (can the command line pass boolean arguments?  Wouldn't the api be better being something like `-ws` which if added will flag `warm_start` to `True`?) so yeah, some guidance there would also be appreciated as well.  I'm not 100% clear on how tpot is called from the command line either...  does it boot up a new python process whenever you do so?  In which case, is warm_start even possible in that context?

So!  That's where I'm at, hopefully I didn't miss the boat completely!

Let me know what you think at your convenience!
",hi made crack push got far pop case general plan made seem run fine feel like add sure test exist appreciate guidance would easier form pull request also feature also meant accessible command line thought add something like python true false previously entirely sure sense command line pas would better something like added flag true yeah guidance would also well clear command line either boot new python process whenever case even possible context hopefully miss boat completely let know think convenience,issue,positive,positive,positive,positive,positive,positive
244586138,"Thanks!

2016-09-03 15:11 GMT+02:00 Randy Olson notifications@github.com:

> This is one of those, ""Well, it _shouldn't_ be doing that..."" bugs. Will
> look into it soon. Thank you for filing it!
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/259#issuecomment-244545591, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AAAB9P0UmSQMsH9RfUjwkooIwQVTH3e0ks5qmXIJgaJpZM4J0QEn
> .

## 

JJ
",thanks randy one well look soon thank filing thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
244573540,"You can specify the F1 score as a string. These are your options (emphasis mine):

'accuracy', 'adjusted_rand_score', 'average_precision', **'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted'**, 'log_loss', 'mean_absolute_error', 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc'
",specify score string emphasis mine,issue,negative,neutral,neutral,neutral,neutral,neutral
244545862,"We're trying to figure out the best way to allow the user to limit the classifiers optimized over (see #146), but it's been a bit of an interface challenge. Please feel free to drop ideas on #146, and if it's an easy enough solution, we can aim to get that feature in the 0.7 release in 2 weeks.

In the meantime, if you absolutely don't want XGBoost in your TPOT pipelines, you have two options:

1) Uninstall XGBoost

2) Remove the following lines from your TPOT install:

[Lines 34-37](https://github.com/rhiever/tpot/blob/master/tpot/operators/classifiers/__init__.py#L34) and [lines 29-32](https://github.com/rhiever/tpot/blob/master/tpot/operators/regressors/__init__.py#L29).
",trying figure best way allow user limit see bit interface challenge please feel free drop easy enough solution aim get feature release absolutely want two remove following install,issue,positive,positive,positive,positive,positive,positive
244545591,"This is one of those, ""Well, it _shouldn't_ be doing that..."" bugs. Will look into it soon. Thank you for filing it!
",one well look soon thank filing,issue,positive,neutral,neutral,neutral,neutral,neutral
244529921,"Thanks!

2016-09-02 22:28 GMT+02:00 Randy Olson notifications@github.com:

> Hi @JJ https://github.com/JJ, we just released TPOT v0.6 today. Try
> upgrading TPOT via pip and using it on your regression data set. Usage
> docs: link http://rhiever.github.io/tpot/using/
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/255#issuecomment-244479707, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AAAB9FiteW6XdDlgFDWLNjnJAwKTpenHks5qmIb2gaJpZM4Jy6U-
> .

## 

JJ
",thanks randy hi today try via pip regression data set usage link reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
244489022,"Awesome! Thanks.
On Fri, 2 Sep 2016 at 22:29, Randy Olson notifications@github.com wrote:

> Hi @dbikard https://github.com/dbikard, we just released TPOT v0.6
> today with regression support. Try upgrading TPOT via pip and using it on
> your regression data set. Usage docs: link
> http://rhiever.github.io/tpot/using/
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/186#issuecomment-244479960, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AAl3Er6skb_L4T7hfee964eB4QkleecVks5qmIdBgaJpZM4I9i1e
> .
",awesome thanks randy wrote hi today regression support try via pip regression data set usage link reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
244479960,"Hi @dbikard, we just released TPOT v0.6 today with regression support. Try upgrading TPOT via pip and using it on your regression data set. Usage docs: [link](http://rhiever.github.io/tpot/using/)
",hi today regression support try via pip regression data set usage link,issue,negative,neutral,neutral,neutral,neutral,neutral
244479829,"Hi @KeithBrodie, we just released TPOT v0.6 today. Try upgrading TPOT via pip and using it on your regression data set. Usage docs: [link](http://rhiever.github.io/tpot/using/)
",hi today try via pip regression data set usage link,issue,negative,neutral,neutral,neutral,neutral,neutral
244477024,"I've added a [very basic example](http://rhiever.github.io/tpot/examples/Boston_Example/) using the Boston data set to the docs. Feel free to expand upon it.
",added basic example boston data set feel free expand upon,issue,positive,positive,positive,positive,positive,positive
244472888,"Sign me up. 

Let's see how well TPOT does.
",sign let see well,issue,negative,neutral,neutral,neutral,neutral,neutral
244470013,"Awesome! Just wrapping up the v0.6 release today, so will take a deep look at this PR once that's rolled out to master.
",awesome wrapping release today take deep look rolled master,issue,positive,positive,positive,positive,positive,positive
244467814,"I think I finished all the edit requests. Tests are passing on my machine now.

```
Ran 82 tests in 11.678s

OK
```
",think finished edit passing machine ran,issue,negative,neutral,neutral,neutral,neutral,neutral
244402297,"Happy to hop on Google Hangout or Skype for a few minutes to explain. :-)
",happy hop hangout explain,issue,positive,positive,positive,positive,positive,positive
244402156,"For now, we encode string features as integers in the TPOT pipeline representation. Then we use the `preprocess_args` function to transform that integer into a corresponding string. See [here](https://github.com/rhiever/tpot/blob/master/tpot/operators/classifiers/logistic_regression.py#L49) for an example.
",encode string pipeline representation use function transform integer corresponding string see example,issue,negative,neutral,neutral,neutral,neutral,neutral
244395483,"I think I know where the error came from. 

Another issue I have. I don't know how to treat the arguments to the `TPOTOneHotEncoder` method `preprocess_args`.

The internal `_OneHotEncoder' I inserted uses a string in the constructor. 

```
  def __init__(self, categorical_features=""all"", dtype=np.float,
                 sparse=True, minimum_fraction=None):
        self.categorical_features = categorical_features
        self.dtype = dtype
        self.sparse = sparse
        self.minimum_fraction = minimum_fraction
```

Is it important that `arg_types` tuple not contain `string`? `categorical_features` has no concrete type. 

```
    Parameters
    ----------

    categorical_features: ""all"" or array of indices or mask
        Specify what features are treated as categorical.

        - 'all' (default): All features are treated as categorical.
        - array of indices: Array of categorical feature indices.
        - mask: Array of length n_features and with dtype=bool.
```
",think know error came another issue know treat method internal inserted string constructor self sparse important contain string concrete type array index mask specify categorical default categorical array index array categorical feature index mask array length,issue,negative,positive,positive,positive,positive,positive
244388140,"Can you try using the [ZeroCount](https://github.com/rhiever/tpot/blob/master/tpot/operators/preprocessors/zero_count.py) preprocessor as a template? Maybe that will help with the build errors too.
",try template maybe help build,issue,negative,neutral,neutral,neutral,neutral,neutral
244298436,"Sorry, didn't come so far. Thanks for the answer.
",sorry come far thanks answer,issue,negative,negative,neutral,neutral,negative,negative
244295825,"I'm not planning to remove support for custom code from auto-sklearn, that should still be possible through a plug-in like mechanism. What I want is to remove custom ML code from auto-sklearn as much as possible to make the package lean. ML code should reside in packages built for that, like scikit-learn or xgboost.

In the long run, the OneHotEncoder in its current form will be removed and replaced by a feature union, which allows a separate processing flow for categorical and numerical attributes. I'm not sure though what this means for the feature which removes seldom values and replaces them by an 'other' indicator. My advice is to use this 'other' indicator, it proved helpful for datasets where categorical attributes have lots of values. Before we had that, auto-sklearn would very often choose FeatureAgglomeration for feature preprocessing to reduce the number of features again.
",remove support custom code still possible like mechanism want remove custom code much possible make package lean code reside built like long run current form removed feature union separate flow categorical numerical sure though feature seldom indicator advice use indicator proved helpful categorical lot would often choose feature reduce number,issue,positive,positive,neutral,neutral,positive,positive
244294522,"@JJ , @rhiever just said:

> If not, beware: We won't support regression problems until TPOT 0.6 (to be released tomorrow).

You're problem looks like your ""class"" column is numeric. You will have to wait for the new TPOT release.
",said beware wo support regression tomorrow problem like class column wait new release,issue,negative,positive,positive,positive,positive,positive
244264613,"PR from/into both on development branch correctly this time.

Also, added the unit test from auto-sklearn. Right now, it's just testing the internal auto-sklearn class. 

I assume the goal is to test out the new class as well.
",development branch correctly time also added unit test right testing internal class assume goal test new class well,issue,negative,positive,positive,positive,positive,positive
244263574,"I'm gonna close this PR as suggested and submit another [located at](https://github.com/rhiever/tpot/pull/256)
",gon na close submit another,issue,negative,neutral,neutral,neutral,neutral,neutral
244242705,"Yes, that's right. If you commit against the branch that you sent the PR from, then they will automatically update on the PR. Those unit tests look like good ones to add.
",yes right commit branch sent automatically update unit look like good add,issue,positive,positive,positive,positive,positive,positive
244184489,"It looks like auto-sklearn OHE test is located [here](https://github.com/automl/auto-sklearn/blob/6acc1ae49727c22ecef30aacda8beccea7632f07/test/test_pipeline/implementations/test_OneHotEncoder.py) 

This is the only sklearn import it uses: `
from sklearn.utils.testing import assert_array_almost_equal`

I changed the PR target to `development`, but didn't create a new PR. Does that work correctly? I think I should have been working off the `development` branch on my machine, but I only added new files so I don't think this will cause a problem. 

[It looks like I can add more commits to my branch and in the unit test](http://stackoverflow.com/a/9790530/629014). Still getting used to this PR thing.
",like test import import target development create new work correctly think working development branch machine added new think cause problem like add branch unit test still getting used thing,issue,positive,positive,positive,positive,positive,positive
244182272,"Also, please close this PR and send a new PR to the `development` branch. The `master` branch is reserved for the latest stable release of TPOT.
",also please close send new development branch master branch reserved latest stable release,issue,positive,positive,positive,positive,positive,positive
244182118,"@slcott, I'm 👍 on using the unit tests for @mfeurer's OHE implementation and putting them into `tests.py`. I think it will be important to maintain unit tests for new features such as this one.
",unit implementation think important maintain unit new one,issue,negative,positive,positive,positive,positive,positive
244178221,"Good idea @mfeurer. Do you have any advice for using this custom version of the OHE? I know you mentioned dropping support for non-sklearn classes in auto-sklearn. Do you plan to drop this custom OHE from auto-sklearn?
",good idea advice custom version know dropping support class plan drop custom,issue,positive,positive,positive,positive,positive,positive
244177479,"Hi @JJ! You would set the file up as follows:

```
DayOfWeek,Col1,Col2,Col3,class
Wednesday,10,2,3,56
Sunday,10,0,4,67
Wednesday,10,1,14,298
```

Naming the column `class` tells TPOT that that is the column to predict. I assume that this is a classification problem? If not, beware: We won't support regression problems until TPOT 0.6 (to be released tomorrow).

Also make sure to encode all strings as numerical equivalents, else scikit-learn won't be able to work with the data. 
",hi would set file col col col class naming column class column predict assume classification problem beware wo support regression tomorrow also make sure encode numerical else wo able work data,issue,negative,positive,positive,positive,positive,positive
244126906,"You maybe want to copy the tests from auto-sklearn as well to ensure it stays functional.
",maybe want copy well ensure stay functional,issue,positive,neutral,neutral,neutral,neutral,neutral
244126529,"I followed your comments and created a pull request of the product. 

Honestly, it's my first PR so I hope it works.

I'm still getting used to the library so I haven't tested the code yet. Some guidance on that would help me make that for following some best practices.
",pull request product honestly first hope work still getting used library tested code yet guidance would help make following best,issue,positive,positive,positive,positive,positive,positive
244078497,"OK, I will keep you posted if we come up with anything.
",keep posted come anything,issue,negative,neutral,neutral,neutral,neutral,neutral
244074297,"I don't know of any solution other than the pynisher or writing it yourself. If I knew of, I would use it. To learn more about this, you should ask @sfalkner who wrote the pynisher.
",know solution writing knew would use learn ask wrote,issue,negative,neutral,neutral,neutral,neutral,neutral
244063535,"Thank you for the tip @mfeurer! I was checking that package out last week and noticed the non-Windows limitation as well. Have you run across any promising ideas for limiting evaluations similar to pynisher that will work on all platforms?
",thank tip package last week limitation well run across promising limiting similar work,issue,positive,positive,neutral,neutral,positive,positive
243991431,"If you don't want to write that yourself, have a look at the [pynisher package](https://github.com/sfalkner/pynisher). It doesn't run on windows though.
",want write look package run though,issue,negative,neutral,neutral,neutral,neutral,neutral
243952850,"Hi, just wanted to check in on this. Do you plan to add t-SNE? We can slot it for the 0.7 release in a couple weeks.
",hi check plan add slot release couple,issue,negative,neutral,neutral,neutral,neutral,neutral
243939107,"@teaearlgraycold, can you please write up a script here to pull all operators in TPOT and pass a sparse matrix to each of them individually? I'd like to see what operators we need to work on to natively support sparse matrices.
",please write script pull pas sparse matrix individually like see need work natively support sparse matrix,issue,positive,neutral,neutral,neutral,neutral,neutral
243937518,"OK. Can you please add the `XGBRegressor` for our `TPOTRegressor` class as well?
",please add class well,issue,positive,neutral,neutral,neutral,neutral,neutral
243937015,"Happy to hear you'd like to contribute to our project, @slcott! That's a good start toward implementing the OHE in TPOT. Perhaps we'd change the `sparse=True` setting to `sparse=False` for now until we have full support for sparse matrices in TPOT (see #29).

@teaearlgraycold could also chime in on this implementation. I think we could simply copy the auto-sklearn custom OHE implementation into TPOT and import that internally. Of course, we would have to make sure that we respect their license and attribution terms.
",happy hear like contribute project good start toward perhaps change setting full support sparse matrix see could also chime implementation think could simply copy custom implementation import internally course would make sure respect license attribution,issue,positive,positive,positive,positive,positive,positive
243922217,"I'd like to help out with this one. I'm a little new to data science, about to complete an udacity machine learning nanodegree, worked for a few months doing software development for a machine learning startup. Udacity recommended I make contributions to github projects. 

I read over deap and tpot at a high level for a few hours and the examples to get a big picture of how it works. I'm a little confused about how to integrate auto-sklearn's implementation of OneHotEncoder functionality that you would like.

Here's a quick draft. I used nystroem.py as  a template to match style.

```
from .base import Preprocessor
from sklearn.preprocessing import OneHotEncoder


class TPOTOneHotEncoder(Preprocessor):
    """"""Uses scikit-learn's OneHotEncoder to transform the feature set

    Parameters
    ----------
    None

    """"""
    import_hash = {'sklearn.preprocessing': ['OneHotEncoder']}
    sklearn_class = OneHotEncoder
    arg_types = (float, )

    def __init__(self):
        pass

    def preprocess_args(self, categorical_features=""all"", dtype=np.float,
                 sparse=True, minimum_fraction=None): #, kernel, gamma, n_components):

        # kernel_types = ['rbf', 'cosine', 'chi2', 'laplacian', 'polynomial', 'poly', 'linear', 'additive_chi2', 'sigmoid']
        # kernel_name = kernel_types[kernel % len(kernel_types)]

        # n_components = max(1, n_components)

        return {
            'categorical_features': categorical_features,
            'dtype': dtype,
            'sparse': sparse,
            'minimum_fraction': minimum_fraction,
            # 'gamma': gamma,
            # 'n_components': n_components
        }
```
",like help one little new data science complete machine learning worked development machine learning make read high level get big picture work little confused integrate implementation functionality would like quick draft used template match style import import class transform feature set none float self pas self kernel gamma kernel return sparse gamma,issue,positive,negative,neutral,neutral,negative,negative
243921937,"Resolved. ""Class"" versus ""class"" for the response column name in the CSV file.
",resolved class versus class response column name file,issue,negative,neutral,neutral,neutral,neutral,neutral
243915183,"MDR-SampleData runs fine, so my data should too. I'll try it again.

bwhite@login:~/analysis/TPOT_tests$ tail -f slurm-2386.out
Number of parallel processes: 32
Running TPOT example...

TPOT settings:
CROSSOVER_RATE  =   0.05
GENERATIONS =   5
INPUT_FILE  =   MDR-SampleData.csv
INPUT_SEPARATOR =   ,
MUTATION_RATE   =   0.9
NUM_CV_FOLDS    =   5
OUTPUT_FILE =   tpot_exported_pipeline.py
POPULATION_SIZE =   20
RANDOM_STATE    =   42
SCORING_FN  =   balanced_accuracy
VERBOSITY   =   2

Generation 1 - Current best internal CV score: 0.552890116002
Generation 2 - Current best internal CV score: 0.556965576831
Generation 3 - Current best internal CV score: 0.589809609884
Generation 4 - Current best internal CV score: 0.624165243127
Generation 5 - Current best internal CV score: 0.643017539329

Best pipeline: DecisionTreeClassifier(RandomizedPCA(MinMaxScaler(StandardScaler(input_matrix)), 2))

Training accuracy: 1.0
Holdout accuracy: 0.678571428571
",fine data try login tail number parallel running example verbosity generation current best internal score generation current best internal score generation current best internal score generation current best internal score generation current best internal score best pipeline training accuracy holdout accuracy,issue,positive,positive,positive,positive,positive,positive
243910742,"Sorry, the predictors are SNPs, so integers. I also want to test RNA-Seq, which is counts/integers, with both discrete and continuous responses.
",sorry also want test discrete continuous,issue,negative,negative,negative,negative,negative,negative
243910118,"bwhite@login:~/analysis/TPOT_tests$ python -c ""import tpot; print('tpot %s' % tpot.**version**)""
tpot 0.5.2

As I mentioned in the issue description:

Submitted a run of TPOT on a genetics data set of binary predictors and a binary response to our cluster. The data set is 89 subjects/rows by 3925 predictors.

I missed the scikit-learn  requirement for the data, but looking through all your examples and scikit-learn docs, I can make the data compatible: http://skll.readthedocs.io/en/latest/run_experiment.html

Thanks!
",login python import print version issue description run genetics data set binary binary response cluster data set requirement data looking make data compatible thanks,issue,negative,positive,positive,positive,positive,positive
243897503,"No. None of the operators are explicitly tested at the moment.
",none explicitly tested moment,issue,negative,neutral,neutral,neutral,neutral,neutral
243897224,"Is this new operator automatically tested in the unit tests?
",new operator automatically tested unit,issue,negative,positive,positive,positive,positive,positive
243895926,"What version of TPOT are you running? Run on the command line:

`python -c ""import tpot; print('tpot %s' % tpot.__version__)""`

My first thought is that this is a data issue. Is the column being predicted discrete or continuous? Are all of the columns in ""scikit-learn-compatible"" format (i.e., all numerical)?
",version running run command line python import print first thought data issue column discrete continuous format numerical,issue,negative,positive,positive,positive,positive,positive
243639958,"[![Coverage Status](https://coveralls.io/builds/7680789/badge)](https://coveralls.io/builds/7680789)

Coverage increased (+1.2%) to 88.571% when pulling **387e8e1268bea230edae7cc6808b69e140f2be5b on teaearlgraycold:regression** into **a369227152a9ad3e5e896fc1d3ee3397bd545f63 on rhiever:development**.
",coverage status coverage regression development,issue,negative,neutral,neutral,neutral,neutral,neutral
243587834,"Looks like there are merge conflicts -- can you please look into it and try to resolve them?
",like merge please look try resolve,issue,positive,neutral,neutral,neutral,neutral,neutral
243197726,"For now, we decided to do this in the `_evaluate_individual` function. We will:

1) Compile the DEAP representation to a sklearn pipeline object

2) Recurse through the sklearn objects in the pipeline, use `hasattr('random_state', obj)` to determine if it has `random_state`, and set `random_state` to some fixed number.
",decided function compile representation pipeline object recurse pipeline use determine set fixed number,issue,negative,positive,neutral,neutral,positive,positive
243150138,"We still waiting on unit tests for this? Also looks like we have a merge conflict now.
",still waiting unit also like merge conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
243007184,"Per your note, I've removed the input validation, and updated some of the info regarding scoring functions in the docs. 
",per note removed input validation regarding scoring,issue,negative,neutral,neutral,neutral,neutral,neutral
242956281,"We're already adding code to tell users what went wrong when all pipelines fail to evaluate.

As is standard with Python, you just have to trust the user will pass valid objects as parameters.
",already code tell went wrong fail evaluate standard python trust user pas valid,issue,negative,negative,negative,negative,negative,negative
242946330,"I made that decision because that was how I interpreted #238 and the decision to restrict possible functions to those supported by cross_val_score. I can revert that change (and will since it seems to break the build around fetching the params). 

I guess the question is, what do we allow then? We can go completely the way of `cross_val_score()` and allow both the appropriate strings and all callables with the signature `scorer(estimator, X, y)`. However, since we don't actually evaluate the scoring_function until we call `cross_val_score()` within `_evaluate_individual()`, any malformed scoring_functions will invalidate every pipeline (since we have that catchall Exception handler). What do you think would be elegant for this? Is a check for three args going to suffice?
",made decision decision restrict possible revert change since break build around fetching guess question allow go completely way allow appropriate signature scorer estimator however since actually evaluate call within malformed invalidate every pipeline since catchall exception handler think would elegant check three going suffice,issue,positive,positive,positive,positive,positive,positive
242934791,"Why restrict scoring_function to strings? We already need to use functions for scoring due to balanced_accuracy.
",restrict already need use scoring due,issue,negative,negative,negative,negative,negative,negative
242197980,"Hmmm, no dice. The pipeline complains about the 3-dimensional output before even passing it to the FunctionTransformer:

``` python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.datasets import make_classification

features, targets = make_classification(n_classes=10, n_informative=20, n_features=100, n_samples=1000)

def identity(x):
    return x

def subset(x):
    return x[0]

clf = make_pipeline(make_union(make_pipeline(
            VotingClassifier([('rf1', RandomForestClassifier())], voting='soft'),
            FunctionTransformer(subset)),
                               VotingClassifier([('rf1', RandomForestClassifier())], voting='hard'),
                               FunctionTransformer(identity)),
                    RandomForestClassifier())
```
",dice pipeline output even passing python import import import import identity return subset return subset identity,issue,negative,neutral,neutral,neutral,neutral,neutral
242149100,"Sounds good to me @bartleyn -- please roll it out as a patch to the dev branch.

Although preferably, let's try to avoid relying on parsingdocstrings to find out whether to maximize or minimize. Maybe we can hard-code the multiplier based on the known functions that `cross_val_score` supports. 
",good please roll patch dev branch although preferably let try avoid find whether maximize minimize maybe multiplier based known,issue,negative,positive,positive,positive,positive,positive
242125850,"The issue is solved, in that if I send data to it in the proper format, it behaves correctly :)
",issue send data proper format correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
242122487,"@rhiever There is a change of mine that got lost in the 0.5.0 release that accounts for the loss functions, which I think might be (at least) part of the reason for this. I agree that there should maybe be limits on what scoring functions TPOT uses, but I can put this back in if you like. 
",change mine got lost release loss think might least part reason agree maybe scoring put back like,issue,negative,negative,negative,negative,negative,negative
242119326,"Docs are updated with the latest push. Sorry for the confusion!
",latest push sorry confusion,issue,negative,neutral,neutral,neutral,neutral,neutral
242111458,"Oh duh, I knew that. Was clearly over tired yesterday.
",oh knew clearly tired yesterday,issue,negative,negative,negative,negative,negative,negative
242110484,"👍 I like the clean layout of bootstrap sphinx. Thank you for the suggestion @mfeurer!
",like clean layout bootstrap sphinx thank suggestion,issue,positive,positive,positive,positive,positive,positive
242109800,"Checking: Are you using TPOT for a regression problem? TPOT doesn't currently support regression, but we hope to do so in the near future (0.6).

Thank you for reporting this issue! This made me realize that we should place a limit on what scoring functions that TPOT accepts at the moment, as log loss is not appropriate for classification problems.
",regression problem currently support regression hope near future thank issue made realize place limit scoring moment log loss appropriate classification,issue,negative,positive,positive,positive,positive,positive
242109012,"Closing this issue since it's already open at #235. Please report if TPOT 0.5.1 fixed this issue for you there.
",issue since already open please report fixed issue,issue,negative,positive,neutral,neutral,positive,positive
241979473,"When using sphink, you could use [bootstrap sphinx](https://ryan-roemer.github.io/sphinx-bootstrap-theme/). It makes the sphinx output look like a modern html5 website. That's what we use for [auto-sklearn](http://automl.github.io/auto-sklearn/stable/)
",could use bootstrap sphinx sphinx output look like modern use,issue,negative,positive,positive,positive,positive,positive
241921463,"Greetings @rhiever! 

Issue is fixed in v0.5.1:

https://app.dominodatalab.com/u/earino/tpot_reprex/runs/57bcdfed489465196851cb58

With the following packages installed:
- tpot-0.5.1 
- deap-1.0.2 
- tqdm-4.8.4 
- update-checker-0.12

Thanks for being so responsive! Now to actually get some TPOTing!
",issue fixed following thanks responsive actually get,issue,positive,positive,neutral,neutral,positive,positive
241892430,"no, pipelines don't require a predict at the end. You should listen to one of my talks ;P
",require predict end listen one,issue,negative,neutral,neutral,neutral,neutral,neutral
241880995,"Hi @earino, try giving the TPOT v0.5.1 release a try (just now uploaded to PyPi) and please let us know if that fixed this issue. You might want to try it with a shorter TPOT run just to make sure it runs to completion and outputs everything as expected.
",hi try giving release try please let u know fixed issue might want try shorter run make sure completion everything,issue,positive,positive,positive,positive,positive,positive
241877342,"FunctionTransformers only have transform. Pipelines require a predict at the end. Need some sort of ""TransformerPipeline"".
",transform require predict end need sort,issue,negative,neutral,neutral,neutral,neutral,neutral
241873715,"> Obvious workaround for now is FunctionTransformer :-/

That was my thought too. Though I think I'm too tired right now to figure out how to apply the FunctionTransformer right after the soft VotingClassifier.
",obvious thought though think tired right figure apply right soft,issue,negative,positive,neutral,neutral,positive,positive
241868607,"If `voting=’soft’`: :

```
array-like = [n_classifiers, n_samples, n_classes]

    Class probabilities calculated by each classifier.
```

Maybe add a ""flatten"" parameter to VotingClassifier?

Obvious workaround for now is FunctionTransformer :-/
",soft class calculated classifier maybe add flatten parameter obvious,issue,negative,positive,neutral,neutral,positive,positive
241849728,"I've almost got it working, except that `VotingClassifier.transform` returns a matrix of shape `(1, # samples, # features)` instead of `(# samples, # features)` as the FeatureUnion expects. What's up with that? Any workaround?

``` python
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.datasets import make_classification

features, targets = make_classification(n_classes=10, n_informative=20, n_features=100, n_samples=1000)

def identity(x):
    return x

clf = make_pipeline(make_union(VotingClassifier([('rf1', RandomForestClassifier())], voting='soft'),
                               VotingClassifier([('rf1', RandomForestClassifier())], voting='hard'),
                               FunctionTransformer(identity)),
                    RandomForestClassifier())
```
",almost got working except matrix shape instead python import import import import identity return identity,issue,negative,neutral,neutral,neutral,neutral,neutral
241827791,"Not a waste at all! You helped us realize that we could output a more useful failure message when users pass data in a format that scikit-learn can't handle.
",waste u realize could output useful failure message pas data format ca handle,issue,negative,negative,neutral,neutral,negative,negative
241827422,"Yep, those are the latest contribution docs. To clarify, [this](https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L307) is the final compile call that I referenced in the original issue.
",yep latest contribution clarify final compile call original issue,issue,positive,positive,positive,positive,positive,positive
241825231,"@rhiever as_matrix seemed to work, that's what got me to error #235. I will still use `.values` as per your suggestion to verify it's the same error.
",work got error still use per suggestion verify error,issue,negative,neutral,neutral,neutral,neutral,neutral
241824672,"@rhiever this sounds interesting! just making sure, is http://rhiever.github.io/tpot/contributing/ still the right contribution docs?
",interesting making sure still right contribution,issue,positive,positive,positive,positive,positive,positive
241822240,"Thanks, sorry about wasting your time, and thanks for TPOT, totally cool.

On Aug 23, 2016 10:55 AM, ""Randy Olson"" notifications@github.com wrote:

> Hi @KeithBrodie https://github.com/KeithBrodie,
> 
> Thank you for sharing your data and a reproducible example so we could
> figure out what's going on. From looking at your data, it looks like your
> predicted target is continuous, which is a regression problem. At the
> moment, TPOT only supports classification problems.
> 
> We plan to add support for regression problems in the next release (0.6),
> hopefully within a couple weeks.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/233#issuecomment-241818121, or
> mute the thread
> https://github.com/notifications/unsubscribe-auth/AFz0rx6AuI290mazoXwLYT3e169bmoITks5qizQGgaJpZM4Jp_Vn
> .
",thanks sorry wasting time thanks totally cool randy wrote hi thank data reproducible example could figure going looking data like target continuous regression problem moment classification plan add support regression next release hopefully within couple reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
241819944,"@earino, I think your issue could be fixed if you replaced the lines:

``` python
X_train = train.ix[:, df.columns != ""class""].as_matrix()
y_train = train[""class""].as_matrix()
X_test = test.ix[:, df.columns != ""class""].as_matrix()
y_test = test[""class""].as_matrix()
```

with

``` python
X_train = train.ix[:, df.columns != ""class""].values
y_train = train[""class""].values
X_test = test.ix[:, df.columns != ""class""].values
y_test = test[""class""].values
```

Please let me know how that works for you.
",think issue could fixed python class train class class test class python class train class class test class please let know work,issue,negative,positive,neutral,neutral,positive,positive
241818121,"Hi @KeithBrodie,

Thank you for sharing your data and a reproducible example so we could figure out what's going on. From looking at your data, it looks like your predicted target is continuous, which is a regression problem. At the moment, TPOT only supports classification problems.

We plan to add support for regression problems in the next release (0.6), hopefully within a couple weeks.
",hi thank data reproducible example could figure going looking data like target continuous regression problem moment classification plan add support regression next release hopefully within couple,issue,positive,neutral,neutral,neutral,neutral,neutral
241812394,"Looks like another issue popping up from Python 2.7. :-)

@teaearlgraycold, we should roll out a quick patch for this by removing the formatting. I have a related patch to that section of the code coming up, so I will include this in the patch.
",like another issue python roll quick patch removing related patch section code coming include patch,issue,negative,positive,positive,positive,positive,positive
241811833,"Thank you for the bug report, @KeithBrodie! There seems to be an issue with our ""compile to sklearn Pipeline"" functionality for Python 2.7. We need to dig into it soon and see what we can find out.

In the meantime, we thoroughly tested on Python 3.5 and TPOT should run without a hitch there.
",thank bug report issue compile pipeline functionality python need dig soon see find thoroughly tested python run without hitch,issue,negative,neutral,neutral,neutral,neutral,neutral
241810873,"It's still not clear to me if it's ever a good idea to stop TPOT early. I don't think I've seen a case where TPOT's generalization accuracy went down from optimizing over another e.g. 100 generations.
",still clear ever good idea stop early think seen case generalization accuracy went another,issue,positive,positive,positive,positive,positive,positive
241788715,"I'm +1 for trying a different theme before jumping to some other framework. If we're keeping the current relatively flat structure of the docs (without nesting the pages for each of the operators), then it should be easy to play with different themes. 
",trying different theme framework keeping current relatively flat structure without easy play different,issue,negative,positive,neutral,neutral,positive,positive
241780971,"I've been trying to read up on the topic, but I wonder if there's a good reason to explore early stopping for these GP problems. Should we always let TPOT run the specified number of generations, or is there some reliable criteria that we can use to stop the EA early? Sure, we do internal cross-validation, but can we assume that that is always going to be reliable (i.e., the number of training samples is small, the distribution of training labels/responses doesn't reflect the general population, etc)?
",trying read topic wonder good reason explore early stopping always let run number reliable criterion use stop ea early sure internal assume always going reliable number training small distribution training reflect general population,issue,negative,positive,positive,positive,positive,positive
241738092,"The error still occurs when the X and Y datasets presented to the fit method are numpy arrays and the shape of the target array is (n,) i.e. one dimensional.  I have code and data to replicate it here:
#233 , fourth comment.
",error still fit method shape target array one dimensional code data replicate fourth comment,issue,negative,positive,positive,positive,positive,positive
241716639,"Here's another version which explicitly reshapes the target array.  The problem occurs with numpy array input with the label array of shape (n,).  This example uses X2.csv and y2.csv posted with my earlier comment.

Keith

Code:

```

import pandas as pd
from tpot import TPOT

Xdf = pd.read_csv('X2.csv',index_col=None)
ydf = pd.read_csv('y2.csv',index_col=None)

Dates = Xdf.Date.values

Xdf.drop(u'Date',axis=1,inplace = True)

ydf.drop(u'Date',axis=1,inplace = True)

print (Xdf.columns)
print (ydf.columns)

X_train = Xdf[:-100].values
y_train = ydf.Target[:-100].values
X_test  = Xdf[-100:].values
y_test  = ydf.Target[-100:].values

y_train.ravel()
y_test.ravel()

print X_train.shape
print y_train.shape

pipeopt = TPOT(generations=5, 
               population_size=20, 
               num_cv_folds=5, 
               random_state=42, 
               verbosity=2)

pipeopt.fit(X_train, y_train)
print(pipeopt.score(X_test, y_test))
pipeopt.export('tpot_exported_pipeline.py')

```

Results (Note shape of X and y):

```

Index([u'DLR_P0', u'DLR_P1', u'DLR_P2', u'DLOC_P0', u'DLOC_P1', u'DLOC_P2',
       u'LVOL_P0', u'LVOL_P1', u'LVOL_P2', u'LMA5_P0', u'LMA5_P1', u'LMA5_P2',
       u'LSD5_P0', u'LSD5_P1', u'LSD5_P2', u'ARO10_P0', u'ARO10_P1',
       u'ARO10_P2'],
      dtype='object')
Index([u'Target'], dtype='object')
(1554, 18)
(1554,)

GP Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]
GP Progress:   3%|3         | 4/120 [00:00<00:03, 36.54pipeline/s]
GP Progress:   6%|5         | 7/120 [00:06<01:09,  1.63pipeline/s]
GP Progress:  12%|#1        | 14/120 [00:06<00:46,  2.29pipeline/s]
GP Progress:  13%|#3        | 16/120 [00:06<00:38,  2.70pipeline/s]
GP Progress:  15%|#5        | 18/120 [00:06<00:31,  3.27pipeline/s]
GP Progress:  17%|#6        | 20/120 [00:12<01:44,  1.05s/pipeline]

Traceback (most recent call last):
  File ""/home/northwood/Dropbox/AutoDex/Extractor/tp6.py"", line 40, in <module>
    pipeopt.fit(X_train, y_train)
  File ""/usr/local/lib/python2.7/dist-packages/tpot/tpot.py"", line 307, in fit
    self._fitted_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)
  File ""/usr/local/lib/python2.7/dist-packages/tpot/tpot.py"", line 431, in _compile_to_sklearn
    sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr))
  File ""/usr/local/lib/python2.7/dist-packages/tpot/export_utils.py"", line 80, in expr_to_tree
    for node in ind:
TypeError: 'NoneType' object is not iterable
>>> 

```
",another version explicitly target array problem array input label array shape example posted comment code import import true true print print print print print note shape index index progress progress progress progress progress progress progress recent call last file line module file line fit file line file line node object iterable,issue,positive,positive,positive,positive,positive,positive
241711460,"I read that and have tried explicitly reshaping to (n,).  The problem does
not go away

On Aug 22, 2016 9:06 PM, ""Daniel"" notifications@github.com wrote:

Your problem seems identical to #234
https://github.com/rhiever/tpot/issues/234. I'm guessing the shape of
your labels is (N_rows, 1) instead of (N_rows, ).

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub
https://github.com/rhiever/tpot/issues/233#issuecomment-241621265, or mute
the thread
https://github.com/notifications/unsubscribe-auth/AFz0r2WvzoMaSPQrl0ql6zkhNOuO_g7lks5qinHSgaJpZM4Jp_Vn
.
",read tried explicitly problem go away wrote problem identical guessing shape instead thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
241621265,"Your problem seems identical to #234. I'm guessing the shape of your labels is `(N_rows, 1)` instead of `(N_rows, )`.
",problem identical guessing shape instead,issue,negative,neutral,neutral,neutral,neutral,neutral
241621035,"Ok, replicated the error with a smaller dataset, this time on Windows.  Including a reduced data-set that also generates the error.

Code:

```
import pandas as pd
#from BuildXY import BuildXY
from tpot import TPOT

Xdf = pd.read_csv('X2.csv',index_col=None)
ydf = pd.read_csv('y2.csv',index_col=None)

Dates = Xdf.Date.values

Xdf.drop(u'Date',axis=1,inplace = True)

ydf.drop(u'Date',axis=1,inplace = True)

print (Xdf.columns)
print (ydf.columns)

X_train = Xdf[:-100].values
y_train = ydf[:-100].values
X_test  = Xdf[-100:].values
y_test  = ydf[-100:].values

pipeopt = TPOT(generations=5, 
               population_size=20, 
               num_cv_folds=5, 
               random_state=42, 
               verbosity=2)

pipeopt.fit(X_train, y_train)
print(pipeopt.score(X_test, y_test))
pipeopt.export('tpot_exported_pipeline.py')
```

Result:

Python 2.7.11 |Anaconda 4.0.0 (64-bit)| (default, Feb 16 2016, 09:58:36) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://anaconda.org

> > > runfile('C:/Users/Keith/Dropbox/AutoDex/Extractor/tp4.py', wdir='C:/Users/Keith/Dropbox/AutoDex/Extractor')
> > > Index([u'DLR_P0', u'DLR_P1', u'DLR_P2', u'DLOC_P0', u'DLOC_P1', u'DLOC_P2',
> > >        u'LVOL_P0', u'LVOL_P1', u'LVOL_P2', u'LMA5_P0', u'LMA5_P1', u'LMA5_P2',
> > >        u'LSD5_P0', u'LSD5_P1', u'LSD5_P2', u'ARO10_P0', u'ARO10_P1',
> > >        u'ARO10_P2'],
> > >       dtype='object')
> > > Index([u'Target'], dtype='object')

GP Progress:   0%|          | 0/120 [00:00<?, ?pipeline/s]
GP Progress:   4%|?         | 5/120 [00:01<00:34,  3.36pipeline/s]
GP Progress:   5%|¦         | 6/120 [03:01<1:42:50, 54.13s/pipeline]
GP Progress:  11%|¦         | 13/120 [03:01<1:07:34, 37.89s/pipeline]
GP Progress:  12%|¦?        | 15/120 [06:09<1:35:46, 54.73s/pipeline]
GP Progress:  13%|¦?        | 16/120 [06:09<1:06:28, 38.35s/pipeline]
GP Progress:  16%|¦¦        | 19/120 [09:16<1:16:42, 45.57s/pipeline]
GP Progress:  17%|¦?        | 20/120 [09:16<53:14, 31.94s/pipeline]  

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Keith\Anaconda2\lib\site-packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 699, in runfile
    execfile(filename, namespace)
  File ""C:\Users\Keith\Anaconda2\lib\site-packages\spyderlib\widgets\externalshell\sitecustomize.py"", line 74, in execfile
    exec(compile(scripttext, filename, 'exec'), glob, loc)
  File ""C:/Users/Keith/Dropbox/AutoDex/Extractor/tp4.py"", line 35, in <module>
    pipeopt.fit(X_train, y_train)
  File ""C:\Users\Keith\Anaconda2\lib\site-packages\tpot\tpot.py"", line 307, in fit
    self._fitted_pipeline = self._toolbox.compile(expr=self._optimized_pipeline)
  File ""C:\Users\Keith\Anaconda2\lib\site-packages\tpot\tpot.py"", line 431, in _compile_to_sklearn
    sklearn_pipeline = generate_pipeline_code(expr_to_tree(expr))
  File ""C:\Users\Keith\Anaconda2\lib\site-packages\tpot\export_utils.py"", line 80, in expr_to_tree
    for node in ind:
TypeError: 'NoneType' object is not iterable
>>> 
```

Attaching screenshot and datafiles.

![tpot err 4 windows](https://cloud.githubusercontent.com/assets/6091951/17880048/bd307c80-68ab-11e6-9cd6-102dc849e63a.png)

[X2.zip](https://github.com/rhiever/tpot/files/431669/X2.zip)
[y2.zip](https://github.com/rhiever/tpot/files/431670/y2.zip)
",replicated error smaller time reduced also error code import import import true true print print print result python default bit win type help copyright license information anaconda brought continuum analytics please check index index progress progress progress progress progress progress progress progress recent call last file line module file line file line compile file line module file line fit file line file line node object iterable err,issue,positive,positive,positive,positive,positive,positive
241584645,"#113

Essentially, pandas has a much larger memory footprint (**Edit** than numpy matrices).

Also, and this was a very recent decision that didn't get much discussion on GitHub, TPOT now does almost no data management, with all data just passed to sklearn pipelines directly. This works by actually exporting each tested pipeline to Python code (using the same code as the `.export()` function) and running `eval` on that.
",essentially much memory footprint edit matrix also recent decision get much discussion almost data management data directly work actually tested pipeline python code code function running,issue,negative,positive,neutral,neutral,positive,positive
241583453,"@teaearlgraycold oh how interesting, is there a link to some discussion on why Pandas dataframes were dropped out out TPOT?
",oh interesting link discussion,issue,negative,positive,positive,positive,positive,positive
241581288,"Granted, the error message should be more explanatory. I'll definitely fix that for the next release.
",error message explanatory definitely fix next release,issue,negative,neutral,neutral,neutral,neutral,neutral
241580938,"TPOT no longer uses Pandas dataframes, so that may be part of the issue. However, converting each of X_train, y_train, etc. to numpy matrices with the function `.to_matrix()` still yields the error.

Comparing against known working datasets, the difference appears to be that the shape of your `y` is `(N_rows, 1)`, rather than the expected `(N_rows, )`. So for the labels you're passing a list of lists of length 1, rather than just a list.
",longer may part issue however converting matrix function still error known working difference shape rather passing list length rather list,issue,negative,neutral,neutral,neutral,neutral,neutral
241469277,"No, something I wrote.  I will post the output dataframes.  I'm re-running
it passing X and y as numpy arrays.  Behavior is different.  Don't know yet
if it runs to completion.

Thanks for looking at it

Keith

On Aug 22, 2016 9:11 AM, ""Daniel"" notifications@github.com wrote:

> I'm trying to replicate your issue, but I'm not sure where you're getting
> the BuildXY package from. Is that something that's bundled with the
> Spyder IDE?
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/233#issuecomment-241464352, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AFz0r9Z4LOZN_CGfn0qm2S7F5ffg7j-Wks5qicoqgaJpZM4Jp_Vn
> .
",something wrote post output passing behavior different know yet completion thanks looking wrote trying replicate issue sure getting package something ide thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
241464352,"I'm trying to replicate your issue, but I'm not sure where you're getting the `BuildXY` package from. Is that something that's bundled with the Spyder IDE?
",trying replicate issue sure getting package something ide,issue,negative,positive,positive,positive,positive,positive
241284680,"I pulled from the main tpot-mdr branch, added the ekf changes and re-uploaded the file.
",main branch added file,issue,negative,positive,positive,positive,positive,positive
241280401,"Looks like we have some merge conflicts -- can you pull from this `tpot-mdr` branch and sort out the conflict?
",like merge pull branch sort conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
241271157,"Please let me know when you think this is ready for review.
",please let know think ready review,issue,positive,positive,positive,positive,positive,positive
241271005,"We're basically looking for a way to do, e.g.,

(hacky code incoming)

```
Pipeline(FeatureUnion(RandomForestClassifier().predict(), RandomForestClassifier().predict_proba(), FunctionTransformer(lambda x: return x)), GaussianNB())
```

So every time there's a nested classifier within the pipeline (e.g. Data --> RF --> GaussianNB, as above), we pass to the next operator a copy of the current features with new features appended: the predicted classes from that nested classifier, and the predicted class probabilities from that nested classifier.
",basically looking way hacky code incoming pipeline lambda return every time classifier within pipeline data pas next operator copy current new class classifier class classifier,issue,negative,positive,neutral,neutral,positive,positive
241265640,"As I understand it, we need something in a pipeline that will call fit and subsequently predict_proba on an estimator. But pipelines are designed to call fit and then predict.
",understand need something pipeline call fit subsequently estimator designed call fit predict,issue,positive,positive,positive,positive,positive,positive
241262672,"I'm not sure what you mean. What do you mean by an internal pipeline?
",sure mean mean internal pipeline,issue,negative,negative,neutral,neutral,negative,negative
241216434,"Do you mean making it so pipelines during optimization have e.g. a fixed `random_state` of 42?
",mean making optimization fixed,issue,negative,negative,negative,negative,negative,negative
241216075,"I tried to demo this but couldn't figure out how to get a regular pipeline (that's using `predict`) to call `predict_prob` on an internal pipeline. Ideas @amueller?
",tried could figure get regular pipeline predict call internal pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
241214946,"Do you have an example of that in a pipeline? I'm not sure what that would look like.
",example pipeline sure would look like,issue,positive,positive,positive,positive,positive,positive
241214901,"Thank you @westonplatter! We're looking into adding Regressors to TPOT (#186), but need to implement functionality in `fit` to determine whether it's a regression or classification problem first.
",thank looking need implement functionality fit determine whether regression classification problem first,issue,negative,positive,positive,positive,positive,positive
241214781,"I've tested it in a pipeline and didn't get any exceptions.

I'll write a test that generates pipelines for each operator and tries to fit each one.
",tested pipeline get write test operator fit one,issue,negative,positive,positive,positive,positive,positive
241214696,"I think we need a test using this new operator in a pipeline.
",think need test new operator pipeline,issue,negative,positive,positive,positive,positive,positive
241133895,"[![Coverage Status](https://coveralls.io/builds/7530197/badge)](https://coveralls.io/builds/7530197)

Changes Unknown when pulling **cd5407ed4c0bbe75839fc93d4c1217598a4e9ea2 on teaearlgraycold:development** into *\* on rhiever:development**.
",coverage status unknown development development,issue,negative,negative,neutral,neutral,negative,negative
241101908,"Please rework the code as suggested and send the PR to the [tpot-mdr](https://github.com/rhiever/tpot/tree/tpot-mdr) branch. We can't merge this on master because it's a separate project.
",please rework code send branch ca merge master separate project,issue,negative,neutral,neutral,neutral,neutral,neutral
241099329,"This issue is now handled because we're working directly with sklearn Pipeline objects that are persistent.
",issue handled working directly pipeline persistent,issue,negative,positive,neutral,neutral,positive,positive
241037792,"Sure

On Fri, Aug 19, 2016, 10:42 AM Randy Olson notifications@github.com wrote:

> Merged #217 https://github.com/rhiever/tpot/pull/217.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/217#event-761356980, or mute the
> thread
> https://github.com/notifications/unsubscribe-auth/ADISY5hDMnXR_c4GJfwuCo1t0PYUdsw2ks5qhcC_gaJpZM4JoHU-
> .
",sure randy wrote thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
241037226,"I'm just gonna merge this and start hacking at some of these issues. Can you work on the unit tests in the meantime?
",gon na merge start hacking work unit,issue,negative,neutral,neutral,neutral,neutral,neutral
241034060,"Testing this change now. It looks elegant. I love it!

Some initial notes:
- [ ] A bunch of unit tests are broken because of all the removed code
- [x] Balanced accuracy should be the default scoring function, not regular accuracy. Where did our balanced accuracy function go?
- [x] We should explicitly specify `cv=3` in `cross_val_score`
- [ ] I keep getting these warnings. Maybe we need to suppress these warnings again in the production version.

```
anaconda/lib/python3.5/site-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.)
```

and

```
anaconda/lib/python3.5/site-packages/sklearn/naive_bayes.py:766: RuntimeWarning: divide by zero encountered in log
```
- [x] In all cases throughout TPOT, set `n_jobs=1` instead of `-1`. We should not assume that the user would like to use multithreading. By default, all sklearn `n_job` values are `1`, so we just need to remove all cases in TPOT that set it to `-1`.
- [x] `predict` and `score` doesn't seem to be passing the data around properly
",testing change elegant love initial bunch unit broken removed code balanced accuracy default scoring function regular accuracy balanced accuracy function go explicitly specify keep getting maybe need suppress production version selected either data noisy selection test strict divide zero log throughout set instead assume user would like use default need remove set predict score seem passing data around properly,issue,positive,positive,neutral,neutral,positive,positive
240994802,"I've been thinking about this issue a little more and I wonder if we could take it a step further. Instead of allowing the user to specify what classifiers and operators within our existing set to use or not use, what if we provide an interface for the user to specify all the operators (and params that can evolve) during init?

The user essentially passes a list of sklearn-compatible objects and some metadata for each (e.g., what params to evolve in what ranges, whether it's a classifier or feature preprocessor, etc.), and we can parse them into the evolvable TPOT pipeline structure.

We would even specify our default TPOT operator set the same way, and allow it to be overriden by the user. I think such a setup would make TPOT much more flexible for us (no more need to write a separate file for every operator) and for the user (they can restrict or expand the operator and parameter space as they like).

cc @teaearlgraycold 
",thinking issue little wonder could take step instead user specify within set use use provide interface user specify evolve user essentially list evolve whether classifier feature parse evolvable pipeline structure would even specify default operator set way allow user think setup would make much flexible u need write separate file every operator user restrict expand operator parameter space like,issue,positive,positive,neutral,neutral,positive,positive
240918754,"Given how much code this removes from TPOT, I'm glad to say I was wrong about what I said about this change. It's a great improvement upon the over-complexity of TPOT.
",given much code glad say wrong said change great improvement upon,issue,positive,positive,positive,positive,positive,positive
240868016,"An option to limit TPOT to one classifier (e.g., Random Forest) would be very helfpul. 
",option limit one classifier random forest would,issue,negative,negative,negative,negative,negative,negative
240185085,"Close:

``` python
SelectFromModel(ExtraTreesClassifier(criterion=criterion, max_features=max_features, min_weight_fraction_leaf=mwfl), threshold=threshold)
```

where each setting is evolvable.
",close python setting evolvable,issue,negative,neutral,neutral,neutral,neutral,neutral
240184680,"So, something like this:

``` Python
SelectFromModel(ExtraTreesClassifier, threshold=0.1)
```

Or is that not how it would be done
",something like python would done,issue,negative,neutral,neutral,neutral,neutral,neutral
240103881,"Hello @rhiever! I think I'll prefer to leave it to @teaearlgraycold , as I'm currently short on time :(
I'll keep an eye on the project, and as soon as I have more spare time I'll head back and try to make some more contributions.
",hello think prefer leave currently short time keep eye project soon spare time head back try make,issue,negative,neutral,neutral,neutral,neutral,neutral
239939386,"`LinearSVC(input_matrix, 0.73999999999999999, 14, True)`, which corresponds to:

``` Python
LinearSVC(C=0.74, dual=False, penalty=""l1"")
```

Given that the tests are passing, the scoring function still produces the same results as it did in 0.4. So the issue is somewhere in fit.
",true python given passing scoring function still issue somewhere fit,issue,positive,positive,positive,positive,positive,positive
239939034,"Something is definitely wrong there. What kind of pipeline did it produce?
",something definitely wrong kind pipeline produce,issue,negative,positive,neutral,neutral,positive,positive
239937155,"WIth IRIS it reaches a CV of 1.0 and a final score of 0.24 so something's up. That makes me think that somehow the classes are leaking into the features.
",iris final score something think somehow class,issue,negative,neutral,neutral,neutral,neutral,neutral
239930037,"Not exactly 0.5. MNIST has the issue

On Mon, Aug 15, 2016, 4:58 PM Randy Olson notifications@github.com wrote:

> There's still the issue of pipelines generated by fit() performing
> extremely poorly on the testing data.
> 
> You mean pipelines achieving .5 accuracy? On what data set(s)?
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/209#issuecomment-239927056, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/ADISY6pnHwk3y8Tg4xVv-pPkXm3S2uIMks5qgNLngaJpZM4JjvNc
> .
",exactly issue mon randy wrote still issue fit extremely poorly testing data mean accuracy data set thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
239927056,"> There's still the issue of pipelines generated by fit() performing extremely poorly on the testing data.

You mean pipelines achieving .5 accuracy? On what data set(s)?
",still issue fit extremely poorly testing data mean accuracy data set,issue,negative,negative,negative,negative,negative,negative
239926044,"There's still the issue of pipelines generated by fit() performing extremely poorly on the testing data.
",still issue fit extremely poorly testing data,issue,negative,neutral,neutral,neutral,neutral,neutral
239925779,"I'm not sure given the recent merge from the scoring PR.

You could try on my refactor branch which also fixes a couple of issues that I haven't merged into your repo yet: https://github.com/teaearlgraycold/tpot/commits/refactor
",sure given recent merge scoring could try branch also couple yet,issue,negative,positive,positive,positive,positive,positive
239925125,"Haven't started running it yet. Is the dev branch ready for mass TPOT tests?
",running yet dev branch ready mass,issue,negative,positive,positive,positive,positive,positive
239924212,"Actually I tested this yesterday and haven't been able to recreate the issue. @rhiever, have you see this in any testing?
",actually tested yesterday able recreate issue see testing,issue,negative,positive,positive,positive,positive,positive
239919692,"👍 Will close this issue then. @campbx, please re-open the issue if it persists in the 0.5 release later this week.
",close issue please issue release later week,issue,negative,neutral,neutral,neutral,neutral,neutral
239919111,"Yes.

0.5 should be out this week, so honestly I'd recommend just waiting for the official 0.5 release.
",yes week honestly recommend waiting official release,issue,positive,positive,positive,positive,positive,positive
239919021,"Will definitely appreciate any and all code contributions! Adding t-SNE as an alternative to PCA sounds very interesting. Our latest dev version is on the `development` branch, so I'd start (and PR to) there. Here's the contribution guidelines: [link](http://rhiever.github.io/tpot/contributing/)

Basically, adding t-SNE would entail adding the operator to TPOT itself, adding unit tests, and adding an entry to the documentation.
",definitely appreciate code alternative interesting latest dev version development branch start contribution link basically would entail operator unit entry documentation,issue,positive,positive,positive,positive,positive,positive
239918481,"Thank you @campbx! @teaearlgraycold, I think this will be handled in the 0.5 release, right?
",thank think handled release right,issue,negative,positive,positive,positive,positive,positive
239917849,"@bartleyn: Just merged this PR and @teaearlgraycold will jump in to wrap it up for the 0.5 release (hopefully this week). Thank you again!
",jump wrap release hopefully week thank,issue,positive,neutral,neutral,neutral,neutral,neutral
239636231,"So far -
- [ ] An imputer must be the first step in the pipeline (see #194)
- [ ] A scaler must be the second step in the pipeline
- [X] Only classifiers can be the final step in the pipeline
- [ ] special terminal types for each parameter, such that it never, e.g., uses integers to encode string parameters (see #189)
",far imputer must first step pipeline see scaler must second step pipeline final step pipeline special terminal parameter never encode string see,issue,negative,positive,positive,positive,positive,positive
239636154,"Will do, thanks!
On Sat, Aug 13, 2016 at 14:56 Randy Olson notifications@github.com wrote:

> Totally understand! If you end up putting together another PR, please PR
> it to the development branch. master is reserved for the latest stable
> release now.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/100#issuecomment-239636059, or mute
> the thread
> https://github.com/notifications/unsubscribe-auth/AAIQ-dKl7oHdutT3Ub886CgjOvlHlKYWks5qfhNAgaJpZM4Hm4zt
> .
> 
> ## 
> 
> iPhone'd
",thanks sat randy wrote totally understand end together another please development branch master reserved latest stable release thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
239636059,"Totally understand! If you end up putting together another PR, please PR it to the `development` branch. `master` is reserved for the latest stable release now.
",totally understand end together another please development branch master reserved latest stable release,issue,positive,positive,positive,positive,positive,positive
239635812,"Sorry for dropping off; job took over and hasn't let up. However I am working on designing additional experiments for a paper that will involve testing multiple ML pipelines, so if teaching this semester can become somewhat self-sustaining I may be able revisit this issue sooner rather than later. 

> On Aug 13, 2016, at 14:34, Randy Olson notifications@github.com wrote:
> 
> Going to close this PR since we won't be able to merge it into the existing dev branch, but thank you for putting the demo together. We will refer to this PR when considering parallelization options for TPOT.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",sorry dropping job took let however working designing additional paper involve testing multiple teaching semester become somewhat may able revisit issue sooner rather later randy wrote going close since wo able merge dev branch thank together refer considering parallelization thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
239635076,"Going to close this PR since we won't be able to merge it into the existing dev branch, but thank you for putting the demo together. We will refer to this PR when considering parallelization options for TPOT.
",going close since wo able merge dev branch thank together refer considering parallelization,issue,negative,positive,positive,positive,positive,positive
239635039,"Going to close this now since it didn't seem to work out, but let's keep this idea in mind for future TPOT iterations.
",going close since seem work let keep idea mind future,issue,negative,neutral,neutral,neutral,neutral,neutral
239635002,"Going to close this PR since we have a version of it in the dev branch now.
",going close since version dev branch,issue,negative,neutral,neutral,neutral,neutral,neutral
239634916,"This issue is now encapsulated in #206, so I'm going to close this issue.
",issue going close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
239634812,"Going to close this issue. Please re-open if the problem persists on other data sets.
",going close issue please problem data,issue,negative,neutral,neutral,neutral,neutral,neutral
239634557,"@teaearlgraycold, how much of this have we already integrated into the dev branch?
",much already dev branch,issue,negative,positive,positive,positive,positive,positive
238401679,"@westonplatter, that code requires you use the version of TPOT that is currently in the development branch (0.5).
",code use version currently development branch,issue,negative,neutral,neutral,neutral,neutral,neutral
238401425,"@teaearlgraycold thanks for the example. I'll take a look.
",thanks example take look,issue,negative,positive,positive,positive,positive,positive
238401239,"@westonplatter - here's the code I used

``` Python
from sklearn import * # Needed to discover all subclasses
import tpot
import sklearn

def all_subclasses(cls):
    return cls.__subclasses__() + [g for s in cls.__subclasses__() for g in all_subclasses(s)]

tpot_estimators = set([x.__name__ for x in tpot.operators.Operator.inheritors()])
sklearn_estimators = set([x.__name__ for x in all_subclasses(sklearn.base.BaseEstimator)
    if x.__name__[0] != '_' and not x.__name__.startswith(""Base"")])

for est in sklearn_estimators:
    marker = 'X' if est in tpot_estimators else ' '
    print(""- [{}] {}"".format(marker, est))

```
",code used python import discover import import return set set base marker else print marker,issue,negative,negative,negative,negative,negative,negative
238401140,"@teaearlgraycold for someone coming brand new into the project, is there a good example of what needs to be done for the list you just posted?
",someone coming brand new project good example need done list posted,issue,negative,positive,positive,positive,positive,positive
238400962,"This is our current coverage:
- [ ] LabelEncoder
- [ ] NearestNeighbors
- [ ] OneVsOneClassifier
- [ ] OneVsRestClassifier
- [ ] AdditiveChi2Sampler
- [X] LogisticRegression
- [ ] PriorProbabilityEstimator
- [ ] NuSVC
- [ ] LinearSVR
- [ ] NeighborsBase
- [ ] LabelSpreading
- [ ] LinearModel
- [ ] AffinityPropagation
- [ ] FunctionTransformer
- [ ] ElasticNetCV
- [ ] SpectralBiclustering
- [ ] VBGMM
- [ ] RandomizedLasso
- [ ] LassoCV
- [ ] PatchExtractor
- [ ] GridSearchCV
- [ ] PLSSVD
- [ ] MiniBatchSparsePCA
- [ ] ZeroEstimator
- [ ] LSHForest
- [ ] MultiLabelBinarizer
- [ ] FeatureHasher
- [X] FastICA
- [ ] Ridge
- [ ] LedoitWolf
- [ ] MiniBatchDictionaryLearning
- [ ] ElasticNet
- [ ] Imputer
- [ ] KernelRidge
- [x] RandomizedPCA
- [X] SelectKBest
- [ ] MultiTaskElasticNet
- [ ] LassoLarsIC
- [ ] GraphLassoCV
- [ ] LabelBinarizer
- [X] MaxAbsScaler
- [ ] VotingClassifier
- [ ] IsotonicRegression
- [ ] FactorAnalysis
- [ ] LabelPropagation
- [X] GradientBoostingClassifier
- [ ] LogOddsEstimator
- [ ] SparsePCA
- [ ] TfidfVectorizer
- [X] PassiveAggressiveClassifier
- [ ] RFECV
- [ ] RandomizedLogisticRegression
- [ ] AgglomerativeClustering
- [ ] ScaledLogOddsEstimator
- [ ] OrthogonalMatchingPursuitCV
- [ ] SVR
- [ ] KernelCenterer
- [X] AdaBoostClassifier
- [ ] NearestCentroid
- [ ] TheilSenRegressor
- [ ] GaussianProcess
- [ ] SVC
- [ ] PLSCanonical
- [ ] SelectFromModel
- [ ] CountVectorizer
- [X] StandardScaler
- [X] GaussianNB
- [ ] FeatureUnion
- [ ] DummyClassifier
- [ ] GaussianRandomProjection
- [ ] KNeighborsRegressor
- [ ] MultiTaskElasticNetCV
- [ ] ForestClassifier
- [X] ExtraTreesClassifier
- [ ] SparseRandomProjection
- [ ] GraphLasso
- [X] FeatureAgglomeration
- [ ] EllipticEnvelope
- [ ] OneHotEncoder
- [ ] NuSVR
- [ ] RadiusNeighborsClassifier
- [ ] RidgeCV
- [ ] MeanShift
- [ ] LatentDirichletAllocation
- [X] Binarizer
- [ ] KMeans
- [ ] LarsCV
- [ ] SGDClassifier
- [ ] BaggingClassifier
- [ ] AdaBoostRegressor
- [ ] RidgeClassifierCV
- [ ] TruncatedSVD
- [ ] Isomap
- [ ] PCA
- [ ] SkewedChi2Sampler
- [ ] ExtraTreesRegressor
- [ ] RandomizedSearchCV
- [ ] Lasso
- [ ] IncrementalPCA
- [ ] LassoLars
- [X] PolynomialFeatures
- [ ] KernelDensity
- [ ] PassiveAggressiveRegressor
- [ ] SpectralCoclustering
- [ ] QuantileEstimator
- [ ] SpectralClustering
- [X] MultinomialNB
- [ ] LinearRegression
- [ ] Pipeline
- [X] SelectFwe
- [ ] GMM
- [ ] MeanEstimator
- [X] Nystroem
- [X] RandomForestClassifier
- [ ] DummyRegressor
- [ ] Perceptron
- [ ] CCA
- [X] DecisionTreeClassifier
- [ ] ExtraTreeRegressor
- [X] RBFSampler
- [ ] BayesianRidge
- [ ] RANSACRegressor
- [ ] DictionaryLearning
- [ ] TSNE
- [ ] HashingVectorizer
- [ ] RidgeClassifier
- [ ] ForestRegressor
- [ ] DecisionTreeRegressor
- [ ] DBSCAN
- [ ] OutputCodeClassifier
- [X] KNeighborsClassifier
- [X] ZeroCount
- [ ] GradientBoostingRegressor
- [ ] ARDRegression
- [ ] LinearDiscriminantAnalysis
- [ ] CalibratedClassifierCV
- [X] SelectPercentile
- [ ] OAS
- [ ] DPGMM
- [ ] SelectFpr
- [ ] DictVectorizer
- [X] BernoulliNB
- [ ] Normalizer
- [X] LinearSVC
- [X] VarianceThreshold
- [ ] MiniBatchKMeans
- [ ] RandomForestRegressor
- [ ] MultiTaskLasso
- [ ] ProjectedGradientNMF
- [ ] RandomTreesEmbedding
- [ ] OneClassSVM
- [ ] SGDRegressor
- [ ] GaussianRandomProjectionHash
- [ ] SparseCoder
- [ ] MultiTaskLassoCV
- [ ] QuadraticDiscriminantAnalysis
- [ ] Birch
- [ ] RadiusNeighborsRegressor
- [ ] ExtraTreeClassifier
- [ ] MinCovDet
- [ ] LinearModelCV
- [X] MinMaxScaler
- [ ] NMF
- [ ] TfidfTransformer
- [ ] SelectFdr
- [ ] KernelPCA
- [ ] BaggingRegressor
- [ ] Lars
- [ ] SpectralEmbedding
- [X] RobustScaler
- [ ] BernoulliRBM
- [ ] ShrunkCovariance
- [ ] OrthogonalMatchingPursuit
- [ ] NewBase
- [ ] GenericUnivariateSelect
- [X] RFE
- [ ] LassoLarsCV
- [ ] LocallyLinearEmbedding
- [ ] PLSRegression
- [ ] EmpiricalCovariance
- [ ] LogisticRegressionCV
- [ ] MDS
",current coverage ridge imputer lasso pipeline normalizer birch,issue,negative,neutral,neutral,neutral,neutral,neutral
238301409,"Good idea. I've added this to the enhancements list.
",good idea added list,issue,negative,positive,positive,positive,positive,positive
237515680,"Yes using y_train.values in the TPOT that could work .many thanks for your help !

发自我的 iPhone

> 在 2016年8月4日，上午9:18，Randy Olson notifications@github.com 写道：
> 
> One more thought I had: I noticed in the screenshot you posted, y_train is a pandas Series. Make sure to convert everything to numpy arrays of matrices (e.g. y_train.values) before passing it to TPOT or any sklearn object.
> 
> ―
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",yes could work thanks help one thought posted series make sure convert everything matrix passing object thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
237423389,"One more thought I had: I noticed in the screenshot you posted, `y_train` is a pandas Series. Make sure to convert everything to numpy arrays or matrices (e.g. `y_train.values`) before passing it to TPOT or any sklearn object.
",one thought posted series make sure convert everything matrix passing object,issue,negative,positive,positive,positive,positive,positive
237421296,"My train data has already been toarray  before feeding in.but they are word's tf-idf value .so the dense matrix have many zero value..

发自我的 iPhone

> 在 2016年8月4日，上午8:44，Randy Olson notifications@github.com 写道：
> 
> If you're using the sklearn interface, the name of the columns shouldn't matter (since they're dropped once in numpy matrix format). Currently, we don't support sparse matrices, which may be the source of the issue. What happens if you try the same call and change your array to a dense matrix? (via .toarray())
> 
> ―
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or mute the thread.
",train data already feeding word value dense matrix many zero value interface name matter since matrix format currently support sparse matrix may source issue try call change array dense matrix via thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
237418392,"If you're using the sklearn interface, the name of the columns shouldn't matter (since they're dropped once in numpy matrix format). Currently, we don't support sparse matrices, which may be the source of the issue. What happens if you try the same call and change your array to a dense matrix? (via `.toarray()`)
",interface name matter since matrix format currently support sparse matrix may source issue try call change array dense matrix via,issue,negative,neutral,neutral,neutral,neutral,neutral
237405646,"input: print(set(y_train))

output:  set([0, 1, 2, 3])
",input print set output set,issue,negative,neutral,neutral,neutral,neutral,neutral
237372518,"I've pulled those changes -- the refactor looks great. I should at least be able to resolve the merge conflicts in the main tpot.py (and maybe the tests). But maybe I'll hand it off at that point, since I'm not quite sure (yet) where things plug into the new framework. 
",great least able resolve merge main maybe maybe hand point since quite sure yet plug new framework,issue,positive,positive,positive,positive,positive,positive
237369299,"@tonyfast, check out the [development branch](https://github.com/rhiever/tpot/tree/development) if you'd like to see where we're heading with TPOT in the immediate future. I think, using the same kind of compile-DEAP-pipelines-to-sklearn-pipelines code, we could also have TPOT directly evolving sklearn pipelines as well.
",check development branch like see heading immediate future think kind code could also directly well,issue,positive,positive,positive,positive,positive,positive
237368869,"Hi @KhaoticMind, thank you so much for working XGBoost back into TPOT as an optional dependency! As you may have seen, we've been refactoring TPOT so it's easier to maintain. You can find the latest version of the refactor in the [development branch](https://github.com/rhiever/tpot/tree/development).

This change will require some rework of your PR. Would you prefer to rework the PR into the new framework, or would you like @teaearlgraycold and I to take your code and rework it?

[Also note: all PRs must be sent to the `development` branch now, as the `master` branch is intended for the latest stable version of TPOT.]
",hi thank much working back optional dependency may seen easier maintain find latest version development branch change require rework would prefer rework new framework would like take code rework also note must sent development branch master branch intended latest stable version,issue,positive,positive,positive,positive,positive,positive
237368550,"Hi @bartleyn, thank you so much for reworking the scoring system! As you may have seen, we've been refactoring TPOT so it's easier to maintain. You can find the latest version of the refactor in the [development branch](https://github.com/rhiever/tpot/tree/development).

This change will require some slight rework of your PR. Would you prefer to rework the PR into the new framework, or would you like @teaearlgraycold and I to take your code and rework it?
",hi thank much scoring system may seen easier maintain find latest version development branch change require slight rework would prefer rework new framework would like take code rework,issue,positive,positive,positive,positive,positive,positive
237361479,"This looks like an issue with sklearn's `train_test_split` and related functionality. What do you get when you look at the unique values in `y_train`, e.g., `print(set(y_train))`?
",like issue related functionality get look unique print set,issue,negative,positive,positive,positive,positive,positive
237085724,"This build will still fail the tests but I guess you can go ahead and merge it into the dev branch.
",build still fail guess go ahead merge dev branch,issue,negative,negative,negative,negative,negative,negative
236708023,"Anaconda shouldn't be necessary, and your package versions seem to check out. This is very strange!

What happens if you run the MNIST example? https://github.com/rhiever/tpot#example
",anaconda necessary package seem check strange run example,issue,negative,negative,neutral,neutral,negative,negative
236681912,"Please PR this to the `TPOT-MDR` branch on this repo instead of the `master` branch. You will need to create a new PR.
",please branch instead master branch need create new,issue,positive,positive,positive,positive,positive,positive
236470830,"I am not using Anaconda. Is that a problem?

```
Python 3.5.1
deap (1.0.2)
numpy (1.11.1)
pandas (0.18.1)
pip (8.1.2)
python-dateutil (2.5.3)
pytz (2016.6.1)
requests (2.10.0)
scikit-learn (0.17.1)
scipy (0.18.0)
setuptools (18.3.1)
six (1.10.0)
TPOT (0.4.1)
tqdm (4.8.1)
update-checker (0.12)
wheel (0.26.0)
```
",anaconda problem python pip six wheel,issue,negative,neutral,neutral,neutral,neutral,neutral
235900248,"Can't reproduce this bug on an Anaconda install w/ Python 3. Can you please report the versions of Python & packages that you're using?
",ca reproduce bug anaconda install python please report python,issue,negative,neutral,neutral,neutral,neutral,neutral
235338455,"That seems reasonable to me

On Tue, Jul 26, 2016, 1:13 PM Randy Olson notifications@github.com wrote:

> The progress bar is nice for regular TPOT users, but for me it's useful to
> have the stats on min/mean/max fitness etc. that we had in TPOT 0.3 and
> earlier. We should add a --science option to TPOT that replaces the
> progress bar with the old stats.
> 
> cc @teaearlgraycold https://github.com/teaearlgraycold
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/200, or mute the thread
> https://github.com/notifications/unsubscribe-auth/ADISYwqfCjoM-KG6PXb5g8LN79xJPckAks5qZkBGgaJpZM4JVYM9
> .
",reasonable tue randy wrote progress bar nice regular useful fitness add science option progress bar old reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
235038391,"After digging around for some code-introspection ideas, I found that you can access the docstring of the sklearn metrics with commands like `help()`. With some straightforward parsing, this would allow for flexibility when it comes to using a classifier's `decision_function` versus `predict_proba` (because the relevant sklearn metric docstrings specify which function to use), and eliminates the need to warn the user about this issue. I'll update accordingly. 
",digging around found access metric like help straightforward would allow flexibility come classifier versus relevant metric specify function use need warn user issue update accordingly,issue,positive,positive,positive,positive,positive,positive
235023811,"So this is what we're looking at now with exported code:

``` Python
import numpy as np
import pandas as pd

from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier
from sklearn.feature_selection import RFE
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from sklearn.svm import SVC

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')
training_indices, testing_indices = train_test_split(tpot_data.index, stratify=tpot_data['class'].values, train_size=0.75, test_size=0.25)

exported_pipeline = make_pipeline(
    make_union(
        make_union(VotingClassifier(estimators=[('branch',
            GradientBoostingClassifier(learning_rate=1.0, max_features=1.0, min_weight_fraction_leaf=0.5, n_estimators=500)
        )]), FunctionTransformer(lambda X: X)),
        RFE(estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
          decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
          max_iter=-1, probability=False, random_state=42, shrinking=True,
          tol=0.001, verbose=False), step=0.18)
    ),
    KNeighborsClassifier(n_neighbors=5, weights=""distance"")
)

exported_pipeline.fit(tpot_data.loc[training_indices].drop('class', axis=1).values,
                      tpot_data.loc[training_indices, 'class'].values)
results = exported_pipeline.predict(tpot_data.loc[testing_indices].drop('class', axis=1))
```
",looking code python import import import import import import import import import note make sure class data file lambda distance,issue,negative,positive,positive,positive,positive,positive
232485148,"Interesting idea! This is a feature that we should consider in the future after we add support for regression (#186) and unsupervised learning (#195).
",interesting idea feature consider future add support regression unsupervised learning,issue,positive,positive,positive,positive,positive,positive
232193022,"Sorry, I've been horribly negligent of my duties on this (I had a personal emergency to deal with, and I regret not saying something before going dark). I've got the lion's share of it done, but need to test edge cases (multiclass, multilabel, etc) a little more thoroughly before putting up a PR. I should comfortably have something by the weekend.  
",sorry horribly negligent personal emergency deal regret saying something going dark got lion share done need test edge little thoroughly comfortably something weekend,issue,negative,negative,negative,negative,negative,negative
232036206,"On the page [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html), in chapter **2.3.9. Clustering performance evaluation**, there are several clustering performance measures mentioned. Besides silhouette coefficient (2.3.9.4. Silhouette Coefficient), there are also:
2.3.9.1. Adjusted Rand index
2.3.9.2. Mutual Information based scores
2.3.9.3. Homogeneity, completeness and V-measure
",page chapter clustering performance evaluation several clustering performance besides silhouette coefficient silhouette coefficient also rand index mutual information based homogeneity completeness,issue,negative,neutral,neutral,neutral,neutral,neutral
232034453,"I like this idea. I'd like to explore it after we have regression integrated into TPOT.

We should explore additional metrics for scoring unsupervised results as well.
",like idea like explore regression explore additional metric scoring unsupervised well,issue,positive,neutral,neutral,neutral,neutral,neutral
231513345,"Now just need to write some more tests so the coverage actually means something
",need write coverage actually something,issue,negative,neutral,neutral,neutral,neutral,neutral
231382691,"While you're at it, consider allowing a parameter for repeated cross validation. That shouldn't be much of a problem. What could be more interesting--although a more difficult implementation--is only performing the repeated cross validation on a few of the candidate models that achieve the highest score. This would prevent the wasted computation on low-accuracy models while allowing discrimination between the top models that might be missed due to the idiosyncratic sampling of one round of CV.
",consider parameter repeated cross validation much problem could interesting although difficult implementation repeated cross validation candidate achieve highest score would prevent wasted computation discrimination top might due idiosyncratic sampling one round,issue,negative,positive,neutral,neutral,positive,positive
231199348,"Some more commits to fix some pending issues discussed above.
Please feel free to comment on this new version.
",fix pending please feel free comment new version,issue,positive,positive,positive,positive,positive,positive
231199047,"Sorry, now you have enlightened me!
Althou I knew that the new features where being used WITH the original ones, I was under the impression that this was stacking, when it really is not. 
",sorry enlightened knew new used original impression really,issue,positive,positive,neutral,neutral,positive,positive
231191155,"Yes, this is also known as creating ""synthetic features"" -- basically using the output of one model as a feature for another model.

The code you show appears to be creating synthetic features correctly. It trains the first classifier on only the training data, then stores the predictions on all of the data as the synthetic feature. The same training subset is then used to train the second classifier, which then makes the final classification on the full data set.
",yes also known synthetic basically output one model feature another model code show synthetic correctly first classifier training data data synthetic feature training subset used train second classifier final classification full data set,issue,negative,positive,positive,positive,positive,positive
230147233,"sudo pip install --upgrade tpot 
Installs the 0.4.1 version of the release - which solves the issue. 
Thank you. 
",pip install upgrade version release issue thank,issue,negative,neutral,neutral,neutral,neutral,neutral
230138302,"The above fix for #191  is not working fine. 
@teaearlgraycold - Is this the only fix done to the code in tpot/export_utils.py? Are there no changes done to the main tpot.py script?

Receiving these errors : - 
pipeline_list
[['result1', '_feat_agg', 'ARG0', '36', '13', '5'], ['result2', '_passive_aggressive', 'result1', '0.08', '11']]
File ""generate_tpot_model.py"", line 89, in 
runTpot(train_file, test_file, gen_num)
File ""generate_tpot_model.py"", line 80, in runTpot
tpot.export(name)
File ""/usr/local/lib/python2.7/dist-packages/tpot/tpot.py"", line 480, in export
pipeline_text += replace_function_calls(pipeline_list)
File ""/usr/local/lib/python2.7/dist-packages/tpot/export_utils.py"", line 191, in replace_function_calls
operator_num = int(operator[0].strip('result'))
ValueError: invalid literal for int() with base 10: ' '

and 
Another error sample for a different run.
[['result1', '_select_fwe', 'ARG0', '12.0'], ['result2', '_extra_trees', 'result1', '6', '48.0', '0.1']]

Traceback (most recent call last):
File ""generate_tpot_model.py"", line 89, in 
runTpot(train_file, test_file, gen_num)
File ""generate_tpot_model.py"", line 80, in runTpot
tpot.export(name)
File ""/usr/local/lib/python2.7/dist-packages/tpot/tpot.py"", line 477, in export
pipeline_text = generate_import_code(pipeline_list)
File ""/usr/local/lib/python2.7/dist-packages/tpot/export_utils.py"", line 99, in generate_import_code
operators_used = set([operator[1] for operator in pipeline_list])
IndexError: string index out of range
",fix working fine fix done code done main script file line file line name file line export file line operator invalid literal base another error sample different run recent call last file line file line name file line export file line set operator operator string index range,issue,negative,negative,neutral,neutral,negative,negative
230135836,"@teaearlgraycold Thank you for addressing this problem and providing a quick resolution for the same. 
",thank problem providing quick resolution,issue,negative,positive,positive,positive,positive,positive
230129970,"@ankitshah009 I've solved the problem. The fix should be released in a bugfix version of 0.4 soon.
",problem fix version soon,issue,negative,neutral,neutral,neutral,neutral,neutral
230095761,"@robotijn - 
In the 0.4.0 version of tpot, were you able to solve the model saving error?
Also - please share whether just by changing the version from 0.3 to 0.4 solved the overfitting issue or were there some more modifications done locally to the tpot code?
",version able solve model saving error also please share whether version issue done locally code,issue,negative,positive,positive,positive,positive,positive
230095593,"For the CSV - To use TPOT, we need to label columns as features and their corresponding class. 
F1,F2,.......,class <-- This could be the first row of the CSV and then have one's data to be able to use the generated pipeline from TPOT
",use need label corresponding class class could first row one data able use pipeline,issue,negative,positive,positive,positive,positive,positive
230011211,"@teaearlgraycold 
Awaiting an update on this issue. Let me know if further information is needed. 
Thanks 
Ankit Shah
",update issue let know information thanks shah,issue,negative,positive,positive,positive,positive,positive
229736389,"Please find the attached code essential to replicate the issue. Change .txt version of the .py file by removing the extension. 
1. generate_tpot_model.py - Used to generate the tpot generated pipeline
Command to use: - python generate_tpot_model.py fold1_train.txt 0.75 15 | tee tpot_run15.log
2. The above command would create a file similar to tpot_pipeline_fold1_train_15.py

We see here that in sample file tpot_pipeline_fold1_train_15.py the classifier output is not generated. Generally this is seen when is selecting best features is done before running classifier in the output tpot generated pipeline.  
Please let me know if further details are needed. 

[tpot_pipeline_fold1_train_15.py.txt](https://github.com/rhiever/tpot/files/342043/tpot_pipeline_fold1_train_15.py.txt)
[fold1_train.txt](https://github.com/rhiever/tpot/files/342045/fold1_train.txt)
[generate_tpot_model.py.txt](https://github.com/rhiever/tpot/files/342044/generate_tpot_model.py.txt)
",please find attached code essential replicate issue change version file removing extension used generate pipeline command use python tee command would create file similar see sample file classifier output generally seen best done running classifier output pipeline please let know,issue,positive,positive,positive,positive,positive,positive
229258863,"@ teaearlgraycold currently dont have a stack trace for the issue. 
",currently dont stack trace issue,issue,negative,neutral,neutral,neutral,neutral,neutral
229203389,"The issue is updated now. Please review the same. 
In export_utils.py, we are using pipeline_list[0] to generate the pipeline list as return value from function def unroll_nested_fuction_calls

Thus select_pipeline is being called twice. 
",issue please review generate pipeline list return value function thus twice,issue,positive,neutral,neutral,neutral,neutral,neutral
229195787,"Can you please add more detail to your issue? Do you have a stack trace of the error? Is the error repeatable?
",please add detail issue stack trace error error repeatable,issue,negative,neutral,neutral,neutral,neutral,neutral
229164565,"👍 This is a better way to represent categorical parameters than integers.
",better way represent categorical,issue,negative,positive,positive,positive,positive,positive
228874916,"I was thinking on something on these lines this days. But not exactly 5-fold. Maybe let the user specify how many folds they want (or even if they want no folding at all - just keep train-test as it is).

Implementing it like this would be interesting if we consider #146, or a variation of it, where the user wants to first find the best classifier with no CV (find the best, even if its overfitting). And latter fine tune the parameters of this one classifier with 3, 5 or even 10-fold CV (I've seen kagglers doing 10 fold CV... dunno the real benefit thou).
",thinking something day exactly maybe let user specify many want even want folding keep like would interesting consider variation user first find best classifier find best even latter fine tune one classifier even seen fold real benefit thou,issue,positive,positive,positive,positive,positive,positive
228610329,"@minimumnz: it's fairly easy to make that change - https://github.com/teaearlgraycold/tpot/tree/parallelize

But TPOT itself likely won't have local parallelization until cluster support is also added, since it'd be much nicer to have both cases covered by one library.
",fairly easy make change likely wo local parallelization cluster support also added since much covered one library,issue,positive,positive,positive,positive,positive,positive
228594160,"I'd love better parallel processing on a single machine. I feel sad when i see 3 cores at 0% and 1 at 100%
",love better parallel single machine feel sad see,issue,positive,positive,neutral,neutral,positive,positive
228582820,"[This](https://github.com/conda/conda-docs/blob/master/update-gh-pages.sh) seems like a good starting point.
",like good starting point,issue,positive,positive,positive,positive,positive,positive
228551681,"This should be a constraint in #91 .  

[`ClassifierMixins`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py#L301) and [`RegessorMixins`](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/base.py#L330) define their own scoring functions.  I have been experimenting in this [notebook](https://github.com/tonyfast/tpot/blob/refactor/Untitled163.ipynb).
",constraint define scoring notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
228549849,"@teaearlgraycold / @tonyfast, it should be pretty easy to swap out the classifiers for regressors, the scoring function with MSE, and have TPOT work the same otherwise, yes? Am I missing anything? Maybe we should prioritize this addition.
",pretty easy swap scoring function work otherwise yes missing anything maybe addition,issue,positive,positive,positive,positive,positive,positive
228310019,"TPOT doesn't currently support regression, but we have plans to do so in the near future once we have the basic algorithm worked out in the supervised classification case.
",currently support regression near future basic algorithm worked classification case,issue,negative,positive,neutral,neutral,positive,positive
227770035,"Just to threshold the parameter value under these conditions so the sklearn
classifier isn't so noisy

On Wed, Jun 22, 2016, 10:46 AM Antonio Augusto Santos <
notifications@github.com> wrote:

> The idea is to thrown an exception on TPOT when fit is called with data
> in this condition?
> Or to make it so that Nystroem doesn't make part of the population when
> this is the case?
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/182#issuecomment-227767469, or mute
> the thread
> https://github.com/notifications/unsubscribe/ADISYxN9CfFMYiwlNtYtyu_KHeyAhK9Sks5qOUrZgaJpZM4I7Mhn
> .
",threshold parameter value classifier noisy wed wrote idea thrown exception fit data condition make make part population case reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
227769597,"It's mostly to make sure that it doesn't throw the error while running, so we cap the parameter value when running Nystroem.
",mostly make sure throw error running cap parameter value running,issue,negative,positive,positive,positive,positive,positive
227767469,"The idea is to thrown an exception on TPOT when `fit` is called with data in this condition?
Or to make it so that `Nystroem` doesn't make part of the population when this is the case?
",idea thrown exception fit data condition make make part population case,issue,negative,positive,positive,positive,positive,positive
227714845,"Hi @teaearlgraycold. 
1) I kept _gradient_boosting and _xg_boosting because I was under the impression that this was the intent. But this is easy to overcome. Just say the word.
2) I prefer to keep the import at the top of the file, and use a flag do detect error, to respect pep8
3) I'll update the setup.py to add an extra_requires flag, so to make xgboost explicitly optional.
4) I'll update the travis scripts to depend on XGBoost

Please comment on 1 and 2.
I'll push new commits for 3 and 4 soon.
",hi kept impression intent easy overcome say prefer keep import top file use flag detect error respect pep update add flag make explicitly update travis depend please comment push new soon,issue,positive,positive,positive,positive,positive,positive
227618007,"I'd suggest doing this instead:

In `__init__()`

``` Python
try:
    from xgboost import XGBClassifier
    self._pset.addPrimitive(self._xg_boosting, [pd.DataFrame, float, float, int, int], pd.DataFrame)
except ImportError:
    self._pset.addPrimitive(self._gradient_boosting, [pd.DataFrame, float, float, float], pd.DataFrame)
```

Should also add XGBoost setup and version printing to `ci/.travis_install.sh`, `ci/.travis_test.sh`, and `setup.py`
",suggest instead python try import float float except float float float also add setup version printing,issue,negative,neutral,neutral,neutral,neutral,neutral
227583606,"@rhiever Found some time, and already sent a pull request. Please be free to point any issues (as I said, my first big PR ;) .)
",found time already sent pull request please free point said first big,issue,positive,positive,positive,positive,positive,positive
227578284,"There's also several cases where DecisionTreeClassifiers are used in the example code but never imported.
",also several used example code never,issue,negative,neutral,neutral,neutral,neutral,neutral
227498702,"My common use case is parallel cloud computing and I think that in order for any interesting dataset to come in handy with TPOT it has to scale.
I might consider leaving it up to the user which one he prefers to use since he knows best the use case.
",common use case parallel cloud think order interesting come handy scale might consider leaving user one use since best use case,issue,positive,positive,positive,positive,positive,positive
227492871,"That gets messy for us, too, because we're mixing setup (which should be in init) with the actual training (which should be in fit).

An alternative is to find another way to support the progress bar update call.
",messy u setup actual training fit alternative find another way support progress bar update call,issue,positive,positive,neutral,neutral,positive,positive
227491783,"Alternatively we can do something like this in fit():

``` Python
select_func = toolbox.select

toolbox.register('select', partial(_gp_next_generation, select_func))
```

But that's kinda messy.

Not really a fan of having 2 of each of the GP functions just so one can be freely overwritten.
",alternatively something like fit python partial messy really fan one freely,issue,positive,positive,positive,positive,positive,positive
227491098,"Maybe the best path is to expose specific functions to the user that actually perform the selection/mutation/etc. and can be overwritten, and have separate decorated internal functions that call those functions.
",maybe best path expose specific user actually perform separate decorated internal call,issue,positive,positive,positive,positive,positive,positive
227488795,"The decorator needs to wrap some piece of code that gets called at the start of each generation.

https://github.com/DEAP/deap/blob/master/deap/algorithms.py#L164

The only external function call is to the toolbox.select function.
",decorator need wrap piece code start generation external function call function,issue,negative,neutral,neutral,neutral,neutral,neutral
227487819,"What if we assumed that any changes to the GP functionality would occur before `fit()`, and placed the decorator around the function in the `fit()` call?
",assumed functionality would occur fit decorator around function fit call,issue,positive,positive,positive,positive,positive,positive
227467773,"We could if we controlled the toolbox.register function - or if there was some function in the TPOT class like `insert_custom_selection_function`. Even just overwriting the tpot selection function (like below) wouldn't work, as it would overwrite the decorator.

``` Python
tpot = TPOT()
tpot._combined_selection_operator = new_function
```
",could function function class like even selection function like would work would overwrite decorator python,issue,positive,neutral,neutral,neutral,neutral,neutral
227466871,"We couldn't just decorate the incoming function?

On Tuesday, June 21, 2016, Daniel notifications@github.com wrote:

> It's also important to note that our selection operator is decorated with
> a function that updates the tqdm progress bar, so any custom selection
> operators would need to also be decorated if the progress bar is used.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/171#issuecomment-227464239, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7t-e8G3kptBmBNORJuWlSxPBzVPcoks5qN_oYgaJpZM4I5dJv
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",could decorate incoming function june wrote also important note selection operator decorated function progress bar custom selection would need also decorated progress bar used reply directly view mute thread postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
227464239,"It's also important to note that our selection operator is decorated with a function that updates the tqdm progress bar, so any custom selection operators would need to also be decorated if the progress bar is used.
",also important note selection operator decorated function progress bar custom selection would need also decorated progress bar used,issue,positive,positive,positive,positive,positive,positive
227444960,":+1: Good thinking. Maybe we should write a short guide describing that
hook in to TPOT for customizing the GP algorithm for advanced users.

On Tuesday, June 21, 2016, Daniel notifications@github.com wrote:

> We're using some customized code from GP to perform tree generation in
> TPOT, so as of right now you wouldn't be able to just throw in some other
> DEAP function like genGrow or genHalfandHalf.
> 
> As for other paramaterizations - the TPOT constructor is already rather
> bulky. You can still do something like this:
> 
> tpot = TPOT()
> tpot._toolbox.register('select', my_custom_selector)
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/171#issuecomment-227442257, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7t-Trc5zjcfu4lzQc49Wabw4RKAPGks5qN-l3gaJpZM4I5dJv
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",good thinking maybe write short guide hook algorithm advanced june wrote code perform tree generation right would able throw function like constructor already rather bulky still something like reply directly view mute thread postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
227442257,"We're using some customized code from GP to perform tree generation in TPOT, so as of right now you wouldn't be able to just throw in some other DEAP function like `genGrow` or `genHalfandHalf`.

As for other paramaterization - the TPOT constructor is already rather bulky. You can still do something like this:

``` Python
tpot = TPOT()
tpot._toolbox.register('select', my_custom_selector)
```
",code perform tree generation right would able throw function like constructor already rather bulky still something like python,issue,positive,positive,positive,positive,positive,positive
227431965,"Good to hear. We refined TPOT's optimization process pretty significantly
in 0.4. Hopefully you won't have major overfitting issues now.

On Tuesday, June 21, 2016, robotijn notifications@github.com wrote:

> 0.4.0 works pretty good except for the model saving, this is giving an
> error sometimes (I posted the error). I can't share my data set,
> unfortunately...
> 
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/172#issuecomment-227431277, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7t1YogFIyZscaDJet0CkArWsPzj-_ks5qN9_QgaJpZM4I5vuL
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",good hear refined optimization process pretty significantly hopefully wo major june wrote work pretty good except model saving giving error sometimes posted error ca share data set unfortunately reply directly view mute thread postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
227431277,"0.4.0 works pretty good except for the model saving, this is giving an error sometimes (I posted the error). I can't share my data set, unfortunately...
",work pretty good except model saving giving error sometimes posted error ca share data set unfortunately,issue,negative,positive,positive,positive,positive,positive
227424907,"@teaearlgraycold, can you please look into this? We should thoroughly test all of the operators for exporting before the 0.4 release.
",please look thoroughly test release,issue,negative,neutral,neutral,neutral,neutral,neutral
227424542,"👍 Let us know how it goes. If you still have issues, please share your data set (if possible) and I'll try to find out what's going on.
",let u know go still please share data set possible try find going,issue,positive,neutral,neutral,neutral,neutral,neutral
227359574,"In [4]: tpot.__ version__ 
Out[4]: '0.3.0'

I installed it using pip in Anaconda (python 3.5.1)

I see you have version 0.4.0 in the repo.
I'll try to install that one and check the results.
",pip anaconda python see version try install one check,issue,negative,neutral,neutral,neutral,neutral,neutral
227335480,"Eventually, yes. There's been discussion of using Dask to parallelize TPOT (cc @tonyfast). We've also been thinking about PySpark for parallel cloud computing. However, we're still focused on getting the core algorithm and tool finished before we really work our way into scaling to distributed environments.
",eventually yes discussion parallelize also thinking parallel cloud however still getting core algorithm tool finished really work way scaling distributed,issue,negative,positive,neutral,neutral,positive,positive
227335271,"Sounds good, @drorasaf! I'll mark the issue as being worked on.
",good mark issue worked,issue,negative,positive,positive,positive,positive,positive
227308981,"Could I take up this ticket? seems like a good way for me to delve into more tpot internals
",could take ticket like good way delve internals,issue,positive,positive,positive,positive,positive,positive
227260262,"> For example, the standard is now to use the NSGA2 algorithm for selection, but I would like to experiment with other selection criteria also. Also the initialization method,crossover and mutation methods can be opened up.

That's interesting. Our idea with TPOT was that the GP process would be hidden from the user: we would figure out the GP settings, and the user wouldn't need to worry about it.

I suppose we could parameterize the selection, mutation, etc. functions as we do the scoring function. Thoughts @teaearlgraycold?
",example standard use algorithm selection would like experiment selection criterion also also method crossover mutation interesting idea process would hidden user would figure user would need worry suppose could selection mutation scoring function,issue,negative,positive,positive,positive,positive,positive
227259873,"> Another issue is the score that comes out of the algorithm. When I asked about the final score not being what I see during the runs I got the following answer: . The score achieved during training is an internal cross-validation score, whereas the score at the end is based on the holdout testing data. That's why you see a difference in the score. In the next version of TPOT, we're removing that information about score throughout the run because it seems to be confusing people for no good reason.

In the latest version of TPOT (0.4, release coming soon, but you can access it on master now), we still print the best internal CV score as the algorithm proceeds:

<img width=""1431"" alt=""screen shot 2016-06-20 at 4 27 17 pm"" src=""https://cloud.githubusercontent.com/assets/1719223/16209262/34ab4076-3704-11e6-899f-024ba76b2747.png"">

I will change the output text to clarify that this is an internal CV score, as you suggest.
",another issue score come algorithm final score see got following answer score training internal score whereas score end based holdout testing data see difference score next version removing information score throughout run people good reason latest version release coming soon access master still print best internal score algorithm proceeds screen shot change output text clarify internal score suggest,issue,positive,positive,positive,positive,positive,positive
227254108,"What @teaearlgraycold said. `get_params` is necessary for `sklearn.cross_validation.cross_val_score` and related CV methods to work.
",said necessary related work,issue,negative,neutral,neutral,neutral,neutral,neutral
227117286,"No problem! We'll slate it for a later release than the one coming up this
week.

On Monday, June 20, 2016, Antonio Augusto Santos notifications@github.com
wrote:

> Hello @rhiever https://github.com/rhiever. The last couple of weeks I'm
> having a lot of work at Day Job (where I can do most of my contribution). I
> hope to get something done by next week or so.
> 
> In the mean time, If there is any one else interested on it, please don't
> be shy to jump in :) (But I'm still committed to it!).
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/154#issuecomment-227115420, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7tzdR1AR3-wssVNI12chSNJ_zyALSks5qNnbigaJpZM4Ikwbk
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",problem slate later release one coming week june wrote hello last couple lot work day job contribution hope get something done next week mean time one else interested please shy jump still reply directly view mute thread postdoctoral researcher institute university twitter,issue,negative,negative,neutral,neutral,negative,negative
227115420,"Hello @rhiever. The last couple of weeks I'm having a lot of work at Day Job (where I can do most of my contribution). I hope to get something  done by next week or so.

In the mean time, If there is any one else interested on it, please don't be shy to jump in :) (But I'm still committed to it!).
",hello last couple lot work day job contribution hope get something done next week mean time one else interested please shy jump still,issue,positive,negative,negative,negative,negative,negative
226844359,"> so the one you are using now is different from the one you posted above, right?

Yes that's correct.
",one different one posted right yes correct,issue,negative,positive,positive,positive,positive,positive
226831679,"so the one you are using now is different from the one you posted above, right?
I wouldn't say there is consensus but we can discuss there.
",one different one posted right would say consensus discus,issue,negative,positive,positive,positive,positive,positive
226792649,"How's this coming along, @KhaoticMind? Need any feedback?
",coming along need feedback,issue,negative,neutral,neutral,neutral,neutral,neutral
226792533,"Just wanted to ping you, @bartleyn, to see how this was coming along.
",ping see coming along,issue,negative,neutral,neutral,neutral,neutral,neutral
226790153,"See https://github.com/scikit-learn/scikit-learn/issues/6747#issuecomment-217587210 for a detailed discussion of balanced accuracy. I think there is consensus to add this metric to sklearn now.
",see detailed discussion balanced accuracy think consensus add metric,issue,negative,positive,positive,positive,positive,positive
225735371,"> Is there a place we can start putting research and ideas into for the post 0.4 release development?

You can start up issues on the repo for us to discuss, or use an existing issue if it's related.
",place start research post release development start u discus use issue related,issue,negative,neutral,neutral,neutral,neutral,neutral
225454968,"Sorry for the delay: got confused by the future state of DEAP on master, which had a c extension, etc. Turns out, in its current form, `1.0.2.post2`, DEAP is pretty straightforward, so I imagine it will work the first time! I've done a bit more research about xgboost: doable, but maybe not pretty.
",sorry delay got confused future state master extension turn current form post pretty straightforward imagine work first time done bit research doable maybe pretty,issue,negative,positive,neutral,neutral,positive,positive
225381601,"I would really like to discuss this idea with y'all.  I would like champion bringing the API closer to native `scikit-learn` opinions.  I started [working on a refactor with the expectation of making generic model pipelines](https://github.com/tonyfast/tpot/tree/refactor), but I think at this point is was largely exploratory to understand the data flow.  One of the main successes was being able to have `tpot` always end on a classifier.

After some deeper dives in to `scikit-learn`, there are methods like [`Pipelines` and `FeatureUnion`](http://scikit-learn.org/stable/modules/pipeline.html) that would assist `tpot` through the evolution process.  There is an example of a model [`deap` model built on `scikit-learn`](https://github.com/rsteca/sklearn-deap).

Is there a place we can start putting research and ideas into for the post 0.4 release development?  
",would really like discus idea would like champion closer native working expectation making generic model think point largely exploratory understand data flow one main able always end classifier like would assist evolution process example model model built place start research post release development,issue,positive,positive,positive,positive,positive,positive
225366145,"Confirmed. Looks like the same is true for all GradientBoostingClassifier export code: https://github.com/rhiever/tpot/blob/master/tpot/export_utils.py#L445

Let's make sure this doesn't happen in any of the export code.

Adding this bugfix request to the 0.4 release.
",confirmed like true export code let make sure happen export code request release,issue,positive,positive,positive,positive,positive,positive
225318293,"Once we get 0.4 out, I'd love to explore this idea. Several people (including @tonyfast) have expressed interest in designing a ""grammar"" for TPOT pipelines to help constrain the pipeline structure.
",get love explore idea several people expressed interest designing grammar help constrain pipeline structure,issue,positive,positive,positive,positive,positive,positive
225318048,"Are we still interested in testing some of this in the current state of TPOT? I went ahead and implemented my idea, and am happy to try and get a PR working.
",still interested testing current state went ahead idea happy try get working,issue,positive,positive,positive,positive,positive,positive
225171135,"Hi @mglowacki100! We have an example notebook [here](https://github.com/rhiever/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb) (linked in the [docs](http://rhiever.github.io/tpot/)) showing how to do that. But do you mean putting such an example in the README?
",hi example notebook linked showing mean example,issue,negative,negative,negative,negative,negative,negative
224970960,"In your formula, `len(result[result['class'] == this_class]` is just `np.sum(result['class'] == this_class]`, right?

So you compute for each class

TP / (TP  + FN)

which is recall.

and then average over classes, right? That's what your code says and that's what wikipedia suggests, I think (though only for the two-class case https://en.wikipedia.org/wiki/Accuracy_and_precision)

Computing recall and averaging over all classes is macro average recall.

I'm not sure what you mean by not including TNR. Your definition as in the code above doesn't include it, right. Do you have a reference for that being the semantics of balanced accuracy in the multi-class case?

I don't think arguing about if it is a good metric or should be changed is a good idea if you use a name that already has particular semantics. In the multi-class case they don't seem to be that established, but it would be good to know what other people mean.
",formula result result result right compute class recall average class right code think though case recall class macro average recall sure mean definition code include right reference semantics balanced accuracy case think good metric good idea use name already particular semantics case seem established would good know people mean,issue,positive,positive,positive,positive,positive,positive
224895094,"I'm reopening this issue now that I'm unsure again. The primary difference seems to be that our implementation of balanced accuracy also takes into account TNR, whereas other implementations only take into account TPR (recall).

I don't quite understand the intuition in not including TNR in the multiclass case. I understand that in the binary classification case, TPR for class `0` = TNR for class `1`. In the multiclass case that becomes muddled: TPR for class `0` = TNR for all other classes.
",issue unsure primary difference implementation balanced accuracy also account whereas take account recall quite understand intuition case understand binary classification case class class case becomes class class,issue,negative,positive,positive,positive,positive,positive
224320607,"> use 2to3 to convert it to a Python 3-compatible version when it's installed on a Python 3 distribution

Heh, that part works, at least. But the tests are just straight up busted.
",use convert python version python distribution part work least straight busted,issue,negative,negative,neutral,neutral,negative,negative
224312420,"The DEAP devs seem to have the project in an odd state where they've written the code in Python 2.7 and use `2to3` to convert it to a Python 3-compatible version when it's installed on a Python 3 distribution. Not sure why they do that, really.

We have #154 to make XGBoost an optional dependency, but it likely won't be coming out in v0.4.
",seem project odd state written code python use convert python version python distribution sure really make optional dependency likely wo coming,issue,negative,positive,positive,positive,positive,positive
224304766,"I started here:
- https://github.com/bollwyvl/staged-recipes/tree/tpot

### deap

I am trying to get its compiled stuff building (hypervolume), starting on OSX (as it's what I've got locally, and often causes the most problems... well, aside from windows).

The tests distributed with the current pypi version are broken... so for the time being I'm building against `master`. It would be nice if we could work with them to get a release out in concert with tpot 0.4... seems a bit overdue!

### xgboost

> we've dropped XGBoost as a hard dependency

Will it use it if available? Does it make tpot better/faster/stronger? If so, it might be worth it to go ahead and bet it built. There are a number of builds of it out on [anaconda.org](https://anaconda.org/search?q=xgboost), so we'll have some places to start.
",trying get stuff building hypervolume starting got locally often well aside distributed current version broken time building master would nice could work get release concert bit overdue hard dependency use available make might worth go ahead bet built number start,issue,negative,positive,neutral,neutral,positive,positive
223848175,"In v0.4 (coming soon), we've dropped XGBoost as a hard dependency. So that won't be an issue soon. That's good news about DEAP.
",coming soon hard dependency wo issue soon good news,issue,negative,positive,positive,positive,positive,positive
223847986,"Welp. Basically we have to track down all the pypi-only deps (and deps of deps) we'll need first.... bundling is bad practice.

DEAP has been built before, so should be easy. XGBoost might be trickier. Hopefully, it's gcc deps aren't that bad.

I prefer to get all the tests running for each lib. If this has pypi deps, that's okay.

Finally we can write the actual tpot recipe.

Hopefully, we can bring them all along in one pr to staged-recipes.... i did one with like six, which was too many. Either way, we'll end up with a number of feedstocks.

I am happy to start the process, having now done it a couple of times! I'll get some wifi on my flight, hopefully have something going soon!
",basically track need first bad practice built easy might hopefully bad prefer get running finally write actual recipe hopefully bring along one one like six many either way end number happy start process done couple time get flight hopefully something going soon,issue,positive,positive,neutral,neutral,positive,positive
223825189,"Below are the UML diagrams for the current refactor.  The refactor is mostly working, I need to track down some heisenbugs. It is weird when you get different errors every time you run the same function.   I have been [using this notebook for development](https://github.com/tonyfast/tpot/blob/refactor/refactor.ipynb).

The [`models.base`](https://github.com/tonyfast/tpot/blob/refactor/tpot/models/base.py) does a lot of the heavy lifting.  It decides whether to produce a `transform`, `masking`, or `classification` using the `sklearn` base classes.

![](https://raw.githubusercontent.com/tonyfast/tpot/refactor/packages_tpot.png)

![](https://raw.githubusercontent.com/tonyfast/tpot/refactor/classes_tpot.png)

I made some changes to the Primitive diagram. [`main`](https://github.com/tonyfast/tpot/blob/refactor/tpot/tpot.py#L127) exports a Pandas series at the end.  Only [certain `sklearn` models](https://github.com/tonyfast/tpot/blob/refactor/tpot/models/base.py#L71) can return a `Series`.  Exporting a series is analogous to saying, ""Hey I made a classification"".  Classifiers [can also return a `DataFrame`](https://github.com/tonyfast/tpot/blob/refactor/tpot/tpot.py#L196) which allows them to be placed as an intermediate in the graph.  Basically I added this to assure that the algorithm evaluates a classifier.

---

> [Update: All of the models complete for the MNIST dataset](https://github.com/tonyfast/tpot/blob/refactor/refactor.ipynb)

### The highest score is 0.982261640798

##### fit errors 4 vs. score errors 2 of 275 executions

```
knnc(df, sub(87, 98))
```
",current mostly working need track weird get different every time run function notebook development lot heavy lifting whether produce transform classification base class made primitive diagram main series end certain return series series analogous saying hey made classification also return intermediate graph basically added assure algorithm classifier update complete highest score fit score sub,issue,positive,negative,neutral,neutral,negative,negative
223733533,"I intend to bring the coding style back closer to what y'all have been working with.  All of the code is pep8 compliant at the moment except for some comments.  I am trying to get a hold of the model itself; it is a bit confusing.  This pull request is part research and part serious.  

I am offering up this code to see if I am understanding the model clearly from a total outsider perspective.  I think there are some awesome UI features that can be built onto `tpot` using the Jupyter notebook.  I hope some of these intentions can be useful to the project.
",intend bring style back closer working code pep compliant moment except trying get hold model bit pull request part research part serious offering code see understanding model clearly total outsider perspective think awesome built onto notebook hope useful project,issue,positive,positive,positive,positive,positive,positive
223733295,"Well I'm the main guy who will be pushing the 0.4 release forward. I'd say
maybe hold off until that's out (should be soon).

Also you seem to be changing up the code style a lot, which I'd warn again.

On Fri, Jun 3, 2016, 11:19 PM Tony Fast notifications@github.com wrote:

> I totally respect adding dependencies. traitlets is part of Anaconda
> because it is used in the notebook and ipython. toolz can be installed in
> an Anaconda environment using pip in the environment; it extends itertools
> and functools from the standard lib with an underlying interest in
> parallelizable. If there is a hard stop on dependencies being available in
> Anaconda then conda forge is always an option.
> 
> I am going to keep working on this. I'd be stoked to bounce ideas off of
> @teaearlgraycold https://github.com/teaearlgraycold while you are tied
> up with the 0.4 release.
> 
> —
> You are receiving this because you were mentioned.
> 
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/164#issuecomment-223733175, or mute
> the thread
> https://github.com/notifications/unsubscribe/ADISY0kIm00NQXr8tPkNZaaWLtY5GPeFks5qIO6xgaJpZM4It4IR
> .
",well main guy pushing release forward say maybe hold soon also seem code style lot warn tony fast wrote totally respect part anaconda used notebook anaconda environment pip environment standard underlying interest hard stop available anaconda forge always option going keep working bounce tied release reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
223733175,"I totally respect adding dependencies.  `traitlets` is part of Anaconda because it is used in the `notebook` and `ipython`.  `toolz` can be installed in an Anaconda environment using `pip` in the environment; it extends `itertools` and `functools` from the standard lib with an underlying interest in parallelizable code.  If there is a hard stop on dependencies being available in Anaconda then conda forge is always an option.

I am going to keep working on this.  I'd be stoked to bounce ideas off of @teaearlgraycold while you are tied up with the 0.4 release.
",totally respect part anaconda used notebook anaconda environment pip environment standard underlying interest code hard stop available anaconda forge always option going keep working bounce tied release,issue,positive,positive,neutral,neutral,positive,positive
223731684,"Hey! I'm stoked that you're so into TPOT lately. I'm currently focused on getting v0.4 out, but I promise I'll join the conversation about the major refactor soon. :-)

BTW, one thing to keep in mind: We're trying to keep Python lean in terms of dependencies, so adding a new dependency (especially ones not in Anaconda) will be a hard sell. It's very important to me that TPOT remains easy to install.
",hey lately currently getting promise join conversation major soon one thing keep mind trying keep python lean new dependency especially anaconda hard sell important remains easy install,issue,positive,positive,positive,positive,positive,positive
223651070,"👍 Will raise a new issue for that then.

Otherwise, do you think the terminal (parameter) set in https://github.com/rhiever/tpot/issues/162#issuecomment-223483641 looks good, considering the use cases?
",raise new issue otherwise think terminal parameter set good considering use,issue,negative,positive,positive,positive,positive,positive
223649400,"Not really. That's only true for PCA. For all other methods, changing `n_components` changes all components. And for NMF for example, it is not limited by the number of features in the original set.
",really true example limited number original set,issue,positive,positive,positive,positive,positive,positive
223648619,"Why do you disagree? :-) I'd say it was even supported by what we found from the benchmark.
",disagree say even found,issue,negative,neutral,neutral,neutral,neutral,neutral
223648431,"That's fair. Can you think of anything wrong with simply removing `n_components` as an evolvable feature and using the defaults? Worst case, ""just"" has extra not-so-useful features that can be pruned by feature selection, yeah?
",fair think anything wrong simply removing evolvable feature worst case extra feature selection yeah,issue,negative,negative,negative,negative,negative,negative
223647469,"> Is it typical to create hundreds/thousands of features from a RBFSampler?

yes. I wouldn't go below hundreds.

> Typically people use PCA etc. to compress the features, right? Is it useful to use PCA and maintain the same feature space?

Depends. But with ""scales with"" I meant ""is on the order of"". So if you want half as many components as you had features, it might be a large number.
",typical create yes would go typically people use compress right useful use maintain feature space scale meant order want half many might large number,issue,positive,positive,positive,positive,positive,positive
223646319,"> `n_components` scales with the number of features (kind of).

Typically people use PCA etc. to compress the features, right? Is it useful to use PCA and maintain the same feature space?

> For `RBFSampler`, it could be hundreds or thousands even if the dimensionality of the original data is small.

Is it typical to create hundreds/thousands of features from a `RBFSampler`?

We _could_ just remove `n_components` as an evolvable parameter and always use the default, which in most cases equals the original number of features.
",scale number kind typically people use compress right useful use maintain feature space could even dimensionality original data small typical create remove evolvable parameter always use default original number,issue,positive,positive,positive,positive,positive,positive
223642515,"Yes, we fix `n_estimators=500` for the GradientBoostingClassifier because the docs claim:

> The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.

And I felt safe fixing `n_estimators=500` for AdaBoostClassifier because the docs say (emphasis mine):

> The **maximum** number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.
",yes fix claim number perform gradient fairly robust large number usually better performance felt safe fixing say emphasis mine maximum number case perfect fit learning procedure stopped early,issue,positive,positive,positive,positive,positive,positive
223641673,"`n_components` scales with the number of features (kind of). For `RBFSampler`, it could be hundreds or thousands even if the dimensionality of the original data is small.
",scale number kind could even dimensionality original data small,issue,positive,positive,positive,positive,positive,positive
223641584,"e.g. `[round(x, 2) for x in np.linspace(0.01, 1.00, 100).tolist()]`?

Just remembered that the issue only rises when I convert the numpy array to a list.
",round issue convert array list,issue,negative,negative,negative,negative,negative,negative
223640770,"> Obviously not a huge deal for most use cases, but I don't want to be outputting 0.049999999999 in the exported code when it should be 0.05. (Stupid floating point precision issues.)

How about limiting the precision?
",obviously huge deal use want code stupid floating point precision limiting precision,issue,negative,negative,negative,negative,negative,negative
223640422,"> Do you mean ""list of possible parameters to choose from""?

Yep!

> Btw `np.arange(1., 101.) / 100. == np.linspace(0.01, 1.00, 100)` which might be more readable.

The main difference is that `np.linspace` produces imprecise floats, e.g., it represents 0.05 as 0.049999999999. Obviously not a huge deal for most use cases, but we don't want to be outputting 0.049999999999 in the exported code when it should be 0.05. (Stupid floating point precision issues.)

> I often use C=100, which is currently not covered.

We can add coarse coverage beyond 50 (e.g., 60, 70, 80, 90, 100) in the float terminal set.

> Some parameters can be floats or integers, like max_features and min_samples_split in trees. How do you deal with that?

The GP trees are strongly typed, so we pre-select a type for each parameter. So yes, we tend to favor floats when choosing the type because they tend to represent fractions rather than counts.

> I'm not sure where you use integers and where you use floats, so I can't tell you if the ranges are good.
> If you're searching over tol, 0.0001 might be a bit big. In general maybe just also add 1e-5 and 1-e6 to the floats.

#161 includes many of the parameters and types that we will be optimizing. https://github.com/rhiever/tpot/issues/162#issuecomment-223482183 is a pretty exhaustive list of our use cases, I believe.
",mean list possible choose yep might readable main difference imprecise obviously huge deal use want code stupid floating point precision often use currently covered add coarse coverage beyond float terminal set like deal strongly type parameter yes tend favor choosing type tend represent rather sure use use ca tell good searching tol might bit big general maybe also add many pretty exhaustive list use believe,issue,positive,positive,positive,positive,positive,positive
223635073,"Do you mean ""list of possible parameters to choose from""?

Btw `np.arange(1., 101.) / 100. == np.linspace(0.01, 1.00, 100)` which might be more readable.

I often use C=100, which is currently not covered.
Some parameters can be floats or integers, like `max_features` and `min_samples_split` in trees. How do you deal with that?

if you use integers for `max_features` and `min_samples_split` the first scales with the number of features and the second (maybe?) with the number of samples. I guess that's a good argument to use floats there? Otherwise you'd need to expand your range. Do you search n_estimators on trees still?

I'm not sure where you use integers and where you use floats, so I can't tell you if the ranges are good.
If you're searching over `tol`, `0.0001` might be a bit big. In general maybe just also add 1e-5 and 1-e6 to the floats.
",mean list possible choose might readable often use currently covered like deal use first scale number second maybe number guess good argument use otherwise need expand range search still sure use use ca tell good searching tol might bit big general maybe also add,issue,positive,positive,positive,positive,positive,positive
223483641,"So perhaps a good `float` terminal set would be:

```
np.concatenate(([0., 1e-6, 1e-5, 1e-4, 1e-3],
                np.linspace(0.01, 1.00, 100),
                np.linspace(2., 50., 49),
                np.linspace(60., 100., 5)))
```

and a good `int` terminal set would be:

```
np.arange(0, 51, 1)
```

Am I missing any use cases here? @amueller, thoughts?
",perhaps good float terminal set would good terminal set would missing use,issue,positive,positive,positive,positive,positive,positive
223482183,"Considerations for `float` terminals:
- We will need good coverage in the range `[0., 1.]`, as many models and preprocessors use values in that range to specify percentages
- We will need good coverage in the much lower ranges (<= 0.01), as many models with the `C` and `learning_rate` parameters require lower float values
- We will need moderate coverage in the higher ranges (>=1), as some models benefit from e.g. higher `C` values. It's still unclear to us what specific values are useful here.
- SelectFwe uses `float`s in the range `[0.001, 0.05]`
- VarianceThreshold, Binarizer, RBFSampler, & Nystroem use `float`s in an unknown range

Considerations for `int` terminals:
- We use `int`s to encode multi-choice model parameters (e.g., options A, B, or C), so we need good coverage between 0-9
- The KNeighborsClassifier uses `int`s for its `n_neighbors` parameter, but `n_neighbors` doesn't seem to be useful beyond 25
- RFE & SelectKBest use `int`s to specify the number of features to select
- RandomizedPCA, RBF, FastICA, FeatureAgglomeration, & Nystroem use `int`s to specify the number of components

So we need to decide on terminal sets that account for all these use cases without creating a huge search space.
",float need good coverage range many use range specify need good coverage much lower many require lower float need moderate coverage higher benefit higher still unclear u specific useful float range use float unknown range use encode model need good coverage parameter seem useful beyond use specify number select use specify number need decide terminal account use without huge search space,issue,positive,positive,positive,positive,positive,positive
223479455,"> having so many mathematical operators in the TPOT trees causes most of the mutations and crossovers to happen on the mathematical operators rather than the pipeline operators, which is probably bad.

That's what I've seen.

> BTW: We should add booleans to the terminal set for the True/False parameters.

I agree
",many mathematical happen mathematical rather pipeline probably bad seen add terminal set agree,issue,negative,negative,neutral,neutral,negative,negative
223479368,"I've been thinking about removing the mathematical operators in favor of having a fixed terminal set. I think that in addition to being prone to producing crazy values, having so many mathematical operators in the TPOT trees causes most of the mutations and crossovers to happen on the mathematical operators rather than the pipeline operators, which is probably bad. (I still need to test that experimentally.)

Let's just apply the caps for now and we'll raise a separate issue for dealing with the terminal set.

BTW: We should add booleans to the terminal set for the True/False parameters.
",thinking removing mathematical favor fixed terminal set think addition prone crazy many mathematical happen mathematical rather pipeline probably bad still need test experimentally let apply raise separate issue dealing terminal set add terminal set,issue,negative,negative,neutral,neutral,negative,negative
223478551,"Any good alternatives to threshholding `int`s and `float`s besides just min/max-ing them?

I feel like taking the wild values that GP can provide us with and just doing `max(1.0, min(0., max_features))` won't be ideal. You'd just end up with a bunch of 1s and 0s.

What if I added a new terminal type, something like:

``` Python
class float_limited(float):
    def __init__(self):
        pass

. . .

for val in [1.0, 0.1, 0.01, 0.001, 0.0001]:
            self._pset.addTerminal(val, float_limited)

. . .

self._pset.addPrimitive(self._extra_trees, [pd.DataFrame, int, float_limited], pd.DataFrame)
```

For everything that should be in a distribution like 1 to 50, we could simply multiply the `float_limited` by some scalar in the operator, so in that case `50`.

---

Alternatively we could try to make **all** floats in the 1.0 to 0.0001 range and then scale them appropriately as mentioned above.
",good float besides feel like taking wild provide u min wo ideal end bunch added new terminal type something like python class float self pas everything distribution like could simply multiply scalar operator case alternatively could try make range scale appropriately,issue,positive,positive,positive,positive,positive,positive
223470922,"Alright. I had not started work on it yet so that's fine.
",alright work yet fine,issue,negative,positive,positive,positive,positive,positive
223470818,"@teaearlgraycold, forget this issue for now. I will have an updated version of it in a separate issue soon.
",forget issue version separate issue soon,issue,negative,neutral,neutral,neutral,neutral,neutral
223427744,"Yes. If you download this repo and run `python setup.py install` in the base directory (make sure to `pip uninstall tpot` first, you'll have access to the current dev state of TPOT 0.4. No guarantees that it's bug-free yet, though. :-)
",yes run python install base directory make sure pip first access current dev state yet though,issue,positive,negative,neutral,neutral,negative,negative
223422864,"@tonyfast I pushed out a commit so your line number is off. You were referring to the _apply_default_params() method though, correct?

That method is largely there so that certain parameters can be blindly applied to all estimators (regardless of whether they're applicable), and behaves differently than set_params.

Also, currently my refactor branch is in a state where it can be ran - albeit with only 2 classifiers and one preprocessor at the moment.
",commit line number method though correct method largely certain blindly applied regardless whether applicable differently also currently branch state ran albeit one moment,issue,positive,negative,neutral,neutral,negative,negative
223406424,"I started looking into a refactor myself to understand the project a little bit more.  I haven't put this into scripts yet, but the idea in [drawn out in the notebook](https://anaconda.org/tonyfast/untitled154/notebook).

There were a few main opinions for this to study.
- `training` and `testing`  classes information are contained in a Pandas DataFrame multiindex.
- Make a custom `BaseEstimator` that has a `fit_predict` classmethod.  This method fits the model with the training then applies a `transform`, `predict`, or `selection/support` operation.
- New models are created by subclassing an existing `sklearn` model with some defaults.

With a limited corpus of models so far this gets all the way through.  There is a problem with the scoring function at the moment.

Are there are reasons not to use a direct `sklearn` models as subclasses?  The [`BaseEstimator`](http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator) provides method for `get_params` and `set_params` which [may help here](https://github.com/teaearlgraycold/tpot/blob/refactor/tpot/operators/base.py#L48).  
",looking understand project little bit put yet idea drawn notebook main study training testing class information make custom method model training transform predict operation new model limited corpus far way problem scoring function moment use direct method may help,issue,negative,positive,neutral,neutral,positive,positive
223391119,"I'll keep that in mind when I start working on the refactored code's export utils
",keep mind start working code export,issue,negative,neutral,neutral,neutral,neutral,neutral
223390530,"Here's some example code that works:

``` python
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.feature_selection import SelectKBest, VarianceThreshold
from sklearn.datasets import load_digits
from sklearn.cross_validation import cross_val_score

data = load_digits()

clf = make_pipeline(make_union(PolynomialFeatures(),
                               VotingClassifier(estimators=[('rf1', RandomForestClassifier())])),
                    VarianceThreshold(),
                    SelectKBest(k=5),
                    RandomForestClassifier())

cross_val_score(clf, data.data, data.target, cv=5)
```
",example code work python import import import import import import data,issue,negative,neutral,neutral,neutral,neutral,neutral
223390196,"That's awesome. Looks like we'll be able to export to sklearn pipelines after all, @teaearlgraycold!
",awesome like able export,issue,positive,positive,positive,positive,positive,positive
223360934,"Ahhh, I get it now. Just looked at the docs for the RFE predict function. Basically:

```
make_pipeline(make_union(PolynomialFeatures(),
VotingClassifier(estimators=['rf1', RandomForestClassifier()])),
RFE(estimator=RandomForestClassifier()))
```

and

```
make_pipeline(make_union(PolynomialFeatures(),
VotingClassifier(estimators=['rf1', RandomForestClassifier()])),
RFE(estimator=RandomForestClassifier()),
RandomForestClassifier())
```

would do the same thing.
",get predict function basically would thing,issue,negative,neutral,neutral,neutral,neutral,neutral
223359725,"`RFE` has a `predict`, so both of the pipelines you outlined can predict. They do different things, though.
",predict outlined predict different though,issue,negative,neutral,neutral,neutral,neutral,neutral
223359001,"So we could do something like:

```
make_pipeline(make_union(PolynomialFeatures(),
VotingClassifier(estimators=['rf1', RandomForestClassifier()])),
RFE(estimator=SVC(kernel='linear')),
RandomForestClassifier())
```

if we wanted RF classification at the end of the pipeline.
",could something like classification end pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
223356960,"Btw, this is feature selection using RF. That doesn't necessarily imply classification with RF.
",feature selection necessarily imply classification,issue,negative,neutral,neutral,neutral,neutral,neutral
223356781,"RFE is a model-based feature selection. How should it do feature selection without a model? SelectFromModel is also a meta-estimator, while the feature selection methods that are not model based are not.
",feature selection feature selection without model also feature selection model based,issue,negative,neutral,neutral,neutral,neutral,neutral
223355830,"Wait, why? Is that specific to RFE or feature selection methods?
",wait specific feature selection,issue,negative,neutral,neutral,neutral,neutral,neutral
223333406,"How would that look in sklearn code? Like this?

```
make_pipeline(make_union(PolynomialFeatures(), VotingClassifier(estimators=['rf1', RandomForestClassifier()])), RFE(), RandomForestClassifier())
```
",would look code like,issue,negative,neutral,neutral,neutral,neutral,neutral
223332613,"> and take the predictions of the Random Forest

That was the part I wasn't sure about. Ok then `VotingClassifier`.
",take random forest part sure,issue,negative,neutral,neutral,neutral,neutral,neutral
223332329,"Let's use this pipeline as an example:

![image](https://cloud.githubusercontent.com/assets/1719223/15751014/850ae794-28b6-11e6-813d-aaf06d368c46.png)

but instead of PCA there's a Random Forest there. So what this pipeline does is:

1) Take a copy of the data set and apply Polynomial Features to create data set A

2) Take another copy of the data set, fit a Random Forest, and take the predictions of the Random Forest and add them to the data set as a new feature to create data set B

3) Combine the features of data sets A and B into a single data set

4) Apply RFE

5) Fit another Random Forest to the features left after RFE and use that Random Forest's predictions as the final prediction for the pipeline

So we're looking for a sklearn-compatible way to represent that as a pipeline. We originally thought we could do:

```
make_pipeline(make_union(PolynomialFeatures(), RandomForestClassifier()), RFE(), RandomForestClassifier())
```

but I'm pretty sure that doesn't work out of the box.
",let use pipeline example image instead random forest pipeline take copy data set apply polynomial create data set take another copy data set fit random forest take random forest add data set new feature create data set combine data single data set apply fit another random forest left use random forest final prediction pipeline looking way represent pipeline originally thought could pretty sure work box,issue,positive,negative,neutral,neutral,negative,negative
223330746,"wait depends what you want. for feature selection? Did I provide the example in the first post? Seems odd lol
",wait want feature selection provide example first post odd,issue,negative,positive,neutral,neutral,positive,positive
223172240,"So with this setup if you want to do something like refactor TPOT so it just uses Numpy matrices instad of Pandas DataFrames you can edit the Operator class, Classifer class, and PreProcessor class and leave all actuall classifiers and preprocessors untouched.

**Edit:**

And something I'm interested in doing is largely forgoing the preprocess_args() method (from the refactor) as it is now, and just implement some general rules for arguments that will be applied based off of what argument names are used.

So for example:

If you have a Classifier that takes arguments 'max_features' and 'max_depth', there will be general rules that apply that say max_features should be between 1 and `len(training_features.columns)`. There will be another rule that states max_depth should be between 1 and max_depth.

So when you add a new classifier or pre-processor you don't need to add extra code that threshholds values that we've already determined reasonable limits for. You'd just need to say you want some set of parameters, and if any of them have pre-defined limits it'll use those. Failing that it would run any code you specify to limit the arguments.

This however assumes that an argument's name can reliably be used to determine what kind of threshholding is useful for that argument.

Doing this would also means that instead of testing each operator with extreme values that are covered in argument preprocessing code, you can test the general rules once.
",setup want something like matrix edit operator class class class leave untouched edit something interested largely method implement general applied based argument used example classifier general apply say another rule add new classifier need add extra code already determined reasonable need say want set use failing would run code specify limit however argument name reliably used determine kind useful argument would also instead testing operator extreme covered argument code test general,issue,negative,positive,positive,positive,positive,positive
223170537,"**Don't try to run this code - atm it's just a structural layout**

**Edit:** Some of it will kinda work now

**Edit 2:** You can actually do a fit_predict run now

https://github.com/teaearlgraycold/tpot/tree/refactor

Code of interest is in tpot/operators and tpot/tpot.py#134
",try run code structural layout edit work edit actually run code interest,issue,negative,neutral,neutral,neutral,neutral,neutral
223147637,"@tonyfast: I just remembered that we have this issue open to discuss the major refactor coming up after the 0.4 release.

@teaearlgraycold, please link your WIP refactor here so we can take a look at it.
",issue open discus major coming release please link take look,issue,negative,positive,neutral,neutral,positive,positive
223147092,"Ah, right: `FeatureUnion` only accepts sklearn preprocessors that have a `transform()` function. So unless there's an easy way to wrap the sklearn classifiers such that they have a `transform()` function that simply adds the classifier's predictions as a new feature...
",ah right transform function unless easy way wrap transform function simply classifier new feature,issue,negative,positive,positive,positive,positive,positive
223146609,"IIRC it only works on feature preprocessors. You can't pass it classifiers.

Wherever it was, it should be apparent from the documentation.

On Wed, Jun 1, 2016, 6:37 PM Randy Olson notifications@github.com wrote:

> @teaearlgraycold https://github.com/teaearlgraycold, do you remember
> why we decided this wasn't feasible with FeatureUnion? We should document
> that here.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/issues/117#issuecomment-223145945, or mute
> the thread
> https://github.com/notifications/unsubscribe/ADISYywOFmZVhr2HaI5MFbvIE2B8XAkcks5qHgm0gaJpZM4H0qUh
> .
",work feature ca pas wherever apparent documentation wed randy wrote remember decided feasible document reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
223145945,"@teaearlgraycold, do you remember why we decided this wasn't feasible with `FeatureUnion`? We should document that here.
",remember decided feasible document,issue,negative,neutral,neutral,neutral,neutral,neutral
223144256,"Okay. That's neat

On Wed, Jun 1, 2016, 6:23 PM Randy Olson notifications@github.com wrote:

> BTW: You can name PRs Fixed #[issue num] and they will automatically
> close the named issue when the PR is merged.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/158#issuecomment-223143221, or mute
> the thread
> https://github.com/notifications/unsubscribe/ADISYyDGgIXpraHY1KmlIrHWhDpFmq4Xks5qHgZ3gaJpZM4IrD6Q
> .
",neat wed randy wrote name fixed issue automatically close issue thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
223143221,"BTW: You can name PRs `Fixed #[issue num]` and GitHub will automatically close the named issue when the PR is merged.
",name fixed issue automatically close issue,issue,negative,positive,neutral,neutral,positive,positive
223134402,"If your deap Issue gets recognized and a contributor says it's not a bad idea I'll go ahead and PR the fix - there's only a few hundred references to random in deap, all of which should be easily enough replaced with calls to np.random.

They already use np.random, too - so it wouldn't be adding anything new.

They have automated tests, so I won't be totally in the dark when working on it and screw it all up.
",issue contributor bad idea go ahead fix hundred random easily enough already use would anything new wo totally dark working screw,issue,negative,negative,negative,negative,negative,negative
223133613,"Yup, as expected. That's too bad - we just won't test that function.
",bad wo test function,issue,negative,negative,negative,negative,negative,negative
223133287,"Looks like it's still failing - it's making a call to deap in the tested function, so that's not really a surprise.
",like still failing making call tested function really surprise,issue,negative,positive,positive,positive,positive,positive
223077217,"From what I can tell, sklearn solely uses the numpy RNG. Good news there.

I filed an issue on the DEAP repo suggesting that they switch over to the numpy RNG.

In the meantime, let's see if this bugfix helps with #157. If not, we just won't test that function for now.
",tell solely good news issue suggesting switch let see wo test function,issue,negative,positive,positive,positive,positive,positive
223075213,"Damn, looks like DEAP uses base Python's random: https://github.com/DEAP/deap/blob/af334219c168ed6c30403c587064a03f39d7d12a/deap/tools/mutation.py#L40

but we should still fix this.
",damn like base python random still fix,issue,negative,negative,negative,negative,negative,negative
223075022,"Looks like std. lib random is only called once in TPOT, so that should be an easy fix.
",like random easy fix,issue,positive,negative,neutral,neutral,negative,negative
223072663,"Well, that doesn't work:

```
>>> import random
>>> random.seed(90, version=2)
>>> [random.randint(0, 100) for x in range(10)]
[26, 91, 11, 61, 84, 73, 75, 39, 81, 45]
>>> random.seed(90, version=1)
>>> [random.randint(0, 100) for x in range(10)]
[26, 91, 11, 61, 84, 73, 75, 39, 81, 45]
```
",well work import random range range,issue,negative,negative,negative,negative,negative,negative
223072521,"Okay. Please file it as a bug and fix it ASAP if anywhere in TPOT is using the base `random` library. We'll replace it with the numpy one instead.

I think sklearn and DEAP use the numpy RNG.
",please file bug fix anywhere base random library replace one instead think use,issue,negative,negative,negative,negative,negative,negative
223070016,"The issue I believe is that the libraries we're calling are using random from the standard lib
",issue believe calling random standard,issue,negative,negative,negative,negative,negative,negative
223069344,"http://stackoverflow.com/a/11929775

I'll go ahead and specify the version in **init**
",go ahead specify version,issue,negative,neutral,neutral,neutral,neutral,neutral
223068812,"numpy is consistent across versions though:

```
Python 3.5.1 |Anaconda 2.4.0 (x86_64)| (default, Dec  7 2015, 11:24:55)
>>> import numpy as np
>>> np.random.seed(90)
>>> np.random.randint(0, 90, 10)
array([29, 31, 67, 39, 68, 58, 37, 18, 74, 51])
>>> np.__version__
'1.11.0'
```

```
Python 2.7.11 |Anaconda 2.5.0 (x86_64)| (default, Dec  6 2015, 18:57:58)
>>> import numpy as np
>>> np.random.seed(90)
>>> np.random.randint(0, 90, 10)
array([29, 31, 67, 39, 68, 58, 37, 18, 74, 51])
>>> np.__version__
'1.11.0'
```
",consistent across though python default import array python default import array,issue,negative,positive,positive,positive,positive,positive
223068220,"Well how about that.

```
Python 3.5.1 |Anaconda 2.4.0 (x86_64)| (default, Dec  7 2015, 11:24:55)
>>> import random
>>> random.seed(90)
>>> [random.randint(0, 100) for x in range(10)]
[26, 91, 11, 61, 84, 73, 75, 39, 81, 45]
```

```
Python 2.7.11 |Anaconda 2.5.0 (x86_64)| (default, Dec  6 2015, 18:57:58)
>>> import random
>>> random.seed(90)
>>> [random.randint(0, 100) for x in range(10)]
[20, 9, 66, 59, 64, 35, 45, 89, 82, 26]
```
",well python default import random range python default import random range,issue,negative,negative,negative,negative,negative,negative
222855858,"Already did that and it ran successfully.

**Edit:**

My test run

```
-> import pdb; pdb.set_trace()
(Pdb) from sklearn.cross_validation import cross_val_score
(Pdb) import numpy as np
(Pdb) np.mean(cross_val_score(tpot,digits.data,digits.target,cv=10,scoring='accuracy'))
0.95042830540037238
```
",already ran successfully edit test run import import import,issue,negative,positive,positive,positive,positive,positive
222855676,"Can you do a quick verification w/ this change to see if it allows TPOT to be used in sklearn's cross_val_score?
",quick verification change see used,issue,negative,positive,positive,positive,positive,positive
222842584,"Somehow we lost the `test_gp_new_generation()` test during a merge or something, so I re-added that.
",somehow lost test merge something,issue,negative,neutral,neutral,neutral,neutral,neutral
222839939,"Yes, that's from an old version of TPOT. :-)

On Tuesday, May 31, 2016, Daniel notifications@github.com wrote:

> Looks like the string I was pulling from your seed_pipelines module,
> '_random_forest(ARG0, mul(100, 5), 0)', is not a valid pipeline string.
> 
> _random_forest() only takes one integer arg.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/157#issuecomment-222837642, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7t0qjw9yyrOGFvzq58mHsSO7t3qXQks5qHLLFgaJpZM4IqKdw
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",yes old version may wrote like string module valid pipeline string one integer reply directly view mute thread postdoctoral researcher institute university twitter,issue,positive,positive,neutral,neutral,positive,positive
222837642,"Looks like the string I was pulling from your seed_pipelines module, `'_random_forest(ARG0, mul(100, 5), 0)'`, is not a valid pipeline string.

`_random_forest()` only takes one integer arg.
",like string module valid pipeline string one integer,issue,negative,neutral,neutral,neutral,neutral,neutral
222827293,"Okay, I discovered `deap.creator.Invididual.from_string(string, pset)`. Seems there's some other issue happening in the evaluation method since that did not solve my problem.
",discovered string issue happening evaluation method since solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
222825683,"Well I'm asking because after running `score()`, and during the subsequent call to `_evaluate_individual()` there's this bit of code:

``` Python
# Transform the tree expression in a callable function
func = self._toolbox.compile(expr=individual)

# Count the number of pipeline operators as a measure of pipeline complexity
. . .

result = func(training_testing_data)
```

But with the from_string object used for `_optimized_pipeline`, `result` here has a value of 0, and is not a DataFrame. Normally `_evaluate_individual()` takes an `Individual` object, not a `PrimitiveTree` object.
",well running score subsequent call bit code python transform tree expression callable function count number pipeline measure pipeline complexity result object used result value normally individual object object,issue,positive,neutral,neutral,neutral,neutral,neutral
222824641,"You have to compile the GP tree representation:

```
self._toolbox.compile(expr=individual)
```

then you should be able to execute the individual as a series of functions.
",compile tree representation able execute individual series,issue,negative,positive,positive,positive,positive,positive
222822014,"Hey, @rhiever, how would I make a Deap Individual object from a PrimitiveTree.

The following code does seem to create a pipeline that can be evaluated by TPOT.

``` Python
tpot_obj._optimized_pipeline = gp.PrimitiveTree.from_string('_random_forest(ARG0, mul(100, 5), 0)', tpot_obj._pset)
```
",hey would make individual object following code seem create pipeline python,issue,negative,neutral,neutral,neutral,neutral,neutral
222721950,"@teaearlgraycold should be raising that issue soon (we just finished meeting). He's been working on a PR with a base implementation of it.
",raising issue soon finished meeting working base implementation,issue,negative,negative,negative,negative,negative,negative
222720325,"We're actually heading toward a major refactor of TPOT after the next release (v0.4). :-)
",actually heading toward major next release,issue,negative,positive,neutral,neutral,positive,positive
222720051,"pandas comes with quite a bit of overhead. sklearn doesn't use pandas; I don't think it's necessary for TPOT to use it either.
",come quite bit overhead use think necessary use either,issue,negative,neutral,neutral,neutral,neutral,neutral
222719615,"Would using some of the `sklearn` Mixins help with problems like [`SparseCoeffMixin`](https://github.com/scikit-learn/scikit-learn/blob/51a765acfa4c5d1ec05fc4b406968ad233c75162/sklearn/linear_model/base.py#L295)?

There is a lot of talk about a rework of TPOT in the issues.  
",would help like lot talk rework,issue,positive,neutral,neutral,neutral,neutral,neutral
222717506,"I am not too sure what I meant in the beginning.  Maybe I should have said ""We don't know how complicated these models will be in the future.  Choosing types to build pipelines now may pigeonhole building complicated pipelines in the future.

Thanks for that reference @bartleyn.  That clarifies the process for me.  Some like [`multipledispatch`](http://multiple-dispatch.readthedocs.io/en/latest/) or [`odo`](odo.readthedocs.org) could be helpful with automating these pipelines.  

I don't know much about DEAP, but from my brief research into over the past few days I doubt it would have to be gutted.  Maybe a few things could get monkey patched, but it seems to have a nice plugin system to extend methods.

It is starting to appear that `TPOT` may be something like `class TPOT(sklearn.base.ClassifierMixin, deap.gp.eaSimple)` 
",sure meant beginning maybe said know complicated future choosing build may pigeonhole building complicated future thanks reference process like could helpful know much brief research past day doubt would maybe could get monkey nice system extend starting appear may something like class,issue,positive,negative,neutral,neutral,negative,negative
222709791,"> I imagine that very complex graphs could exist where preprocessors could switch roles. 

Can you explain a little more about what you mean by preprocessors switching roles? 

> Are pipelines understood well enough to create a rigid ontology?

I think it's less that pipelines are understood well enough, and more that we're trying to align ontologies for structured pipelines with related work in autoML (e.g., the auto-sklearn project and paper found [here](http://papers.nips.cc/paper/5872-efficient-and-robust-automated-machine-learning.pdf)). So it seems mostly just convenient to me (but I could be wrong). 

> The DataFrame class is very robust and it may be dangerous to choose something else instead. It is good to know DataFrame in and DataFrame out; custom classes could get confusing.

I mean this is just a tentative approach to structuring these pipelines, and it seems that rather than gutting the DEAP evolutionary algorithm we could start with this to see if it's worthwhile investing more time in it.

I think the base model class idea is super interesting, and aligns with that pending major OOP refactor -- your idea is perhaps a more long-term solution to the problem.
",imagine complex could exist could switch explain little mean switching understood well enough create rigid ontology think le understood well enough trying align structured related work project paper found mostly convenient could wrong class robust may dangerous choose something else instead good know custom class could get mean tentative approach rather evolutionary algorithm could start see time think base model class idea super interesting pending major idea perhaps solution problem,issue,positive,negative,neutral,neutral,negative,negative
222704491,"Perhaps it might be best to use `pandas` under the hood?  It is a supercharged `numpy` object.  Much of `tpot` relies on `pandas` machinery and it might make extra work to manage state with it.
",perhaps might best use hood supercharged object much machinery might make extra work manage state,issue,positive,positive,positive,positive,positive,positive
222703492,"I can whip something up for this. I think there was an initial concern about how to optimize the scoring functions for DataFrames, but it should be pretty straightforward to just ship off all those concerns to the underlying sklearn functions. 
",whip something think initial concern optimize scoring pretty straightforward ship underlying,issue,positive,positive,positive,positive,positive,positive
222701862,"As of now, you would have to write a wrapper function as shown in the docs page you listed. Now that I look at that, though, that doesn't seem very ideal. I'm going to turn this issue into an enhancement request to improve the custom scoring functionality. It should be possible to pass an arbitrary scoring function from sklearn and allow TPOT to use that, without writing a custom wrapper function.
",would write wrapper function shown page listed look though seem ideal going turn issue enhancement request improve custom scoring functionality possible pas arbitrary scoring function allow use without writing custom wrapper function,issue,positive,positive,positive,positive,positive,positive
222594889,"I imagine that very complex graphs could exist where preprocessors could switch roles.  Are pipelines understood well enough to create a rigid ontology?

The DataFrame class is very robust and it may be dangerous to choose something else instead.  It is good to know DataFrame in and DataFrame out; custom classes could get confusing.

#### A base class

A TPOT model base class will have to 1. Set its state, 2. Fit a model, 3. Predict a model, 4. Update a DataFrame, and 5. Export code.

This [notebook](http://nbviewer.jupyter.org/gist/tonyfast/91c61d75d22622cc3eb1df7cd8a7070f) tinkers around with the idea of the TPOT model.  This class would export jinja templated code along with the docstring in a notebook cell.  

A sample for a random forest may look like.

``` python
class _random_forest(Model):
    """"""Fits a random forest classifier

    Parameters
    ----------
    input_df: pandas.DataFrame {n_samples, n_features+['class', 'group', 'guess']}
        Input DataFrame for fitting the random forest
    max_features: int
        Number of features used to fit the decision tree; must be a positive value

    Returns
    -------
    input_df: pandas.DataFrame {n_samples, n_features+['guess', 'group', 'class', 'SyntheticFeature']}
        Returns a modified input DataFrame with the guess column updated according to the classifier's predictions.
        Also adds the classifiers's predictions as a 'SyntheticFeature' column.

    """"""
    _package = 'sklearn.ensemble.RandomForestClassifier'
    _source = """"""
from {{package_path[:-1] | join('.')}} import {{package_path | last}}

rfc{{operator_num}} = RandomForestClassifier(
    n_estimators = {{n_estimators}},
    max_features = {{max_features}},
)
rfc{{operator_num}}.fit(
    {{input_df}}.loc[training_indices].drop('class', axis=1).values, 
    {{input_df}}.loc[training_indices, 'class'].values,
)
    """"""

    def preprocess(self, inputdf:pd.DataFrame, max_features:int=4):
        if max_features < 1:
            max_features = 'auto'
        elif max_features == 1:
            max_features = None
        elif max_features > len(input_df.columns) - 3:
            max_features = len(input_df.columns) - 3
        return {
            'max_features': max_features,
            'n_estimators': 500,
            'random_state': 42, 
            'n_jobs': -1
        }
```

Some of the models are very similar in execution.  There maybe a few base models required to cover the current set of sklearn models.

Many of these models could be combined using

``` python
from toolz import compose
pipeline = compose( _random_forest, _select_kbest, _binarizer, _standard_scaler)
```
",imagine complex could exist could switch understood well enough create rigid ontology class robust may dangerous choose something else instead good know custom class could get base class model base class set state fit model predict model update export code notebook around idea model class would export jinja code along notebook cell sample random forest may look like python class model random forest classifier input fitting random forest number used fit decision tree must positive value input guess column according classifier also join import last self none return similar execution maybe base cover current set many could combined python import compose pipeline compose,issue,positive,negative,negative,negative,negative,negative
222591238,"There's a way to convert a string representation of a pipeline into a
TPOT individual. Let's chat about that tomorrow.

On Tuesday, May 31, 2016, Daniel notifications@github.com wrote:

> Looks like Python 2.7 has different PRNGs.
> 
> I should bring up that my testing method of score() isn't amazing anyway,
> though - I run a call to fit() with a population size of 1 and generation
> count of zero with TPOT seeded to a specific seed. That way I know exactly
> what the only pipeline generated should look like.
> 
> This however will be affected by the number of operatators and many other
> factors.
> 
> Googling around it seems like I'd want to pickle the deap Individual
> object. Should we have a bunch of pickled objects ready for use by the test
> suite?
> 
> Also the extra test I added for train_model_and_predict() seems to have
> issues when it comes to asserting that the testing guesses are identical,
> so I've commented that out for now.
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> https://github.com/rhiever/tpot/pull/157#issuecomment-222590596, or mute
> the thread
> https://github.com/notifications/unsubscribe/ABo7t8F3v4zpW55zwha7x-jlCe1BsY_vks5qG7qAgaJpZM4IqKdw
> .

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",way convert string representation pipeline individual let chat tomorrow may wrote like python different bring testing method score amazing anyway though run call fit population size generation count zero seeded specific seed way know exactly pipeline look like however affected number many around like want pickle individual object bunch ready use test suite also extra test added come testing identical thread reply directly view mute thread postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
222590596,"Looks like Python 2.7 has different PRNGs and that's probably what's causing its Travis build to fail.

I should bring up that my testing method of score() isn't amazing anyway, though - I run a call to fit() with a population size of 1 and generation count of zero with TPOT seeded to a specific seed. That way I know exactly what the only pipeline generated should look like.

This however will be affected by the number of operatators and many other factors.

Googling around it seems like I'd want to pickle the deap Individual object. Should we have a bunch of pickled objects ready for use by the test suite?

Also the extra test I added for train_model_and_predict() seems to have issues when it comes to asserting that the testing guesses are identical, so I've commented that out for now.
",like python different probably causing travis build fail bring testing method score amazing anyway though run call fit population size generation count zero seeded specific seed way know exactly pipeline look like however affected number many around like want pickle individual object bunch ready use test suite also extra test added come testing identical,issue,positive,positive,positive,positive,positive,positive
222121823,"I'll try to implement it and send a PR when I get it done.

This will be my first contribution to TPOT (and one of the few in general) so be patient ;)
",try implement send get done first contribution one general patient,issue,negative,positive,positive,positive,positive,positive
221695930,"While you sort out the merge conflict (hopefully it's straightforward), please add a newline between the ""Best pipeline"" and ""Generation `x`"" outputs, just to make it look a little cleaner.

<img width=""719"" alt=""screen shot 2016-05-25 at 4 23 01 pm"" src=""https://cloud.githubusercontent.com/assets/1719223/15555064/fcc70f42-2294-11e6-88fe-68686a53b054.png"">
",sort merge conflict hopefully straightforward please add best pipeline generation make look little cleaner screen shot,issue,positive,positive,positive,positive,positive,positive
221368926,"You could check out [WhizzML](https://bigml.com/whizzml), which is oriented towards representing ML workflows. Of course, WhizzML is a very new thing, and by supporting it you would become interoperable only with BigML's platform.

But it should be interesting reference material nonetheless.
",could check towards course new thing supporting would become platform interesting reference material nonetheless,issue,positive,positive,positive,positive,positive,positive
221271230,"> giving a shot at implementing some PMML interoperability.

@vruusmann that would be great! Thanks in advance for the effort.
",giving shot would great thanks advance effort,issue,positive,positive,positive,positive,positive,positive
221254686,"Thank you @vruusmann! TPOT is built almost entirely on top of sklearn classes, with the exception of XGBoost and a couple custom feature preprocessors. However, we are dropping XGBoost due to installation issues for many users, so that will not be an issue in the next version.

Here are some resources that will help you get started with TPOT:
- [Installation instructions](http://rhiever.github.io/tpot/installing/)
- [Usage instructions](http://rhiever.github.io/tpot/using/)
- [Example with iris](http://rhiever.github.io/tpot/examples/IRIS_Example/)
- [Example with MNIST](http://rhiever.github.io/tpot/examples/MNIST_Example/)
",thank built almost entirely top class exception couple custom feature however dropping due installation many issue next version help get installation usage example iris example,issue,positive,positive,positive,positive,positive,positive
221251190,"As a contributor to JPMML-family of projects, here's my perspective.

Fundamentally, PMML is about capturing the **_final state_** of a model development workflow (not the workflow itself). In other words, the final state is the winning solution, which is appropriate for scoring data in production environment (where the goal is not to replay model development procedure, but to apply the model to new data records).

I think PFA is also more about capturing the state rather than the process. So, you should be focusing more on domain-specific languages that address workflow markup.

JPMML-SkLearn supports the conversion of over 50 Scikit-Learn transformers and estimators. The only class that couldn't be represented in ""native"" PMML (but could be put in action using Java user-defined function) has been `sklearn.decomposition.NMF`. So, if TPOT is relying on regular Scikit-Learn classes, then it's probably fairly straightforward to implement a converter to it.

Anyway, if you have any pointers for getting started with TPOT (something to do with the Iris dataset?), then I wouldn't mind giving a shot at implementing some PMML interoperability.
",contributor perspective fundamentally model development final state winning solution appropriate scoring data production environment goal replay model development procedure apply model new data think also state rather process address markup conversion class could native could put action function regular class probably fairly straightforward implement converter anyway getting something iris would mind giving shot,issue,positive,positive,positive,positive,positive,positive
221247135,"Agreed. In the meantime, we're still interested in #51 and being able to export TPOT pipelines to Orange. Seems like that would be quite useful.
",agreed still interested able export orange like would quite useful,issue,positive,positive,positive,positive,positive,positive
221245915,"I guess 1) and 2) can be answered with a straight no for PMML, although I think it would still be beneficial to support it with only a subset of TPOT pipelines being actually exportable. 
PFA should meet the requirements, though, at least on paper. I can't say how easy or hard it will be to come up with a generic way of exporting an intricate TPOT pipeline to PFA without manual work. As of now the only open source PFA tool I could find, namely [Hadrian/Titus](https://github.com/opendatagroup/hadrian), doesn't really focus on converting models from existing frameworks, but rather on standalone model construction ([example](https://github.com/opendatagroup/hadrian/wiki/Producing-tree-models-in-Titus)).

As for 3), I haven't been able to find any visualization tools so far. There are a few mentions of a tool that was developed during a [research project](http://cordis.europa.eu/result/rcn/80712_en.html), but it seems like it was never made available to the public...

Seems to me that we may have to wait a while for better PFA tooling to emerge in order to meet all the requirements.
",guess straight although think would still beneficial support subset actually exportable meet though least paper ca say easy hard come generic way intricate pipeline without manual work open source tool could find namely really focus converting rather model construction example able find visualization far tool research project like never made available public may wait better tooling emerge order meet,issue,positive,positive,positive,positive,positive,positive
221057484,"That's interesting that XGBoost is so much faster. Do you know if it performs better, though? On the 155 data set benchmark that I have, it XGBoost didn't really perform better accuracy-wise.

We're going to replace XGBoost with the sklearn GradientBoostingClassifier for now, and raise a separate issue (#154) to make XGBoost an optional dependency in the future. @KhaoticMind, I welcome you to contribute that code, else we'll have it on our TODO stack for a while.
",interesting much faster know better though data set really perform better going replace raise separate issue make optional dependency future welcome contribute code else stack,issue,positive,positive,positive,positive,positive,positive
220969163,"Hi @rhiever , while I couldn't do a Memory wise benchmark, I do have some CPU based results where you can easily see XGB performs A LOT better than GradientBoostingClassifier (about 4x times better with the same conditions).
https://www.kaggle.com/khaoticmind/expedia-hotel-recommendations/analysis-of-time/run/245245/log

Check the code here: 
https://www.kaggle.com/khaoticmind/expedia-hotel-recommendations/analysis-of-time/run/245245

Note that I've set nthreads=1 in XGBoost, to make it comparable with sklearn inplementation (that does not support multithread).
If you see any errors on the code, please point than out and I can re-run the tests.
",hi could memory wise based easily see lot better time better check code note set make comparable support see code please point,issue,positive,positive,positive,positive,positive,positive
220809843,"It is really a dilemma between ease of use and extendibility.
",really dilemma ease use extendibility,issue,negative,positive,positive,positive,positive,positive
220808312,"That makes sense about XML then. :-)

This is something that we'll have to explore more before committing to. I'm primarily interested in finding out:

1) Can it support the arbitrary pipeline structures that TPOT may create?

2) Does it support all operators in sklearn?

3) Is there any nice visualization software that can read and visualize ML pipelines in PMML? (Related to #51)
",sense something explore primarily interested finding support arbitrary pipeline may create support nice visualization read visualize related,issue,positive,positive,positive,positive,positive,positive
220796325,"Actually I'm pretty sure that print statement should be a `UserWarning`, and we should be ignoring UserWarnings for FastICA.

Looking through the sklearn code for FastICA, I'm convinced this print statement is debug code mistakenly left in the module. The biggest hint is that `from __future__ import print_function` is no where in fastica_.py.

https://github.com/scikit-learn/scikit-learn/pull/6807
",actually pretty sure print statement looking code convinced print statement code mistakenly left module biggest hint import,issue,positive,positive,positive,positive,positive,positive
220794852,"That error seems to be coming from FastICA.

Here's the sklearn code:

https://github.com/scikit-learn/scikit-learn/blob/06bf797c0deabe2a2f166d19abbd0c305da4d123/sklearn/decomposition/fastica_.py#L296

As it is right now that warning will probably only show up if you have a ton of features and not very many actual data rows. Honestly there's no reason for there to be a print statement there IMO. I can just duplicate the following code from sklearn to avoid that print statement:

``` Python
n, p = X.shape

if (n_components > min(n, p)):
        n_components = min(n, p)
```

Instead of what we're currently using:

``` Python
n_components = min(n_components, len(training_features.columns.values))
```
",error coming code right warning probably show ton many actual data honestly reason print statement duplicate following code avoid print statement python min min instead currently python min,issue,negative,positive,positive,positive,positive,positive
220794694,"The format was first specified in 1998, hence the use of XML. [PFA](http://dmg.org/pfa/docs/motivation/), PMML's successor, will be based on JSON and will provide quite a lot more flexibility. It's almost a high-level programming language actually. However, it is still in the making (IIRC the first draft of the specification was published towards the end of last year) and therefore not really being used in production by more than a handful of people.

PMML, on the other hand, is being used quite a lot, and I don't think it's going away even if PFA picks up speed over the next couple years. [Here](http://dmg.org/pmml/products.html) is a list of projects/frameworks that use/support PMML (the list can't be complete, though, because there is no mention of [Pattern](http://www.cascading.org/projects/pattern/)).
",format first hence use successor based provide quite lot flexibility almost language actually however still making first draft specification towards end last year therefore really used production handful people hand used quite lot think going away even speed next couple list list ca complete though mention pattern,issue,positive,positive,positive,positive,positive,positive
220754319,"Interesting idea. How popular is this format? XML seems a bit outdated. (I thought JSON replaced XML everywhere.)
",interesting idea popular format bit outdated thought everywhere,issue,positive,positive,positive,positive,positive,positive
220754250,"That's fine. It largely depends on the data set too.
",fine largely data set,issue,negative,positive,positive,positive,positive,positive
220712854,"So most of the time the current best score doesn't seem to improve over the first generation.

Maybe that's because I'm doing smaller populations with not many generations (10 - 20 population, 2 - 5 generations). I _am_ seeing small improvements some of the time, just not too often.
",time current best score seem improve first generation maybe smaller many population seeing small time often,issue,positive,positive,positive,positive,positive,positive
220679936,"Okay. I'll be able to do that very easily then.

On Fri, May 20, 2016, 2:12 PM Randy Olson notifications@github.com wrote:

> I don't think that information is accessible from outside the GP. The only
> way you can access it is by using the stats, right? And that just pumps out
> information through stdout.
> 
> The best pipelines should be stored in self.hof, and each individual in
> self.hof should have their fitness attached to them.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/144#issuecomment-220679257
",able easily may randy wrote think information accessible outside way access right information best individual fitness attached thread reply directly view,issue,positive,positive,positive,positive,positive,positive
220679257,"> I don't think that information is accessible from outside the GP. The only way you can access it is by using the stats, right? And that just pumps out information through stdout.

The best pipelines should be stored in `self.hof`, and each individual in `self.hof` should have their fitness attached to them.
",think information accessible outside way access right information best individual fitness attached,issue,positive,positive,positive,positive,positive,positive
220679024,"> Oh, and should we somehow specify dependency versions so people use sklearn 0.17, tqdm 4.5.0, etc?

Yes, definitely! We should specify minimum versions required.
",oh somehow specify dependency people use yes definitely specify minimum,issue,positive,neutral,neutral,neutral,neutral,neutral
220678779,"Happy to see said benchmarks, @KhaoticMind.

It's possible that we could rework TPOT to make it an optional dependency and do as you said.
",happy see said possible could rework make optional dependency said,issue,positive,positive,positive,positive,positive,positive
220667697,"Can't we keep xgboost in the pipeline, but not explicity it as a dependency?

So at runtime, it is used if it's present, otherwise we fall back to sklearn implementation.
In my experience xgboost uses A LOT less memmory than GradientBoostingClassifier. I might try to come up with some benchmarks from kaggle competitions to prove the point of keeping XGBoost in.
",ca keep pipeline dependency used present otherwise fall back implementation experience lot le might try come prove point keeping,issue,negative,neutral,neutral,neutral,neutral,neutral
220648984,"Oh, and should we somehow specify dependency versions so people use sklearn 0.17, tqdm 4.5.0, etc?
",oh somehow specify dependency people use,issue,negative,neutral,neutral,neutral,neutral,neutral
220648554,"I don't think that information is accessible from outside the GP. The only way you can access it is by using the stats, right? And that just pumps out information through stdout.
",think information accessible outside way access right information,issue,negative,positive,positive,positive,positive,positive
220648179,"Now that I think about it: Maybe it would be useful to report the internal CV accuracy of the best pipeline discovered so far at the end of every generation. That way, the user will know when to stop early.
",think maybe would useful report internal accuracy best pipeline discovered far end every generation way user know stop early,issue,positive,positive,positive,positive,positive,positive
220613988,"The screenshot I showed was from a run that ended normally, so the progress bar doesn't hide at the end currently.
",run ended normally progress bar hide end currently,issue,negative,positive,neutral,neutral,positive,positive
220609349,"The progress bar should hide itself after a completed run. I think it may double-print when someone force-quits fit(). I'll look into that.
",progress bar hide run think may someone fit look,issue,positive,positive,positive,positive,positive,positive
220607926,"Actually, is it possible to hide the progress bar entirely when the run finishes (either normally or prematurely)?
",actually possible hide progress bar entirely run either normally prematurely,issue,negative,positive,neutral,neutral,positive,positive
220607728,"See my edit. :-)

Anyway, I think the output needs to be cleaned up a bit. Here's what I have for a small TPOT run:

<img width=""1432"" alt=""screen shot 2016-05-20 at 9 35 03 am"" src=""https://cloud.githubusercontent.com/assets/1719223/15429273/343043b6-1e6e-11e6-85ab-9e0c6ab1ce6d.png"">

1) No need to print the message about finishing generations.

2) There should be a newline between the progress bar and the ""Best pipeline:"" line.

3) Not sure why the progress bar showed up twice?

4) When someone CTRL-C's, the progress bar gets messy. Any way to prevent that?
",see edit anyway think output need bit small run screen shot need print message finishing progress bar best pipeline sure progress bar twice someone progress bar messy way prevent,issue,positive,positive,positive,positive,positive,positive
220606582,"What version of tqdm are you running? I've got `4.5.0-a4e00b2`

That's pretty annoying that it's not giving any hint as to which operator is causing that warning. What code are you running?
",version running got pretty annoying giving hint operator causing warning code running,issue,negative,negative,negative,negative,negative,negative
220605637,"Edit: cancel that. I had an outdated version of tqdm.
",edit cancel outdated version,issue,negative,negative,negative,negative,negative,negative
220427161,"I like the one parameter idea. On the Python interface the parameter could be a list of classifiers instances (or classes). So you/we could just iterate over the list and call each classifier in order. If None, create a list with all classifiers on it.
",like one parameter idea python interface parameter could list class could iterate list call classifier order none create list,issue,negative,neutral,neutral,neutral,neutral,neutral
220290772,"This PR has merge conflicts now. Pretty sure it has something to do with the argparse help.
",merge pretty sure something help,issue,positive,positive,positive,positive,positive,positive
220290390,"Thanks all for your input! We'll still allow TPOT to optimize the parameter for now, and ignore the warnings (per #149).
",thanks input still allow optimize parameter ignore per,issue,positive,positive,positive,positive,positive,positive
220207395,"Looks like an XGBoost install issue. Check out this page and see if it can address your issue: https://github.com/dmlc/xgboost/blob/master/python-package/build_trouble_shooting.md
",like install issue check page see address issue,issue,negative,neutral,neutral,neutral,neutral,neutral
220152661,"using python3.5 on Ubuntu.
getting the following error during import

# Code

from tpot import TPOT

# Error

/home/ubuntu/anaconda2/lib/python2.7/site-packages/xgboost-0.4a30-py2.7.egg/xgboost/./wrapper/libxgboostwrapper.so: invalid elf header
",python getting following error import code import error invalid elf header,issue,negative,neutral,neutral,neutral,neutral,neutral
220082331,"if you want to tune it, I'd also just ignore the warning.
",want tune also ignore warning,issue,negative,neutral,neutral,neutral,neutral,neutral
220081885,"So I'm not too familiar with the model, but the convergence criteria is essentially that the change in each estimated component falls under the tolerance, right? If that's the case, then to double down on what's been previously said, we _could_ have useful (or at least mediocre) results without convergence if a subset of the components were just being stubborn. I agree with @teaearlgraycold in ignoring the warning and letting the GP weed them out (assuming they're usually worse pipelines). 
",familiar model convergence criterion essentially change component tolerance right case double previously said useful least mediocre without convergence subset stubborn agree warning weed assuming usually worse,issue,positive,negative,neutral,neutral,negative,negative
220074255,"The default probably hasn't been tuned. But if you don't consider time-accuracy trade-offs, tuning seems odd.
",default probably tuned consider tuning odd,issue,negative,negative,negative,negative,negative,negative
220073276,"The defaults have been giving me the same warning, though. It's particularly odd.

**Edit:**

Oh, and it occurs with Iris and MNIST, but much more frequently with MNIST.
",giving warning though particularly odd edit oh iris much frequently,issue,negative,negative,neutral,neutral,negative,negative
220072359,"Should we just use the default then? Has that default been tuned?

On Wednesday, May 18, 2016, Andreas Mueller notifications@github.com
wrote:

> it's a speed optimization though. Does tpot have a concept of accuracy
> time trade-off?
> Higher tol will always be better, but take longer, and have diminishing
> returns...
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/148#issuecomment-220063093

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",use default default tuned may wrote speed optimization though concept accuracy time higher tol always better take longer reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
220068937,"If FastICA does not converge, does that mean that its results are not useful?

I'm wondering if it'd be harmless to ignore those warnings. If it just results in mediocre results then I'd like to just have the GP weed out those not-so-great configurations.
",converge mean useful wondering harmless ignore mediocre like weed,issue,positive,negative,negative,negative,negative,negative
220063093,"it's a speed optimization though. Does tpot have a concept of accuracy time trade-off?
Higher tol will always be better, but take longer, and have diminishing returns...
",speed optimization though concept accuracy time higher tol always better take longer,issue,positive,positive,positive,positive,positive,positive
220062069,"Should we just leave it at the default? What if it affects performance?

On Wednesday, May 18, 2016, Andreas Mueller notifications@github.com
wrote:

> Well depends on your community. Well tol and max_iter interact. And the
> algorithm not converging doesn't mean the value is not valid. Searching
> over it seems weird in the first place, though... Why do you do that?
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/148#issuecomment-220060747

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",leave default performance may wrote well community well tol interact algorithm converging mean value valid searching weird first place though reply directly view postdoctoral researcher institute university twitter,issue,positive,negative,negative,negative,negative,negative
220060747,"Well depends on your community. Well tol and max_iter interact. And the algorithm not converging doesn't mean the value is not valid. Searching over it seems weird in the first place, though... Why do you do that?
",well community well tol interact algorithm converging mean value valid searching weird first place though,issue,positive,negative,negative,negative,negative,negative
220060127,"Right. How do we gauge the valid range for tol?

And yes, GP = Genetic Programming. It's meant that for decades. ;-)

On Wednesday, May 18, 2016, Andreas Mueller notifications@github.com
wrote:

> (also GP here means genetic program, right? that's an evil way to
> abbreviate it ;)
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/148#issuecomment-220057296

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",right gauge valid range tol yes genetic meant may wrote also genetic program right evil way abbreviate reply directly view postdoctoral researcher institute university twitter,issue,negative,negative,neutral,neutral,negative,negative
220057296,"(also GP here means genetic program, right? that's an evil way to abbreviate it ;)
",also genetic program right evil way abbreviate,issue,negative,negative,negative,negative,negative,negative
220057181,"Sorry, what's the question? What the range for the tolerance should be?
",sorry question range tolerance,issue,negative,negative,negative,negative,negative,negative
219968860,"Xgboost has been disabled from pip by the authors. 

> pip installation on windows is currently disabled for further investigation, please install from github.

To install on a Windows 32-bit environment you need to build from source. These are the steps I took (note that I do not use Anaconda as my Python installation either.)
- Download and install MinGW-64: http://sourceforge.net/projects/mingw-w64/
- On the first screen of the install prompt make sure you set the Architecture to i386 and the Threads to win32
- add mingw to your environment path
- git clone the xgboost repository
- cd into the xgboost main directory
- type the following at a command prompt

```
git submodule init 
git submodule update`
cp make/mingw64.mk config.mk
notepad config.mk
```
- Then edit the export CXX and CC statements to remove the -m64 flags. 
- again at a command prompt type

`make -j4`

If the build finishes successfully, you should have a file called xgboost.exe located in the project root. 
You should be able to double click on this file and have a cmd window briefly appear with no errors. If you get an error about this file not being able to be run on this system, check that the -m64 compilation flags were removed.

To install the Python package do the following 

```
cd python-package
python setup.py install
```

Hopefully at this stage you should be able to import xgboost from python.

For 64 bit architecture the build is similar except of course you don't want to remove the the -m64 flags in the build config file.
",disabled pip pip installation currently disabled investigation please install install environment need build source took note use anaconda python installation either install first screen install prompt make sure set architecture win add environment path git clone repository main directory type following command prompt git git update edit export remove command prompt type make build successfully file project root able double click file window briefly appear get error file able run system check compilation removed install python package following python install hopefully stage able import python bit architecture build similar except course want remove build file,issue,positive,positive,positive,positive,positive,positive
219888069,"So it looks like by default `tol` has a value of `1e-04` in sklearn

https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/fastica_.py#L113

But quite frequently I'm finding that the limit that is compared against `tol` is something like ~ `0.3` (it needs to be smaller). I've seen it as high as `1.5`, though.

What's really weird though is that this issue seems to be really common on my progress_bar branch, and not on my branch for working on this issue (which comes off of master). Another odd thing, I've seen that on the same run sometimes a tolerance of `0.0001` will run just fine, but a tolerance later in the GP of `0.01` causes FastICA to throw the `UserWarning`.

Upping `max_iter` all the way to `1000` has not helped.
",like default tol value quite frequently finding limit tol something like need smaller seen high though really weird though issue really common branch branch working issue come master another odd thing seen run sometimes tolerance run fine tolerance later throw upping way,issue,positive,negative,neutral,neutral,negative,negative
219836716,"All done. This update will be in the next release.
",done update next release,issue,negative,neutral,neutral,neutral,neutral,neutral
219831869,"That's a shame about xgboost on Windows. Have seen several people have that issue. I might consider dropping xgboost from TPOT because of that.
",shame seen several people issue might consider dropping,issue,negative,neutral,neutral,neutral,neutral,neutral
219749950,"I tried with Anaconda on Win 10:
`conda install numpy scipy pandas scikit-learn`
`conda run -- pip install deap`
`conda run -- pip install xgboost`
but the last one failed. I lately noticed that `xgboost` claims ""pip installation may not work on some windows environment"" and in fact I did not manage to install it. I guess I will try `tpot` again in the future, it seems really promising.
",tried anaconda win install run pip install run pip install last one lately pip installation may work environment fact manage install guess try future really promising,issue,positive,positive,positive,positive,positive,positive
219745609,"We should explore a wide range of `tol` parameters for FastICA on e.g. the MNIST data set, and see where it throws this warning. It'd be a good idea to do the same sweep across a few different data sets and see if the findings hold true.

Have you seen this warning with any other operators?
",explore wide range tol data set see warning good idea sweep across different data see hold true seen warning,issue,negative,positive,positive,positive,positive,positive
219744879,"This was the other option I've been considering, but that seems hard to use, right? The user will have to specify certain values in the list (or dict), look up what those values should be, etc.
",option considering hard use right user specify certain list look,issue,negative,positive,neutral,neutral,positive,positive
219743821,"I think maybe we can just add one parameter to **init**(). This parameter takes ""None"" as default value, and in this case various classifiers will be used. But when users want to limit what kind of classifiers TPOT uses, they should specify a classifier and assign the name or object of the classifier to the parameter. 
",think maybe add one parameter parameter none default value case various used want limit kind specify classifier assign name object classifier parameter,issue,negative,positive,positive,positive,positive,positive
219734906,"Wanted to ping you to see if you tried an Anaconda install and could still reproduce the bug there. Very curious to see what's causing this error. I've run several thousand instances of TPOT on over 200 different data sets and never experienced this error.
",ping see tried anaconda install could still reproduce bug curious see causing error run several thousand different data never experienced error,issue,negative,negative,negative,negative,negative,negative
219730219,"When writing test cases for classifiers, test with normal parameters as well as extreme parameters: negative values, out-of-bounds values, etc. That will help catch issues where we're allowing invalid parameters to be passed to the various models.
",writing test test normal well extreme negative help catch invalid various,issue,negative,negative,neutral,neutral,negative,negative
219582636,"So I removed the stats that were set up with DEAP and did a bit of refactoring on how I defined the _gp_new_generation decorator based on what StackOverflow seemed to indicate was the proper way to define this type of decorator (having it at the module level).

Since it seems decorators need to be defined _before_ they are called I opted to add a decorators file to import _gp_new_generation from rather than keep it permanently above the TPOT class definition in tpot.py.
",removed set bit defined decorator based indicate proper way define type decorator module level since need defined add file import rather keep permanently class definition,issue,negative,neutral,neutral,neutral,neutral,neutral
219565209,"Ok. An Anaconda install is probably the most promising next step then.

On Monday, May 16, 2016, Pierluigi Failla notifications@github.com wrote:

> I actually: pip install TPOT==0.3.0 ...
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/141#issuecomment-219564208

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",anaconda install probably promising next step may wrote actually pip install reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,neutral,neutral,positive,positive
219558360,"I'm curious: Is your TPOT install from a clone of the current master branch?
",curious install clone current master branch,issue,negative,negative,neutral,neutral,negative,negative
219460884,"Perplexing. Do you use a specific random number seed to replicate the issue, or it literally happens every time?
",perplexing use specific random number seed replicate issue literally every time,issue,negative,negative,negative,negative,negative,negative
219456224,"The issue seems to be related with `deap` I found the following refs:
- http://deap.readthedocs.io/en/master/tutorials/advanced/gp.html#strongly-typed-gp
- https://github.com/DEAP/deap/issues/61
- https://github.com/DEAP/deap/pull/53
",issue related found following,issue,negative,neutral,neutral,neutral,neutral,neutral
219448988,"@teaearlgraycold, here's the list of unit tests that @GJena and I brainstormed back when she was working on this issue. Some of them are already implemented.

Unit tests

Run nose tests https://nose.readthedocs.org/en/latest/ Nose/Nose2?

If no features, return copy of data frame..for every feat operator
Df has 3: class, guess, group

File: tpot.py
Function: `__init__`

Assertion tests present; add more assertions

Cover the new parameters that have been added since the **init**() test was created

Function: fit
fit() can be tested in the integration tests

Function: pareto_eq
Cannot be tested because it is inside fit()

Function: predict
Testing features = training features

Function: score
Use fixed pipeline, RNG, and data sets. Should output the same score each time.

Function: export
Should not be necessary to unit test because all of its functionality will be tested in export_utils.py

Function: decision_tree
Checking if output same as sklearn decision tree

Function: random_forest
Same as sklearn

Function: logistic_regression
Same as sklearn

Function: svc
Same as sklearn

Function: knnc
Same as sklearn

Function: xgradient_boosting
Same as XGBClassifier

Function: train_model_and_predict
Same as sklearn

Function: combine_dfs
input_df1 = input_df2

Function: _rfe
Compare with sklearn

Function: select_percentile
Compare with sklearn

Function: select k_best
Compare with sklearn

Function: select_fwe
Compare with sklearn

Function: variance_threshold
Compare with sklearn

Function: standard_scaler
Compare with sklearn

Function: robust_scaler
Compare with sklearn

Function: polynomial_features
Compare with sklearn

Function: min_max_scaler
Check with features in range
Check with features out of range
If train < test...don’t scale between 0,1..for all feat scaling

Function: max_abs_scaler
Check with features in range
Check with features out of range

Function: binarizer
Check with different thresholds

Function: pca
Compare with sklearn

Function: div remove params, no storing result 
Underflow?
Divide by 0

More verbose comments

Function: evaluate_individual
Evaluate balanced accuracy on a balanced and imbalanced data set

Try with different scoring functions

Function: balanced_accuracy
Test with hard-coded values?

Function: combined_selection_operatior
Create a fixed population, find what this function currently outputs, then assert that the resulting population should be the same as what this function currently outputs. (We are assuming that the function, as it is implemented now, is correct.)

Do this with a few different population sizes

Function: random_mutation_operator
Fix the RNG seed, then apply mutations to a few different individuals

Function: main
This can be tested as part of the integration tests

Function: positive_integer
Cannot be tested because it is encapsulated in the main() function

Function: float_range
Cannot be tested because it is encapsulated in the main() function

File: export_utils.py

Function: replace_mathematical_operators

Try with known values and results
Test with combination of operators too

Function: unroll_nested_function_calls

Try with known values and results

Function: generate_import_code

Try with known values and results

Function: replace_function_calls

Check for different params of learning rate, max_depth, n_estimators
For Scikit-learn functions 

DEAP library
Don’t forget this file :-)

Integration tests

Fixed dataset: use MNIST data set from sklearn
Fixed random number generator seed
Fixed # of generations (5)
Few parameters
Training score should be >=0 by the end
Testing score should be >=0 by the end

Helpful links
https://coveralls.io/github/rhiever/tpot?branch=master
https://travis-ci.org/rhiever/tpot
https://landscape.io/github/rhiever/tpot/211/messages/style
",list unit back working issue already unit run nose return copy data frame every feat operator class guess group file function assertion present add cover new added since test function fit fit tested integration function tested inside fit function predict testing training function score use fixed pipeline data output score time function export necessary unit test functionality tested function output decision tree function function function function function function function function compare function compare function select compare function compare function compare function compare function compare function compare function check range check range train test scale feat scaling function check range check range function check different function compare function div remove result underflow divide verbose function evaluate balanced accuracy balanced data set try different scoring function test function create fixed population find function currently assert resulting population function currently assuming function correct different population size function fix seed apply different function main tested part integration function tested main function function tested main function file function try known test combination function try known function try known function check different learning rate library forget file integration fixed use data set fixed random number generator seed fixed training score end testing score end helpful link,issue,positive,positive,neutral,neutral,positive,positive
219435878,"I like this idea. It should be possible to limit what kind of classifiers TPOT uses.

The main question is how we can implement this into the current TPOT interface. My first thought is to add a new boolean parameter to `__init__()` for each classifier -- each `True` by default -- that will allow the user to toggle the various classifiers on or off. This setup will work well with the command-line TPOT interface as well, as each classifier can be toggled off with a `--no-x-classifier` command.

However, that method seems to bloat the interface quite a bit, especially if we end up with 10+ classifiers implemented in TPOT. Any ideas on a better interface?
",like idea possible limit kind main question implement current interface first thought add new parameter classifier true default allow user toggle various setup work well interface well classifier command however method bloat interface quite bit especially end better interface,issue,positive,positive,positive,positive,positive,positive
219322321,"Technically TPOT should work fine with any Python install, but the main
reason that I limit my technical support to Anaconda is because there's a
million ways that Python installs can go wrong. Anaconda makes those
installs easy, clean, and most importantly consistent.

The reason I brought up Anaconda in this case is to see whether you can
reproduce the issue in your system with an Anaconda install. If you can't,
then we know it's an issue with your Python install rather than TPOT itself.

On Saturday, May 14, 2016, Pierluigi Failla notifications@github.com
wrote:

> I can try to do that, but I really like to use it in my own environment.
> Do you plan to release TPOT only for Anaconda env?
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/141#issuecomment-219219590

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",technically work fine python install main reason limit technical support anaconda million way python go wrong anaconda easy clean importantly consistent reason brought anaconda case see whether reproduce issue system anaconda install ca know issue python install rather may wrote try really like use environment plan release anaconda reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
219219590,"I can try to do that, but I really like to use it in my own environment. Do you plan to release TPOT only for Anaconda env?
",try really like use environment plan release anaconda,issue,negative,positive,positive,positive,positive,positive
219200887,"The only major difference between our setups - as far as I can tell - is
that we use Anaconda and your install doesn't. Can you replicate this error
on your machine with an Anaconda install?

On Friday, May 13, 2016, Pierluigi Failla notifications@github.com wrote:

> Also updating deps the issues is still there. I replicated your reqs
> except for update_checker that I do not think is really relevant.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/141#issuecomment-219151680

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",major difference far tell use anaconda install replicate error machine anaconda install may wrote also still replicated except think really relevant reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,positive,positive,positive,positive
219151680,"Also updating deps the issues is still there. I replicated your reqs except for `update_checker` that I do not think is really relevant.
",also still replicated except think really relevant,issue,negative,positive,positive,positive,positive,positive
219149153,"Yes, all of the three notebooks are affected by the same issue. I'm trying to compare with your deps:

Python 2.7.11 2.7.11
numpy 1.11.0 1.8.1
scipy 0.17.0 0.14.0
sklearn 0.17.1 0.17.1
pandas 0.18.1 0.17.1
deap 1.0 1.0.1
xgboost 0.4 0.4a30 
update_checker 0.11 I do not have this

So it seems that we have few mismatched versions. I will try to update my reqs if possibile. In the meanwhile it seems that there is some empty list going around...
",yes three affected issue trying compare python try update meanwhile empty list going around,issue,negative,negative,neutral,neutral,negative,negative
219095625,"Here are the package versions that we're running for Python 2:

Python 2.7.11
numpy 1.11.0
scipy 0.17.0
sklearn 0.17.1
pandas 0.18.1
deap 1.0
xgboost 0.4 
update_checker 0.11

Do you get this error every time you run the code in the notebook?
",package running python python get error every time run code notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
218890056,"No I'm not running in Anconda, here is my setting:
- deap==1.0.1
- numpy==1.8.1
- scikit-learn==0.17.1
- scipy==0.14.0
- TPOT==0.3.0
- xgboost==0.4a30

and Python 2.7.11. Do I miss something?
",running setting python miss something,issue,negative,neutral,neutral,neutral,neutral,neutral
218871681,"I can't reproduce this error in Python 2.7 nor Python 3.5 with the TPOT v0.3.0. What version are you running? Are you running it on top of an up-to-date Anaconda install?
",ca reproduce error python python version running running top anaconda install,issue,negative,positive,positive,positive,positive,positive
218520850,"Sadly I don't think it's easy to test this much more without running GP during the tests (ew), or displaying the progress bar in stdout during tests (also bad IMO).

**Edit:**

Looks like Python 2.7 doesn't like the hacking I did on there. I'll look into that later.
",sadly think easy test much without running progress bar also bad edit like python like hacking look later,issue,negative,negative,negative,negative,negative,negative
218496167,"> There's also a reference to StratifiedShuffleSplit in tutorials/Titanic_Kaggle.ipynb. Not sure if you want that changed.

Sure, let's remove all of them while we're at it. Thank you for going through and cleaning up the docs like this.
",also reference sure want sure let remove thank going cleaning like,issue,positive,positive,positive,positive,positive,positive
218493623,"There's also a reference to StratifiedShuffleSplit in `tutorials/Titanic_Kaggle.ipynb`. Not sure if you want that changed.
",also reference sure want,issue,negative,positive,positive,positive,positive,positive
218337763,"All good. I did manage to use the right version by uninstalling and reinstalling scikit-learn. Sudo was not an issue here. It works now. Thanks for pointing me to the right version.
",good manage use right version issue work thanks pointing right version,issue,positive,positive,positive,positive,positive,positive
218337523,"They include a version string in the scikit-learn module:

https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/__init__.py#L40

You're just outputting the string to the console by typing `sklearn.__version__`.

---

When you run `$ sudo pip install <package>` you're installing packages to your system, not to your local installation.

Make sure you followed the installation instructions exactly (http://rhiever.github.io/tpot/installing/).

If it says to use `pip` instead of `sudo pip`, _then only type `pip`_.
",include version string module string console run pip install package system local installation make sure installation exactly use pip instead pip type pip,issue,negative,positive,positive,positive,positive,positive
218336869,"Yes. you are genius. Running that script in my IPython notebook gives me 0.16.1 . How does that work? How do I change my IPython to use this version?
",yes genius running script notebook work change use version,issue,negative,neutral,neutral,neutral,neutral,neutral
218336099,"That class was new in 0.17 so I'm not surprised it didn't work with 0.16. Are you sure that 0.17 is being used by TPOT? If you're using Anaconda it's possible that you modified your system installation of Python and not your local installation.

This should output your current version:

`$ python`

``` python
import sklearn
sklearn.__version__ # '0.17.1'
```
",class new work sure used anaconda possible system installation python local installation output current version python python import,issue,negative,positive,positive,positive,positive,positive
218279669,"Looks like it might be a bit better to do it in the selection method, `_combined_selection_operator`. That occurs at the start of every generation.
",like might bit better selection method start every generation,issue,positive,positive,positive,positive,positive,positive
218276618,"That's unfortunate wrt the `with` statement. That's an entire indentation on code that's already fairly indented.

> Is there some piece of code that DEAP touches once a generation finishes? I could have it bump the progress bar up to the correct level for the current generation if cached pipelines were executed.

It would be super hacky and probably not good practice, but you could probably inject something here: https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L259

That line is used to calculate the statistics (min, mean, max) at the end of every generation. Maybe worth digging around the DEAP docs more.
",unfortunate statement entire indentation code already fairly indented piece code generation could bump progress bar correct level current generation executed would super hacky probably good practice could probably inject something line used calculate statistic min mean end every generation maybe worth digging around,issue,positive,positive,positive,positive,positive,positive
218275034,"We could add a null pointer in the `__init__()` method but tqdm is made to work with the `with` block so that it can add pre and post-processing (initializing the bar, hiding it after the block of code finishes), so it shouldn't be initialized in `__init__()`.

Is there some piece of code that DEAP touches once a generation finishes? I could have it bump the progress bar up to the correct level for the current generation if cached pipelines were executed.
",could add null pointer method made work block add bar block code piece code generation could bump progress bar correct level current generation executed,issue,negative,neutral,neutral,neutral,neutral,neutral
218272637,"I like that idea. The only catch is that DEAP caches results for duplicate pipelines in the current generation, so it's likely that the actual executed number of `_evaluate_individual()` calls will be less than `pop size` x `generations`.

BTW: Probably better to just assign `self.pbar` in `__init__`.

... and of course all of this will only occur when `verbosity >= 2`.
",like idea catch duplicate current generation likely actual executed number le pop size probably better assign course occur verbosity,issue,positive,positive,positive,positive,positive,positive
218247647,"Unless we're going to monkey-patch over DEAP I think we should consider the number of steps in the process of fitting a classifier pipeline to be equal to the population count multiplied by the generation count, and then we can use the manual approach to tqdm's stepping like so:

``` python
def fit(self, features, classes):
    total = self.population_size * self.generations

    with tqdm(total=total) as self.pbar:
        # Continue with remainder of function

def _evaluate_individual(self, individual, training_testing_data):
    try:
        # Perform evaluation
        . . .
    except Exception:
        # Catch-all: Do not allow one pipeline that crashes to cause TPOT to crash
        # Instead, assign the crashing pipeline a poor fitness
        return 5000., 0.
    finally:
        self.pbar.update(1) # One more pipeline evaluated

    # Return values etc.
```

That's assuming that the GP is scoring pipelines by using the `_evaluate_individual()` method during the different generations.
",unless going think consider number process fitting classifier pipeline equal population count generation count use manual approach stepping like python fit self class total continue remainder function self individual try perform evaluation except exception allow one pipeline cause crash instead assign pipeline poor fitness return finally one pipeline return assuming scoring method different,issue,negative,positive,neutral,neutral,positive,positive
218243824,"We'll have to figure out how to get tqdm wrapped into the evolutionary algorithm. As we discussed yesterday, @teaearlgraycold, the GP algorithm is all in the `eaSimple` call. Here's the `eaSimple` code: https://github.com/DEAP/deap/blob/master/deap/algorithms.py#L162
",figure get wrapped evolutionary algorithm yesterday algorithm call code,issue,negative,neutral,neutral,neutral,neutral,neutral
218239501,"I'm a big fan of this idea (and tqdm in general).
",big fan idea general,issue,negative,positive,neutral,neutral,positive,positive
218237821,"It only prints the progress per generation with `verbosity=2`. I was aiming for a ""quiet"" classifier by default.

> Or is the initialization particularly costly? (The next steps were much shorter)

The initialization shouldn't be much more expensive than every other generation. Perhaps what happened was your initial population had a complex pipeline in it that takes a long time to evaluate, and the selection procedure for the next TPOT generation (that aims to maximize accuracy but minimize pipeline complexity) threw that slow pipeline out because it was too complex. That's my best guess.
",progress per generation aiming quiet classifier default particularly costly next much shorter much expensive every generation perhaps initial population complex pipeline long time evaluate selection procedure next generation maximize accuracy minimize pipeline complexity threw slow pipeline complex best guess,issue,positive,negative,neutral,neutral,negative,negative
218220036,"Thanks! Now I see that it prints progress per step (with `verbosity=2` or all options?). 
Yet, since my computer is relatively old, I though it doesn't (only after a few minutes on the `hill_valley_noisy_data` example I got he first step; I didn't know if it was running at all).

Or is the initialization particularly  costly? (The next steps were much shorter)
",thanks see progress per step yet since computer relatively old though example got first step know running particularly costly next much shorter,issue,positive,positive,positive,positive,positive,positive
218219023,"Interesting idea. We currently print the progress every generation of the algorithm, but perhaps we could also print the progress toward evaluating each generation's population along the way too. I certainly appreciate having a tqdm-like progress bar for long-running for loops.
",interesting idea currently print progress every generation algorithm perhaps could also print progress toward generation population along way certainly appreciate progress bar,issue,positive,positive,positive,positive,positive,positive
218173156,"> Well both of those would end up being O(n) operations. Would using the `apply()` method be faster because of C speedups?

Yes, I believe so. I've typically found speedups from replacing `for` loops with `apply()` calls on pandas DataFrames.
",well would end would apply method faster yes believe typically found apply,issue,positive,negative,negative,negative,negative,negative
218169275,"> feature_cols_only.apply(lamda row: np.count_nonzero(row), axis=1)

Well both of those would end up being O(n) operations. Would using the `apply()` method be faster because of C speedups?
",row row well would end would apply method faster,issue,negative,neutral,neutral,neutral,neutral,neutral
218082024,"@rhiever really cool idea. See also #137, as the ideas are pretty closely related.

Is there a link to the discussion you had with the Orange devs? What pipeline operators have/haven't been implemented in Orange?
",really cool idea see also pretty closely related link discussion orange pipeline orange,issue,positive,positive,positive,positive,positive,positive
218025049,"👍 

This could make a neat study.
",could make neat study,issue,negative,neutral,neutral,neutral,neutral,neutral
218010648,"This is why we need unit tests. :-)

On Monday, May 9, 2016, Daniel notifications@github.com wrote:

> My bad on this one
> 
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/138#issuecomment-218009142

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",need unit may wrote bad one thread reply directly view postdoctoral researcher institute university twitter,issue,negative,negative,negative,negative,negative,negative
218009904,"My vote is put exactly what you said here into the help output, and avoid a Field Guide until absolutely necessary.  I really like the idea of ""I can run this out of the box, and it will start with sane defaults, and I know this because it tells me so, and I don't feel like I need to go read up on stuff first""
",vote put exactly said help output avoid field guide absolutely necessary really like idea run box start sane know feel like need go read stuff first,issue,positive,positive,positive,positive,positive,positive
217990992,"Good idea. Most of the defaults are ideal according to a benchmark we did a while back, and we regularly re-benchmark TPOT to re-evaluate these defaults. The main parameter that you might change is `generations`, depending on whether you need to give TPOT more or less time to optimize the pipeline.

Perhaps an extra section -- something like a ""Field Guide for TPOT"" -- would be useful. Either way, 👍 to this suggestion.
",good idea ideal according back regularly main parameter might change depending whether need give le time optimize pipeline perhaps extra section something like field guide would useful either way suggestion,issue,positive,positive,positive,positive,positive,positive
217776687,"Sorry, had forgotten to push that commit until now. Should be ready to merge in.
",sorry forgotten push commit ready merge,issue,negative,negative,negative,negative,negative,negative
217677899,"Hrm. This branch has conflicts now because of the export_utils refactor. Should be a small fix, yes?
",branch small fix yes,issue,negative,negative,negative,negative,negative,negative
217587465,"That's brilliant! Yes, I think that would work just fine for these purposes. The only tricky part could be allowing TPOT to perform no preprocessing, but I suppose we could create a ""no preprocessing"" operator for each step.
",brilliant yes think would work fine tricky part could perform suppose could create operator step,issue,positive,positive,positive,positive,positive,positive
216684072,"Can we bypass having to roll custom mutation and crossover operators by changing how we pass the datasets around? I propose that we wrap the primary data structure (a pandas DataFrame / NumPy matrix) in different TPOT-specific subclasses and change the pipeline operators' typed contracts accordingly to restrict how operators get assembled. For example, the code where we add operators might look like the following:

```
# data preprocessor
self._pset.addPrimitive(self._standard_scaler, [pd.DataFrame], tpot.DataPreprocessed)

# feature preprocessor
self._pset.addPrimitive(self._binarizer, [tpot.DataPreprocessed, float], tpot.FeaturePreprocessed)

# feature selection
self._pset.addPrimitive(self._select_kbest, [tpot.FeaturePreprocessed, int], tpot.FeatureSelected)

# machine learning operator
self._pset.addPrimitive(self._random_forest, [tpot.FeatureSelected, int], tpot.MachineLearned)
```

Is this sensible? I'm assuming that the subclasses would pretty much just be wrappers for the original data structure, in an effort to keep things lightweight. 
",bypass roll custom mutation crossover pas around propose wrap primary data structure matrix different change pipeline accordingly restrict get example code add might look like following data feature float feature selection machine learning operator sensible assuming would pretty much original data structure effort keep lightweight,issue,positive,positive,positive,positive,positive,positive
216556976,"Also added operator that adds features for the count of zero and non-zero elements as per #133
",also added operator count zero per,issue,negative,neutral,neutral,neutral,neutral,neutral
216448868,"Sorry for late reply. Thats fine. Looking forward to your pr. 
",sorry late reply thats fine looking forward,issue,negative,negative,negative,negative,negative,negative
216417560,"Well, send in the PR once it's finished and we'll compare with what @KobaKhit has. :-)
",well send finished compare,issue,negative,neutral,neutral,neutral,neutral,neutral
216417439,"Oh, my bad. Well if not I've pretty much finished it
",oh bad well pretty much finished,issue,negative,negative,neutral,neutral,negative,negative
216417331,"I think @KobaKhit is working on this issue. Can you please confirm @KobaKhit?
",think working issue please confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
216325846,"What's the definition of balanced accuracy? Is it 1 - balanced error rate? Then this should be true.
",definition balanced accuracy balanced error rate true,issue,negative,positive,positive,positive,positive,positive
216230235,"#129 looks like a great PR to work on! I don't think anyone has been working on that one.

Just to make sure: @teaearlgraycold, what issues are you working on this week?

#126 is an ongoing project that I'm working on with a student for the next couple weeks. I've just merged the PR since all the unit tests look fine. If any of your changes affect those unit tests, please make sure to update them.

Cheers!
",like great work think anyone working one make sure working week ongoing project working student next couple since unit look fine affect unit please make sure update,issue,positive,positive,positive,positive,positive,positive
216230003,"Going to go ahead and merge this PR. @GJena, when you have more unit tests ready, please open a separate PR. Thank you!
",going go ahead merge unit ready please open separate thank,issue,positive,positive,neutral,neutral,positive,positive
216081164,"Hey, not exactly sure where is best to message you about this but what would you advise as a next step for contributing to TPOT? I'm looking at issue #129 which doesn't seem to have anyone working on it currently while also seeming like it won't require any major code restructuring.

I am also confused about the ongoing PR #126 with unit tests being added, it appears you are waiting for something before you merge it with the rest of the code. Not sure where this leaves me in terms of wanting to help adding tests.

Do you have any thoughts?
",hey exactly sure best message would advise next step looking issue seem anyone working currently also seeming like wo require major code also confused ongoing unit added waiting something merge rest code sure leaf wanting help,issue,positive,positive,positive,positive,positive,positive
216000657,"[![Coverage Status](https://coveralls.io/builds/6000042/badge)](https://coveralls.io/builds/6000042)

Coverage remained the same at 28.706% when pulling **1e6d9f85c9a99f64b3dc3e22e817eb9208939471 on screwed99:contr-docs** into **8417028dc6940c4fdefd98233b61e0c66716e062 on rhiever:master**.
",coverage status coverage screwed master,issue,negative,neutral,neutral,neutral,neutral,neutral
212637717,"Looks like Monte Carlo Search Tree would be helpful. Give a time constraint, the system could get the best results within that time. AlphaGO is using this method.
",like monte search tree would helpful give time constraint system could get best within time method,issue,positive,positive,positive,positive,positive,positive
212637000,"We can extend this idea to a ""pipeline"" knowledge base, or knowledge graph. Then, given a pile of data, the system can figure out some ""pipeline"" .
",extend idea pipeline knowledge base knowledge graph given pile data system figure pipeline,issue,negative,negative,negative,negative,negative,negative
211506354,"@teaearlgraycold is adding unit tests for _rbf() (new feature preprocessing method), so don't worry about that one.
",unit new feature method worry one,issue,negative,positive,positive,positive,positive,positive
211361012,"Happy to see the coverage going up slowly but surely! 👍 
",happy see coverage going slowly surely,issue,positive,positive,positive,positive,positive,positive
211359919,"Well done! Not too bad of a code base, right? :-)

I'll have to add details on how to run `mkdocs` to the contributing section. Expect updates on that to come soon.

I'll fix the parenthesis issue now.
",well done bad code base right add run section expect come soon fix parenthesis issue,issue,negative,negative,negative,negative,negative,negative
208615297,"Check these example notebooks:
- https://github.com/rhiever/tpot/blob/master/tutorials/IRIS.ipynb
- https://github.com/rhiever/tpot/blob/master/tutorials/MNIST.ipynb
- https://github.com/rhiever/tpot/blob/master/tutorials/Titanic_Kaggle.ipynb

TPOT requires the ""standard"" supervised data set format. Check this tutorial to see what that entails: https://github.com/amueller/scipy_2015_sklearn_tutorial/blob/master/notebooks/01.3%20Data%20Representation%20for%20Machine%20Learning.ipynb
",check example standard data set format check tutorial see,issue,negative,neutral,neutral,neutral,neutral,neutral
208613487,"@rhiever ok, which is the input data format, let's say a typical row I would have from the `word2vec` vector for a label is the array of floating point numbers, do this work?
",input data format let say typical row would vector label array floating point work,issue,negative,negative,negative,negative,negative,negative
208606325,"Gotcha. If you can feed TPOT features such as those distances (or even the vector values from word2vec) and provide it labels for those features, then it can try to optimize a pipeline that maximizes classification accuracy for that data set.
",feed even vector provide try optimize pipeline classification accuracy data set,issue,negative,neutral,neutral,neutral,neutral,neutral
208600850,"So, since I have no description of a `topic` then the `label`, I thought to set as metrics `word2vec` features vector for each `label`, so I have the distances basically.
",since description topic label thought set metric vector label basically,issue,negative,neutral,neutral,neutral,neutral,neutral
208594769,"Ok thanks. Let's suppose that I have manually tagged features (i.e. in this case topics), so labelled them in order to belong to a most generic topic (new one) so let's say we have a test set with a initial classification in N categories/topics.
",thanks let suppose manually tagged case order belong generic topic new one let say test set initial classification,issue,negative,positive,neutral,neutral,positive,positive
208591186,"Currently, TPOT only supports supervised classification tasks. Is what you're describing a supervised classification task, i.e., do you have a set of features with labels that you're attempted to model?
",currently classification classification task set model,issue,negative,neutral,neutral,neutral,neutral,neutral
208550232,"@KobaKhit I think pickle is generally a good idea in terms of efficiency; however, and maybe I am too paranoid :P, I always worry about compatibility issues (e.g., the different pickle protocols and Py version incompatibilities). In any case, if pickle is only used internally during model training on the particular system where TPOT is ""trained"", this wouldn't be a problem I guess. 
However, instead of pickle, I'd suggest joblib maybe since it's better at storing NumPy arrays; there was a discussion on the mailing list today where someone  pickled a random forest (50 trees, 23 features, 20 mb dataset) -> 50 mb with standard pickle, ~15 mb with joblib.

Personally, I switched over to using JSON files for model persistence (e.g., when using scikit-learn or also other things). This way, I always have a human readable record of everything (in case pickle files get corrupted or are incompatible in a different environment); sure, this would probably slower computationally, but I think it would be the more ""robust"" or ""reproducible"" option. Someone else asked me about that recently so I put up a quick ipynb with an example specific to sklearn if you are interested: http://nbviewer.jupyter.org/github/rasbt/python-machine-learning-book/blob/master/code/bonus/scikit-model-to-json.ipynb
",think pickle generally good idea efficiency however maybe paranoid always worry compatibility different pickle version case pickle used internally model training particular system trained would problem guess however instead pickle suggest maybe since better discussion list today someone random forest standard pickle personally switched model persistence also way always human readable record everything case pickle get corrupted incompatible different environment sure would probably think would robust reproducible option someone else recently put quick example specific interested,issue,positive,positive,positive,positive,positive,positive
208516855,"Might be helpful [3.4. Model persistence](http://scikit-learn.org/stable/modules/model_persistence.html). Basically, an example of how to save a model in as a pickle. Below is code example from linked page

``` python
>>> from sklearn import svm
>>> from sklearn import datasets
>>> clf = svm.SVC()
>>> iris = datasets.load_iris()
>>> X, y = iris.data, iris.target
>>> clf.fit(X, y)  
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',
    max_iter=-1, probability=False, random_state=None, shrinking=True,
    tol=0.001, verbose=False)

>>> import pickle
>>> s = pickle.dumps(clf)
>>> clf2 = pickle.loads(s)
>>> clf2.predict(X[0:1])
array([0])
>>> y[0]
0
```

Maybe create a new self variable `self.saved_pipe =  pickle.dumps(clf)` that saves the pipeline of the best model in a generation and update it every generation.
",might helpful model persistence basically example save model pickle code example linked page python import import iris import pickle array maybe create new self variable pipeline best model generation update every generation,issue,positive,positive,positive,positive,positive,positive
205047579,"Yes, please feel free to do that. I will be happy to review the PR.
",yes please feel free happy review,issue,positive,positive,positive,positive,positive,positive
205046898,"OK great, thank-you for the feedback!

Can I go ahead and raise an issue for the contributor document changes and submit a pull request with the changes (unless you'd prefer doing it another way)?
",great feedback go ahead raise issue contributor document submit pull request unless prefer another way,issue,positive,positive,positive,positive,positive,positive
205042904,"Many thanks, and nice work on your first PR. :+1: 

> I was wondering what conda environment you would suggest setting up for development (I would guess installing the tpot package is not necessary and for all testing of changes just set your working directory to inside the clones git project?)

Hmmm... we should probably add section to the contributor document answering this question. The short answer is:
- Use an install of the Anaconda Python distribution w/ Python 3, all packages updated to the latest version
- pip install the other dependencies (DEAP, XGBoost, etc.)
- Before making any changes to the code, create a new branch in your fork of the TPOT repo
- cd into the base TPOT directory, and when running TPOT during development:
  - import TPOT normally if using the script version -- make sure you don't have TPOT installed locally, just in case, so you know that you're using the dev version
  - Call TPOT on the command line with `python -m tpot.tpot`

> Further to the above, the contribution guidelines mention CI but don't really mention how to test locally before committing, pushing and issues a pull request. I installed nose within my conda environment and ran nosetests -s -v as found in the CI files, is that sufficient?

Yes, running nosetests in the base TPOT directory should run all the unit tests for you. We're still working on expanding the unit tests to cover more of the code base.
",many thanks nice work first wondering environment would suggest setting development would guess package necessary testing set working directory inside git project probably add section contributor document question short answer use install anaconda python distribution python latest version pip install making code create new branch fork base directory running development import normally script version make sure locally case know dev version call command line python contribution mention really mention test locally pushing pull request nose within environment ran found sufficient yes running base directory run unit still working expanding unit cover code base,issue,positive,positive,neutral,neutral,positive,positive
204783820,"I agree that we could probably structure the pipelines at least a little bit to reduce the complexity, as I've definitely gotten runs where the best pipeline had feature selection/preprocessing steps happening after the modeling steps, which seems unnecessary. 
",agree could probably structure least little bit reduce complexity definitely gotten best pipeline feature happening modeling unnecessary,issue,positive,positive,neutral,neutral,positive,positive
202142506,"I suppose the default could be n_estimators=100 and it should still work fine. I just want to make sure that RFs and other ensemble methods don't get skipped over simply because they didn't have enough estimators.
",suppose default could still work fine want make sure ensemble get simply enough,issue,positive,positive,positive,positive,positive,positive
202142332,"Increasing the estimators will increase performance, but I would think when you're trying a lot of different parameters it will increase the time for all those decision trees to run, and whether it wouldn't be better to increase n_estimators later. I guess if they don't perform well enough in the first instance they're not going to make it into the next generation. Really just curious on your thoughts.
",increasing increase performance would think trying lot different increase time decision run whether would better increase later guess perform well enough first instance going make next generation really curious,issue,positive,positive,neutral,neutral,positive,positive
202127012,"The RF operator has had its n_estimators parameter removed, and it now defaults to 500 for n_estimators. The remaining ensemble operators still need to be reworked.
",operator parameter removed ensemble still need reworked,issue,negative,neutral,neutral,neutral,neutral,neutral
202054136,"Cool beans. Can you also please make sure that it's using this accuracy
metric: https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L1192

On Sun, Mar 27, 2016 at 12:55 AM, Nathan notifications@github.com wrote:

> The only thing that changes here is that you shouldn't pass a verbosity
> parameter.
> 
> —
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/121#issuecomment-201991946

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",cool also please make sure accuracy metric sun mar wrote thing pas verbosity parameter reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
201991946,"The only thing that changes here is that you shouldn't pass a verbosity parameter. The logbook with all the generation data will write to file after the eaSimple finishes, and using verbosity seems to flush the logbook output to stdout rather than to file. 
",thing pas verbosity parameter logbook generation data write file verbosity flush logbook output rather file,issue,negative,neutral,neutral,neutral,neutral,neutral
201987029,"You're right. I neglected to trim out some code. I'll clean it up, run a test, and submit a commit shortly.
",right trim code clean run test submit commit shortly,issue,positive,positive,positive,positive,positive,positive
201962378,"Wow, this required a lot more code than I expected! Was the custom implementation of eaSimple necessary? And all the consensus_two etc operators?
",wow lot code custom implementation necessary,issue,positive,positive,neutral,neutral,positive,positive
201077786,"Sounds good to me. Want to send in a PR on this branch?

I'm currently finishing up some other TPOT benchmarks -- shouldn't take
more than the weekend -- but I can slate this benchmark for the next batch.

On Thu, Mar 24, 2016 at 6:54 PM, Nathan notifications@github.com wrote:

> You're right that the data I was using was perhaps too easy– I was using
> testing code that tested with the sklearn digits dataset, rather than
> MNIST! This is embarrassing to say the least. On the bright side, at least
> these tests suggest that the operators are somewhat robust in the
> smaller-data, slightly-longer, slightly-bigger population 'regime'.
> 
> In the interest of time, how about I'll run the same tests on random
> samples from the GAMETES-hard and MNIST proper to see if there's promise,
> and in the mean-time we can prep for a larger HPCC benchmark? I can run my
> tests in a more parallel manner so it's not a week turnaround.
> 
> —
> You are receiving this because you authored the thread.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/105#issuecomment-201064327

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",good want send branch currently finishing take weekend slate next batch mar wrote right data perhaps testing code tested rather embarrassing say least bright side least suggest somewhat robust population interest time run random proper see promise prep run parallel manner week turnaround thread reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,neutral,neutral,positive,positive
201064327,"You're right that the data I was using was perhaps too easy– I was using testing code that tested with the sklearn digits dataset, rather than MNIST! This is embarrassing to say the least. On the bright side, at least these tests suggest that the operators are somewhat robust in the smaller-data, slightly-longer, slightly-bigger population 'regime'. 

In the interest of time, how about I'll run the same tests on random samples from the GAMETES-hard and MNIST proper to see if there's promise, and in the mean-time we can prep for a larger HPCC benchmark? I can run my tests in a more parallel manner so it's not a week turnaround. 
",right data perhaps testing code tested rather embarrassing say least bright side least suggest somewhat robust population interest time run random proper see promise prep run parallel manner week turnaround,issue,negative,negative,neutral,neutral,negative,negative
200990267,"What benchmarks are you running it on? It looks like the classification accuracy for many of the runs are fairly high, so there probably isn't much room for ensembles to improve. What about a harder data set, e.g., GAMETES-hard? Maybe we should just run a large benchmark on the HPCC?
",running like classification accuracy many fairly high probably much room improve harder data set maybe run large,issue,positive,positive,positive,positive,positive,positive
200644752,"I'm getting a lot of variance in a few of the columns, so I may run some more trials. 
",getting lot variance may run,issue,negative,neutral,neutral,neutral,neutral,neutral
200415767,"Alright, so I took the same ideas from the consensus operators that we tried and applied them here. Each individual / population is evaluated on the test dataset. 

### Weights

acc_\* – each individual's guess is weighted according to their individual accuracy.
uni_\* – each individual's guess has the same weight.

### Selection

*_max_class –  the class that has the highest weight (or in the uni case, the highest frequency) is the ensemble's guess for that test instance.
*_mean_class – the class with the mean weight / frequency is the ensemble's guess
*_median_class – the class with the median weight / frequency is the ensemble's guess
*_min_class – the class with the minimum weight / frequency is the ensemble's guess
*_threshold_class – the first class that passes a certain threshold in percentage of weight is the ensemble's guess. 
",alright took consensus tried applied individual population test individual guess weighted according individual accuracy individual guess weight selection class highest weight case highest frequency ensemble guess test instance class mean weight frequency ensemble guess class median weight frequency ensemble guess class minimum weight frequency ensemble guess first class certain threshold percentage weight ensemble guess,issue,negative,positive,neutral,neutral,positive,positive
200299085,"What are each of the new columns? I'm looking at the data this morning.
",new looking data morning,issue,negative,positive,positive,positive,positive,positive
200149822,"Hey so I've cleaned up some of my data and made it available [here](https://drive.google.com/folderview?id=0B0837fFXf76rQUowY3AzQTNoREU&usp=sharing). I've been trying to come up with useful visualizations, and figured it'd be more productive to share it in the meantime. 
",hey data made available trying come useful figured productive share,issue,positive,positive,positive,positive,positive,positive
198927034,"We should look at using [sklearn Pipeline objects](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to represent our pipelines. I believe that could go a long way toward solving this issue.
",look pipeline represent believe could go long way toward issue,issue,negative,negative,neutral,neutral,negative,negative
198840814,"Let me know if you want to schedule another video chat. I'm excited to hear how this turned out!
",let know want schedule another video chat excited hear turned,issue,negative,positive,positive,positive,positive,positive
198838645,"Some of the shorter tests are wrapping up and I think I have enough for some preliminary results -- I'll try to clean things up and link them here in the next couple of days. 
",shorter wrapping think enough preliminary try clean link next couple day,issue,negative,positive,positive,positive,positive,positive
198749986,"Yes, I was also thinking about the complexity to understand this for a first time user. I will look into it. 
",yes also thinking complexity understand first time user look,issue,negative,positive,positive,positive,positive,positive
198695419,"I think all the extra models makes the example code look too complex. How about we just have the example code be a generated Random Forest call with 100+ estimators? That's usually what comes out of TPOT for the MNIST data set now anyway.
",think extra example code look complex example code random forest call usually come data set anyway,issue,negative,negative,negative,negative,negative,negative
198090266,"That stinks, but negative results are useful results too, I suppose. I'll take a look at testing without Pareto optimization when I get the chance, but I agree that #105 is probably more promising. 
",negative useful suppose take look testing without optimization get chance agree probably promising,issue,negative,positive,neutral,neutral,positive,positive
197867695,"Sure, submit the PR? :-)

On Thursday, March 17, 2016, PRONOjit Saha notifications@github.com wrote:

> Hey @rhiever https://github.com/rhiever , we need to change the
> StratifiedShuffleSplit to train_test_split in the above tpot example as
> well.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/114#issuecomment-197866047

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",sure submit march wrote hey need change example well reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
197866047,"Hey @rhiever , we need to change the `StratifiedShuffleSplit` to `train_test_split` in the above tpot example as well. 
",hey need change example well,issue,negative,neutral,neutral,neutral,neutral,neutral
197858790,"Do you mean [this example](https://github.com/rhiever/tpot#example)? Here's an example of that code transforming the data from `load_digits()` into pandas DataFrame format.

``` python
import numpy as np
import pandas as pd

from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import load_digits

digits = load_digits()

tpot_data = pd.DataFrame(digits.data)
tpot_data['class'] = digits.target

training_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))

result1 = tpot_data.copy()

# Perform classification with a logistic regression classifier
lrc1 = LogisticRegression(C=2.8214285714285716)
lrc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)
result1['lrc1-classification'] = lrc1.predict(result1.drop('class', axis=1).values)
```
",mean example example code transforming data format python import import import import import next iter result perform classification logistic regression classifier result,issue,negative,negative,negative,negative,negative,negative
197381886,"Welp... I'm sad to report that TPOT doesn't really seem to be evolving pipelines with the consensus operator. Only 1.5% of the pipelines from the benchmark even contained a consensus operator, and none of those really seemed to use them in a meaningful way.

It's possible that Pareto optimization is disfavoring the larger pipelines that the consensus operators entail. If you want to roll back the GP selection process to simply maximize classification accuracy again, I can grab the latest from this fork and re-run the benchmark.

I should also note that a large portion (over half) of the runs didn't finish in time -- I only gave each run 8 hours to complete 100 generations -- so it's possible that consensus operators were being used there. That's still a bad sign, though, as it likely means that TPOT with the consensus operators are _even slower_ than it already is. Not good!

Perhaps a more promising path is to try to combine the population of pipelines into ensembles, as in #105. Really looking forward to hearing how that pans out.
",sad report really seem consensus operator even consensus operator none really use meaningful way possible optimization consensus entail want roll back selection process simply maximize classification accuracy grab latest fork also note large portion half finish time gave run complete possible consensus used still bad sign though likely consensus already good perhaps promising path try combine population really looking forward hearing,issue,negative,positive,neutral,neutral,positive,positive
196912692,"Agreed. It's not worth it to fix the merge if the results aren't looking good. But if they are (fingers crossed), at least this PR is only ~a week behind.  
",agreed worth fix merge looking good crossed least week behind,issue,positive,positive,neutral,neutral,positive,positive
196904433,"The jobs are finishing up today, so I should be able to analyze the results tomorrow morning and see how this turned out.

Also looks like this branch has conflicts with the latest version of TPOT. Argh. Let's not bother cleaning up that merge until we see if this feature will allow for better pipelines.
",finishing today able analyze tomorrow morning see turned also like branch latest version let bother cleaning merge see feature allow better,issue,positive,positive,positive,positive,positive,positive
196636861,"> We're always changing the data into numpy matrices anyway when passing them to the sklearn operations, so I'm not seeing the point of using pandas DataFrames any more.

Yes, I agree 100%

> Ensure that the class column is always the last entry in the matrix (in place of having a class column)

I'd suggest using 2 arrays (""matrices"") instead of 1. The reason is that you may want to have separate Numpy `dtype`s: The feature array (e.g. X) as `float` array and the class label array (e.g., `y`) as integer array. Here, you can simply operate via a meta-array, an index array, that is created at the very beginning of the TPOT tree: something like `idx = np.asarray(range(data.shape[0])`. Operating via the `idx` variable then (`X[idx]`, `y[idx]`) could also help you to avoid creating unnecessary copies of the arrays internally (plus, having ""shared memory"" for parallelization at some point is probably a good ideas as well)
",always data matrix anyway passing seeing point yes agree ensure class column always last entry matrix place class column suggest matrix instead reason may want separate feature array float array class label array integer array simply operate via index array beginning tree something like range operating via variable could also help avoid unnecessary internally plus memory parallelization point probably good well,issue,positive,positive,neutral,neutral,positive,positive
196524505,"Got it working with the statistics object, thanks for the tips. I'm gonna spin up these tests.
",got working statistic object thanks gon na spin,issue,negative,positive,positive,positive,positive,positive
196461986,"Interesting, I had convinced myself that the Statistics object wouldn't be able to give us access to the population directly. I'll test this out; it shouldn't change my analysis after the fact that much. 
",interesting convinced statistic object would able give u access population directly test change analysis fact much,issue,positive,positive,positive,positive,positive,positive
196459042,"I don't think you'll need to roll your own version of eaSimple. Here's some code from another project I ran where you can store the population in the log and then do post-analysis on the population in the log.

``` python
stats = tools.Statistics(lambda ind: (int(ind.fitness.values[0]), round(ind.fitness.values[1], 2)))
stats.register(""Minimum"", np.min, axis=0)
stats.register(""Maximum"", np.max, axis=0)
# This should store a copy of pop every generation
stats.register(""Population"", lambda x: copy.deepcopy(pop))

# Use normal TPOT settings, of course -- not these settings
pop, log = algorithms.eaSimple(pop, toolbox, cxpb=0., mutpb=0.5, ngen=1000, 
                               stats=stats, halloffame=hof, verbose=False)
```

Let me know if that works. Alternatively, you can modify the HOF to store the top 100 best pipelines discovered so far, and change

``` python
stats.register(""Population"", lambda x: copy.deepcopy(pop))
```

to

``` python
stats.register(""HOF"", lambda x: copy.deepcopy(hof))
```

and that will only change the analysis slightly -- using the _best 100 pipelines ever_ as the ensemble instead of the pipelines currently in the population.
",think need roll version code another project ran store population log population log python lambda round minimum maximum store copy pop every generation population lambda pop use normal course pop log pop toolbox let know work alternatively modify store top best discovered far change python population lambda pop python lambda change analysis slightly ensemble instead currently population,issue,positive,positive,positive,positive,positive,positive
196448253,"I've created my own version of the eaSimple algorithm where we can dig into the individual/ensemble statistics, but I've had difficulty in exposing and aggregating each individual pipeline's guesses. But in the last day or so I've broken through that, and have started getting numbers. I think I'm gonna spin up a cheap AWS instance and just run a ton of tests. 
",version algorithm dig statistic difficulty individual pipeline last day broken getting think gon na spin cheap instance run ton,issue,negative,neutral,neutral,neutral,neutral,neutral
196050062,"Understood. Alright, looks good to merge. Thanks again! :-)
",understood alright good merge thanks,issue,positive,positive,positive,positive,positive,positive
195950895,"If you set the `random_state` to the same thing in your tests, they should come out the same. That's what I verified on my end yesterday.
",set thing come end yesterday,issue,negative,neutral,neutral,neutral,neutral,neutral
195928777,"I have checked the splits, and the split ratio is same for both `train_test_split` and `StratifiedShuffleSplit`, but the split indices are somewhat different which is expected.  
",checked split ratio split index somewhat different,issue,negative,neutral,neutral,neutral,neutral,neutral
195822986,"Just checked and, by default, `train_test_split` doesn't stratify the data by class. You have to pass the `stratify` option and a list of the class labels, e.g.,

``` python
X_train, X_test, y_train, y_test = train_test_split(input_data.drop('class', axis=1).values, 
                                                    input_data['class'].values,
                                                    train_size=0.75, test_size=0.25,
                                                    random_state=RANDOM_STATE,
                                                    stratify=input_data['class'].values)
```

Please make that change and let's see if that passes on Travis-CI. If it does, we'll merge away!
",checked default stratify data class pas stratify option list class python please make change let see merge away,issue,negative,neutral,neutral,neutral,neutral,neutral
195798074,"In hurry, forgot to commit! Should be good now. 
",hurry forgot commit good,issue,positive,positive,positive,positive,positive,positive
195781748,"Hmmm... it's saying that no changes were made. Did you update from `master` and overwrite your changes?
",saying made update master overwrite,issue,negative,neutral,neutral,neutral,neutral,neutral
195466993,"Another small update: HPCC is taking bloody forever to run these jobs. They're stuck in a queue behind some bigger jobs I had queued. Bad queue management system... sigh.
",another small update taking bloody forever run stuck queue behind bigger bad queue management system sigh,issue,negative,negative,negative,negative,negative,negative
195428527,"Also: From reading your xgboost issue, it sounds like you could actually just disable xgboost in TPOT by removing only these two lines in your TPOT install:
- https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L41
- https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L129
",also reading issue like could actually disable removing two install,issue,negative,neutral,neutral,neutral,neutral,neutral
195427949,"I'm going to close this issue and move the convo to the xgboost issue. Hopefully this can be addressed, else we may have to consider rolling back to using the sklearn GradientBoostingClassifier.
",going close issue move issue hopefully else may consider rolling back,issue,negative,neutral,neutral,neutral,neutral,neutral
195254329,"Windows 7, LoadLibrary call fails due to some unknown (to me) dependency. 
here is [issue](https://github.com/dmlc/xgboost/issues/933) I submitted. Frankly, it is a weird issue and I believe it may be something wrong with my specific setup, although I can not quite figure out what exactly :(
",call due unknown dependency issue frankly weird issue believe may something wrong specific setup although quite figure exactly,issue,negative,negative,negative,negative,negative,negative
195137275,"Thank you! Please feel free to send more PRs with other small code optimizations like these. I believe there were a few other changes from the PR that would fit well in TPOT.
",thank please feel free send small code like believe would fit well,issue,positive,positive,positive,positive,positive,positive
194893919,"What is the error that you're having with xgboost? What OS are you running?

If you're familiar with Python, you can remove xgboost by:
- Removing it from [this list](https://github.com/rhiever/tpot/blob/master/setup.py#L36)
- Removing it from the following lines/functions:
  - https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L41
  - https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L129
  - https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L545

then running `python setup.py install` in the base of the tpot directory.

I'm very curious to learn what issues you're having with xgboost, though. I only added it to TPOT because I was assured that it would be easy for everyone to install.
",error o running familiar python remove removing list removing following running python install base directory curious learn though added assured would easy everyone install,issue,positive,negative,neutral,neutral,negative,negative
194113986,"Oh wait I merged the upstream changes without thinking about the possible consequences for the benchmark tests; should I go ahead and revert the merge?
",oh wait upstream without thinking possible go ahead revert merge,issue,negative,neutral,neutral,neutral,neutral,neutral
194033291,"Ah, yes. I usually think of threshold as ""if X% of guesses are for one
class, then it's that class."" Maybe that will be too difficult in the multi
class case though.

On Tuesday, March 8, 2016, Nathan notifications@github.com wrote:

> I realized that I might have a different idea of thresholding than what
> you're talking about: I'm thinking of assigning a DataFrame a 0 weight
> (eliminating impact on the guesses) if they do not pass a (perhaps
> parameterized) threshold of accuracy.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/96#issuecomment-194010819.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",ah yes usually think threshold one class class maybe difficult class case though march wrote might different idea talking thinking weight impact pas perhaps threshold accuracy reply directly view postdoctoral researcher institute university twitter,issue,negative,negative,negative,negative,negative,negative
194010819,"I realized that I might have a different idea of thresholding than what you're talking about: I'm thinking of assigning a DataFrame a 0 weight (eliminating impact on the guesses) if they do not pass a (perhaps parameterized) threshold of accuracy. 
",might different idea talking thinking weight impact pas perhaps threshold accuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
193923461,"Possibly not true after further validation. Closing this issue until we figure it out.
",possibly true validation issue figure,issue,negative,positive,positive,positive,positive,positive
193500097,"If you run [this script](https://github.com/rhiever/sklearn-benchmarks/blob/master/download_data.py), you'll have access to a whole bunch of 'em. Take your pick. :-)

I think just one data set is fine to start with though, as a proof of concept.
",run script access whole bunch take pick think one data set fine start though proof concept,issue,negative,positive,positive,positive,positive,positive
193496251,"Should we choose a couple/few data sets to test on, to try and create a more robust analysis? Which ones might be most appropriate? MNIST, wine, breast cancer?
",choose data test try create robust analysis might appropriate wine breast cancer,issue,negative,positive,positive,positive,positive,positive
193494856,"BTW, if you want to take a stab at #105 in a separate branch in the meantime, that would be awesome. I think that's a huge issue to address on the research end right now.
",want take stab separate branch would awesome think huge issue address research end right,issue,negative,positive,positive,positive,positive,positive
193494561,"Benchmarks are queued now. Have 10 copies of TPOT-Consensus running against 90 different data sets. Analyzing the resulting best pipelines should give us a good sense of whether the consensus operators are usefully contributing or not.

> Are there any other schemes we want to test, though? Threshold?

Yes, that could be a good one.
",running different data resulting best give u good sense whether consensus usefully want test though threshold yes could good one,issue,positive,positive,positive,positive,positive,positive
193075344,"Okay, then I will hold off from getting deeper into this until the benchmarks and work on something else. I think that there's a lot more information encoded in the input features from the various DataFrames rather than just using the guesses, but perhaps it's not worth the effort right now. Are there any other schemes we want to test, though? Threshold?
",hold getting work something else think lot information input various rather perhaps worth effort right want test though threshold,issue,negative,positive,positive,positive,positive,positive
192891360,"I dint start on this one. Ok, please go ahead. 
",dint start one please go ahead,issue,negative,neutral,neutral,neutral,neutral,neutral
192891231,"Yes, I did give it a try couple of days back, but getting some spurious results. Checking on it. yes you can mark it.
",yes give try couple day back getting spurious yes mark,issue,positive,neutral,neutral,neutral,neutral,neutral
192890168,"The three preprocessing operators are now implemented in this branch: https://github.com/rhiever/tpot/tree/more-feature-preprocessors

TODO:
- [x] Add `export()` support for them
- [x] Add them to the docs
",three branch add export support add,issue,negative,neutral,neutral,neutral,neutral,neutral
192885270,"We won't be able to add support for `OneHotEncoder` until #29 is solved. I will add support for the other three preprocessors.

Further complication with `OneHotEncoder`: It should only be applied to categorical columns. Thus, we need an effective heuristic to detect categorical vs. continuous columns. I've raised a question on StackOverflow to discuss ideas: http://stackoverflow.com/questions/35826912/what-is-a-good-heuristic-to-detect-if-a-column-in-a-pandas-dataframe-is-categori
",wo able add support add support three complication applied categorical thus need effective heuristic detect categorical continuous raised question discus,issue,positive,positive,positive,positive,positive,positive
192885245,"Now that we're looking at adding support for `sklearn.preprocessing.OneHotEncoder`, I'm looking more seriously at sparse matrix support for TPOT. I've changed this issue to high priority and will think about it more in the near future.

We may have to rework TPOT's internals entirely to work with numpy arrays/matrices in place of pandas DataFrames. I think I'm okay with that -- it would actually eliminate the pandas dependency -- but that means this would entail a significant rework of TPOT.
",looking support looking seriously sparse matrix support issue high priority think near future may rework internals entirely work place think would actually eliminate dependency would entail significant rework,issue,positive,positive,neutral,neutral,positive,positive
192882067,"Ping. How's this coming along? Should I mark it as ""being worked on""? This job will require quite a lot of TPOT runs, so I may take this one on myself because I have access to a HPCC.
",ping coming along mark worked job require quite lot may take one access,issue,negative,neutral,neutral,neutral,neutral,neutral
192881991,"Ping. How's this coming along? Should I mark it as ""being worked on""?
",ping coming along mark worked,issue,negative,neutral,neutral,neutral,neutral,neutral
191755333,"Gotcha. I'm currently doing a comprehensive analysis of TPOT on about 180 data sets. It'll be interesting to see what comes out of that.
",currently comprehensive analysis data interesting see come,issue,positive,positive,positive,positive,positive,positive
191552291,"@rhiever in this case, I think it was like 30 generations (and the best hadn't changed for a big chunk of those).

My hope using TPOT was that it would find something that could match RF's accuracy but have better test-time speed. This result was with a toy dataset though, I'll keep fiddling. =)
",case think like best big chunk hope would find something could match accuracy better speed result toy though keep fiddling,issue,positive,positive,positive,positive,positive,positive
191537947,"Can't believe how easy that looks when it's all said and done, @magsol! Thank you for figuring that out. I added some comments on the PR directly.
",ca believe easy said done thank added directly,issue,positive,positive,positive,positive,positive,positive
191537722,"Cool beans! That's all it took to parallelize DEAP? Very curious to see how effectively it shares memory. I'm rushing against paper deadlines again, but I'm very eager to tinker with this PR to see if we can merge it into TPOT-master.

My main concerns are:
- If we pass a large data set to TPOT, does memory usage explode because it's evaluating many pipelines simultaneously? How effectively does it share memory? (Perhaps hard-code some complicated pipelines and run them in parallel.)
- How well do TPOT pipelines parallelize? Is there a speedup over a non-parallelized version?
- Does parallelizing the pipeline evaluations affect reproducibility? Will there be race conditions that affect reproducibility? (We've tried hard to code the pipeline operators such that they're deterministic, but it's possible parallelization may discover something we missed.)
",cool took parallelize curious see effectively memory rushing paper eager tinker see merge main pas large data set memory usage explode many simultaneously effectively share memory perhaps complicated run parallel well parallelize version pipeline affect reproducibility race affect reproducibility tried hard code pipeline deterministic possible parallelization may discover something,issue,positive,positive,positive,positive,positive,positive
191536107,"Just a small update: The base TPOT benchmark _should_ finish up by the weekend, after which point I'll be able to throw on ""TPOT-Consensus"" and give it a serious spin. Will keep you posted.
",small update base finish weekend point able throw give serious spin keep posted,issue,negative,negative,negative,negative,negative,negative
191525162,"How long did you run it for? Throwing a random forest at it usually comes out on top early on, depending on the data. Random forests became an industry standard for a long time for a reason. :-)
",long run throwing random forest usually come top early depending data random industry standard long time reason,issue,negative,negative,neutral,neutral,negative,negative
191524332,"Ah, thanks for the source pointer, that clarifies things a lot!

But LOL it's a little bit depressing that TPOT came back to me with ""use a random forest with as many trees as you can afford and automatic feature selection"". =D
",ah thanks source pointer lot little bit depressing came back use random forest many afford automatic feature selection,issue,negative,negative,neutral,neutral,negative,negative
191515257,"That's right. The first one is the # of trees, the second one is
max_features: https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L439

On Wed, Mar 2, 2016 at 6:18 PM, Juan Nunez-Iglesias <
notifications@github.com> wrote:

> Ah sure, so those expressions ((41 \* (38 \* 5), (3 - 94)?) evaluate to
> parameters to the sklearn RandomForestClassifier? Which parameters?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/101#issuecomment-191489015.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",right first one second one wed mar wrote ah sure evaluate reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,positive,positive,positive,positive
191491065,"> unless you have a lot of parallelized computation power

Which I do. ;) But I imagine that would require a lot of concerted software engineering effort to get working. Nevertheless, I'm wondering whether there is a better strategy for local TPOT than naive and dramatic subsampling, as I mentioned, such as varying the subsets used by individuals in the populations.
",unless lot computation power imagine would require lot concerted engineering effort get working nevertheless wondering whether better strategy local naive dramatic used,issue,negative,negative,neutral,neutral,negative,negative
191187882,"That's simply because the most trees allowed in a TPOT RF is 500. So when
the mathematical expression evaluates to >500, we clamp it to 500.

On Wednesday, March 2, 2016, Juan Nunez-Iglesias notifications@github.com
wrote:

> Just a quick (I hope) question. When I interrupt TPOT training, I get the
> following line output:
> 
> Best pipeline: _random_forest(ARG0, mul(41, mul(38, 5)), sub(3, 94))
> 
> But when I export the pipeline, I just get a simple RF with 500
> estimators. Is this expected? What's the format of these one-line summaries?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/101.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",simply mathematical expression clamp march wrote quick hope question interrupt training get following line output best pipeline sub export pipeline get simple format reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
191187359,"4M rows is definitely too large for TPOT right now until we figure out a
method like the one discussed in this issue for training on subsets. It
would even take a long time to train one _model_ on 4M rows, so you can
imagine that a tool that trains many pipelines would be very slow on 4M
rows.

One option for you is to try to reduce the number of rows by removing
duplicates and other data reduction methods. Otherwise any sort of model or
pipeline optimization technique will not really be feasible for your data
set unless you have a lot of parallelized computation power.

On Wednesday, March 2, 2016, Juan Nunez-Iglesias notifications@github.com
wrote:

> Would it make sense to train/evaluate each individual in the population
> with a different subset of a large dataset every time? I have a dataset of
> ~4M rows and TPOT appears to be kinda useless in this scenario... =)
> 
> Sorry if my question is naive; I don't have much experience with
> evolutionary algorithms.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/87#issuecomment-191098163.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",definitely large right figure method like one issue training would even take long time train one imagine tool many would slow one option try reduce number removing data reduction otherwise sort model pipeline optimization technique really feasible data set unless lot computation power march wrote would make sense individual population different subset large every time useless scenario sorry question naive much experience evolutionary reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,neutral,neutral,positive,positive
191098163,"Would it make sense to train/evaluate each individual in the population with a different subset of a large dataset every time? I have a dataset of ~4M rows and TPOT appears to be kinda useless in this scenario... =)

Sorry if my question is naive; I don't have much experience with evolutionary algorithms.
",would make sense individual population different subset large every time useless scenario sorry question naive much experience evolutionary,issue,negative,negative,negative,negative,negative,negative
190911586,"_phew_ Progress! Likely need to move `pareto_eq` outside of the `fit()` function, then see what else @bartleyn did after that.
",progress likely need move outside fit function see else,issue,positive,positive,positive,positive,positive,positive
190902873,"Yep; upgraded. Am now getting the same sequence of errors as @bartleyn (with `pareto_eq` inside of `fit` and as a class function).
",yep getting sequence inside fit class function,issue,positive,positive,positive,positive,positive,positive
190893756,"That's what it currently is; if I remove the prepended `.` I get a different error:

```
/opt/python/bin/python: Error while finding spec for 'tpot.tpot' (<class 'ImportError'>: No module named '_version')
```

**EDIT** Nevermind; stupid mistake on my part. Ignore this post.
",currently remove get different error error finding spec class module edit stupid mistake part ignore post,issue,negative,negative,negative,negative,negative,negative
190892065,"Change the code in `tpot.py` back to saying

``` python
from ._version import __version__
```
",change code back saying python import,issue,negative,neutral,neutral,neutral,neutral,neutral
190873408,"@rhiever This is just plain bizarre:

```
$> python -m tpot.tpot dataset.csv -v 2
Version 0.2.7 of tpot is outdated. Version 0.2.8 was released 2 days ago.
Traceback (most recent call last):
  File ""/opt/python/lib/python3.5/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/opt/python/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/tpot/tpot/tpot.py"", line 1235, in <module>
    main()
  File ""/tpot/tpot/tpot.py"", line 1114, in main
    from _version import __version__
ImportError: No module named '_version'
```

How is it not recognizing the `_version` module, and yet the version module is spitting out the fact that it's out of date?
",plain bizarre python version outdated version day ago recent call last file line file line code file line module main file line main import module module yet version module spitting fact date,issue,negative,positive,neutral,neutral,positive,positive
190507214,"@magsol: You won't need to change the export_utils import if you call TPOT as `python -m tpot.tpot` at the base directory of the repo, e.g.,

```
cd tpot-master
python -m tpot.tpot dataset.csv -v 2
```
",wo need change import call python base directory python,issue,negative,negative,negative,negative,negative,negative
190478963,"I ended up getting that error as well. I bet you that I have similarly structured code. I've tried a few different iterations on where I instantiate the pool object (inside the class, etc). I've also tried messing around with the TPOT object's dictionary to see if we can ignore pickling the pool object (using the __get/set_state__ functions), but I think I'm barking up the wrong tree, as the TPOT object isn't the object getting pickled. 
",ended getting error well bet similarly structured code tried different pool object inside class also tried messing around object dictionary see ignore pool object think barking wrong tree object object getting,issue,negative,negative,negative,negative,negative,negative
190475648,"@bartleyn how are you structuring your code, i.e. the addition of the `Pool.map` function to the TPOT toolbox? In my case, it's complaining that the pool object I'm using is attempting to pickle itself.
",code addition function toolbox case pool object pickle,issue,negative,neutral,neutral,neutral,neutral,neutral
190430375,"No; changed the one import to `from export_utils import *` so it wasn't a relative import anymore, and then running it directly as `python tpot.py`. Should I do it some other way?
",one import import relative import running directly python way,issue,negative,positive,neutral,neutral,positive,positive
190429191,"How are you running it? `python -m tpot.tpot` out of the main `tpot` repo directory?
",running python main directory,issue,negative,positive,positive,positive,positive,positive
190427015,"Strange; I'm getting a slightly different error (might be because I'm running `tpot.py` directly, instead of importing it into an external script).

```
Traceback (most recent call last):
  File ""tpot.py"", line 1235, in <module>
    main()
  File ""tpot.py"", line 1224, in main
    tpot.fit(training_features, training_classes)
  File ""tpot.py"", line 247, in fit
    stats=stats, halloffame=self.hof, verbose=verbose)
  File ""/opt/python/lib/python3.5/site-packages/deap/algorithms.py"", line 147, in eaSimple
    fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
  File ""/opt/python/lib/python3.5/multiprocessing/pool.py"", line 260, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""/opt/python/lib/python3.5/multiprocessing/pool.py"", line 608, in get
    raise self._value
  File ""/opt/python/lib/python3.5/multiprocessing/pool.py"", line 385, in _handle_tasks
    put(task)
  File ""/opt/python/lib/python3.5/multiprocessing/connection.py"", line 206, in send
    self._send_bytes(ForkingPickler.dumps(obj))
  File ""/opt/python/lib/python3.5/multiprocessing/reduction.py"", line 50, in dumps
    cls(buf, protocol).dump(obj)
  File ""/opt/python/lib/python3.5/multiprocessing/pool.py"", line 492, in __reduce__
    'pool objects cannot be passed between processes or pickled'
NotImplementedError: pool objects cannot be passed between processes or pickled
```
",strange getting slightly different error might running directly instead external script recent call last file line module main file line main file line fit file line file line map return iterable file line get raise file line put task file line send file line protocol file line pool,issue,negative,positive,neutral,neutral,positive,positive
189925944,"I'll do a set of runs on the MNIST data and report some stats on the performance and appearance of consensus operators compared to others.
",set data report performance appearance consensus,issue,negative,neutral,neutral,neutral,neutral,neutral
189913025,"@rhiever Agreed, I like the thought process. Will look into datacleaner and discuss with you once I get some airtime! BTW did you have a look at https://github.com/wdm0006/categorical_encoding? 
",agreed like thought process look discus get look,issue,positive,neutral,neutral,neutral,neutral,neutral
189893605,"Try moving the `pareto_eq` function out of `fit()` and make it a TPOT class function. Will it work then?
",try moving function fit make class function work,issue,negative,positive,positive,positive,positive,positive
189893544,"Integration tests (i.e., running TPOT on a fixed data set with a fixed RNG
seed for a fixed number of generations and checking the output) are always
useful, but I;m not sure if we can do that here.

On Sun, Feb 28, 2016 at 10:48 AM, Nathan notifications@github.com wrote:

> With the above commits I've made the necessary changes to run all the
> tests in tests.py, and tested functionality with some small examples all
> within a Python 3.5 environment. Are there any tests I'm missing?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/96#issuecomment-189893324.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",integration running fixed data set fixed seed fixed number output always useful sure sun wrote made necessary run tested functionality small within python environment missing reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,neutral,neutral,positive,positive
189893464,"I dug around and found [this](http://deap.readthedocs.org/en/master/tutorials/basic/part4.html). It seems all we SHOULD need to do is replace the toolbox's map function with one that interfaces with some parallel lib. I've run it once with multiprocessing but get the following:

> Traceback (most recent call last):
>   File ""test_tpot.py"", line 10, in <module>
>     tpot.fit(X_train, y_train)
>   File ""/Users/Bartley/Documents/personal_dev/tpot/tpot/tpot/tpot.py"", line 256, in fit
>     stats=stats, halloffame=self.hof, verbose=verbose)
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/site-packages/deap-1.1.0-py3.5-macosx-10.5-x86_64.egg/deap/algorithms.py"", line 149, in eaSimple
>     fitnesses = toolbox.map(toolbox.evaluate, invalid_ind)
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/multiprocessing/pool.py"", line 260, in map
>     return self._map_async(func, iterable, mapstar, chunksize).get()
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/multiprocessing/pool.py"", line 608, in get
>     raise self._value
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/multiprocessing/pool.py"", line 385, in _handle_tasks
>     put(task)
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/multiprocessing/connection.py"", line 206, in send
>     self._send_bytes(ForkingPickler.dumps(obj))
>   File ""/Users/Bartley/anaconda/envs/py35/lib/python3.5/multiprocessing/reduction.py"", line 50, in dumps
>     cls(buf, protocol).dump(obj)
> AttributeError: Can't pickle local object 'TPOT.fit.<locals>.pareto_eq'
",dug around found need replace toolbox map function one parallel run get following recent call last file line module file line fit file line file line map return iterable file line get raise file line put task file line send file line protocol ca pickle local object,issue,negative,positive,neutral,neutral,positive,positive
189893324,"With the above commits I've made the necessary changes to run all the tests in tests.py, and tested functionality with some small examples all within a Python 3.5 environment. Are there any tests I'm missing?
",made necessary run tested functionality small within python environment missing,issue,negative,negative,negative,negative,negative,negative
189892051,"Thank you again for these suggestions! I do like some of them (as noted in the line-by-line comments), but I err toward a longer and more explicit way of writing code to make it easier to read.
",thank like noted err toward longer explicit way writing code make easier read,issue,positive,neutral,neutral,neutral,neutral,neutral
189783180,"Isn't DEAP supposed to provide some support for parallelizing the evaluation? Or does it make copies of the data set?
",supposed provide support evaluation make data set,issue,negative,neutral,neutral,neutral,neutral,neutral
189714063,"I'm out and about right now, but I wouldn't be surprised if I was testing with older tests. I'll test again when I get back.
",right would testing older test get back,issue,negative,positive,positive,positive,positive,positive
189675533,"Tch tch tch... join us in ze future! :+1: 
",tch tch tch join u future,issue,negative,neutral,neutral,neutral,neutral,neutral
189675446,"As of now, I'm writing it to handle my daily cleaning needs. So:
- Encode categorical features/classes/strings as numerals
- Impute or drop NaNs (which one is determined by a setting; imputes w/ median by default)

From there, I'll likely raid the sklearn preprocessing module and a couple other packages I know of to find some other common uses.

Feedback is of course welcomed! Feel free to file an issue on the datacleaner repo.
",writing handle daily cleaning need encode categorical impute drop one determined setting median default likely raid module couple know find common feedback course feel free file issue,issue,positive,positive,neutral,neutral,positive,positive
189673555,"Whoops, that's what I get for being stuck in the 2.7 past. 
",whoop get stuck past,issue,negative,negative,negative,negative,negative,negative
189672766,"Looks like your tests are having some issues with Python 3. I think it's because you're using Python 2 `print` statements. :-1: ;-)
",like python think python print,issue,negative,neutral,neutral,neutral,neutral,neutral
189672682,"Sounds promising! I look forward to benchmarking the code then.

It may take a while to get to the benchmark, though. Just a heads up.
",promising look forward code may take get though,issue,negative,positive,positive,positive,positive,positive
189671189,"@pronojitsaha, I'm going to close this issue and scrap the feature. Now that I think about it, with all the conversations going on in this thread, I think that this convenience function would unnecessarily bloat the code while we try to accommodate so many different potential inputs. TPOT is meant to be a pipeline optimizer, not a data cleaner, so this convenience function would undoubtedly be scope creep.

Instead, I think we should clearly state the expected format of the input and throw an exception if those expectations are violated. This is what's done in sklearn, and it's the more Pythonic way of handling inputs.

However, I do believe that automatically cleaning data is valuable, so I've created the [datacleaner](https://github.com/rhiever/datacleaner) project to build a separate tool for that functionality. Please feel free to direct your contributions there if you've worked on this problem. Ultimately, I think having the two tasks as separate tools is better software design, especially if we follow Doug McIlroy's [teachings](http://www.catb.org/esr/writings/taoup/html/ch01s06.html):

> **(i) Make each program do one thing well.** To do a new job, build afresh rather than complicate old programs by adding new features.
",going close issue scrap feature think going thread think convenience function would unnecessarily bloat code try accommodate many different potential meant pipeline data cleaner convenience function would undoubtedly scope creep instead think clearly state format input throw exception done pythonic way handling however believe automatically cleaning data valuable project build separate tool functionality please feel free direct worked problem ultimately think two separate better design especially follow make program one thing well new job build afresh rather complicate old new,issue,positive,positive,positive,positive,positive,positive
189668062,"Yeah I ran numerous small tests that ended up with consensus in the pipeline. Performed well, but tough to compare since some of the other runs ended up with (presumably) overfit simple pipelines with perfect accuracy.
",yeah ran numerous small ended consensus pipeline well tough compare since ended presumably overfit simple perfect accuracy,issue,positive,positive,neutral,neutral,positive,positive
189664970,"Happy to see this PR come in! Were the consensus operators used in any of your tests? I'm currently running a big TPOT benchmark on the cluster, but I'll line this PR up for the next benchmark in line.
",happy see come consensus used currently running big cluster line next line,issue,positive,positive,positive,positive,positive,positive
187976067,"See #84. Likely a similar issue.
",see likely similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
186344234,"SGTM. I've been running the same thing but on Python 3 and haven't run into
any problems yet (it's already run for twice as long). Will update on 85 if
I hit a roadblock. Thanks!
On Fri, Feb 19, 2016 at 12:49 Randy Olson notifications@github.com wrote:

> Since #85 https://github.com/rhiever/tpot/issues/85 should address this
> issue, I'm going to close this one. Please feel free to reopen this one if
> you need further assistance.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/84#issuecomment-186328011.
> 
> ## 
> 
> iPhone'd
",running thing python run yet already run twice long update hit roadblock thanks randy wrote since address issue going close one please feel free reopen one need assistance reply directly view,issue,positive,positive,positive,positive,positive,positive
186328011,"Since #85 should address this issue, I'm going to close this one. Please feel free to reopen this one if you need further assistance.
",since address issue going close one please feel free reopen one need assistance,issue,positive,positive,positive,positive,positive,positive
186326216,"I think we should simply remove the sklearn GBC entry and make a new entry for XGBoost. You can likely use the sklearn GBC entry as a template so the changes will be minimal.
",think simply remove entry make new entry likely use entry template minimal,issue,negative,positive,neutral,neutral,positive,positive
186326013,"@tcfuji, when you get a moment, can you please submit a separate PR to update the docs? Currently, they say we still use the [sklearn GBC](http://rhiever.github.io/tpot/reference/pipeline_operators/models/classifiers/ensemble/GradientBoostingClassifier/). We need to update that.

The docs are located [here](https://github.com/rhiever/tpot/tree/master/docs/sources).

You can view the docs as you change them by running `mkdocs serve` in the `docs` directory. (This will require you to `pip install mkdocs` if you haven't already.)
",get moment please submit separate update currently say still use need update view change running serve directory require pip install already,issue,negative,neutral,neutral,neutral,neutral,neutral
186270585,"Working through PRs this morning. I'll merge and rerun it. Thanks again @pronojitsaha!
",working morning merge rerun thanks,issue,negative,positive,positive,positive,positive,positive
186242723,"Thank you for writing this up, Ying. This is definitely an idea that we'll have to experiment with.
",thank writing definitely idea experiment,issue,positive,neutral,neutral,neutral,neutral,neutral
186203572,"Interesting. Is that even with Pareto optimization (as implemented in the
latest version of TPOT)? We should video chat to discuss what's going on.

On Thu, Feb 18, 2016 at 11:31 PM, Nathan notifications@github.com wrote:

> Sure, if you'd like. I've got most of the logic in for the Consensus
> pipeline operator(s), but I'm getting some major memory blowups (which was
> to be expected to some degree). I've taken the approach to allow for
> weighting each DataFrame by some metric (accuracy of the guesses, uniform
> weights, etc) before combining them with some evolvable method (max, mean,
> etc).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/77#issuecomment-186045683.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",interesting even optimization latest version video chat discus going wrote sure like got logic consensus pipeline operator getting major memory degree taken approach allow weighting metric accuracy uniform combining evolvable method mean reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
186045683,"Sure, if you'd like. I've got most of the logic in for the Consensus pipeline operator(s), but I'm getting some major memory blowups (which I suppose was to be expected to some degree). I've taken the approach to allow for weighting each DataFrame by some metric (accuracy of the guesses, uniform weights, etc) before combining them with some evolvable method (max, mean, etc). 
",sure like got logic consensus pipeline operator getting major memory suppose degree taken approach allow weighting metric accuracy uniform combining evolvable method mean,issue,positive,positive,neutral,neutral,positive,positive
185817035,"It will probably be worth rerunning and recommitting the notebook with the
latest version of TPOT. Quite a bit has changed with the addition of Pareto
optimization. Please ping me when that is done and I will take a closer
look.

On Thu, Feb 18, 2016 at 11:15 AM, PRONOjit Saha notifications@github.com
wrote:

> Hi @rhiever https://github.com/rhiever , would you want me to change
> something for this PR?
> 
> Also I was thinking of digging deeper into #61
> https://github.com/rhiever/tpot/issues/61 , is that ok?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/pull/71#issuecomment-185796612.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",probably worth notebook latest version quite bit addition optimization please ping done take closer look wrote hi would want change something also thinking digging reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
185796612,"Hi @rhiever , would you want me to change something for this PR? 

Also I was thinking of digging deeper into #61 , is that ok?
",hi would want change something also thinking digging,issue,negative,neutral,neutral,neutral,neutral,neutral
185761552,"Interesting. TPOT must have been performing some feature transformations
before passing them to the classifier.

On Thursday, February 18, 2016, Shannon notifications@github.com wrote:

> That seems to work just fine; it spits out an accuracy with no problems
> (in both Python 2 and 3).
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/84#issuecomment-185749319.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",interesting must feature passing classifier wrote work fine accuracy python reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
185749319,"That seems to work just fine; it spits out an accuracy with no problems (in both Python 2 and 3).
",work fine accuracy python,issue,negative,positive,positive,positive,positive,positive
185744696,"Well, I always recommend upgrading to Python 3. ;-)

Try running this code on your data set and see if it generates an error:

``` python
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.cross_validation import train_test_split

X_train, X_test, y_train, y_test = train_test_split(my_data_features, my_data_targets,
                                                    train_size=0.75, test_size=0.25)
clf = GradientBoostingClassifier()
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))
```
",well always recommend python try running code data set see error python import import print,issue,negative,neutral,neutral,neutral,neutral,neutral
185743841,"@tcfuji: Will review this tomorrow. Looks good from a high-level glance.
",review tomorrow good glance,issue,negative,positive,positive,positive,positive,positive
185743664,"Hopefully in a few months the paper I'm working on with this dataset will be published and I can publish the dataset as well ;)

I noticed this bug pops up a lot across Python 2 installs (since iterators are lists, rather than generators). Would switching to Python 3 be a potential fix?
",hopefully paper working publish well bug lot across python since rather would switching python potential fix,issue,positive,neutral,neutral,neutral,neutral,neutral
185743276,"Interesting... this appears to be an issue with sklearn's `GradientBoostingClassifier`, or more specifically, with the decision trees that it constructs during gradient tree boosting.

Is the data set you're using publicly available (or can it be)? I'd be interested to see if this bug is reproducible on a simple `GradientBoostingClassifier` call.

Regardless, I think your issue raises a broader point: We should put exception handling around the pipeline evaluation function so all of TPOT doesn't crash when one invalid or faulty pipeline pops up. I've filed this issue as a bug in #85.
",interesting issue specifically decision gradient tree data set publicly available interested see bug reproducible simple call regardless think issue point put exception handling around pipeline evaluation function crash one invalid faulty pipeline issue bug,issue,negative,positive,positive,positive,positive,positive
185526442,"To each their own. If you look at the examples above, XGBoost is very easy to use. Same interface as sklearn models. I hope the sklearn team will integrate XGBoost soon.
",look easy use interface hope team integrate soon,issue,positive,positive,positive,positive,positive,positive
185525531,"You are welcome! I dunno, someday maybe, but I feel like XGBoost is more of a ML competition / Kaggle thing :P . Currently, I am more excited about toying around with TensorFlow :)
",welcome someday maybe feel like competition thing currently excited toying around,issue,positive,positive,positive,positive,positive,positive
185525160,"Thanks @rasbt! You should give XGBoost a try. Even without OpenMP, it seems to be streets ahead of regular gradient boosting.
",thanks give try even without street ahead regular gradient,issue,negative,positive,neutral,neutral,positive,positive
185524946,"I am not using boosting and don't know if XGBoost has any other dependencies. However, if it can be installed via `pip install xgboost` then the steps Randy outlined above are the way to go. Maybe set up a fresh virtual environment with numpy, scipy, and scikit-learn installed and test if `pip install xgboost` is sufficient or if you need something like `pip install xgboost -r requirements.txt`
",know however via pip install randy outlined way go maybe set fresh virtual environment test pip install sufficient need something like pip install,issue,positive,positive,positive,positive,positive,positive
185476659,"I believe you need to do the following. @rasbt can confirm.
- [ ] Add the required version of XGBoost to the lines around https://github.com/rhiever/tpot/blob/master/.travis.yml#L7
- [ ] pip install XGBoost similar to how DEAP is installed here: https://github.com/rhiever/tpot/blob/master/ci/.travis_install.sh#L46
- [ ] print the XGBoost version similar to how the others are printed here: https://github.com/rhiever/tpot/blob/master/ci/.travis_install.sh#L58
- [ ] Also print the XGBoost version similarly here: https://github.com/rhiever/tpot/blob/master/ci/.travis_test.sh#L17
",believe need following confirm add version around pip install similar print version similar printed also print version similarly,issue,negative,neutral,neutral,neutral,neutral,neutral
185467161,"@rhiever I'm not too familiar with travis. Should I just add `pip install xgboost==$XGBOOST_VERSION` to `.travis_install`?
",familiar travis add pip install,issue,negative,positive,positive,positive,positive,positive
185380984,"Hey @bartleyn, I wanted to check in to see how this issue is coming along. Want to video chat about it?
",hey check see issue coming along want video chat,issue,negative,neutral,neutral,neutral,neutral,neutral
185380814,"Looks good to me. Just tried running the benchmarks myself and XGBoost looks like a solid improvement over the GradientBoostingClassifier. Easy to pip install too. Go ahead and put together a PR to replace the GradientBoostingClassifier with XGBoost.

Thanks for looking into this, @tcfuji.
",good tried running like solid improvement easy pip install go ahead put together replace thanks looking,issue,positive,positive,positive,positive,positive,positive
185371118,"Just ran the same code without OpenMP. As expected, not as fast but still consistently faster (about 2x to 4x) than scikit's GradientBoostingClassifier. 

If we can make OpenMP an optional dependency, I think xgboost would be a great addition. 
",ran code without fast still consistently faster make optional dependency think would great addition,issue,positive,positive,positive,positive,positive,positive
184942566,"How do the benchmarks look without OpenMP?

On Tuesday, February 16, 2016, Ted notifications@github.com wrote:

> According to the xgboost docs (
> https://xgboost.readthedocs.org/en/latest/build.html), it does not appear
> to be a hard dependency.
> 
> On Tue, Feb 16, 2016 at 6:27 PM Randy Olson <notifications@github.com
> <javascript:_e(%7B%7D,'cvml','notifications@github.com');>>
> wrote:
> 
> > Thank you for running these benchmarks, @tcfuji
> > https://github.com/tcfuji! Is that a hard dependency on OpenMP, or is
> > it optional? I'm concerned that making OpenMP a requirement for TPOT
> > would
> > cut down on its potential user base pretty significantly.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/rhiever/tpot/issues/81#issuecomment-184918637.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/81#issuecomment-184923018.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",look without ted wrote according appear hard dependency tue randy wrote thank running hard dependency optional concerned making requirement would cut potential user base pretty significantly reply directly view reply directly view postdoctoral researcher institute university twitter,issue,negative,negative,neutral,neutral,negative,negative
184923018,"According to the xgboost docs (
https://xgboost.readthedocs.org/en/latest/build.html), it does not appear
to be a hard dependency.

On Tue, Feb 16, 2016 at 6:27 PM Randy Olson notifications@github.com
wrote:

> Thank you for running these benchmarks, @tcfuji
> https://github.com/tcfuji! Is that a hard dependency on OpenMP, or is
> it optional? I'm concerned that making OpenMP a requirement for TPOT would
> cut down on its potential user base pretty significantly.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/81#issuecomment-184918637.
",according appear hard dependency tue randy wrote thank running hard dependency optional concerned making requirement would cut potential user base pretty significantly reply directly view,issue,negative,negative,neutral,neutral,negative,negative
184918637,"Thank you for running these benchmarks, @tcfuji! Is that a hard dependency on OpenMP, or is it optional? I'm concerned that making OpenMP a requirement for TPOT would cut down on its potential user base pretty significantly.
",thank running hard dependency optional concerned making requirement would cut potential user base pretty significantly,issue,negative,negative,neutral,neutral,negative,negative
184915359,"@rhiever As we discussed yesterday, you wanted me to evaluate the performance of xgboost itself, not my fork.

The results were better than I expected:

``` python
from sklearn.datasets import load_digits, make_classification
from sklearn.cross_validation import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from time import perf_counter
import numpy as np

gb = GradientBoostingClassifier()
xgb = XGBClassifier()
```

MNIST:

``` python
digits = load_digits()
X_train_digit, X_test_digit, y_train_digit, y_test_digit = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25)

start = perf_counter()
gb.fit(X_train_digit, y_train_digit)
print(gb.score(X_test_digit, y_test_digit))
print(""%f seconds"" % (perf_counter() - start))
```

0.957777777778
6.918697 seconds

``` python
start = perf_counter()
xgb.fit(X_train_digit, y_train_digit)
print(np.mean(xgb.predict(X_test_digit) == y_test_digit))
print(""%f seconds"" % (perf_counter() - start))
```

0.955555555556
1.720479 seconds

---

Using the [make_classification](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) method:

``` python
X, y = make_classification(n_samples=10000, n_features=500, n_informative=10, n_classes=10)
X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y,
                                                    train_size=0.7, test_size=0.3)
start = perf_counter()
gb.fit(X_train_mc, y_train_mc)
print(gb.score(X_test_mc, y_test_mc))
print(""%f seconds"" % (perf_counter() - start))
```

0.494
763.447380 seconds

``` python
start = perf_counter()
xgb.fit(X_train_mc, y_train_mc)
print(np.mean(xgb.predict(X_test_mc) == y_test_mc))
print(""%f seconds"" % (perf_counter() - start))
```

0.513
52.425525 seconds

With a few other variations of make_classification (changing the parameters), xgboost consistently performed about 14x faster than scikit GB.  

One caveat is that this speed increase is likely due to OpenMP. I think people with Linux and OS X (by running `brew install gcc --without-multilib`) this shouldn't be a problem, but it's still another dependency.
",yesterday evaluate performance fork better python import import import import time import import python start print print start python start print print start method python start print print start python start print print start consistently faster one caveat speed increase likely due think people o running brew install problem still another dependency,issue,negative,positive,positive,positive,positive,positive
181978116,"@bartleyn As Randy mentioned, the xgboost python API makes it easy since it can construct its main data structure (DMatrix) from numpy arrays. Also, the `tpot` method `_train_model_and_predict` converts the pandas dataframe inputs into numpy arrays (using the `.values` method).

@rhiever Sure, I'll work on it over the weekend. Want something similar to `tutorials/IRIS.ipynb` and `tutorials/MNIST.ipynb` while keeping track of the training time?
",randy python easy since construct main data structure also method method sure work weekend want something similar keeping track training time,issue,positive,positive,positive,positive,positive,positive
181902071,"@tcfuji: If it works better than sklearn's GradientBoostingClassifier, isn't incredibly slow (in comparison), and the XGBoost library isn't a pain to integrate with, then I'm not opposed to integrating XGBoost into TPOT. Are you free to do a small benchmark on, say, [MNIST](http://yann.lecun.com/exdb/mnist/) or [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)? I'd be interested to see performance in terms of accuracy and training time.

@bartleyn: From my readings, the Python implementation of XGBoost has the exact same interface as all other sklearn classifiers. I don't think that would be a difficulty.
",work better incredibly slow comparison library pain integrate opposed free small say interested see performance accuracy training time python implementation exact interface think would difficulty,issue,negative,positive,positive,positive,positive,positive
181690119,"Besides being highly optimized like tcfuji mentioned, I understand it also has the ability to be trained in a distributed fashion. Would it easily interface with pandas though?
",besides highly like understand also ability trained distributed fashion would easily interface though,issue,positive,positive,positive,positive,positive,positive
181682868,"It's mostly just a faster version of GradientBoostingClassifier:
http://auduno.com/post/96084011658/some-nice-ml-libraries

However, it's also mentioned in a number of Kaggle winning solutions
because Gradient Boosting apparently seems to do quite well in those
competitions:
1.
https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov
2. https://github.com/dmlc/xgboost/tree/master/demo/kaggle-higgs
3. https://github.com/daxiongshu/kaggle-tradeshift-winning-solution
4.
http://blog.kaggle.com/2015/12/21/rossmann-store-sales-winners-interview-1st-place-gert/
5.
http://blog.kaggle.com/2015/12/03/dato-winners-interview-1st-place-mad-professors/

(Just found out it has its own tag on the Kaggle blog):
http://blog.kaggle.com/tag/xgboost/

On Mon, Feb 8, 2016 at 9:11 PM Randy Olson notifications@github.com wrote:

> I've been looking into XGBoost and I'm trying to understand what it adds
> over sklearn's implementation of GradientBoostingClassifier. Do you know?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/81#issuecomment-181671278.
",mostly faster version however also number winning gradient apparently quite well found tag mon randy wrote looking trying understand implementation know reply directly view,issue,positive,positive,positive,positive,positive,positive
181671278,"I've been looking into XGBoost and I'm trying to understand what it adds over sklearn's implementation of GradientBoostingClassifier. Do you know?
",looking trying understand implementation know,issue,negative,neutral,neutral,neutral,neutral,neutral
181540169,"@dmarx: Okay, now is probably the best time to try to merge this before I move on to the next paper. You have bandwidth to try to link it with the current TPOT version?
",probably best time try merge move next paper try link current version,issue,positive,positive,positive,positive,positive,positive
181119500,":+1: Looking forward to seeing what we can do with this idea.

On Sunday, February 7, 2016, Nathan notifications@github.com wrote:

> Agreed. I was actually thinking about a similar ensemble approach to see
> if it helps address overfitting even more than we have in #64
> https://github.com/rhiever/tpot/issues/64, so I wonder if we should
> just implement a set number of Consensus operators for now and flesh out
> this ensemble approach elsewhere.
> 
> As for how to meaningfully combine the classifications, I bet we can take
> inspiration from some meta-learning algorithms like AdaBoost. I'll look
> into it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/77#issuecomment-181118625.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",looking forward seeing idea wrote agreed actually thinking similar ensemble approach see address even wonder implement set number consensus flesh ensemble approach elsewhere meaningfully combine bet take inspiration like look reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
181118625,"Agreed. I was actually thinking about a similar ensemble approach to see if it helps address overfitting even more than we have in #64, so I wonder if we should just implement a set number of Consensus operators for now and flesh out this ensemble approach elsewhere.

As for how to meaningfully combine the classifications in addition to what you mentioned, I bet we can take inspiration from some meta-learning algorithms like AdaBoost. I'll look into it. 
",agreed actually thinking similar ensemble approach see address even wonder implement set number consensus flesh ensemble approach elsewhere meaningfully combine addition bet take inspiration like look,issue,positive,positive,positive,positive,positive,positive
181109810,"Right. It's not 100% clear how large the ensembles should be.

The population ensemble approach would require a custom version of eaSimple
because the population is evaluated together. Probably worth looking into
learning classifier systems (specifically, Michigan style LCS) for
inspiration on that end.

In the near future, I think the former approach is more promising. Just
need to make sure that evolution can actually make use of those Consensus
operators.

On Sunday, February 7, 2016, Nathan notifications@github.com wrote:

> I suppose that we could reliably constrain the total number of pipelines
> being combined together, as it's not like we'll be combining hundreds of
> pipelines.
> 
> As for the ensemble approach, would it be as simple as adding a parameter,
> or would we have to roll our own version of eaSimple?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/77#issuecomment-181103327.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",right clear large population ensemble approach would require custom version population together probably worth looking learning classifier specifically michigan style inspiration end near future think former approach promising need make sure evolution actually make use consensus wrote suppose could reliably constrain total number combined together like combining ensemble approach would simple parameter would roll version reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
181103327,"I suppose that we could reliably constrain the total number of pipelines being combined together, as it's not like we'll be combining hundreds of pipelines. 

As for the ensemble approach, would it be as simple as adding a parameter, or would we have to roll our own version of eaSimple?
",suppose could reliably constrain total number combined together like combining ensemble approach would simple parameter would roll version,issue,negative,neutral,neutral,neutral,neutral,neutral
181063856,"That seems to be one way to do it, but I think it would balloon the number of operators and make it more difficult for evolution to work with. I suspect the ""best"" way to do this is to add a bunch of Consensus operators with increasing numbers of DataFrames as input.

Alternatively, we can think of the GP population of pipelines as the ensemble and add an evolvable parameter to allow evolution to pick the best way to combine their classifications.
",one way think would balloon number make difficult evolution work suspect best way add bunch consensus increasing input alternatively think population ensemble add evolvable parameter allow evolution pick best way combine,issue,positive,positive,positive,positive,positive,positive
180448235,"Would it be naive to implement two additional 'helper' operators alongside the consensus one, one of type [list, DataFrame] -> [list] and the other of type [DataFrame, DataFrame] -> [list]? It would balloon the number of operators, but might give us that flexibility. 
",would naive implement two additional alongside consensus one one type list list type list would balloon number might give u flexibility,issue,negative,negative,negative,negative,negative,negative
180398160,"What about hyperparameters that have a mixture of string and integer values? I think that's why I used `if` statements to handle string hyperparameters -- many of them have a mixture.

> I was thinking about adding a couple more classifiers

What classifiers are you thinking of? :-)
",mixture string integer think used handle string many mixture thinking couple thinking,issue,negative,positive,positive,positive,positive,positive
180396334,"I'll change this to an enhancement and modify the title correspondingly. Cheers!
",change enhancement modify title correspondingly,issue,negative,neutral,neutral,neutral,neutral,neutral
179978959,"I'm pretty sure it would allow lists as an input, but how could we then make an easy-to-evolve list of pipelines to pass to it?
",pretty sure would allow input could make list pas,issue,positive,positive,positive,positive,positive,positive
179942457,"I think the primary constraint is whether or not DEAP's `PrimitiveSetTyped` will allow for operators that take in iterables as parameters, no? I can dig around to see if there's something obvious we're missing.
",think primary constraint whether allow take dig around see something obvious missing,issue,negative,positive,neutral,neutral,positive,positive
179749149,"I was thinking about adding a couple more classifiers and I have a small suggestion for string valued hyperparameters. For each string valued hyperparameter we could simple pass an int value that we can use to index into the list of possible choices defined in the pipeline operator for the particular classifier e.g.:  for RandomForest something like:

```
max_features = [""auto"", ""log2"", None][index1]
class_weight   = [""balanced"", ""balanced_subsample""][index2]
```

The search space over the number of max_features is in 1 < x <= n_features , but using this simple indexing into possible options at least we can try out reasonable options. Same goes for class_weight.
",thinking couple small suggestion string valued string valued could simple pas value use index list possible defined pipeline operator particular classifier something like auto log none index balanced index search space number simple indexing possible least try reasonable go,issue,positive,negative,neutral,neutral,negative,negative
179736469,"I'm not sure if this is helpful but in sklearn the VotingClassifier takes an arbitrary number of estimators as input and returns a single estimator.

http://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#example-ensemble-plot-voting-decision-regions-py
",sure helpful arbitrary number input single estimator,issue,positive,positive,positive,positive,positive,positive
179715647,"@rhiever I think this is a fair answer and you should close this issue/question. It would be good to add your comment to the introduction/readme/notes of the repository.
",think fair answer close would good add comment repository,issue,positive,positive,positive,positive,positive,positive
179241343,"It's not immediately clear to me how to create a pipeline operator that takes an arbitrary number of DataFrames. I think this would be a non-trivial task, at least looking at it at a high level. It may be necessary to implement multiple versions of the Consensus operator that take a varying number of DataFrames as input.
",immediately clear create pipeline operator arbitrary number think would task least looking high level may necessary implement multiple consensus operator take number input,issue,positive,negative,neutral,neutral,negative,negative
175198043,"I believe the issue was a data shape issue. The predictors should be an array of arrays, whereas the outcomes should be an array of numbers.

I corrected the code below and it seems to work fine.

``` python
## Now TPOT
import pandas as pd
from numpy import array
from tpot import TPOT  
from sklearn.cross_validation import train_test_split  

# Import data
Data = pd.DataFrame.from_csv(""Trial.txt"",sep='\t',index_col=0)
Data=Data.ix[:,0:40]
Data=Data.dropna()
Data = Data.reset_index()

# I changed the two lines below this comment
Outcome=Data[""Population""].values
Predictors=Data.ix[:,4:13].values

# Make sure to specify test_size also
X_train, X_test, y_train, y_test = train_test_split(Predictors, Outcome,  
                                                    train_size=0.75,
                                                    test_size=0.25)

tpot = TPOT(generations=5)  
tpot.fit(X_train, y_train)  
tpot.score(X_train, y_train, X_test, y_test)
```
",believe issue data shape issue array whereas array corrected code work fine python import import array import import import data data data two comment population make sure specify also outcome,issue,negative,positive,positive,positive,positive,positive
174384016,"PR #71 shows the implementation using the Titanic dataset. Basically dropped the values which do not appear in the training set. 
",implementation titanic basically appear training set,issue,negative,neutral,neutral,neutral,neutral,neutral
174309803,"@MichaelMarkieta, that's a tough question because we're never sure if TPOT has reached the global optimum or not. We could implement something to, say, stop TPOT if no better solution has been discovered in X generations or Y minutes, but it's difficult to gauge what X or Y should be because there's so many different problem domains that TPOT can be applied to. This is why I prefer the stop conditions to be controlled by the user: They decide how much time they want to commit to TPOT, and can stop execution at any point and see the best solution so far.

@kadarakos, that's what TPOT currently does. It maintains an archive of the best solutions discovered thus far and uses the best one at the end.
",tough question never sure global optimum could implement something say stop better solution discovered difficult gauge many different problem applied prefer stop user decide much time want commit stop execution point see best solution far currently archive best discovered thus far best one end,issue,positive,positive,positive,positive,positive,positive
174278483,"TPOT could remember the solution with the best fitness after each generation and just roll back to the best one if it wasn't in the last generation. What do you think? 
",could remember solution best fitness generation roll back best one last generation think,issue,positive,positive,positive,positive,positive,positive
174259189,"I just came across this again in a new dataset where 1 column is a categorical feature with values such as ""c1"",""c2"",""c3"", etc. The `pandas.get_dummies()` worked well for me. I just passed the X_train data frame like `X_train = pandas.get_dummies(X_train)` and off I went without issue (it one-hot-encoded the categorical column and kept the rest of the data frame in tact (and in order)).
",came across new column categorical feature worked well data frame like went without issue categorical column kept rest data frame tact order,issue,positive,positive,positive,positive,positive,positive
173629872,"True enough. We do this for the command-line version, so it seems reasonable to do it for the scripted version as well. Filed as issue #74.
",true enough version reasonable version well issue,issue,positive,positive,positive,positive,positive,positive
173626179,"Could also print some stuff out right off the bat to tell the user things have been initialized. The fit statement is lonely waiting for gen 0 to finish.
",could also print stuff right bat tell user fit statement lonely waiting gen finish,issue,negative,positive,positive,positive,positive,positive
173624970,"It would probably help if we added an ""Expectations"" section explaining that TPOT will be rather slow for large data sets. Although in #59 we're exploring ideas on speeding the initial seeding phase up to get the user _something_ useful right off the bat.
",would probably help added section explaining rather slow large data although exploring speeding initial phase get user useful right bat,issue,positive,positive,neutral,neutral,positive,positive
173623942,"I got greedy... the dataset was too large and I didn't wait long enough for the first gen (0) to get reported. Working as expected. Carry on...

I am used to the verbosity of http://h2o.ai which is an unfair bias.
",got greedy large wait long enough first gen get working carry used verbosity unfair bias,issue,negative,negative,neutral,neutral,negative,negative
173615706,"With maximum verbosity (=2), it should also report the best/worst/average scores every generation. Depending on your data size, each generation may take a while.

We've been thinking about ways to provide an update on the best model every generation in #2. What information are you looking for while TPOT is running?
",maximum verbosity also report every generation depending data size generation may take thinking way provide update best model every generation information looking running,issue,positive,positive,positive,positive,positive,positive
173614474,"Looks like there is very little in the way of print statements... only at the end of the `fit` routine do we learn the best pipeline. I thought there would be more along the way. Hmm.
",like little way print end fit routine learn best pipeline thought would along way,issue,positive,positive,positive,positive,positive,positive
173609910,"I'm not seeing any output in python3 when running `fit` for any verbosity level. What is one of the first things TPOT is supposed to report, and how soon after starting `fit` should that happen? This is running in the interpreter.
",seeing output python running fit verbosity level one first supposed report soon starting fit happen running interpreter,issue,positive,positive,positive,positive,positive,positive
173607236,"pandas has `get_dummies()` as a convenience function, which by default scans the data frame and one hot encodes the obvious categorical features: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html
",convenience function default data frame one hot obvious categorical,issue,negative,positive,positive,positive,positive,positive
173310448,"Also please note that in #72 PCA was changed to RandomizedPCA. The operator is still named `_pca`, but it calls RandomizedPCA instead.
",also please note operator still instead,issue,negative,neutral,neutral,neutral,neutral,neutral
173263603,"You're totally right. I reused the first part for all of the examples, and neglected to update import statements. I'll throw those in when I get a chance. 
",totally right first part update import throw get chance,issue,negative,positive,positive,positive,positive,positive
173241527,"Great start! I think the ""example exported code"" would also need to include the import statements for the corresponding operator, right?
",great start think example code would also need include import corresponding operator right,issue,positive,positive,positive,positive,positive,positive
172957605,"> This branch has conflicts that must be resolved

Know what's causing that conflict?
",branch must resolved know causing conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
172597093,"PR #71 is related and still in review (will get to it soon, promise -- I'm back from vacation now), but otherwise I believe that's the only pending change to the docs.
",related still review get soon promise back vacation otherwise believe pending change,issue,negative,neutral,neutral,neutral,neutral,neutral
172384499,"Anyone working on documenting the pipeline operators and public functions? I've made some significant headway on it, but want to make sure I'm not duplicating labor.
",anyone working pipeline public made significant headway want make sure labor,issue,positive,positive,positive,positive,positive,positive
169788415,"Thanks for your response !

>  Opening the entire Python language to GP entails a much larger search space, so the idea here is to take pre-built bits of code (primarily from sklearn) and use them as the building blocks.

Actually, a configurable subset will do - in fact,  you will see that this is how ""deap"" works: you can register primitives/terminals and use Python callbacks for those, while specifying their signature/arity, including even strong typing: http://deap.gel.ulaval.ca/doc/default/examples/gp_symbreg.html

The example you can see there is using just a handful of Python callbacks for the tree representation, which makes it possible to use Python as both, the input but also the output for the trees that are manipulated by GP to evolve algorithms.

The paper you mentioned looks interesting, note that this way of using Python to literally support ""recursion"" is very powerful, as it allows genetic metaprogramming, i.e. a genetic program used to modify a GP to evolve algorithms: https://mitpress.mit.edu/sites/default/files/titles/alife/0262297140chap52.pdf
",thanks response opening entire python language much search space idea take code primarily use building actually subset fact see work register use python even strong example see handful python tree representation possible use python input also output evolve paper interesting note way python literally support recursion powerful genetic genetic program used modify evolve,issue,positive,positive,positive,positive,positive,positive
169675869,"Ok, I understand. Have a good one..All the best! 
",understand good one best,issue,positive,positive,positive,positive,positive,positive
169423193,":+1: I'm pretty tight on time while conferencing, but I plan to review this PR when I get back. Cheers!
",pretty tight time plan review get back,issue,negative,positive,neutral,neutral,positive,positive
169417225,"The two you mentioned -- number of pipeline operators and runtime -- but
also the number of features in the pipeline. Interested to hear more ideas
if you have some.

On Wed, Jan 6, 2016 at 12:49 AM, kadarakos notifications@github.com wrote:

> What kind of measures are you using for complexity?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/70#issuecomment-169295780.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",two number pipeline also number pipeline interested hear wed wrote kind complexity reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
169280814,"I'm currently developing and testing a multi-objective version of TPOT.
Including various measures of model complexity as an axis to minimize does
indeed eliminate cases like this.

On Tuesday, January 5, 2016, kadarakos notifications@github.com wrote:

> Could we solve this by having a multi-objective fitness function? Combined
> from:
> - reward accuracy/f-score
> - penalize the number of operations
> - penalize runtime
> 
> As far as I understand deap supports multi-objective optimization
> out-of-the-box.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/70#issuecomment-169274627.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",currently testing version various model complexity axis minimize indeed eliminate like wrote could solve fitness function combined reward penalize number penalize far understand optimization reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,neutral,neutral,positive,positive
169274627,"Could we solve this by having a multi-objective fitness function? Combined from:
- reward accuracy/f-score
- penalize the number of operations
- penalize runtime

As far as I understand deap supports multi-objective optimization out-of-the-box.
",could solve fitness function combined reward penalize number penalize far understand optimization,issue,positive,positive,neutral,neutral,positive,positive
169170665,"As far as I understand this work also uses the kind of warm start I described:
https://drive.google.com/file/d/0BzRGLkqgrI-qSWJ0MXBJbmpSYmpSQlJySkt2UHQ4allueThr/view

Their method is implemented in this library:
https://github.com/automl/auto-sklearn
",far understand work also kind warm start method library,issue,positive,positive,positive,positive,positive,positive
168729017,"yeah but you should really rather use OneHotEncoder - which unfortunately doesn't support strings at the moment, because the changes for that haven't been reviewed.
",yeah really rather use unfortunately support moment,issue,negative,negative,negative,negative,negative,negative
168401848,":+1: I noticed some of these bugs when reviewing the code the other day as well. Will look to address these soon.
",code day well look address soon,issue,negative,neutral,neutral,neutral,neutral,neutral
168313614,"No worries -- I appreciate that you took the time to contribute! :-) I just noted one more minor error, which when fixed, I believe will make this PR ready to merge.
",appreciate took time contribute noted one minor error fixed believe make ready merge,issue,negative,positive,neutral,neutral,positive,positive
168308743,"Thanks for the suggestions! I apologize for the messy PR, but I'm not very experienced in contributing to projects on github :-).
",thanks apologize messy experienced,issue,negative,positive,positive,positive,positive,positive
168232520,"Thank you for this PR @kadarakos. This looks great! I commented on some of the lines of code to request some fixes before we merge it.
",thank great code request merge,issue,positive,positive,positive,positive,positive,positive
167990147,"@amueller While I thought the same about MultiLabelBinarizer initially, but dont you think we can represent an input feature with it by treating the features's values as classes and then stacking the (n_sample x n_classes) obtained through MultiLabelBinarizer (on the input feature) with the original dataset for further analysis?
",thought initially dont think represent input feature treating class input feature original analysis,issue,negative,positive,positive,positive,positive,positive
167879468,"That's exactly right. It seems the feature selection in this case was ""junk code"" that wasn't pruned by the optimization process. i.e., because the feature selection didn't do anything, it wasn't optimized away. I'm working on code now that selects against bloat like that currently.

In the most recent version, we've actually removed the decision tree-based feature selection entirely and replaced it with more standard feature selection operators from sklearn: RFE, variance threshold, and various forms of univariate feature selection. Hopefully that will be out soon. You can check it out on the development version in the meantime.
",exactly right feature selection case junk code optimization process feature selection anything away working code bloat like currently recent version actually removed decision feature selection entirely standard feature selection variance threshold various feature selection hopefully soon check development version,issue,positive,positive,neutral,neutral,positive,positive
167865491,"Actually from observing the code a bit more precisely it seems to me that ""result3"" is just a sorted version of the original features:

```
result3 = result2[sorted(list(set(best_pairs + ['class'])))]
```

and then the kNN is fitted to the this sorted data frame

```
knnc4.fit(result3.loc[training_indeces].drop('class', axis=1).values, result3.loc[training_indeces, 'class'].values)
```

, so as far as I understand the feature selection was not actually performed. Running this piece of code

```
feature_clf = DecisionTreeClassifier()
feature_clf = feature_clf.fit(training_features, training_class_vals)
feature_select = SelectFromModel(feature_clf, prefit=True)
training_features_new = feature_select.transform(training_features)
```

actually shows - unsurprisingly - that the most informative features are the decisions of the previous classifiers.
",actually observing code bit precisely result sorted version original result result sorted list set fitted sorted data frame far understand feature selection actually running piece code actually informative previous,issue,negative,positive,positive,positive,positive,positive
167859381,"Thanks for the quick reply!

I evolved another piece of code that scores 1.0 on the iris data set, which is pretty impressive. However, I did raise some questions.

```
import numpy as np
import pandas as pd

from sklearn.cross_validation import StratifiedShuffleSplit
from itertools import combinations
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier # ME IMPORTING

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('iris.csv', sep=',')
tpot_data['class'] = digits['target'] # ME CHANGING THE STRINGS TO INTEGERS

training_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75)))

result1 = tpot_data.copy()

# Perform classification with a k-nearest neighbor classifier
knnc1 = KNeighborsClassifier(n_neighbors=min(13, len(training_indeces)))
knnc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)
result1['knnc1-classification'] = knnc1.predict(result1.drop('class', axis=1).values)

# Perform classification with a logistic regression classifier
lrc2 = LogisticRegression(C=2.75)
lrc2.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces, 'class'].values)
result2 = result1
result2['lrc2-classification'] = lrc2.predict(result2.drop('class', axis=1).values)

# Decision-tree based feature selection
training_features = result2.loc[training_indeces].drop('class', axis=1)
training_class_vals = result2.loc[training_indeces, 'class'].values

pair_scores = dict()
for features in combinations(training_features.columns.values, 2):
    print features
    dtc = DecisionTreeClassifier()
    training_feature_vals = training_features[list(features)].values
    dtc.fit(training_feature_vals, training_class_vals)
    pair_scores[features] = (dtc.score(training_feature_vals, training_class_vals), list(features))

best_pairs = []
print pair_scores
for pair in sorted(pair_scores, key=pair_scores.get, reverse=True)[:1070]:
    best_pairs.extend(list(pair))
best_pairs = sorted(list(set(best_pairs)))

result3 = result2[sorted(list(set(best_pairs + ['class'])))]

# Perform classification with a k-nearest neighbor classifier
knnc4 = KNeighborsClassifier(n_neighbors=min(6, len(training_indeces)))
knnc4.fit(result3.loc[training_indeces].drop('class', axis=1).values, result3.loc[training_indeces, 'class'].values)
result4 = result3
result4['knnc4-classification'] = knnc4.predict(result4.drop('class', axis=1).values)
```

A minor issue was that the DecisionTreeClassifier wasn't imported for the feature selection. Apart from that the I was a bit surprised by the way the feature selection part was implemented. I believe, could be replaced with the shorter - and maybe more general - code snippet from the sklearn documentation:

```
from sklearn.feature_selection import SelectFromModel
iris = load_iris()

feature_clf = DecisionTreeClassifier()
feature_clf = clf.fit(training_features, training_class_vals)
feature_select = SelectFromModel(feature_clf, prefit=True)
training_features_new = model.transform(training_features)
```

Is it just me or would this be a bit more concise?

Best,
Ákos
",thanks quick reply another piece code iris data set pretty impressive however raise import import import import import import import note make sure class data file next iter result perform classification neighbor classifier result perform classification logistic regression classifier result result result based feature selection print list list print pair sorted list pair sorted list set result result sorted list set perform classification neighbor classifier result result result minor issue feature selection apart bit way feature selection part believe could shorter maybe general code snippet documentation import iris would bit concise best,issue,positive,positive,positive,positive,positive,positive
167848567,"wrt ensembles of classifiers: I agree 100%! This is also something we're working on in the near future -- adding a pipeline operator that pools classifications from multiple classifiers in different ways (majority etc.).
",agree also something working near future pipeline operator multiple different way majority,issue,negative,positive,neutral,neutral,positive,positive
167848285,"Ah, I see what happened. The predict function is missing the `.loc` call at the end:

``` python
return result[result['group'] == 'testing', 'guess'].values
```

should be

``` python
return result.loc[result['group'] == 'testing', 'guess'].values
```

This has already been fixed in the development version, but I haven't rolled it out to pip yet. I will do this soon!
",ah see predict function missing call end python return result result python return result already fixed development version rolled pip yet soon,issue,negative,negative,neutral,neutral,negative,negative
167833858,"Hi @rhiever ,

Both iris features and classes are encoded as floats.

Your explanation makes it clear how to interpret the generated code. It makes me wonder, however, if this is the best way to ensemble models. Imho using the [VotingClassifier](http://scikit-learn.org/stable/modules/ensemble.html) object would be a more standard/straightforward way of ensembling different classifiers, plus it provides some additional flexibility. 
",hi iris class explanation clear interpret code wonder however best way ensemble object would way different plus additional flexibility,issue,positive,positive,positive,positive,positive,positive
167829846,"Jeff said it's possible to make sure that the encoding is the same, if you remember the possible values for the categorical variable.
You just have to take care of that, as ignoring it might indeed be disastrous.
",jeff said possible make sure remember possible categorical variable take care might indeed disastrous,issue,negative,negative,neutral,neutral,negative,negative
167828389,"Hmmm, that's a good point @amueller. I wonder if we should simply make it a requirement that the user perform the data cleaning beforehand. Otherwise, it's possible for the feature encodings to change between `fit()` and `score()`/`predict()` calls, which could be quite disastrous.
",good point wonder simply make requirement user perform data cleaning beforehand otherwise possible feature change fit score predict could quite disastrous,issue,negative,positive,neutral,neutral,positive,positive
167825400,"these are for labels, not input features.
For trees ""types don't make a difference"" if the tree knows what to do with them. The scikit-learn trees don't at the moment, and there will be a difference between one-hot encoding a variable and not doing so.

The problem with the pandas dummy variables is that they don't know about a training and a test phase, and so you need to make sure that the categorical variables in testing have the same possible values as in training.
",input make difference tree moment difference variable problem dummy know training test phase need make sure categorical testing possible training,issue,negative,positive,positive,positive,positive,positive
167587837,"Hi @kadarakos!

### Error with .predict for iris example

Please check if the iris features and classes are encoded as numerical features. This is likely the source of your error. We've raised issue #61 to address this problem in the near future.

### Interpreting generated code

Happy to see feedback about the generated code! The following is occurring in the pipeline you posted:

1) The training features and class labels are used to train the first decision tree

2) The class predictions from the first decision tree are then added as a new feature in the training features

3) A second decision tree is then trained on the training features (+ the predictions from the first decision tree) and class labels

4) `result2['dtc2-classification']` contains the final classifications from the pipeline. These values should correspond to what you see when you call `.predict()` on the TPOT object.

If you have thoughts on how to make the generated code clearer or easier to use, please let me know.

Best,

Randy
",hi error iris example please check iris class numerical likely source error raised issue address problem near future code happy see feedback code following pipeline posted training class used train first decision tree class first decision tree added new feature training second decision tree trained training first decision tree class result final pipeline correspond see call object make code clearer easier use please let know best randy,issue,positive,positive,positive,positive,positive,positive
167329100,"Good idea! I think that's something we could explore along with #49. We've been collecting numerous supervised classification data sets to explore TPOT performance on, and it could be interesting to see what pipeline components are shared amongst the best-performing pipelines on these data sets.
",good idea think something could explore along numerous classification data explore performance could interesting see pipeline amongst data,issue,positive,positive,positive,positive,positive,positive
167314833,"@rhiever Thanks for the tip! I think this looks good now. Let me know. 
",thanks tip think good let know,issue,positive,positive,positive,positive,positive,positive
167242984,"I think you need to change:

``` python
from tpot_export import ...
```

to

``` python
from .tpot_export import ...
```

to clarify that it's a local import.

Merry Christmas all! :santa: 
",think need change python import python import clarify local import merry,issue,positive,neutral,neutral,neutral,neutral,neutral
167242682,"Hey rhiever,

I'm not very familiar so far with TPOT, but I really like the idea! So sorry if my idea is not helping much, but here it goes:
- The TPOT package would come with the winning populations for e.g.: the Iris and MNIST data set. 
- A random selection from these populations with random mutations could be the starting population for the new task given by the user. 

This idea would do I think similar to what you were explaining. There are ""obvious"" ingredients to solving a classification task such as a classifier and TPOT should only search through solutions that actually include one. My idea is to go a bit further and provide pre-trained populations on several data sets that the user can choose from and can mix populations for iris, mnist olivetti etc. to initialize the population to solve his own task. What do you think? 
",hey familiar far really like idea sorry idea helping much go package would come winning iris data set random selection random could starting population new task given user idea would think similar explaining obvious classification task classifier search actually include one idea go bit provide several data user choose mix iris initialize population solve task think,issue,positive,positive,neutral,neutral,positive,positive
167199200,"Yes I saw that too, the error is in importing the new module. `ERROR: Failure: ImportError (No module named 'tpot_export')`. In my local machine it runs perfectly. Can you run the build once again and check?

Else will check this over the weekend. Renaming to `export_utils.py` is a good idea. :+1: Will implement that too. 

And lastly but most importantly, Merry Christmas :christmas_tree: to you and also to @rasbt and everyone else!
",yes saw error new module error failure module local machine perfectly run build check else check weekend good idea implement lastly importantly merry also everyone else,issue,positive,positive,positive,positive,positive,positive
167133083,"It looks like the build for Python 3 is failing. Maybe something to do with the new module?

Also, perhaps we should rename `tpot_export.py` to `export_utils.py` and move all of the export-related functions there (except for `export()`, of course).
",like build python failing maybe something new module also perhaps rename move except export course,issue,negative,positive,positive,positive,positive,positive
166955605,"Thanks ! Apologies, I hadn't seen that that discussion encompassed pickling. I'll close up over here.
",thanks seen discussion close,issue,negative,positive,positive,positive,positive,positive
166904105,"Sounds good to me.

On Wednesday, December 23, 2015, PRONOjit Saha notifications@github.com
wrote:

> @rhiever https://github.com/rhiever While we await the integration of
> #63 https://github.com/rhiever/tpot/pull/63, it will be good to break
> the final part i.e. 'Replace the function calls with their corresponding
> Python code' into a separate function or a code that the main tpot file
> imports from for improved code readability and comprehensibility
> significantly (similar way that I did the first two). I already have a
> basic structure, if you want can create a PR for it and you can look at it?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/44#issuecomment-166890685.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",good wrote await integration good break final part function corresponding python code separate function code main file code readability comprehensibility significantly similar way first two already basic structure want create look reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
166890685,"@rhiever While we await the integration of #63, it will be good to break the final part i.e. 'Replace the function calls with their corresponding Python code' into a separate function or a code that the main tpot file imports from for improved code readability and comprehensibility significantly (similar way that I did the first two). I already have a basic structure, if you want can create a PR for it and you can look at it?  
",await integration good break final part function corresponding python code separate function code main file code readability comprehensibility significantly similar way first two already basic structure want create look,issue,positive,positive,positive,positive,positive,positive
166760058,"There's definitely interest. I've discussed pickling TPOT pipelines with @rasbt in #2 if you'd like to join the conversation there.
",definitely interest like join conversation,issue,positive,neutral,neutral,neutral,neutral,neutral
166634833,"Thanks so much for this refactor! Since it'll require a fair amount of work to integrate, I'm going to hold off on merging this until mid-January or so when I'm done with the latest paper.
",thanks much since require fair amount work integrate going hold done latest paper,issue,positive,positive,positive,positive,positive,positive
166351989,"No, the locale information somehow got corrupted in .bash_profile and that affected pandas. Corrected it as

```
export LC_ALL=en_US.UTF-8
export LANG=en_US.UTF-8
```
",locale information somehow got corrupted affected corrected export export,issue,negative,neutral,neutral,neutral,neutral,neutral
166326624,"So the problem was with my pandas implementation only. Fixed it. Also got predict to work! Will update the material soon.  
",problem implementation fixed also got predict work update material soon,issue,negative,positive,neutral,neutral,positive,positive
166295150,"Thanks @rhiever. I did use .values earlier but it dint work either. I think result.loc is the solution for this hashing issue. 

Anyways, I updated my local from your master, and now I get a new error `ValueError: unknown locale: UTF-8` at the `import pandas as pd` line in tpot. Any changes you implemented that I should know of?
",thanks use dint work either think solution issue anyways local master get new error unknown locale import line know,issue,negative,positive,neutral,neutral,positive,positive
166133017,"#62 is now merged if you want to merge that into your fork, @dmarx.

Pretty stoked to see this refactor!
",want merge fork pretty see,issue,positive,positive,positive,positive,positive,positive
166132881,"Try:

> tpot.predict(titanic_new.drop('class', axis=1).values, titanic_new['class'].values, titanic_sub.values)

Otherwise a DataFrame is being passed rather than a numpy array.

If that doesn't work:

It looks like the `predict` function wasn't coded correctly at the end:

> return result[result['group'] == 'testing', 'guess'].values

should be

> return result.loc[result['group'] == 'testing', 'guess'].values

I'll fix that now.
",try otherwise rather array work like predict function correctly end return result result return result fix,issue,negative,neutral,neutral,neutral,neutral,neutral
165978796,"Ok, so your code works. I had used `train_test_split` and I believed that was the culprit in my case, not quite sure why though. 

I did some preprocessing as follows to make the date compliant with tpot requirement:

```
titanic.rename(columns={'Survived': 'class'}, inplace=True)
titanic['Sex'] = titanic['Sex'].map({'male':0,'female':1})
titanic['Embarked'] = titanic['Embarked'].map({'S':0,'C':1,'Q':2})
titanic_new = titanic.drop(['Name','Ticket','Cabin'], axis=1)
titanic_new = titanic_new.fillna(-999)
```

Did a `fit()` and `score()` to get ~80% accuracy. Now to predict on the submission test applied the same pre processing as above and the following command:
`tpot.predict(titanic_new.drop('class', axis=1), titanic_new['class'], titanic_sub)`. 

However this results in the following error:
`TypeError: 'Series' objects are mutable, thus they cannot be hashed` on the line `return result[result['group'] == 'testing', 'guess'].values` from `tpot.predict()`. Any inputs? 
",code work used culprit case quite sure though make date compliant requirement titanic titanic titanic titanic fit score get accuracy predict submission test applied following command however following error mutable thus line return result result,issue,negative,positive,positive,positive,positive,positive
165857378,"@dmarx Ok..great! Will get it done tomorrow.
",great get done tomorrow,issue,positive,positive,positive,positive,positive,positive
165849292,"Ok. I see that @dmarx's implementation still has the first two functions in export i.e. 
- Replace all of the mathematical operators with their results
- Unroll the nested function calls into serial code

Should I break that into separate functions, it will streamline export() further and then incorporate @dmarx's refactor on the third part i.e. 'Replace the function calls with their corresponding Python code'?
",see implementation still first two export replace mathematical unroll function serial code break separate streamline export incorporate third part function corresponding python code,issue,negative,positive,neutral,neutral,positive,positive
165842149,"@rhiever quite a bit. My goal is to push all code generation particular to operators to two loops: one that constructs the import statement and one that constructs the pipeline. So basically everything below the comment `# Replace the TPOT functions with their corresponding Python code` is going to go, as is the block that constructs the import statements. Call it a reduction of about 250 lines (for that one function. Much more for the TPOT class in general)? After I'm done the function should look something like this: https://gist.github.com/dmarx/a35a94cb0e42b3cb7811
",quite bit goal push code generation particular two one import statement one pipeline basically everything comment replace corresponding python code going go block import call reduction one function much class general done function look something like,issue,negative,positive,positive,positive,positive,positive
165839244,"Here's the code I used:

``` python
from tpot import TPOT
from sklearn.cross_validation import StratifiedShuffleSplit
import pandas as pd

pipeline_optimizer = TPOT(verbosity=2)

tpot_data = pd.read_csv('train.csv', sep=',')
tpot_data.rename(columns={'Survived': 'class'}, inplace=True)
training_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75, test_size=0.25)))

pipeline_optimizer.fit(tpot_data.loc[training_indeces].drop('class', axis=1).values,
                       tpot_data.loc[training_indeces, 'class'].values)
```

It seems to work fine, except it throws `ValueError: could not convert string to float: 'W.E.P. 5734'` when the data is passed to a `StandardScaler` (as would be expected since it is non-numerical data).
",code used python import import import next iter work fine except could convert string float data would since data,issue,negative,positive,positive,positive,positive,positive
165836524,"@dmarx, I'm :+1: on this refactor. It looks fantastic! How much do you think it's going to be able to reduce the code size of `export()`?
",fantastic much think going able reduce code size export,issue,positive,positive,positive,positive,positive,positive
165833895,"@pronojitsaha, it may be best to hold off on this issue until we see what affect @dmarx's refactor (see https://github.com/rhiever/tpot/issues/43#issuecomment-165660908) has on the export function. I suspect it will shrink the export function significantly.
",may best hold issue see affect see export function suspect shrink export function significantly,issue,negative,positive,positive,positive,positive,positive
165830722,"@rhiever While I am working on this, can you provide some inputs on the 'type' and 'contents' of the following just to be sure
-exported_pipeline, 
-pipeline_list, and 
-operator_text
",working provide following sure,issue,negative,positive,positive,positive,positive,positive
165827939,"Can you please share the latest version of this data set? I'll take a look.
",please share latest version data set take look,issue,positive,positive,positive,positive,positive,positive
165827637,"Also: I'll note that I'm currently trying to nail down a semi-stable version of TPOT for the next conference paper, so I probably won't pursue a major refactor until ~late January. But that doesn't mean we can't develop it in a fork in the meantime.
",also note currently trying nail version next conference paper probably wo pursue major mean ca develop fork,issue,negative,negative,neutral,neutral,negative,negative
165827089,"Oooohh, this is interesting @dmarx! I'm poking around in your fork now to see how things work. I like how this could also allow us to tie the `export()` code to the individual objects rather than having a giant single function as we do now.

wrt input validation: DEAP doesn't currently perform input validation. All `PrimitiveSetTyped` does is ensure that the right _type_ is passed, but doesn't perform any validation such as ensuring that the integer is positive.
",interesting poking around fork see work like could also allow u tie export code individual rather giant single function input validation currently perform input validation ensure right perform validation integer positive,issue,positive,positive,positive,positive,positive,positive
165807965,"Pretty sure these are the sklearn tools you're talking looking for:
- sklearn.preprocessing.LabelBinarizer
- sklearn.preprocessing.MultiLabelBinarizer
",pretty sure talking looking,issue,positive,positive,positive,positive,positive,positive
165740172,"I was thinking the same, that for tress data type doesnt make a difference. So may be for other models we do preprocessing specific to the model. Further I believe for binarizing, we need to chose a threshold, which then becomes a tuning parameter. How about the following encodings which require no decision parameters:
- pandas.get_dummies()
- sklearn.preprocessing.LabelEncoder()  

Both lead to information gain in most cases. 
",thinking tress data type doesnt make difference may specific model believe need chose threshold becomes tuning parameter following require decision lead information gain,issue,positive,neutral,neutral,neutral,neutral,neutral
165740026,"Thanks! Good news, I imported from the base directory and it now uses the development version. Bad news, the problem remain exactly the same.

I then dropped the features having categorical values and kept only features having numerical values i.e. 

> PassengerId      int64
> Survived         int64
> Pclass           int64
> Sex              int64
> Age            float64
> SibSp            int64
> Parch            int64
> Fare           float64
> Embarked       float64
> dtype: object

and then imputed missing values with placeholder value (-999), but still the exact same problem with indices persists.  
",thanks good news base directory development version bad news problem remain exactly categorical kept numerical sex age float parch fare float float object missing value still exact problem index,issue,negative,negative,neutral,neutral,negative,negative
165660908,"I've started playing with this in my fork:  https://github.com/dmarx/tpot/tree/modularize_operators

This is obviously going to be a significant refactoring of the project so I'd appreciate the feedback of anyone who cares enough to poke around (here's lookin at you, @rhiever) as I'd rather not make significant unilateral decisions on a project I haven't previously been involved with.

I've managed to get it working for one operator (random forest), shouldn't be too hard to port the rest over now that I'm past that hurdle.
",fork obviously going significant project appreciate feedback anyone enough poke around rather make significant unilateral project previously involved get working one operator random forest hard port rest past hurdle,issue,positive,negative,negative,negative,negative,negative
165589638,"Ok, I see. An abstract function sounds like a good approach. You're right about treating this as a ""contract"" as well. I was starting to get into attaching learned hyperparameters to operator instances (which would move towards satisfying issue #11) but that might be getting ahead of ourselves. Just abstracting out the ""contract"" would significantly simplify the code for the TPOT class, and is really all I was doing in the demo I linked anyway.

In fact, while we're keeping the scope constrained to the ""contract"" here, maybe it'd be simpler to just let each respective model's ""fit"" method be responsible for input validation. 
",see abstract function like good approach right treating contract well starting get learned operator would move towards satisfying issue might getting ahead contract would significantly simplify code class really linked anyway fact keeping scope constrained contract maybe simpler let respective model fit method responsible input validation,issue,positive,positive,positive,positive,positive,positive
165587000,"I should specify that I mean validating that the model parameters and input dataframe are usable (e.g., non-negative parameters and a minimum number of columns in the dataframe), rather than just type validation, which is the only thing I think `addPrimitive` provides. 

As for tying together the instances of operator objects to the pipeline, that's a good question. Perhaps the `tpot` object would have a function that wraps `addPrimitive` and allows the user to add additional operators to a default set of operators. As far as I know, all we would really need to do is come up with the 'contract' (in the functional programming sense) for each operator in order to add it to the `deap` pipeline. 
",specify mean model input usable minimum number rather type validation thing think tying together operator pipeline good question perhaps object would function user add additional default set far know would really need come functional sense operator order add pipeline,issue,negative,positive,positive,positive,positive,positive
165583189,"We could probably just attach that to the base class. In the gist, I included `inputtypes` as an input parameter to the base class with the intention of using that to pass the appropriate values to `pset.addPrimitive`. I imagine we could add a simple input validation function to the base class as well that would access these values. Probably wouldn't need to be overridden in most cases.

Personally, I'm more concerned about what the best way would be to tie instances of these ""operator"" objects with `deap` pipeline nodes (i.e. what we're currently referring to as ""operators"" in the tpot code). I'm new to deap (and only just started looking at `tpot` today as well) so that might actually be a fairly trivial consideration.

Do we even need a separate input validation function for each class? Or does `deap.gp.PrimitiveSetTyped` already handle input validation for us? 
",could probably attach base class gist included input parameter base class intention pas appropriate imagine could add simple input validation function base class well would access probably would need personally concerned best way would tie operator pipeline currently code new looking today well might actually fairly trivial consideration even need separate input validation function class already handle input validation u,issue,positive,negative,neutral,neutral,negative,negative
165580694,"Agreed with the oo approach, as among other things it'll also avoid bloating the main TPOT class down the line. Just to flesh this idea out, where would you propose handling input validation? An abstract function that each operator implements?
",agreed approach among also avoid bloating main class line flesh idea would propose handling input validation abstract function operator,issue,negative,positive,positive,positive,positive,positive
165577683,"I think a better approach would be to create a class for each operator, where the separate operators would inherit from base classes that define the expected API similar to how scikit-learn is organized. If done correctly, this would greatly simplify adding any arbitrary operator in the future, including empowering users to create customized operators. I'm thinking of a framework that would look something like this (non-working demo to roughly demonstrate the idea): https://gist.github.com/dmarx/7d72263a02b82cd276e1
",think better approach would create class operator separate would inherit base class define similar organized done correctly would greatly simplify arbitrary operator future create thinking framework would look something like roughly demonstrate idea,issue,positive,positive,neutral,neutral,positive,positive
165541535,"I'm thinking binarizing is the best option, yes. IIRC sklearn (or was it pandas?) has a built-in function for this, so we can probably harness that.
",thinking best option yes function probably harness,issue,positive,positive,positive,positive,positive,positive
165540735,"Okay, so the onus for dealing with ordinal data would then be on the user. As for encoding the non-numerical features, should we be binarizing them? If not, what should the range be? I.e., should we consider normalizing other features?
",onus dealing ordinal data would user range consider,issue,negative,neutral,neutral,neutral,neutral,neutral
165537018,"sklearn assumes everything is numerical basically. (well or binary)

For trees ordinal vs numeric doesn't make a difference, how would you even handle it in other models?
",everything numerical basically well binary ordinal make difference would even handle,issue,negative,neutral,neutral,neutral,neutral,neutral
165536182,"@amueller, how does sklearn handle this (w.r.t. what @bartleyn mentioned)? Does sklearn just assume that the user will take care of issues w.r.t. numerical vs. ordinal data?
",handle assume user take care numerical ordinal data,issue,negative,neutral,neutral,neutral,neutral,neutral
165535584,"Yeah I can imagine it's difficult without the information. Maybe that'll just something on the backburner/wishlist then.
",yeah imagine difficult without information maybe something,issue,negative,negative,negative,negative,negative,negative
165534627,"I can definitely see the value in that, but how could we accomplish it without explicit information from the user?
",definitely see value could accomplish without explicit information user,issue,positive,neutral,neutral,neutral,neutral,neutral
165534227,"I see. That fix isn't in the latest version on pip. You'd have to install tpot from development:

1) Download and unzip https://github.com/rhiever/tpot/archive/master.zip

2) `cd` into the tpot directory you unzipped

3) `python setup.py build`

4) `python setup.py install`

That will install the development version of tpot onto your system.

Alternatively, you can `cd` into the directory you're using to develop tpot, sync (i.e., pull) the latest updates from github, and then any code you run in the base tpot directory will reference the development tpot version rather than the version installed via pip.
",see fix latest version pip install development directory python build python install install development version onto system alternatively directory develop sync pull latest code run base directory reference development version rather version via pip,issue,negative,positive,neutral,neutral,positive,positive
165522566,"As an additional note, should we make an effort to distinguish between numerical and ordinal data?
",additional note make effort distinguish numerical ordinal data,issue,negative,neutral,neutral,neutral,neutral,neutral
165521978,"No, I did not clean it. However I do not think the error is due to that as the problem is in the train test splitting (we dont get to the pipeline optimisation stages at all). 

I tried to upgrade tpot using pip, which succeeded, but as I stated earlier my anaconda folder still shows an old version of tpot.py which gets referenced in the error. I believe the error may be due to this. Screenshot attached. Any inputs on this?
![screen shot 2015-12-17 at 10 57 28 pm](https://cloud.githubusercontent.com/assets/5354387/11876846/aa2c426c-a511-11e5-9259-51eba8f9984f.png)
",clean however think error due problem train test splitting dont get pipeline tried upgrade pip stated anaconda folder still old version error believe error may due attached screen shot,issue,negative,positive,neutral,neutral,positive,positive
165516074,"That version of the titanic data set is quite messy: it contains non-numerical values, missing values, etc. Do you have a clean version that you passed to TPOT?
",version titanic data set quite messy missing clean version,issue,negative,negative,neutral,neutral,negative,negative
165449125,"For the titanic data, a call to `tpot.fit(X_train, y_train)` raises a `KeyError` from the line `training_testing_data.loc[training_indeces, 'group'] = 'training'` as follows:

> KeyError: '[460  75 196 430 221 350 294 610 561 207  84  24 291 281 432  29 134 456\n 467 126 289 336 246 104  38  22 220 488 273 418 177 457 590 613 484 557\n 151 609 642 322 152 558 556 127 532 284 361 657 564 487 358 123 539 380\n 280 441  43 227 549 202 204 449  72 629 165 143 265 553   6 311 173 200\n 297 599 634 192 435 219 568 156 277 544 531 224 563 379 225 399 398   1\n 570 529  14  97 517 575 428 189 187 353 534 344 130 434 643 502 442  70\n 272  56 305  76  85 217 174 420 140 581 522 182 144   3 631 505 268 472\n 396 326 400  10 264] not in index'

So as it turns out some of the training indices produced by 

```
training_indices, testing_indices = next(iter(StratifiedShuffleSplit(training_testing_data['class'].values, n_iter=1, train_size=0.75)))
```

in `tpot.fit()` are not in X_train but in X_test (which is not even used in the call to `fit()`. What seems to be missing here? I have attached the data set. 
[titanic_train.csv.zip](https://github.com/rhiever/tpot/files/65403/titanic_train.csv.zip)

Further, checking my `anaconda/lib/python2.7/site-packages/tpot/tpot.pyc` it seems its an old version and not the current one (which has `test_size` mentioned in `StratifiedShuffleSplit`).  
",titanic data call line turn training index produced next iter even used call fit missing attached data set old version current one,issue,negative,positive,neutral,neutral,positive,positive
165221197,"> Further we can certainly look at a different pre-processing for non-numerical/categorical features by creating dummy flags and encoding categorical features as a separate issue?

Absolutely, I think this is a convenience feature we should look into adding to TPOT: Detect if there are non-numerical features and encode them as numerical features before passing them to the optimizer.
",certainly look different dummy categorical separate issue absolutely think convenience feature look detect encode numerical passing,issue,negative,positive,positive,positive,positive,positive
165194372,"Thanks!
- Yes, the idea is for much larger notebooks in the later phases. This is just a container for now. 
- I will just check on the titanic data set again and report back. Yes, I think we should include the limitation of numerical features in the documentation for now. Will do that. I think that might be the culprit here as well, since the titanic data set has non-numerical features. 

Further we can certainly look at a different pre-processing for non-numerical/categorical features by creating dummy flags and encoding categorical features as a separate issue? 
",thanks yes idea much later phase container check titanic data set report back yes think include limitation numerical documentation think might culprit well since titanic data set certainly look different dummy categorical separate issue,issue,positive,positive,neutral,neutral,positive,positive
165168718,"Great start! Thank you @pronojitsaha.

> Added a tutorial folder with Jupyter notebooks for follow along examples. Let me know if this is required?

Sounds fine to me. Although it's easy enough for people to copy-and-paste the code from our docs, I could see this being a useful folder for detailed notebooks that describe the process step-by-step, like [this one](https://github.com/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb).

> Need to add more detailed descriptions of fit, score & export.

:+1: 

> Encountered an error while working on the titanic data set for the examples. Need your input on the same.

Can you please describe the error you encountered? Was it an issue with data encoding? At this point, I think we need to clarify in the docs that TPOT expects all of the features and classes to be numerical. Although this is certainly something we can raise an issue for to allow TPOT to handle non-numeric features/classes by mapping them to numerical representations within TPOT.
",great start thank added tutorial folder follow along let know fine although easy enough people code could see useful folder detailed describe process like one need add detailed fit score export error working titanic data set need input please describe error issue data point think need clarify class numerical although certainly something raise issue allow handle numerical within,issue,positive,positive,positive,positive,positive,positive
164995359,"I think I'm good for now. If you think it's ready, then great!
",think good think ready great,issue,positive,positive,positive,positive,positive,positive
164936348,"This looks ready to merge now. Anything else you planning on adding to this PR?
",ready merge anything else,issue,negative,positive,positive,positive,positive,positive
164821760,"I'll try to squash some of my commits for brevity's sake.
",try squash brevity sake,issue,negative,neutral,neutral,neutral,neutral,neutral
164800498,"I see. We should definitely remove all of the `@staticfunction` decorators then, and add `self` as the first parameter to these functions. That will allow us to call the internal functions as `self.function_name()`.
",see definitely remove add self first parameter allow u call internal,issue,positive,positive,neutral,neutral,positive,positive
164799105,"I settled on moving `_train_model_and_predict()` out of the class to see if we could remove the `TPOT.` that prefaced the function call in the static models; I agree that ideally the function should remain in the TPOT class. However, without passing an instantiated TPOT object into the static model call, I'm not sure how to avoid the static `TPOT._train_model_and_predict()`. 
",settled moving class see could remove function call static agree ideally function remain class however without passing object static model call sure avoid static,issue,positive,positive,positive,positive,positive,positive
164786270,"IMO we should keep `_train_model_and_predict()` within the TPOT class. It's simply an abstraction of the  TPOT classifier operators.
",keep within class simply abstraction classifier,issue,negative,neutral,neutral,neutral,neutral,neutral
164785754,"Great work as always! Thank you for adding the test.

Addressing your questions:

> 1) Maybe it's just me, but It feels a little odd to call a static function from another static function. Does it make sense to move this method (and perhaps some others) to a utility function module? Either way, is it faster to call this abstract method as a static method inside the class, a class method inside the class, or something else?

AFAIK, there is no performance difference between the two. The difference between a static and regular class function is whether the function relies on the current state of the object, i.e., it affects the scope. Static functions always have their own scope, whereas class functions share the scope of the object they're called on.

I think we have to ask ourselves: Do we want users using these functions outside of TPOT? e.g.,

``` python
from tpot import random_forest

result = random_forest(...)
...
```

I originally envisioned the `export()` function doing something like that, but I ended up exporting the pipelines directly to sklearn code (even if it looks ugly). Perhaps we should change all of the functions to regular class methods since we don't really want users doing that. I don't think any of the functions in TPOT were really made to be used outside of TPOT.

> 2) Is there a more appropriate name for the method?

Name looks fine to me!

> 3) Is there a better place to do the input validation (i.e. if there are only three columns in the input_df) than within this nested function call?

That's a tough one, since we're never really sure where the pipeline operator functions will be getting their input from and where they're sending it to. I think putting in that quick check at the beginning of the function is the best we can do.
",great work always thank test maybe little odd call static function another static function make sense move method perhaps utility function module either way faster call abstract method static method inside class class method inside class something else performance difference two difference static regular class function whether function current state object scope static always scope whereas class share scope object think ask want outside python import result originally export function something like ended directly code even ugly perhaps change regular class since really want think really made used outside appropriate name method name fine better place input validation three within function call tough one since never really sure pipeline operator getting input sending think quick check beginning function best,issue,positive,positive,positive,positive,positive,positive
163631070,"Awesome!

On Wednesday, December 9, 2015, Nathan notifications@github.com wrote:

> I've got a long plane ride tomorrow evening that I could use to knock this
> out, especially now that I'm more familiar with DEAP.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/43#issuecomment-163486702.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",awesome wrote got long plane ride tomorrow evening could use knock especially familiar reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
163486702,"I've got a long plane ride tomorrow evening that I could use to knock this out, especially now that I'm more familiar with DEAP. 
",got long plane ride tomorrow evening could use knock especially familiar,issue,negative,positive,positive,positive,positive,positive
163426047,"Note to self: Investigate this more.

> self._select_percentile(self._polynomial_features(training_testing_data), 10)

Results in the warning:

> UserWarning: Features [0] are constant.

Same for passing the results of `_polynomial_features()` to any feature selection function.

---

Also, still need to implement export functionality for the new preprocessing operators.
",note self investigate warning constant passing feature selection function also still need implement export functionality new,issue,negative,positive,neutral,neutral,positive,positive
163028408,"I had a feeling that was it. Thanks for looking into it!

> BTW, really neat idea and project.

Thanks! Happy to have you involved more if you have any ideas or any of the open issues catch your fancy.
",feeling thanks looking really neat idea project thanks happy involved open catch fancy,issue,positive,positive,positive,positive,positive,positive
163025220,"Think I tracked it down. The two features in my training set had (integer) names 1 and 2, and so pandas seemed to have trouble applying the integer mask. Renaming the columns to strings made it work with the original code, but the updated version with `iloc` seems to work in both situations, so is probably a safer bet.

BTW, really neat idea and project.
",think tracked two training set integer trouble integer mask made work original code version work probably bet really neat idea project,issue,negative,positive,positive,positive,positive,positive
163020311,"Thanks @rcarneva! Both versions work fine on my end, though I believe the `.iloc[:, mask]`syntax is ""more correct."" What version of Python/pandas are you running?

I'll merge this in the meantime.
",thanks work fine end though believe mask syntax correct version running merge,issue,positive,positive,positive,positive,positive,positive
162914392,"Gotcha, thanks! I won't concern myself with cleaning up the export code then. I'll get it right one day though, I promise :P. 
",thanks wo concern cleaning export code get right one day though promise,issue,positive,positive,positive,positive,positive,positive
162908389,"Just noting some minor issues in the export code. About to do some final tests of the pipeline operators themselves, and if those turn out fine, I'll merge this and do some final cleanup.
",minor export code final pipeline turn fine merge final cleanup,issue,negative,positive,neutral,neutral,positive,positive
162903494,"I see what you mean now. I've heard of researchers using GP as a way to create computer programs that take given inputs and produce a specific output. With TPOT, the idea is to constrain the available grammar to only machine learning operators, with the hope that such constraints will aid in faster discovery of effective pipelines. Opening the entire Python language to GP entails a much larger search space, so the idea here is to take pre-built bits of code (primarily from sklearn) and use them as the building blocks.

My boss, Jason Moore, has actually developed a system similar to what you propose. He has dozens of papers out on it now; [here's one of them](http://link.springer.com/chapter/10.1007%2F978-3-540-78757-0_12). His GP system evolves both the rules and the features that are used to make the classification. The big difference with his work is that he's not evolving Python code; rather, he's evolving mathematical expressions.
",see mean way create computer take given produce specific output idea constrain available grammar machine learning hope aid faster discovery effective opening entire python language much search space idea take code primarily use building bos actually system similar propose one system used make classification big difference work python code rather mathematical,issue,positive,positive,positive,positive,positive,positive
162861003,"@rhiever Ok, I think it makes sense to work on small/sub sets now and focus more on the implementation of the examples. Will look into it. 
",think sense work focus implementation look,issue,negative,neutral,neutral,neutral,neutral,neutral
162840991,"Note that OpenCL is just an abstraction mechanism, i.e. the underlying ""kernels"" (C-like code) will work on CPUs, GPUs and FPGA hardware.
Wrappers like pyopencl even hide the nitty gritty details and expose all this flexibility to scripting space, which means that a python script can implement heavy algorithms as ""kernels"" that will automatically  make use of dedicated hardware if available.
the only real issue is that OpenCL does not currently lend itself to clustering/distribution.

Since you  mention MNIST: I suggest to run a corresponding google search, there are a number of examples where using the GPU instead of the CPU (via OpenCL/CUDA) provided a x100 factor speedup when using the MNIST dataset, e.g. see: http://corpocrat.com/2014/11/09/running-a-neural-network-in-gpu/ (note that this is also using python and skl)

http://www.cs.berkeley.edu/~demmel/cs267_Spr11/Lectures/CatanzaroIntroToGPUs.pdf
",note abstraction mechanism underlying code work hardware like even hide nitty gritty expose flexibility space python script implement heavy automatically make use hardware available real issue currently lend since mention suggest run corresponding search number instead via provided factor see note also python,issue,negative,positive,neutral,neutral,positive,positive
162834366,"regarding Abstract Syntax Trees (ASTs), frameworks like deap can be used to support configurable input/output tree formats, which is to say that a DSL (domain specific language) can be internally used by the GP framework for representation and crossover/mutation purposes.

The powerful thing here is to support a  programming language, like Python, as both, its output, but als input - i.e .similar to LISP.
In other words, you could throw python code at the GP framework, in terms of functions and building blocks (e.g. a subset of python) and let tpot mutate that, and then dump the resulting python code to a file.

For simplicity, let's imagine a python ""hello world"" script that is fed to tpot for functions/terminals, with its fitness function requiring it to output ""Hello tpot"", and provide the script as python-code.

Thanks for your clarifying comments regardign GPU support, you will probably want to take a look at pyopencl sooner or later. 
",regarding abstract syntax like used support tree say domain specific language internally used framework representation powerful thing support language like python output input lisp could throw python code framework building subset python let mutate dump resulting python code file simplicity let imagine python hello world script fed fitness function output hello provide script thanks support probably want take look sooner later,issue,positive,positive,neutral,neutral,positive,positive
162693912,"Yeah I'm on it. I'll work on it when I get home tonight.
",yeah work get home tonight,issue,negative,neutral,neutral,neutral,neutral,neutral
162689906,"Alrighty, I think that's all of the comments for now. In all, very nice work on this PR! Thank you for putting this together. Let me know if you want to hack at these comments, else I can merge the PR and clean it up later this week.

> Could use some optimization (caching?)

At least with large steps (>= 0.1), I found RFE to run pretty quickly on my test data sets. Maybe we could limit the RFE steps to >= 0.05 or so and it'll be fine?

> Could use some toying around with out-of-bounds parameters (e.g., if num_features < 0 in RFE)

Except in the export function, everything looks good to me. A great way to test it is to run TPOT with the new operators for several hundred generations. If it doesn't crash by the end of the run, your code has run the gauntlet and _probably_ caught all of the possible parameter edge cases. :-)

> Terminals for both estimators & scoring functions that plug into the new operators

Great idea! I'd imagine we can easily encode various estimators and scoring functions with integer values. It may be tricky to have actual estimators and scoring functions as terminals.

Let's file an issue for this to work on after this is merged.
",alrighty think nice work thank together let know want hack else merge clean later week could use optimization least large found run pretty quickly test data maybe could limit fine could use toying around except export function everything good great way test run new several hundred crash end run code run gauntlet caught possible parameter edge scoring plug new great idea imagine easily encode various scoring integer may tricky actual scoring let file issue work,issue,positive,positive,positive,positive,positive,positive
162685997,"In addition to the comments I've made directly inline, as you mentioned the exports need to ensure that the variables are within their proper limits.
",addition made directly need ensure within proper,issue,negative,positive,neutral,neutral,positive,positive
162669634,"Yeah RFE's the slowest, perhaps because I instantiate the estimator every time.
",yeah perhaps estimator every time,issue,negative,neutral,neutral,neutral,neutral,neutral
162665172,"Reviewing this now. Is RFE the one that's quite slow?

I may also drop `dt_feature_selection()` as a part of this upgrade since it's not supported in sklearn, and exporting it is quite ugly. Perhaps I'll contact the sklearn folks about merging a variant of `dt_feature_selection()` into sklearn.

Probably going to drop `subset_df()` as well, as I now can't imagine it being useful at this point.
",one quite slow may also drop part upgrade since quite ugly perhaps contact variant probably going drop well ca imagine useful point,issue,negative,negative,negative,negative,negative,negative
162661641,"Woohoo! :+1:

In your PR, please note what operators are slow (and how they can be optimized) and how else the code can be improved so we can file them as PRs or fix them on the spot.

Cheers!
",woohoo please note slow else code file fix spot,issue,positive,negative,negative,negative,negative,negative
162656993,"Was actually about to submit my PR -- I've gotten all four to work as additional operators, but could probably use some optimization (no caching, etc). Also could probably abstract a couple additional things.
",actually submit gotten four work additional could probably use optimization also could probably abstract couple additional,issue,negative,neutral,neutral,neutral,neutral,neutral
162655039,"Going to close this issue and open a new one for expanding classifier parameter search.
",going close issue open new one expanding classifier parameter search,issue,negative,positive,neutral,neutral,positive,positive
162654213,"@bartleyn, just wanted to ping you about this one to see if you're working on it. If not, I'll likely try to tackle this issue this week.
",ping one see working likely try tackle issue week,issue,negative,neutral,neutral,neutral,neutral,neutral
162627247,"feedback welcome. I haven't actually reviewed this, not sure if someone else has ;)
",feedback welcome actually sure someone else,issue,positive,positive,positive,positive,positive,positive
162625569,"Nice. Looks like [this](https://github.com/zermelozf/scikit-learn/blob/default-cv/sklearn/grid_search_default.py) will be a good start. Thank you @amueller!

``` python
_DEFAULT_PARAM_GRIDS = {'AdaBoostClassifier':
                        [{'learning_rate': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'AdaBoostRegressor':
                        [{'learning_rate': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'DecisionTreeClassifier':
                        [{'max_features': [""auto"", None]}],
                        'DecisionTreeRegressor':
                        [{'max_features': [""auto"", None]}],
                        'ElasticNet':
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'GradientBoostingClassifier':
                        [{'max_depth': [1, 3, 5]}],
                        'GradientBoostingRegressor':
                        [{'max_depth': [1, 3, 5]}],
                        'KNeighborsClassifier':
                        [{'n_neighbors': [1, 5, 10, 100],
                          'weights': ['uniform', 'distance']}],
                        'KNeighborsRegressor':
                        [{'n_neighbors': [1, 5, 10, 100],
                          'weights': ['uniform', 'distance']}],
                        'Lasso':
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'LinearRegression':
                        [{}],
                        'LinearSVC':
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'LogisticRegression':
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'SVC': [{'C': [0.01, 0.1, 1.0, 10.0, 100.0],
                                 'gamma': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'MultinomialNB':
                        [{'alpha': [0.1, 0.25, 0.5, 0.75, 1.0]}],
                        'RandomForestClassifier':
                        [{'max_depth': [1, 5, 10, None]}],
                        'RandomForestRegressor':
                        [{'max_depth': [1, 5, 10, None]}],
                        'Ridge':
                        [{'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'SGDClassifier':
                        [{'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01],
                          'penalty': ['l1', 'l2', 'elasticnet']}],
                        'SGDRegressor':
                        [{'alpha': [0.000001, 0.00001, 0.0001, 0.001, 0.01],
                          'penalty': ['l1', 'l2', 'elasticnet']}],
                        'LinearSVR':
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0]}],
                        'SVR':
                        [{'C': [0.01, 0.1, 1.0, 10.0, 100.0],
                          'gamma': [0.01, 0.1, 1.0, 10.0, 100.0]}]}
```
",nice like good start thank python auto none auto none none none,issue,positive,positive,positive,positive,positive,positive
162621437,"TPOT v0.2.0 is out on pip with the export functionality. Enjoy, and thanks for your feedback!
",pip export functionality enjoy thanks feedback,issue,positive,positive,positive,positive,positive,positive
162573525,"> 1) if/how can this be used to directly deal with ASTs (parse trees) - i.e. beyond GAs and so that this can be used to seed/create, mutate syntax trees (e.g. from python ast) 

Can you elaborate on this? I'm not familiar with ASTs. There are function within TPOT that generate and mutate the GP trees that represent machine learning pipelines, but those are currently hidden from the user.

> 2) if there are any plans to support OpenCL, e.g. for running things concurrently using GPUs or idle CPU cores ?

My boss is very much pushing for GPU support, so we may go that way eventually, but currently we're still focusing on fully developing the TPOT functionality (e.g., adding more pipeline operators #45 / #46) and expanding support to other ML problems (e.g., regression #30). We'll be looking at optimizations such as GPU support after we've reached a fairly stable state for TPOT.

Currently, we set `n_jobs=-1` everywhere possible in the sklearn code to support multithreading. Random forests, for example, will make use of all available cores when fitting and predicting. It looks like it may be possible to [support multithreading](http://deap.gel.ulaval.ca/doc/dev/tutorials/distribution.html#distribution-deap) in DEAP (the GA library) as well.
",used directly deal parse beyond gas used mutate syntax python ast elaborate familiar function within generate mutate represent machine learning currently hidden user support running concurrently idle bos much pushing support may go way eventually currently still fully functionality pipeline expanding support regression looking support fairly stable state currently set everywhere possible code support random example make use available fitting like may possible support ga library well,issue,positive,positive,positive,positive,positive,positive
162569740,"For now, I think we'll stick to smaller data sets (e.g., the sklearn MNIST subset) for the examples in the docs. i.e., examples that can be executed and see results in less than 10 minutes. I wouldn't want to require the user to fire up an EC2 instance or hop on a HPCC to run a basic TPOT example.

However, for some use cases it may take several hours to run TPOT -- especially with large data sets -- and I think it would be a good idea to note that in the docs. Perhaps in an ""Expectations for TPOT"" section of the docs?
",think stick smaller data subset executed see le would want require user fire instance hop run basic example however use may take several run especially large data think would good idea note perhaps section,issue,negative,positive,positive,positive,positive,positive
162567330,"I'll push out a new version to pip today with the export feature.
",push new version pip today export feature,issue,negative,positive,positive,positive,positive,positive
162464069,"> hardware is a challenge as increasing data set sizes will slow down TPOT considerably

FWIW, other Python based GP projects tend to use OpenCL/PyOpenCL to make better use of dedicated CPU/GPU and FPGA resources. In fact, a number are even using CUDA (which is NVIDIA specific)
",hardware challenge increasing data set size slow considerably python based tend use make better use fact number even specific,issue,positive,positive,neutral,neutral,positive,positive
162461719,"@rhiever Ok, I will look into the two points. I understand at this point we have only implemented for classifications tasks, so for examples following are few data sets in my mind, please let me know your views:
1. Iris Dataset
2. Titanic Dataset
3. [Lending Club Data](https://www.lendingclub.com/info/download-data.action)
4. [Facial Keypoint Detection](https://www.kaggle.com/c/facial-keypoints-detection)
5. [Forest Cover Type Dataset](http://archive.ics.uci.edu/ml/datasets/Covertype)

However, hardware is a challenge as increasing data set sizes will slow down TPOT considerably and increase the time involvement. This also applies for #41 for unit testing. As such have you thought of having EC2 instances for this project or any other alternative to account for this?
",look two understand point following data mind please let know iris titanic club data facial detection forest cover type however hardware challenge increasing data set size slow considerably increase time involvement also unit testing thought project alternative account,issue,positive,negative,neutral,neutral,negative,negative
162255165,"re: export() - thanks for clarifying - you may want to add a corresponding note to the example on the website, because that is kinda unexpected, i.e. most people will probably try to run your examples first
",export thanks may want add corresponding note example unexpected people probably try run first,issue,negative,positive,positive,positive,positive,positive
162254285,"Fair enough. If you import the library, you can check the version with:

```
import tpot
print(tpot.__version__)
```

Should be easy enough to add --version as a command line parameter.

BTW: the `export()` functionality has not been included with a release yet; it's only available in the dev version.
",fair enough import library check version import print easy enough add version command line parameter export functionality included release yet available dev version,issue,positive,positive,positive,positive,positive,positive
162252732,"We want to add a ""Default grid"" https://github.com/scikit-learn/scikit-learn/pull/5564 but it is somewhat stalled. I'm crazy busy at the moment but I hope to work on that soonish.
",want add default grid somewhat crazy busy moment hope work soonish,issue,negative,negative,negative,negative,negative,negative
162138163,"Awesome. I'll keep an eye out for the PR!

On Friday, December 4, 2015, Nathan notifications@github.com wrote:

> Right, duh, because dt stands for decision tree. These operators would
> then act as alternatives to _dt_feature_selection. I'd be interested in
> taking a stab at it tonight/this weekend.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/45#issuecomment-162132367.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",awesome keep eye wrote right decision tree would act interested taking stab weekend reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,positive,positive,positive,positive
162132367,"Right, duh, because dt stands for decision tree. These operators would then act as alternatives to _dt_feature_selection. I'd be interested in taking a stab at it tonight/this weekend.
",right decision tree would act interested taking stab weekend,issue,negative,positive,positive,positive,positive,positive
162130495,"Nope - these will be separate functions.

On Friday, December 4, 2015, Nathan notifications@github.com wrote:

> Would these be options that fall under the _dt_feature_selection method?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/45#issuecomment-162125985.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",nope separate wrote would fall method reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,neutral,neutral,positive,positive
162122255,"@Chris7: I noticed that today too when I was implementing the new classifiers. I'll look into abstracting the common bits next week, as that would indeed save quite a lot of repeated code.

@amueller: :+1: Looking forward to ANNs in sklearn. I'll add gradient boosting as well. Do you have a sense for what are the 2-3 most important parameters (if there are that many) for each model? I've tinkered around with various parameters for various models but don't have a comprehensive view of them like you might.
",today new look common next week would indeed save quite lot repeated code looking forward add gradient well sense important many model around various various comprehensive view like might,issue,positive,positive,positive,positive,positive,positive
162094946,"ANNs are in dev. They don't have dropout yet, but will soon.

How about gradient boosting?
",dev dropout yet soon gradient,issue,negative,neutral,neutral,neutral,neutral,neutral
162084560,"Nice to see classifiers are coming along!

One thing I've noticed is you have a ton of repetitive code and it seems like you could abstract the implementation of all these methods to just a generic_regressor/generic_classifier function and a common structure for storing the regressors/parameters. Have you tried any generic approach yet?
",nice see coming along one thing ton repetitive code like could abstract implementation function common structure tried generic approach yet,issue,positive,positive,neutral,neutral,positive,positive
162080518,"I agree with your intuition -- out of curiosity I'll take some time later tonight and see if there are any existing empirical test results pointing one way or the other, otherwise I'll run my own tests and see. 
",agree intuition curiosity take time later tonight see empirical test pointing one way otherwise run see,issue,negative,positive,neutral,neutral,positive,positive
162061388,"I've never checked to be honest. My assumption has always been that dropping a few specific columns is faster than finding out the column names of all the _other_ columns and slicing down to those.
",never checked honest assumption always dropping specific faster finding column slicing,issue,negative,positive,positive,positive,positive,positive
162060512,"> training_features = input_df.loc[input_df['group'] == 'training'].drop(['class', 'group', 'guess'], axis=1).values

I've always wondered -- is it faster to drop columns or to create a slice all the other columns?
",always faster drop create slice,issue,negative,neutral,neutral,neutral,neutral,neutral
162060254,"We're definitely open to suggestions. :-) It's nice having DataFrames because you can (theoretically) maintain the names of the columns through the whole process, which helps with inspection if you're, e.g., looking at feature importance scores.
",definitely open nice theoretically maintain whole process inspection looking feature importance,issue,positive,positive,positive,positive,positive,positive
162059499,"I'm happy to take another look and see if there's a smarter/more efficient way to handle this -- I imagine passing the dataframes to the scoring function will get bulky after a point. 
",happy take another look see efficient way handle imagine passing scoring function get bulky point,issue,positive,positive,positive,positive,positive,positive
162034933,"@amueller, do you have any advice on what additional model parameters to open up to evolution?
",advice additional model open evolution,issue,negative,neutral,neutral,neutral,neutral,neutral
161976249,"Alrighty, I think I'll merge this for now since I'd like to push this functionality out. Please file bug reports against it if you see anything wrong.
",alrighty think merge since like push functionality please file bug see anything wrong,issue,negative,negative,negative,negative,negative,negative
161896119,"Besides that, the code looks fine to me so far. But I haven't run it through a debugger yet and looked at it in detail... sorry, it's the end of the year and I am pretty busy wrapping things up before I go on a family visit, but January should be a good time for a fresh start :)
",besides code fine far run yet detail sorry end year pretty busy wrapping go family visit good time fresh start,issue,positive,positive,positive,positive,positive,positive
161895711,"> export() actually runs pretty quick even on large pipelines, so it wouldn't be much overhead to run it every generation.

Oh, sure, but I was more thinking of ""bloated log files"" here. I think that log/status files are generally helpful to keep track of errors but also to judge the progress if your are running stuff remotely, and analyzing what's going on under the hood (e.g., think bakc of ye goode olde times submitting jobs to HPCC ;)). So, I was thinking to refactor it into a more bare-bones ""export_params"" and a ""export_pipeline_standalone"" or so. But this is just a general suggestion, doesn't have to be now :)
",export actually pretty quick even large would much overhead run every generation oh sure thinking bloated log think generally helpful keep track also judge progress running stuff remotely going hood think ye time thinking general suggestion,issue,positive,positive,positive,positive,positive,positive
161805844,"Just to bug you even more, @rasbt, check out https://github.com/rhiever/tpot/commit/96c69a4fd260e1dfeeed1e360943a86803204cb7 for the implementation of these new classifiers.

Any other parameters that you would include?

Would you implement any parameters differently?
",bug even check implementation new would include would implement differently,issue,negative,positive,neutral,neutral,positive,positive
161507787,"`export()` actually runs pretty quick even on large pipelines, so it wouldn't be much overhead to run it every generation.
",export actually pretty quick even large would much overhead run every generation,issue,negative,positive,positive,positive,positive,positive
161470999,"> Oh, and let's make sure to close #36 when this is merged. Now when the user ends TPOT early on the command line and they provided an output file, the current best pipeline will be exported.

Sounds cool! Maybe a nice addition would be to refactor the `export`method a bit so that a slimmer ""core"" component can be written to a log-file. This let's the user also conveniently check the progress over time (e.g., via `tail tpot_run_x.log`) or so.
",oh let make sure close user early command line provided output file current best pipeline cool maybe nice addition would export method bit core component written let user also conveniently check progress time via tail,issue,positive,positive,positive,positive,positive,positive
161457069,"Oh, and let's make sure to close #36 when this is merged. Now when the user ends TPOT early on the command line and they provided an output file, the current best pipeline will be exported.
",oh let make sure close user early command line provided output file current best pipeline,issue,positive,positive,positive,positive,positive,positive
161454910,"Sure, here's an example:

``` python
from tpot import TPOT
from sklearn.datasets import load_digits
from sklearn.cross_validation import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75)

tpot = TPOT(generations=1, verbosity=2)
tpot.fit(X_train, y_train)
print(tpot.score(X_train, y_train, X_test, y_test))
tpot.export('tpot_export.py')
```

TPOT will likely discover that a random forest alone does well on the data set, so `tpot_export.py` should contain something like:

``` python
from itertools import combinations

import numpy as np
import pandas as pd

from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

# NOTE: Make sure that the class is labeled 'class' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR')
training_indeces, testing_indeces = next(iter(StratifiedShuffleSplit(tpot_data['class'].values, n_iter=1, train_size=0.75)))


result1 = tpot_data.copy()

# Perform classification with a random forest classifier
rfc1 = RandomForestClassifier(n_estimators=97, max_features=min(8, len(result1.columns) - 1))
rfc1.fit(result1.loc[training_indeces].drop('class', axis=1).values, result1.loc[training_indeces]['class'].values)
result1['rfc1-classification'] = rfc1.predict(result1.drop('class', axis=1).values)
```

If you want to perform further inspection on the pipelines, if you `print()` the optimized pipeline at any point it will give you the nested function version, from which you can deduce what the linear code version should look like.
",sure example python import import import print likely discover random forest alone well data set contain something like python import import import import import import note make sure class data file next iter result perform classification random forest classifier result want perform inspection print pipeline point give function version deduce linear code version look like,issue,positive,neutral,neutral,neutral,neutral,neutral
161445110,"> @rasbt, I would greatly appreciate if you could review this before we merge it.

Sure, I can take a more detailed look at it later when I get home. However, would it be possible to add a small unit test comparing the actual output with what you'd expect? I think that would be helpful as ""documentation"" and to follow along.
",would greatly appreciate could review merge sure take detailed look later get home however would possible add small unit test actual output expect think would helpful documentation follow along,issue,positive,positive,positive,positive,positive,positive
161442986,"I see your point about making the effort now to support the arbitrary functions. I'll make an attempt at it and provide examples for at least F/P/R. 
",see point making effort support arbitrary make attempt provide least,issue,negative,negative,negative,negative,negative,negative
161440625,"> > Yes. I think we should eventually support allowing the user to pass arbitrary scoring functions, similar to how sklearn does it.
> 
> Would passing in a keyword for the specific metric work for now?

At first thought, I think it'd be better/easier to simply allow the user to pass an arbitrary scoring function. Otherwise, we have to choose what scoring functions to support, write a special case for each one, etc. Not very scalable from a coding point of view. Of course, we'd have to also clarify that the user should provide a scoring function that's appropriate for their data.

> > As in, use one scoring functions for optimization then a different scoring metric for final model selection? Interesting idea. I'm currently working on a version of TPOT that allows it to optimize on multiple criteria simultaneously, so perhaps that will help in this regard.
> 
> Yeah, I was thinking that and/or adding additional metrics to the report of the final model. 

Is there a specific use case for this that you can think of?

> > That's what we currently do with accuracy. I think it makes sense to do the same with other measures.
> 
> I'm happy to contribute by adding simple support for F1/Precision/Recall in the same vein as the accuracy. I think it should be relatively straightforward.

Would you be interested in making an attempt at the implementation that allows the passing of arbitrary scoring functions discussed above? Perhaps we could also provide some example snippets in the docs of how to expand F1/precision/recall/etc. to support multiple classes, which can then be passed as the arbitrary scoring function.
",yes think eventually support user pas arbitrary scoring similar would passing specific metric work first thought think simply allow user pas arbitrary scoring function otherwise choose scoring support write special case one scalable point view course also clarify user provide scoring function appropriate data use one scoring optimization different scoring metric final model selection interesting idea currently working version optimize multiple criterion simultaneously perhaps help regard yeah thinking additional metric report final model specific use case think currently accuracy think sense happy contribute simple support vein accuracy think relatively straightforward would interested making attempt implementation passing arbitrary scoring perhaps could also provide example expand support multiple class arbitrary scoring function,issue,positive,positive,positive,positive,positive,positive
161438320,"> Yes. I think we should eventually support allowing the user to pass arbitrary scoring functions, similar to how sklearn does it.

Would passing in a keyword for the specific metric work for now?

> As in, use one scoring functions for optimization then a different scoring metric for final model selection? Interesting idea. I'm currently working on a version of TPOT that allows it to optimize on multiple criteria simultaneously, so perhaps that will help in this regard.

Yeah, I was thinking that and/or adding additional metrics to the report of the final model. 

> That's what we currently do with accuracy. I think it makes sense to do the same with other measures.

I'm happy to contribute by adding simple support for F1/Precision/Recall in the same vein as the accuracy. I think it should be relatively straightforward. 
",yes think eventually support user pas arbitrary scoring similar would passing specific metric work use one scoring optimization different scoring metric final model selection interesting idea currently working version optimize multiple criterion simultaneously perhaps help regard yeah thinking additional metric report final model currently accuracy think sense happy contribute simple support vein accuracy think relatively straightforward,issue,positive,positive,positive,positive,positive,positive
161438285,"The decline in coverage is expected. The new `export()` function introduces a fair bit of new code without a unit test for it (yet). I'll eventually get to expanding the unit tests... :-)
",decline coverage new export function fair bit new code without unit test yet eventually get expanding unit,issue,negative,positive,positive,positive,positive,positive
161437790,"Since we #32 now, I think we can close this issue.
",since think close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
161435738,"@rhiever I'd recommend not to cram too much into the README file but focus on the ""essentials"" like an overview, a quick example, installation, license info, and short contributing info. I would insert a ""important links section at the top pointing to the actual documentation then.
Otherwise, I'd suggest to just assemble the README.md, e.g., 

```
cat index.md installation.md contributing.md MNIST_Example.md ... > README.md
```
",recommend cram much file focus like overview quick example installation license short would insert important link section top pointing actual documentation otherwise suggest assemble cat,issue,positive,positive,positive,positive,positive,positive
161433317,"@rasbt, I've been updating the docs for the new export functionality and it takes double the work to update both the README and the docs. Any recommendations to avoid this duplication of labor?

@pronojitsaha, now that we have docs up and running, I can think of a couple things that would be invaluable at this point:

1) Not all of the public TPOT functions are thoroughly documented. fit, score, and export in particular need more documentation since those are the primary functions that people will be using. Currently we have a basic example of using them in the README, but it'd be great to expand on those docs and go into detail on what each function -- and what parameter of each function -- does.

2) More examples are always welcome! Currently we only have the MNIST example from sklearn, but it'd be great to provide code examples of many different types of data sets.
",new export functionality double work update avoid duplication labor running think couple would invaluable point public thoroughly fit score export particular need documentation since primary people currently basic example great expand go detail function parameter function always welcome currently example great provide code many different data,issue,positive,positive,positive,positive,positive,positive
161396584,"@rhiever Thanks! I figured that it would indeed be slow to run iterations on the training set.

I think this is a really cool project.
",thanks figured would indeed slow run training set think really cool project,issue,positive,positive,neutral,neutral,positive,positive
161391259,"> Does having alternate performance metrics/fitness functions make sense for the users?

Yes. I think we should eventually support allowing the user to pass arbitrary scoring functions, similar to how sklearn does it.

> Does it make sense to add alternate metrics when reporting the best model? If so, which metric do we use as the fitness function?

As in, use one scoring functions for optimization then a different scoring metric for final model selection? Interesting idea. I'm currently working on a version of TPOT that allows it to optimize on multiple criteria simultaneously, so perhaps that will help in this regard.

> Since there is native support for multi-class/label classification regular precision, recall, and F1 may not be that useful. Should we just take the average version of scores like these when necessary?

That's what we currently do with accuracy. I think it makes sense to do the same with other measures.
",alternate performance make sense yes think eventually support user pas arbitrary scoring similar make sense add alternate metric best model metric use fitness function use one scoring optimization different scoring metric final model selection interesting idea currently working version optimize multiple criterion simultaneously perhaps help regard since native support classification regular precision recall may useful take average version like necessary currently accuracy think sense,issue,positive,positive,positive,positive,positive,positive
161390287,"Okay, finally back from vacation! I'm happy to hear that you're excited about contributing to TPOT.

Looking at @Chris7's gist, he's right on the money. I'd imagine instead of having a special keyword passed to TPOT, we'd be better off implementing separate ""classification"" and ""regression"" versions of TPOT (e.g., TPOTRegressor and TPOTClassifier) that build off of a base TPOT object with the code that is shared between the two.
",finally back vacation happy hear excited looking gist right money imagine instead special better separate classification regression build base object code two,issue,positive,positive,positive,positive,positive,positive
161066715,"Hi @SimplyAhmazing,

The first thing I should clarify is that TPOT will be quite slow on large data sets such as the full MNIST data set. Using the default TPOT settings, each iteration of the algorithm is evaluating 100 pipelines on the training set, many of which are training multiple classifiers on the data. This statement is true of most Evolutionary Computation-based methods, where it's not uncommon to allow the algorithm to run for several hours, days, or even weeks. Your best bet is to set TPOT to running on the data set and give it a couple days to crunch on the data.

Regarding outputting the pipeline: Currently, TPOT on the command line only outputs the best pipeline in terms of TPOT functions _at the end of the run_. If you terminate a command line version early, you won't see the final pipeline. I've raised #36 as a suggestion to fix that.

We're still working on outputting the pipelines as sklearn code. You can see the latest on [this branch](https://github.com/rhiever/tpot/blob/export/Testing%20TPOT%20usage.ipynb). It turned out to be quite tricky to convert these pipelines to useable Python code, so that feature is somewhat delayed.
",hi first thing clarify quite slow large data full data set default iteration algorithm training set many training multiple data statement true evolutionary uncommon allow algorithm run several day even best bet set running data set give couple day crunch data regarding pipeline currently command line best pipeline end terminate command line version early wo see final pipeline raised suggestion fix still working code see latest branch turned quite tricky convert python code feature somewhat,issue,positive,positive,positive,positive,positive,positive
160895761,"@pronojitsaha Just got home and read your message; I thought: up the template literally just takes 10 minutes, let's do this ;). See pull request #35 

I basically just pasted the sections from the Readme file for now, you can see it live at
http://rasbt.github.io/tpot/

(If you fetch or merge it, you can see it live locally by running `mkdocs serve` from the `docs/source` directory -- by default it's http://127.0.0.1:8000/.)

So, I guess I'll take a look at the API documentation in January then, but I wanted to set this up so that you guys can maybe write the rest of the documentation and come up with some more examples and tutorials or so in the mean time.
",got home read message thought template literally let see pull request basically pasted file see live fetch merge see live locally running serve directory default guess take look documentation set maybe write rest documentation come mean time,issue,negative,negative,neutral,neutral,negative,negative
160867502,"@rasbt Hope you had a good thanksgiving. Ok, I will look into it and setup the initial framework using Mkdocs which we can later work on together once you are available in January. Enjoy the vacation!
",hope good thanksgiving look setup initial framework later work together available enjoy vacation,issue,positive,positive,positive,positive,positive,positive
160820940,"@rhiever @pronojitsaha Sorry for the late response, I took a few days off over the long Thanksgiving weekend. Unfortunately, I am in the midst of wrapping up a few research projects before I I'll go on vacation in a few days so I probably wouldn't get to it before January. But setting up a basic framework via Sphinx or Mkdocs should be pretty straight-forward I guess. The gplearn library is actually a nice, lean example: https://gplearn.readthedocs.org/en/latest/examples.html

I would suggest using the Readme file as a template; I think the goal of the documentation would be a to have an ""appealing"" with a convenient navigation to find relevant information.
I think that it will definitely pay off in the long run when the code base grows (regarding the API documentation) as well as the number of tutorials and examples.

Maybe I'd start with the following sections/pages
- ""Contributing"" (basic GitHub instructions: filing issues, forking, and pull requests etc.)
- ""Version History"" (keeping track of new features and changes over time)
- ""API documentation"" i.e., auto-parsing the docstrings
- ""Installation""
- ""Tutorials"" or ""Examples""
",sorry late response took day long thanksgiving weekend unfortunately midst wrapping research go vacation day probably would get setting basic framework via sphinx pretty guess library actually nice lean example would suggest file template think goal documentation would appealing convenient navigation find relevant information think definitely pay long run code base regarding documentation well number maybe start following basic filing pull version history keeping track new time documentation installation,issue,positive,negative,neutral,neutral,negative,negative
159865532,"@rhiever & @rasbt thanks. 

@rasbt As you already have a similar framework in place, I dont believe its make sense to reinvent the wheel again! You can share the existing framework as a separate repository and then we can decide the structure and contribute to individual pages as mutually decided. Does that work for you? 
",thanks already similar framework place dont believe make sense reinvent wheel share framework separate repository decide structure contribute individual mutually decided work,issue,positive,positive,neutral,neutral,positive,positive
159695667,"@rhiever @pronojitsaha Alright, sounds like a plan. I suggested to setup the MkDocs framework with API generator and stuff as I've done this for other projects already, but if @pronojitsaha wants to do it, it would be fine with me too. Just let me know so that we don't implement the same thing twice :).
",alright like plan setup framework generator stuff done already would fine let know implement thing twice,issue,positive,positive,positive,positive,positive,positive
159692092,"I would be happy for you two to take the helm on establishing the project
docs. Once I get back on Monday, I'll be focusing on development again.

On Wednesday, November 25, 2015, Sebastian Raschka notifications@github.com
wrote:

> @rhiever https://github.com/rhiever Yes, I was also thinking more in
> terms of ""in the long run."" It would certainly help though to start early
> and document ""as we go.""
> 
> If we were to set up a project documentation, we probably want to use a
> static html builder like Sphinx, MkDocs, or Jekyll. I think it's typical
> for Python projects to use Sphinx. It's really a neat tool, but it's also a
> pretty complex beast, and personally, I find that the default themes are
> really clunky and ugly. I think MkDocs would work just fine and I don't see
> any disadvantage of using Markdown over the restructured text format.
> 
> Once it's setup, it's actually pretty easy to maintain:
> 1. make a change in the markdown file(s)
> 2. view the changes live via mkdocs serve
> 3. build the HTML via mkdocs build --clean
> 4. deploy the changes to Gihub-Pages via mkdocs gh-deploy
> 
> That's basically it.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/32#issuecomment-159689865.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",would happy two take helm project get back development wrote yes also thinking long run would certainly help though start early document go set project documentation probably want use static builder like sphinx think typical python use sphinx really neat tool also pretty complex beast personally find default really ugly think would work fine see disadvantage markdown text format setup actually pretty easy maintain make change markdown file view live via serve build via build clean deploy via basically reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,positive,positive,positive,positive
159689865,"@rhiever Yes, I was also thinking more in terms of ""in the long run."" It would certainly help though to start early and document ""as we go.""

If we were to set up a project documentation, we probably want to use a static html builder like Sphinx, MkDocs, or Jekyll. I think it's typical for Python projects to use Sphinx. It's really a neat tool, but it's also a pretty complex beast, and personally, I find that the default themes are really clunky and ugly. I think MkDocs would work just fine and I don't see any disadvantage of using Markdown over the restructured text format.

Once it's setup, it's actually pretty easy to maintain:
1. make a change in the markdown file(s)
2. view the changes live via `mkdocs serve`
3. build the HTML via `mkdocs build --clean`
4. deploy the changes to Gihub-Pages via `mkdocs gh-deploy`

That's basically it.
",yes also thinking long run would certainly help though start early document go set project documentation probably want use static builder like sphinx think typical python use sphinx really neat tool also pretty complex beast personally find default really ugly think would work fine see disadvantage markdown text format setup actually pretty easy maintain make change markdown file view live via serve build via build clean deploy via basically,issue,positive,positive,positive,positive,positive,positive
159686467,"Hi @rhiever & @rasbt, I am quite interested & motivated by the possibilities & impact potential of TPOT. If possible, I would like to contribute to it and I think starting with the project documentation would be good, if you require? Look forward to hear from you guys. 

Thanks. 
",hi quite interested impact potential possible would like contribute think starting project documentation would good require look forward hear thanks,issue,positive,positive,positive,positive,positive,positive
159675823,"Doesn't hurt to have the web page docs then. I don't think the project is
large enough to merit that yet, but we will probably get there soon.

On Wednesday, November 25, 2015, Sebastian Raschka notifications@github.com
wrote:

> Well, of course you can always put 'everything' into a README file as
> well, but depending on future additions, this README file can become huge
> and user unfriendly. I'd say it's the same reason why people don't build
> websites as 1 large html/text file ...
> I think for larger projects, breaking it -- the documentation -- down into
> logical sections (e.g., one document to list and describe version changes,
> one to document the API, and several ones for tutorials/examples) wouldn't
> hurt.
> I think that a README file is important though, it should certainly
> contain the most important information about a project.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/32#issuecomment-159643964.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",hurt web page think project large enough merit yet probably get soon wrote well course always put file well depending future file become huge user unfriendly say reason people build large file think breaking documentation logical one document list describe version one document several hurt think file important though certainly contain important information project reply directly view postdoctoral researcher institute university twitter,issue,negative,positive,positive,positive,positive,positive
159643964,"Well, of course you can always put 'everything' into a README file as well, but depending on future additions, this README file can become huge and user unfriendly. I'd say it's the same reason why people don't build websites as 1 large html/text file ...
I think for larger projects, breaking it -- the documentation -- down into logical sections (e.g., one document to list and describe version changes, one to document the API, and several ones for tutorials/examples) wouldn't hurt. 
I think that a README file is important though, it should certainly contain the most important information about a project.
",well course always put file well depending future file become huge user unfriendly say reason people build large file think breaking documentation logical one document list describe version one document several would hurt think file important though certainly contain important information project,issue,positive,positive,positive,positive,positive,positive
159613393,"What's the advantage over a standard README? How tough is it to maintain?

On Tuesday, November 24, 2015, Sebastian Raschka notifications@github.com
wrote:

> I was thinking that it may be worthwhile setting up a project
> documentation page other than this github repo -- for example, via Sphinx
> or MkDocs. This would have the advantage to create & organize an API
> documentation and tutorials/examples. I could set up something like at
> http://rasbt.github.io/biopandas/ if you'd find it useful.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/rhiever/tpot/issues/32.

## 

Randal S. Olson, Ph.D.
Postdoctoral Researcher, Institute for Biomedical Informatics
University of Pennsylvania

E-mail: rso@randalolson.com | Twitter: @randal_olson
https://twitter.com/randal_olson
http://www.randalolson.com
",advantage standard tough maintain wrote thinking may setting project documentation page example via sphinx would advantage create organize documentation could set something like find useful reply directly view postdoctoral researcher institute university twitter,issue,positive,positive,neutral,neutral,positive,positive
158985785,"okay sure, makes sense! Was just something I stumbled upon!
",sure sense something upon,issue,negative,positive,positive,positive,positive,positive
158933486,"Hi! Just wanted to acknowledge that I saw your issue and hope to get to it soon. @Chris7 is on the right track with his response.
",hi acknowledge saw issue hope get soon right track response,issue,negative,positive,positive,positive,positive,positive
158933250,"I'd rather pursue the route of requesting that they cite actual papers when using the code. It's unlikely that these code citations will be picked up by Google Scholar, so it's rather pointless to use them until then.
",rather pursue route cite actual code unlikely code picked scholar rather pointless use,issue,negative,negative,negative,negative,negative,negative
158460726,"+1 to this, it would make it even closer to sklearn
",would make even closer,issue,negative,neutral,neutral,neutral,neutral,neutral
158456171,"I looked at doing this briefly. From what I saw:

1) You would need to add new methods for feature selection for continuous data, 
2) You would add a new function for each regressor along with their arguments, and add it to the primitives.

Here's a gist I made implementing an AdaBoostRegressor:
https://gist.github.com/Chris7/d46a57f03ed7507c59e6

Ultimately, TPOT should have a keyword indicate we are using continuous versus labeled data. Quite a bit of the guts need to be reworked for splitting the train/test set and feature selection for continuous data.
",briefly saw would need add new feature selection continuous data would add new function regressor along add gist made ultimately indicate continuous versus data quite bit need reworked splitting set feature selection continuous data,issue,negative,positive,neutral,neutral,positive,positive
157413017,"@rasbt Ok, thought you have a tool which at least generate some of the stuff... I currently use pycharm and it at least generates the structure and the names of the variables...
",thought tool least generate stuff currently use least structure,issue,negative,negative,negative,negative,negative,negative
157409209,"@JanSchulz 
These were all ""manually"" written, I don't think this can be done via a tool (since it need to guess your intention as programmer). But maybe we can automate the type-checking part some day ;) (https://www.python.org/dev/peps/pep-0484/)
",manually written think done via tool since need guess intention programmer maybe part day,issue,negative,neutral,neutral,neutral,neutral,neutral
157406731,"I further recommend `toarray` over `todense`. In practice, this may often not be an issue, but `todense` returns a numpy matrix whereas `toarray` returns a numpy array.

@msjgriffiths Yes, I think sparse matrices shouldn't be a problem anymore -- in the majority of cases. Even the random forest has sparse matrix support now.

> Do you know how this data format interacts with pandas? Right now, TPOT uses pandas to pass around the data sets between pipeline operators.

As far as I can tell you are using numpy arrays as input to the scikit-line pipelines, right?

E.g., 

```
training_features = input_df.loc[input_df['group'] == 'training'].drop(['class', 'group', 'guess'], axis=1).values
```

So, in this case, we could add an intermediate transformer step for the conversion

```
pipe_1 = Pipeline([
    (prep', CountVectorizer(analyzer='word',
                      decode_error='replace',
                      preprocessor=lambda text: re.sub('[^a-zA-Z]', ' ', text.lower()), 
                      stop_words=stopwords,) ),
    ('to_dense', DenseTransformer()),
    ('clf', RandomForestClassifier())
])
```

(Note that it is not necessary for the RandomForest anymore.

And the `DenseTransformer` could simply be:

```
class DenseTransformer(object):
    """"""
    A transformer for scikit-learn's Pipeline class that converts
    a sparse matrix into a dense matrix.
    """"""

    def __init__(self, some_param=True):
        pass

    def transform(self, X, y=None):
        return X.toarray()

    def fit(self, X, y=None):
        return self

    def fit_transform(self, X, y=None):
        return X.toarray()

    def get_params(self, deep=True):
        return {'some_param': True}
```

I think we'd need to check for which scikit transformers this would be necessary, but I could add the general functionality if you like.
",recommend practice may often issue matrix whereas array yes think sparse matrix problem majority even random forest sparse matrix support know data format right pas around data pipeline far tell input right case could add intermediate transformer step conversion pipeline prep text note necessary could simply class object transformer pipeline class sparse matrix dense self pas transform self return fit self return self self return self return true think need check would necessary could add general functionality like,issue,positive,positive,neutral,neutral,positive,positive
157370004,"Do you know how this data format interacts with pandas? Right now, TPOT uses pandas to pass around the data sets between pipeline operators.
",know data format right pas around data pipeline,issue,negative,positive,positive,positive,positive,positive
157218208,"Great! Fix should be in the source now. Please let us know if you run into any more issues, or have any ideas on how to improve TPOT. :+1: 
",great fix source please let u know run improve,issue,positive,positive,positive,positive,positive,positive
157217499,"Thanks. tpot looks quite useful for many things I'm working on.
",thanks quite useful many working,issue,positive,positive,positive,positive,positive,positive
157217083,"Yep. I'm going to fix this by changing

> self.toolbox.register('evaluate', self.evaluate_individual, training_testing_data=training_testing_data)

to

> self.toolbox.register('evaluate', self._evaluate_individual, training_testing_data=training_testing_data)

Will commit against this issue with any additional fixes as well.
",yep going fix commit issue additional well,issue,positive,neutral,neutral,neutral,neutral,neutral
157216608,"Yes, I'm running a cloned copy. Should the function stay private?
",yes running copy function stay private,issue,negative,neutral,neutral,neutral,neutral,neutral
157215748,"Did you install from source when you got this error? I see that one of the function registrations wasn't updated in #23: https://github.com/rhiever/tpot/blob/master/tpot/tpot.py#L162
",install source got error see one function,issue,negative,neutral,neutral,neutral,neutral,neutral
157178920,"> Any idea why the coverage is still showing up as unknown on the repo? I guess it takes a while to update?

May be related to the github caching thing and will show up after time. If you click on the badge, you can see that it already points to the correct page:

![screen shot 2015-11-16 at 4 39 20 pm](https://cloud.githubusercontent.com/assets/5618407/11195584/9de9db8a-8c80-11e5-9af7-ec2fb0c238ca.png)
",idea coverage still showing unknown guess update may related thing show time click badge see already correct page screen shot,issue,negative,negative,neutral,neutral,negative,negative
157177698,"I see. I think it literally just counts the lines of code that are executed during the unit tests divided by the total number of lines
",see think literally code executed unit divided total number,issue,negative,neutral,neutral,neutral,neutral,neutral
157177615,"Any idea why the coverage is still showing up as unknown on the repo? I guess it takes a while to update?
",idea coverage still showing unknown guess update,issue,negative,negative,neutral,neutral,negative,negative
157175520,"I'm amazed the coverage is even 54% right now tbh. Those were just some quick unit tests to get things rolling.
",amazed coverage even right quick unit get rolling,issue,negative,positive,positive,positive,positive,positive
157175144,"Okay, should be all set now and ready to be merged. Wow, but we really have to work a bit on the unit tests (coverage is 54%) :P There were a few things in the code that caught my eye: perfect candidates for refactoring, but that's a task for another day
",set ready wow really work bit unit coverage code caught eye perfect task another day,issue,positive,positive,positive,positive,positive,positive
157174036,"Yeah, sorry, these things are a bit messy, I am not sure if there is a (good) offline way to validate those files
",yeah sorry bit messy sure good way validate,issue,positive,positive,positive,positive,positive,positive
157166273,"Good idea. Don't think I can use the prostate cancer data set (because I can't publicly share that data set), but there's other data sets we can use.
",good idea think use prostate cancer data set ca publicly share data set data use,issue,negative,positive,positive,positive,positive,positive
157153977,"> I'm wondering if the tests should be moved to a separate directory.

I would recommend setting up at least a directory that contains different test files. It's easier to organize the tests this way, and you can selectively test certain tests -- it's cheaper during development if you only need to test things that changed.

e.g.,

```
`nosetests tests/test_somemethod`
```

If you are planning to add submodules, I would recommend moving them into /tpot/tpot and create a separate test dir in each submodule dir -- helps to stay organized
",wondering separate directory would recommend setting least directory different test easier organize way selectively test certain development need test add would recommend moving create separate test stay organized,issue,positive,negative,neutral,neutral,negative,negative
157152641,"Ah yes, that would do it. :-)

I'm wondering if the tests should be moved to a separate directory.
",ah yes would wondering separate directory,issue,negative,neutral,neutral,neutral,neutral,neutral
157149963,"> I imagine the tests are failing because those private methods are no longer available for inspection.

Yes, and I was running nosetests in the tpot/tpot dir so that it actually didn't run the tests.py at all :P
",imagine failing private longer available inspection yes running actually run,issue,negative,positive,positive,positive,positive,positive
157148380,"I imagine the tests are failing because those private methods are no longer available for inspection.
",imagine failing private longer available inspection,issue,negative,positive,positive,positive,positive,positive
157148221,"Hah, weird, somehow my local nosetest didn't complain ... have to look into this ... 
",hah weird somehow local complain look,issue,negative,negative,negative,negative,negative,negative
157138209,"Thanks for all these PRs. This is great.

Yes, I think we should improve the docstrings. Please file an issue for that so we don't forget. :-)
",thanks great yes think improve please file issue forget,issue,positive,positive,positive,positive,positive,positive
157138069,"Gotcha. I think I was confused by their aggressive push on upgrading to the pro version. I think it's set up now.
",think confused aggressive push pro version think set,issue,negative,negative,negative,negative,negative,negative
157137957,"See here: https://coveralls.io/pricing 

> OPEN 
> SOURCE
> Coveralls is and will always be free for open source projects. You can always add a subscription to get coverage reports on private repos later.
",see open source coverall always free open source always add subscription get coverage private later,issue,positive,positive,neutral,neutral,positive,positive
157125224,">  Do you have a sense of how long is ""too long""?

Sry, not really, it's more like coveralls is pretty flaky, sometimes the same stuff works, sometimes it doesn't (I think it has something to do with their hardware utilization on their side). But your tests seem pretty gentle so far, so let's try (I will submit another pull request for that later)
",sense long long really like coverall pretty flaky sometimes stuff work sometimes think something hardware utilization side seem pretty gentle far let try submit another pull request later,issue,positive,positive,positive,positive,positive,positive
157118239,"Perfect. Thanks again @rasbt!

So far, the tests don't take too long because we don't have unit tests on the `fit()` function. Do you have a sense of how long is ""too long""?
",perfect thanks far take long unit fit function sense long long,issue,positive,positive,positive,positive,positive,positive
157113679,"http://blaze.pydata.org/blog/2015/10/19/dask-learn/ Check this out

Dask can also be used to parallelize. 
",check also used parallelize,issue,negative,neutral,neutral,neutral,neutral,neutral
157064240,"@MichaelMarkieta I agree with you; maybe we should start with a simple log file to get it going, and we can later come up with a ""parameter file"" to directly initialize and parameterize the model as I mentioned above.
",agree maybe start simple log file get going later come parameter file directly initialize model,issue,negative,positive,neutral,neutral,positive,positive
157063390,"We could also add coverage results via coveralls later. Was a bit hesitant with this for now since I have bad experience with coveralls for ""expensive"" code (often aborts if evaluations take to long)
",could also add coverage via coverall later bit hesitant since bad experience coverall expensive code often take long,issue,negative,negative,negative,negative,negative,negative
157062896,"I'd have nothing against that as a 99% Python 3.x user, but I would keep 2.7 for now maybe. Yap, it's ready for merge. If you like, you could add more of the older Py 3.x version to the `.travis.yaml` file (in the top directory), currently it's only

```
- PYTHON_VERSION=""2.7"" LATEST=""true"" ""DEAP_VERSION=1.0.1""
- PYTHON_VERSION=""3.4"" LATEST=""true"" ""DEAP_VERSION=1.0.1""
- PYTHON_VERSION=""3.5"" LATEST=""true"" ""DEAP_VERSION=1.0.1
```

thought it suffices.
",nothing python user would keep maybe yap ready merge like could add older version file top directory currently true true true thought,issue,positive,positive,positive,positive,positive,positive
157051278,"Keep in mind the usability of the temp output and what might be possible.... no big deal if its just a log of the current state of the model upon exit, but if you wanted to use that log as some sort parameterized object for specifying a new model run based on the last attempt, or (in some blue-sky thinking way), using the temp output from the previous model to train the new model to skip certain generations....
",keep mind usability temp output might possible big deal log current state model upon exit use log sort object new model run based last attempt thinking way temp output previous model train new model skip certain,issue,negative,positive,neutral,neutral,positive,positive
157050818,"I'm not opposed to dropping 2.6 support. Almost tempted to drop 2.7 support.

Is this ready to merge?
",opposed dropping support almost drop support ready merge,issue,positive,positive,positive,positive,positive,positive
156918958,"Should be all fixed now, needed to create a new pull request, see #19 for details!

>  Does it automatically update the README with the build status?

Yes! :)
",fixed create new pull request see automatically update build status yes,issue,positive,positive,positive,positive,positive,positive
156918409,"Btw. are you really planning to support Python 2.6? Saw it listed in the trove classifier section. Just wanted to ask, because I find Python 2.6 really annoying (`six` module and stuff)
",really support python saw listed trove classifier section ask find python really annoying six module stuff,issue,negative,negative,negative,negative,negative,negative
156917151,"Ha, there we go! I really love green checkmarks ;)
",ha go really love green,issue,positive,positive,positive,positive,positive,positive
156896669,"Let's give this a go. I just set it up and made a push. Does it automatically update the README with the build status?
",let give go set made push automatically update build status,issue,negative,neutral,neutral,neutral,neutral,neutral
156879895,"I suppose I meant that the _resulting lists_ from functions like `filter()` are returned as iterators. lists themselves are treated the same of course.
",suppose meant like filter returned course,issue,negative,neutral,neutral,neutral,neutral,neutral
156879717,"Hm, I don't quite understand!? As far as I know, lists are iteratable in both Python 2 and 3. The only difference I could think if is `range` where `range` in Python 3 is like `xrange` in Python 2. In contrast to Python 2's `range`, the Python 3 range doesn't create a list but is rather like a generator ...
Or that `next` in Python 2 is a method rather than a function such as in Python 3. Anyway, seems that you already fixed that :). 
",quite understand far know python difference could think range range python like python contrast python range python range create list rather like generator next python method rather function python anyway already fixed,issue,positive,positive,neutral,neutral,positive,positive
156543732,"May I ask why you chose this design decision over a regular module? Just an idea, but I'd consider changing it to a standard package/module layout and provide ""example scripts"" and or the command line interfaces extra, which import for the package. What do you think? This way it's much more flexible and you can make different flavors of command line executable scripts. (I am doing this for one of my projects as well, the ""large scale virtual screening"" one. For example, I would have all the base classes and function as a module, and then I have command line scripts such as ""parse database"", ""overlay molecules"" etc.
",may ask chose design decision regular module idea consider standard layout provide example command line extra import package think way much flexible make different command line executable one well large scale virtual screening one example would base class function module command line parse overlay,issue,negative,negative,neutral,neutral,negative,negative
156252864,"jap okay! I think it's not that urgent anyway. As long as your local unittests pass there is actually no need for travis. It's just a convenience service for users to see if the current dev version is ""passing"" and if new contributions are ""passing"". Let's wait until tomorrow then.
",jap think urgent anyway long local pas actually need travis convenience service see current dev version passing new passing let wait tomorrow,issue,negative,positive,neutral,neutral,positive,positive
156227170,"Well, since the repo is probably going public tomorrow, maybe we'll just set it up then?
",well since probably going public tomorrow maybe set,issue,negative,neutral,neutral,neutral,neutral,neutral
156224356,"Okay cool. I will setup the files tonight when I get hope. Should be pretty quick. But you need to do 1 more thing since you are the owner: give travis permission to access your repo for the testing. Just realized there is one little problem: this is a private repo. I am sure that you don't want to pay for travis pro, do you? :D https://travis-ci.com/plans  But I see that there is a trial for ""pro."" Maybe we could use the free trial temporarily until the repo is public and then downgrade? alternatively we could just wait until it's public (but I can already set up the files tonight).
",cool setup tonight get hope pretty quick need thing since owner give travis permission access testing one little problem private sure want pay travis pro see trial pro maybe could use free trial temporarily public downgrade alternatively could wait public already set tonight,issue,positive,positive,positive,positive,positive,positive
156222877,"Unit tests are up. Let's see that shmexy Travis CI integration now, @rasbt? :-)
",unit let see travis integration,issue,negative,neutral,neutral,neutral,neutral,neutral
156189326,"In practice, I think it really doesn't matter ... However, it's probably not a bad idea down the road to ensure compatibility with scikit-learn functions. E.g., maybe you'd want to use it in a wrapper function like `cross_val_score` or so later on. Since it doesn't hurt, I'd recommend renaming it to `fit` :)
",practice think really matter however probably bad idea road ensure compatibility maybe want use wrapper function like later since hurt recommend fit,issue,negative,negative,neutral,neutral,negative,negative
156107869,"Yeah - doing this with nested pipeline objects is going to be a challenge, especially because some of those pipeline objects are functions of custom code. In fact, all of the pipelines are nested functions. I think the saved state should represent that.
",yeah pipeline going challenge especially pipeline custom code fact think saved state represent,issue,positive,neutral,neutral,neutral,neutral,neutral
155985310,"For example, to construct a ""clone,"" we could  first initialize a new object with similar parameter settings. Assuming that we wrote the contents of `lr.get_params()` to a yaml file, and `lr.get_params()` shall represent `yaml.load(file_stream)['parameters']`

```
>>> lr = LogisticRegression(lr.get_params())
>>> lr.get_params()
{'tol': 0.0001, 'max_iter': 100, 'warm_start': False, 'solver': 'liblinear', 'C': 1.0, 'dual': False, 'fit_intercept': True, 'random_state': None, 'n_jobs': 1, 'multi_class': 'ovr', 'verbose': 0, 'class_weight': None, 'intercept_scaling': 1, 'penalty': 'l2'}
```

we could then initialize the new object as

```
>>> lr2 = LogisticRegression(lr.get_params())
```

and to set the ""fitted"" parameters, we just use `setattr`:

```
yaml_cont = yaml.load(file_stream)
for a in yaml_cont['attributes']:
    settattr(lr2, a, yaml_cont['attributes'][a])
```

Practical example:

```
>>> lr.fit([[1], [2], [3]], [0, 1, 1])
>>> setattr(lr, 'coef_', 99.9)
>>> getattr(lr, 'coef_')
```

   99.9

Of course, we need to go a few levels further since we have multiple nested objects in a pipeline, but I think this should not be too difficult.
",example construct clone could first initialize new object similar parameter assuming wrote content file shall represent false false true none none could initialize new object set fitted use practical example course need go since multiple pipeline think difficult,issue,negative,negative,neutral,neutral,negative,negative
155983526,"I'd love to see a demo of this if you have a good solution in mind. There certainly is an issue with model persistency in the command-line version: once the Python call ends, the model is gone.

My one request is that we try to avoid adding more external dependencies. We already have two major external dependencies (scikit-learn and DEAP), and I'm wary of adding more.
",love see good solution mind certainly issue model persistency version python call model gone one request try avoid external already two major external wary,issue,positive,positive,positive,positive,positive,positive
155978686,"I actually need to work through the pipeline operator code and probably remove all the copy() operations. I think they're unnecessary, but I'm actually not sure if the DataFrames are being passed by copy or reference.
",actually need work pipeline operator code probably remove copy think unnecessary actually sure copy reference,issue,negative,positive,neutral,neutral,positive,positive
155978215,"Agreed! Plus, the training set may already be copied a bunch of times already (unless you disable the mutliprocessing in certain pipeline objects)
",agreed plus training set may already copied bunch time already unless disable certain pipeline,issue,negative,positive,positive,positive,positive,positive
155976178,"Actually, this is a bad idea. It can be disastrous to have multiple copies of a very large data set.
",actually bad idea disastrous multiple large data set,issue,negative,negative,negative,negative,negative,negative
155967930,"Okay, maybe -- in far future -- it would be worthwhile adding a feature for accepting a custom scorer via scikit-learn (since everything depends on scikit-learn) so that the user can choose whatever performance metric, positive label, and ""greater is better: True/False"" setting the user may prefer. 
",maybe far future would feature custom scorer via since everything user choose whatever performance metric positive label greater better setting user may prefer,issue,positive,positive,positive,positive,positive,positive
155967081,"Yes that's right. That's how it worked previously.
",yes right worked previously,issue,negative,positive,neutral,neutral,positive,positive
155964587,"Related to that, the ""positive"" class label is always always assumed to be 1, right? (E.g., for computing metrics other than accuracy like precision, recall, F1 and so forth)? 
",related positive class label always always assumed right metric accuracy like precision recall forth,issue,positive,positive,positive,positive,positive,positive
155906412,"Yes, I think pickle would generally be more convenient since you wouldn't have to worry about the structure of the parameter files etc. However, I think having a parameter file would be better for compatibility (e.g., python 2 vs 3, different pickle protocols etc.). I think that pickle is fine if you are working only on one machine, but for record keeping, reproducibility, and sharing, a parameter file in a simple, human readable format like yaml would be much better.
",yes think pickle would generally convenient since would worry structure parameter however think parameter file would better compatibility python different pickle think pickle fine working one machine record keeping reproducibility parameter file simple human readable format like would much better,issue,positive,positive,positive,positive,positive,positive
155905175,"Sure, I can deal with the Travis stuff once the unit tests are there. It should be pretty straightforward. You just need to implement the unit tests in a way that they can be executed by e.g., running `nosetests` from the `tpot` main directory
",sure deal travis stuff unit pretty straightforward need implement unit way executed running main directory,issue,positive,positive,positive,positive,positive,positive
155904518,"What about just `pickle`ing the model? Haven't tested `pickle` with DEAP, but in theory that would make life easier.
",pickle ing model tested pickle theory would make life easier,issue,negative,neutral,neutral,neutral,neutral,neutral
155904052,"I'm building some basic unit tests. @rasbt, how should unit tests be written to integrate with Travis CI? Can you lead the way with Travis CI integration?
",building basic unit unit written integrate travis lead way travis integration,issue,negative,neutral,neutral,neutral,neutral,neutral
155903750,"@rhiever Yes, that's a bit tricky. I would suggest adding an default option for writing two files
-  `model_param.yaml`
- and `model_param.yaml.tmp`

on the fly. After each iteration, we write the current parameters to the  `model_param.yaml.tmp`; then we use  `model_param.yaml.tmp` to overwrite `model_param.yaml`; I think this 2-step approach is safer (accounting for rare scenarios where the job quits while writing one of the files).

Okay, this sounds pretty complex right? However, writing small files in Python is pretty quick (especially compared to one iteration in the deap algo), and it wouldn't really impact the computational efficiency.

The idea is to only store the parameters to these yaml files that are essential for reconstructing the last state of the model. I can see several reasons why this is useful
- avoid starting from scratch if the job crashed
- run  additional iterations if the results are not satisfactory
- re-use parameters from other models that may come in handy in related projects
- having a record of the experiment

I would therefore suggest implementing a `dump_params` method (or function)

```
 model.dump_params('current_state.yaml')
```

that can be called in each iteration in the pipeline evaluation by default. 

```
 # run experiment
 for x in range(something):
     evolve_model()
     model.dump_params('current_state.yaml.tmp')
     copyfile(''current_state.yaml.tmp', 'current_state.yaml')
```

And the load method

```
new_model = XXX()
new_model.load_params('current_state.yaml')
```
",yes bit tricky would suggest default option writing two fly iteration write current use overwrite think approach accounting rare job quits writing one pretty complex right however writing small python pretty quick especially one iteration would really impact computational efficiency idea store essential last state model see several useful avoid starting scratch job run additional satisfactory may come handy related record experiment would therefore suggest method function iteration pipeline evaluation default run experiment range something load method,issue,positive,positive,positive,positive,positive,positive
155877751,"@rasbt, I don't think there's much we can do here wrt early termination from running out on time on a PBS job. AFAIK those terminations immediately kill the job, and there's no way to gracefully exit the program at that point.

However, we can certainly catch a keyboard interrupt and store the best discovered pipeline so far.
",think much early termination running time job immediately kill job way gracefully exit program point however certainly catch keyboard interrupt store best discovered pipeline far,issue,negative,positive,positive,positive,positive,positive
