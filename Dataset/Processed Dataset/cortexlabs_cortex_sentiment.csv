id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
1635445604,"We also came across your project and noticed that it contains a vulnerability (CVE-2023-25153). In your project, the github.com/containerd/containerd package is being used at version github.com/containerd/containerd v1.5.4, but the patched version is v1.6.18. To fix the vulnerability, we recommend modifying the go.mod file to update the version to v1.6.18 or higher. Thank you for your attention to this matter.",also came across project vulnerability project package used version version fix vulnerability recommend file update version higher thank attention matter,issue,negative,positive,positive,positive,positive,positive
1634198320,"we came across your project and noticed that it contains a vulnerability (CVE-2022-32149). In your project, the golang.org/x/text package is being used at version golang.org/x/text v0.3.7, but the patched version is v0.3.8. To fix the vulnerability, we recommend modifying the go.mod file to update the version to v0.3.8 or higher. Thank you for your attention to this matter.",came across project vulnerability project package used version version fix vulnerability recommend file update version higher thank attention matter,issue,negative,positive,positive,positive,positive,positive
1589541781,Awesome work üòé thanks for keeping the project alive! I'll test this out on our setup this week.,awesome work thanks keeping project alive test setup week,issue,positive,positive,positive,positive,positive,positive
1544646930,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/cortexlabs/cortex?pullRequest=2457) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/cortexlabs/cortex?pullRequest=2457) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/cortexlabs/cortex?pullRequest=2457) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept sub already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
1432465739,Just a heads-up: I'm still running into this in v0.42.1 on macOS.  Setting the env variable as per the PR fixes it.,still running setting variable per,issue,negative,neutral,neutral,neutral,neutral,neutral
1225497614,What is the recommended solution for this issue ? I want to implement something similar.,solution issue want implement something similar,issue,negative,neutral,neutral,neutral,neutral,neutral
1133256688,hi @riquellopes no we don't - you might be able to set it up yourself if they support Prometheus metrics,hi might able set support metric,issue,negative,positive,positive,positive,positive,positive
1106823964,@creatorrr did you find a solution yet? Can you share anything? I'm having a deep interest in this feature as well üëç ,find solution yet share anything deep interest feature well,issue,positive,neutral,neutral,neutral,neutral,neutral
1105430640,"Yes, you will have to fiddle around with the Prometheus crds and operator. We dropped all metrics which we weren‚Äôt showing in the default dashboards by default because the volume of metrics was causing issues in larger clusters. This could be mitigated by setting a larger Prometheus instance type in the cortex cluster config.",yes fiddle around operator metric showing default default volume metric causing could setting instance type cortex cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1105426608,This is not currently supported. I will leave this issue open as a feature request ,currently leave issue open feature request,issue,negative,neutral,neutral,neutral,neutral,neutral
1063685650,"Unfortunately we are still waiting for a new release of the [AWS CNI](https://github.com/aws/amazon-vpc-cni-k8s) which supports g5 instances. Once that happens, it should be straightforward to add support in Cortex.",unfortunately still waiting new release straightforward add support cortex,issue,negative,positive,neutral,neutral,positive,positive
1051718386,"I just tried creating a new cluster with the cluster configuration you provided, and it worked for me in us-west-2. I ran this from the master branch, but there have not been any changes that should affect the cluster creation process since the v0.42.0 release. Do you mind trying again?",tried new cluster cluster configuration provided worked ran master branch affect cluster creation process since release mind trying,issue,negative,positive,positive,positive,positive,positive
1033105356,"For custom metrics exporting via statsd, the metrics name must start with `cortex_`.

Found in prometheus config,
```
- job_name: serviceMonitor/prometheus/operator/0
...
  metric_relabel_configs:
  - source_labels: [__name__]
    separator: ;
    regex: cortex_(.+)
    replacement: $1
    action: keep
```

```
import statsd

metrics = statsd.StatsClient(host='prometheus-statsd-exporter.prometheus.svc.cluster.local', port=9125)

n_clean = 0
n_error = 0

n_clean += 1

with metrics.pipeline() as pipe:
    pipe.incr(stat=""cortex_api_n_clean"" count=n_clean)
    pipe.incr(stat=""cortex_api_n_error"", count=n_error)
```

Now the metrics are shown correctly in grafana
",custom metric via metric name must start found separator replacement action keep import metric pipe metric shown correctly,issue,negative,positive,neutral,neutral,positive,positive
1032815861,I am using AWS elasticache based on redis for this. It would be nice to deploy the cache with `cortex cluster up`,based would nice deploy cache cortex cluster,issue,negative,positive,positive,positive,positive,positive
1027305532,@deliahu Apologies for dripping these changes in - I think this is the last! I built the cli locally with `make cli` and it's working perfectly for me now.,dripping think last built locally make working perfectly,issue,positive,positive,positive,positive,positive,positive
1027266685,"Thanks for the fix!

We are planning on releasing v0.42.1 soon, but we do have a strict version check for the cli <> operator API (since we don't guarantee backwards compatibility yet). I do think it would make sense to relax the validation to just check that the major/minor versions match. In the meantime, the easiest thing to do would be to cherry-pick the fix on top of the v0.42.0 tag, and then build the CLI. It should be straightforward for you to do in the future if you need to, but for this fix here is the [linux cli](https://get-cortex-dev.s3.us-west-2.amazonaws.com/jack/0.42.0/cli/linux/cortex) and here is the [darwin cli](https://get-cortex-dev.s3.us-west-2.amazonaws.com/jack/0.42.0/cli/darwin/cortex).",thanks fix soon strict version check operator since guarantee backwards compatibility yet think would make sense relax validation check match easiest thing would fix top tag build straightforward future need fix,issue,positive,positive,positive,positive,positive,positive
1027194947,Would it be possible to include this in a `0.42.1` version of the CLI which I can use against my `0.42.0` Cortex cluster?,would possible include version use cortex cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
1021466039,"Thanks for the PR!

It looks good to me. I made a few minor tweaks, please take a look / test it out, and let me know if it all looks good on your end, at which point I'll merge it in!",thanks good made minor please take look test let know good end point merge,issue,positive,positive,positive,positive,positive,positive
1020850701,"> LGTM, thanks!

I really appreciate you and your team's awesome work! thanks",thanks really appreciate team awesome work thanks,issue,positive,positive,positive,positive,positive,positive
1015915711,"As an update on this, form data is supported for making the request. Only JSON data is supported for the API's response.",update form data making request data response,issue,negative,neutral,neutral,neutral,neutral,neutral
1014844055,"Only JSON is currently supported. As a workaround, If you need to pass some sort of binary data to the API you can upload it first to s3, send the URI to that file in the payload, and process it in the API.",currently need pas sort binary data first send file process,issue,negative,positive,positive,positive,positive,positive
1001572560,@miguelvr  Any pointers on how to do this with triton? Just set the instance_groups settings and ELB will automatically see what's available?,triton set elb automatically see available,issue,negative,positive,positive,positive,positive,positive
991173537,"I just tested this and I observe that my requests timeout when a spot instance node goes down. For testing I use locust to put load on the instance and I use [AWS Fault Injector Simulator](https://aws.amazon.com/fis/) to provoke spot termination. 
Maybe AWS's spot termination handler can fix the issue.

As a workaround I got some success by patching the virtualservice for the API to use a retry:
``` json
     {
        ""match"": [
          {
            ""uri"": {
              ""prefix"": ""/myapi/""
            }
          }
        ],
        ""retries"": {
          ""attempts"": 2,
          ""retryOn"": ""gateway-error,connect-failure,refused-stream,reset"",
          ""perTryTimeout"": ""10s""
        },
        ""rewrite"": {
          ""uri"": ""/""
        },
 ```
https://istio.io/latest/docs/reference/config/networking/virtual-service/#HTTPRetry",tested observe spot instance node go testing use locust put load instance use fault injector simulator provoke spot termination maybe spot termination handler fix issue got success use retry match prefix reset rewrite,issue,negative,positive,positive,positive,positive,positive
984153305,"@tfriedel awesome, thanks for bringing this to our attention, and for confirming that the fix is working as intended!

I'm working on a separate PR now to support custom preStop commands.",awesome thanks attention confirming fix working intended working separate support custom,issue,positive,positive,positive,positive,positive,positive
984122638,"Thanks for the PR! I built the images and tried my test (route which sleeps 1 sec). Contrary to the previous verion, deleting a pod did not result in 503s. Good job!",thanks built tried test route sec contrary previous pod result good job,issue,positive,positive,positive,positive,positive,positive
983993692,"@miguelvr yeah, that would be cleaner!

I just tried adding it there, but it didn't work as we expected. So I dug into it, and realized that we weren't handling the TERM signal (just INT). After listening for TERM too, checking the breaker's in-flight count was no longer necessary, since `server.Shutdown()` seems to handle this automatically.

What are your thoughts on the diff now?",yeah would cleaner tried work dug handling term signal listening term breaker count longer necessary since handle automatically,issue,negative,neutral,neutral,neutral,neutral,neutral
983957493,"> I think we can achieve the same without adding an endpoint. You can simply wait for in-flight requests when the proxy gets the `SIGTERM` signal and before calling `server.Shutdown()`

Ref to the code:
https://github.com/cortexlabs/cortex/blob/16ffd084fc2fe1448a2f2bd6cad14861562b76ee/cmd/proxy/main.go#L172-L189",think achieve without simply wait proxy signal calling ref code,issue,negative,neutral,neutral,neutral,neutral,neutral
933096791,"the `operator-controller-manager` is responsible for the cleanup of all the resources, so if it starts failing, it requires a lot of intervention.

The first thing I would try is If the `operator-controller-manager` is getting OOMKilled, is to increase its memory limits.

If that doesn't work, there are ways to ""fix"" that weird state, but still require a lot of manual intervention, or eventually an automated script.

When you create a BatchAPI job this happens:
1. A `BatchJob` kubernetes resource is created
2. The `operator-controller-manager` creates / updates / deletes the required resources referring to that `BatchJob` resource.

In order to fix that weird state you have to:
1. Delete the created `BatchJob` resources from the cluster using `kubectl delete` with the `--force` flag
2. Delete all the created SQS queues manually or with a script
3. Delete S3 resources that might have been created for that `BatchJob` resource

",responsible cleanup failing lot intervention first thing would try getting increase memory work way fix weird state still require lot manual intervention eventually script create job resource resource order fix weird state delete cluster delete force flag delete manually script delete might resource,issue,negative,negative,negative,negative,negative,negative
926948472,@eaedk the best way to improve this would be to build your docker images with your dependencies already installed. I can send the relevant docs if you aren't sure how to do this; which version of cortex are you using?,best way improve would build docker already send relevant sure version cortex,issue,positive,positive,positive,positive,positive,positive
917050850,"@dipen-epi I believe this was reverted because it was due to a bug in AWS's AMI, which was resolved fairly quickly by AWS",believe due bug ami resolved fairly quickly,issue,negative,positive,positive,positive,positive,positive
915827715,"@vishalbollu why was this change reverted?
We're using cortex and facing the same issue of gpus not joining the cluster - probably because we're using a `overrideBootstrapCommand` in the manager image
The logs look pretty similar to the issue you linked

```
[[32m  OK  [0m] Started Docker Application Container Engine.
[   79.501444] cloud-init[4890]: Created symlink from /etc/systemd/system/multi-user.target.wants/kubelet.service to /etc/systemd/system/kubelet.service.
[[1;31mFAILED[0m] Failed to start Kubernetes Kubelet.
See 'systemctl status kubelet.service' for details.
[   79.571532] cloud-init[4890]: Job for kubelet.service failed because a configured resource limit was exceeded. See ""systemctl status kubelet.service"" and ""journalctl -xe"" for details.
[   79.573805] cloud-init[4890]: Exited with error on line 437
[   79.574571] cloud-init[4890]: Sep 08 09:29:18 cloud-init[4890]: util.py[WARNING]: Failed running /var/lib/cloud/instance/scripts/runcmd [1]
[   79.576214] cloud-init[4890]: Sep 08 09:29:18 cloud-init[4890]: cc_scripts_user.py[WARNING]: Failed to run module scripts-user (scripts in /var/lib/cloud/instance/scripts)
[   79.576479] cloud-init[4890]: Sep 08 09:29:18 cloud-init[4890]: util.py[WARNING]: Running module scripts-user (<module 'cloudinit.config.cc_scripts_user' from '/usr/lib/python2.7/site-packages/cloudinit/config/cc_scripts_user.pyc'>) failed
[   79.625532] cloud-init[4890]: Cloud-init v. 19.3-44.amzn2 finished at Wed, 08 Sep 2021 09:29:18 +0000. Datasource DataSourceEc2.  Up 79.62 seconds
[[1;31mFAILED[0m] Failed to start Execute cloud user/final scripts.
```",change cortex facing issue joining cluster probably manager image look pretty similar issue linked docker application container engine start see status job resource limit see status error line warning running warning run module warning running module module finished wed start execute cloud,issue,negative,positive,positive,positive,positive,positive
913737954,"The best way would be to create GitHub issues (please include the relevant context/motivation in the ticket), and then we'll prioritize accordingly and I can keep you posted regarding timelines",best way would create please include relevant ticket accordingly keep posted regarding,issue,positive,positive,positive,positive,positive,positive
907575412,It always fails quickly. and I could guess the reason due to your sufficient and kind explanation. Thanks for your help!,always quickly could guess reason due sufficient kind explanation thanks help,issue,positive,positive,positive,positive,positive,positive
907572967,"Does it fail quickly and consistently, or is it sporadic and takes some time?

If it's the former, it could be a misconfiguration with your probe. They way you've specified it, an HTTP GET to `/` must return within 1 second to succeed (otherwise the container will be terminated).

If it's the latter, one possibility is that the server that's running in your container is not configured to use enough concurrency to respond to the liveness probe on `/` while also handling the batch requests and the readiness probes.",fail quickly consistently sporadic time former could misconfiguration probe way get must return within second succeed otherwise container latter one possibility server running container use enough concurrency respond liveness probe also handling batch readiness,issue,negative,negative,neutral,neutral,negative,negative
907290536,I may use some query excluding that message using AWS Log Insights. thanks,may use query excluding message log thanks,issue,negative,positive,positive,positive,positive,positive
906866579,"By the way, what‚Äôs the best way to measure time spent by the service in different stages during startup? Just looking at the logs?",way best way measure time spent service different looking,issue,positive,positive,positive,positive,positive,positive
906866036,Yup yup. Basically because we bundle ML models within the image itself so it‚Äôs not the runtime that‚Äôs causing the bloat (although that could use some optimisation too). We have tried using better compression but that didn‚Äôt yield significant benefits.  ,basically bundle within image causing bloat although could use tried better compression yield significant,issue,positive,positive,positive,positive,positive,positive
906718302,"@creatorrr just a passing thought: depending on whether you run Python code in your container or not, you might be able to reduce the size of your image considerably with a tool like https://github.com/google/subpar. Haven't used it yet, just stumbled upon it a few days ago. 16GB is a _heck_ of a lot!",passing thought depending whether run python code container might able reduce size image considerably tool like used yet upon day ago lot,issue,negative,positive,positive,positive,positive,positive
906713429,It would be very interesting to see if this is indeed faster. We use fairly large images (average around 16GB) and the network overhead takes a heavy load on startup,would interesting see indeed faster use fairly large average around network overhead heavy load,issue,negative,positive,neutral,neutral,positive,positive
903260026,"Cortex now deploys docker containers built by the users.

So you just need to do `docker run <container_name>` to run locally",cortex docker built need docker run run locally,issue,negative,neutral,neutral,neutral,neutral,neutral
903258702,@vishalbollu it looks like this has been removed again. At least I can't find it on the current release (0.40.0) nor in the documentation. What is the current approach to running a local setup (e.g. for testing purposes or even for those with a local GPU cluster)? Thanks,like removed least ca find current release documentation current approach running local setup testing even local cluster thanks,issue,positive,negative,neutral,neutral,negative,negative
899768249,"@wise-east There are some examples in our test folder, which could be helpful: https://github.com/cortexlabs/cortex/tree/master/test/apis

In terms of the Docker workflow, I agree that we could help show some examples for how to do this. For any of our examples, it should be just two commands; from a directory with a Dockerfile, it would be:

```bash
docker build . -f <dockerfile_path> -t <image_url>
```

followed by:

```bash
docker push <image_url>
```

`<dockerfile_path>` is the name of a Dockerfile (e.g. `cpu.Dockerfile`), and `<image_url>` is the URL for your image repository (e.g. `764403040460.dkr.ecr.us-west-2.amazonaws.com/cortexlabs/realtime-sleep-cpu` for EKS, `docker.io/cortexlabs/realtime-sleep-cpu` for Docker Hub, or `quay.io/cortexlabs/realtime-sleep-cpu` for quay).",test folder could helpful docker agree could help show two directory would bash docker build bash docker push name image repository docker hub quay,issue,positive,neutral,neutral,neutral,neutral,neutral
899172338,"I also agree that the previous versions (<=0.35) were better suited for my use case and was much easier to use, but I understand why the move to being language agnostic makes more sense in the longer run.

It would be nice though, to show how to set up previous example APIs using cortex with versions >0.35. I am not that great with Docker and it seems like the documentation has become very succinct on how to set up APIs with the new version. This would help me transition to the newer versions and also I am sure it will be helpful for many others who have used the previous versions of cortex. ",also agree previous better use case much easier use understand move language agnostic sense longer run would nice though show set previous example cortex great docker like documentation become succinct set new version would help transition also sure helpful many used previous cortex,issue,positive,positive,positive,positive,positive,positive
898603190,"That is a solid solution, I was worried about the complexity of pulling in a whole templeting engine but `envsubst` would be elegant for what I'm trying to do. Thanks! ",solid solution worried complexity whole engine would elegant trying thanks,issue,positive,positive,positive,positive,positive,positive
897801582,"@sevro the best way to accomplish what you're trying to do would be do use some sort of templating to generate `cluster.yaml` files. There are a few ways you can do this. For example you could have a `cluster.yaml` that looks like this:

```yaml
cluster_name: cortex-${ENV_NAME}
region: us-west-2
...
```

And then in your CI environment, you can set the `ENV_NAME` environment variable to dev/staging/prod, and then use `envsubst` to generate the files (e.g. something like `envsubst < cluster-template.yaml > cluster-prod.yaml`.

Alternatively, you can use something like Jinja, or you could write a Python script which takes in an argument for the name of your environment and writes the appropriate `cluster.yaml` file (or it could take no args and just write all three).",best way accomplish trying would use sort generate way example could like region environment set environment variable use generate something like alternatively use something like jinja could write python script argument name environment appropriate file could take write three,issue,positive,positive,positive,positive,positive,positive
897039767,"Also I am currently on version `0.35` if that makes a difference, but I see no difference in the docs about this.",also currently version difference see difference,issue,negative,neutral,neutral,neutral,neutral,neutral
893018870,"The response has to be JSON, this is likely never going to change, since the gateway is what responds.

If the user needs a binary format, it should be saved somewhere and return a URL",response likely never going change since gateway user need binary format saved somewhere return,issue,negative,neutral,neutral,neutral,neutral,neutral
892080680,"Ran the E2E test and it seems to be passing (ran from my devbox).

Oddly enough, it doesn't seem to be passing in https://app.circleci.com/pipelines/github/cortexlabs/cortex/11294/workflows/2ee47280-4f7f-4697-8e6f-511e1e2ae6c2/jobs/15289.",ran test passing ran oddly enough seem passing,issue,negative,neutral,neutral,neutral,neutral,neutral
891991882,"I think this might break the scale to zero tests, but I'm not sure",think might break scale zero sure,issue,negative,positive,positive,positive,positive,positive
890412357,"In the case of zeros being allowed, then a pointer should be used to avoid the defaulting.",case pointer used avoid,issue,negative,neutral,neutral,neutral,neutral,neutral
890411359,"The controller is 100% independent from the ""cortex operator"" it shouldn't rely on specific defaults to be set. Therefore it needs to work by applying a Kubernetes manifest. This is how the  development of the controller should look like.

If there are no proper defaults set, the controller will try to access stuff that is expected to be set and it will lead to nil pointer dereferences. Tried this and saw it failing.

Whatever you saw failing, it needs to be fixed with the correct annotations, not by removing all the defaulting ones",controller independent cortex operator rely specific set therefore need work manifest development controller look like proper set controller try access stuff set lead nil pointer tried saw failing whatever saw failing need fixed correct removing,issue,negative,positive,neutral,neutral,positive,positive
890371025,"Sure, the operator defaults, but that doesn't matter as long as it still sets the value to zero (whichever field that is). The outcome is that the functionality is broken. It just doesn't work. Tested it and it fails.

> By removing them, the controller does not work properly without the operator.

Where exactly does the controller not work properly? I have tested it and it seems to work well. Could you please provide a granular explanation as to where this is required?

>  I spent quite a while figuring out the correct defaulting scheme

Don't see why they would be required as long as we don't do require the users to create their realtimeapi deployments directly. Could you expand on this one?

Pertaining to the above paragraph, we concluded last night that we wouldn't be expecting the user to create a realtimeapi resource directly using a subset of the available fields in the api spec. We expect the operator to be the sole creator of these resources (for now). This becomes important only when we'd expect the user to do this (i.e. if we were to remove the operator/cli).",sure operator matter long still value zero whichever field outcome functionality broken work tested removing controller work properly without operator exactly controller work properly tested work well could please provide granular explanation spent quite correct scheme see would long require create directly could expand one pertaining paragraph last night would user create resource directly subset available spec expect operator sole creator becomes important expect user remove,issue,positive,positive,neutral,neutral,positive,positive
890362156,"There's no chance the user has to set anything because the operator defaults everything. By removing them, the controller does not work properly without the operator. This is not good for different reasons, but the main one is development. I spent quite a while figuring out the correct defaulting scheme",chance user set anything operator everything removing controller work properly without operator good different main one development spent quite correct scheme,issue,negative,positive,positive,positive,positive,positive
890359745,"@miguelvr the Kubebuilder default annotations were removed for a good reason. With them in there, there was a decent chance that if the user had set a value to 0 (say `min_replicas` for instance, could be any other that allows 0 as a value), then the controller would step in and give it the default value, which is really bad because the operator would no longer be able to download the API spec - they would no longer match because the controller deliberately changed the spec.

And couldn't just double-comment them because they would then appear in the description of each field, which is also bad. They are definitely necessary if we weren't to have the CLI/operator and if the user were to create the RealtimeAPI resource(s) directly, yes. But until then, I'm not seeing a good reason for having them around, not as long as they also break the functionality.

The controller seems to be working well, no issues with it (as of this moment). Where are these annotations required?",default removed good reason decent chance user set value say instance could value controller would step give default value really bad operator would longer able spec would longer match controller deliberately spec could would appear description field also bad definitely necessary user create resource directly yes seeing good reason around long also break functionality controller working well moment,issue,positive,positive,neutral,neutral,positive,positive
890337860,You removed the kubebuilder default annotations I had. Those were intentional and necessary for the controller to work well. Please revert,removed default intentional necessary controller work well please revert,issue,positive,neutral,neutral,neutral,neutral,neutral
890297825,@vishalbollu Yeah that sounds like a fair assessment toward ensuring host-terminated TLS with managed certificates via AWS Certificate Manager.,yeah like fair assessment toward via certificate manager,issue,positive,positive,positive,positive,positive,positive
890293820,"Yup @RobertLucian . Your snippet works for me too. I think it may have something to do with nvidia driver version. Closing for now, will report back when I have updates",snippet work think may something driver version report back,issue,negative,neutral,neutral,neutral,neutral,neutral
890283992,"@creatorrr I've run the following API on the latest version of Cortex and it seems to be working for me:

```yaml
- name: pytorch
  kind: RealtimeAPI
  pod:
    containers:
    - name: api
      image: pytorch/pytorch:1.9.0-cuda11.1-cudnn8-runtime
      command:
        - /bin/bash
        - ""-c""
        - |
          python - << EOF
          import socket, sys, os, time
          import threading as td

          def start_health_probe():
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            server_address = (""localhost"", int(os.getenv(""CORTEX_PORT"", ""8080"")))
            print(f'starting up on %s port {server_address}')
            sock.bind(server_address)
            sock.listen(1)
            while True:
                connection, client_address = sock.accept()
          td.Thread(target=start_health_probe).start()

          import torch
          while True:
            if torch.cuda.is_available():
              print(""gpu(s) are available"")
            else:
              print(""no available gpu"")
            time.sleep(10)
          EOF
      env:
        PYTHONUNBUFFERED: ""1""
      compute:
        cpu: 200m
        mem: 128Mi
        gpu: 1
```

Running the above deployment will print `""gpu(s) are available""` every 10 seconds. Ran this on a `g4dx.xlarge` node. Does this work for you?",run following latest version cortex working name kind pod name image command python import socket o time import sock print port true connection import torch true print available else print available compute mem mi running deployment print available every ran node work,issue,positive,positive,positive,positive,positive,positive
890028719,"Since the node has the GPU properly configured, could it be an issue with the scheduler configuration?

cc/ @deliahu plz halp! ",since node properly could issue configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
890028187,"`ssh`d into one of the GPU nodes of the cluster and ran `nvidia-smi`. Output:

```bash
[root@ip-10-0-108-88 ~]# nvidia-smi
Fri Jul 30 16:58:53 2021
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   26C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

```",one cluster ran output bash root driver version version name volatile fan temp compute mig mib mib default type process name memory id id usage running found,issue,negative,neutral,neutral,neutral,neutral,neutral
888662051,"The automatic management of certs, although related, could be a separate feature. It sounds like that may not necessarily help your use case.

It looks like the minimum work that Cortex can do to support your use case is:
* Route HTTPS traffic to the pod
* Provide a way to mount kubernetes secrets containing the certificates managed by you to your pod. Containers in the pod, either Cortex's proxy or your own proxy side car can use this certificate to decrypt HTTPS traffic.

Let me know if this is a fair assessment.",automatic management although related could separate feature like may necessarily help use case like minimum work cortex support use case route traffic pod provide way mount pod pod either cortex proxy proxy side car use certificate traffic let know fair assessment,issue,positive,positive,positive,positive,positive,positive
888514898,"We should also confirm the following 2 things:

1. That updating the requested replicas for an API (through the tweaking of `min_replicas`/`max_replicas`) has the same behavior as what's currently on master.
1. That re-deploying an API (realtime) that hasn't had its API spec updated doesn't trigger a rolling-update process, as it's normally expected from the refresh command.",also confirm following behavior currently master spec trigger process normally refresh command,issue,negative,positive,neutral,neutral,positive,positive
888500060,"Hi @vishalbollu,

I will have to do a bit more research to see if this approach would work for us. There may be concerns with usage of Let's encrypt and we may want to use our own API proxy pod.

Another alternative could be to terminate SSL in the istio pod but then resume another encrypted connection between isto and the API pod (potentially using mTLS). I will also have to research if we could use this approach.

Thank you for looking into this.",hi bit research see approach would work u may usage let encrypt may want use proxy pod another alternative could terminate pod resume another connection pod potentially also research could use approach thank looking,issue,negative,neutral,neutral,neutral,neutral,neutral
888400735,"Thanks for sharing the context. 

It might be possible for Cortex to automatically issue and certificates. Here is one potential design:
* use cert-manager in the cluster to issue a certificate using let's encrypt
* configure the cluster to satisfy the HTTP-01 challenge
* mount certificate into pod and configure Cortex's proxy to decrypt HTTPS using the certificate and forward HTTP requests

This design makes a few assumptions:
* one domain per cluster, the certificate is shared across multiple APIs
* let's encrypt will be used as the certificate issuer
* HTTP-01 challenge will be used

Would this design satisfy your use case or does your use case require more fine-grained control in the certificate issuance and management.",thanks context might possible cortex automatically issue one potential design use cluster issue certificate let encrypt configure cluster satisfy challenge mount certificate pod configure cortex proxy certificate forward design one domain per cluster certificate across multiple let encrypt used certificate issuer challenge used would design satisfy use case use case require control certificate issuance management,issue,positive,positive,neutral,neutral,positive,positive
887967552,"If the `API pod (proxy side car)` is always running on the same EC2 node as `API pod (your container)` then that would be the earliest possible point for TLS termination in our use case. 

We do use an internal NLB and run ec2 nodes private subnets but still require end-to-end encryption of our data.",pod proxy side car always running node pod container would possible point termination use case use internal run private still require encryption data,issue,negative,neutral,neutral,neutral,neutral,neutral
887962654,"At this point TLS termination at the pod isn't supported because various network hops within the cluster assume that the requests being routed are HTTP. Ingress gateway pods augment requests with headers for internal routing and the proxy sidecar attached to Realtime APIs assumes that the requests are HTTP.

Here is a rough overview of the network hops currently and where HTTPS is decrypted:

client --HTTPS--> NLB (decrypted) --HTTP--> cluster nodes --HTTP--> istio ingress pod --HTTP--> API pod (proxy side car) --HTTP--> API pod (your container)

If we were to move the TLS termination to istio ingress pod which lives inside cluster, would that satisfy your use case? If not, where is the earliest networking hop where TLS termination can occur in your use case?

Would any of the other cluster configuration settings change the your answer to the previous question? For example, you can set the NLB to be internal, force the ec2 nodes to be scheduled in private subnets.",point termination pod various network within cluster assume ingres gateway augment internal routing proxy sidecar attached rough overview network currently client cluster ingres pod pod proxy side car pod container move termination ingres pod inside cluster would satisfy use case hop termination occur use case would cluster configuration change answer previous question example set internal force private,issue,negative,negative,neutral,neutral,negative,negative
887905113,"@deliahu I'm no k8s expert, but talking to some k8s folks offline, it seems the following may need some updates to move TLS-termination from the NLB to the pod:

https://github.com/cortexlabs/cortex/blob/67c0b52001bcbcc9d251fb6066594119ff9f7c64/manager/manifests/istio.yaml.j2#L105

https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/

and that we'd need to get the TLS cert+key into a k8s Secret so the pods can access it, using cert-manager.",expert talking following may need move pod need get secret access,issue,negative,negative,negative,negative,negative,negative
887826335,"Hi @deliahu while adding a ACM certificates to the API load balancer is a good start, we are looking to encrypt the traffic between the load balancer and the cluster nodes to ensure traffic is decrypted in the API pod. Does Cortex provide support for this?",hi load balancer good start looking encrypt traffic load balancer cluster ensure traffic pod cortex provide support,issue,positive,positive,positive,positive,positive,positive
887745024,"Hi @g-karthik, you can add an ACM SSL certificate to the API load balancer by using the `ssl_certificate_arn` field in the cluster configuration ([docs](https://docs.cortex.dev/clusters/management/create)), is that what you had in mind? You would follow [this guide](https://docs.cortex.dev/clusters/networking/custom-domain) to set up your custom domain, and then [this guide](https://docs.cortex.dev/clusters/networking/https) to add the cert.",hi add certificate load balancer field cluster configuration mind would follow guide set custom domain guide add,issue,negative,neutral,neutral,neutral,neutral,neutral
880830284,We won't add the plugin by default to the cluster. What we can eventually do is to support custom annotations and it becomes the responsibility of the user to manage the plugin,wo add default cluster eventually support custom becomes responsibility user manage,issue,negative,neutral,neutral,neutral,neutral,neutral
879276875,"Updating `iam_policy_arns` requires changes to eksctl config. 

There is an option of bypassing eksctl config and updating the nodeinstance roles manually. However this option becomes difficult to maintain because the `iam_policy_arns` changes need to be applied to new node instances roles belonging to newly added nodegroups as well.

We will postpone the implementation of this for now.",option manually however option becomes difficult maintain need applied new node belonging newly added well postpone implementation,issue,negative,negative,neutral,neutral,negative,negative
878725345,This issue is less relevant now that we dropped our Python FaaS interface. Installing the Python Client with the latest version of pip worked as expected.,issue le relevant python interface python client latest version pip worked,issue,negative,positive,positive,positive,positive,positive
876742246,"@lminer Thanks for following up. Yes, I do understand how some of the features that were removed were useful for you. Please see my response above for some of our thoughts that went into this decision, and I'd be happy to chat live if you'd like to hear more about our thought process and/or would like tips for migrating to the new version (should you choose to try it).

Regarding updating models without any downtime, this is still possible with Cortex today. There are two approaches:

1. You can bake your model into your Docker image, and then build a new image when you want to do a rolling update. If you reuse the same Docker tag (e.g. `my-repo/my-model:latest`), you would use the `cortex refresh` command, and if you change the docker tag (e.g. `my-repo/my-model:v2`), then you can update the `image` field in the api configuration file and run `cortex deploy`. Generally I would recommend updating the image tag (you would have a tag for each version of your model), since that makes it easier to roll back should you need to.

2. You can download your model from s3 during container initialization; if you use a model server which supports automatic reloading from s3 (e.g. TensorFlow Serving or Triton), then you would just upload a new version of your model to your s3 bucket, and the model server will automatically load the new version. We take this approach in our [Inferentia test api](https://github.com/cortexlabs/cortex/blob/master/test/apis/realtime/image-classifier-resnet50/cortex_inf.yaml) (see the `--model_base_path`).

Option 2 is nice for development, but in production I would recommend going with option #1 since there are fewer moving parts, it's less likely to make mistakes when updating your models, APIs will likely be able to scale up faster, and you'd have a complete versioned ""servable unit"" for your model which does not rely on external resources.

I'd be happy to dive into this (or anything else) further if you're interested, feel free to email me at david@cortex.dev or DM me on the community Slack.",thanks following yes understand removed useful please see response went decision happy chat live like hear thought process would like new version choose try regarding without still possible cortex today two bake model docker image build new image want rolling update reuse docker tag latest would use cortex refresh command change docker tag update image field configuration file run cortex deploy generally would recommend image tag would tag version model since easier roll back need model container use model server automatic serving triton would new version model bucket model server automatically load new version take approach test see option nice development production would recommend going option since moving le likely make likely able scale faster complete servable unit model rely external happy dive anything else interested feel free community slack,issue,positive,positive,positive,positive,positive,positive
876674917,"Just to add my two cents. I was super happy with the old API as it abstracted away a lot of the pain around serving and managing models (for example being able to update models without any downtime). It's a shame to see so many useful features being discarded. Taking a look at the new API, the rewrite that we would have to do is so extensive that we're considering moving to a different service.",add two super happy old abstracted away lot pain around serving example able update without shame see many useful taking look new rewrite would extensive considering moving different service,issue,positive,positive,positive,positive,positive,positive
875288790,">
> It seems that the issue has to do with the fact that I was earlier trying
> GPT-2 XL on g4 instances. I'd likely need to upgrade to something better
> than g4 for GPT-2 XL. Do you have any recommendations for inference
> instance types (supported by cortex) with models like GPT-2 XL (1.5
> billion), T5 (11 billion), etc.?


Here is a blog
<https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86>
describing the GPU instance types available on AWS. Cortex should be able
to support all of them.

I tried deploying GPT-2 small on g4 and load tested it, it does seem to be
> failing with 5xx whenever the API is ""updating"" (to fetch more resources to
> account for my request load) and runs fine with 2xx when the API is ""live"".
> Is this expected behavior? Shouldn't an API in ""updating"" state never fail
> with 5xx, i.e., use the existing resources while more is being provisioned
> by the auto-scaling group?


When you initially deploy the API, it will automatically be in an
""updating"" state because the ""requested"" number of workers does not match
the ""up to date"" number of workers. When the status is in the updating
state during the initial deployment, it will return 503s because no workers
have been provisioned for it yet. Once the API becomes live, your requests
will be satisfied. The API will continue to return 2XX in any subsequent
API status changes to the ""updating"" state because the requests will still
be routed to the previous version of the replica until the new version is
ready.

It is understandable why the ""updating"" status can be confusing. Thanks for
bringing this to our attention. Improvements into the status are our top
priority.

@vishalbollu <https://github.com/vishalbollu> also, how can I ensure that
> there are at least N (configurable) instances running at any given time
> for a real-time API endpoint?
>
It looks like there might be a confusion between workers and nodegroups.
Workers represent the number of copies of your webserver running on the
Cortex cluster and nodegroups represent the EC2 instances that cluster is
using to run your workers. They are different because you can schedule
multiple workers onto a single EC2 instance. For example, if you ran a
p2.8xlarge instance which has 8 GPUs, you can schedule 8 workers each
requiring 1 GPU onto a single instance. Cortex automatically provisions EC2
instances based on the min/max settings of your nodegroups and the
resources requirements of your API's workers. The nodegroup min/max setting
is typically used to reduce cold starts by eliminating ec2 instance
provision time and setting an upperbound on spend.

To increase the minimum number of workers, you need to scale the API which
is why you were able to get more workers when you ran a load test. To do it
manually, you can change autoscaling: min_replicas and autoscaling:
max_replicas in your API configuration
<https://docs.cortex.dev/workloads/realtime/configuration>(double check
that docs version matches your cortex version) and run cortex deploy.


On Tue, Jul 6, 2021 at 6:47 PM Karthik Gopalakrishnan <
***@***.***> wrote:

> @vishalbollu <https://github.com/vishalbollu> also, how can I ensure that
> there are at least N (configurable) instances running at any given time
> for a real-time API endpoint?
>
> I tried scaling up my cluster for that API by doing:
>
> cortex cluster scale --node-group ng-gpu --min-instances 10 --max-instances 10
>
> However, I do not see the API getting more nodes when I run cortex get.
> It seems that the API will only ""update"" to request more nodes if I bombard
> the API with a lot of traffic via locust-based load testing.
>
> Can you please point to the relevant doc for this? I basically want to
> ensure that even if 1 container crashes for whatever reason, there is
> another container ready upstream to take the traffic coming in.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1262#issuecomment-875131944>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABBJYH2TMLBSBXUKEGC7D6LTWOBXBANCNFSM4P7FSOJQ>
> .
>


-- 
- Vishal Bollu
",issue fact trying likely need upgrade something better inference instance cortex like billion billion instance available cortex able support tried small load tested seem failing whenever fetch account request load fine live behavior state never fail use group initially deploy automatically state number match date number status state initial deployment return yet becomes live satisfied continue return subsequent status state still previous version replica new version ready understandable status thanks attention status top priority also ensure least running given time like might confusion represent number running cortex cluster represent cluster run different schedule multiple onto single instance example ran instance schedule onto single instance cortex automatically based setting typically used reduce cold instance provision time setting spend increase minimum number need scale able get ran load test manually change configuration check version cortex version run cortex deploy tue wrote also ensure least running given time tried scaling cluster cortex cluster scale however see getting run cortex get update request bombard lot traffic via load testing please point relevant doc basically want ensure even container whatever reason another container ready upstream take traffic coming reply directly view,issue,positive,positive,positive,positive,positive,positive
875006138,Closed because user confirmed that this was an issue on their end.,closed user confirmed issue end,issue,negative,positive,positive,positive,positive,positive
874589314,"@jminuscula we have stopped the support for multi-model caching as of version 0.36. 

You are now able to run any arbitrary docker container in Cortex, so you could potentially use other solutions for multi-model caching. The Triton Inference Server comes to mind as a potential useful solution",stopped support version able run arbitrary docker container cortex could potentially use triton inference server come mind potential useful solution,issue,positive,positive,positive,positive,positive,positive
874586704,@miguelvr could you please share context on why this issue was closed? Very interested in following up. Thank you!,could please share context issue closed interested following thank,issue,positive,positive,neutral,neutral,positive,positive
873339264,"@vishalbollu I'd already seen the logs, there wasn't anything useful in there. It isn't a code implementation error, it's well-tested for all edge cases.

It seems that the issue has to do with the fact that I was earlier trying GPT-2 XL on g4 instances. I'd likely need to upgrade to something better than g4 for GPT-2 XL. Do you have any recommendations for inference instance types (supported by cortex) with models like GPT-2 XL (1.5 billion), T5 (11 billion), etc.?

I tried deploying GPT-2 small on g4 and load tested it, it does seem to be failing with 5xx whenever the API is ""updating"" (to fetch more resources to account for my request load) and runs fine with 2xx when the API is ""live"". Is this expected behavior? Shouldn't an API in ""updating"" state never fail with 5xx, i.e., use the existing resources while more is being provisioned by the auto-scaling group?",already seen anything useful code implementation error edge issue fact trying likely need upgrade something better inference instance cortex like billion billion tried small load tested seem failing whenever fetch account request load fine live behavior state never fail use group,issue,positive,positive,positive,positive,positive,positive
872310873,"There could be a few reasons for why you may get 500s and no healthy
upstream.

At this point it seems to be an implementation issue and not a Cortex
issue. Maybe your container is crashing and/or you haven't provided enough
resources.

I would begin by investigating the logs for your API
https://docs.cortex.dev/clusters/observability/logging. You may be able to
find useful information in
https://docs.cortex.dev/workloads/realtime/troubleshooting.


On Tue, Jun 29, 2021 at 5:36 PM Karthik Gopalakrishnan <
***@***.***> wrote:

> @vishalbollu <https://github.com/vishalbollu> I was able to set it up
> successfully!
>
> However, the API is really unstable, and constantly keeps failing with a no
> healthy upstream error.
>
> When I run cortex get:
>
> env      realtime api           status     up-to-date   requested   last update   avg request   2XX   4XX   5XX
> cortex   response-generator     live       1            1           3d15h         959.402 ms    509   2     194
> cortex   response-generator-2   updating   0            1           19h27m        959.402 ms    509   2     194
>
> There's a whole bunch of 5xx errors from testing. How do I resolve this? I
> need this API to be exceptionally stable.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1262#issuecomment-870935511>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABBJYH222N5AJ26WIPBCDJDTVI4HLANCNFSM4P7FSOJQ>
> .
>


-- 
- Vishal Bollu
",could may get healthy upstream point implementation issue cortex issue maybe container provided enough would begin investigating may able find useful information tue wrote able set successfully however really unstable constantly failing healthy upstream error run cortex get status last update request cortex live cortex whole bunch testing resolve need exceptionally stable reply directly view,issue,positive,positive,positive,positive,positive,positive
870935511,"@vishalbollu I was able to set it up successfully!

However, the API is really unstable, and constantly keeps failing with a `no healthy upstream` error.

When I run `cortex get`:

```
env      realtime api           status     up-to-date   requested   last update   avg request   2XX   4XX   5XX
cortex   response-generator     live       1            1           3d15h         959.402 ms    509   2     194
cortex   response-generator-2   updating   0            1           19h27m        959.402 ms    509   2     194
```

There's a whole bunch of 5xx errors from testing. How do I resolve this? I need this API to be exceptionally stable.",able set successfully however really unstable constantly failing healthy upstream error run cortex get status last update request cortex live cortex whole bunch testing resolve need exceptionally stable,issue,negative,positive,positive,positive,positive,positive
869736134,"Yes, GCP support has been removed. We definitely want to bring it back but we are focusing our efforts on ironing out the user experience on AWS for now. It's a lot easier for us to iterate on a single cloud provider while also ensuring that Cortex will be reliable for large-scale production deployments.",yes support removed definitely want bring back user experience lot easier u iterate single cloud provider also cortex reliable production,issue,positive,negative,neutral,neutral,negative,negative
869621343,"Right. Yes I agree but it is a relatively easy solution and we had been using this prior to switching to cortex. With some precautions, it went smoothly. GPUs are damn expensive and underutilisation is very irritating. In any case, it is a complex decision so you‚Äôre the best judge. ",right yes agree relatively easy solution prior switching cortex went smoothly damn expensive irritating case complex decision best judge,issue,positive,positive,positive,positive,positive,positive
869525032,"That solution is not production-ready for general use-cases because it does not provide resource encapsulation. Exceeding GPU memory in one pod could lead to the failure of multiple pods using that same GPU.

There is a warning about this in the blog post.
> Be aware that the resource allocation only happens in the scheduler of Kubernetes. There‚Äôs no constraint for GPU consumption with containers, which is different from CPU and memory allocation. By default, GPU does not support resource isolation while multiple containers share one GPU.

Their example works because they use the encapsulation provided by TF Serving.",solution general provide resource encapsulation exceeding memory one pod could lead failure multiple warning post aware resource allocation constraint consumption different memory allocation default support resource isolation multiple share one example work use encapsulation provided serving,issue,negative,negative,neutral,neutral,negative,negative
869008861,"It's great to hear that you've got the example working.

1. A good way to pass these fields is by setting the `env` field to pass
environment variables into your container. You can specify the path to the
model in S3.
2. The image built by the Dockerfile must be a webserver.
Uvicorn/gunicorn/fastapi/flask or a similar alternative must be used. It is
generally a good idea to build an image per microservice so to reduce cross
API dependencies. However, there are ways to use the same docker image to
serve different models if you think that the dependencies are the same for
now. You can follow my recommendation above. You can use environment
variables to change the behaviour of your webserver by specifying
environment variables such as `S3_MODEL_PATH`, `MODEL_TYPE`,
`HYPER_PARAMETER_1` etc in your API spec. These variables can be read in
your main.py and your behaviour can change accordingly.


On Fri, Jun 25, 2021 at 3:20 AM Karthik Gopalakrishnan <
***@***.***> wrote:

> @vishalbollu <https://github.com/vishalbollu> So I was able to get your
> example working!
>
> I have the following customizations to make now and wanted your help with:
>
>    1. my main.py should be able to read in a configuration file that
>    primarily specifies which model class (GPT-2, BERT, etc.) to load and the
>    path to the model checkpoint in S3 to pull the checkpoint from during API
>    startup, and secondarily specifies some inference hyper-parameters
>    2. my Dockerfile should be common to all APIs I want to deploy (I will
>    want to deploy multiple APIs for multiple checkpoints, as you can see
>    above, but use the exact same code) -- in that sense, at the very least,
>    uvicorn should not be tied to / executed within my Dockerfile itself
>
> How do I go about making this happen? Suppose I want two APIs, 1 for GPT-2
> small and 1 for GPT-2 XL, and both their checkpoints are stored in my S3
> bucket.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1262#issuecomment-868282787>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABBJYH7G5KGM22PCDMDAHBTTUQU33ANCNFSM4P7FSOJQ>
> .
>


-- 
- Vishal Bollu
",great hear got example working good way pas setting field pas environment container specify path model image built must similar alternative must used generally good idea build image per reduce cross however way use docker image serve different think follow recommendation use environment change behaviour environment spec read behaviour change accordingly wrote able get example working following make help able read configuration file primarily model class load path model pull secondarily inference common want deploy want deploy multiple multiple see use exact code sense least tied executed within go making happen suppose want two small bucket reply directly view,issue,positive,positive,positive,positive,positive,positive
863572268,I'll go ahead and close this issue since we had our conversation. Feel free to reach out if you have any questions as you migrate to the new version!,go ahead close issue since conversation feel free reach migrate new version,issue,positive,positive,positive,positive,positive,positive
863188076,"This is a non-issue. The operator is working as it should.

The problem is with our test client (`requests` package), where the first request (once the script is started) doesn't have the payload attached, which will lead the operator to return the above error ""unexpected end of JSON input"". When the test client gets the error, it retries with the correct payload the 2nd time, and this time it goes through. One important note is that this 2-step process is happening transparently to the user implementing the client, so to the user, this could look like `requests.post(...)`.

Regardless, this doesn't appear to affect the nightly test because the client isn't raising an exception (it just retries in the background).",operator working problem test client package first request script attached lead operator return error unexpected end input test client error correct time time go one important note process happening transparently user client user could look like regardless appear affect nightly test client raising exception background,issue,negative,positive,positive,positive,positive,positive
862855684,An abandoned request is appropriately handled in the proxy.,abandoned request appropriately handled proxy,issue,negative,positive,positive,positive,positive,positive
861484606,did you check this works (i.e. that you are able to write metrics to statsd)? ,check work able write metric,issue,negative,positive,positive,positive,positive,positive
860434785,"@vishalbollu thanks for sharing this! I was trying to run some existing code based on cortex 0.25.0 with its own predictor implemented, and faced some error like `supervisor not listening` which we weren't facing before.

I'm going to try setting up your example above with the latest cortex. Looking further though, I would need to package specific portions of a private repo I'm working with into the `app` in the first URL you linked - unlike in your self-contained `main.py`, some of my dependencies will need to come from local code paths in my private repo, not just from a `requirements.txt`. I'll try to get this working and reach out if I face any roadblocks.",thanks trying run code based cortex predictor faced error like supervisor listening facing going try setting example latest cortex looking though would need package specific private working first linked unlike need come local code private try get working reach face,issue,negative,positive,positive,positive,positive,positive
857966737,"@g-karthik you can find an example of deploying a hugging face transformer to a GPU instance: https://github.com/cortexlabs/cortex/tree/master/test/apis/realtime/text-generator.

Given that you've mentioned the predictor, I wanted to bring to your attention that the latest version of Cortex v0.36 expects your application to be packaged in a docker container as opposed to a python project with a `predictor.py`. The example I've linked to above complies with the latest version of Cortex. You can find more documentation at https://docs.cortex.dev/.

",find example hugging face transformer instance given predictor bring attention latest version cortex application docker container opposed python project example linked latest version cortex find documentation,issue,negative,positive,positive,positive,positive,positive
857958346,"Hey @creatorrr thanks for reaching out!

1. There were many factors that went into this decision, since as you mentioned, it is a significant API change. The main motivation was for us to focus on the aspects where we feel that we provide the most value (infrastructure automation), while at the same time allowing for the most flexibility in terms of what kinds of workloads that can run on Cortex. There are many ways to serve a model within a container (TensorFlow Serving, TorchServe, Triton, standard Python servers like Flask/FastAPI, etc), all which have their own advantages and disadvantages, and many of which are not written in Python. By making our interface agnostic, we‚Äôre moving to a more standard and portable interface, as well as allowing for more flexibility and use cases. I‚Äôd be happy to discuss this further if you‚Äôre interested in chatting. 

2. There are not currently plans to bring back the Python interface.

3. AWS credentials still exist within the container‚Äôs runtime, so it is possible to download a model from s3 when your server initializes. Here is an example from our test folder (you would download your model in `def startup`): https://github.com/cortexlabs/cortex/blob/0.36/test/apis/realtime/text-generator/app/main.py . Also, some model servers (like TensorFlow Serving) support downloading models directly from s3. I‚Äôd be happy to jump on a call to help you more specifically with anything you get stuck on with the migration.

4. Our preference is to help you migrate to the new API. We will continue to help with simple questions or small tasks on the old version, but our main focus is on supporting the new API. 

Feel free to email me at david@cortex.dev or DM me on the community Slack if you want to set up a time to chat about our thought process with the product updates or for help with migrating your APIs!",hey thanks reaching many went decision since significant change main motivation u focus feel provide value infrastructure time flexibility run cortex many way serve model within container serving triton standard python like many written python making interface agnostic moving standard portable interface well flexibility use happy discus interested chatting currently bring back python interface still exist within container possible model server example test folder would model also model like serving support directly happy jump call help specifically anything get stuck migration preference help migrate new continue help simple small old version main focus supporting new feel free community slack want set time chat thought process product help,issue,positive,positive,positive,positive,positive,positive
857332409,@deliahu I am unable to find these examples any more. Is there a tutorial on configuring/deploying a Cortex predictor for Hugging Face transformers to an AWS GPU instance for inference?,unable find tutorial cortex predictor hugging face instance inference,issue,negative,negative,negative,negative,negative,negative
855023999,"@creatorrr Thanks for bringing this to our attention!

I've made the fix [here](https://github.com/cortexlabs/cortex/pull/2225), and it will be in our next release (scheduled for early next week).

That said, if it had behaved as expected, it would have hit the next line which would have returned the correct error message: ""no default environment configured"".

Since you are running inside of a Cortex cluster, there exists a CLI configuration ready to go; you can configure your CLI to use it by exporting `CORTEX_CLI_CONFIG_DIR=/mnt/client` before using your client. In our next release, this env var will automatically be set, so you won't have to manually export it in the future.

If you weren't in a Cortex cluster, you would instead use the `cortex.new_client(name, operator_endpoint)` constructor.",thanks attention made fix next release early next week said would hit next line would returned correct error message default environment since running inside cortex cluster configuration ready go configure use client next release automatically set wo manually export future cortex cluster would instead use name constructor,issue,negative,positive,neutral,neutral,positive,positive
853825777,"Thanks! I'll try to work on this over the weekend. In the meantime, sharing notes from my research:

- You're right, a lot of the projects mentioned in the description are stale
- [AliyunContainerService/gpushare-scheduler-extender](https://github.com/AliyunContainerService/gpushare-scheduler-extender/blob/master/docs/install.md#3-deploy-device-plugin) requires replacing the nvidia-device-plugin and I am not sure that's a good idea
- [Deepomatic/shared-gpu-nvidia-k8s-device-plugin](https://github.com/Deepomatic/shared-gpu-nvidia-k8s-device-plugin) is a very limited and hacky workaround
- nvidia seems to be working on this and there is even an [open PR](https://gitlab.com/nvidia/kubernetes/device-plugin/-/merge_requests/86) for partial support but it doesn't look like it's landing anytime soon
- until then, I am looking at [tkestack/gpu-manager](https://github.com/tkestack/gpu-manager) and [NTHU-LSALAB/KubeShare](https://github.com/NTHU-LSALAB/KubeShare/) as possible candidates
",thanks try work weekend research right lot description stale sure good idea limited hacky working even open partial support look like landing soon looking possible,issue,positive,positive,positive,positive,positive,positive
853545591,Yes that works (pinning `starlette==0.14.0`). It‚Äôd be great to have it documented somewhere if possible. ,yes work pinning great somewhere possible,issue,positive,positive,positive,positive,positive,positive
853308534,This is a temporary workaround until we spend more time investigating this https://github.com/cortexlabs/cortex/issues/2217.,temporary spend time investigating,issue,negative,neutral,neutral,neutral,neutral,neutral
853290114,I don't think the order matters in the container declaration.,think order container declaration,issue,negative,neutral,neutral,neutral,neutral,neutral
853286319,"This is an issue on kubexit. We are using my fork currently, but we can fork to cortexlabs and make the fix",issue fork currently fork make fix,issue,negative,neutral,neutral,neutral,neutral,neutral
853100266,"@creatorrr we've tabled this for now. That's because we didn't find a good reliable solution to this and some of the projects that would support this also appear to be stale.

From what I can recall, there were issues with memory isolation between different vGPUs. @miguelvr might be able to give more context as he did some research on one or two of these projects.

That being said, if you want to support this, you probably want to start with:
1. https://github.com/cortexlabs/cortex/blob/master/CONTRIBUTING.md
2. This is the Nvidia device plugin: https://github.com/cortexlabs/cortex/blob/master/manager/manifests/nvidia.yaml
3. This is where we install it on a new cluster: https://github.com/cortexlabs/cortex/blob/master/manager/install.sh#L65-L68
4. You might have to edit the labels on the instances: https://github.com/cortexlabs/cortex/blob/6ca17086cf7019d2ba8e233f3beb5ab6e1ab073f/manager/generate_eks.py#L119-L120
5. Check out for all the resource requests/limits `""nvidia.com/gpu""` in https://github.com/cortexlabs/cortex/blob/be39ba721efd4f0cd5aafaa9efdb7b6922f5db0c/pkg/workloads/k8s.go.
6. Make sure the taints/affinities are correctly applied.",tabled find good reliable solution would support also appear stale recall memory isolation different might able give context research one two said want support probably want start device install new cluster might edit check resource make sure correctly applied,issue,positive,positive,positive,positive,positive,positive
852807142,"What's the status of this, @RobertLucian ? We could really use this for our workloads. Would love to help as well",status could really use would love help well,issue,positive,positive,positive,positive,positive,positive
850586781,"Yes, that makes sense. Have you attempted pinning startlette and fastapi to the appropriate versions? E.g.

```python
# requirements.txt

fastap==0.65.1
starlette==0.14.2
```",yes sense pinning appropriate python,issue,negative,positive,positive,positive,positive,positive
850246301,"Using a library that depends on `^0.14.2`, I was able to pin to a lower version but it'd be great if it was possible to update cortex deps.",library able pin lower version great possible update cortex,issue,negative,positive,positive,positive,positive,positive
849909407,"Yes, and it also seems to work by default if `starlette` is not specified in `requirements.txt` (the pre-installed version is `0.13.6`). It is generally not recommended to change the version of cortex's internal dependencies (such as starlette); is there a feature that you need access to in starlette 0.14?",yes also work default version generally change version cortex internal feature need access,issue,negative,positive,neutral,neutral,positive,positive
849121936,"@creatorrr thanks for opening this issue.

I was unable to recreate the error. I tried a simple handler like this:

```python
from starlette.responses import Response

class Handler:
    def __init__(self, config):
        pass

    def handle_post(self, payload, query_params):
        return Response(""ok"")
```

Do you mind creating a minimal example which reproduces the error? Perhaps it has to do with the contents of your `requirements.txt`?",thanks opening issue unable recreate error tried simple handler like python import response class handler self pas self return response mind minimal example error perhaps content,issue,negative,negative,neutral,neutral,negative,negative
845418072,"> Can we also take this opportunity and merge the types / functions?
> 
> I think they can exist in the enqueuer package and be imported by the operator

As discussed live, we will address this in a separate PR",also take opportunity merge think exist package operator live address separate,issue,negative,positive,positive,positive,positive,positive
835400652,this actually breaks the batchjob controller tests (it needs a bucket),actually controller need bucket,issue,negative,neutral,neutral,neutral,neutral,neutral
833386855,"@RobertLucian I would prefer to keep the scope of this PR to the `async-gateway` only, for several reasons",would prefer keep scope several,issue,negative,neutral,neutral,neutral,neutral,neutral
832964574,"I'll go ahead and mark this issue as resolved.

After discussing, the best approach is to allow Cortex to create the VPC, and to configure the API load balancer to be public (`api_load_balancer_scheme: internet-facing`) and the instance subnets to be private (`subnet_visibility: private`). If necessary in the future, an IP whitelist can be used to restrict access to the API load balancer (`api_load_balancer_cidr_white_list`), or the API load balancer can be made private (`api_load_balancer_scheme: internal`) and VPC peering can be used to connect to the cluster's VPC. Here is our [VPC Peering guide for AWS](https://docs.cortex.dev/clusters/networking/vpc-peering), and it may be possible to configure VPC peering across cloud providers.",go ahead mark issue resolved best approach allow cortex create configure load balancer public instance private private necessary future used restrict access load balancer load balancer made private internal used connect cluster guide may possible configure across cloud,issue,positive,positive,positive,positive,positive,positive
832306182,Add another column to what cortex cluster info outputs: `async gateways`. Hide it if there are no async APIs in the cluster.,add another column cortex cluster hide cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
830147384,Closing due to inactivity. Feel free to reopen this issue or a file a new one.,due inactivity feel free reopen issue file new one,issue,positive,positive,positive,positive,positive,positive
830145963,"Following up on this, in cortex v0.34, a new command `cortex prepare-debug` has been added to help debug API containers locally. You can read more about it [here](https://docs.cortex.dev/v/0.34/workloads/debugging).",following cortex new command cortex added help locally read,issue,negative,positive,neutral,neutral,positive,positive
828927635,"Thanks for your response! Yes, I think it might be best to jump on a quick call, since there are a few more questions I'd like to ask about your setup (e.g. would it work to set up VPC peering to your Cortex cluster's VPC?). Feel free to email me at david@cortex.dev and we can find a time!",thanks response yes think might best jump quick call since like ask setup would work set cortex cluster feel free find time,issue,positive,positive,positive,positive,positive,positive
828123450,"Hi @deliahu , sry for the delayed response. Our use case is, we have an existing application in eks cluster, to enhance the functionality of this application, we wanted to deploy a ML model which is served through Cortex. The model behind Cortex is internal only application, so it should be only reachable by the services which are already running the existing eks cluster. 

So, we wanted to deploy cortex with in the existing cluster so that existing applications/services can reach the cortex without having the traffic to go out of current eks cluster. Also, we wanted to avoid setting up any extra infrastructure for cost optimization.

Please let me know if I was able to answer your questions, else I can jump on a call and explain in detail.",hi response use case application cluster enhance functionality application deploy model cortex model behind cortex internal application reachable already running cluster deploy cortex cluster reach cortex without traffic go current cluster also avoid setting extra infrastructure cost optimization please let know able answer else jump call explain detail,issue,negative,positive,neutral,neutral,positive,positive
824915993,"That's excellent news! I'm glad you were able to get it fixed so quickly - thank you! I don't think we need an urgent patch release or anything - next Tuesday should be fine. :grinning: 
Thanks again!",excellent news glad able get fixed quickly thank think need urgent patch release anything next fine grinning thanks,issue,positive,positive,positive,positive,positive,positive
824273025,@wilson-zimp I wanted to follow up on my previous comment; let me know if you have any thoughts. I just want to get a clearer picture of your requirements before we'd decide to expand our support for deploying into an existing VPC.,follow previous comment let know want get clearer picture decide expand support,issue,positive,negative,negative,negative,negative,negative
824241581,"@Haplo-Dragon This issue has been fixed on our master branch (and we've updated our tests to catch it in the future). Thanks for filing this detailed bug report!

Since it's been merged to master, it will be in our next release (v0.34), which is planned for next Tuesday. If this is an urgent issue for you, we'd be happy to make a patch release for it today or tomorrow, so just let us know!",issue fixed master branch catch future thanks filing detailed bug report since master next release next urgent issue happy make patch release today tomorrow let u know,issue,positive,positive,positive,positive,positive,positive
823004044,"Thanks for submitting this.

I think this was introduced in https://github.com/cortexlabs/cortex/pull/2072

It should be an easy fix",thanks think easy fix,issue,positive,positive,positive,positive,positive,positive
821210690,"@miguelvr it doesn't seem to sound like alerts only work with Slack. Check this out:

```text
Alerts can be configured for a variety of notification channels such as for Slack, Discord, Microsoft Teams, PagerDuty, Telegram and traditional webhooks. In this example, we'll use Slack
```",seem sound like work slack check text variety notification slack discord telegram traditional example use slack,issue,negative,positive,positive,positive,positive,positive
820631421,"@wilson-zimp Does the VPC that you deploy Cortex into have other applications running in it? Or do you create an empty VPC with the configuration you want, and then run only Cortex in that VPC?

Also, do we (or if not, could we) expose a way to configure the VPC that Cortex creates to meet your needs? What specifically do you configure in your VPC that means that you prefer not to (or cannot) allow Cortex to create its own VPC? The reason I ask is that we are not sure about whether we should support deploying into existing VPCs in the long run; it will make it harder to add additional features on our roadmap (such as more automated cluster upgrades) if we can't assume a ""blank slate"" VPC that is owned by Cortex. Also, deploying into an existing VPC requires a fair amount of manual configuration (as I'm sure you know) that can be hard to get right and hard to debug. And since Cortex is running alongside other apps, unexpected and unpredictable conflicts could arise. Since environments would not be consistent across different users, this could be hard for us to debug and support.

Sorry for all of the questions, it's because this is an active topic that the team is discussing internally. I'd also be happy to jump on a call if you think that'd be easier (if so, feel free to email me at david@cortex.dev to find a time).",deploy cortex running create empty configuration want run cortex also could expose way configure cortex meet need specifically configure prefer allow cortex create reason ask sure whether support long run make harder add additional cluster ca assume blank slate cortex also fair amount manual configuration sure know hard get right hard since cortex running alongside unexpected unpredictable could arise since would consistent across different could hard u support sorry active topic team internally also happy jump call think easier feel free find time,issue,positive,positive,neutral,neutral,positive,positive
820441477,"@deliahu - we already have a VPC with NAT gateway's, networking and security groups defined as we want. Hence we wanted to use existing VPC.
",already nat gateway security defined want hence use,issue,negative,neutral,neutral,neutral,neutral,neutral
820186274,"@wilson-zimp Yes, I believe it should be possible in theory, although is not currently supported by Cortex (for some reason I recall thinking that this was not possible when we originally implemented the feature, so either I was mistaken or `eksctl` has since enabled it.

That said, it is certainly possible (and common practice) to do this when allowing Cortex to create the VPC; setting `subnet_visibility: private` in your cluster configuration will achieve your desired configuration (where the nodes will be in private subnets with no external IPs, and the load balancer will be in the public subnets). Is there a reason that you cannot create the VPC during cluster creation time? Generally we recommend this approach (in combination with VPC Peering when necessary, although in your case, since the load balancer will be public, VPC Peering is not necessary).",yes believe possible theory although currently cortex reason recall thinking possible originally feature either mistaken since said certainly possible common practice cortex create setting private cluster configuration achieve desired configuration private external load balancer public reason create cluster creation time generally recommend approach combination necessary although case since load balancer public necessary,issue,positive,positive,neutral,neutral,positive,positive
819910792,I'll go ahead and close this ticket here since it seems to be an issue that needs to be addressed by the TF Serving team. Users can also use the Python predictor type as an alternative to TF Serving.,go ahead close ticket since issue need serving team also use python predictor type alternative serving,issue,negative,neutral,neutral,neutral,neutral,neutral
819905075,It is trivial for the user to write the code do download the files they need (which can be specified in the API's config field),trivial user write code need field,issue,negative,neutral,neutral,neutral,neutral,neutral
819562924,"Enabling containerd is possible with the following `preBootstrapCommands`:

```python
""preBootstrapCommands"": [
            ""yum install containerd -y"",
            ""truncate -s-1 /etc/systemd/system/kubelet.service.d/10-eksclt.al2.conf"",
            ""echo -n ' --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock' >> /etc/systemd/system/kubelet.service.d/10-eksclt.al2.conf"",
        ],
```

Problem is that when letting eksctl do this, the CNI can no longer be initialized when the node is joining the cluster.

A ticket has been created on https://github.com/weaveworks/eksctl/issues/3572 to get feedback from the eksctl team. Messages have also been posted on the eksctl slack, but with no reply yet.

The research has been conducted on this branch https://github.com/cortexlabs/cortex/tree/reliability/containerd-cri-runtime.",possible following python install truncate echo problem longer node joining cluster ticket get feedback team also posted slack reply yet research branch,issue,negative,neutral,neutral,neutral,neutral,neutral
819099476,"I'll go ahead and close this issue, since this can be easily implemented by adding the secret key to the `config` field of the predictor, and then checking a header / query param / body field for the secret key in `predict()`",go ahead close issue since easily secret key field predictor header query param body field secret key predict,issue,negative,negative,negative,negative,negative,negative
819098249,I'll go ahead and close this issue since the above solution is the recommended approach,go ahead close issue since solution approach,issue,negative,neutral,neutral,neutral,neutral,neutral
818176678,"For local development, users typically can initialize their predictor in a python runtime and invoke .predict() function with the expected payload by call.

```
import time

class PythonPredictor:
    def __init__(self, config):
        ...
    def predict(self, payload):
        ...

predictor = PythonPredictor(config={<insert config from api spec here>})
predictor.predict(payload={})
```

This is a good place to start when testing the predictor implementation (although this approach may not be possible in a few scenarios). For testing the API as a whole, a few users maintain a dev cluster for more rigorous testing before deploying to a prod cluster. For the in-between testing, such as just testing the container, we have filed this ticket https://github.com/cortexlabs/cortex/issues/2077 to design ways to run the API containers locally. Ideally we would like to get to a point where you can `docker run [flags]` the container to test it locally.",local development typically initialize predictor python invoke function call import time class self predict self predictor insert spec good place start testing predictor implementation although approach may possible testing whole maintain dev cluster rigorous testing prod cluster testing testing container ticket design way run locally ideally would like get point docker run container test locally,issue,positive,positive,positive,positive,positive,positive
818026291,"This is not really a problem.

The above errors are returned if tasks are being submitted while the task API itself is being deleted. With respect to the E2E tests, this can happen if all tasks haven't been submitted within a given time frame, which will then prompt the test to stop (so that it can throw a timeout error), which will remove the API while tasks are briefly still being submitted. During this brief moment in time, the tasks that are still being submitted will fail - and this is expected.",really problem returned task respect happen within given time frame prompt test stop throw error remove briefly still brief moment time still fail,issue,negative,negative,neutral,neutral,negative,negative
817542851,"@ospillinger I can personally say that our team used this primarily for local development of the API's before deployment.

We're still quite new to Cortex, so perhaps you could elaborate on how to do local dev/debugging/testing etc. with your framework? Perhaps we simple go around it the wrong way.",personally say team used primarily local development deployment still quite new cortex perhaps could elaborate local framework perhaps simple go around wrong way,issue,negative,positive,neutral,neutral,positive,positive
816817521,"Hi @codeghees, could you please share with us some of your use cases for local?",hi could please share u use local,issue,positive,neutral,neutral,neutral,neutral,neutral
816595197,"Can we fork this and maybe collaborate on the development of just local?

Could help A LOT of cases.

Would anyone be interested?

On Fri, 9 Apr 2021, 15:43 Miguel Varela Ramos, ***@***.***>
wrote:

> What was the reason for removing this feature again?
>
> In a nutshell:
>
> It had costs for our limited development time, and it was not aligned with
> the product focus, which is running ML at scale.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/109#issuecomment-816594320>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AJVMFWEI3CTRGHGICPXHM2DTH3K4RANCNFSM4HPQLYYQ>
> .
>
",fork maybe collaborate development local could help lot would anyone interested wrote reason removing feature nutshell limited development time product focus running scale reply directly view,issue,positive,positive,neutral,neutral,positive,positive
816594320,"> What was the reason for removing this feature again?

In a nutshell:

It had costs for our limited development time, and it was not aligned with the product focus, which is running ML at scale.",reason removing feature nutshell limited development time product focus running scale,issue,negative,negative,neutral,neutral,negative,negative
816142784,The cluster autoscaler reshuffles nodes based on node utilization.,cluster based node utilization,issue,negative,neutral,neutral,neutral,neutral,neutral
816032866,Closing because cortex get shows OOM status and k8s pod events are now exported to cloudwatch.,cortex get status pod,issue,negative,neutral,neutral,neutral,neutral,neutral
815110590,Closing due to vagueness (will create separate tickets for any items on this list which are still relevant),due vagueness create separate list still relevant,issue,negative,positive,positive,positive,positive,positive
814120327,"@H4dr1en we have added multi instance types clusters as a feature recently. This can mitigate costs already, by allowing to run both CPU / GPU and Spot instances in the same cluster.

I know it is not remotely the same as Elastic Inference, but it is an improvement :)

We will look into Elastic Inference again soon since we are re-focusing the team's efforts on improving the Cortex UX on AWS.",added instance feature recently mitigate already run spot cluster know remotely elastic inference improvement look elastic inference soon since team improving cortex,issue,positive,negative,neutral,neutral,negative,negative
814117120,This issue has been depriorized and the relevant eksctl issue is closed for inactivity but using EI would be cost saving for most of the Cortex users. Is there any plan to solve this issue in the following releases?,issue relevant issue closed inactivity would cost saving cortex plan solve issue following,issue,negative,positive,positive,positive,positive,positive
813678084,"Ok sounds good, feel free to reach out if you have additional questions! You can also reach us on our [community slack channel](https://community.cortex.dev/).",good feel free reach additional also reach u community slack channel,issue,positive,positive,positive,positive,positive,positive
813621125,"We want to optimize cost by eliminating creation of additional LoadBalancers, NAT gateways and EKS cluster and also for ease of management. Thanks for answering my questions promptly, we will look into setting up a new EKS cluster if there isn't a way to reuse existing EKS cluster.",want optimize cost creation additional nat cluster also ease management thanks promptly look setting new cluster way reuse cluster,issue,positive,positive,positive,positive,positive,positive
813618084,"Cortex can only be run in a separate EKS cluster (it is also recommended to run Cortex in a dedicated VPC, which is the default behavior when a cluster is created). This is for many of the reasons I mentioned above: since Cortex has full control of the cluster, it can automate more tasks, e.g. node autoscaling, configuring nodegroups, installing GPU and/or Inferentia daemonsets, managing spot instances, validating resource capacity, etc.

What is the reason behind your preference to run it in your existing eks cluster?",cortex run separate cluster also run cortex default behavior cluster many since cortex full control cluster node spot resource capacity reason behind preference run cluster,issue,negative,positive,positive,positive,positive,positive
813586230,"@deliahu - yes we have AWS eks cluster already deployed and we want to explore the option to deploy cortex to an existing EKS cluster and not the onprem kubernetes cluster. Please let me know if that not supported too.

I am actually trying to eliminate having 2 eks clusters for our application.",yes cluster already want explore option deploy cortex cluster cluster please let know actually trying eliminate application,issue,positive,neutral,neutral,neutral,neutral,neutral
813555441,Closing issue because cortex local is no longer supported.,issue cortex local longer,issue,negative,neutral,neutral,neutral,neutral,neutral
813536745,"@wilson-zimp thanks for reaching out. Support for installing Cortex on an existing cluster was removed in v0.30. We made this decision so that we can focus our efforts on improving the scalability, reliability, and security for Cortex on AWS; overall, we think Cortex can provide more value as a cloud-based service, where it can control cluster-related things such as node autoscaling, nodegroups, spot instances, auth, security, etc.

Are you able to create, or do you have access to an AWS account? Some of our users have had success with creating an AWS account dedicated to running Cortex (making inference requests from their backends on other clouds / on prem, or from clients directly, depending on the architecture and use case).",thanks reaching support cortex cluster removed made decision focus improving reliability security cortex overall think cortex provide value service control node spot security able create access account success account running cortex making inference directly depending architecture use case,issue,positive,positive,positive,positive,positive,positive
813281952,"@deliahu  - the link above for docs is not working (Feb 8,2021 comment). Can you please point me to the right documentation on how to install cortex on an existing Kubernetes cluster ? Thanks!",link working comment please point right documentation install cortex cluster thanks,issue,positive,positive,positive,positive,positive,positive
812206822,We are focusing on AWS at the moment and supporting Azure is not in our near term roadmap.,moment supporting azure near term,issue,negative,positive,positive,positive,positive,positive
812201351,The warmup can be done in the constructor of the predictor implementation.,done constructor predictor implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
812197945,Multiple nodegroups can be used to address this problem.,multiple used address problem,issue,negative,neutral,neutral,neutral,neutral,neutral
811945709,"@dimasheva1 the error message says `model '_cortex_default' of version '1' couldn't be loaded: error: signature_key was not configured by user, please specify one the following keys 'serving_default, __saved_model_init_op' for model '_cortex_default' of version '1' (found in the signature def map)`.

What this means is that you have to specify the signature key for your model and that can either be `serving_default` or `__saved_model_init_op`. You can set this field in your API spec - the field is called `predictor.models.signature_key` (at the same level as the `path` field for your model).

Also, you can check out the docs on this here as well: https://docs.cortex.dev/workloads/realtime-apis/configuration#tensorflow-predictor

Let us know if this was your problem!",error message model version could loaded error user please specify one following model version found signature map specify signature key model either set field spec field level path field model also check well let u know problem,issue,negative,neutral,neutral,neutral,neutral,neutral
811406244,"After Cortex completely uses CRDs, we may be able to piggyback off of https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ or other open source tools like that.",cortex completely may able open source like,issue,negative,positive,positive,positive,positive,positive
811387000,I believe this has been resolved as of version 0.22. Please let us know if this problem persists.,believe resolved version please let u know problem,issue,negative,neutral,neutral,neutral,neutral,neutral
810345705,"> This seems extremely confusing to me. Now it's not clear what is related to HTTP or to gRPC. I would strongly suggest creating sections for HTTP and gRPC separately, where the specifics are addressed separately.

I'm referring to the `predictor.md` and `configuration.md` sections.

I think gRPC should be split in a similar fashion to the different predictor types",extremely clear related would strongly suggest separately separately think split similar fashion different predictor,issue,positive,positive,positive,positive,positive,positive
804993752,"Yes, it's been resolved. Thank you for the suggestions! I'll have backup images in my registry.  ",yes resolved thank backup registry,issue,positive,neutral,neutral,neutral,neutral,neutral
804946472,"Quay appears to have resolved their outage. This should no longer be a problem, however let us know if this issue is still being encountered.

We are going to investigate solutions to avoid/mitigate the impacts of such outages and come up with recovery strategies.

Agreed, impacts of registry outages can be mitigated by a combination of registry mirroring with retries and cluster-wide caching. The recovery process for such outages can be improved by encouraging users to backup images to their own private registry and making it easy to update images on a running cluster.",quay resolved outage longer problem however let u know issue still going investigate come recovery agreed registry combination registry recovery process encouraging backup private registry making easy update running cluster,issue,positive,positive,positive,positive,positive,positive
804601017,"Might be worthwhile to consider adding a cluster-wide image layer cache to cortex. A traditional docker mirror might suffice here, or something more fancy like [Kube-fledged](https://github.com/senthilrch/kube-fledged) could be added to pre-pull images down on nodes. I'm sure there are more out there but continuously pulling from the master registry seems error prone given both this ticket and #1989",might consider image layer cache cortex traditional docker mirror might suffice something fancy like could added sure continuously master registry error prone given ticket,issue,negative,positive,positive,positive,positive,positive
804530150,"There is a quay.io outage https://status.quay.io/ that is the source of the problem here. Cortex uses quay.io to host and deliver images. Because of the outage, your cluster is unable to pull images. Any provisioning of new replicas/workers will be impacted (cortex cluster* commands , cortex deploy and autoscaling).

To get around the quay outage and become more resilient to outages in the future, you can configure your cortex cluster and your APIs to pull images from a registry of your choice. This will involve spinning up a new cluster and redeploying your APIs with the image fields set to your images from your registry.

Unfortunately because quay.io is down, it isn't possible to export images from Cortex's quay registry to your own registry. You will have to build images from directly from the Cortex repo.

Here are some high level instructions that may help (I have not run through all of the scenarios to verify if they work as expected).

It can take 60 to 90 minutes to build and push all of the images. It is recommended to do this on an ec2 instance in the same region as your ECR to speed up the build and push time.

1. Clone the Cortex repository and check in to the tag that matches cortex version exactly `git checkout v0.25.0`.
1. Export your AWS credentials
```bash
export AWS_ACCESS_KEY_ID=""""
export AWS_SECRET_ACCESS_KEY=""""
export AWS_ACCOUNT_ID=""***""
export AWS_REGION=""us-west-2""
```

For cortex <= 0.23
1. `make registry-create`
1. `make images-all`

For cortex >= 0.24
1. `make registry-create-aws`
1. `make images-all-aws`

The path for the image will take on the form: `<AWS_ACCOUNT_ID>.dkr.ecr.<AWS_REGION>.amazonaws.com/cortexlabs/operator:<CORTEX_VERSION>`

Building predictor image might fail because the pip version wasn't frozen until the newer versions of Cortex. Edit the Dockerfiles of the predictor images to install 19.* version of pip. You may run into uvloop requires python 3.6 errors. To get around this, search for `uvicorn==` to find the relevant requirements.txt file and add a new requirement to force installation of an older version that is compatible with python 3.6 `uvloop==0.14.0`.

Once you've built all of the images:
* Navigate to your cortex version's cluster installation docs to find the complete list of images used by the cluster. Set all of these values to point to your new registry. That should look something like this (the complete images may change based on your cortex version):
```yaml
# cluster.yaml
...
image_operator: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/operator:<CORTEX_VERSION>
image_manager: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/manager:<CORTEX_VERSION>
image_downloader: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/downloader:<CORTEX_VERSION>
image_request_monitor: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/request-monitor:<CORTEX_VERSION>
image_cluster_autoscaler: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/cluster-autoscaler:<CORTEX_VERSION>
image_metrics_server: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/metrics-server:<CORTEX_VERSION>
image_inferentia: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/inferentia:<CORTEX_VERSION>
image_neuron_rtd: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/neuron-rtd:<CORTEX_VERSION>
image_nvidia: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/nvidia:<CORTEX_VERSION>
image_fluentd: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/fluentd:<CORTEX_VERSION>
image_statsd: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/statsd:<CORTEX_VERSION>
image_istio_proxy: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/istio-proxy:<CORTEX_VERSION>
image_istio_pilot: <account_id>.dkr.ecr.<region>.amazonaws.com/cortexlabs/istio-pilot:<CORTEX_VERSION>
```
* Navigate to your cortex version's api specs to find the relevant image fields you need to set for your API specs and update the `cortex.yaml` accordingly.

Spin up the new cluster pointing to your new images and verify that your API deployments are working as expected.",outage source problem cortex host deliver outage cluster unable pull new impacted cortex cluster cortex deploy get around quay outage become resilient future configure cortex cluster pull registry choice involve spinning new cluster image set registry unfortunately possible export cortex quay registry registry build directly cortex high level may help run verify work take build push instance region speed build push time clone cortex repository check tag cortex version exactly git export bash export export export export cortex make make cortex make make path image take form building predictor image might fail pip version frozen cortex edit predictor install version pip may run python get around search find relevant file add new requirement force installation older version compatible python built navigate cortex version cluster installation find complete list used cluster set point new registry look something like complete may change based cortex version region region region region region region region region region region region region region navigate cortex version spec find relevant image need set spec update accordingly spin new cluster pointing new verify working,issue,negative,positive,neutral,neutral,positive,positive
803254525,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/cortexlabs/cortex?pullRequest=1986) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/cortexlabs/cortex?pullRequest=1986) before we can accept your contribution.<br/>**1** out of **2** committers have signed the CLA.<br/><br/>:white_check_mark: miguelvr<br/>:x: dependabot[bot]<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/cortexlabs/cortex?pullRequest=1986) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept bot sub already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
801516189,Thanks for the update @vishalbollu. Is there an issue for the Tensorflow predictor type that I could follow?,thanks update issue predictor type could follow,issue,negative,positive,positive,positive,positive,positive
801515686,"AsyncAPI is currently not compatible with Tensorflow predictor type in this release, only the Python predictor type is. However we are planning to iterate on AsyncAPI further and will keep you posted. Here are the [docs](https://docs.cortex.dev/workloads/introduction) for async.",currently compatible predictor type release python predictor type however iterate keep posted,issue,negative,neutral,neutral,neutral,neutral,neutral
801504483,This is great! A couple of questions. Can this be used with the TensorFlow predictor (i.e. will `self.client.predict` return a job id)? Will docs for this be released soon?,great couple used predictor return job id soon,issue,positive,positive,positive,positive,positive,positive
800322552,I think the behavior it's fine as it is and it will probably create more headaches than advantages. It is also consistent with the Kubernetes resources limits behavior.,think behavior fine probably create also consistent behavior,issue,negative,positive,positive,positive,positive,positive
799318155,"this is not an end-to-end test and it will run already on every PR, so I think it is not necessary",test run already every think necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
799315890,@jackmpcollins just run `make format` to fix the linting errors,run make format fix,issue,negative,neutral,neutral,neutral,neutral,neutral
797657430,"> is this issue still open to pick?

feel free to contribute! this would be a cool one",issue still open pick feel free contribute would cool one,issue,positive,positive,positive,positive,positive,positive
797625482,is this issue still open to pick?,issue still open pick,issue,negative,neutral,neutral,neutral,neutral,neutral
793808881,"> After reading this thread and #973 it isn't clear to me yet how to deploy a container locally.

it's not possible to deploy cortex locally anymore, since version 0.26
If you really need to run cortex locally, you can use an earlier version",reading thread clear yet deploy container locally possible deploy cortex locally since version really need run cortex locally use version,issue,negative,positive,neutral,neutral,positive,positive
793792893,After reading this thread and https://github.com/cortexlabs/cortex/pull/973 it isn't clear to me yet how to deploy a container locally.,reading thread clear yet deploy container locally,issue,negative,positive,neutral,neutral,positive,positive
791739343,"Sounds great, thank you! Could you please email me at omer@cortexlabs.com so that we can set up a call?",great thank could please omer set call,issue,positive,positive,positive,positive,positive,positive
791734876,Thanks @ospillinger. We'd be happy to give some more details on our use case. I'm including @adamwylie and @MyNameIsURL as they have the best understanding of the details.,thanks happy give use case best understanding,issue,positive,positive,positive,positive,positive,positive
791011673,"* Converted the test script to a unit test.
* Refactored the `thread_id*` symbols to `sample_id*`.

@cbensimon let me know if the unit tests look good to you too. Thanks again for your effort!!",converted test script unit test let know unit look good thanks effort,issue,positive,positive,positive,positive,positive,positive
790500407,"Hi @RobertLucian, the library is great, that's a pleasure to contribute !

Yes, I think that your view on the problem is right

---

I think that the max batch size is already enforced as long as it equals num threads (which is currently mandatory) : each thread can add a sample to the queue, one at a time, therefore the sample queue cannot exceed the number of threads (for each thread the cycle is : add a sample, wait for the prediction, prediction is ready so the sample is deleted, add another sample, etc..)

But it is totally ok to manually enforce this anyway

---

Additional note : with the new way of generating ""thread"" ID (itertools.count), I realize it actually generates a ""sample"" ID, so maybe that it would be more explicit to rename `thread_id*` to `sample_id*`


",hi library great pleasure contribute yes think view problem right think batch size already enforced long currently mandatory thread add sample queue one time therefore sample queue exceed number thread cycle add sample wait prediction prediction ready sample add another sample totally manually enforce anyway additional note new way generating thread id realize actually sample id maybe would explicit rename,issue,positive,positive,positive,positive,positive,positive
790045484,"@cbensimon this is really awesome!! Thanks for catching this bug!!

So, to re-summarize the problem at hand, we can start by looking at the following code-block:

```python
def _enqueue_request(self, **kwargs):
        """"""
        Enqueue sample for batch inference. This is a blocking method.
        """"""
        thread_id = td.get_ident()

        self.waiter.wait()
        self.samples[thread_id] = kwargs
        try:
            self.barrier.wait()
        except td.BrokenBarrierError:
            pass
```

Let's assume the waiter is set and prediction requests are coming in:

1. A bunch of requests will be appended to the `self.samples` list until the barrier is waited `n` times.
2. There's a chance that when the last breaking wait is being waited on, there are still some that may have made it past the `self.waiter.wait()` instruction, got added to the `self.samples` list all while at the same time, the `self.samples` list already got passed to the `self._make_batch` method. If that were to happen, I then think that at the end of the batch, the samples that didn't get processed (by having been passed on to the `self._make_batch` method) will get removed and never get to be processed.

The above situation can lead `_get_prediction` to hang indefinitely on the respective threads. Do you confirm this?

---

With this change in the code, a new problem arises - the max batch size is no longer enforced. So for that, I tweaked the code a bit to pick the thread IDs with the smallest IDs in case there are more samples than the max batch size. And since the number of threads is a known number, we can be sure that the queue doesn't grow to enormous sizes.

Let me know what you think about this!

---

Also added a test script (the one which I also used for testing). This isn't the final one - it's just temporary. We shall convert that to a unit test in our test suite.",really awesome thanks catching bug problem hand start looking following python self sample batch inference blocking try except pas let assume waiter set prediction coming bunch list barrier time chance last breaking wait still may made past instruction got added list time list already got method happen think end batch get method get removed never get situation lead indefinitely respective confirm change code new problem batch size longer enforced code bit pick thread case batch size since number known number sure queue grow enormous size let know think also added test script one also used testing final one temporary shall convert unit test test suite,issue,positive,positive,positive,positive,positive,positive
789249168,"Cluster specific auth has been addressed in https://github.com/cortexlabs/cortex/pull/1908. Rather than creating service accounts, we have decided to go with attaching policies to the roles for the cluster.",cluster specific rather service decided go cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
788299829,"The features of Cortex that used to manage docker container deployments (also referred to as Cortex local) has been deprecated and is no longer being supported. We happened to build a model server along our journey to building a distributed model inference cluster. Creating a model server isn't our primary focus. 

Having said that, if you would like adopt Cortex local for a different architecture, you can take a look at [Cortex v0.25](https://github.com/cortexlabs/cortex/tree/v0.25.0) which is the last version of Cortex with local support. The requirements listed in this ticket pertain to making the different components of Cortex cluster compatible with ARM before ARM instances can be supported. From the top of my head, Cortex local relies on Docker. You may have to recompile the cortex go binary for your architecture as well.",cortex used manage docker container also cortex local longer build model server along journey building distributed model inference cluster model server primary focus said would like adopt cortex local different architecture take look cortex last version cortex local support listed ticket pertain making different cortex cluster compatible arm arm top head cortex local docker may recompile cortex go binary architecture well,issue,positive,positive,neutral,neutral,positive,positive
788276461,"Thank you for letting us know, we'll try to prioritize it for one of our upcoming releases. Would you be open to sharing a little bit more about your use case? I'd be happy to schedule some time to chat if you're open to it.",thank u know try one upcoming would open little bit use case happy schedule time chat open,issue,positive,positive,positive,positive,positive,positive
788119328,"Are the required enhancements listed the same for say running the realtime API locally on a Jetson? I am considering taking a swing at this vs. using another model server, I would much rather use Cortex.

The CLI fails to run at all so I guess that would need to be fixed also:

```bash
datenstrom@ant:~$ cortex
Traceback (most recent call last):
  File ""/home/datenstrom/.local/bin/cortex"", line 8, in <module>
    sys.exit(run())
  File ""/home/datenstrom/.local/lib/python3.6/site-packages/cortex/binary/__init__.py"", line 32, in run
    process = subprocess.run([get_cli_path()] + sys.argv[1:], cwd=os.getcwd())
  File ""/usr/lib/python3.6/subprocess.py"", line 423, in run
    with Popen(*popenargs, **kwargs) as process:
  File ""/usr/lib/python3.6/subprocess.py"", line 729, in __init__
    restore_signals, start_new_session)
  File ""/usr/lib/python3.6/subprocess.py"", line 1364, in _execute_child
    raise child_exception_type(errno_num, err_msg, err_filename)
OSError: [Errno 8] Exec format error: '/home/datenstrom/.local/lib/python3.6/site-packages/cortex/binary/cli'
```",listed say running locally considering taking swing another model server would much rather use cortex run guess would need fixed also bash ant cortex recent call last file line module run file line run process file line run process file line file line raise format error,issue,negative,positive,neutral,neutral,positive,positive
787844659,"@vishalbollu I just remember, that it might be relevant to update the metrics in the grafana dashboards",remember might relevant update metric,issue,negative,positive,positive,positive,positive,positive
787817655,@cbensimon thanks for your response. We will try to reproduce it and we'll get back to you!,thanks response try reproduce get back,issue,negative,positive,neutral,neutral,positive,positive
787815708,"Hi @miguelvr,
This happens after some amount of time.
I tested it with a dummy predictor function :
```
def predict(self, payload):
        time.sleep(0.2)
        return payload
```

I fixed the max batch size to 32 

Then, the lower the batch interval is set, the quicker the threads are going to be stuck forever waiting for their respective predictions.

Setting the batch interval around 500ms makes the bug appear after a few minutes of API stress loading (64 concurrent workers)

To reproduce the bug quickly, setting the batch interval to a lower value like 1ms is the best option

I inspected the threads using pystuck and saw the threads were stuck waiting for their respective predictions",hi amount time tested dummy predictor function predict self return fixed batch size lower batch interval set going stuck forever waiting respective setting batch interval around bug appear stress loading concurrent reproduce bug quickly setting batch interval lower value like best option saw stuck waiting respective,issue,positive,positive,positive,positive,positive,positive
787810212,"HI @cbensimon,
Can you elaborate as to when the dynamic batcher hangs? We had tested it and haven't observed any hanging behavior. Is it in an edge case or all the time?",hi elaborate dynamic batcher tested hanging behavior edge case time,issue,negative,positive,positive,positive,positive,positive
787525530,This would be a great feature for us!,would great feature u,issue,positive,positive,positive,positive,positive,positive
786369519,This conversation has a lot of useful information that extend beyond the scope of the initial ticket. The ideas discussed in this ticket regarding authorization have been summarized and moved to this ticket https://github.com/cortexlabs/cortex/issues/1748. Feel free to add any additional context.,conversation lot useful information extend beyond scope initial ticket ticket regarding authorization ticket feel free add additional context,issue,positive,positive,positive,positive,positive,positive
786131726,"@deliahu Alright, I‚Äôll keep that in mind! I just need to provide a number of users with an option to run image classification ML inference online. I‚Äôll keep you posted!
",alright keep mind need provide number option run image classification inference keep posted,issue,negative,neutral,neutral,neutral,neutral,neutral
784450929,I appreciate your interest in our project. We are planning to focus on AWS and GCP in the foreseeable future so we don't have a timeline for IBM Cloud at this time.,appreciate interest project focus foreseeable future cloud time,issue,positive,neutral,neutral,neutral,neutral,neutral
783919021,"@da-source those limits should not be a problem, since there will only be up to 3 Auto Scaling Groups and Launch Configurations per Cortex cluster, regardless of the number of instances in the cluster.

Also, I wanted to make sure that you are aware that the 5000 vCPU limit is regardless of the number of instances; in fact, at this scale, I'd recommend going with large instances, e.g. `m5.24xlarge` (or similar, depending on your use case). With this instance type, your cluster will have fewer instances, which means less overhead and less stress on the control plane. Keep us posted on how you decide to set it up, and how it goes for you; this sounds like a super interesting use case! What is the nature of your application?",problem since auto scaling launch per cortex cluster regardless number cluster also make sure aware limit regardless number fact scale recommend going large similar depending use case instance type cluster le overhead le stress control plane keep u posted decide set go like super interesting use case nature application,issue,positive,positive,positive,positive,positive,positive
781578156,"Awesome!! @miguelvr gets all the credit, it was actually his suggestion :)",awesome credit actually suggestion,issue,positive,positive,positive,positive,positive,positive
781221677,"It finally worked, after making the `api` NS record change! I get `{""message"":""make a prediction by sending a post request to this endpoint with a json payload""}` when calling `curl api.my-domain.net/iris`. Thanks!",finally worked making record change get message make prediction sending post request calling curl thanks,issue,negative,positive,neutral,neutral,positive,positive
781219678,"I've just updated current NS records to be `api` instead of `api.my-domain.net`.
I tried `curl api.my-domain.net.my-domain.net/iris` (replacing `my-domain` with my actual domain-name), before editing the NS records, but it didn't work.
I've sent the screenshot of the DNS panel to dev@cortex.dev",current instead tried curl actual work sent panel dev,issue,negative,neutral,neutral,neutral,neutral,neutral
780787668,"I see.

I don't think creating an A record for a subdomain in GoDaddy is the correct approach. The next thing I would try is to remove the NS records in GoDaddy, and then make new NS records in GoDaddy like before, except this time, instead of using `api.my-domain.net` as the name, try using just `api`. I believe these are equivalent in Google domains, but perhaps that is not the case in GoDaddy (maybe they would treat a name value of `api.my-domain.net` as `api.my-domain.net.my-domain.net`). Perhaps before deleting the current NS records, it could be worth seeing if `curl api.my-domain.net.my-domain.net/iris` works. Although even if it doesn't work, I still think it would be worth trying what I described above.

(please be sure to replace `my-domain` in all examples above)",see think record correct approach next thing would try remove make new like except time instead name try believe equivalent perhaps case maybe would treat name value perhaps current could worth seeing curl work although even work still think would worth trying please sure replace,issue,positive,positive,positive,positive,positive,positive
780420611,"Oh, I see.

48 hours have passed now for sure, but curl api.my-domain.net/iris still gives curl: (6) Could not resolve host: api.my-domain.net,
even though curl http://acbaffb42affb40dab522f6b68467bba-a91cd3894d755e24.elb.us-east-2.amazonaws.com/iris gives ""message"":""make a prediction by sending a post request to this endpoint with a json payload"". After redeploying my cluster, I had to make Alias record change in Route 53, to update the load balancer value. I think that the Alias record changes propagate faster and shouldn‚Äôt take 48 hours and if not, should I keep my cluster up for longer, to allow alias change to take place?
Have you received the screenshots by email? Perhaps it is neccessary to do this: https://pk.godaddy.com/help/create-a-subdomain-4080 ?

*Also, I haven't yet generated SSL certificate for my custom domain (the skippable section in the tutorial). Could that be causing the problem?",oh see sure curl still curl could resolve host even though curl message make prediction sending post request cluster make alias record change route update load balancer value think alias record propagate faster take keep cluster longer allow alias change take place received perhaps also yet certificate custom domain skippable section tutorial could causing problem,issue,positive,positive,positive,positive,positive,positive
780325929,"@nirbd We have now updated the docs and added a helper script in v0.29 to make this process smoother (you can update the Python version via `dependencies.sh` ([docs](https://www.docs.cortex.dev/workloads/dependencies/system-packages)) or custom Docker images ([docs](https://www.docs.cortex.dev/workloads/dependencies/images)).

Thanks for flagging this!",added helper script make process smoother update python version via custom docker thanks flagging,issue,positive,positive,positive,positive,positive,positive
780324034,I'm going to go ahead and close this ticket since it seems like it's working as expected. Let me know if you feel otherwise!,going go ahead close ticket since like working let know feel otherwise,issue,negative,neutral,neutral,neutral,neutral,neutral
780323472,I'm gonna go ahead and close this ticket since I believe it's a duplicate of https://github.com/cortexlabs/cortex/issues/1828,gon na go ahead close ticket since believe duplicate,issue,negative,neutral,neutral,neutral,neutral,neutral
780303429,"@imagine3D-ai sending a request to the bare load balancer URL is not expected to respond; the message you got back when sending a request to your API endpoint is correct!

Do you have the custom domain set up now? Can you confirm that `curl a0e38071d793649269520afea7225115-efbefc8af26102fc.elb.us-east-2.amazonaws.com/iris` still works, and then try `curl my.domain.com/iris` (replace `my.domain.com` accordingly)?",sending request bare load balancer respond message got back sending request correct custom domain set confirm curl still work try curl replace accordingly,issue,negative,positive,neutral,neutral,positive,positive
780268639,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
779686634,Just tried the same with `0.28` and no output is given when calling the load balancer,tried output given calling load balancer,issue,negative,neutral,neutral,neutral,neutral,neutral
779649069,"Yes, with `curl a0e38071d793649269520afea7225115-efbefc8af26102fc.elb.us-east-2.amazonaws.com/iris` I get:
`{""message"":""make a prediction by sending a post request to this endpoint with a json payload""}`,
but with `curl http://a0e38071d793649269520afea7225115-efbefc8af26102fc.elb.us-east-2.amazonaws.com`, I get an empty response. I'm using `cortex 26.0`
",yes curl get message make prediction sending post request curl get empty response cortex,issue,negative,negative,neutral,neutral,negative,negative
779471721,"@imagine3D-ai This is not normal, and we should first determine why the load balancer URL is not working.

When you run `cortex get API_NAME`, it should print out the endpoint for your API. If you use curl to make a GET request to that endpoint (e.g. `curl http://a6b379e30fac64b4da92b3856c47a647-95c3df6d162d5fd7.elb.us-west-2.amazonaws.com/iris-classifier`), do you get a response back?",normal first determine load balancer working run cortex get print use curl make get request curl get response back,issue,negative,positive,positive,positive,positive,positive
779278847,"I've sent the files to `dev@cortex.dev`

When doing the `curl` requests to my http and https (`curl -k`) load balancer and nothing happens. Is this normal?",sent dev curl curl load balancer nothing normal,issue,negative,positive,positive,positive,positive,positive
778846209,"I would have expected the DNS to be good to go by now, so my guess is that something else is not configured correctly.

Do you mind sending screenshots of your DNS record and Route 53 configuration, as well as your cluster configuration file? Also, if you try `curl` with the load balancer URL (not the custom domain), does it work? You can try both `http` and `https` (when using `https` with the load balancer url, you will need to use `curl -k` to skip certificate verification)",would good go guess something else correctly mind sending record route configuration well cluster configuration file also try curl load balancer custom domain work try load balancer need use curl skip certificate verification,issue,positive,positive,positive,positive,positive,positive
778754701,"I've added the GoDaddy `NS` records yesterday and I still get the `curl: (6) Could not resolve host:` error today when testing connectivity by:

1) redeploying the cluster
2) deploying an api
3) making subdomain `curl` request: `curl https://api.mydomain.net/testapi`

GoDaddy says that it can take up to 48 hours for the new records to be active, so maybe that is the issue and I'll try again later.

When re-testing today, I went to the `AWS Route 53` Console to make `load balancer` endpoint alias change, assuming that the cluster that I redeployed will have a new `load balancer` endpoint, but it didn't, although the `operator endpoint` endpoint of my today's cluster was different from the one my cluster had yesterday. I hope that's normal. 
",added yesterday still get curl could resolve host error today testing connectivity cluster making curl request curl take new active maybe issue try later today went route console make load balancer alias change assuming cluster new load balancer although operator today cluster different one cluster yesterday hope normal,issue,positive,positive,neutral,neutral,positive,positive
778719093,"@mutal It is very possible (even likely) that AWS API Gateway does not support SSE. Therefore, if you need https, I would recommend using a custom SSL certificate ([instructions](https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/custom-domain)). Then, you would still be able to send the request directly to the load balancer (rather than going through API Gateway)",possible even likely gateway support therefore need would recommend custom certificate would still able send request directly load balancer rather going gateway,issue,positive,positive,positive,positive,positive,positive
778718860,"@imagine3D-ai You will need to update your DNS settings in GoDaddy to point your subdomain to the AWS Route 53 name servers. This is accomplished by adding an `NS` record with GoDaddy (the guide you linked to shows how to do the same thing with Google Domains, but it should be similar with GoDaddy). Here is a [GoDaddy guide](https://www.godaddy.com/help/add-an-ns-record-19212) which should help.",need update point route name accomplished record guide linked thing similar guide help,issue,positive,positive,neutral,neutral,positive,positive
778655074,"I'm trying to implement `https:// `by creating the custom domain. In this tutorial (https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/custom-domain), in the `Configure DNS` part, it is said that I need to choose a subdomain. I'm using GoDaddy to host my website and I was wondering if I need to explicitly create my subdomain (`api.my-domain.net`). I followed through the instructions without explicitly creating the subdomain with GoDaddy and when I tried accessing the `api.my-domain.net`, there seem to be nothing there, just like with `api.cortexlabs.dev`. Also, when making a curl request with `api.my-domain.net`, I got an `curl: (6) Could not resolve host:` error. Was it due to not creating the subdomain w/ GoDaddy, before following through the instructions?",trying implement custom domain tutorial configure part said need choose host wondering need explicitly create without explicitly tried seem nothing like also making curl request got curl could resolve host error due following,issue,negative,negative,neutral,neutral,negative,negative
778653387,"@deliahu  Hi, I noticed that I can't get the stream when using an `https://` API requests. I implemented the https Cortex API by using AWS API GATEWAY (https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/https), and the output is outputed only when the whole process has finished, while the http version works fine. Can the output stream be made to work with https?",hi ca get stream cortex gateway output whole process finished version work fine output stream made work,issue,negative,positive,positive,positive,positive,positive
776735011,"1 m5.large instance has 2 vCPUs, if you use them fully or not, it's up to you. This error is from your AWS account. Basically what it says is that you do not have permission (quota) to launch more than 500 vCPUs. Since 500 m5.large = 1000 vCPUs, it raises an error.

There are 2 solutions:
1. Increase the vCPU quota on your AWS account
2. Change instance type to something that has 1 vCPU (not sure if this fits your use case, with a lot of memory)
",instance use fully error account basically permission quota launch since error increase quota account change instance type something sure use case lot memory,issue,negative,positive,positive,positive,positive,positive
776306852,"@JackHopkins the docs have been updated in the latest version (0.28), but we can still merge this into the 0.27 branch. The merging is blocked on the CLA check, do you mind signing it [here](https://cla-assistant.io/cortexlabs/cortex?pullRequest=1839)?",latest version still merge branch blocked check mind,issue,negative,positive,positive,positive,positive,positive
775663704,"@sebastianschramm I just wanted to follow up on this thread, to say that we now support installing Cortex on an existing Kubernetes cluster (on AWS or GCP); here are the [docs](https://docs.cortex.dev/clusters/cortex-core-on-kubernetes/install).",follow thread say support cortex cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
775341820,Just adding to this. It would be nice to expose `tensorflow_client._spec_model_names` as well.,would nice expose well,issue,positive,positive,positive,positive,positive,positive
775238563,"You would just have to do that once, to configure the APIs gateway. In your case, it would be easier to use the AWS API Gateway for easy HTTPS setup (https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/https)",would configure gateway case would easier use gateway easy setup,issue,positive,positive,positive,positive,positive,positive
775184032,"Part of my application requires dynamically creating and invoking Cortex APIs, but in the links that you‚Äôve provided, https must be enabled manually by going to the AWS Console. Is there a way to programatically  make Cortex APIs `https` available?",part application dynamically cortex link provided must manually going console way make cortex available,issue,negative,positive,positive,positive,positive,positive
775068157,"If you don't need a custom domain: 
https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/https

If you need a custom domain:
https://docs.cortex.dev/clusters/cortex-cloud-on-aws/index/custom-domain",need custom domain need custom domain,issue,negative,neutral,neutral,neutral,neutral,neutral
775039106,"Thanks, for responding! I contacted the `gpt2tc` creator about the issue and turns there was some internal problem within the program, which would sometimes make it fail to launch. So the issue wasn't really the `source.stream()` failing to open, but rather my external program failing to launch. I then simply changed the gpt2tc version and the issue dissapeared. ",thanks creator issue turn internal problem within program would sometimes make fail launch issue really failing open rather external program failing launch simply version issue,issue,negative,negative,neutral,neutral,negative,negative
774904554,"I'm not sure what the issue is related to, and I don't know whether closing the process is necessary.

There is one more place we can try to add a `console.log(e)`, which should capture all events: In `sse.js` on line 65 ( `e.source = this;`), right after that line, you can add `console.log(e)`. This should run on all events. Do you see any different behavior when the stream closes immediately versus when it works as intended?

If not, the only other thing I can think of would be to return some text from the API which would signal success (and check for that in the client), to retry if the stream closed before any output was sent (you can use a local variable to track the output), or to check how much time has passed after calling `source.stream();` until the stream is closed, and retrying if it was too fast. Probably the second of those three options makes the most sense.",sure issue related know whether process necessary one place try add capture line right line add run see different behavior stream immediately versus work intended thing think would return text would signal success check client retry stream closed output sent use local variable track output check much time calling stream closed fast probably second three sense,issue,positive,positive,positive,positive,positive,positive
774894255,"Splitting it will only be more expensive if you plan on having fewer instances than models (otherwise it should be the same). Yes, it is a bit more complex if you split them. However the simplicity on the infrastructure side can have some advantages too (such as independent autoscaling, monitoring, etc for each API)",splitting expensive plan otherwise yes bit complex split however simplicity infrastructure side independent,issue,negative,negative,negative,negative,negative,negative
774892809,"I think this is happening because `on_demand_backup` is enabled, so in theory, it could request 100% on-demand instances if spot instances are not available.

This does seem like maybe it's too aggressive of a check, perhaps we should allow the check to be bypassed. On the other hand, it is possible (even somewhat common) for the GPU spot instances to be completely unavailable in some regions. Are you able to request a limit increase from AWS?",think happening theory could request spot available seem like maybe aggressive check perhaps allow check hand possible even somewhat common spot completely unavailable able request limit increase,issue,positive,positive,positive,positive,positive,positive
774884511,"Yes. That's the one!

I could split it into 6 separate APIs and may end up doing that. It's just more expensive/complex.",yes one could split separate may end,issue,negative,neutral,neutral,neutral,neutral,neutral
774882900,"```
# EKS cluster name
cluster_name: foo

# AWS region
region: us-east-1

# list of availability zones for your region
availability_zones: # default: 3 random availability zones in your region, e.g. [us-east-1a, us-east-1b, us-east-1c]

# instance type
instance_type: g4dn.xlarge

# minimum number of instances
min_instances: 6

# maximum number of instances
max_instances: 6  # <- errors out if I set this any higher even though spot G quota level is 120

# disk storage size per instance (GB)
instance_volume_size: 50

# instance volume type [gp2 | io1 | st1 | sc1]
instance_volume_type: gp2

# instance volume iops (only applicable to io1)
# instance_volume_iops: 3000

# subnet visibility [public (instances will have public IPs) | private (instances will not have public IPs)]
subnet_visibility: private

# NAT gateway (required when using private subnets) [none | single | highly_available (a NAT gateway per availability zone)]
nat_gateway: single

# API load balancer scheme [internet-facing | internal]
api_load_balancer_scheme: internal

# operator load balancer scheme [internet-facing | internal]
# note: if using ""internal"", you must configure VPC Peering to connect your CLI to your cluster operator
operator_load_balancer_scheme: internet-facing

# to install Cortex in an existing VPC, you can provide a list of subnets for your cluster to use
# subnet_visibility (specified above in this file) must match your subnets' visibility
# this is an advanced feature (not recommended for first-time users) and requires your VPC to be configured correctly; see https://eksctl.io/usage/vpc-networking/#use-existing-vpc-other-custom-configuration
# here is an example:
# subnets:
#   - availability_zone: us-west-2a
#     subnet_id: subnet-060f3961c876872ae
#   - availability_zone: us-west-2b
#     subnet_id: subnet-0faed05adf6042ab7

# additional tags to assign to AWS resources (all resources will automatically be tagged with cortex.dev/cluster-name: <cluster_name>)
tags: # <string>: <string> map of key/value pairs

# whether to use spot instances in the cluster (default: false)
spot: true

spot_config:
  # additional instance types with identical or better specs than the primary cluster instance type (defaults to only the primary instance type)
  instance_distribution: # [similar_instance_type_1, similar_instance_type_2]

  # minimum number of on demand instances (default: 0)
  on_demand_base_capacity: 0

  # percentage of on demand instances to use after the on demand base capacity has been met [0, 100] (default: 50)
  # note: setting this to 0 may hinder cluster scale up when spot instances are not available
  on_demand_percentage_above_base_capacity: 0

  # max price for spot instances (default: the on-demand price of the primary instance type)
  max_price: # <float>

  # number of spot instance pools across which to allocate spot instances [1, 20] (default: number of instances in instance distribution)
  instance_pools: 3

  # fallback to on-demand instances if spot instances were unable to be allocated (default: true)
  on_demand_backup: true

# SSL certificate ARN (only necessary when using a custom domain)
ssl_certificate_arn:

# primary CIDR block for the cluster's VPC
vpc_cidr: 192.168.0.0/16

```",cluster name foo region region list availability region default random availability region instance type minimum number maximum number set higher even though spot quota level disk storage size per instance instance volume type io st instance volume applicable io visibility public public private public private nat gateway private none single nat gateway per availability zone single load balancer scheme internal internal operator load balancer scheme internal note internal must configure connect cluster operator install cortex provide list cluster use file must match visibility advanced feature correctly see example additional assign automatically tagged string string map whether use spot cluster default false spot true additional instance identical better spec primary cluster instance type primary instance type minimum number demand default percentage demand use demand base capacity met default note setting may hinder cluster scale spot available price spot default price primary instance type float number spot instance across allocate spot default number instance distribution fallback spot unable default true true certificate arn necessary custom domain primary block cluster,issue,positive,positive,neutral,neutral,positive,positive
774877981,"@lminer Yes that is a good idea for a feature, and not something we currently support. If I understand correctly, is this ticket the same as your request? https://github.com/cortexlabs/cortex/issues/1828

Also, in this case, if you have 6 models and 6 replicas, is it possible for you to split it into 6 separate APIs?",yes good idea feature something currently support understand correctly ticket request also case possible split separate,issue,positive,positive,positive,positive,positive,positive
774876955,"@lminer I believe it does check both the spot and on-demand quotas independently, do you mind sending your cluster configuration file so I can double-check?",believe check spot independently mind sending cluster configuration file,issue,negative,neutral,neutral,neutral,neutral,neutral
774876680,"Here's my `cortex.yaml`

```
- name: foo
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: foo/cortex_server.py
    models:
      paths:
        - foo
      cache_size: 1
      disk_cache_size: 6
    processes_per_replica: 1
    threads_per_process: 1
    image: quay.io/cortexlabs/tensorflow-predictor:0.27.0
    tensorflow_serving_image: quay.io/cortexlabs/tensorflow-serving-gpu:0.27.0
  compute:
    gpu: 1
  autoscaling:
    min_replicas: 6
    max_replicas: 6
```",name foo kind predictor type path foo image compute,issue,positive,positive,positive,positive,positive,positive
774787130,"Yeah. If I request the same model repeatedly, I don't see the 10 second switching cost. I'm only caching one model at a time in memory, 7 on disk. This is because I'm using GPU instances and the models are too large to hold in memory all at the same time.",yeah request model repeatedly see second switching cost one model time memory disk large hold memory time,issue,negative,positive,positive,positive,positive,positive
773540227,"There doesn't seem to be any useful information when placing  `console.log(e)` in the `'message'` EventListener (the output is just empty when stream fails to open) and `console.log(e)` in the `'readystatechange'` seems to be the same when the stream closes after opening as it is after when stream fails to open:

```
CustomEvent {isTrusted: false, readyState: 2, source: SSE, detail: null, type: ""readystatechange"", ‚Ä¶}
bubbles: false
cancelBubble: false
cancelable: false
composed: false
currentTarget: null
defaultPrevented: false
detail: null
eventPhase: 0
isTrusted: false
path: []
readyState: 2
returnValue: true
source: SSE {INITIALIZING: -1, CONNECTING: 0, OPEN: 1, CLOSED: 2, url: ""http://a210757f9c8f148f3bcbedae4f9813e1-33465feef05d67ea.elb.us-east-2.amazonaws.com/fast8"", ‚Ä¶}
srcElement: null
target: null
timeStamp: 262056.92000000272
type: ""readystatechange""
__proto__: CustomEvent
```
Could the issue be due to not closing the process after opening the command with `asyncio.create_subprocess_shell` in the predictor?

```
async def run_command():
    st = os.stat('./myprogram')
    os.chmod('./myprogram', st.st_mode | stat.S_IEXEC ) 
    command = ""./myprogram -command""


    proc = await asyncio.create_subprocess_shell(
        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )

    while True:
        output = await proc.stdout.read(1)
        if not output:
            return
        yield output.decode('UTF-8', 'ignore')
```",seem useful information output empty stream open stream opening stream open false source detail null type false false cancelable false composed false null false detail null false path true source open closed null target null type could issue due process opening command predictor st command await command true output await output return yield,issue,positive,negative,negative,negative,negative,negative
773350451,"@deliahu addressed all the comments

let's sync in order to add the env vars / create the test environments",let sync order add create test,issue,negative,neutral,neutral,neutral,neutral,neutral
773046237,"@mutal there aren't any precautions I can think of. Are you able to `console.log` the entire event (e.g. `console.log(e)` should work) when it closes unexpectedly, and do you notice any useful debugging information?",think able entire event work unexpectedly notice useful information,issue,negative,positive,positive,positive,positive,positive
771914757,"@deliahu Random empty spaces in the output, turns out, where due to the bug in my model which I was able to fix. 

However, after making 2-3 few `source.stream()` calls, stream sometimes doesn't open and I get `CLOSED` message in the console immediatly after pressing the button which makes the call, followed by `CLIPPED` (which indicates that the output was trimmed). So after pressing the button a couple of times, `source.steam()` works perfectly and `clip()` trims the output, but after about third time, the stream doesn't open (immediate `CLOSED` message) and the output in the textbox gets trimmed before being increased, so the part of textbox data is lost, and after pressing the button a few more times `source.stream()` starts sending data again.  I ran `cortex logs` but there was no indication of the stream failing to open. Are there any precautions that need to be made, to avoid `stream.source()` start failure?",random empty output turn due bug model able fix however making stream sometimes open get closed message console pressing button call clipped output pressing button couple time work perfectly clip output third time stream open immediate closed message output part data lost pressing button time sending data ran cortex indication stream failing open need made avoid start failure,issue,negative,positive,neutral,neutral,positive,positive
771829908,"This can be done manually using 
```
from starlette.responses import Response
import gzip

return Response(
    gzip.compress(bytes(json.dumps(result), ""utf-8"")),
    status_code=200,
    headers={""Content-Encoding"": ""gzip""},
    media_type=""application/json"",
)

```",done manually import response import return response result,issue,negative,neutral,neutral,neutral,neutral,neutral
771381746,"@playground I have created a separate issue for supporting IBM Cloud: https://github.com/cortexlabs/cortex/issues/1848, since this issue will be closed in our next release",playground separate issue supporting cloud since issue closed next release,issue,negative,positive,neutral,neutral,positive,positive
771380927,"@mutal yes that is a good guess. You can confirm this by checking: if you add the `console.log(""CLOSED"")` where I had it for debugging, do you see ""CLOSED"" printed multiple times on the same request?",yes good guess confirm add closed see closed printed multiple time request,issue,positive,positive,positive,positive,positive,positive
771289411,"@RobertLucian Appreciate your quick response, I will try it on my side. ",appreciate quick response try side,issue,negative,positive,positive,positive,positive,positive
770972046,"@dorkforcesron you have a few ways of customizing your API.
1. For installing Python dependencies with pip or conda, check out this [page](https://docs.cortex.dev/workloads/managing-dependencies/python-packages).
2. For installing generic system packages, check out [this](https://docs.cortex.dev/workloads/managing-dependencies/system-packages).
3. For creating your own image using ours as a base, check out [this](https://docs.cortex.dev/workloads/managing-dependencies/images). For this, we recommend using the slim images.",way python pip check page generic system check image base check recommend slim,issue,negative,negative,negative,negative,negative,negative
770425475,"Thanks for looking into it! I‚Äôm running a test clip() function as callback (which removes few characters at the end of the output) and with `readystatechange` the output is no longer clipped before the stream starts, however parts of the output now just get clipped randomly multiple times as output is being streamed. I‚Äôm guessing this is because the stream gets periodically ‚Äôclosed‚Äô as it runs asynchronously?",thanks looking running test clip function end output output longer clipped stream however output get clipped randomly multiple time output guessing stream periodically closed,issue,negative,positive,neutral,neutral,positive,positive
769588138,"@nirbd Yes, it is possible ot change the Python version. I'd recommend building your own Docker image if possible, since otherwise it would re-install Python every time each API replica initialized (this is possible; if you'd like to go with this approach to avoid building your own image, let me know and I can send the steps).

See the guide for [creating a custom image](https://docs.cortex.dev/workloads/managing-dependencies/images). In your `Dockerfile`, you would have something like this:

```Dockerfile
FROM quay.io/cortexlabs/python-predictor-cpu-slim:0.27.0  # use gpu-slim if you're running on GPUs

# update python runtime version
RUN conda install conda-forge::python=3.8
# and reinstall core cortex runtime dependencies
RUN pip --no-cache-dir install \
    -r /src/cortex/serve/serve.requirements.txt \
    /src/cortex/serve/
```

We recently realized that this is not well-documented and could be a smoother process, so we created https://github.com/cortexlabs/cortex/issues/1779 to improve this, and plan on addressing it in the next week or two!
",yes possible change python version recommend building docker image possible since otherwise would python every time replica possible like go approach avoid building image let know send see guide custom image would something like use running update python version run install reinstall core cortex run pip install recently could smoother process improve plan next week two,issue,positive,neutral,neutral,neutral,neutral,neutral
769584184,"@mutal the function passed into `source.addEventListener()` runs asynchronously whenever new data is sent, so `callback()` needs to run only after the connection is closed (in your code, `callback()` will be called right after starting the stream, rather than after the stream is closed).

There is an event for `readystatechange`, which can be used like this:

```javascript
  function post(callback) {
    var v = String(textArea.value);
    console.log(v);
    var source = new SSE(url, {
      method: 'POST',
      headers: { 'Content-Type': 'text/plain' },
      payload: v
    });
    var arr = [];
    source.addEventListener('message', function (e) {
      arr.push(e.data);
      textArea.value = arr.map(el => el || "" "").join(''); //format text-gen output
    });
    source.addEventListener('readystatechange', function (e) {
      if (e.readyState === source.CLOSED) {
        console.log(""CLOSED"");
        callback();
      }
    });
    source.stream();
  }
```",function whenever new data sent need run connection closed code right starting stream rather stream closed event used like function post string source new method function el el output function closed,issue,negative,positive,neutral,neutral,positive,positive
769443209,"Perhaps It takes longer for an instance to spin up if the autoscaling group (the cluster's workers nodes) already has instances. Or maybe on demand instances spin up faster than spot instances? Unfortunately, I don't have any more insight into this. At this point I believe we rely on AWS's ability to provision instances and add them to the cluster. If you do notice any reproducible patterns, please let us know.",perhaps longer instance spin group cluster already maybe demand spin faster spot unfortunately insight point believe rely ability provision add cluster notice reproducible please let u know,issue,negative,negative,negative,negative,negative,negative
769141304,I also noticed that sometimes it‚Äôs possible for a cluster to get a new EC2 instance very quickly (less than 30 seconds). I was wondering if it is possible to replicated that behaviour,also sometimes possible cluster get new instance quickly le wondering possible replicated behaviour,issue,negative,positive,positive,positive,positive,positive
769116025,"In the latest version of cortex this message has been updated to `waiting for replica/worker to initialize ...` to help reduce the confusion. Your API is not waiting to fetch the logs, your `cortex logs` command is waiting for a replica or worker to become ready before logs can be streamed to your terminal.

The underlying reason for why you see `fetching logs...` for a long period of team is because your API replicas are taking a while to initialize. The main reason why you might have to wait 1 to 2 minutes before a replica can initialize is because your cluster is autoscaling up and it takes that long to get a new EC2 instance from AWS. You can minimize this by configuring your cluster's `min_instances` to be non-zero (or enough to avoid scale-ups on initial deploys). 

There is a tradeoff between speeding up the initial api time and the cost of maintaining a minimum number of EC2 instances.",latest version cortex message waiting initialize help reduce confusion waiting fetch cortex command waiting replica worker become ready terminal underlying reason see fetching long period team taking initialize main reason might wait replica initialize cluster long get new instance minimize cluster enough avoid initial speeding initial time cost minimum number,issue,negative,positive,positive,positive,positive,positive
769077119,"I noticed when watching the logs that the most time-consuming part of launching an API is actually when it is in the `fetching logs...` state. 

`fetching logs..` takes about 2 minutes and downloading the image and the model takes about 1-1.5 minutes (after moving the image to ECR in the same region as EC2, as @deliahu suggested). Is there a way to reduce the time the API spends in `fetching logs` state when launching?",watching part actually fetching state fetching image model moving image region way reduce time fetching state,issue,negative,neutral,neutral,neutral,neutral,neutral
768978191,"closing this issue, as I think the question was answered already",issue think question already,issue,negative,neutral,neutral,neutral,neutral,neutral
768964327,"> If I have an EC2 instance running in `us-east-2`, which downloads the data from ECR located in the same region and I myself am in Europe and use that EC2 instance, will I get additional data transfer charges?

I don't think so, because the data is transferred within the same zone. You may want to check AWS docs on that",instance running data region use instance get additional data transfer think data transferred within zone may want check,issue,negative,neutral,neutral,neutral,neutral,neutral
768963613,"If I have an EC2 instance running in `us-east-2`, which downloads the data from ECR located in the same region and I myself am in Europe and use that EC2 instance, will I get additional data transfer charges?",instance running data region use instance get additional data transfer,issue,negative,neutral,neutral,neutral,neutral,neutral
768961608,"> If I'm in Europe and use the service which is hosted in `us-east-2`, will the perfomance I get be slower?

the latency will be larger if you are making requests across zones.

> or will there be any additional charges?

you might have data transfer charges from AWS, depending on what you are doing.",use service get latency making across additional might data transfer depending,issue,negative,neutral,neutral,neutral,neutral,neutral
768959615,"I have a question about aws regions: as I understand, the regions for all the aws services, need to be the same to maximize performance, but what does that mean for the users outside of the regions where services are located? If I'm in Europe and use the service which is hosted in `us-east-2`, will the perfomance I get be slower or will there be any additional charges?",question understand need maximize performance mean outside use service get additional,issue,negative,negative,negative,negative,negative,negative
768948210,"> @deliahu Is there an option to make custom code changes to the slim python predictor image? I would like to change the `predict()` and `post_predict()` functions to accommodate for my application

the `predict` and `post_predict` can be defined in the predictor.py, which will be loaded in the docker image, just as before.

To speed up the deployment you can do one of these options:

**Option 1 - Easiest option**
1. Change to the slim image in your `cortex.yaml` by setting the `image` field.
2. Implement the predictor just as before

**Option 2 - Most optimized option**
1. Create a custom docker image, by starting from the cortex slim image (as described in the [docs](https://docs.cortex.dev/workloads/managing-dependencies/images)) and add the dependencies your predictor needs to start-up (e.g. install the pip packages).
2. Host your image in a container registry of your choice
3. Set the custom image in the `image` field in your `cortex.yaml`
4. Implement the predictor just as before

Bear in mind that the start-up time of an API depends on different factors:
1. Size of the docker image that needs to be downloaded
2. Time that it takes to download and install the python dependencies
3. Time that takes to download your model

All of the above depend on the internet connection and therefore having everything running in the same region and zone, minimizes the download speeds",option make custom code slim python predictor image would like change predict accommodate application predict defined loaded docker image speed deployment one option easiest option change slim image setting image field implement predictor option option create custom docker image starting cortex slim image add predictor need install pip host image container registry choice set custom image image field implement predictor bear mind time different size docker image need time install python time model depend connection therefore everything running region zone,issue,positive,neutral,neutral,neutral,neutral,neutral
768938223,@deliahu Is there an option to make a custom code changes to the slim python predictor image? I would like to change the `predict()` and `post_predict()` functions to accomodate for my application,option make custom code slim python predictor image would like change predict application,issue,negative,neutral,neutral,neutral,neutral,neutral
768357837,"Apologies for the delayed response, other higher priority issues came up. We are still planning to address this issue in the next release (which is scheduled for next week).

The design you've suggested does make sense and that is how `create_api` is currently implemented. When you invoke `cx.create_api(predictor=PythonPredictor)` a temporary project directory is created and a pickled version of the PythonPredictor is written to that project directory. The pickling aspect is causing us problems when the predictor is imported (as you've noticed) and in some environments #1824. I have updated the proposed design accordingly to clearly reflect the different scenarios that need to be handled and how we plan on handling them.",response higher priority came still address issue next release next week design make sense currently invoke temporary project directory version written project directory aspect causing u predictor design accordingly clearly reflect different need handled plan handling,issue,negative,positive,neutral,neutral,positive,positive
767653775,"@deliahu I need to feed the output of my text-generator API to a sentiment-analyzer model and I'm having an issue with [sse.js](https://raw.githubusercontent.com/mpetazzoni/sse.js/master/lib/sse.js) library you were so kind to provide earlier.
Before feeding the text-gen output down the pipeline, text-generator's output in the textbox needs to be clipped and JS callback is not working for some reason:

```
url = ""http://a12b45ea28c0345f3aa0645b19e42138-a43d1b7cca7216fc.elb.us-east-1.amazonaws.com/generate-analyze"";
let textArea = document.getElementById(""value"");

<button type=""button"" onclick=""post(clip)"">get</button>
function clip(){
  s = textArea.value;
  s = s.slice(0, -5);
  textArea.value = s;
  console.log('ready');
}

function post(callback){
	var v = String(textArea.value);
	console.log(v);
	var source = new SSE(url, {
	  method: 'POST',
	  headers: { 'Content-Type': 'text/plain' },
	  payload: v
	});
	var arr = [];
	source.addEventListener('message', function (e) {
	  arr.push(e.data);
	  textArea.value = arr.map(el => el || "" "").join(''); //format text-gen output
	});
	source.stream();
        callback(); //run clip() after stream is done
}

```

The value is getting clipped before the SSE stream starts. Is JS callback not compatible with SSE functionality or might be there something wrong with `sse.js`?",need feed output model issue library kind provide feeding output pipeline output need clipped working reason let value button button post clip get function clip function post string source new method function el el output clip stream done value getting clipped stream compatible functionality might something wrong,issue,positive,positive,neutral,neutral,positive,positive
767315704,"""downloading the python serving image"" refers to downloading the docker image for the API.

By default, the image is hosted on quay.io, and likely has dependencies installed which you do not need. So, the best way to speed this up is to make the image smaller, and host the image on ECR in the same region as your cluster (assuming you're running on AWS). Here is our documentation for both of these approaches: https://docs.cortex.dev/workloads/managing-dependencies/images",python serving image docker image default image likely need best way speed make image smaller host image region cluster assuming running documentation,issue,positive,positive,positive,positive,positive,positive
766929304,I see. Wouldn't it then make more sense in the context of an interactive REPL to copy the referenced class to a temp folder and then trigger the project dir referencing that temp for? That way the two cases both do the same thing but with just one additional step at the start,see would make sense context interactive copy class temp folder trigger project temp way two thing one additional step start,issue,negative,neutral,neutral,neutral,neutral,neutral
766842037,"As far as I know, pickling is the only way to package a class definition that has been defined in an interactive REPL such as jupyter notebooks. 

`cortex.create_api(project_dir=""."", api_spec=api_spec)` does zip the directory/module. In the `api_spec` you can specify the file path (relative to the project_dir) to a predictor.py file containing a predictor class definition. This approach doesn't require any pickling. The project_dir approach is recommended approach in non jupyter settings because the entire project directory is sent to the serving container. Other files in your project can be imported in your predictor implementation and you can have imports at the top of predictor.py rather having imports within function scope.",far know way package class definition defined interactive zip specify file path relative file predictor class definition approach require approach approach non entire project directory sent serving container project predictor implementation top rather within function scope,issue,negative,positive,positive,positive,positive,positive
766183641,Perhaps a naive comment: Why is there a necessity to pickle in the first place? Isn't it easier to base64 encode the referenced file / module or zip it or some alternative method?,perhaps naive comment necessity pickle first place easier base encode file module zip alternative method,issue,negative,negative,negative,negative,negative,negative
766138021,"@deliahu It was a timing mistake on my part, it takes about 10 minutes for replicas to get deleted.",timing mistake part get,issue,negative,neutral,neutral,neutral,neutral,neutral
765741635,"@imagine3D-ai I have found another workaround until we fix https://github.com/cortexlabs/cortex/issues/1824; feel free to go with whichever approach you think is cleaner:

```bash
# first, create a lambda named ""test"" using the aws console. I used the default python version (3.8)
# then, using the aws console, add environment variables for `CORTEX_AWS_ACCESS_KEY_ID` and `CORTEX_AWS_SECRET_ACCESS_KEY`

mkdir my-function && cd my-function
pip3 install --target ./package cortex==0.27.0
cd package/ && zip -r ../my-deployment-package.zip . && cd ..
touch lambda_function.py
chmod 755 lambda_function.py # I was getting a permissions error without this

# edit lambda_function.py (see below)

zip -g my-deployment-package.zip lambda_function.py
aws lambda update-function-code --function-name test --zip-file fileb://my-deployment-package.zip

# click the ""test"" button in the aws console
```

```python
# lambda_function.py

import os
import json
import cortex
import dill


os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""


class PythonPredictor:
    def __init__(self, config):
        self.config = config

    def predict(self, payload):
        import names

        gender = payload.get(""gender"")
        return [names.get_full_name(gender=gender) for i in range(self.config[""num_names""])]


requirements = [""names""]

os.mkdir(""/tmp/name-generator"")

with open(""/tmp/name-generator/predictor.py"", ""w"") as f:
    f.write(dill.source.importable(PythonPredictor, source=True))

with open(""/tmp/name-generator/requirements.txt"", ""w"") as f:
    f.write(""\n"".join(requirements))


client = cortex.cluster_client(
    name=""aws"",
    provider=""aws"",
    operator_endpoint=""https://a9860a09b9eb949878c792254407d50c-c1a271a86dff290c.elb.us-west-2.amazonaws.com"",
    aws_access_key_id=os.getenv(""CORTEX_AWS_ACCESS_KEY_ID""),
    aws_secret_access_key=os.getenv(""CORTEX_AWS_SECRET_ACCESS_KEY""),
)

api_spec = {
    ""name"": ""name-generator"",
    ""kind"": ""RealtimeAPI"",
    ""predictor"": {
        ""type"": ""python"",
        ""path"": ""predictor.py"",
        ""config"": {""num_names"": 2},
    },
}


def lambda_handler(event, context):
    response = client.create_api(
        api_spec=api_spec,
        project_dir=""/tmp/name-generator"",
    )

    return {""statusCode"": 200, ""body"": response}
```",found another fix feel free go whichever approach think cleaner bash first create lambda test console used default python version console add environment pip install target zip touch getting error without edit see zip lambda test click test button console python import o import import cortex import dill class self predict self import gender gender return range open open client name kind predictor type python path event context response return body response,issue,positive,positive,positive,positive,positive,positive
765737203,"Also, I created https://github.com/cortexlabs/cortex/issues/1824 to track the issue with unpickling the Predictor class. @vishalbollu has made some progress in investigating this, so we should have the fix merged before our next release.",also track issue predictor class made progress investigating fix next release,issue,negative,neutral,neutral,neutral,neutral,neutral
765736794,"@imagine3D-ai I was able to get it working with the files-based approach like this:

```bash
# first, create a lambda named ""test"" using the aws console. I used the default python version (3.8)
# then, using the aws console, add environment variables for `CORTEX_AWS_ACCESS_KEY_ID` and `CORTEX_AWS_SECRET_ACCESS_KEY`

mkdir my-function && cd my-function
pip3 install --target ./package cortex==0.27.0
cd package/ && zip -r ../my-deployment-package.zip . && cd ..
touch lambda_function.py
mkdir name-generator && touch name-generator/predictor.py name-generator/requirements.txt
chmod 755 lambda_function.py name-generator name-generator/*  # I was getting a permissions error without this

# edit lambda_function.py, name-generator/predictor.py, and name-generator/requirements.txt (see below)

zip -g my-deployment-package.zip lambda_function.py name-generator name-generator/*  # I needed to add name-generator separately from name-generator/* to get the permissions to pass through
aws lambda update-function-code --function-name test --zip-file fileb://my-deployment-package.zip

# click the ""test"" button in the aws console
```

```python
# lambda_function.py

import os
import shutil
import json
import cortex

os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""
shutil.copytree(""./name-generator"", ""/tmp/name-generator"")  # I had to do this because cortex writes a temporary file to the project directory

client = cortex.cluster_client(
    name=""aws"",
    provider=""aws"",
    operator_endpoint=""https://a9860a09b9eb949878c792254407d50c-c1a271a86dff290c.elb.us-west-2.amazonaws.com"",
    aws_access_key_id=os.getenv(""CORTEX_AWS_ACCESS_KEY_ID""),
    aws_secret_access_key=os.getenv(""CORTEX_AWS_SECRET_ACCESS_KEY""),
)

api_spec = {
    ""name"": ""name-generator"",
    ""kind"": ""RealtimeAPI"",
    ""predictor"": {
        ""type"": ""python"",
        ""path"": ""predictor.py"",
        ""config"": {""num_names"": 2},
    },
}


def lambda_handler(event, context):
    response = client.create_api(
        api_spec=api_spec,
        project_dir=""/tmp/name-generator"",
    )

    return {""statusCode"": 200, ""body"": response}
```

```python
# name-generator/requirements.txt

names
```

```python
# name-generator/predictor.py

import names

class PythonPredictor:
    def __init__(self, config):
        self.config = config

    def predict(self, payload):
        gender = payload.get(""gender"")
        return [names.get_full_name(gender=gender) for i in range(self.config[""num_names""])]
```

```bash
# test it out

curl -H ""Content-Type: application/json"" -d '{""gender"":""male""}' http://a6168860e23fd434aaf54bad400725a3-aca70c6f2439b1cb.elb.us-west-2.amazonaws.com/name-generator
```

Let us know if that works for you!",able get working approach like bash first create lambda test console used default python version console add environment pip install target zip touch touch getting error without edit see zip add separately get pas lambda test click test button console python import o import import import cortex cortex temporary file project directory client name kind predictor type python path event context response return body response python python import class self predict self gender gender return range bash test curl gender male let u know work,issue,positive,positive,positive,positive,positive,positive
765634526,"I don't think you will have to make a change to `downscale_stabilization_period`, since the API replicas will not scale below 1. I also can't think of any other configuration parameters that would need to be changed",think make change since scale also ca think configuration would need,issue,negative,neutral,neutral,neutral,neutral,neutral
765626065,Thank you for the quick and detailed response! For our particular use-case it is actually just the endpoint that needs to have this format - which I have just discovered can be specified in the configuration yaml file for each api independently of its name. This works perfectly for us.,thank quick detailed response particular actually need format discovered configuration file independently name work perfectly u,issue,positive,positive,positive,positive,positive,positive
765487344,"Only affected a user running their API using the local provider.

The docker binary was where this panic came from and the root cause is explained here https://github.com/golang/go/wiki/LinuxKernelSignalVectorBug. To solve this, the user has to upgrade their OS' kernel.",affected user running local provider docker binary panic came root cause solve user upgrade o kernel,issue,negative,neutral,neutral,neutral,neutral,neutral
765399859,"@deliahu If I set the 'self-destruct' timer to 30 minutes, do I need to set `downscale_stabilization_period` to be 35 minutes (to make sure that replica won't spin down before the timer runs out) or is there some other parameter in `cortex.yaml` that needs to be changed?",set timer need set make sure replica wo spin timer parameter need,issue,negative,positive,positive,positive,positive,positive
765381846,"@deliahu I tried deploying two APIs and then deleting them, I made sure that no APIs were running by calling `cortex get `and then called `cortex cluster info`:
```
your cluster currently costs $0.28 per hour

aws resource                            cost per hour
1 eks cluster                           $0.10
2 instances for your apis               $0.076 total
2 50gb ebs volumes for your apis        $0.014 total
1 t3.medium instance for the operator   $0.0416
1 20gb ebs volume for the operator      $0.003
2 network load balancers                $0.045 total

your cluster has 0 API replicas running across 2 instances

instance type   lifecycle   replicas   CPU (requested / total allocatable)   memory (requested / total allocatable)
m5.large        spot        0          0m / 1290m                            0 / 6824584Ki
m5.large        spot        0          0m / 1290m                            0 / 6824584Ki
```

It shows that no APIs are running, but it appears that Cortex didn't delete those two instances",tried two made sure running calling cortex get cortex cluster cluster currently per hour resource cost per hour cluster total total instance operator volume operator network load total cluster running across instance type total allocatable memory total allocatable spot spot running cortex delete two,issue,negative,positive,neutral,neutral,positive,positive
765323450,"@RobertLucian just as a note, this can probably be simplified into

```go
if k8sJob == nil {
    return nil
}
```

as opposed to checking if it's different than nil everywhere",note probably simplified go nil return nil opposed different nil everywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
765186992,"@da-source Yes that should work! (assuming that the API will be deployed once; every time it's updated it would just restart the timer).

Something like this:

```python
import threading
import cortex

client = cortex.cluster_client(
    name=""aws"",
    provider=""aws"",
    operator_endpoint=""https://ac63318382b184ce7b03026c5f30fff3-b6bfe88ba5fe46e8.elb.us-west-2.amazonaws.com"",
    aws_access_key_id=""***"",
    aws_secret_access_key=""***"",
)


def delete_api():
    client.delete_api(""my-api"")


class PythonPredictor:
    def __init__(self, config):
        # initialize your model

        threading.Timer(20, delete_api).start()

    def predict(self, payload):
        return ""ok""
```",yes work assuming every time would restart timer something like python import import cortex client class self initialize model predict self return,issue,positive,neutral,neutral,neutral,neutral,neutral
765179507,"@deliahu 
I was thinking about setting a timer inside a predictor on init() and also specifying the time during which a replica will be alive to be as long as or longer than the timer‚Äôs timeperiod, so it won‚Äôt spin down and timer won‚Äôt have to be restarted. And then calling api_delete() when timer runs out. Whould that work? (considering that API will be deployed once and won‚Äôt be updated)",thinking setting timer inside predictor also time replica alive long longer timer spin timer calling timer work considering,issue,negative,positive,neutral,neutral,positive,positive
765178747,"@jackmpcollins We could probably make it a little bit higher, but not a lot more, since both service names and pods names are capped at 63 characters. We arrived at 42 because, according to a comment I added a while back, k8s added 21 characters to the pod name, which contains the API name (63-21=42). We could definitely make it a little bigger (k8s seems to truncate the pod name in a deployment if necessary to make room for the IDs it adds on to the deployment name to form the pod name), and when I just checked now it seems like actually only 19 characters are added (4 of which are coming from Cortex itself, since we prefix it with `api-` which we could probably avoid if necessary), but 63 would still be the hard upper limit best-case.

Let us know if that would move the needle for you and we could research if we could increase it to 63 (or less, but still more than 42).",could probably make little bit higher lot since service capped according comment added back added pod name name could definitely make little bigger truncate pod name deployment necessary make room deployment name form pod name checked like actually added coming cortex since prefix could probably avoid necessary would still hard upper limit let u know would move needle could research could increase le still,issue,positive,negative,neutral,neutral,negative,negative
765169809,"@da-source perhaps that could work, although if I understand your proposal correctly, you would still need to make the deletion request at the correct time. What are you thinking in terms of how to know when to make the deletion request? One approach could be a cron job or a lambda function that runs on some interval, and uses the python client to check for stale APIs (the responses from `get_api` and `list_apis` include the time the API was last updated). Do you think that would work, or did you have a different idea?",perhaps could work although understand proposal correctly would still need make deletion request correct time thinking know make deletion request one approach could job lambda function interval python client check stale include time last think would work different idea,issue,negative,negative,negative,negative,negative,negative
765045537,"@lminer for the TensorFlow Predictor with multi-model caching enabled, you'd need this:

```python
class TensorFlowPredictor:
    def __init__(self, tensorflow_client, config):
        self.client = tensorflow_client

    def get_model_version(self, model_name):
        """"""For multi-model caching""""""
        try:
            # determine model version
            return self.client._get_latest_model_version_from_tree(
                model_name, self.client._models_tree.model_info(model_name)
            )
        except ValueError:
            # if model_name hasn't been found
            raise RuntimeError(
                f""'{model_name}' model of tag latest wasn't found in the list of available models""
            )

    def predict(self, payload, query_params):
        model_name = query_params[""model""]
        model_version = self.get_model_version(model_name)
        
        try:
            prediction = self.client.predict({""foo"": payload[""foo""]}, model_name)
            return {""status"": 200, ""prediction"": prediction, ""model"": model_name, ""version"": model_version}
        except:
            return {""status"": 500, ""prediction"": None, ""model"": model_name, ""version"": model_version}
```

Let me know if you encounter any problems along the way!",predictor need python class self self try determine model version return except found raise model tag latest found list available predict self model try prediction foo foo return status prediction prediction model version except return status prediction none model version let know encounter along way,issue,negative,positive,positive,positive,positive,positive
765017129,Thanks for such a speedy response! This is exactly what I need. I'm actually using multi-model caching. What modification do I need the make?,thanks speedy response exactly need actually modification need make,issue,negative,positive,positive,positive,positive,positive
765015307,"@lminer that is a good idea and I can see its use. I will bring this up with the team when we prioritize for the next sprint.

Until then, a workaround for you can be this:

```python
class TensorFlowPredictor:
    def __init__(self, tensorflow_client, config):
        self.client = tensorflow_client

    def get_model_version(self, model_name):
        """"""For live-reloading""""""
        versions = self.client._client.poll_available_model_versions(model_name)
        if len(versions) == 0:
            raise RuntimeError(
                f""model '{model_name}' accessed with tag latest couldn't be found""
            )
        return str(max(map(lambda x: int(x), versions)))

    def predict(self, payload, query_params):
        model_name = query_params[""model""]
        model_version = self.get_model_version(model_name)
        
        try:
            prediction = self.client.predict({""foo"": payload[""foo""]}, model_name)
            return {""status"": 200, ""prediction"": prediction, ""model"": model_name, ""version"": model_version}
        except:
            return {""status"": 500, ""prediction"": None, ""model"": model_name, ""version"": model_version}
```

Be advised that this workaround only works for the TensorFlowPredictor type when multi-model caching is disabled. Let me know if you need the workaround for other predictor types or for multi-model caching.

Let me know if this works for you! :) ",good idea see use bring team next sprint python class self self raise model tag latest could found return map lambda predict self model try prediction foo foo return status prediction prediction model version except return status prediction none model version advised work type disabled let know need predictor let know work,issue,negative,positive,positive,positive,positive,positive
764992983,Perhaps creating a Python Cortex cluster client within the API‚Äôs predictor.py and then calling api_delete() on itself?,perhaps python cortex cluster client within calling,issue,negative,neutral,neutral,neutral,neutral,neutral
764799400,"@da-source Bear in mind that there will always be at least one instance running for the cortex operator, which has costs and cannot be scaled down to zero",bear mind always least one instance running cortex operator scaled zero,issue,negative,negative,negative,negative,negative,negative
764797785,"da-source The behavior you desire is the intended behavior: It should terminate the extra instance if there are no APIs.

Do you mind confirming that there are no APIs by running `cortex get`? If none are listed and your cluster is still in this state, do you mind running `cortex cluster info --debug` and sending the cluster debug file to us at dev@cortex.dev so we can take a look?",behavior desire intended behavior terminate extra instance mind confirming running cortex get none listed cluster still state mind running cortex cluster sending cluster file u dev take look,issue,negative,neutral,neutral,neutral,neutral,neutral
764423563,"@imagine3D-ai In that case, it must be some issue with how we pickle the class, or how lambda is set up. We'll investigate this and keep you posted.

In the meantime, I would recommend deploying an API using the files-based approach, as this is more battle tested, and recommended for production environments. You can still use the Python client to deploy your API, you would just move your PythonPredictor implementation into its own file. You would still use [create_api()](https://docs.cortex.dev/clients/python#create_api) and you can still pass in `api_spec`, but you would use the `project_dir` argument instead of `predictor` (you may also need to make a `requirements.txt` file with `sse-starlette` instead of passing it in through the `requirements` parameter). Here is an example of what that would look like: https://docs.cortex.dev/workloads/managing-dependencies/example",case must issue pickle class lambda set investigate keep posted would recommend approach battle tested production still use python client deploy would move implementation file would still use still pas would use argument instead predictor may also need make file instead passing parameter example would look like,issue,negative,neutral,neutral,neutral,neutral,neutral
764418562,"Oh I see! Yes, I think @RobertLucian meant to write `.cortexignore` instead of `.dockerignore` (I didn't notice that either when I read it).

You can use `.cortexignore` to ignore directories or files when packaging your API, and it follows the same syntax as `.gitignore`.",oh see yes think meant write instead notice either read use ignore syntax,issue,negative,neutral,neutral,neutral,neutral,neutral
763487392,"@deliahu After moving my class definition into `lambda_handler()`, `""PicklingError""` occurs:

```
{
  ""errorMessage"": ""Can't pickle <class 'lambda_function.lambda_handler.<locals>.PythonPredictor'>: it's not found as lambda_function.lambda_handler.<locals>.PythonPredictor"",
  ""errorType"": ""PicklingError"",
  ""stackTrace"": [
    ""  File \""/var/task/lambda_function.py\"", line 66, in lambda_handler\n    client.create_api(api_spec={\n"",
    ""  File \""/var/task/cortex/client.py\"", line 141, in create_api\n    dill.dump(predictor, pickle_file)\n"",
    ""  File \""/var/task/dill/_dill.py\"", line 259, in dump\n    Pickler(file, protocol, **_kwds).dump(obj)\n"",
    ""  File \""/var/task/dill/_dill.py\"", line 446, in dump\n    StockPickler.dump(self, obj)\n"",
    ""  File \""/var/lang/lib/python3.8/pickle.py\"", line 485, in dump\n    self.save(obj)\n"",
    ""  File \""/var/lang/lib/python3.8/pickle.py\"", line 558, in save\n    f(self, obj)  # Call unbound method with explicit self\n"",
    ""  File \""/var/task/dill/_dill.py\"", line 1381, in save_type\n    StockPickler.save_global(pickler, obj, name=name)\n"",
    ""  File \""/var/lang/lib/python3.8/pickle.py\"", line 1068, in save_global\n    raise PicklingError(\n""
  ]
}
```",moving class definition ca pickle class found file line file line predictor file line pickler file protocol file line self file line file line self call unbound method explicit file line pickler file line raise,issue,negative,neutral,neutral,neutral,neutral,neutral
763419863,"about this
> .cortexignore file, which follows the same syntax and behavior as a .gitignore file.",file syntax behavior file,issue,negative,neutral,neutral,neutral,neutral,neutral
763344489,">can i use `.cortexignore` instead `.dockerignore` to add my env folder?

@vtrokhymenko I don't fully understand your question, do you mind explaining more about what you would like to do?",use instead add folder fully understand question mind explaining would like,issue,negative,neutral,neutral,neutral,neutral,neutral
763344095,"@da-source I'll go ahead and close this issue, let us know if you have additional questions",go ahead close issue let u know additional,issue,negative,neutral,neutral,neutral,neutral,neutral
763343323,"I'll go ahead and close this issue, since we have #445 to track supporting scale-to-zero",go ahead close issue since track supporting,issue,negative,positive,positive,positive,positive,positive
763342614,"@rafaelsandroni I'll go ahead and close this issue, let me know if you have additional questions",go ahead close issue let know additional,issue,negative,neutral,neutral,neutral,neutral,neutral
763199665,"@imagine3D-ai One thing that is worth a try: could you try moving your class definition to be inside `lambda_handler()`, e.g.

```python
import cortex
from cortex.binary import run_cli
import json
import os

os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""

env_name = ""env""
provider = ""aws""
operator_endpoint = ""@""
aws_access_key_id = ""@""
aws_secret_access_key = ""@""

cli_args = [""env"", ""configure"", env_name,  ""--provider"", provider, ""--operator-endpoint"", operator_endpoint, ""--aws-access-key-id"", aws_access_key_id, ""--aws-secret-access-key"", aws_secret_access_key]

run_cli(cli_args, hide_output=True)

client = cortex.client(env_name)


def lambda_handler(event, context):
    class PythonPredictor:
        def __init__(self, config):
            import boto3
            s3 = boto3.client('s3', aws_access_key_id='@',
            aws_secret_access_key='@')
            s3.download_file('bucket', 'config['model'], '@')
        
    
       def predict(self, payload):
            do something


    os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""
    client.create_api(api_spec={
	  'name': event['api_name'],
	  'kind':'RealtimeAPI',
	  'predictor': {
	      'type': 'python',
	      'config': {'modelname': event['model']},
	  },
	  'compute': {
	    'cpu': '1',
	    'mem': '5G'
	  }}, predictor=PythonPredictor, requirements=['sse-starlette'])
    return {
        'statusCode': 200,
        'body': json.dumps(event['api_name'])
    }
```",one thing worth try could try moving class definition inside python import cortex import import import o provider configure provider provider client event context class self import predict self something event event return event,issue,negative,positive,positive,positive,positive,positive
763189534,"@imagine3D-ai is the `lambda_function` package used or imported anywhere in your code? My guess is that it's not, but I just wanted to confirm",package used anywhere code guess confirm,issue,negative,neutral,neutral,neutral,neutral,neutral
762773894,"@deliahu Thanks for the client! I was able to connect to my cluster and create an api using your python client. However, the API instance didn't really go live and was stuck in the `error` state. After running `cortex logs`, I got:

```
File ""/opt/conda/envs/env/lib/python3.8/site-packages/cortex_internal/lib/api/predictor.py"", line 271, in _get_class_impl
    return dill.load(pickle_file)
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/dill/_dill.py"", line 270, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/dill/_dill.py"", line 473, in load
    obj = StockUnpickler.load(self)
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/dill/_dill.py"", line 463, in find_class
    return StockUnpickler.find_class(self, module, name)
ModuleNotFoundError: No module named 'lambda_function'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/cortex_internal/serve/serve.py"", line 311, in start_fn
    predictor_impl = api.predictor.initialize_impl(project_dir, client)
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/cortex_internal/lib/api/predictor.py"", line 182, in initialize_impl
    class_impl = self.class_impl(project_dir)
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/cortex_internal/lib/api/predictor.py"", line 251, in class_impl
    predictor_class = self._get_class_impl(
  File ""/opt/conda/envs/env/lib/python3.8/site-packages/cortex_internal/lib/api/predictor.py"", line 273, in _get_class_impl
    raise UserException(""unable to load pickle"", str(e)) from e
cortex_internal.lib.exceptions.UserException: error: error in predictor.pickle: unable to load pickle: No module named 'lambda_function'
```

I define my Predictor class in the `lamda_function.py` (which runs on the Lambda platform) , like I showed in the earlier comments and `lambda_handler()` then instantiates Cortex Predictor class on Lambda API call. I'm not sure what to make of this error message that I've got.",thanks client able connect cluster create python client however instance really go live stuck error state running cortex got file line return file line load return file file line load self file line return self module name module exception direct cause following exception recent call last file line client file line file line file line raise unable load pickle error error unable load pickle module define predictor class lambda platform like cortex predictor class lambda call sure make error message got,issue,negative,positive,neutral,neutral,positive,positive
762620531,"@imagine3D-ai We have implemented the fix in https://github.com/cortexlabs/cortex/pull/1798, which will be released in v0.27 (likely by tomorrow night).

In the meantime, I have built a new version of the v0.26 python client [here](https://github.com/cortexlabs/cortex/files/5833908/cortex-0.26.0.tar.gz). You can install it by running `pip install https://github.com/cortexlabs/cortex/files/5833908/cortex-0.26.0.tar.gz`.

Thanks again for bringing this up, and let us know if that fixes it!
",fix likely tomorrow night built new version python client install running pip install thanks let u know,issue,negative,positive,positive,positive,positive,positive
762435725,"thx, in the near future, get to testing 
and more, can i use `.cortexignore` instead `.dockerignore` to add my `env` folder?",near future get testing use instead add folder,issue,negative,positive,neutral,neutral,positive,positive
762398733,"@vtrokhymenko what I notice is that you're trying to upload the environment directory for your Cortex API - you don't have to do this as the API has its environment already configured. In order for this directory `env` to be ignored, you need to add it to `.dockerignore`, which follows the same syntax and behavior as a `.gitignore` file:

```gitignore
# .dockerignore
dev/
```

I also recommend reading [this page](https://docs.cortex.dev/workloads/realtime/predictors), which explains more about this subject.",notice trying environment directory cortex environment already order directory need add syntax behavior file also recommend reading page subject,issue,negative,negative,negative,negative,negative,negative
762034278,"@deliahu Thanks! It would be great to get workaround before Tuesday, so that I could continue experimenting with Cortex + Lambda in the meanwhile, so please share the custom version of Python client if you could",thanks would great get could continue cortex lambda meanwhile please share custom version python client could,issue,positive,positive,positive,positive,positive,positive
761996101,"@lminer When running in a cluster, the `/mnt` path is shared across the containers; let us know if you're able to get it working, and we'll add it to the docs. Thanks for the suggestion!",running cluster path across let u know able get working add thanks suggestion,issue,negative,positive,positive,positive,positive,positive
761993608,"@imagine3D-ai Thanks for following up, this is a spot in code where we did not check for the `$CORTEX_CLI_CONFIG_DIR` environment variable! (I just did a full search of our codebase to confirm there are no others)

We will get a fix out for this by end-of-day Tuesday. Is that soon enough for you, or would you like us to investigate a workaround that you could use before then? The easiest workaround would probably be for us to build you a custom version of the Python client that you could `pip install` directly from our S3 bucket.",thanks following spot code check environment variable full search confirm get fix soon enough would like u investigate could use easiest would probably u build custom version python client could pip install directly bucket,issue,positive,positive,positive,positive,positive,positive
761820879,"@deliahu I'm still getting `OSError`, after changing `CORTEX_CLI_CONFIG_DIR` variable:

```
import cortex
from cortex.binary import run_cli
import json
import os

os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""


class PythonPredictor:
    def __init__(self, config):
        import boto3
        s3 = boto3.client('s3', aws_access_key_id='@',
        aws_secret_access_key='@')
        s3.download_file('bucket', 'config['model'], '@')
    

   def predict(self, payload):
        do something


env_name = ""env""
provider = ""aws""
operator_endpoint = ""@""
aws_access_key_id = ""@""
aws_secret_access_key = ""@""

cli_args = [""env"", ""configure"", env_name,  ""--provider"", provider, ""--operator-endpoint"", operator_endpoint, ""--aws-access-key-id"", aws_access_key_id, ""--aws-secret-access-key"", aws_secret_access_key]

run_cli(cli_args, hide_output=True)

client = cortex.client(env_name)



def lambda_handler(event, context):
    os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""
    client.create_api(api_spec={
	  'name': event['api_name'],
	  'kind':'RealtimeAPI',
	  'predictor': {
	      'type': 'python',
	      'config': {'modelname': event['model']},
	  },
	  'compute': {
	    'cpu': '1',
	    'mem': '5G'
	  }}, predictor=PythonPredictor, requirements=['sse-starlette'])
    return {
        'statusCode': 200,
        'body': json.dumps(event['api_name'])
    }

```

Error message that I get:

```
{
  ""errorMessage"": ""[Errno 30] Read-only file system: '/home/sbx_user1051'"",
  ""errorType"": ""OSError"",
  ""stackTrace"": [
    ""  File \""/var/task/lambda_function.py\"", line 65, in lambda_handler\n    client.create_api(api_spec={\n"",
    ""  File \""/var/task/cortex/client.py\"", line 99, in create_api\n    project_dir.mkdir(parents=True)\n"",
    ""  File \""/var/lang/lib/python3.8/pathlib.py\"", line 1291, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n"",
    ""  File \""/var/lang/lib/python3.8/pathlib.py\"", line 1291, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n"",
    ""  File \""/var/lang/lib/python3.8/pathlib.py\"", line 1291, in mkdir\n    self.parent.mkdir(parents=True, exist_ok=True)\n"",
    ""  File \""/var/lang/lib/python3.8/pathlib.py\"", line 1287, in mkdir\n    self._accessor.mkdir(self, mode)\n""
  ]
}
```

It seems to have something to do with `clent.create_api()`. Is there a way to fix this?",still getting variable import cortex import import import o class self import predict self something provider configure provider provider client event context event event return event error message get file system file line file line file line file line file line file line self mode something way fix,issue,negative,neutral,neutral,neutral,neutral,neutral
761164288,"@da-source @RobertLucian I just wanted to be clear and double check for others reading this thread that this is the recommended approach:

```python
class PythonPredictor:
  def __init__(self, config):
    import boto3
    # boto3 can be used here


  def predict(self):
    import boto3
    # boto3 can be used here
```",clear double check reading thread approach python class self import used predict self import used,issue,negative,positive,neutral,neutral,positive,positive
761161762,">So I‚Äôm guessing that those packages come pre-installed on AWS instances and that‚Äôs why it has worked?

Yes that is correct, these packages are necessary for Cortex to run the web server, so they are already installed in the container",guessing come worked yes correct necessary cortex run web server already container,issue,negative,neutral,neutral,neutral,neutral,neutral
761147517,"@imagine3D-ai Yes, it is possible to change the default directory that the Cortex client uses to save its configuration. Try exporting the `CORTEX_CLI_CONFIG_DIR` environment variable before running any of the client commands, e.g. `os.environ[""CORTEX_CLI_CONFIG_DIR""] = ""/tmp/.cortex""`. Let me know if that works!",yes possible change default directory cortex client save configuration try environment variable running client let know work,issue,positive,neutral,neutral,neutral,neutral,neutral
761133124,@deliahu looks like this is a known problem and one that is stalled. See [here](https://github.com/tensorflow/io/issues/414) and [here](https://github.com/tensorflow/serving/issues/1411).,like known problem one see,issue,negative,neutral,neutral,neutral,neutral,neutral
761055316,"@deliahu My `lambda_function.py` looks like this:

```
import cortex
from cortex.binary import run_cli
import json


class PythonPredictor:
    def __init__(self, config):
        import boto3
        s3 = boto3.client('s3', aws_access_key_id='@',
        aws_secret_access_key='@')
        s3.download_file('bucket', 'config['model'], '@')
    

   def predict(self, payload):
        do something


env_name = ""env""
provider = ""aws""
operator_endpoint = ""@""
aws_access_key_id = ""@""
aws_secret_access_key = ""@""

cli_args = [""env"", ""configure"", env_name,  ""--provider"", provider, ""--operator-endpoint"", operator_endpoint, ""--aws-access-key-id"", aws_access_key_id, ""--aws-secret-access-key"", aws_secret_access_key]

run_cli(cli_args, hide_output=True)

client = cortex.client(env_name)



def lambda_handler(event, context):
    client.create_api(api_spec={
	  'name': event['api_name'],
	  'kind':'RealtimeAPI',
	  'predictor': {
	      'type': 'python',
	      'config': {'modelname': event['model']},
	  },
	  'compute': {
	    'cpu': '1',
	    'mem': '5G'
	  }}, predictor=PythonPredictor, requirements=['sse-starlette'])
    return {
        'statusCode': 200,
        'body': json.dumps(event['api_name'])
    }

```",like import cortex import import class self import predict self something provider configure provider provider client event context event event return event,issue,negative,neutral,neutral,neutral,neutral,neutral
761042930,"@deliahu I'm just trying to implement a low cost interface/microservice, which can be called with JS (Lambda's Serverless REST API endpoint, seemed good for this) to create and delete APIs in Cortex cluster.",trying implement low cost lambda rest good create delete cortex cluster,issue,negative,positive,positive,positive,positive,positive
761041342,"@deliahu After packaging and uploading Cortex to Lambda, I receive this error in Lambda when trying to run my code:

```
START RequestId: 47574c0d-b571-4da8-8f68-c43935eb9316 Version: $LATEST
[ERROR] CortexBinaryException: error: unable to write to home directory: /home/sbx_user1051/.cortex: mkdir /home/sbx_user1051: read-only file system

Traceback (most recent call last):
¬†¬†File ""/var/lang/lib/python3.8/imp.py"", line 234, in load_module
¬†¬†¬†¬†return load_source(name, filename, file)
¬†¬†File ""/var/lang/lib/python3.8/imp.py"", line 171, in load_source
¬†¬†¬†¬†module = _load(spec)
¬†¬†File ""<frozen importlib._bootstrap>"", line 702, in _load
¬†¬†File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
¬†¬†File ""<frozen importlib._bootstrap_external>"", line 783, in exec_module
¬†¬†File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
¬†¬†File ""/var/task/lambda_function.py"", line 54, in <module>
¬†¬†¬†¬†run_cli(cli_args, hide_output=True)
¬†¬†File ""/var/task/cortex/binary/__init__.py"", line 110, in run_cli
¬†¬†¬†¬†raise CortexBinaryException(process.stderr.read())END RequestId: 47574c0d-b571-4da8-8f68-c43935eb9316
REPORT RequestId: 47574c0d-b571-4da8-8f68-c43935eb9316	Duration: 3639.67 ms	Billed Duration: 3640 ms	Memory Size: 128 MB	Max Memory Used: 28 MB	
Unknown application error occurred
```

After searching a bit online, it seems that Lambda allows to write only to the `/tmp` directory and Cortex is trying to write something to `/home/` and therefore I'm getting this error.

Is this something that can be fixed or is there an alternative similar to Lambda REST API, which could be used with JS to create and delete Cortex APIs?",cortex lambda receive error lambda trying run code start version latest error error unable write home directory file system recent call last file line return name file file line module spec file frozen line file frozen line file frozen line file frozen line file line module file line raise end report duration billed duration memory size memory used unknown application error searching bit lambda write directory cortex trying write something therefore getting error something fixed alternative similar lambda rest could used create delete cortex,issue,negative,neutral,neutral,neutral,neutral,neutral
760743612,"> @mutal You might also need to run `!pip install starlette`. In addition, you might also want to install `uvicorn`, `requests`, and `python-multipart`, depending on your use case.

So I‚Äôm guessing that those packages come pre-installed on AWS instances and that‚Äôs why it has worked?",might also need run pip install addition might also want install depending use case guessing come worked,issue,negative,neutral,neutral,neutral,neutral,neutral
760681226,"I haven't tried it with the predictor type. I only tried it in a separate jupyter notebook in an conda environment with both tensorflow-io and tensorflow installed. I doubt it would work locally with the tensorflow predictor.

I don't know if there's a system package. I doubt it since I haven't installed anything special locally. My guess is that you need tensorflow-io installed and linked to tensorflow. Maybe the python package does this...",tried predictor type tried separate notebook environment doubt would work locally predictor know system package doubt since anything special locally guess need linked maybe python package,issue,negative,positive,positive,positive,positive,positive
760677859,"@lminer `requirements.txt` does not apply to the tensorflow-serving image (Python is not actually installed on that image, since the tensorflow-serving implementation is in C++). Do you know if there is there a system package that might need to be installed?

Also, was it working locally with the TensorFlow predictor type? This is interesting to me, since that should be the same exact image that is running in the cluster.",apply image python actually image since implementation know system package might need also working locally predictor type interesting since exact image running cluster,issue,negative,positive,positive,positive,positive,positive
760308828,"@imagine3D-ai It should work like any other pip package; here is some documentation from AWS, let us know if you aren't able to get cortex working (but are able to get other packages working, like `requests` in their example).

https://docs.aws.amazon.com/lambda/latest/dg/python-package.html#python-package-dependencies",work like pip package documentation let u know able get cortex working able get working like example,issue,positive,positive,positive,positive,positive,positive
760306759,"@mutal You might also need to run `!pip install starlette`. In addition, you might also want to install `uvicorn`, `requests`, and `python-multipart`, depending on your use case.",might also need run pip install addition might also want install depending use case,issue,negative,neutral,neutral,neutral,neutral,neutral
760276571,"@deliahu I noticed something peculiar with `sse-starlette`. When passing `sse-starlette` into `requirements.txt` and then using 
`from sse_starlette.sse import EventSourceResponse` in `predictor.py` everything is working fine, but when I try to do the same on a local machine, by using `!pip install sse-starlette`, `from sse_starlette.sse import EventSourceResponse` returns `ModuleNotFoundError: No module named 'starlette'`. It seems strange, that installing and importing `starlette` works on AWS instances, but not in Colab or locally. Why could that be?",something peculiar passing import everything working fine try local machine pip install import module strange work locally could,issue,negative,positive,neutral,neutral,positive,positive
760268363,"@da-source as long as you're required to use boto3 in your predict method, yes, for now.

That being said, I would still point out that importing the modules wherever they are needed is the better alternative - in your case, I would strive to import them in the constructor and use them to initialize stuff just there.",long use predict method yes said would still point wherever better alternative case would strive import constructor use initialize stuff,issue,positive,positive,positive,positive,positive,positive
760266552,"@RobertLucian So for now, this:
```
class PythonPredictor:
  def __init__(self, config):
    self.boto3 = __import__(""boto3"")
    # or import boto3 and then assign boto3 to boto3 attribute

  def predict(self):
    return self.boto3
```
is the best option, am I correct?",class self import assign attribute predict self return best option correct,issue,positive,positive,positive,positive,positive,positive
760252658,"@da-source the imports' scope is limited to their respective module/class/function(or method). In your case, the `boto3` package is only limited to the constructor method. One way to make this work is to assign the module to a class attribute such as `boto3`:

```python
class PythonPredictor:
  def __init__(self, config):
    self.boto3 = __import__(""boto3"")
    # or import boto3 and then assign boto3 to boto3 attribute

  def predict(self):
    return self.boto3
```

Generally, packages should not be made available this way (through assigning them to class attributes) - the imports should be done in the constructor, you initialize whatever you need there, and then in the `predict` method (or others), you use whatever was initialized in the constructor.

I wouldn't recommend doing the imports at the class level like this either:

```python
class PythonPredictor:
  boto3 = __import__(""boto3"")
```

And that's because the imports will be done when the class is defined (and not when an object of this class is instantiated).

---

There may be some work we have to do to improve the UX with regards to importing modules though.",scope limited respective method case package limited constructor method one way make work assign module class attribute python class self import assign attribute predict self return generally made available way class done constructor initialize whatever need predict method use whatever constructor would recommend class level like either python class done class defined object class may work improve though,issue,positive,positive,neutral,neutral,positive,positive
759911992,"Yeah, that's correct. The same thing holds with a numpy array as well.",yeah correct thing array well,issue,positive,neutral,neutral,neutral,neutral,neutral
759911404,"I'll go ahead and close this issue, since the gRPC resource exhausted error has been resolved and will be released in 0.27 (next week). #1774 will remain open as we investigate if there is an avoidable issue that is causing the slowdown.",go ahead close issue since resource exhausted error resolved next week remain open investigate avoidable issue causing slowdown,issue,negative,negative,negative,negative,negative,negative
759910662,"> where audio is a float32 tensor of shape (None, 2)

@lminer just to confirm, was `audio` of type `tf.Tensor`?",audio float tensor shape none confirm audio type,issue,negative,neutral,neutral,neutral,neutral,neutral
759903686,"@langep sounds good, let me know if you run into any issues or have any questions about the workaround",good let know run,issue,negative,positive,positive,positive,positive,positive
759891452,A strong preference to send the data to the an endpoint. Keeps things consistent across the types.,strong preference send data consistent across,issue,positive,positive,positive,positive,positive,positive
759885372,"Thank you @deliahu, I was thinking about the `dependencies.sh` hack as well but didn't have a chance to testing if the environment variables from the API config are set at the time `dependencies.sh` runs. Thanks for confirming that they are in fact set at the time.",thank thinking hack well chance testing environment set time thanks confirming fact set time,issue,positive,positive,positive,positive,positive,positive
759831724,"@JakeCowton would you want to enqueue requests directly to SQS, or via an endpoint in the cluster (like it works now with [batch APIs](https://docs.cortex.dev/workloads/batch/example)), or do you have no preference?",would want directly via cluster like work batch preference,issue,negative,positive,neutral,neutral,positive,positive
759635973,"@dakshvar22 Yes, we would be open to that, and happy to point you in the right direction!

It would probably be best to start with a quick chat to understand your use case and design the feature (we have a few proposals for it, which would provide different user experiences). Please email me at david@cortex.dev and we can find a time.",yes would open happy point right direction would probably best start quick chat understand use case design feature would provide different user please find time,issue,positive,positive,positive,positive,positive,positive
759446434,This would be a major improvement for my use case.,would major improvement use case,issue,negative,positive,neutral,neutral,positive,positive
759384739,@deliahu Thanks for giving that update. The issue is sort of a blocker for us. I am happy to contribute to the framework to make this possible if I could be pointed at what needs to change. Let me know what you think. :),thanks giving update issue sort blocker u happy contribute framework make possible could pointed need change let know think,issue,positive,positive,positive,positive,positive,positive
759384062,"@RobertLucian Just picking your brain further on this - 
> with a lambda, you could schedule the API to run at specific times. The lambda would deploy/delete the API when programmed to do so.

I am not sure if that solves our use case because there isn't a specific time when we want the API to be up or down. It should be done according to the traffic it is receiving. So, not sure if we can program to do so. Am I missing something?",brain lambda could schedule run specific time lambda would sure use case specific time want done according traffic sure program missing something,issue,negative,positive,positive,positive,positive,positive
759379012,"@RobertLucian Thanks! And what should should I do about importing libraries? I tried doing it on `init` but that didn't seem to be working:

```
class PythonPredictor:
  def __init__(self, config):
    import boto3

  def predict(self):
    return boto3 # Outputs 'boto3' not defined

```",thanks tried seem working class self import predict self return defined,issue,negative,positive,positive,positive,positive,positive
759125396,"Yes, we would support nested paths. We'll keep you posted on our priority for this!

In the meantime, there could be a somewhat hacky workaround that might work: in theory it should be possible to set an environment variable via the `env` field in the predictor section of the [API config](https://docs.cortex.dev/workloads/realtime/configuration), and use it in `dependencies.sh` to determine what commands to run.

E.g.

```yaml
# cortex.yaml

- name: my-api
  predictor:
    env:
      api_name: my-api
```

```shell
# dependencies.sh

if [ ""$api_name"" == ""my-api"" ]; then
  pip install some-subdir/alternative-requirements.txt
fi
```

Another approach (which would have some added benefits as well) would be to build a custom docker image containing your dependencies ([docs](https://docs.cortex.dev/workloads/managing-dependencies/images)). The benefits of this approach are that it's faster to download from ECR/GCR than from quay.io, you would remove a dependency on quay.io (which could have downtime), and there would be no need to re-install dependencies on each api replica init().",yes would support keep posted priority could somewhat hacky might work theory possible set environment variable via field predictor section use determine run name predictor shell pip install fi another approach would added well would build custom docker image approach faster would remove dependency could would need replica,issue,positive,neutral,neutral,neutral,neutral,neutral
759123840,"@da-source so let's answer your question.

> 1. When creating an API using Python client, what should I do about the 'predictor path' argument? When using CLI, I usually set it to predictor.py which is located in my project folder, but should I omit it with Python client since I'm passing a predictor=PythonPredictor?

When using the Python Client, if the `predictor` parameter is specified, then setting the `predictor.path` field is no longer required. If the `predictor` parameter is passed, then specifying the `predictor.path` is required.

> 2. I would like to use AWS Lambda to run the Python client, which will create/return/delete Cortex APIs. What would be the best way to specify which kind of predictor the Cortex API to be deployed will use (assuming that all the APIs will have the same specs, but different predictors), should I just go with simple if statements:

The best way to configure the API at deploy time is to populate the `predictor.config` field (dictionary). Then you will use the `config` parameter as it is passed into your API predictor class constructor to configure the API's behavior.

One thing I notice is that the PythonPredictor implementation in your example has the incorrect signature for your constructor - at the very least, the `config` parameter must be present in the constructor's signature. Check out the docs [here](https://docs.cortex.dev/workloads/realtime/predictors#python-predictor).

2nd thing I notice is that in
```python
client.create_api(api_spec= {'name': 'modelname',
  'kind':'RealtimeAPI',
  'predictor':
    {'type': 'python'},
    'compute':
    {'cpu': '1',
    'mem': '5G'}}, predictor=PythonPredictor('modelname'))
```
you're initializing the `PythonPredictor` class, when in reality, it shouldn't. The class will get initialized when the API gets deployed on the _Cortex cluster_. The `predictor` parameter only takes in a class, not an initialized object - just like when a `cortex.yaml` API is getting deployed from a directory - the class definition is there, but it's not initialized at deploy time.

All in all, your example would look like this:

```python
# define predictor
class PythonPredictor:
  def __init__(self, config):
    self.config = config
    if self.config[""condition""] == ""value-1"":
        # initialization of type 1
    elif self.config[""condition""] == ""value-2"":
        # initialization of type 2
    else:
        # whatever other kind of initialization

  def predict(self):
    #do something

# create api
client.create_api(api_spec={
  'name': 'modelname',
  'kind':'RealtimeAPI',
  'predictor': {
      'type': 'python',
      'config': {'condition': 'value-1'},
  },
  'compute': {
    'cpu': '1',
    'mem': '5G'
  }}, predictor=PythonPredictor)
```

Let us know if this clears up things for you!",let answer question python client path argument usually set project folder omit python client since passing python client predictor parameter setting field longer predictor parameter would like use lambda run python client cortex would best way specify kind predictor cortex use assuming spec different go simple best way configure deploy time populate field dictionary use parameter predictor class constructor configure behavior one thing notice implementation example incorrect signature constructor least parameter must present constructor signature check thing notice python class reality class get predictor parameter class object like getting directory class definition deploy time example would look like python define predictor class self condition type condition type else whatever kind predict self something create let u know,issue,positive,positive,positive,positive,positive,positive
759104397,"@lminer interesting. So there may be something in our codebase or something with TFS itself (grpc-related). We will be investigating this issue. Also, what is the payload size that you've experimented with and observed that it was very slow? Is it 40MB as it was specified in https://github.com/cortexlabs/cortex/issues/1774?",interesting may something something investigating issue also size experimented slow,issue,negative,positive,neutral,neutral,positive,positive
759066836,"@deliahu Unfortunately I can't send along the model. The data object I was sending was as follows:

```self.client.predict({""audio"": audio})``` where audio is a float32 tensor of shape (None, 2) where none corresponds to the number of samples in the audio file.",unfortunately ca send along model data object sending audio audio audio float tensor shape none none number audio file,issue,negative,negative,negative,negative,negative,negative
759011072,"@lminer I'm glad to hear you got it working!

I'd like to keep this issue open for now, since I have one more theory I'd like to try out (it still seems like it takes too long if it were only a matter of networking).

When you were passing in the data via `self.client.predict()`, what was the type of the object you were passing in? Would you be able to send us an example model / input file / `predictor.py` that reproduces the super long latency? (I'm assuming the 40mb input you referenced above comes from the user's request). Feel free to email us at dev@cortex.dev.",glad hear got working like keep issue open since one theory like try still like long matter passing data via type object passing would able send u example model input file super long latency assuming input come user request feel free u dev,issue,positive,positive,positive,positive,positive,positive
758907562,@deliahu maybe we should close this as it is a consequence of the change made in #1740. The problem does indeed seem to be with passing large amounts of data to tensorflow-serving.,maybe close consequence change made problem indeed seem passing large data,issue,negative,positive,positive,positive,positive,positive
758906390,"@RobertLucian so I managed to solve this by using the tensorflow-predictor, but reading from s3 directly in the model itself. Now the model is significantly faster, basically equivalent to local tests. So I think the problem is the passing of large datasets via grpc",solve reading directly model model significantly faster basically equivalent local think problem passing large via,issue,negative,positive,positive,positive,positive,positive
758589199,"@vishalbollu Yes, that was a simple copy-paste error :)

1. When creating an API using Python client, what should I do about the 'predictor path' argument? When using CLI, I usually set it to `predictor.py` which is located in my project folder, but should I omit it with Python client since I'm passing a `predictor=PythonPredictor`?

```
client.create_api(api_spec= {'name': 'testapi3',
  'kind':'RealtimeAPI',
  'predictor':
    {'type': 'python',
    'path': 'predictor.py'},
    'compute':
    {'cpu': '1',
    'mem': '5G'}}, predictor=PythonPredictor)
```

2. I would like to use AWS Lambda to run the Python client, which will create/return/delete Cortex APIs. What would be the best way to specify which kind of predictor the Cortex API to be deployed will use (assuming that all the APIs will have the same specs, but different predictors), should I just go with simple `if` statements:

```
#define predictor
class PythonPredictor:
    def __init__(self, model_name):
      s3 = boto3.client('s3', aws_access_key_id='@', aws_secret_access_key='@')
      if (model_name='a'):
        s3.download_file('a', 'a', ""./a"")
      if (model_name='b'):
        s3.download_file('b', 'b', ""./b"")

    def predict(self):
        #do something

#create api
client.create_api(api_spec= {'name': 'modelname',
  'kind':'RealtimeAPI',
  'predictor':
    {'type': 'python'},
    'compute':
    {'cpu': '1',
    'mem': '5G'}}, predictor=PythonPredictor('modelname'))
```

or would you recommend some other way?",yes simple error python client path argument usually set project folder omit python client since passing would like use lambda run python client cortex would best way specify kind predictor cortex use assuming spec different go simple define predictor class self predict self something create would recommend way,issue,positive,positive,positive,positive,positive,positive
758427553,"Yes that design would work great, assuming that the individual paths can be relative to the root e.g. `pip: some-subdir/alternative-requirements.txt`.",yes design would work great assuming individual relative root pip,issue,positive,positive,positive,positive,positive,positive
758408983,"@langep Yes I think this is a great idea. We actually have a ticket for this (https://github.com/cortexlabs/cortex/issues/1191), but I'll go ahead and close that one as a duplicate since your motivation section is more detailed.

Here is the proposed design from the other ticket, does it sound reasonable to you?

```yaml
- name: my-api
  predictor:
    dependencies:
      pip: alternative-requirements.txt
      conda: alternative-conda-packages.txt
      shell: alternate-dependencies.sh
```",yes think great idea actually ticket go ahead close one duplicate since motivation section detailed design ticket sound reasonable name predictor pip shell,issue,positive,positive,positive,positive,positive,positive
758405231,"We don't currently have a timeline for this (we generally plan 2-4 weeks ahead). Since we have been focusing our recent efforts on production use cases / features / integrations, scale-to-0 has not bubbled up in priority.",currently generally plan ahead since recent production use priority,issue,negative,positive,neutral,neutral,positive,positive
758329956,"Thanks! Any timeline on when can we expect the above ticket to be tackled?

On Mon, Jan 11, 2021, 21:06 Robert Lucian Chiriac <notifications@github.com>
wrote:

> Yes, you are correct - we need to address that ticket. Nonetheless, with a
> lambda, you could schedule the API to run at specific times. The lambda
> would deploy/delete the API when programmed to do so.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1775#issuecomment-758192056>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGLHR4ZSXKLREAAIOD3SZNK4DANCNFSM4V5MQVYA>
> .
>
",thanks expect ticket tackled mon wrote yes correct need address ticket nonetheless lambda could schedule run specific time lambda would reply directly view,issue,positive,positive,positive,positive,positive,positive
758316994,Everything is set to the default: 1. Do you know what version I would need to downgrade to? I'm currently on 2.4.0,everything set default know version would need downgrade currently,issue,negative,neutral,neutral,neutral,neutral,neutral
758308742,"@lminer can you tell us if you're running multiple threads per process `threads_per_process`? You should set it to 1 when the TensorFlow framework is used with the PythonPredictor because of the way the framework works. A long time ago, it didn't work at all and so we had a [patch](https://github.com/cortexlabs/cortex/pull/1146) that made the TensorFlow framework work using the Python Predictor when `thread_per_process` is set to 1 - and if I recall correctly, for `processes_per_replica` too.

If setting the `threads_per_process` field to 1 doesn't fix it, then maybe downgrading the TensorFlow version might help - assuming the model can be loaded with said version.",tell u running multiple per process set framework used way framework work long time ago work patch made framework work python predictor set recall correctly setting field fix maybe version might help assuming model loaded said version,issue,negative,negative,neutral,neutral,negative,negative
758244283,"@RobertLucian I've been trying to get this working with the python predictor, but I think I'm having an issue where there is a confusion because the session information from when I load the model in the constructor is being[ lost at predict time](https://github.com/tensorflow/tensorflow/issues/28287). The solution seems to be to convert everything to graph mode. Unfortunately, this isn't so easy given my code. Do you know any other way to get around this error?",trying get working python predictor think issue confusion session information load model constructor lost predict time solution convert everything graph mode unfortunately easy given code know way get around error,issue,negative,positive,positive,positive,positive,positive
758192056,"Yes, you are correct - we need to address that ticket. Nonetheless, with a lambda, you could schedule the API to run at specific times. The lambda would deploy/delete the API when programmed to do so.",yes correct need address ticket nonetheless lambda could schedule run specific time lambda would,issue,negative,neutral,neutral,neutral,neutral,neutral
758168810,"Ohh so it wouldn't be possible in an automated way. Then we'll have to wait
for the above linked PR to be implemented.

On Mon, Jan 11, 2021, 20:21 Robert Lucian Chiriac <notifications@github.com>
wrote:

> @dakshvar22 <https://github.com/dakshvar22> almost. Setting min_instances
> to 0 can reduce the cluster's number of nodes for its APIs down to zero as
> long as the underlying API(s) are also *deleted* - and to delete the API,
> the user's intervention is required. That's because the minimum number of
> replicas an API can have is 1. Does this make sense to you?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1775#issuecomment-758167456>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGJIESR2G6TERFK7T5LSZNFSRANCNFSM4V5MQVYA>
> .
>
",would possible way wait linked mon wrote almost setting reduce cluster number zero long underlying also delete user intervention minimum number make sense reply directly view,issue,negative,positive,neutral,neutral,positive,positive
758167456,"@dakshvar22 almost. Setting `min_instances` to 0 can reduce the cluster's number of nodes for its APIs down to zero as long as the underlying API(s) are also _deleted_ - and to delete the API, the user's intervention is required. That's because the minimum number of replicas an API can have is 1. Does this make sense to you?",almost setting reduce cluster number zero long underlying also delete user intervention minimum number make sense,issue,negative,negative,neutral,neutral,negative,negative
758160473,"So, if I understand correctly I should be able to achieve my use case by
setting `min_instances` to 0 and `min_replicas` to 1?

On Mon, Jan 11, 2021, 19:55 Robert Lucian Chiriac <notifications@github.com>
wrote:

> @dakshvar22 <https://github.com/dakshvar22> an API can have multiple
> replicas - in technical terms, pods that run your API as it is specified in
> your Cortex project. The more there are, the higher the performance of your
> API. A collection of API replicas is called an API.
>
> A cluster can have multiple instances (of t3.medium, g4dn.xlarge, etc
> type). These are effectively the cluster's nodes on which the APIs are
> residing. It's on these that the API replicas reside. As traffic increases,
> Cortex decides to increase the number of API replicas, which in turn
> increases the number of cluster nodes (instances). The opposite happens
> when the traffic gets smaller.
>
> All in all, the replica term is used in the context of APIs, and the
> instance term is used in the context of a cluster. Does this clarify the
> situation?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1775#issuecomment-758152927>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGOA4TNFJQUVWUCPYH3SZNCRLANCNFSM4V5MQVYA>
> .
>
",understand correctly able achieve use case setting mon wrote multiple technical run cortex project higher performance collection cluster multiple type effectively cluster reside traffic cortex increase number turn number cluster opposite traffic smaller replica term used context instance term used context cluster clarify situation reply directly view,issue,positive,positive,positive,positive,positive,positive
758152927,"@dakshvar22 an API can have multiple replicas - in technical terms, pods that run your API as it is specified in your Cortex project. The more there are, the higher the performance of your API. A collection of API replicas is called an API.

A cluster can have multiple instances (of `t3.medium`, `g4dn.xlarge`, etc type). These are effectively the cluster's nodes on which the APIs are residing. It's on these that the API replicas reside. As traffic increases, Cortex decides to increase the number of API replicas, which in turn increases the number of cluster nodes (instances). The opposite happens when the traffic gets smaller.

All in all, the replica term is used in the context of APIs, and the instance term is used in the context of a cluster. Does this clarify the situation?",multiple technical run cortex project higher performance collection cluster multiple type effectively cluster reside traffic cortex increase number turn number cluster opposite traffic smaller replica term used context instance term used context cluster clarify situation,issue,positive,positive,positive,positive,positive,positive
758146617,Ahh I see `min_instances` in `cluster.yaml`. What's the difference between replica and instance?,see difference replica instance,issue,negative,neutral,neutral,neutral,neutral,neutral
758097023,"@dakshvar22 thanks for reaching out regarding this.

Yes, your use case is a valid one; we have https://github.com/cortexlabs/cortex/issues/445 to track supporting it. We have not supported this in the past; perhaps you are thinking of instances: we support setting `min_instances` to 0, which will allow for the instances to be terminated if there are no deployed APIs.",thanks reaching regarding yes use case valid one track supporting past perhaps thinking support setting allow,issue,positive,positive,neutral,neutral,positive,positive
758096163,"@lminer I did not. But I remember trying with 140MB payloads and for something like https://github.com/cortexlabs/cortex/pull/1770 it would take a few seconds to run the inference - what I can recall was ~10 seconds. This was done on `t3.medium`s, which are pretty slow anyway.

Assuming there's something wrong with grpc + TFS, I wonder if this would be in any way related to https://github.com/tensorflow/serving/issues/1725.

I think to rule out a problem with grpc + TFS is to run the model as a `PythonPredictor` - that is you implement the loading procedure in the predictor's constructor and then you run the prediction in the `predict` method. I would only go with `processes_per_replica` set to 1 in this case because of https://docs.cortex.dev/v/0.24/running-on-aws/gpu#tips.",remember trying something like would take run inference recall done pretty slow anyway assuming something wrong wonder would way related think rule problem run model implement loading procedure predictor constructor run prediction predict method would go set case,issue,negative,negative,negative,negative,negative,negative
758049922,"Is it possible that there is a copy-paste error? Based on the script you've specified above, `'env_name'` is in single quotes in the line `cli_args = [...]`. Therefore, it is possible that rather than creating an environment named `test`, an environment named `env_name` is created. Please try again after removing the single quotes around `'env_name'` and let us know how it goes.",possible error based script single line therefore possible rather environment test environment please try removing single around let u know go,issue,negative,negative,neutral,neutral,negative,negative
757801211,"> cortex cluster info --configure-env test

The operator I get by calling `cortex cluster info --configure-env test` matches the one I'm using in Python and running `cortex get --env test` in the CLI returns `no apis are deployed`.

Also, I retried running this snippet, which mimics the behaviour of `cortex.cluster_client()`:

```
import cortex
from cortex.binary import run_cli

env_name = ""test""
provider = ""aws""
operator_endpoint = ""@""
aws_access_key_id = ""@""
aws_secret_access_key = ""@""

cli_args = [""env"", ""configure"", 'env_name',  ""--provider"", provider, ""--operator-endpoint"", operator_endpoint, ""--aws-access-key-id"", aws_access_key_id, ""--aws-secret-access-key"", aws_secret_access_key]

run_cli(cli_args, hide_output=True)

client = cortex.client(env_name)
```

And got this error:

`NotFound: can't find environment test, create one by calling cortex.cluster_client()`

Which is weird because `cortex.cluster_client()` isn't working
",cortex cluster test operator get calling cortex cluster test one python running cortex get test also running snippet behaviour import cortex import test provider configure provider provider client got error ca find environment test create one calling weird working,issue,negative,negative,negative,negative,negative,negative
757562021,"@lminer yes that could be it, although 124 seconds still seems high to me for this kind of issue. Maybe it has something to do with how the tensor is created/encoded before sending off to TF Serving? Although you did mention that CPU is idle too...

One alternative you could look into is using Cortex's Python predictor type instead of the TensorFlow type (then there would not be an extra hop). How are you importing your model and running inference in your notebook, and would that be easily transferrable to the PythonPredictor?

In the Python predictor, you would pass in the path to your model in the API's config field, and download/load it on `__init__()`. Here are the [docs](https://docs.cortex.dev/workloads/realtime/predictors#python-predictor), and we have a couple examples that work like this, e.g. [pytorch/iris-classifier](https://github.com/cortexlabs/cortex/tree/master/test/apis/pytorch/iris-classifier)",yes could although still high kind issue maybe something tensor sending serving although mention idle one alternative could look cortex python predictor type instead type would extra hop model running inference notebook would easily python predictor would pas path model field couple work like,issue,positive,positive,positive,positive,positive,positive
757557776,"@deliahu I did some logging and it looks like the slowdown is more like 10X. 124 seconds is spent in `self.client.predict()`. When I run inference via jupyter, it only takes 11 seconds.

If I look at GPU utilization during the 124 seconds, it appears as if GPU is only being used for ~10 seconds.

I'm wondering if this issue relates to #1740. I'm passing 40 mb inputs to tensorflow-serving and back, and maybe it's throttling on this for some reason.

Incidentally, the issue is the same on the cluster.",logging like slowdown like spent run inference via look utilization used wondering issue passing back maybe throttling reason incidentally issue cluster,issue,positive,negative,neutral,neutral,negative,negative
757554803,"@da-source This error is happening because for some reason the Python client is not able to connect to the cluster.

When you run `cortex cluster info --configure-env test`, does it print out the operator endpoint, and does it match what you are using in Python (`https://aaaca5bfce6a64cd4aefb45335fb109f-8c4bfefdef26a3e8.elb.us-east-1.amazonaws.com`)? Also, if you run `cortex get --env test` on the CLI does it work, or give you the same error?",error happening reason python client able connect cluster run cortex cluster test print operator match python also run cortex get test work give error,issue,negative,positive,positive,positive,positive,positive
757554243,"@lminer It would be worth profiling where the time is going. An easy first step is to add a few log statements within your `predict()` function, to see what is eating the bulk of the time (e.g. is it before `predict()` is even called, or is it all in the `self.client.predict()` call?).

Also, since local mode has been removed going forward, and because local does not have exactly the same architecture as when running in the cluster, it would probably be best to check on the cluster.",would worth time going easy first step add log within predict function see eating bulk time predict even call also since local mode removed going forward local exactly architecture running cluster would probably best check cluster,issue,positive,positive,positive,positive,positive,positive
757553231,"@deliahu probably best to forget about the AWS cluster for now. This is an issue on my local box, which has 90 GB of ram and 2 GPUs, and a threadripper 1950. If I run inference on the model in a jupyter notebook, it is > 6X times faster than if I spin up a local cortex server and run inference through the server. When I run the model through the server, I only see ~10 seconds when the GPU is actually being used. The rest of the time both the CPU and GPU are idle.",probably best forget cluster issue local box ram run inference model notebook time faster spin local cortex server run inference server run model server see actually used rest time idle,issue,positive,positive,positive,positive,positive,positive
757552717,"@lminer yes that makes sense. However, we have actually removed support for running Cortex locally in our latest release (v0.26). We've found that the best way to develop/test a predictor implementation locally is to import it in a separate Python file and call the `__init__()` and `predict()` functions directly. The best way to test an API is to deploy it to an actual dev/test cluster, since this will most closely resemble the production environment.",yes sense however actually removed support running cortex locally latest release found best way predictor implementation locally import separate python file call predict directly best way test deploy actual cluster since closely resemble production environment,issue,positive,positive,positive,positive,positive,positive
757551744,"@lminer Another possibility is insufficient system memory (assuming the memories on the GPUs are equivalent). It might be worth trying on e.g. `g4dn.4xlarge` which still has 1 GPU, but has 64gb memory (versus 16gb for the g4dn.xlarge). ",another possibility insufficient system memory assuming equivalent might worth trying still memory versus,issue,negative,positive,positive,positive,positive,positive
757514609,"@deliahu  It would be nice, if possible, to have the env vars show up locally as well. Makes testing a lot easier. I have found that the env vars show up locally in the api image, just not the tensorflow-serving image.",would nice possible show locally well testing lot easier found show locally image image,issue,positive,positive,positive,positive,positive,positive
757513634,@RobertLucian Have you looked into whether there's any slowdown incurred by passing such a large input to tensorflow-serving? I'm currently trying to figure out why inference is so slow. Right now inference takes 3 minutes and for most of that time both the CPU and GPU are idle. GPU is running for at most 6 seconds during that time. It makes me wonder whether that limit was put there for a reason.,whether slowdown passing large input currently trying figure inference slow right inference time idle running time wonder whether limit put reason,issue,negative,positive,neutral,neutral,positive,positive
757340519,@miguelvr I already have. That number is for the RTX 2080 ti. 6X is the difference between running the model in the server on my local machine and running the model in a jupyter notebook.,already number ti difference running model server local machine running model notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
757340394,"@vishalbollu I was able to create a client, but I'm getting an error when trying to `create_api()`:

```
class PythonPredictor:
    def __init__(self):
        s3 = boto3.client('s3', aws_access_key_id='key',
        aws_secret_access_key='skey')
        s3.download_file('bucket', 'key', ""filename"")

    def predict(self):
        return 'hello'

client.create_api(api_spec= {'name': 'testapi',
  'kind':'RealtimeAPI',
    'type': 'python',
    'path': 'predictor.py',
    'cpu': '1',
    'mem': '1G',
    'api_gateway': 'none'}, predictor=PythonPredictor)

---------------------------------------------------------------------------
CortexBinaryException                     Traceback (most recent call last)
<ipython-input-14-fd690c2c9ee5> in <module>()
      6     'cpu': '1',
      7     'mem': '1G',
----> 8     'api_gateway': 'none'}, predictor=PythonPredictor)

2 frames
/usr/local/lib/python3.6/dist-packages/cortex/binary/__init__.py in run_cli(args, hide_output, mixed_output)
    108         raise CortexBinaryException(result + ""\n"" + process.stderr.read())
    109 
--> 110     raise CortexBinaryException(process.stderr.read())
    111 
    112 

CortexBinaryException: error: Post ""https://aaaca5bfce6a64cd4aefb45335fb109f-8c4bfefdef26a3e8.elb.us-east-1.amazonaws.com/deploy: dial tcp: lookup aaaca5bfce6a64cd4aefb45335fb109f-8c4bfefdef26a3e8.elb.us-east-1.amazonaws.com on 127.0.0.11:53: no such host

unable to connect to your cluster in the test environment (operator endpoint: https://aaaca5bfce6a64cd4aefb45335fb109f-8c4bfefdef26a3e8.elb.us-east-1.amazonaws.com)

if you don't have a cluster running:
    ‚Üí if you'd like to create a cluster, run `cortex cluster up --configure-env test`
    ‚Üí otherwise you can ignore this message, and prevent it in the future with `cortex env delete test`

if you have a cluster running:
    ‚Üí run `cortex cluster info --configure-env test` to update your environment (include `--config <cluster.yaml>` if you have a cluster configuration file)
    ‚Üí if you set `operator_load_balancer_scheme: internal` in your cluster configuration file, your CLI must run from within a VPC that has access to your cluster's VPC (see https://docs.cortex.dev/v/0.26/)

```

I tried running `cortex cluster info --configure-env test`  in the console and got:

`your cluster has 0 API replicas running across 1 instance`

```
instance type   lifecycle   replicas   CPU (requested / total allocatable)   memory (requested / total allocatable)
t3.medium       on-demand   0          0m / 1290m                            0 / 2851464Ki

an environment named ""test"" has been configured to point to this cluster (and was set as the default environment)
```

But after re-running `create_api()` code again, I got the same error. (I'm testing the code in Colab environment, but my cluster is running on AWS so I'm not sure if it makes a difference).",able create client getting error trying class self predict self return recent call last module raise result raise error post dial host unable connect cluster test environment operator cluster running like create cluster run cortex cluster test otherwise ignore message prevent future cortex delete test cluster running run cortex cluster test update environment include cluster configuration file set internal cluster configuration file must run within access cluster see tried running cortex cluster test console got cluster running across instance instance type total allocatable memory total allocatable environment test point cluster set default environment code got error testing code environment cluster running sure difference,issue,negative,positive,neutral,neutral,positive,positive
757338871,@lminer can you please try it out on a notebook with the same GPU used with cortex? T4 GPUs are considerably slower than an RTX 2080 TI.,please try notebook used cortex considerably ti,issue,negative,positive,neutral,neutral,positive,positive
757099220,"(just posting here in case anyone else is following the thread, since @HodorTheCoder and I connected via email)

I've created [this branch](https://github.com/cortexlabs/cortex/compare/0.26...gcp-private-lb) with a temporary implementation to test the feature.",posting case anyone else following thread since connected via branch temporary implementation test feature,issue,negative,neutral,neutral,neutral,neutral,neutral
757022081,Let me know how I can help with this.   I am trying to find the options in gke.go and online to add to make this work.  Thanks.,let know help trying find add make work thanks,issue,positive,positive,positive,positive,positive,positive
756955799,"Oh I see, I think it's because I checked the container when it was running in the cluster, whereas you are just checking locally. If you deploy your API to the cluster, the env vars should be there (they get populated at runtime and are not baked into the image).",oh see think checked container running cluster whereas locally deploy cluster get baked image,issue,negative,neutral,neutral,neutral,neutral,neutral
756923237,"There is a bug in `cortex.cluster_client()` so it won't work. The bug will be fixed in the next release. 

As a workaround, you can mimic the behaviour of `cortex.cluster_client()` by using the Cortex CLI in a terminal to create an environment for your cluster using the command `cortex env configure`. Once the environment is created you can directly access the client using `client = cortex.client(""test"")`.

If running Cortex CLI commands isn't feasible in your use case, you can run the python code below which will mimic the behaviour of `cortex.cluster_client()`.

```python
import cortex
from cortex.binary import run_cli

env_name = ""test""
provider = ""aws""
operator_endpoint = ""your operator endpoint""
aws_access_key_id = ""your aws access key id""
aws_secret_access_key = ""your aws secret key""

cli_args = [""env"", ""configure"", env_name, ""--provider"", provider, ""--operator-endpoint"", operator_endpoint, ""--aws-access-key-id"", aws_access_key_id, ""--aws-secret-access-key"", aws_secret_access_key]

run_cli(cli_args, hide_output=True)

client = cortex.client(env_name)
```

Let me know if this works for you.",bug wo work bug fixed next release mimic behaviour cortex terminal create environment cluster command cortex configure environment directly access client client test running cortex feasible use case run python code mimic behaviour python import cortex import test provider operator access key id secret key configure provider provider client let know work,issue,negative,negative,neutral,neutral,negative,negative
756901647,"> @da-source Thanks for bringing this to our attention. There is indeed a bug in the code that prevents the creation and updating of environments in python. This PR will fix the bug #1772.
> 
> As a temporary workaround, please use the `cortex env configure` CLI command in the terminal to create and update environments. Once the environment has been created/updated using the CLI, creating a client from that environment e.g. `client = cx.client(""test"")` should work as expected.

Alright! But it still haven't resolved the problem that @deliahu and I started adressing:

```
cortex.cluster_client('a','aws','op',
                      'key','skey')
TypeError: string indices must be integer
```",thanks attention indeed bug code creation python fix bug temporary please use cortex configure command terminal create update environment client environment client test work alright still resolved problem string index must integer,issue,positive,positive,positive,positive,positive,positive
756890265,"I used something like the following.

```bash
docker exec -it elegant_hertz /bin/bash
```

The image I'm using right now is non-standard so maybe that's it: `quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0`",used something like following bash docker image right maybe,issue,negative,positive,positive,positive,positive,positive
756856997,"@da-source Thanks for bringing this to our attention. There is indeed a bug in the code that prevents the creation and updating of environments in python. This PR will fix the bug https://github.com/cortexlabs/cortex/pull/1772. 

As a temporary workaround, please use the `cortex env configure` CLI command in the terminal to create and update environments. Once the environment has been created/updated using the CLI, creating a client from that environment e.g. `client = cx.client(""test"")` should work as expected.",thanks attention indeed bug code creation python fix bug temporary please use cortex configure command terminal create update environment client environment client test work,issue,positive,positive,positive,positive,positive,positive
756776536,"and what is the name of the environment in which your cluster is?

please run `cortex env list` in your terminal",name environment cluster please run cortex list terminal,issue,negative,neutral,neutral,neutral,neutral,neutral
756775168,"@miguelvr I don't think that is the issue, I'm getting:

```
client = cx.client(test)
NotFound: can't find environment test, create one by calling `cortex.cluster_client()`
```",think issue getting client test ca find environment test create one calling,issue,negative,neutral,neutral,neutral,neutral,neutral
756707629,"@da-source it seems to me that you are not using the correct cortex client.

please try using this:
```python
import cortex as cx

client = cx.client(your_env_name)
```",correct cortex client please try python import cortex client,issue,negative,neutral,neutral,neutral,neutral,neutral
756618059,"```
TypeError                                 Traceback (most recent call last)
<ipython-input-95-88a0fb64dd75> in <module>()
      2 import cortex
      3 cortex.cluster_client('a','aws','operator',
----> 4                       'key','skey')

1 frames
/usr/local/lib/python3.6/dist-packages/cortex/__init__.py in cluster_client(name, provider, operator_endpoint, aws_access_key_id, aws_secret_access_key)
     86     run_cli(cli_args, hide_output=True)
     87 
---> 88     return Client(name)
     89 
     90 

/usr/local/lib/python3.6/dist-packages/cortex/client.py in __init__(self, env)
     43         """"""
     44         self.env = env
---> 45         self.env_name = env[""name""]
     46 
     47     # CORTEX_VERSION_MINOR

TypeError: string indices must be integers
```

Yes, of course, I replace the arguments with the actual keys and operator

",recent call last module import cortex name provider return client name self name string index must yes course replace actual operator,issue,negative,neutral,neutral,neutral,neutral,neutral
756568164,"@lminer that is weird, it seemed to work for me when I tried last night. What command are you using to connect to the container?

I did something like:

```
kubectl exec api-iris-classifier-6c5556595c-mkv4l -c serve -it -- /bin/bash
```

And then when I ran `echo $AWS_ACCESS_KEY_ID` it showed up",weird work tried last night command connect container something like serve ran echo,issue,negative,negative,negative,negative,negative,negative
756565858,"@da-source Do you also see a stack trace? If so, do you mind sending that? Also, what version of Cortex are you using?

Also, just to make sure, when you are creating the client, are you replacing ""myoperator"" with the actual operator endpoint (e.g. `https://a3d8ff1b76d304a399ef0c3cd97adcd4-170b826c42af4167.elb.us-west-2.amazonaws.com`, and ""key""/""skey"" with your actual key and secret key?",also see stack trace mind sending also version cortex also make sure client actual operator key skey actual key secret key,issue,negative,positive,neutral,neutral,positive,positive
756400081,@lminer that's great news! We plan to release 0.27 on the 19th of January (about 2 weeks from now). ,great news plan release th,issue,positive,positive,positive,positive,positive,positive
756392629,"Thanks, it works! I don't anticipate getting higher than 256 MB. Do you have a tentative timeline for when 0.27 is coming out?",thanks work anticipate getting higher tentative coming,issue,negative,positive,positive,positive,positive,positive
756260126,"@deliahu  Are you sure that the env vars from the yaml file are propagated to the tensorflow-serving image? I'm still getting the error of aws creds not being properly set. Moreover, when I attach to the tensorflow-serving docker container and run `printenv` I don't see the environment variables.

Here's my cortex.yaml

```yaml
- name: pasta
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: serving/cortex_server.py
    models:
      path: ../pasta_bucket/
      signature_key: serving_default
    image: quay.io/cortexlabs/tensorflow-predictor:0.25.0
    tensorflow_serving_image: quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0
    env:
      AWS_ACCESS_KEY_ID: foo
      AWS_SECRET_ACCESS_KEY: bar 
```

Here's the error:

```
2021-01-07 17:22:25.306428: W external/org_tensorflow/tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at whole_file_read_ops.cc:116 : Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location
<_InactiveRpcError of RPC that terminated with:
        status = StatusCode.FAILED_PRECONDITION
        details = ""2 root error(s) found.
  (0) Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location
         [[{{node ReadFile}}]]
         [[StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/while/body/_1860/while/StatefulPartitionedCall/StatefulPartitionedCall/up_sample_layer_2/batch_normalization_177/FusedBatchNormV3/truediv/_3347]]
  (1) Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location
         [[{{node ReadFile}}]]
0 successful operations.
0 derived errors ignored.""
        debug_error_string = ""{""created"":""@1610040145.309085820"",""description"":""Error received from peer ipv4:172.17.0.2:9000"",""file"":""src/core/lib/surface/call.cc"",""file_line"":1061,""grpc_message"":""2 root error(s) found.\n  (0) Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location\n\t [[{{node ReadFile}}]]\n\t [[StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/while/body/_1860/while/StatefulPartitionedCall/StatefulPartitionedCall/up_sample_layer_2/batch_normalization_177/FusedBatchNormV3/truediv/_3347]]\n  (1) Failed precondition: AWS Credentials have not been set properly. Unable to access the specified S3 location\n\t [[{{node ReadFile}}]]\n0 successful operations.\n0 derived errors ignored."",""grpc_status"":9}""
```",sure file image still getting error properly set moreover attach docker container run see environment name kind predictor type path path image foo bar error precondition set properly unable access location status root error found precondition set properly unable access location node precondition set properly unable access location node successful derived description error received peer file root error precondition set properly unable access node precondition set properly unable access node successful derived,issue,negative,positive,neutral,neutral,positive,positive
756252179,"@lminer we have fixed this error in #1769. The fix is present on a customized version of 0.25 specifically made for you - this change is now available in `quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs` image. Your API spec should have this layout:

```yaml
- name: foo
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    ...
    image: quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs
    ...
```

This fix will get into 0.27, or if you need this in 0.26, we can make a patch release for you.

---

For reference, I had this tested on a sound recognizer model and big inputs/outputs were used (up to 256 MB). Also, the grpc limit has been increased to 256 MB. Do you think this will get higher than this? We may also consider making this limit configurable in the API spec sometime down the road or sooner if it's really required.",fixed error fix present version specifically made change available image spec layout name foo kind predictor type image fix get need make patch release reference tested sound recognizer model big used also limit think get higher may also consider making limit spec sometime road sooner really,issue,negative,positive,positive,positive,positive,positive
756202892,"@deliahu Deploying an API that creates/deletes Cortex APIs seems to be the best solution in my case!

I'm getting `TypeError: string indices must be integers` when trying to create Python `cluster_client`, using:

```
#Cortex client
import cortex
cortex.cluster_client('name','aws','myoperator','key','skey')
```

What could be causing this?",cortex best solution case getting string index must trying create python cortex client import cortex could causing,issue,positive,positive,positive,positive,positive,positive
756163972,@lminer it doesn't hold for the `.env` file though. It's only for the env vars specified in the API spec.,hold file though spec,issue,negative,neutral,neutral,neutral,neutral,neutral
755917427,Ah. Does this hold if you use a .env file too? I tried it with that and tensorflow wasn't seeing my aws creds.,ah hold use file tried seeing,issue,negative,neutral,neutral,neutral,neutral,neutral
755905504,"> In my case, the deviation in number of user requests I think would usually be below 100. Could something like this be done with the current architecture?

The current architecture supports it, but it would need to be implemented (this functionality is not currently implemented or exposed to the user).

> How would the autoscaling work, in the scenario that you‚Äôve suggested? Is there a way to auto-create/delete API upon each new user‚Äôs request or will I need to launch enormous amount of APIs on a cluster at launch?

Each API would have it's own autoscaling, so it would be based on the traffic generated by each user. Each user would have a dedicated API with at least one replica.

> Is there a way to auto-create/delete API upon each new user‚Äôs request or will I need to launch enormous amount of APIs on a cluster at launch?

You can create/delete APIs programmatically. The user would have to make two different types of requests: one to create the API, and one to call it once it's live. For the API that creates Cortex APIs, for separation of concerns, it might best to run this outside of the cortex cluster on a separate backend (e.g. app engine, elastic beanstock, lambda, heroku, etc). But I don't see why it couldn't run in the Cortex cluster if that's your preference. You would use Cortex's [Python client](https://docs.cortex.dev/clients/python) to create/delete the Cortex APIs.",case deviation number user think would usually could something like done current architecture current architecture would need functionality currently exposed user would work scenario way upon new user request need launch enormous amount cluster launch would would based traffic user user would least one replica way upon new user request need launch enormous amount cluster launch programmatically user would make two different one create one call live cortex separation might best run outside cortex cluster separate engine elastic lambda see could run cortex cluster preference would use cortex python client cortex,issue,positive,positive,neutral,neutral,positive,positive
755896763,"@lminer I'd just like to add that the AWS credential environment variables are populated in the TF serving container. In addition, you can add custom env vars in both the predictor container and the TF serving container by specifying `env` in your API configuration, e.g.

```yaml
- name: iris-classifier
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: predictor.py
    models:
      path: s3://cortex-examples/tensorflow/iris-classifier/nn/
    env:
      MY_VAR: my-value
```

That said, I agree with @RobertLucian that the best approach will be to address #1740.",like add credential environment serving container addition add custom predictor container serving container configuration name kind predictor type path path said agree best approach address,issue,positive,positive,positive,positive,positive,positive
755875207,"@fandy I just wanted to follow up on this - we are planning to start working on supporting preemptible instances in the next week or two. It's still hard to say when it will be released, but hopefully in 2-4 weeks.",follow start working supporting next week two still hard say hopefully,issue,positive,negative,neutral,neutral,negative,negative
755752033,"@lminer I see. That would not be an optimal architecture.

We have increased the priority on https://github.com/cortexlabs/cortex/issues/1740 and it's the next thing for me to work on. Since this seems to be urgent to you, we will probably make a patch release for you or just create a customised image that will work for you. Will this be acceptable to you?",see would optimal architecture priority next thing work since urgent probably make patch release create image work acceptable,issue,positive,neutral,neutral,neutral,neutral,neutral
755433190,"@RobertLucian because of the GRPC limit on the amount of data that I can send to tensorflow-serving, I thought that instead I would simply send a  link to an object in an s3 bucket and then read and write to s3 directly within the model itself. However, in order to do this, I need some way to get the AWS credentials to the model and it appears as if this can only be done via environment variables.",limit amount data send thought instead would simply send link object bucket read write directly within model however order need way get model done via environment,issue,negative,positive,neutral,neutral,positive,positive
755369248,@lminer I see. We will look into this then. I'll keep you posted as we look into this. Thanks for bringing this up to us.,see look keep posted look thanks u,issue,negative,positive,positive,positive,positive,positive
755333350,"@lminer only the `predictor.image` container gets the exported variables from `.env`. So the answer is no, they don't get exported for the `predictor.tensorflow_serving_image` container.

`predictor.tensorflow_serving_image` at least in our view is only for TFS - anything else is done in the API container (`predictor.image`). Is it possible for you to start a service or anything that you want in `dependencies.sh` instead? `dependencies.sh` includes the exported environment variables and it runs before the API, inside the API container. Here's more on [`dependencies.sh`](https://docs.cortex.dev/workloads/managing-dependencies/system-packages).

Also, your API will inherit the cluster's AWS credentials (provided it's an AWS cluster, read [this](https://docs.cortex.dev/clusters/aws/security#cluster-spin-up) for more) and so you don't have to export them again - unless these credentials are for S3 buckets that fall outside the API credentials' scope. Can you tell us more about this use case?

I see that you want to read from an S3 bucket. If it's a model you want to download, Cortex already handles that awesomely. More on that [here](https://docs.cortex.dev/workloads/realtime/models). Is this what you want? If not, can you tell us about your case?",container answer get container least view anything else done container possible start service anything want instead environment inside container also inherit cluster provided cluster read export unless fall outside scope tell u use case see want read bucket model want cortex already awesomely want tell u case,issue,negative,positive,positive,positive,positive,positive
755145160,"> @da-source something like this could be possible for us to implement. Since it would be based on consistent hashing of a request header (e.g. user ID) it would rely on a few things being true to run as smoothly as possible, e.g. do your users each generate a similar number of requests, or is there a wide variety?
> 
> There is another option worth considering, which is to create a separate API for each user. This would ensure that each replica is fully owned by a single user: all requests would be routed to the same replica, however multiple users would not be able to share a replica (I wrote ""replica"" since you can have multiple replicas per instance). Does this approach seem promising?

In my case, the deviation in number of user requests I think would usually be below 100. Could something like this be done with the current architecture?

How would the autoscaling work, in the scenario that you‚Äôve suggested? Is there a way to auto-create/delete API upon each new user‚Äôs request or will I need to launch enormous amount of APIs on a cluster at launch?",something like could possible u implement since would based consistent request header user id would rely true run smoothly possible generate similar number wide variety another option worth considering create separate user would ensure replica fully single user would replica however multiple would able share replica wrote replica since multiple per instance approach seem promising case deviation number user think would usually could something like done current architecture would work scenario way upon new user request need launch enormous amount cluster launch,issue,positive,positive,neutral,neutral,positive,positive
755115167,"@jackmpcollins Today we decided to spend some time researching/designing this in the next week or two, so I'd expect that we could start working on it in time to be released in v0.28 (we release every two weeks on Tuesday, so that would be Feb 2nd). Would that timing work for you?",today decided spend time next week two expect could start working time release every two would would timing work,issue,negative,neutral,neutral,neutral,neutral,neutral
755105505,"@da-source something like this could be possible for us to implement. Since it would be based on consistent hashing of a request header (e.g. user ID) it would rely on a few things being true to run as smoothly as possible, e.g. do your users each generate a similar number of requests, or is there a wide variety?

There is another option worth considering, which is to create a separate API for each user. This would ensure that each replica is fully owned by a single user: all requests would be routed to the same replica, however multiple users would not be able to share a replica (I wrote ""replica"" since you can have multiple replicas per instance). Does this approach seem promising?",something like could possible u implement since would based consistent request header user id would rely true run smoothly possible generate similar number wide variety another option worth considering create separate user would ensure replica fully single user would replica however multiple would able share replica wrote replica since multiple per instance approach seem promising,issue,positive,positive,positive,positive,positive,positive
754854533,Does this expose the environment variables to the tensorflow-serving image as well? I'm trying to read directly from s3 from tensorflow-serving and it isn't seeing my credentials.,expose environment image well trying read directly seeing,issue,negative,positive,neutral,neutral,positive,positive
754809834,"Great, thanks. We would like to be able to use AWS IAM roles / session_token for CLI auth by the end of January. We can use user accounts for most of the setup/development but will require roles to work before we allow others to use cortex.",great thanks would like able use end use user require work allow use cortex,issue,positive,positive,positive,positive,positive,positive
754785870,"I'm still getting the same error message when I add the `new predictor.image`. I'd rather not share the model if possible. Any other ideas? It seems as if the max message size isn't being raised somewhere.

```
<_InactiveRpcError of RPC that terminated with:
        status = StatusCode.RESOURCE_EXHAUSTED
        details = ""Received message larger than max (102484524 vs. 4194304)""
        debug_error_string = ""{""created"":""@1609868017.138526542"",""description"":""Received message larger
than max (102484524 vs. 4194304)"",""file"":""src/core/ext/filters/message_size/message_size_filter.cc"",""file_line"":203,""grpc_status"":8}""
>
2021-01-05 17:33:37.145639:cortex:pid-1558:INFO:200 OK POST /
```",still getting error message add new rather share model possible message size raised somewhere status received message description received message file cortex post,issue,negative,positive,neutral,neutral,positive,positive
754650323,"@lminer your `predictor.image` also has to be set to `quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs`, so your `cortex.yaml` will look like:

```yaml
- name: foo
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    ...
    image: quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs
    tensorflow_serving_image: quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0
  compute:
    gpu: 1
```

Remember that the `predictor.image` is required to fix the original error `""Received message larger than max (102484524 vs. 4194304)""`. `predictor.image` isn't required if all you want to get is version 2.4.0 of TensorFlow on your TFS.

If you've already done this and it still doesn't work, then maybe you can also share the model with me (you can email me at robert@cortexlabs.com to keep this private) so I can test that as well.",also set look like name foo kind predictor type image compute remember fix original error received message want get version already done still work maybe also share model keep private test well,issue,positive,positive,positive,positive,positive,positive
754573443,"> > will instance be chosen at random, and model will have to be downloaded again, if the randomly accessed instance doesn‚Äôt have the previously downloaded model?
> 
> Yes that is correct

Is there a way to modify this behaviour or at least increase chances of the instance with the needed model at hand being used, instead of an instance which doesn‚Äôt have the previously downloaded model? That way, the models that aren‚Äôt being chosen often, wouldn‚Äôt need to be downloaded on each API call. Ideally, each user would repeatedly access only one instance (where the needed model and data would be stored at), before instance spins down",instance chosen random model randomly instance previously model yes correct way modify behaviour least increase instance model hand used instead instance previously model way chosen often need call ideally user would repeatedly access one instance model data would instance,issue,positive,negative,negative,negative,negative,negative
754417820,"@jackmpcollins We're planning to have a team discussion about it tomorrow, and I will keep you posted on our prioritization for it. On what timeline would you need to use it by, and do you have an interim workaround solutions or is it a full blocker?",team discussion tomorrow keep posted would need use interim full blocker,issue,negative,positive,positive,positive,positive,positive
754416655,"> will instance be chosen at random, and model will have to be downloaded again, if the randomly accessed instance doesn‚Äôt have the previously downloaded model?

Yes that is correct",instance chosen random model randomly instance previously model yes correct,issue,negative,negative,negative,negative,negative,negative
754414650,"> The reason we're sticking with GCP is because of better cost and usability over AWS. Even though they have spot instances and it integrates with Cortex it's still not worth switching from GCP.

That makes sense, thanks for your explanation

> We're going to use the Cloud SDK and cross our fingers we don't run into issues. How does Cortex compare to GCP/Cloud SDK?

Cortex uses the Cloud SDK under the hood to manage GKE, GCR, GCS, etc",reason sticking better cost usability even though spot cortex still worth switching sense thanks explanation going use cloud cross run cortex compare cortex cloud hood manage,issue,positive,positive,positive,positive,positive,positive
754175207,@deliahu would you be able to give an estimate for when the original issue here of supporting aws_session_token for CLI auth might be resolved please?,would able give estimate original issue supporting might resolved please,issue,positive,positive,positive,positive,positive,positive
754147984,@RobertLucian Any idea what's going on. Would love to get this working. ,idea going would love get working,issue,positive,positive,positive,positive,positive,positive
754103132,"@RobertLucian I see. If I specify API compute requirement, so that it can fit only on one instance, make a certain model request (which will download model on one instance, if its not present on an instance), and then after some time make the request for the same model, will Cortex access the instance on which model was previously downloaded or will instance be chosen at random, and model will have to be downloaded again, if the randomly accessed instance doesn‚Äôt have  the previously downloaded model?",see specify compute requirement fit one instance make certain model request model one instance present instance time make request model cortex access instance model previously instance chosen random model randomly instance previously model,issue,positive,negative,negative,negative,negative,negative
754006307,"@da-source with model-multi caching, when a request lands in for a certain model, it will only affect a single API replica (not an instance and nor all instances). An API replica can fit a single time or multiple times on a single instance, it depends on how many compute resources the API demands.

I suppose that the thing you are requesting is described by https://github.com/cortexlabs/cortex/issues/1288 ticket. Is this something that you would definitely need?

> And should query_params be used to define which model to request (with multi-model caching)?

Anything that's passed into the `predict` method can be used to determine the model to use. It's just convenient to use `query_params` because the `payload` is generally kept for the model input.

> Multi-model part of the documentation seemed a little unclear to me and you could update it with the information that you‚Äôve given me, to make the documentation a bit more detailed. Also, it‚Äôs not clear from documentation how to use query_params when making a request to an API with curl*

I see! We'll see what we can do about it! Thank you!",request certain model affect single replica instance replica fit single time multiple time single instance many compute suppose thing ticket something would definitely need used define model request anything predict method used determine model use convenient use generally kept model input part documentation little unclear could update information given make documentation bit detailed also clear documentation use making request curl see see thank,issue,positive,positive,positive,positive,positive,positive
753791476,"> @fandy thanks for reaching out. We don't have a timeline on this yet, but will keep you posted. Have you tried using Cortex on GCP with non-preemtable instances, or with spot instances on AWS?

The reason we're sticking with GCP is because of better cost and usability over AWS. Even though they have spot instances and it integrates with Cortex it's still not worth switching from GCP.

We're going to use the Cloud SDK and cross our fingers we don't run into issues. How does Cortex compare to GCP/Cloud SDK?",thanks reaching yet keep posted tried cortex spot reason sticking better cost usability even though spot cortex still worth switching going use cloud cross run cortex compare,issue,positive,positive,positive,positive,positive,positive
753760155,"@fandy thanks for reaching out. We don't have a timeline on this yet, but will keep you posted. Have you tried using Cortex on GCP with non-preemtable instances, or with spot instances on AWS?",thanks reaching yet keep posted tried cortex spot,issue,positive,positive,positive,positive,positive,positive
753701159,Any updates on this? Would really make GCP worth using.,would really make worth,issue,negative,positive,positive,positive,positive,positive
753531536,"> @da-source the models are downloaded only when they are requested. This means that when multi-model caching is enabled, the models won't get downloaded when the API starts, but when requests start coming in. The cache then gets populated until its threshold is hit (the max number of models kept in the cache) and then they are dropped based on the LRU policy.
> 
> The models that will get downloaded are those specified in the API spec (`predictor.models` field), not the ones that are passed into `query_params`. Generally, `query_params` is used to specify the model that has to be used when running the inference.
> 
> To reiterate, the API will start with no models, and only when requests come in, that's when they get downloaded. Subsequent requests for the same model will be faster because the model will already be present on disk / in memory.
> 
> The opposite is when only live reloading is enabled, for which case, the models are downloaded on the API from the very beginning - and they are always available to the user.
> 
> Does that make sense to you? And if not, could you tell us where we could improve our documentation (the parts that are confusing if any) with regards to live-reloading/multi-model caching? Your feedback is gonna help us improve our documentation for future users :)
> 
> Out of pure curiosity, what kind of models are you deploying? 95-100GB per each one is quite a lot.

Thanks, for clearing it up! One small detail: with multi-model caching, when request for a certain model comes, will it affect only one instance or will the requested model be downloaded on all of the running instances? 
And should query_params be used to define which model to request (with multi-model caching)? 

Multi-model part of the documentation seemed a little unclear to me and you could update it with the information that you‚Äôve given me, to make the documentation a bit more detailed. Also, it‚Äôs not clear from documentation how to use query_params when making a request to an API with curl*

I‚Äôm using a number of pretrained XLNET models. What I meant to say was, all of them combined weigh around 100GB (not each one of them).",wo get start coming cache threshold hit number kept cache based policy get spec field generally used specify model used running inference reiterate start come get subsequent model faster model already present disk memory opposite live case beginning always available user make sense could tell u could improve documentation feedback gon na help u improve documentation future pure curiosity kind per one quite lot thanks clearing one small detail request certain model come affect one instance model running used define model request part documentation little unclear could update information given make documentation bit detailed also clear documentation use making request curl number meant say combined weigh around one,issue,positive,positive,positive,positive,positive,positive
753500183,"@da-source the models are downloaded only when they are requested. This means that when multi-model caching is enabled, the models won't get downloaded when the API starts, but when requests start coming in. The cache then gets populated until its threshold is hit (the max number of models kept in the cache) and then they are dropped based on the LRU policy.

The models that will get downloaded are those specified in the API spec (`predictor.models` field), not the ones that are passed into `query_params`. Generally, `query_params` is used to specify the model that has to be used when running the inference.

To reiterate, the API will start with no models, and only when requests come in, that's when they get downloaded. Subsequent requests for the same model will be faster because the model will already be present on disk / in memory.

The opposite is when only live reloading is enabled, for which case, the models are downloaded on the API from the very beginning - and they are always available to the user.

Does that make sense to you? And if not, could you tell us where we could improve our documentation (the parts that are confusing if any) with regards to live-reloading/multi-model caching? Your feedback is gonna help us improve our documentation for future users :)

---

Out of pure curiosity, what kind of models are you deploying? 95-100GB per each one is quite a lot.",wo get start coming cache threshold hit number kept cache based policy get spec field generally used specify model used running inference reiterate start come get subsequent model faster model already present disk memory opposite live case beginning always available user make sense could tell u could improve documentation feedback gon na help u improve documentation future pure curiosity kind per one quite lot,issue,positive,positive,positive,positive,positive,positive
753446065,"@deliahu Multi-model documentation states that you can store many models in cache and then load model that you need. But will all the models be downloaded on an instance, when it is requested? 

Or will the model specified in query_params be downloaded?  If so, if it is downloaded on one instance, and API call randomly redirects user to another instance without that model, will it have to be downloaded again?

I‚Äôm trying to avoid those AWS data transfer prices, because my files are so large (all of them combined currently weigh around 95-100GB)",documentation store many cache load model need instance model one instance call randomly user another instance without model trying avoid data transfer large combined currently weigh around,issue,negative,positive,neutral,neutral,positive,positive
753368194,"> You can configure how many models are cached on disk

But will the models be downloaded on instance startup or API call? I don‚Äôt want to unnecessarily download models/data, since they are quite large and AWS charges 0.15$/1GB",configure many disk instance call want unnecessarily since quite large,issue,negative,positive,positive,positive,positive,positive
753345164,"@miguelvr With multi-model caching, will all the models be downloaded on an instance, when it spins up or will the model of choice be downloaded after each API call specifying which model to get?",instance model choice call model get,issue,negative,neutral,neutral,neutral,neutral,neutral
753332710,"Sounds like what you want might be solved with multi model caching, which Cortex supports. You can check the documentation here: https://docs.cortex.dev/workloads/multi-model",like want might model cortex check documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
753327226,"@miguelvr I have a few quite large models and datasets, which I would like to be available to use at any given time and I would like to make this application as cost-effective as possible. I considered creating separate API for each model/dataset, but doing so would require running many idle instances on the cluster, when APIs are not being used. I also considered using a single API implementation - downloading all the models/datasets to each instance and then loading them when needed, using `load_model`, but doing so would: a) unnecessarily increase AWS data-transfer costs, since most of the models will be unused; b) the instance usual disk capacity of 50G might not be enough to store all the datasets.

If I could instead, create a single API, which would download the model+dataset combination on an instance upon user's request (using `predict.py'`s payload), run the instance for set amount of time and then spin it down and that way I would avoid running many unused APIs or download many files that won't be used on an instance. Keeping only one instance per user, could help avoiding OOM errors, if a user decides to download few datasets or load more than one model (average compute requirement for each model is around `6G`, so running only one model per instance is plauisble). Also, if instances will be given to users at random, a user might download/use one model on one instance and then will have to re-download the model, if he gets access to another instance without his model, and avoiding that is an additional motivation for keeping one instance per user.",quite large would like available use given time would like make application possible considered separate would require running many idle cluster used also considered single implementation instance loading would unnecessarily increase since unused instance usual disk capacity might enough store could instead create single would combination instance upon user request run instance set amount time spin way would avoid running many unused many wo used instance keeping one instance per user could help user load one model average compute requirement model around running one model per instance also given random user might one model one instance model access another instance without model additional motivation keeping one instance per user,issue,positive,positive,neutral,neutral,positive,positive
753304659,"
> Somewhere around 150-300 users. It would be best if each user only had access only to one instance. I will set up a web interface, with a timer set to amount of minutes an instance will be alive, once it spins up (which must be defined in cortex.yaml as I understand), after timer runs out, there will be an option to request a new instance. Since the time window at which instance is alive will be predefined and known by a user, the user will have that time window (around 30-50 minutes) to interact with the instance, after which new instance could be requested if needed.
> 
> Ability to distribute the load, that you‚Äôve mentioned, I think wouldn‚Äôt be an issue, since if all existing users will have to stay on their instances, untill the timer runs out/instance spins down, that would actually be preferable.

Hello!

Engineering-wise that sounds like a very bad idea for an application architecture. Can you tell us what is it that you are trying to achieve? There must be other ways to achieve it, while maintaining a stateless application.

",somewhere around would best user access one instance set web interface timer set amount instance alive must defined understand timer option request new instance since time window instance alive known user user time window around interact instance new instance could ability distribute load think issue since stay untill timer would actually preferable hello like bad idea application architecture tell u trying achieve must way achieve stateless application,issue,positive,positive,positive,positive,positive,positive
753285210,"> @da-source that is an interesting question. I have a few follow up questions:
> 
> 1. How many users do you expect to serve?
> 2. Do you want there to be 1 user per instance, or is it ok if multiple users (at random) access the same instance, as long as a user's requests are all routed to the same instance each time?
> 
> Generally, it would be better to not do it this way if possible, since there is some behavior that could be undefined. For example, what if the user doesn't make a request for a bit, the instance spins down, and then the user makes a new request: will they have lost their data, or is it ok to spin up a new instance?
> 
> Also, locking users to specific instances could affect autoscaling's ability to distribute the load, since instances created during autoscaling could only be used for new users (all existing users would have to stay on their initial instance).

Somewhere around 150-300 users. It would be best if each user only had access only to one instance. I will set up a web interface, with a timer set to amount of minutes an instance will be alive, once it spins up (which must be defined in cortex.yaml as I understand), after timer runs out, there will be an option to request a new instance. Since the time window at which instance is alive will be predefined and known by a user, the user will have that time window (around 30-50 minutes) to interact with the instance, after which new instance could be requested if needed.

Ability to distribute the load, that you‚Äôve mentioned, I think wouldn‚Äôt be an issue, since if all existing users will have to stay on their instances, untill the timer runs out/instance spins down, that would actually be preferable.",interesting question follow many expect serve want user per instance multiple random access instance long user instance time generally would better way possible since behavior could undefined example user make request bit instance user new request lost data spin new instance also locking specific could affect ability distribute load since could used new would stay initial instance somewhere around would best user access one instance set web interface timer set amount instance alive must defined understand timer option request new instance since time window instance alive known user user time window around interact instance new instance could ability distribute load think issue since stay untill timer would actually preferable,issue,positive,positive,positive,positive,positive,positive
753236932,I don't foresee any problems with us using a `cortex cluster set-role` command. I was mostly trying get an understanding of how RBAC might be implemented and how consistent/integrated this could be with our existing permissions. Thanks for the explanation.,foresee u cortex cluster command mostly trying get understanding might could thanks explanation,issue,negative,positive,positive,positive,positive,positive
753230982,"@da-source that is an interesting question. I have a few follow up questions:

1. How many users do you expect to serve?
2. Do you want there to be 1 user per instance, or is it ok if multiple users (at random) access the same instance, as long as a user's requests are all routed to the same instance each time?

Generally, it would be better to not do it this way if possible, since there is some behavior that could be undefined. For example, what if the user doesn't make a request for a bit, the instance spins down, and then the user makes a new request: will they have lost their data, or is it ok to spin up a new instance?

Also, locking users to specific instances could affect autoscaling's ability to distribute the load, since instances created during autoscaling could only be used for new users (all existing users would have to stay on their initial instance).",interesting question follow many expect serve want user per instance multiple random access instance long user instance time generally would better way possible since behavior could undefined example user make request bit instance user new request lost data spin new instance also locking specific could affect ability distribute load since could used new would stay initial instance,issue,positive,positive,positive,positive,positive,positive
753229797,"> if I'm understanding correctly now the cortex operator just checks that the user's AWS creds are valid for the same AWS account and then its own permissions are used to actually execute the command.

Yes, that is correct

Regarding having `cortex cluster set-role` be a wrapper around `eksctl create iamidentitymapping`: That is an interesting idea, however it could limit our ability to support fine-grained access control in the future. For example, if cortex managed the RBAC rather than relying on Kubernetes, we could more easily support granting access to specific APIs (e.g. specifying that a user can update one API but not others). This might be possible to do natively with Kubernetes RBAC via Custom Resources, but it would require us to re-architect some of our backend (right now we use native Kubernetes resources like services, deployments, etc). Would it be possible to automate the call to `cortex cluster set-role` in your terraform, or are you asking this question because it is only possible (or is easier) for you to automate `eksctl create iamidentitymapping` plus kubernetes RBAC?

",understanding correctly cortex operator user valid account used actually execute command yes correct regarding cortex cluster wrapper around create interesting idea however could limit ability support access control future example cortex rather could easily support access specific user update one might possible natively via custom would require u right use native like would possible call cortex cluster question possible easier create plus,issue,positive,positive,positive,positive,positive,positive
753215365,"> What would you say is the advantage of the IAM-based approach when compared to cortex accounts?

We have a strong preference for having this be IAM-based. We would be granting access to the cortex CLI to IAM roles rather than individual users, with users temporarily assuming these roles using our SSO.

> Currently this isn't supported; if you have valid AWS credentials, you have access to all API-related CLI commands (e.g. get, deploy, delete, but not the cluster commands).

Ah I misunderstood this. I thought the user's cortex commands could be restricted by their IAM policies or kubernetes RBAC, but if I'm understanding correctly now the cortex operator just checks that the user's AWS creds are valid for the same AWS account and then its own permissions are used to actually execute the command.

> Do you know if it's possible to do this fully natively in IAM, for example by making custom actions/resources that can be assigned to IAM users/roles, like cortex:deploy, cortex:get, etc?

I'm not familiar enough with IAM to answer this but another question in the same vein: would it be possible (and reasonable) for the `cortex cluster set-role --arn <arn> --role [admin|developer|read_only]` command to just be a wrapper over `eksctl create iamidentitymapping` or kubernetes RBAC? We would like to have permissions declared and managed in terraform code and it is possible to do this for IAM roles/policies and kubernetes RBAC. Our rough plan is to use `cortex cluster up` to create the cluster, then use terraform to manage access and permissions.",would say advantage approach cortex strong preference would access cortex rather individual temporarily assuming currently valid access get deploy delete cluster ah misunderstood thought user cortex could restricted understanding correctly cortex operator user valid account used actually execute command know possible fully natively example making custom assigned like cortex deploy cortex get familiar enough answer another question vein would possible reasonable cortex cluster arn arn role command wrapper create would like declared code possible rough plan use cortex cluster create cluster use manage access,issue,positive,positive,neutral,neutral,positive,positive
753033225,"> I would like to use Cortex functionality, to create an application where each user will be able to request and communicate with AWS instance for a period of time. In this scenario, data of each user will be processed and stored on one whole AWS instance. From the documentation, I understand that each API call will use an instance that it is not busy at the moment. It wouldn‚Äôt be ideal if by making an API call, a user would receive sensitive data stored by a another user on the same instance. Would it be possible to somehow mark an instance to which an API call is being made? That way the data of individual users wouldn‚Äôt be made accesible to everyone, but only to those users who request/use an instance.

For example, when I request an API, Cortex will spin up an instance, and next time I make a call, will I access the same instance or some other one (considering that I will specify Compute requirement to be the max of the type of instance I‚Äôll use, so that one instance will be used per API) ?",would like use cortex functionality create application user able request communicate instance period time scenario data user one whole instance documentation understand call use instance busy moment ideal making call user would receive sensitive data another user instance would possible somehow mark instance call made way data individual made everyone instance example request cortex spin instance next time make call access instance one considering specify compute requirement type instance use one instance used per,issue,positive,positive,positive,positive,positive,positive
753007069,"I see, yeah that makes sense.

What would you say is the advantage of the IAM-based approach when compared to cortex accounts? Is it mostly the convenience of not having to worry about creating cortex users / saving cortex passwords (versus creating/reusing IAM users, creating identity mappings, and using IAM-based credentials for login)? Or is there another advantage? And how strong of a preference do you have, i.e. would only supporting cortex-based accounts and not supporting IAM-based auth be a deal breaker, or would you be able to use cortex-based accounts if necessary?

If I understand the proposals correctly, at a high level, they would be something like this:

### IAM-based

```bash
# first, create an IAM user that you want to grant access to, or identify an existing IAM user

cortex cluster set-role --arn <arn> --role [admin|developer|read_only]
cortex cluster revoke-access --arn <arn>
```

With this approach, CLI commands would use the default AWS credentials chain, or the user could pass in the name of an AWS profile. Optionally, `cortex env configure` could ask you which AWS profile you want to associate with that cluster, and remember your choice in the future (and not use the default chain).

### Cortex-based

```bash
# this command would print or download a password
cortex cluster create-account --username <username> --role [admin|developer|read_only]

cortex cluster delete-account --username <username>
```

With this approach, `cortex env configure` would prompt for the cortex cluster credentials, and CLI commands would use those credentials. There would be no IAM involvement.",see yeah sense would say advantage approach cortex mostly convenience worry cortex saving cortex versus identity login another advantage strong preference would supporting supporting deal breaker would able use necessary understand correctly high level would something like bash first create user want grant access identify user cortex cluster arn arn role cortex cluster arn arn approach would use default chain user could pas name profile optionally cortex configure could ask profile want associate cluster remember choice future use default chain bash command would print password cortex cluster role cortex cluster approach cortex configure would prompt cortex cluster would use would involvement,issue,positive,positive,positive,positive,positive,positive
753000522,"@imagine3D-ai Yes that is correct. And if your instance had 10G of memory, cortex would put 2 APIs on that instance (assuming the CPU requests of the APIs also fit when added up). Keep in mind that the memory available to your APIs is slightly less than the instance's capacity: `cortex cluster info` will show you how much memory is available to APIs on each instance.",yes correct instance memory cortex would put instance assuming also fit added keep mind memory available slightly le instance capacity cortex cluster show much memory available instance,issue,positive,positive,positive,positive,positive,positive
752996433,"I see. One last question: if I set compute requirement to 5G in cortex.yaml and use a cluster with aws instances that have only about 5G memory allocatable, will that mean that each new Cortex API request will instantiate a brand new AWS instance?",see one last question set compute requirement use cluster memory allocatable mean new cortex request brand new instance,issue,negative,negative,neutral,neutral,negative,negative
752866198,"I think cortex keeping a list of IAM users/roles with access to different functions would be best case for us. This is pretty much how access to EKS clusters deployed with eksctl already work. The user/role that deployed the cluster is initially the only one with access via kubectl, but you can add user/roles with a command like

```
eksctl create iamidentitymapping -c ${cluster_name} -r ${region} -p ${profile_name} \
    --arn ""arn:aws:iam::${account_id}:role/${role_name}"" \
    --username ${account_name} \
    --group ${group_name}
```

So there could be a predefined set of cortex groups like {admin, developer, read_only} that user/roles be added to for simple use cases.",think cortex keeping list access different would best case u pretty much access already work cluster initially one access via add command like create region arn arn group could set cortex like developer added simple use,issue,positive,positive,positive,positive,positive,positive
752861186,"@jackmpcollins @sp-davidpichler you both make great points. We're still formulating our thoughts around this, so your feedback is timely and much appreciated.

> I think it's most simple from the user's point of view to have just two sets of credentials

I think this is a good generalization of the 3 sets of credentials that @sp-davidpichler and I were mentioning, which makes sense when pulling the ""current user"" credentials from the AWS order of precedence (which we could let you override by passing in the profile name). In practice, there will likely be the three sets of credentials in terms of actual permissions, but # 1 and # 3 would behave the same way in terms of how those credentials are loaded, so there would be only two mechanisms for loading credentials.

> I think ideally the cluster credentials/user/role would be created and managed by cortex rather than provided by the user.

This is an interesting idea. I could still see a use case for users specifying which credentials to use in the cluster if they want to reduce access, since if Cortex created an IAM user for the cluster, it would probably have to take more permissions than would be necessary (for example `s3:*` instead of just the bucket that contains the models). So maybe cortex would create/manage it automatically, but there would be a way to override the permissions granted.

> We would prefer to use AWS credentials with the CLI. This makes it easy to limit permissions per user/role and grant/revoke access using our existing methods (terraform). We may want some users/services to have permission only to list Cortex APIs, some to additionally be able to deploy/delete, and some to have full ability to run cluster up/down. Using IAM policies for the CLI users allows this granularity.

Access control is a great idea, and I've created https://github.com/cortexlabs/cortex/issues/1748 to track it. Currently this isn't supported; if you have valid AWS credentials, you have access to all API-related CLI commands (e.g. get, deploy, delete, but not the cluster commands). My question is: does it make more sense to implement Cortex's RBAC via IAM roles/permissions, or within Cortex itself? If it's fully managed by Cortex, there would be commands like `cortex cluster create-user`, `cortex cluster create-role`, `cortex cluster assign-role`, etc. Do you know if it's possible to do this fully natively in IAM, for example by making custom actions/resources that can be assigned to IAM users/roles, like `cortex:deploy`, `cortex:get`, etc? I just did a quick test, and although it gave me a warning, I was able to create an IAM policy with made-up actions, but I wasn't able to use made-up resources. If it's not possible to fully rely on IAM, then we might still need to create cortex commands to grant access to users/roles, like `cortex cluster configure-access-policy` which would take in the IAM user/role and which Cortex permissions to grant it. If that is the only way to use IAM, it seems like having all of the RBAC live inside of Cortex could be more straightforward. I'd love to hear your thoughts on this.

> AWS profiles

Regarding from where to pull the AWS credentials: I agree that using AWS profile names, or using the default chain if the profile name is not specified, makes sense. Let's discuss the details after deciding how to best handle CLI-to-cluster auth, since I think the details will be affected by that decision.

I'd also be happy to jump on a call if at some point you feel it'd be easier to discuss this live; feel free to email me at david@cortex.dev if you want to find a time.",make great still around feedback timely much think simple user point view two think good generalization sense current user order precedence could let override passing profile name practice likely three actual would behave way loaded would two loading think ideally cluster would cortex rather provided user interesting idea could still see use case use cluster want reduce access since cortex user cluster would probably take would necessary example instead bucket maybe cortex would automatically would way override would prefer use easy limit per access may want permission list cortex additionally able full ability run cluster granularity access control great idea track currently valid access get deploy delete cluster question make sense implement cortex via within cortex fully cortex would like cortex cluster cortex cluster cortex cluster know possible fully natively example making custom assigned like cortex deploy cortex get quick test although gave warning able create policy able use possible fully rely might still need create cortex grant access like cortex cluster would take cortex grant way use like live inside cortex could straightforward love hear regarding pull agree profile default chain profile name sense let discus best handle since think affected decision also happy jump call point feel easier discus live feel free want find time,issue,positive,positive,positive,positive,positive,positive
752808367,"I don't see the need to distinguish between credentials for provisioning the cluster and credentials for using the CLI. I think it's most simple from the user's point of view to have just two sets of credentials:
1. Credentials for the user currently using the CLI
2. Credentials the cluster uses to manage itself

Credentials for the current user should be obtained following the [AWS order of precedence](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence) so that Cortex CLI works the same way as kubectl and AWS CLI. To run `cortex cluster up` the current user must be authenticated as an IAM user/role with AdministratorAccess. If they want to use a less privileged AWS profile after spinning up the cluster they can configure a new cortex env with this profile for themselves. Non-admin users will be able to use (a subset of) cortex commands against the cluster created by an admin user.

Cluster credentials should be manually specified when running `cortex cluster up` or automatically created as node instance roles or similar as you suggest @sp-davidpichler üëç . I don't agree that cluster credentials should be stored in `~/.aws/credentials` because these aren't for human use and are only provided once - when spinning up the cluster. I think ideally the cluster credentials/user/role would be created and managed by cortex rather than provided by the user. This way a user only has to think about their own AWS credentials.",see need distinguish cluster think simple user point view two user currently cluster manage current user following order precedence cortex work way run cortex cluster current user must want use le privileged profile spinning cluster configure new cortex profile able use subset cortex cluster user cluster manually running cortex cluster automatically node instance similar suggest agree cluster human use provided spinning cluster think ideally cluster would cortex rather provided user way user think,issue,positive,positive,positive,positive,positive,positive
752785606,"I think it's also a good idea to keep cluster authentication tied to AWS IAM authentication. Our IAM credentials are directly linked to our company SSO. I think the best option would be for the cortex cli to respect more of the AWS auth related parameters like AWS_PROFILE and AWS_SESSION_TOKEN. Or make a move to only support AWS profiles(minus the cluster credentials). 

I do think however the move to three sets of logical credentials makes sense:

1. One set of credentials to provision the cluster
1. One set of credentials to run the cluster with (which might also be handled by node instance roles which are created by eksctl)
1. One set of credentials to get, list, and deploy APIs.

A workflow could look something like

- `cortex cluster up -c config.yaml --deployment-aws-profile profile1 --cluster-aws-profile profile2 --user-aws-profile profile3 -e aws-dev`
- `cortex get -e ai-dev`
- `cortex env configure -e ai-dev2 --user-aws-profile profile3`
- `cortex get -e ai-dev2`",think also good idea keep cluster authentication tied authentication directly linked company think best option would cortex respect related like make move support minus cluster think however move three logical sense one set provision cluster one set run cluster might also handled node instance one set get list deploy could look something like cortex cluster profile profile profile cortex get cortex configure profile cortex get,issue,positive,positive,positive,positive,positive,positive
752783230,"Ah. I didn't quite understand the breakdown of responsibilities there. I guess the CuDNN not found warning I saw was a red herring as the api doesn't use the GPU anyway. That being said, it seems as if I'm still seeing the error around resource exhaustion. It does look like the GPU is being used before the error, so maybe the exhaustion is occurring when the server is returning large values.

Here's my current cortex.yaml

```yaml
- name: foo
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: serving/cortex_server.py
    models:
      path: ../foo/
      signature_key: serving_default
    tensorflow_serving_image: quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0
  compute:
    gpu: 1
```

Here's my tensorflow predictor implementation:

```python
import numpy as np


class TensorFlowPredictor:
    def __init__(self, tensorflow_client, config):
        self.client = tensorflow_client
        self.config = config

    def predict(self, payload, query_params, headers):
        target, residual = self.client.predict(
            {""waveform"": np.array(payload[""audio""]).astype(""float32"")}
        )
        return {""target"": target.numpy().tolist(), ""residual"": residual.numpy().tolist()}
```

And here's the error again:

```python
2020-12-30 22:49:04.428932:cortex:pid-1558:INFO:500 Internal Server Error POST /
2020-12-30 22:49:04.429214:cortex:pid-1558:ERROR:Exception in ASGI application
Traceback (most recent call last):
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/uvicorn/protocols/http/httptools_impl.py"", line
390, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/uvicorn/middleware/proxy_headers.py"", line 45, in __call__
    return await self.app(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/fastapi/applications.py"", line 181, in __call__
    await super().__call__(scope, receive, send)  # pragma: no cover
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/applications.py"", line 111, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/errors.py"", line 181, in __call__
    raise exc from None
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 25, in __call__
    response = await self.dispatch_func(request, self.call_next)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/serve/serve.py"", line 187, in parse_payload
    return await call_next(request)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 45, in call_next
    task.result()
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 38, in coro
    await self.app(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 25, in __call__
    response = await self.dispatch_func(request, self.call_next)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/serve/serve.py"", line 134, in register_request
    response = await call_next(request)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 45, in call_next
    task.result()
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/middleware/base.py"", line 38, in coro
    await self.app(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/exceptions.py"", line 82, in __call__
    raise exc from None
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/routing.py"", line 566, in __call__
    await route.handle(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/routing.py"", line 227, in handle
    await self.app(scope, receive, send)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/routing.py"", line 41, in app
    response = await func(request)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/fastapi/routing.py"", line 183, in app
    dependant=dependant, values=values, is_coroutine=is_coroutine
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/fastapi/routing.py"", line 135, in run_endpoint_function
    return await run_in_threadpool(dependant.call, **values)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/starlette/concurrency.py"", line 34, in run_in_threadpool
    return await loop.run_in_executor(None, func, *args)
  File ""/opt/conda/envs/env/lib/python3.6/concurrent/futures/thread.py"", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/serve/serve.py"", line 200, in predict
    prediction = predictor_impl.predict(**kwargs)
  File ""/mnt/project/serving/cortex_server.py"", line 11, in predict
    {""waveform"": np.array(payload[""audio""]).astype(""float32"")}
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/lib/client/tensorflow.py"", line
114, in predict
    return self._run_inference(model_input, consts.SINGLE_MODEL_NAME, model_version)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/lib/client/tensorflow.py"", line
164, in _run_inference
    return self._client.predict(model_input, model_name, model_version)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/cortex_internal/lib/model/tfs.py"", line 376, in
predict
    response_proto = self._pred.Predict(prediction_request, timeout=timeout)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/grpc/_channel.py"", line 826, in __call__
    return _end_unary_response_blocking(state, call, False, None)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/grpc/_channel.py"", line 729, in _end_unary_response_blocking
    raise _InactiveRpcError(state)
grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
        status = StatusCode.RESOURCE_EXHAUSTED
        details = ""Received message larger than max (102484524 vs. 4194304)""
        debug_error_string = ""{""created"":""@1609368544.425429771"",""description"":""Received message larger
than max (102484524 vs. 4194304)"",""file"":""src/core/ext/filters/message_size/message_size_filter.cc"",""file_line"":203,""grpc_status"":8}""
>
```",ah quite understand breakdown guess found warning saw red herring use anyway said still seeing error around resource exhaustion look like used error maybe exhaustion server large current name foo kind predictor type path path compute predictor implementation python import class self predict self target residual audio float return target residual error python cortex internal server error post cortex error exception application recent call last file line result await file line return await scope receive send file line await super scope receive send cover file line await scope receive send file line raise none file line await scope receive file line response await request file line return await request file line file line await scope receive send file line response await request file line response await request file line file line await scope receive send file line raise none file line await scope receive sender file line await scope receive send file line handle await scope receive send file line response await request file line file line return await file line return await none file line run result file line predict prediction file line predict audio float file line predict return file line return file line predict file line return state call false none file line raise state status received message description received message file,issue,negative,positive,neutral,neutral,positive,positive
752781181,"
We would prefer to use AWS credentials with the CLI. This makes it easy to limit permissions per user/role and grant/revoke access using our existing methods (terraform). We may want some users/services to have permission only to list Cortex APIs, some to additionally be able to deploy/delete, and some to have full ability to run cluster up/down. Using IAM policies for the CLI users allows this granularity.

The most straightforward mental model for us would be if the Cortex CLI functioned similarly to kubectl. We use [`saml2aws`](https://github.com/Versent/saml2aws) to enforce MFA and retrieve temporary AWS credentials. These are stored in `~/.aws/credentials`. When setting up a new kubectl context we can optionally associate it with an AWS profile from `~/.aws/credentials`. This shows as an argument to the aws command in the `~/.kube/config` file:
```yml
users:
- name: arn:aws:eks:us-west-2:000000000000:cluster/cortex-dev
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      args:
      - --region
      - us-west-2
      - eks
      - get-token
      - --cluster-name
      - cortex-dev
      - --profile
      - dev
      command: aws
      env: null
```

Our usage of Kubectl then looks like:
- `saml2aws login -a dev` - Fetch temporary credentials which get stored under the ""dev"" profile in `~/.aws/credentials`.
- `kubectl config use-context context-dev` - Switch to correct kubectl context.
- `kubectl get pods` - Run kubectl commands

Alternatively if the AWS profile is not added to the kubeconfig, we can run `saml2aws exec -a dev -- kubectl get pods` which executes the command with credentials as environment variables. Because kubectl uses the AWS credentials chain these environment variables take precedence over the ""default"" AWS profile.

The kubectl commands return a permission error when the AWS user/role does not have the required permissions.

We would expect the cortex usage to look similar:
- `saml2aws login -a dev` - Refresh temporary AWS credentials (not necessary if using IAM user credentials).
- `cortex cluster up --aws-profile dev -e aws-dev` - AWS profile argument here is saved as part of the cortex env. Prompts for cluster aws_access_key_id and cluster aws_secret_access_key which must be explicitly provided.
- `cortex cluster up` If AWS profile not explicitely provided, this follows the AWS credentials chain to get credentials (env vars, then ""default"" profile, etc).
- `cortex get -e aws-dev` - Run cortex commands using credentials from the AWS profile associated with the env. If no AWS profile associated with the env, then this follows the AWS credentials chain to get credentials.

The cortex commands return a permission error if the AWS credentials provided do not have the permissions required for the command. So rather than three sets of credentials (Admin, cluster, and CLI-user), it is just two (CLI-user, cluster).
",would prefer use easy limit per access may want permission list cortex additionally able full ability run cluster granularity straightforward mental model u would cortex similarly use enforce retrieve temporary setting new context optionally associate profile argument command file name arn user region profile dev command null usage like login dev fetch temporary get dev profile switch correct context get run alternatively profile added run dev get command environment chain environment take precedence default profile return permission error would expect cortex usage look similar login dev refresh temporary necessary user cortex cluster dev profile argument saved part cortex cluster cluster must explicitly provided cortex cluster profile provided chain get default profile cortex get run cortex profile associated profile associated chain get cortex return permission error provided command rather three cluster two cluster,issue,negative,positive,positive,positive,positive,positive
752663016,"@lminer you would not need to set the `image` field at all as it will be picking up its adequate image by default. Just to get this clear, the `tensorflow-predictor` image (the one that goes for the `image` field) has the API serving stuff whereas the `tensorflow-serving-*` images (the ones that go for the `tensorflow_serving_image` field) hold the inference engine.

That means that inferences will take place in the container represented by the `tensorflow_serving_image` image - and the way to go about it is to use the `tensorflow_client` (that's passed into the predictor's constructor) to run inferences. Not in any other way. If you try to run inferences inside the actual Tensorflow Predictor implementation using the GPU, that will not work as that is not the inference engine.

Could you also share your TensorFlow Predictor implementation? I might be able to give you a few pointers if required.",would need set image field adequate image default get clear image one go image field serving stuff whereas go field hold inference engine take place container image way go use predictor constructor run way try run inside actual predictor implementation work inference engine could also share predictor implementation might able give,issue,positive,positive,positive,positive,positive,positive
752338712,"@mutal it seems that the built-in sse implementation in the browser does not support POST requests, however there is a 3rd party library that does. If you download [this file](https://raw.githubusercontent.com/mpetazzoni/sse.js/master/lib/sse.js) and import it in your html file (or just copy-paste the contents into a `<script>` tag), then something like this should work:

```javascript
url = ""http://aa9cdc236514c4f20bcbd1b814253658-7e52832be1124fb6.elb.us-west-2.amazonaws.com/my-api"";
var source = new SSE(url, {
  method: 'POST',
  headers: { 'Content-Type': 'text/plain' },
  payload: 'test'
});
source.addEventListener('message', function (e) {
  console.log(e.data);
});
source.stream();
```

Just replace the url with your API's load balancer URL, and update the content-type and the payload based on what your API expects. I used `console.log(e.data);` just to see that it works, but you'll probably want to do something else with the response.",implementation browser support post however party library file import file content script tag something like work source new method function replace load balancer update based used see work probably want something else response,issue,positive,positive,positive,positive,positive,positive
752326505,"@jackmpcollins Yes you make a lot of great points; we definitely want to improve this experience, and make it possible for you to use Cortex!

In fact, we were actually discussing this morning about how we could improve this, and we came up with a proposal that I'd like to run by you to see if it would work for you. We think it would make sense to go one step further and decouple the CLI auth from AWS credentials entirely, since they really aren't related. There are a few advantages of this approach, including adding clarity that the CLI auth is not related to the cluster credentials, and that it would work the same regardless of cloud provider (we support AWS/GCP now and would like to add Azure soon).

It could look something like this: `cortex cluster up` creates a cluster password, and automatically configures your CLI to use it. `cortex cluster info` shows the password in case you need to configure the CLI on a new machine or you lost your password (of course, you can only run `cortex cluster info` if you have the AWS credentials). `cortex env configure` will prompt for your cluster password instead of AWS credentials. We could also add a new command like `cortex cluster reset-password` or `cortex cluster set-password PASSWORD` which would update the password.

Even with this change, there would still be two relevant sets of AWS credentials: the credentials used to spin up the cluster (which require significant access), and the credentials that are persisted in the cluster (require much less access). But it will be easier for users to understand this compared to current approach, which involves three sets of credentials.

Does this make sense to you?",yes make lot great definitely want improve experience make possible use cortex fact actually morning could improve came proposal like run see would work think would make sense go one step entirely since really related approach clarity related cluster would work regardless cloud provider support would like add azure soon could look something like cortex cluster cluster password automatically use cortex cluster password case need configure new machine lost password course run cortex cluster cortex configure prompt cluster password instead could also add new command like cortex cluster cortex cluster password would update password even change would still two relevant used spin cluster require significant access cluster require much le access easier understand current approach three make sense,issue,positive,positive,positive,positive,positive,positive
752309509,"We would like to use cortex but my company uses AWS IAM roles for employees so this issue is blocking. I think the best solution would be to clearly distinguish CLI AWS credentials from cluster AWS credentials, particularly the way in which these are obtained.

Proposal:

- The Cortex CLI can obtain AWS credentials for the current user by using the [standard AWS credentials chain](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.html#cli-configure-quickstart-precedence) (which is built into `aws-sdk-go`). For example [when creating a session without explicitly providing credentials](https://docs.aws.amazon.com/sdk-for-go/api/aws/session/). This prioritizes environment variables over `~/.aws/credentials` (same behavior as Cortex) and handles user credentials, temporary credentials from an assumed role, and other methods of storing AWS credentials.
- Cortex does not cache CLI AWS credentials - caching is pointless for temporary credentials, and long-term credentials are already cached in `~/.aws/credentials`.
- CLI AWS credentials cannot be passed as arguments to Cortex CLI - these should be provided by the standard methods.
- Require Cluster AWS credentials to be explicitly provided to `cortex cluster up`. Do not automatically use the CLI/user credentials for these. A user can provide the same credentials if they wish.
- Cluster AWS credentials are associated with an IAM user (as done currently) - avoids the issue of having to refresh them often for the moment.
- Cluster AWS credentials can be cached by Cortex - these probably don't belong in `~/.aws/credentials` because they are not intended to be used by the human user, only by the cluster.

In future:

- Each Cortex env can optionally be associated with an AWS profile from `~/.aws/credentials` (in a similar way to Kubernetes contexts).
- Allow the Cluster to use an IAM role.

I am happy to take a shot at this over the next week or so if this solution seems reasonable.

Related issue: https://github.com/cortexlabs/cortex/issues/741
",would like use cortex company issue blocking think best solution would clearly distinguish cluster particularly way proposal cortex obtain current user standard chain built example session without explicitly providing environment behavior cortex user temporary assumed role cortex cache pointless temporary already cortex provided standard require cluster explicitly provided cortex cluster automatically use user provide wish cluster associated user done currently issue refresh often moment cluster cortex probably belong intended used human user cluster future cortex optionally associated profile similar way allow cluster use role happy take shot next week solution reasonable related issue,issue,positive,positive,positive,positive,positive,positive
752275734,"@RobertLucian Thanks so much for this! What do I use for the `image` field? I tried the standard `tensorflow-predictor` image and GPU doesn't work. If I keep the python-predictor-gpu image, I get the same issue as before with the file size being too large.",thanks much use image field tried standard image work keep image get issue file size large,issue,negative,positive,positive,positive,positive,positive
752257082,"@imagine3D-ai looked at code and I have a few things to say.

`load_model` is a method that receives an argument that points to a local directory path. That directory path contains the model from your S3/GCS resource as it was _synchronized_ by Cortex for you - you can see the models that were synchronized for you by running `cortex get <api-name>`. That means that in the `load_model` method, you are supposed to actually load the model into memory and then return that loaded model. That returned value is the same as the one that's retrieved by `client.get_model` method (for your given model).

In the `predict` method, you run the prediction on your desired model. If multiple models are present, you would typically use the query params to get the model name, then you retrieve the model using `client.get_model` method (without caring how you got the model there), then you run the inference, and finally, you return the result.

Now, the returned value of `load_model` (and therefore of `get_model`) can be anything, there are no enforcements in that regard, but that object (value) should be loaded from disk as specified by `load_model`.

Your predictor implementation should have this general layout

```python
class PythonPredictor:
    def __init__(self, config, python_client):
    	self.client = python_client

    def load_model(self, model_path):
        # some function/sequence of instructions to load a model into memory
        model = load_model_from_disk(model_path)
        return model

    def predict(self, payload, query_params):
      # get the model
      model = self.client.get_model(query_params[""model""])
      
      # assuming the model has a run method
      result = model.run(payload)

      return result
```

---

I recommend going through [this page](https://docs.cortex.dev/workloads/realtime/models) that explains how live-reloading/multi-model-caching actually works. Let me know if there's anything confusing in there that you think we should address/modify. We are open to suggestions!",code say method argument local directory path directory path model resource cortex see synchronized running cortex get method supposed actually load model memory return loaded model returned value one method given model predict method run prediction desired model multiple present would typically use query get model name retrieve model method without got model run inference finally return result returned value therefore anything regard object value loaded disk predictor implementation general layout python class self self load model memory model return model predict self get model model model assuming model run method result return result recommend going page actually work let know anything think open,issue,positive,negative,neutral,neutral,negative,negative
752253142,"@lminer one immediate problem I notice with your cortex.yaml is that for the TensorFlow Predictor, you must have the `tensorflow-predictor` image in for the `image` field and the `tensorflow-serving-*` image for set for the `tensorflow_serving_image` field.

In your case, it seems to be set to the `python-predictor-gpu-slim`, which is not the way to configure this. The naming scheme we have come up with for the images is related to the predictor type that the user has specified in the API spec. Therefore, `python-predictor-gpu` implies that's an image to be used for the Python Predictor type.

---

Now, let's re-reference this https://github.com/cortexlabs/cortex/blob/c8da08542e0978c0c2f9c94694bf441092aaafe6/images/tensorflow-serving-gpu/Dockerfile#L1-L12

This is the Dockerfile for the `tensorflow-serving-gpu` image, which is automatically set for you in the API spec (when deploying). The current one runs on TF 2.3.0, which from TensorFlow's [docs](https://www.tensorflow.org/install/source#gpu), runs on CUDA 10.1 & cuDNN 7. If we upgrade this to TF 2.4.0, it will give you CUDA 11 and cuDNN 8.

The upgraded Dockerfile would look like this:

```Dockerfile
FROM tensorflow/serving:2.4.0-gpu

RUN apt-get update -qq && apt-get install -y --no-install-recommends -q \
        libnvinfer7=7.1.3-1+cuda11.0 \
        libnvinfer-plugin7=7.1.3-1+cuda11.0 \
        curl \
    && apt-get clean -qq && rm -rf /var/lib/apt/lists/*

COPY images/tensorflow-serving-gpu/run.sh /src/
RUN chmod +x /src/run.sh

ENTRYPOINT [""/src/run.sh""]
```

I already built this image for your - you can pull it from `quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0`. You then need to update the `tensorflow_serving_image` field in your API spec. In case you want to re-do this by yourself, here's how you can do it:

1. Clone the cortex repo and checkout the 0.25 branch (with git clone https://github.com/cortexlabs/cortex.git, cd cortex, and git checkout 0.25)
1. Build the image by running `./build/build-image.sh tensorflow-serving-gpu`.
1. Choose a docker registry (such as Docker Hub or Quay) and create a public image repository on it (for me, I created `quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4`
1. Log into your docker registry using docker login with the appropriate credentials
1. Run `docker tag quay.io/cortexlabs/tensorflow-serving-gpu:0.25.0 quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0` (replace the registry URL with yours)
1. Run `docker push quay.io/robertlucian/cortex-tensorflow-serving-gpu-tf2.4:0.25.0` (replace the registry URL with yours)
1. Update your `cortex.yaml` to point to your newly pushed image.

---

Let me know if this worked for you!",one immediate problem notice predictor must image image field image set field case set way configure naming scheme come related predictor type user spec therefore image used python predictor type let image automatically set spec current one upgrade give would look like run update install curl clean copy run already built image pull need update field spec case want clone cortex branch git clone cortex git build image running choose docker registry docker hub quay create public image repository log docker registry docker login appropriate run docker tag replace registry run docker push replace registry update point newly image let know worked,issue,positive,positive,positive,positive,positive,positive
752196174,"@deliahu According to [mozilla.org](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events): SSE `is one-way connection, so you can't send events from a client to a server.` and the examples that I've seen implementing JS EventSource, seem to be only listening to a server, without making a `POST` request like it's done at Cortex. But how can Cortex API be called without `POST -X payload`?",according connection ca send client seen seem listening server without making post request like done cortex cortex without post,issue,negative,neutral,neutral,neutral,neutral,neutral
752189935,"> @mutal if it's working for you with `curl`, then it is not an issue with Cortex, it's just a matter of implementing the js client to do what you want.
> 
> Here are a few examples I found by googling:
> 
> * https://www.w3schools.com/html/html5_serversentevents.asp
> * https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events

I see, thanks for info!",working curl issue cortex matter client want found see thanks,issue,positive,positive,positive,positive,positive,positive
752160867,"@mutal if it's working for you with `curl`, then it is not an issue with Cortex, it's just a matter of implementing the js client to do what you want.

Here are a few examples I found by googling:

* https://www.w3schools.com/html/html5_serversentevents.asp
* https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events",working curl issue cortex matter client want found,issue,negative,neutral,neutral,neutral,neutral,neutral
752107730,"@RobertLucian 
Thanks, for answering! Could you take a look at this code and say if I'm missing something:

```
import boto3
import sys
import os


class PythonPredictor:
    def __init__(self, config, python_client):
    	s3 = boto3.client('s3', aws_access_key_id='key', 
    		aws_secret_access_key='s-key')
    	self.client = python_client

    def load_model(self, model_path):
        return load_from_disk(model_path)

    def predict(self, payload, query_params):
      file = self.client.get_model(query_params[""model""])
      with open(file) as f:
      	#do something
      return file
```

In the scenario above, I want for each instance to download only a single model/file of user's choice by passing (`query_params['model']` argument), from my bucket, and then do something with the file. Once the specified file is downloaded, should I use the same `predict()` function to load and perform further operations on the file or is it better to use `post_predict()` function?

As I understand `client.get_model()` uses `load_from_disk()` which accesses `cortex.yaml` to get directory structure of S3 bucket where models/files are located, is this correct? Also when using live-reloading using `client.get_model`, how should the aws-access-keys be passed?",thanks could take look code say missing something import import import o class self self return predict self file model open file something return file scenario want instance single user choice passing argument bucket something file file use predict function load perform file better use function understand get directory structure bucket correct also,issue,negative,positive,neutral,neutral,positive,positive
751893684,"@imagine3D-ai instances come with memory (RAM) and some disk capacity. The `cache_size` is referring to the max number of models that will get loaded in memory (RAM) and `disk_cache_size` to the max number of models kept on disk (i.e. as far as I remember, the EC2 instances are configured with 50 GB by default). Disk storage comes cheaper than RAM memory.

I believe that if a model doesn't fit in memory, an OOM error will be reported by Cortex. If a model doesn't fit on disk, then that is an undefined situation - the user must ensure that no more than X amount of GBs are used on disk.

Also, a model's size in GB (on disk) doesn't translate to how much memory will be required upon loading. From personal experience, I can tell that there's more that it takes than what a model takes on disk - you need to take into account the weights + inputs + overhead.

> Does disk_cache_size mean that all the files will be downloaded to user's local device that is being used?

`disk_cache_size` represents the maximum number of models that _can_ be downloaded and kept on disk at any point in time. When the API starts, by default there won't be any models on-disk/in-memory, and as requests come in, that pool gets populated until it hits that threshold, after which, the models are evicted from disk/memory based on their LRU.

I recommend going through the _models_ page we have [here](https://docs.cortex.dev/workloads/realtime/models) to get a better understanding of this.",come memory ram disk capacity number get loaded memory ram number kept disk far remember default disk storage come ram memory believe model fit memory error cortex model fit disk undefined situation user must ensure amount used disk also model size disk translate much memory upon loading personal experience tell model disk need take account overhead mean user local device used maximum number kept disk point time default wo come pool threshold based recommend going page get better understanding,issue,positive,positive,positive,positive,positive,positive
751876866,"Up to now, I've been trying to use the Tensorflow Predictor path. Do you have any TFS and tensorflow predictor images for tensorflow 2.4.0 (CUDA 11.0, CuDNN 8) that work with a higher limit? Here's my cortex.yaml:

```yaml
- name: foo
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: serving/cortex_server.py
    models:
      path: ../foo/
      signature_key: serving_default
    image: quay.io/cortexlabs/python-predictor-gpu-slim:0.25.0-cuda11.0-cudnn8
    tensorflow_serving_image: quay.io/cortexlabs/tensorflow-serving-gpu:0.25.0
  compute:
    gpu: 1

```",trying use predictor path predictor work higher limit name foo kind predictor type path path image compute,issue,positive,positive,positive,positive,positive,positive
751760779,"@lminer yes, we have a version with CUDA 11.0 and cuDNN 8, but it's for the Python Predictor image.

For the TensorFlow Predictor, there are 2 images required: one for the TFS server (which needs to have access to the GPU device) and one for the API server. As of this moment, the TFS server is configured to use version 10.1 of CUDA and (presumably version 7 of cuDNN). Here's its Dockerfile for the GPU version:

https://github.com/cortexlabs/cortex/blob/c8da08542e0978c0c2f9c94694bf441092aaafe6/images/tensorflow-serving-gpu/Dockerfile#L1-L12

Let us know if you want to follow this path, in which case, we can help you along the way!",yes version python predictor image predictor one server need access device one server moment server use version presumably version version let u know want follow path case help along way,issue,positive,neutral,neutral,neutral,neutral,neutral
751548963,@RobertLucian thanks for this! Do you have a version with cuda 11.0 and cudnn 8? I'm testing this out for deployment on a GPU machine.,thanks version testing deployment machine,issue,negative,positive,positive,positive,positive,positive
751361136,"@deliahu Thanks, I've created an empty `env.sh` and got it working!

I'm having a slight issue accessing this live version of Cortex API through HTML interface. When I use `curl` in console it works as expected and returns live output, but when using the same API on a simple HTML page, output seems to be displayed, only after the whole process is finished. Could this have something to do with setting `api_gateway: none` in `cortex.yaml`?

Here's a test page that I tried:

```
<!DOCTYPE html>
<head>
<h1>live-api test</h1>
</head>

  <body id=""home"">
  <div id=main>
    <div><button type=""button"" onclick=""output()"">Output</button>
    <div id=out></div>
  </div>
</body>

  <script>
    var HttpClient = function () {
      this.get = function (url, callback) {
        var xhr = new XMLHttpRequest();
        xhr.open(""GET"", url, true);
        xhr.onreadystatechange = function () {
          if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
            callback(xhr.responseText);
        }
        xhr.send(null);
      }
      this.post = function (url, payload, callback) {
        var xhr = new XMLHttpRequest();
        xhr.open(""POST"", url, true);
        xhr.setRequestHeader(""Content-Type"", ""application/json"");
        xhr.onreadystatechange = function () {
          if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
            callback(xhr.responseText);
        }
        xhr.send(payload);
      }
    }
    var client = new HttpClient();
    function output() {
      document.getElementById(""out"").innerHTML = """";
      var api = 'myapi';
      var payload = 1; //test payload
      client.post(api, payload, function (response) {
        document.getElementById(""out"").innerHTML = response;
      });
    }
  </script>
</html>
```
Is there a way to get the live Cortex output in HTML the same way as in console but without the `data:` prefixes?",thanks empty got working slight issue live version cortex interface use curl console work live output simple page output displayed whole process finished could something setting none test page tried head test body home div div button button output output div script function function new get true function null function new post true function client new function output function response response way get live cortex output way console without data,issue,positive,positive,positive,positive,positive,positive
751348792,"> Batch APIs are asynchronous, so the client will immediately get a response back. It would be up to you to write the prediction result to a location where the client can access it once it's ready. However, it will take minutes for each batch job to start up, so if you are making requests just one at a time, this would not be a good approach. It's designed for jobs that take many minutes to run.
> 
> If your models are quite large and only can fit one or a few per instance, then your best bet is to use an API per model, or to use multi-model caching. with multi-model caching, you can define how many models are kept in memory at a time. If a request comes in which requires a model that is not in memory, it will load the new model and evict an existing model from memory. The request is still synchronous and models can all be kept on disk, so how long it takes for the response to come back depends on how long it takes to load the model from disk to memory and then run the inference. Here are the [relevant section in the docs](https://docs.cortex.dev/workloads/realtime/models#multi-model-caching), but you will probably want to read the entire page, since multi-model caching relies on a few other concepts described above.

I've read through the link that you've provided and I have a few questions about multi-model-caching: 
If a single model weighs around 5Gb, and I specify `cache_size` to 1 (to keep only 1 model in memory) and `disk_cache_size` to 9 (to keep the rest of the models in disk), what happens if the instance doesn't have enough memory to keep all the models on disk? If I use `m5.large` instance, it has about 8Gb of space, so it will fit only 1 model, and I'm not sure how it will work with multi-model-caching. Does `disk_cache_size` mean that all the files will be downloaded to user's local device that is being used?",batch asynchronous client immediately get response back would write prediction result location client access ready however take batch job start making one time would good approach designed take many run quite large fit one per instance best bet use per model use define many kept memory time request come model memory load new model evict model memory request still synchronous kept disk long response come back long load model disk memory run inference relevant section probably want read entire page since read link provided single model around specify keep model memory keep rest disk instance enough memory keep disk use instance space fit model sure work mean user local device used,issue,positive,positive,positive,positive,positive,positive
751293729,"Batch APIs are asynchronous, so the client will immediately get a response back. It would be up to you to write the prediction result to a location where the client can access it once it's ready. However, it will take minutes for each batch job to start up, so if you are making requests just one at a time, this would not be a good approach. It's designed for jobs that take many minutes to run.

If your models are quite large and only can fit one or a few per instance, then your best bet is to use an API per model, or to use multi-model caching. with multi-model caching, you can define how many models are kept in memory at a time. If a request comes in which requires a model that is not in memory, it will load the new model and evict an existing model from memory. The request is still synchronous and models can all be kept on disk, so how long it takes for the response to come back depends on how long it takes to load the model from disk to memory and then run the inference. Here are the [relevant section in the docs](https://docs.cortex.dev/workloads/realtime/models#multi-model-caching), but you will probably want to read the entire page, since multi-model caching relies on a few other concepts described above.",batch asynchronous client immediately get response back would write prediction result location client access ready however take batch job start making one time would good approach designed take many run quite large fit one per instance best bet use per model use define many kept memory time request come model memory load new model evict model memory request still synchronous kept disk long response come back long load model disk memory run inference relevant section probably want read entire page since,issue,positive,positive,positive,positive,positive,positive
751291976,"> @imagine3D-ai There are a few methods to consider
> 
> 1. Deploy multiple models in a single API. You can load all of the models into memory on init, and then have a query parameter in the request or a field in the request payload which specifies which model to use. Here is a simple [example](https://docs.cortex.dev/workloads/multi-model/example#define-a-multi-model-api).
> 2. Use a [Batch API](https://docs.cortex.dev/workloads/batch): if your requests do not need immediate responses (can be asynchronous) and take some time to complete, this is a good approach since it scales to zero (each request would create a replica to run the request on, so it would not be efficient if requests are quick to handle)
> 3. It is possible to run multiple separate APIs on a single instance, depending on the size of the instance and the compute requirements of the API. You can configure the compute requests for the API replicas in the `compute` section in the [API configuration](https://docs.cortex.dev/workloads/realtime/configuration), and check the capacity of your instance with `cortex cluster info`. If you are trying to keep cost/complexity down by keeping everything on a single instance, probably option 1 will be better. This approach could be better if your APIs need to independently autoscale
> 
> Do any of these options seem like they would work?

Batch API seems to me to be the most plausible choice, since my models are quite large and can‚Äôt be fitted on a single instance.
It takes some time to get a response from my model (about 10-12 seconds), could Batch API make this even longer?
Can I specify which model should be downloaded with Batch API, by sending that information via payload? I haven‚Äôt yet tried using Batch API and all the other Cortex examples I‚Äôve seen, download a file on init, and I‚Äôm wondering if this could be determined by predict(payload) function or something alike.

",consider deploy multiple single load memory query parameter request field request model use simple example use batch need immediate asynchronous take time complete good approach since scale zero request would create replica run request would efficient quick handle possible run multiple separate single instance depending size instance compute configure compute compute section configuration check capacity instance cortex cluster trying keep keeping everything single instance probably option better approach could better need independently seem like would work batch plausible choice since quite large fitted single instance time get response model could batch make even longer specify model batch sending information via yet tried batch cortex seen file wondering could determined predict function something alike,issue,positive,positive,positive,positive,positive,positive
751290267,"@imagine3D-ai There are a few methods to consider

1. Deploy multiple models in a single API. You can load all of the models into memory on init, and then have a query parameter in the request or a field in the request payload which specifies which model to use. Here is a simple [example](https://docs.cortex.dev/workloads/multi-model/example#define-a-multi-model-api).
2. Use a [Batch API](https://docs.cortex.dev/workloads/batch): if your requests do not need immediate responses (can be asynchronous) and take some time to complete, this is a good approach since it scales to zero (each request would create a replica to run the request on, so it would not be efficient if requests are quick to handle)
3. It is possible to run multiple separate APIs on a single instance, depending on the size of the instance and the compute requirements of the API. You can configure the compute requests for the API replicas in the `compute` section in the [API configuration](https://docs.cortex.dev/workloads/realtime/configuration), and check the capacity of your instance with `cortex cluster info`. If you are trying to keep cost/complexity down by keeping everything on a single instance, probably option 1 will be better. This approach could be better if your APIs need to independently autoscale

Do any of these options seem like they would work?",consider deploy multiple single load memory query parameter request field request model use simple example use batch need immediate asynchronous take time complete good approach since scale zero request would create replica run request would efficient quick handle possible run multiple separate single instance depending size instance compute configure compute compute section configuration check capacity instance cortex cluster trying keep keeping everything single instance probably option better approach could better need independently seem like would work,issue,positive,positive,positive,positive,positive,positive
751289385,"Thanks for following up, I forgot that step!

You might not actually need anything in that file since you are just building the image and not pushing it anywhere with the `registry.sh` script (you'll do it manually later). So to start, just try creating an empty file (e.g. `mkdir dev/config` and then `touch dev/config/env.sh`. Then try running it again, does it work? If not, you might need to add some environment variables to `env.sh` (see [here](https://github.com/cortexlabs/cortex/blob/master/CONTRIBUTING.md#cluster-configuration)) but I'm hoping not. Keep me posted!",thanks following forgot step might actually need anything file since building image pushing anywhere script manually later start try empty file touch try running work might need add environment see keep posted,issue,negative,positive,neutral,neutral,positive,positive
751289159,"> In other words, must minimum number of instances running equal to number of APIs deployed?

Not exactly, since multiple APIs can run on a single instance depending on the compute request of the API replica(s) and the capacity of your instance.

With 0 APIs running, the minimum is 0 instances

With 1+ APIs running, the minimum is 1 instance, but the minimum could be more (depending on the compute request of the API replica(s) and the capacity of your instance)",must minimum number running equal number exactly since multiple run single instance depending compute request replica capacity instance running minimum running minimum instance minimum could depending compute request replica capacity instance,issue,negative,positive,neutral,neutral,positive,positive
751288915,"> I'm considering a project involving a dozen multiple inference APIs. As I understand, when deploying an API to a cluster, each API has at least 1 instance which constantly runs on the cluster. Is there a way to spin-up an API when user requests it and then spin the API down when the job is complete, to reduce the costs of running multiple instances on a single cluster?
> 
> Let's say that one instance costs 0.5$/hour and the fixed price of running a cluster is 0.2$/hour. Then deploying 10 APIs on a cluster costs 0.5 * 10 + 0.2 = 5.2/hour*24 = 124.8$/day. If instead it would be possible to spin-up/spin-down the APIs on demand, only when user requests them, the cost of running a cluster when no API calls are being made, would be 0.2$/hour = 4.8$/day (which is considerably less).
> 
> Is there a way to achieve reduced costs of running a cluster with multiple APIs?
> 
> (I considered running a VM with Cortex installed, which would accept calls to deploy an API, get the endpoint, make API call and then spin down the API if it's not being used and no instances are needed to be running; is this a good idea?)

Another solution might be to accept payload which would specify which model needs to be downloaded/used (instead of downloading pre-specified model on init) and then a single API could manage all the different models. Is this a better way to implement this?",considering project dozen multiple inference understand cluster least instance constantly cluster way user spin job complete reduce running multiple single cluster let say one instance fixed price running cluster cluster instead would possible demand user cost running cluster made would considerably le way achieve reduced running cluster multiple considered running cortex would accept deploy get make call spin used running good idea another solution might accept would specify model need instead model single could manage different better way implement,issue,positive,positive,neutral,neutral,positive,positive
751283581,"> @da-source I figured out the source of the discrepancy: when go to the [spot instance pricing page](https://aws.amazon.com/ec2/spot/pricing/), it defaults to the `us-east-2` (Ohio) region. However, the default Cortex region is `us-east-1` (N. Virginia). You can check the pricing for `us-east-1` by adjusting the region on their web page, or change the Cortex region by adding `region: us-west-2` to your `cluster.yaml` file. Then the numbers should match (or at least be closer to matching)

Alright, thanks!",figured source discrepancy go spot instance page region however default cortex region check region web page change cortex region region file match least closer matching alright thanks,issue,positive,negative,neutral,neutral,negative,negative
751282837,"> 4\. ./dev/registry.sh update-single python-predictor-cpu --provider local

Thanks, I'll try! 

At step 4, when I try to build an image using `./dev/registry.sh update-single python-predictor-cpu --provider local`, I'm getting:
`cortex/dev/config/env.sh: No such file or directory`
When I check my `dev` folder there are no `config/env.sh`  folder /file to which the `registry.sh` seems to be pointing to. However `images.sh` and `util.sh`, mentioned in `registry.sh` are in place. Should I manually download the `config` folder?",provider local thanks try step try build image provider local getting file directory check dev folder folder pointing however place manually folder,issue,negative,positive,neutral,neutral,positive,positive
751280836,"> > Yes, you can have multiple APIs running on a single instance, as long as the sum of the `compute` requirements that you requested for your APIs don't exceed the amount available on the instance. You can check the instance capacity by running `cortex cluster info`, and specify your compute request with the `compute` field in the [api configuration](https://docs.cortex.dev/workloads/realtime/configuration)
> 
> I see. Let‚Äôs say I‚Äôll deploy 5 different API‚Äôs and specify max_instances to 5. Each API will take up compute of one whole instance. When I deploy a cluster, with those specs, will I have to pay for 5 instances running all the time or just for 1 one of them + extra IF I request one of the other APIs?

In other words, must minimum number of instances running equal to number of APIs deployed?",yes multiple running single instance long sum compute exceed amount available instance check instance capacity running cortex cluster specify compute request compute field configuration see let say deploy different specify take compute one whole instance deploy cluster spec pay running time one extra request one must minimum number running equal number,issue,negative,positive,neutral,neutral,positive,positive
751280526,"> Yes, you can have multiple APIs running on a single instance, as long as the sum of the `compute` requirements that you requested for your APIs don't exceed the amount available on the instance. You can check the instance capacity by running `cortex cluster info`, and specify your compute request with the `compute` field in the [api configuration](https://docs.cortex.dev/workloads/realtime/configuration)

I see. Let‚Äôs say I‚Äôll deploy 5 different API‚Äôs and specify max_instances to 5 and min_instances to 1. Each API will take up compute of one whole instance. When I deploy a cluster, with those specs, will I have to pay for 5 instances running all the time or just for 1 one of them + extra IF I request one of the other APIs?",yes multiple running single instance long sum compute exceed amount available instance check instance capacity running cortex cluster specify compute request compute field configuration see let say deploy different specify take compute one whole instance deploy cluster spec pay running time one extra request one,issue,negative,positive,neutral,neutral,positive,positive
751279853,"Yes, you can have multiple APIs running on a single instance, as long as the sum of the `compute` requirements that you requested for your APIs don't exceed the amount available on the instance. You can check the instance capacity by running `cortex cluster info`, and specify your compute request with the `compute` field in the [api configuration](https://docs.cortex.dev/workloads/realtime/configuration)",yes multiple running single instance long sum compute exceed amount available instance check instance capacity running cortex cluster specify compute request compute field configuration,issue,negative,positive,neutral,neutral,positive,positive
751279708,"@mutal I'd be happy to build the image for you, but it might be best if I show you how to build it, so that you won't have to be blocked on me in the future:

1. Clone the cortex repo and checkout the 0.25 branch (with `git clone https://github.com/cortexlabs/cortex.git`, `cd cortex`, and `git checkout 0.25`)
1. Open up `pkg/cortex/serve/cortex_internal/serve/serve.py` in a text editor
1. Make the edits as shown in [this diff](https://github.com/cortexlabs/cortex/pull/1565/files) (basically, replace `def predict(request: Request):` with `async def predict(request: Request):`, and replace `prediction = predictor_impl.predict(**kwargs)` with the 4 lines if/else shown in the diff)
1. Build the image by running `./dev/registry.sh update-single python-predictor-cpu --provider local`
1. Choose a docker registry (such as Docker Hub or Quay) and create a public image repository on it (for me, I created `quay.io/cortexlabsdev/python-predictor-cpu-async`
1. Log into your docker registry using `docker login` with the appropriate credentials
1. Run `docker tag cortexlabs/python-predictor-cpu:latest quay.io/cortexlabsdev/python-predictor-cpu-async:0.25.0` (replace the registry url with yours)
1. Run `docker push quay.io/cortexlabsdev/python-predictor-cpu-async:0.25.0` (replace the registry url with yours)
1. Update your `cortex.yaml` to point to your newly pushed image

Let me know if that works for you!",happy build image might best show build wo blocked future clone cortex branch git clone cortex git open text editor make shown basically replace predict request request predict request request replace prediction shown build image running provider local choose docker registry docker hub quay create public image repository log docker registry docker login appropriate run docker tag latest replace registry run docker push replace registry update point newly image let know work,issue,positive,positive,positive,positive,positive,positive
751279119,"> @da-source It is possible to create a cluster with 0 instances, as long as there are no APIs deployed to the cluster. When a cluster is first created, there are no APIs running on it. Once there is an API deployed, the cluster will scale to at least 1 instance and will remain at at least 1 instance until all APIs have been deleted.

Just to clarify: can I have multiple APIs deployed with just one instance running by default, when those APIs are not being called or does each API need it‚Äôs own instance running at all time?",possible create cluster long cluster cluster first running cluster scale least instance remain least instance clarify multiple one instance running default need instance running time,issue,negative,negative,neutral,neutral,negative,negative
751229263,"> @imagine3D-ai This could be related to CORS. Are you running Cortex in the `local` environment or on `aws`? Also, which version of cortex are you using? CORS with the local version of Cortex is not supported in all cases, but the latest version of Cortex running on an AWS cluster should work.

It works after updating to latest version. Thanks!",could related running cortex local environment also version cortex local version cortex latest version cortex running cluster work work latest version thanks,issue,negative,positive,positive,positive,positive,positive
751227461,"@deliahu After I've installed latest `0.25 Cortex `, I can no longer spin up cluster using `quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0`. I'm getting this message:
`predictor: image: the specified image (quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0) has a tag (0.23.0) which does not match your Cortex version (0.25.0)`.

Is there a way to update the `0.23` image that you've modified, so that it can be used with latest version of Cortex?",latest cortex longer spin cluster getting message predictor image image tag match cortex version way update image used latest version cortex,issue,negative,positive,positive,positive,positive,positive
751142489,"@lminer this looks like a problem with grpc.

Looking at [this](https://github.com/tensorflow/serving/issues/1382#issuecomment-503375968), it appears that this limit can be configured. I raised the limit to 128 MB on the `0.25-fix-tfs-lengths` branch specifically for you.

To test with the change from the above branch, you only need to modify your API spec (assuming you use the `cortex.yaml`) to use the `quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs` image. To do this, set the `predictor.image` field to `quay.io/robertlucian/tensorflow-predictor:0.25.0-tfs` and let us know if this worked for you.

For more context, this is the [commit](https://github.com/cortexlabs/cortex/commit/652924bb306121babeb278f0662b98da04d2b17f) we did for the above solution.

Needless to say, this is a temporary solution. If this works for you, then we'll address this in one of the next releases. Please let us know if this worked for you.",like problem looking limit raised limit branch specifically test change branch need modify spec assuming use use image set field let u know worked context commit solution needle say temporary solution work address one next please let u know worked,issue,positive,neutral,neutral,neutral,neutral,neutral
751138943,"@lminer awesome! With regards to the error message when the inputs don't match, we've improved it here #1742. Thank you for reporting this!",awesome error message match thank,issue,positive,positive,positive,positive,positive,positive
750977948,"It would be nice to do so with a decorator. Maybe we would need the user to use a cortex package from the predictor.

Something along the lines of

```python
@cortex.middleware(my_middleware)
class PythonPredictor:
    (...)
```

",would nice decorator maybe would need user use cortex package predictor something along python class,issue,negative,positive,positive,positive,positive,positive
750966229,"@da-source I figured out the source of the discrepancy: when go to the [spot instance pricing page](https://aws.amazon.com/ec2/spot/pricing/), it defaults to the `us-east-2` (Ohio) region. However, the default Cortex region is `us-east-1` (N. Virginia). You can check the pricing for `us-east-1` by adjusting the region on their web page, or change the Cortex region by adding `region: us-west-2` to your `cluster.yaml` file. Then the numbers should match (or at least be closer to matching)",figured source discrepancy go spot instance page region however default cortex region check region web page change cortex region region file match least closer matching,issue,negative,negative,negative,negative,negative,negative
750936867,"@imagine3D-ai This could be related to CORS. Are you running Cortex in the `local` environment or on `aws`? Also, which version of cortex are you using? CORS with the local version of Cortex is not supported in all cases, but the latest version of Cortex running on an AWS cluster should work.",could related running cortex local environment also version cortex local version cortex latest version cortex running cluster work,issue,negative,positive,positive,positive,positive,positive
750936107,"@da-source It is possible to create a cluster with 0 instances, as long as there are no APIs deployed to the cluster. When a cluster is first created, there are no APIs running on it. Once there is an API deployed, the cluster will scale to at least 1 instance and will remain at at least 1 instance until all APIs have been deleted.",possible create cluster long cluster cluster first running cluster scale least instance remain least instance,issue,negative,negative,neutral,neutral,negative,negative
750930581,"> @imagine3D-ai Here is an HTML file I created a while back to test CORS, and everything was working as expected. Does this work for you, and if so, are you able to borrow any of this code to get it working for you?
> 
> ```
> <!DOCTYPE html>
> <html lang=""en"">
> 
> <head>
>   <meta charset=""utf-8"">
>   <title>Cortex CORS Test</title>
> 
>   <script>
>     var HttpClient = function () {
>       this.get = function (url, callback) {
>         var xhr = new XMLHttpRequest();
>         xhr.open(""GET"", url, true);
>         xhr.onreadystatechange = function () {
>           if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
>             callback(xhr.responseText);
>         }
>         xhr.send(null);
>       }
>       this.post = function (url, payload, callback) {
>         var xhr = new XMLHttpRequest();
>         xhr.open(""POST"", url, true);
>         xhr.setRequestHeader(""Content-Type"", ""application/json"");
>         xhr.setRequestHeader(""X-Custom-Header1"", ""hi"");
>         xhr.onreadystatechange = function () {
>           if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
>             callback(xhr.responseText);
>         }
>         xhr.send(payload);
>       }
>     }
>     var client = new HttpClient();
>     function predict() {
>       document.getElementById(""prediction"").innerHTML = """";
>       var endpoint = document.getElementById(""endpoint"").value
>       var payload = document.getElementById(""payload"").value
>       client.post(endpoint, payload, function (response) {
>         document.getElementById(""prediction"").innerHTML = response;
>       });
>     }
>     function clearPrediction() {
>       document.getElementById(""prediction"").innerHTML = """";
>     }
>   </script>
> </head>
> 
> <body id=""home"">
>   <div id=main>
>     <div>API endpoint:</div>
>     <div><input type=""text"" id=""endpoint"" size=""120"" style=""margin-bottom: 1em;""></div>
>     <div>JSON payload:</div>
>     <div><textarea id=""payload"" rows=""20"" cols=""120"" style=""margin-bottom: 1em;""></textarea></div>
>     <div><button type=""button"" onclick=""predict()"" style=""margin-bottom: 1em;margin-right: 0.5em;"">Predict</button>
>       <button type=""button"" onclick=""clearPrediction()"" style=""margin-bottom: 1em;"">Clear</button></div>
>     <div id=prediction></div>
>   </div>
> </body>
> 
> </html>
> ```

I tried running this, but nothing happened when I pressed 'predict' button.
After checking logs using `cortex logs enpoint --env aws` I got this message:
`2020-12-24 17:14:02.961627:cortex:pid-240:INFO:200 OK POST /predict`
`2020-12-24 17:15:07.619385:cortex:pid-240:INFO:405 Method Not Allowed OPTIONS /predict`

The first OK POST message was from when I called api using `curl `in the console.
Second `405 Not Allowed` was when I tried calling using JS.
What can be the cause of this?",file back test everything working work able borrow code get working en head meta title cortex test script function function new get true function null function new post true hi function client new function predict prediction function response prediction response function prediction body home div div div input text em div div em div button button predict em predict button button em clear div tried running nothing button cortex got message cortex post cortex method first post message curl console second tried calling cause,issue,positive,positive,positive,positive,positive,positive
750867132,"> @da-source You will be charged based on AWS's spot instance pricing. I'm not sure why the pricing doesn't match (since we call their `DescribeSpotPriceHistory` API), but Cortex doesn't incur any additional cost, you just pay AWS based on their prices.

I see. So I should consider only the pricing displayed on AWS page, instead of Cortex?",based spot instance sure match since call cortex incur additional cost pay based see consider displayed page instead cortex,issue,negative,positive,positive,positive,positive,positive
750866816,"> @da-source APIs can only be scaled down to a minimum of 1 replica. If you set `min_instances` to 0, in order to scale your cluster down to 0 instances, you will have to `cortex delete` your API.

So as I understand, it isn‚Äôt possible to create a cluster with 0 instances, but when I set min_instances to 0 in cluster.yaml, I was able to spin it up as usual. Was that a bug?",scaled minimum replica set order scale cluster cortex delete understand possible create cluster set able spin usual bug,issue,negative,positive,neutral,neutral,positive,positive
750716052,"Adding @RobertLucian who will know for sure, but but based on looking at the code, my best guess is that the structure of the input that's being passed in is not matching the input signature of your model. Perhaps try something like this would work:

```python
target, residual = self.client.predict({""data"": tf.convert_to_tensor(audio)})
```

Also (unrelated), you might not need to pass in a tensor, as the TensorFlow predictor can populate the tensor_proto directly from the numpy array, so maybe something like this:

```python
target, residual = self.client.predict({""data"": audio})
```
",know sure based looking code best guess structure input matching input signature model perhaps try something like would work python target residual data audio also unrelated might need pas tensor predictor populate directly array maybe something like python target residual data audio,issue,positive,positive,positive,positive,positive,positive
750512541,"@da-source You will be charged based on AWS's spot instance pricing. I'm not sure why the pricing doesn't match (since we call their `DescribeSpotPriceHistory` API), but Cortex doesn't incur any additional cost, you just pay AWS based on their prices.",based spot instance sure match since call cortex incur additional cost pay based,issue,negative,positive,positive,positive,positive,positive
750510114,"@da-source APIs can only be scaled down to a minimum of 1 replica. If you set `min_instances` to 0, in order to scale your cluster down to 0 instances, you will have to `cortex delete` your API.",scaled minimum replica set order scale cluster cortex delete,issue,negative,neutral,neutral,neutral,neutral,neutral
750508324,"@imagine3D-ai Here is an HTML file I created a while back to test CORS, and everything was working as expected. Does this work for you, and if so, are you able to borrow any of this code to get it working for you?

```html
<!DOCTYPE html>
<html lang=""en"">

<head>
  <meta charset=""utf-8"">
  <title>Cortex CORS Test</title>

  <script>
    var HttpClient = function () {
      this.get = function (url, callback) {
        var xhr = new XMLHttpRequest();
        xhr.open(""GET"", url, true);
        xhr.onreadystatechange = function () {
          if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
            callback(xhr.responseText);
        }
        xhr.send(null);
      }
      this.post = function (url, payload, callback) {
        var xhr = new XMLHttpRequest();
        xhr.open(""POST"", url, true);
        xhr.setRequestHeader(""Content-Type"", ""application/json"");
        xhr.setRequestHeader(""X-Custom-Header1"", ""hi"");
        xhr.onreadystatechange = function () {
          if (xhr.readyState === XMLHttpRequest.DONE && xhr.status === 200)
            callback(xhr.responseText);
        }
        xhr.send(payload);
      }
    }
    var client = new HttpClient();
    function predict() {
      document.getElementById(""prediction"").innerHTML = """";
      var endpoint = document.getElementById(""endpoint"").value
      var payload = document.getElementById(""payload"").value
      client.post(endpoint, payload, function (response) {
        document.getElementById(""prediction"").innerHTML = response;
      });
    }
    function clearPrediction() {
      document.getElementById(""prediction"").innerHTML = """";
    }
  </script>
</head>

<body id=""home"">
  <div id=main>
    <div>API endpoint:</div>
    <div><input type=""text"" id=""endpoint"" size=""120"" style=""margin-bottom: 1em;""></div>
    <div>JSON payload:</div>
    <div><textarea id=""payload"" rows=""20"" cols=""120"" style=""margin-bottom: 1em;""></textarea></div>
    <div><button type=""button"" onclick=""predict()"" style=""margin-bottom: 1em;margin-right: 0.5em;"">Predict</button>
      <button type=""button"" onclick=""clearPrediction()"" style=""margin-bottom: 1em;"">Clear</button></div>
    <div id=prediction></div>
  </div>
</body>

</html>
```",file back test everything working work able borrow code get working en head meta title cortex test script function function new get true function null function new post true hi function client new function predict prediction function response prediction response function prediction body home div div div input text em div div em div button button predict em predict button button em clear div,issue,positive,positive,positive,positive,positive,positive
750295738,"> > thanks. As I understand I need to set `window` to `600` to keep the instance from downscaling for 10 minutes, Is this correct?
> 
> that sounds about right

one more question: can the same be applied if i set the `min_instances` to 0?",thanks understand need set window keep instance correct right one question applied set,issue,negative,positive,positive,positive,positive,positive
750221873,"> thanks. As I understand I need to set `window` to `600` to keep the instance from downscaling for 10 minutes, Is this correct?

that sounds about right
",thanks understand need set window keep instance correct right,issue,negative,positive,positive,positive,positive,positive
750216318,"> > the link that you've provided seems to be no longer working.
> 
> this one should work fine: https://docs.cortex.dev/v/0.24/workloads/realtime/autoscaling
 
thanks. As I understand I need to set `window` to `600` to keep the instance from downscaling for 10 minutes, Is this correct?",link provided longer working one work fine thanks understand need set window keep instance correct,issue,positive,positive,positive,positive,positive,positive
750207631,"> the link that you've provided seems to be no longer working.

this one should work fine: https://docs.cortex.dev/v/0.24/workloads/realtime/autoscaling

",link provided longer working one work fine,issue,negative,positive,positive,positive,positive,positive
750203499,"> @da-source Assuming you are referring to Realtime APIs (rather than Batch APIs), and that you are referring to instances > 1 (rather than scale-to-zero), this can be configured in the [autoscaling configuration](https://docs.cortex.dev/workloads/realtime-api/autoscaling). Let me know if that's not what you were asking about!

the link that you've provided seems to be no longer working.

(sorry for a late reply) ",assuming rather batch rather configuration let know link provided longer working sorry late reply,issue,negative,negative,negative,negative,negative,negative
749632250,"@JanYunkai You can configure Cortex to use multiple workers with the `processes_per_replica` field.

Here are all of the [configuration options](https://docs.cortex.dev/workloads/realtime/configuration#python-predictor), and in your case, it might work like this:

```yaml
- name: product1
  kind: RealtimeAPI
  predictor:
    type: python
    path: predictor.py
    processes_per_replica: 6
  compute:
    cpu: 3
  networking:
    api_gateway: public
```",configure cortex use multiple field configuration case might work like name product kind predictor type python path compute public,issue,positive,positive,positive,positive,positive,positive
748790771,"Thanks for the kind words, I'm glad to hear you've enjoyed using Cortex!

We have [this guide](https://docs.cortex.dev/running-on-aws/vpc-peering) to help with VPC peering. Keep us posted on how it goes; it'd be nice to hear your thoughts once you get it set up (you can also connect with us on [gitter](https://gitter.im/cortexlabs/cortex) or email us at dev@cortex.dev). ",thanks kind glad hear cortex guide help keep u posted go nice hear get set also connect u u dev,issue,positive,positive,positive,positive,positive,positive
748757167,"We just deployed the cluster today, have not tried peering yet but will likely end up going that route. It does seem a bit inconvenient to have the one service sitting in its own VPC with the rest of the architecture in another. FWIW, Cortex is great and has otherwise made our lives easier so your hard work is much appreciated!",cluster today tried yet likely end going route seem bit inconvenient one service sitting rest architecture another cortex great otherwise made easier hard work much,issue,positive,positive,neutral,neutral,positive,positive
748754555,"@josephlevenson that makes sense. Have you tried to set up peering, without success? Or did you get it working, but it's inconvenient?",sense tried set without success get working inconvenient,issue,negative,negative,negative,negative,negative,negative
748747725,"@deliahu Understood. Our use case is pretty simple, we currently have a thin microservice architecture in a single VPC. The cortex cluster interfaces with resources (DB/caching layer that will be used by other services). It would be nice to have our predict service encapsulated within the same VPC without having to set up peering. Does that make sense?",understood use case pretty simple currently thin architecture single cortex cluster layer used would nice predict service within without set make sense,issue,positive,positive,neutral,neutral,positive,positive
748743595,"@MDLGH48 Thanks for the kind words. It is not possible to run the operator on a smaller instance type, since there are multiple services that run on the instance, which in aggregate require >2 GB of memory.

In case it's relevant, here is a guide that we created for [keeping cluster costs down](https://docs.cortex.dev/v/0.23/guides/low-cost-clusters). Let us know if you have any questions!",thanks kind possible run operator smaller instance type since multiple run instance aggregate require memory case relevant guide keeping cluster let u know,issue,positive,positive,positive,positive,positive,positive
748741742,"@josephlevenson we have been considering this. Our hesitation is that Cortex resources will be mixed with your existing resources in the same network, which can create confusion and has the risk of having side-effects on your existing infra. Keeping them separate allows for a better separation of concerns, and a ""clean"" environment that Cortex is guaranteed to be able to configure properly. What is your motivation for deploying Cortex in an existing VPC?",considering hesitation cortex mixed network create confusion risk infra keeping separate better separation clean environment cortex able configure properly motivation cortex,issue,positive,positive,positive,positive,positive,positive
748173999,Thanks for following up. Keep us posted!,thanks following keep u posted,issue,negative,positive,neutral,neutral,positive,positive
747885573,"@vishalbollu Hello. During the investigation, my team tried to reproduce this issue that happens occasionally but it wasn't possible. So I just updated my team's cortex version to 0.23. I cannot sure the issue is resolved because of the update, but the occasional job termination is not occuring anymore after using 0.23 version. So I think you may close this issue. I'm so grateful for your help.",hello investigation team tried reproduce issue occasionally possible team cortex version sure issue resolved update occasional job termination version think may close issue grateful help,issue,positive,positive,positive,positive,positive,positive
747522407,"@nellaG I have tried to reproduce the issue. I ran this script that submitted 576 jobs in total every 10 seconds. 573 jobs completed successfully. 3 jobs failed, 2 jobs failed because a queue was unable to be created and 1 failed because a log stream was not found. 3 failed jobs out of 576 submitted jobs is drastically different from your scenario with 84 failed requests from 576 submitted jobs 

Is there any other information you can provide to help me set up a test that will reproduce your situation? My test script is below:

```
import requests
import time

items = list(range(300))

job_ids = []

# submit 576 jobs with a 10 second delay between each submission
for i in range(576):
    job = {
        ""workers"": 1,
        ""item_list"": {""items"": items, ""batch_size"": 60},
        ""config"": {""dest_s3_dir"": ""<S3 DIR>""},
    }

    response = requests.post(""http://endpoint"", json=job)

    json_response = response.json()
    job_ids.append(json_response[""job_id""])
    time.sleep(10)


time.sleep(180) # wait for jobs to complete

# verify job status after each one
for job_id in job_ids:
    status = requests.get(f""http://endpoint?jobID={job_id}"").json()[""job_status""][""status""]

    print(f""job id: {job_id}  status: {status}"")
    time.sleep(0.2)
```",tried reproduce issue ran script total every successfully queue unable log stream found drastically different scenario information provide help set test reproduce situation test script import import time list range submit second delay submission range job response wait complete verify job status one status print job id status status,issue,positive,positive,neutral,neutral,positive,positive
747363808,"@vishalbollu addressed your comments and tested again.

let's sync live regarding the opening of the issue to investigate performance",tested let sync live regarding opening issue investigate performance,issue,negative,positive,positive,positive,positive,positive
746767272,"> @mutal I think I have found the issue! It has to do with the compute requirements of your program.
> 
> It seems that the colab notebook has 13GB of memory. However, the t3.medium that I was deploying to has only 4GB of memory. When I ran it with cortex v0.24 and `m5.xlarge` (which has 16GB of memory), and used the following api compute configuration, it responded in 3 seconds.
> 
> ```yaml
> # cortex.yaml
> 
> - name: test
>   kind: RealtimeAPI
>   ...
>   compute:
>     mem: 13G
>     cpu: 2
> ```
> 
> I ran with `m5.xlarge` and set memory to 13GB and cpu to 2 just to match Colab. You may be able to get it to work on an 8 GB instance (you would have to set your compute request to a little less, e.g. 7GB, because of the system overhead, which you can check with `cortex cluster info`).
> 
> Let me know if that works for you.

That solved it! Thank you so much!",think found issue compute program notebook memory however memory ran cortex memory used following compute configuration name test kind compute mem ran set memory match may able get work instance would set compute request little le system overhead check cortex cluster let know work thank much,issue,positive,positive,positive,positive,positive,positive
745620408,"@mutal I think I have found the issue! It has to do with the compute requirements of your program.

It seems that the colab notebook has 13GB of memory. However, the t3.medium that I was deploying to has only 4GB of memory. When I ran it with cortex v0.24 and `m5.xlarge` (which has 16GB of memory), and used the following api compute configuration, it responded in 3 seconds.

```yaml
# cortex.yaml

- name: test
  kind: RealtimeAPI
  ...
  compute:
    mem: 13G
    cpu: 2
```

I ran with `m5.xlarge` and set memory to 13GB and cpu to 2 just to match Colab. You may be able to get it to work on an 8 GB instance (you would have to set your compute request to a little less, e.g. 7GB, because of the system overhead, which you can check with `cortex cluster info`).

Let me know if that works for you.",think found issue compute program notebook memory however memory ran cortex memory used following compute configuration name test kind compute mem ran set memory match may able get work instance would set compute request little le system overhead check cortex cluster let know work,issue,positive,positive,positive,positive,positive,positive
745367709,"@wangzhao765 I believe you intended to file this issue at https://github.com/cortexproject/cortex/issues, so I'll go ahead and close this issue here",believe intended file issue go ahead close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
745104861,"Great!

I can‚Äôt reproduce the fast scenario on 0.20 for some strange reason, but on
11th November (according to my own notes), I‚Äôve tried running the code on
c5.2xlarge and c5.large, and in both cases init time was about 5 secs. I‚Äôm
pretty sure I was using Cortex 0.20 back then, because I haven‚Äôt updated it
since I‚Äôve installed it in September.

On Tue, 15 Dec 2020 at 05:14, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> I was able to successfully run your
> predictor, thanks for providing it!
>
> However, my results are different from yours:
>
>    - On colab, I get time=6.4 word/s and it takes 3-4 seconds
>    - On cortex v0.20 and on cortex v0.24 (both running with the aws
>    provider and t3.medium), I get time=6.9 word/s and it takes 13-14
>    seconds
>
> I would like to figure out why it takes longer on cortex. I think the
> first step is to try to reproduce the scenario where it is fast on cortex:
> can you confirm that it only takes 3-4 seconds for you when running on
> cortex v0.20? Here are the installation docs for v0.20
> <https://docs.cortex.dev/v/0.20/install>.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1500#issuecomment-745040417>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXITVH67EZL5I6XZUEELSU3PAVANCNFSM4TCIV5QA>
> .
>
",great reproduce fast scenario strange reason th according tried running code time pretty sure cortex back since tue wrote able successfully run predictor thanks providing however different get cortex cortex running provider get would like figure longer cortex think first step try reproduce scenario fast cortex confirm running cortex installation reply directly view,issue,positive,positive,positive,positive,positive,positive
745040417,"@mutal I was able to successfully run your predictor, thanks for providing it!

However, my results are different from yours:

* On colab, I get `time=6.4 word/s` and it takes 3-4 seconds
* On cortex v0.20 and on cortex v0.24 (both running with the `aws` provider and `t3.medium`), I get `time=6.9 word/s` and it takes 13-14 seconds

I would like to figure out why it takes longer on cortex. I think the first step is to try to reproduce the scenario where it is fast on cortex: can you confirm that it only takes 3-4 seconds for you when running on cortex v0.20? Here are the [installation docs for v0.20](https://docs.cortex.dev/v/0.20/install).",able successfully run predictor thanks providing however different get cortex cortex running provider get would like figure longer cortex think first step try reproduce scenario fast cortex confirm running cortex installation,issue,positive,positive,positive,positive,positive,positive
744674697,"> Thanks, I'll give it a try soon and let you know if I can reproduce it.
> 
> When you noticed the slowdown, were you running in a cluster, or in the Cortex local provider? If it was the local provider, was it running on your personal computer or a cloud instance (and if so which instance type)? If it was running on a cluster, which instance type were you using?
> 
> I'm just asking so I can be sure to try it with the exact same setup you had.

Slow-down appeared when I was running a cluster, using either t3.medium, c5.large or c5.2xlarge instances",thanks give try soon let know reproduce slowdown running cluster cortex local provider local provider running personal computer cloud instance instance type running cluster instance type sure try exact setup running cluster either,issue,positive,positive,positive,positive,positive,positive
744567612,"Thanks, I'll give it a try soon and let you know if I can reproduce it.

When you noticed the slowdown, were you running in a cluster, or in the Cortex local provider? If it was the local provider, was it running on your personal computer or a cloud instance (and if so which instance type)? If it was running on a cluster, which instance type were you using?

I'm just asking so I can be sure to try it with the exact same setup you had.",thanks give try soon let know reproduce slowdown running cluster cortex local provider local provider running personal computer cloud instance instance type running cluster instance type sure try exact setup,issue,positive,positive,positive,positive,positive,positive
744470644,"> @mutal Are you able to reproduce the slow init time with a command that just runs `echo`? Would you be able to send the exact code that you are running that reproduces the slow init time?

`echo` command doesn't open/run an external .bin program (like the one I use) and that's why I can't reproduce the slow init time with it. 

Of course, here's my exact code:


```
import subprocess
import os
import boto3
import sys
import stat

class PythonPredictor:
    def __init__(self, config):
        s3 = boto3.client('s3', aws_access_key_id='@',
        aws_secret_access_key='@')
        s3.download_file(config['bucket'], config['key'], ""./gpt2_774M.bin"")
        st = os.stat('./gpt2tc')
        os.chmod('./gpt2tc', st.st_mode | stat.S_IEXEC ) # this should be the equivalent to chmod +x ./mymodel and make your model.bin executable
        print(""done"")

    def predict(self):
        command = ""./gpt2tc -m 774M -l 1 g hi""
        result = os.popen(command)
        return result.read()
```

If it would be helpful,  I've created the [notebook](https://colab.research.google.com/drive/1MowDajVnaylb8VT72ZFhvH9KCPZa0dRG?usp=sharing) that fully reproduces the issue in Colab environment (with normal, 6 second startup-time) and also the required [files](https://drive.google.com/drive/folders/1yQI3vNw8X3MoNmdR-Vz6hgiaO-y4trK_?usp=sharing) to do it. Hope, this could be helpful.

",able reproduce slow time command echo would able send exact code running slow time echo command external program like one use ca reproduce slow time course exact code import import o import import import class self st equivalent make executable print done predict self command hi result command return would helpful notebook fully issue environment normal second also hope could helpful,issue,positive,positive,neutral,neutral,positive,positive
744077677,@mutal Are you able to reproduce the slow init time with a command that just runs `echo`? Would you be able to send the exact code that you are running that reproduces the slow init time?,able reproduce slow time command echo would able send exact code running slow time,issue,negative,positive,positive,positive,positive,positive
744019253,"> @mutal I can definitely look into this! Do you remember if you were using a different version of Cortex a few weeks ago? If so, do you mind spinning up a cluster on the version of Cortex you were using before, with the same API and cluster configuration, and checking if it still takes 6 seconds? (you can see old documentation on docs.cortex.dev by changing the version in the sidebar)

Thanks, please do if you can! A few weeks ago, I think I was using Cortex 0.20 (I didn't update it since I first installed it). I've tried running my code on Cortex 0.20, where I got the same long-startup-time (about 16 seconds). 
It's really weird that `c5.large`  which I think is equally if not more powerful instance than machines that Google Colab CPU uses, takes more than twice as long to launch my program. I remember that even `t3.medium` could launch it equally as fast as Colab CPU :/

*Correction: I've been using 0.20 at first and not 0.19*
",definitely look remember different version cortex ago mind spinning cluster version cortex cluster configuration still see old documentation version thanks please ago think cortex update since first tried running code cortex got really weird think equally powerful instance twice long launch program remember even could launch equally fast correction first,issue,positive,positive,neutral,neutral,positive,positive
743948181,"@da-source Assuming you are referring to Realtime APIs (rather than Batch APIs), and that you are referring to instances > 1 (rather than scale-to-zero), this can be configured in the [autoscaling configuration](https://docs.cortex.dev/workloads/realtime-api/autoscaling). Let me know if that's not what you were asking about!",assuming rather batch rather configuration let know,issue,negative,neutral,neutral,neutral,neutral,neutral
743665263,"@lefnire Sounds good, feel free to reach out on our [gitter](https://gitter.im/cortexlabs/cortex) if you have any questions!",good feel free reach,issue,positive,positive,positive,positive,positive,positive
743526589,"> In the meantime, in case it's helpful, it is possible to create/delete APIs programmatically via the Cortex CLI or Python client. So if you know you are expecting traffic, or it happens on a regular schedule, you could create/delete APIs accordingly.

That is indeed helpful. With how valuable Cortex is, any leg-work on our part is worth it for this use-case.

I'll look into the batch jobs. I'm currently using AWS Batch anyway, where the container `exit(0)`s when it hasn't received traffic for a while. So it's a hybrid of real batch jobs vs hosted model, which I might be able to pull off with Cortex batch jobs. AWS Batch has given me ulcers, so if Cortex works well even just replacing Batch could be worth it. 

Thanks for the suggestions! I'll keep subscribed to this issue in case scale->0 ever comes.",case helpful possible programmatically via cortex python client know traffic regular schedule could accordingly indeed helpful valuable cortex part worth look batch currently batch anyway container exit received traffic hybrid real batch model might able pull cortex batch batch given cortex work well even batch could worth thanks keep issue case ever come,issue,positive,positive,positive,positive,positive,positive
743493714,"@lefnire Thanks for reaching out; yes, that makes sense, and is exactly how we'd implement it!

We haven't decided yet on our priority for implementing this feature. One thing that can render it less useful (or at least ""awkward"") is how long it takes to spin up a GPU instance and install the dependencies on it; we'd have to hold on to the request for 5+ minutes before forwarding it along. A more intuitive approach might be to support an asynchronous API instead, where you make the API request and it responds immediately with an execution ID, and then you can make an additional request to another API to query the status/results for the execution ID (we have https://github.com/cortexlabs/cortex/issues/1610 to track this).

In the meantime, in case it's helpful, it is possible to create/delete APIs programmatically via the Cortex CLI or Python client. So if you know you are expecting traffic, or it happens on a regular schedule, you could create/delete APIs accordingly.

Also, we do currently support batch jobs, which is a bit like the asynchronous approach I described, except that autoscaling behaves differently: for batch jobs, you submit a job and indicate how many containers you want to run it on, and then once the job is done, the containers spin down. So it does ""scale to 0"", but is not designed to handle real-time traffic where each individual request is fairly lightweight, and can come at any time from any source.",thanks reaching yes sense exactly implement decided yet priority feature one thing render le useful least awkward long spin instance install hold request forwarding along intuitive approach might support asynchronous instead make request immediately execution id make additional request another query execution id track case helpful possible programmatically via cortex python client know traffic regular schedule could accordingly also currently support batch bit like asynchronous approach except differently batch submit job indicate many want run job done spin scale designed handle traffic individual request fairly lightweight come time source,issue,positive,positive,neutral,neutral,positive,positive
743433417,"To be clear, it's the unused running GPU instance we're concerned about. If it's the case that _something_ has to run, that's fine. Eg, a nano/micro that serves as the orchestrator and must always be on - if that makes anything easier. (Forgive my Cortex newbishness if that's kinda how it already works)",clear unused running instance concerned case run fine orchestrator must always anything easier forgive cortex already work,issue,positive,positive,positive,positive,positive,positive
743347457,"@mutal I can definitely look into this! Do you remember if you were using a different version of Cortex a few weeks ago? If so, do you mind spinning up a cluster on the version of Cortex you were using before, with the same API and cluster configuration, and checking if it still takes 6 seconds? (you can see old documentation on docs.cortex.dev by changing the version in the sidebar)",definitely look remember different version cortex ago mind spinning cluster version cortex cluster configuration still see old documentation version,issue,negative,positive,neutral,neutral,positive,positive
743189197,"@deliahu Thanks! 

The `echo`  livestream example that you've provided has instant response time.
I tried running my own command (generating only a single word) with and without livestream and in both cases, there was 15 seconds delay, before getting response. This delay is due to the command launching my program and this 15 seconds delay is really strange because it never took more than 6 seconds to do it before. When running my command locally or in the Colab, it takes about 6 seconds max (sometimes less), to launch my program. In fact, Cortex API also used to respond within 6 seconds when running my command (without livestream), just 3 weeks ago, using the same instances (`c5.large` or `t3.medium`). So the local, Colab, and Cortex-3-weeks-go program-launch times where 6 seconds long and this sudden pro-longed delay quite weird. 

I tried changing my `cortex.yaml` as it was before, using the the default image and tried running `predictor.py` with my command, without the livestream, but the response time is still 15 secs., when running on `c5.large`.

Here's the old  `cortex.yaml` and `predictor.py` with which I used to get normal program launch times, but can't anymore:

`cortex.yaml`

```
- name: gpt2
  kind: RealtimeAPI
  predictor:
    type: python
    path: predictor.py
    config:
      bucket: 'mybucket'
      key: 'mykey'
  compute:
    cpu: 1.2
    mem: 1.8G

```


`predictor.py`

```
class PythonPredictor:
    def __init__(self, config):
        s3 = boto3.client('s3', aws_access_key_id='key',
        aws_secret_access_key='key')
        s3.download_file(config['bucket'], config['key'], ""./gpt2_774M.bin"")

    def predict(self, payload):
        st = os.stat('./gpt2tc')
        os.chmod('./gpt2tc', st.st_mode | stat.S_IEXEC ) 
        command = ""./gpt2tc -m 774M -l 1 g Something ')'""
        result = os.popen(command)
        return result.read()

```",thanks echo example provided instant response time tried running command generating single word without delay getting response delay due command program delay really strange never took running command locally sometimes le launch program fact cortex also used respond within running command without ago local cortex time long sudden delay quite weird tried default image tried running command without response time still running old used get normal program launch time ca name kind predictor type python path bucket key compute mem class self predict self st command something result command return,issue,positive,positive,neutral,neutral,positive,positive
742980215,"@mutal I'm glad you got it working!

I don't fully understand what is taking long, and what used to be faster. Can you isolate the exact part that is taking longer than you expect? E.g. if you replace your program with a bash command that is instant (e.g. a single `echo`), does it take a long time? If instead of streaming the output, you generate a single word and return it, it takes a long time? If you can include your predictor code for the case that is fast and the case that is slow, it could be helpful (in addition to trying to isolate the part that is adding to the delay)",glad got working fully understand taking long used faster isolate exact part taking longer expect replace program bash command instant single echo take long time instead streaming output generate single word return long time include predictor code case fast case slow could helpful addition trying isolate part delay,issue,negative,positive,neutral,neutral,positive,positive
742270434,"@deliahu David, can something be done to make `curl API ` response faster?",something done make curl response faster,issue,negative,neutral,neutral,neutral,neutral,neutral
740363350,Definitely interested in this. Would make Cortext a no-brainer for ML projects which don't yet have users enough to need 1 consistent GPU. Then we'd benefit from the rest of the scaling/infra when the users _do_ come.,definitely interested would make yet enough need consistent benefit rest come,issue,positive,positive,positive,positive,positive,positive
740022350,"> @mutal When I update my code to look like this, it behaves as expected, returning one character at a time:
> 
> ```python
> import asyncio
> from sse_starlette.sse import EventSourceResponse
> 
> 
> async def run_command():
>     command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""
> 
>     proc = await asyncio.create_subprocess_shell(
>         command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
>     )
> 
>     while True:
>         output = await proc.stdout.read(1)
>         if not output:
>             return
>         yield output.decode()
> 
> 
> class PythonPredictor:
>     def __init__(self, config):
>         pass
> 
>     async def predict(self, payload):
>         return EventSourceResponse(run_command())
> ```
> 
> Let me know if that works for you!

Got it working, awesome! The program's startup time is so much longer than expected though. Without the livestreaming with `starlette` it took about 4 seconds for the `cortex api` to launch my program and get the first word of the output, but with livestreaming, the `Cortex api` is now taking about 15 seconds to launch my program and respond for the first time. Is there way to speed-up livestream launch?",update code look like one character time python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return let know work got working awesome program time much longer though without took cortex launch program get first word output cortex taking launch program respond first time way launch,issue,positive,positive,positive,positive,positive,positive
739687040,"@vishalbollu No, we haven't used Realtime API yet. I will consider your suggestion while the investigation. Thanks for your help. ",used yet consider suggestion investigation thanks help,issue,positive,positive,positive,positive,positive,positive
739391495,"@mutal When I update my code to look like this, it behaves as expected, returning one character at a time:

```python
import asyncio
from sse_starlette.sse import EventSourceResponse


async def run_command():
    command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""

    proc = await asyncio.create_subprocess_shell(
        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )

    while True:
        output = await proc.stdout.read(1)
        if not output:
            return
        yield output.decode()


class PythonPredictor:
    def __init__(self, config):
        pass

    async def predict(self, payload):
        return EventSourceResponse(run_command())
```

Let me know if that works for you!",update code look like one character time python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return let know work,issue,positive,positive,positive,positive,positive,positive
739147307,"> @deliahu Also I was hoping to troubleshoot the code in Google Colab but I'm getting `ModuleNotFound` error even after installing starlette using `!pip install sse-starlette`. Without Colab I'm having to deploy the cluster (which is costing me money) and then redeploy API (which takes time) each time I want to debug/rerun the code :/

Turns out it was a simple mistake, `starlette` needed to be installed manually. I guess Cortex already had it, so that's why there was need to pass it to `requirement.txt`",also code getting error even pip install without deploy cluster costing money redeploy time time want code turn simple mistake manually guess cortex already need pas,issue,negative,neutral,neutral,neutral,neutral,neutral
738936179,@deliahu Also I was hoping to troubleshoot the code in Google Colab but I'm getting `ModuleNotFound` error even after installing starlette using `!pip install sse-starlette`. Without Colab I'm having to deploy the cluster (which is costing me money) and then redeploy API (which takes time) each time I want to debug/rerun the code :/ ,also code getting error even pip install without deploy cluster costing money redeploy time time want code,issue,negative,neutral,neutral,neutral,neutral,neutral
738934009,"> @mutal It should be possible to send one character at a time. Whenever you call `yield`, like in the example that I sent, it should send the provided value to the client. So if you call `yield` with each character one at a time, it should work as expected.
> 
> I am not very familiar with Server-Sent Events, but it seems to me like `data` is something that `curl` is adding to the response. Perhaps there is a flag you can pass in to `curl` to prevent it from printing, or you might have to do post-processing on the results. Even better would be to use a different client; after googling I found [this one](https://pypi.org/project/sseclient/) which seems promising.

I'm getting:

```
data: 2020-12-04 17:57:42.646753

data: M

curl: (18) transfer closed with outstanding read data remaining
```
when using this function that I adapted from yours:

```
async def run_command():
    proc = await asyncio.create_subprocess_shell(
        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )

    while True:
        output = await proc.stdout.read(1)
        if not output:
            return
        while output != b'':  
            print(output.decode('UTF-8','ignore'), end='', flush=True)
            yield output.decode('UTF-8','ignore')
            output = p.stdout.read(1)
```

Why could this be happening?",possible send one character time whenever call yield like example sent send provided value client call yield character one time work familiar like data something curl response perhaps flag pas curl prevent printing might even better would use different client found one promising getting data data curl transfer closed outstanding read data function await command true output await output return output print yield output could happening,issue,positive,positive,positive,positive,positive,positive
738921826,"Thanks for providing the information. 

While we investigate this issue, have you considered using the Realtime API rather than the Batch API? 

The use case you've described above seems involve to frequents requests with each request containing a relatively small payload (300 images). The Realtime API may be better at handling this kind of traffic when compared the the Batch API.",thanks providing information investigate issue considered rather batch use case involve request relatively small may better handling kind traffic batch,issue,positive,positive,positive,positive,positive,positive
738612986,"@vishalbollu 

Yes. It happens occasionally as you said, Here's our experimental condition for submitting jobs:

```

* 576 jobs are requested within 2h 40m.
* Each request processes 300 images (`batch_size` : 60)
* 492 requests succeeded, 84 requests are failed


```",yes occasionally said experimental condition within request,issue,negative,positive,neutral,neutral,positive,positive
738214126,"@wise-east we just merged https://github.com/cortexlabs/cortex/pull/1649, which should fix the CORS configuration.

I'd be happy to build a version of the API image for v0.23 for you if you'd like to switch back to v0.23, just let me know! Otherwise, we plan on releasing v0.24 next week, which will include this fix.",fix configuration happy build version image like switch back let know otherwise plan next week include fix,issue,positive,positive,positive,positive,positive,positive
737973412,@wise-east can you tell us if you get a log about the denied request in your API's logs (using `cortex logs <api-name>`)? Or does it go unnoticed?,tell u get log request cortex go unnoticed,issue,negative,negative,negative,negative,negative,negative
737676269,"@wise-east I was able to find a temporary workaround if you are interested in using v0.23 in the meantime as we work on re-enabling CORS. If you follow the guide that you linked to for setting up [REST API Gateway](https://docs.cortex.dev/running-cortex-on-aws/rest-api-gateway#if-your-api-load-balancer-is-internet-facing), it will work as long as you make one modification: between step 5 and step 6, click on ""Actions"" > ""Enable CORS"", then update ""Access-Control-Allow-Headers"" to `'*'`, and click ""Enable CORS and replace existing CORS headers""

![step 1](https://user-images.githubusercontent.com/808475/100968130-a0829680-34e5-11eb-94f0-6e0d59c7f8eb.png)

![step 2](https://user-images.githubusercontent.com/808475/100968154-a7a9a480-34e5-11eb-8e84-3b8229f8f3d8.png)",able find temporary interested work follow guide linked setting rest gateway work long make one modification step step click enable update click enable replace step step,issue,negative,positive,positive,positive,positive,positive
737669197,"@wise-east I was able to reproduce the CORS error just now on v0.23. We made a few changes to the networking stack in v0.22, so I suspect that is the culprit. We'll look into it and keep you posted, thanks for letting us know!",able reproduce error made stack suspect culprit look keep posted thanks u know,issue,negative,positive,positive,positive,positive,positive
737569239,"is this still done automatically with cortex v0.23? I just deployed my API again with v0.23 and I'm getting: 

> Access to XMLHttpRequest at 'https://hreakcof35.execute-api.us-east-1.amazonaws.com/ya-bot' from origin 'https://spolin.isi.edu' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: It does not have HTTP ok status.

This used to not be a problem when I was using v0.21. I followed the steps in the guide above, but the issue isn't resolved. 

Do I have to follow the instructions here to enable CORS? https://docs.cortex.dev/running-cortex-on-aws/rest-api-gateway#if-your-api-load-balancer-is-internet-facing",still done automatically cortex getting access origin blocked policy response preflight request pas access control check status used problem guide issue resolved follow enable,issue,negative,neutral,neutral,neutral,neutral,neutral
737289935,"It sounds like this happens occasionally, is this correct? If this information isn't reliable reproducible, the following information may help us recreate this issue:

- how frequently are the jobs being submitted every hour?
- out of the jobs being submitted, how may run into this issue?",like occasionally correct information reliable reproducible following information may help u recreate issue frequently every hour may run issue,issue,positive,positive,neutral,neutral,positive,positive
736870536,"@mutal It should be possible to send one character at a time. Whenever you call `yield`, like in the example that I sent, it should send the provided value to the client. So if you call `yield` with each character one at a time, it should work as expected.

I am not very familiar with Server-Sent Events, but it seems to me like `data` is something that `curl` is adding to the response. Perhaps there is a flag you can pass in to `curl` to prevent it from printing, or you might have to do post-processing on the results. Even better would be to use a different client; after googling I found [this one](https://pypi.org/project/sseclient/) which seems promising.",possible send one character time whenever call yield like example sent send provided value client call yield character one time work familiar like data something curl response perhaps flag pas curl prevent printing might even better would use different client found one promising,issue,positive,positive,positive,positive,positive,positive
736255821,@deliahu Is there a way to format the `curl` output so that instead of `data:x` it will give just `x`? And can something be done to eliminate `event:ping` messages?,way format curl output instead data give something done eliminate event ping,issue,negative,neutral,neutral,neutral,neutral,neutral
735872357,"> @mutal I just realized that Server-Sent Events is not supported by API Gateway, but is supported by the load balancer.
> 
> Therefore, if you add `api_gateway: none` to your API configuration, then it should work. For example:
> 
> ```yaml
> - name: test
>   kind: RealtimeAPI
>   predictor:
>     type: python
>     image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
>     path: predictor.py
>   networking:
>     api_gateway: none
> ```
> 
> When you deploy with this config and then run `cortex get test`, it will print the new API endpoint.
> 
> Let me know if that works!

That fixed it! Your test command now outputs in realtime just fine. When running my own command I sometimes get `event:ping` instead of `data:desired output`, for example:

```
curl myendpoint-X POST
event: ping
data: 2020-11-30 15:39:29.870888

data: SOME TEXT

event: ping
data: 2020-11-30 15:39:44.879364

event: ping
data: 2020-11-30 15:39:59.880063

event: ping
data: 2020-11-30 15:40:14.880098

data: SOME TEXT

event: ping
data: 2020-11-30 15:40:29.880243
```

My guess is this is happening because there are no `readline()`, `\n` parts in output text which `run_command()` is looking for, so it's just sitting and waiting for it in meantime. So I found  a way to make my code print out 1 character at a time instead of 1 line:

```
import subprocess
def predict():
      command = ['./gpt2tc', '-k', '1000', '-m', '117M', '-l', '700', 'g', 'hi']
      with subprocess.Popen(command, stdout=subprocess.PIPE, bufsize=-1) as p: 
        char = p.stdout.read(1)
        while char != b'':  
            print(char.decode('UTF-8','ignore'), end='', flush=True)
            char = p.stdout.read(1)
```

This won't use readline() and instead will output each new character. But I can't manage to translate this into `async` method like in your predictor. Can this be done?
",gateway load balancer therefore add none configuration work example name test kind predictor type python image path none deploy run cortex get test print new let know work fixed test command fine running command sometimes get event ping instead data desired output example curl post event ping data data text event ping data event ping data event ping data data text event ping data guess happening output text looking sitting waiting found way make code print character time instead line import predict command command char char print char wo use instead output new character ca manage translate method like predictor done,issue,positive,positive,positive,positive,positive,positive
735461138,"@mutal I just realized that Server-Sent Events is not supported by API Gateway, but is supported by the load balancer.

Therefore, if you add `api_gateway: none` to your API configuration, then it should work. For example:

```yaml
- name: test
  kind: RealtimeAPI
  predictor:
    type: python
    image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
    path: predictor.py
  networking:
    api_gateway: none
```

When you deploy with this config and then run `cortex get test`, it will print the new API endpoint.

Let me know if that works!",gateway load balancer therefore add none configuration work example name test kind predictor type python image path none deploy run cortex get test print new let know work,issue,positive,positive,positive,positive,positive,positive
735344668,"Yes, I‚Äôm using curl to make the request and when using your command
variable, the whole output (test1,test2,test3,done) is shown at once after
a few seconds.

On Sat, 28 Nov 2020 at 20:57, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> Are you using curl to make the request
> to your API? Also, if you change the command variable to the one I sent
> in my example, does it properly stream the output incrementally?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1500#issuecomment-735283445>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIQ6LAHEKXRCIXXGTVDSSFI25ANCNFSM4TCIV5QA>
> .
>
",yes curl make request command variable whole output test test test done shown sat wrote curl make request also change command variable one sent example properly stream output reply directly view,issue,negative,positive,positive,positive,positive,positive
735283445,"@mutal Are you using `curl` to make the request to your API? Also, if you change the `command` variable to the one I sent in my example, does it properly stream the output incrementally?",curl make request also change command variable one sent example properly stream output,issue,negative,neutral,neutral,neutral,neutral,neutral
735255730,"> @mutal I think I have discovered an approach that will work for you.
> 
> This example that works for me:
> 
> #### cortex.yaml
> ```yaml
> - name: test
>   kind: RealtimeAPI
>   predictor:
>     type: python
>     image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
>     path: predictor.py
> ```
> 
> #### predictor.py
> ```python
> import asyncio
> from sse_starlette.sse import EventSourceResponse
> 
> 
> async def run_command():
>     command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""
> 
>     proc = await asyncio.create_subprocess_shell(
>         command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
>     )
> 
>     while True:
>         output = await proc.stdout.readline()
>         if not output:
>             return
>         yield output.decode().strip()
> 
> 
> class PythonPredictor:
>     def __init__(self, config):
>         pass
> 
>     async def predict(self, payload):
>         return EventSourceResponse(run_command())
> ```
> 
> #### requirements.txt
> ```python
> sse-starlette
> ```
> 
> Note that I built a new version of the predictor image (specified in the `image` API configuration field), because it required a [small change](https://github.com/cortexlabs/cortex/pull/1565/files) to one of our source files. It should be straightforward for you to build a custom image with this change for yourself, depending on the version of cortex you want do use. The easiest way would be to use the official Cortex image as a base image in your Dockerfile, and then replace the file that needs to be updated using a `COPY` command. [Here](https://docs.cortex.dev/advanced/system-packages#custom-docker-image) are instructions for using a custom docker image.
> 
> Also note that with this approach, `threads_per_process` will not be enforced.
> 
> We are not ready to merge #1565, because there are some known consequences of this that we would need to address (e.g. `threads_per_process` not being enforced).
> 
> Let me know if that works for you!

@deliahu Your approach gives the desired output, but doesn't seem like it really streams it in realtime, but only after the whole output is processed, like when using the usual Cortex api method. Is this expected? Perhaps it is because Linux console can't show the stream, but only the finished output? I've provided my `predictor.py` if you would find it helpful.",think discovered approach work example work name test kind predictor type python image path python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return python note built new version predictor image image configuration field small change one source straightforward build custom image change depending version cortex want use easiest way would use official cortex image base image replace file need copy command custom docker image also note approach enforced ready merge known would need address enforced let know work approach desired output seem like really whole output like usual cortex method perhaps console ca show stream finished output provided would find helpful,issue,positive,positive,neutral,neutral,positive,positive
735253788,"> @mutal judging by what I see, that might be caused by an OOM (out-of-memory) error. You can check that by running `cortex get <api-name>` and see if it reports this error. The reason I'm leaning towards thinking this is an OOM error is that an OOM error usually kills a process right away, not giving it any chance of logging anything else. You can fix that by increasing the `predictor.mem`'s value.
> 
> That being said, there's a possibility that something else is causing this. Could you also show us the implementation of your predictor?

I've changed `predictor.mem`'s value accordingly and Cortex gives expected output, but not really as a stream? It seems that after the whole output is processed and it is only then it gets displayed, just like when using the usual api method. 

Of course, here's my predictor:

```
import asyncio
from sse_starlette.sse import EventSourceResponse
import os
import boto3
import sys
import stat


async def run_command():
    st = os.stat('./gpt2tc')
    os.chmod('./gpt2tc', st.st_mode | stat.S_IEXEC ) 
    print('done')
    command = ""./gpt2tc -m 117M -l 20 g hi""

    proc = await asyncio.create_subprocess_shell(
        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )

    while True:
        output = await proc.stdout.readline()
        if not output:
            return
        yield output.decode().strip()


class PythonPredictor:
    def __init__(self, config):
        s3 = boto3.client('s3', aws_access_key_id='key',
        aws_secret_access_key='s_key')
        s3.download_file(config['bucket'], config['key'], ""./gpt2_117M.bin"")

    async def predict(self, payload):
        return EventSourceResponse(run_command())
```

If it might be helpful, I'm using [this](https://bellard.org/nncp/gpt2tc-2020-07-25.tar.gz) program while prototyping this.",see might error check running cortex get see error reason leaning towards thinking error error usually process right away giving chance logging anything else fix increasing value said possibility something else causing could also show u implementation predictor value accordingly cortex output really stream whole output displayed like usual method course predictor import import import o import import import st print command hi await command true output await output return yield class self predict self return might helpful program,issue,positive,positive,neutral,neutral,positive,positive
735036252,"@rafaelsandroni Thanks for creating the issue.

If you used a cluster configuration file (e.g. `cluster.yaml`) when creating your cluster, do you mind sharing its contents? If you didn't use a cluster config file, do you mind sharing how you answered the CLI prompts?

Also, did you see any errors when creating your cluster? If you still have the output from your `cortex cluster up` command, do you mind sharing that too?

Lastly, if you try to spin up a new cluster (with a different name), does it work as expected?",thanks issue used cluster configuration file cluster mind content use cluster file mind also see cluster still output cortex cluster command mind lastly try spin new cluster different name work,issue,negative,positive,neutral,neutral,positive,positive
735035477,"@mutal judging by what I see, that might be caused by an OOM (out-of-memory) error. You can check that by running `cortex get <api-name>` and see if it reports this error. The reason I'm leaning towards thinking this is an OOM error is that an OOM error usually kills a process right away, not giving it any chance of logging anything else. You can fix that by increasing the `predictor.mem`'s value.

That being said, there's a possibility that something else is causing this. Could you also show us the implementation of your predictor?",see might error check running cortex get see error reason leaning towards thinking error error usually process right away giving chance logging anything else fix increasing value said possibility something else causing could also show u implementation predictor,issue,negative,positive,neutral,neutral,positive,positive
735016086,"Additionally, seems that the cortex dont created the api gateway, so, I've created an API and included the tag cortex.dev/cluster-name=cortex into api gateway. but error persists.",additionally cortex dont gateway included tag gateway error,issue,negative,neutral,neutral,neutral,neutral,neutral
734966215,"> @mutal I think I have discovered an approach that will work for you.
> 
> This example that works for me:
> 
> #### cortex.yaml
> ```yaml
> - name: test
>   kind: RealtimeAPI
>   predictor:
>     type: python
>     image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
>     path: predictor.py
> ```
> 
> #### predictor.py
> ```python
> import asyncio
> from sse_starlette.sse import EventSourceResponse
> 
> 
> async def run_command():
>     command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""
> 
>     proc = await asyncio.create_subprocess_shell(
>         command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
>     )
> 
>     while True:
>         output = await proc.stdout.readline()
>         if not output:
>             return
>         yield output.decode().strip()
> 
> 
> class PythonPredictor:
>     def __init__(self, config):
>         pass
> 
>     async def predict(self, payload):
>         return EventSourceResponse(run_command())
> ```
> 
> #### requirements.txt
> ```python
> sse-starlette
> ```
> 
> Note that I built a new version of the predictor image (specified in the `image` API configuration field), because it required a [small change](https://github.com/cortexlabs/cortex/pull/1565/files) to one of our source files. It should be straightforward for you to build a custom image with this change for yourself, depending on the version of cortex you want do use. The easiest way would be to use the official Cortex image as a base image in your Dockerfile, and then replace the file that needs to be updated using a `COPY` command. [Here](https://docs.cortex.dev/advanced/system-packages#custom-docker-image) are instructions for using a custom docker image.
> 
> Also note that with this approach, `threads_per_process` will not be enforced.
> 
> We are not ready to merge #1565, because there are some known consequences of this that we would need to address (e.g. `threads_per_process` not being enforced).
> 
> Let me know if that works for you!



> @mutal I think I have discovered an approach that will work for you.
> 
> This example that works for me:
> 
> #### cortex.yaml
> ```yaml
> - name: test
>   kind: RealtimeAPI
>   predictor:
>     type: python
>     image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
>     path: predictor.py
> ```
> 
> #### predictor.py
> ```python
> import asyncio
> from sse_starlette.sse import EventSourceResponse
> 
> 
> async def run_command():
>     command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""
> 
>     proc = await asyncio.create_subprocess_shell(
>         command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
>     )
> 
>     while True:
>         output = await proc.stdout.readline()
>         if not output:
>             return
>         yield output.decode().strip()
> 
> 
> class PythonPredictor:
>     def __init__(self, config):
>         pass
> 
>     async def predict(self, payload):
>         return EventSourceResponse(run_command())
> ```
> 
> #### requirements.txt
> ```python
> sse-starlette
> ```
> 
> Note that I built a new version of the predictor image (specified in the `image` API configuration field), because it required a [small change](https://github.com/cortexlabs/cortex/pull/1565/files) to one of our source files. It should be straightforward for you to build a custom image with this change for yourself, depending on the version of cortex you want do use. The easiest way would be to use the official Cortex image as a base image in your Dockerfile, and then replace the file that needs to be updated using a `COPY` command. [Here](https://docs.cortex.dev/advanced/system-packages#custom-docker-image) are instructions for using a custom docker image.
> 
> Also note that with this approach, `threads_per_process` will not be enforced.
> 
> We are not ready to merge #1565, because there are some known consequences of this that we would need to address (e.g. `threads_per_process` not being enforced).
> 
> Let me know if that works for you!

I'm getting this message in my logs when running my command:
```
Successfully built sse-starlette
Installing collected packages: sse-starlette
Successfully installed sse-starlette-0.6.1
[cont-init.d] bootloader.sh: exited 0.
[cont-init.d] done.
[services.d] starting services
[services.d] done.
2020-11-27 19:43:22.076150:cortex:pid-241:INFO:loading the predictor from predictor.py
2020-11-27 19:43:22.080023:cortex:pid-241:INFO:Started server process [241]
2020-11-27 19:43:22.080230:cortex:pid-241:INFO:Waiting for application startup.
2020-11-27 19:43:22.080905:cortex:pid-241:INFO:Application startup complete.
2020-11-27 19:43:22.081202:cortex:pid-241:INFO:Uvicorn running on unix socket /run/uvicorn/proc-0.sock (Press CTRL+C to quit)
[cont-finish.d] executing container finish scripts...
[cont-finish.d] done.
[s6-finish] waiting for services.
2020-11-27 19:43:29.310160:cortex:pid-237:INFO:Shutting down
2020-11-27 19:43:29.410600:cortex:pid-237:INFO:Waiting for application shutdown.
2020-11-27 19:43:29.410999:cortex:pid-237:INFO:Application shutdown complete.
2020-11-27 19:43:29.411105:cortex:pid-237:INFO:Finished server process [237]
s6-svwait: fatal: timed out
[s6-finish] sending all processes the TERM signal.
s6-svscanctl: fatal: unable to control /var/run/s6/services: supervisor not listening
[s6-finish] sending all processes the KILL signal and exiting.
2020-11-27 19:47:09.893313:cortex:pid-241:INFO:200 OK POST /predict
```
Your test command seems to be working though",think discovered approach work example work name test kind predictor type python image path python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return python note built new version predictor image image configuration field small change one source straightforward build custom image change depending version cortex want use easiest way would use official cortex image base image replace file need copy command custom docker image also note approach enforced ready merge known would need address enforced let know work think discovered approach work example work name test kind predictor type python image path python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return python note built new version predictor image image configuration field small change one source straightforward build custom image change depending version cortex want use easiest way would use official cortex image base image replace file need copy command custom docker image also note approach enforced ready merge known would need address enforced let know work getting message running command successfully built collected successfully done starting done cortex loading predictor cortex server process cortex waiting application cortex application complete cortex running socket press quit container finish done waiting cortex shutting cortex waiting application shutdown cortex application shutdown complete cortex finished server process fatal timed sending term signal fatal unable control supervisor listening sending kill signal cortex post test command working though,issue,positive,positive,positive,positive,positive,positive
734917700,"Yes this will be considered as a feature request.

Oh that's right. Also I think kubernetes jobs are meant to be immutable so `kubectl edit` may not even work on a job spec.",yes considered feature request oh right also think meant immutable edit may even work job spec,issue,negative,positive,positive,positive,positive,positive
734889559,"Thanks for answers.

I think the timing possible to modify job config is right after the job submission.
Unfortunately, the api initialization is quite fast, so the api pod doesn't wait for me to modify job config using kubectl(my hands are so dumb). It starts itself and is killed by its shared memory exception.

If it is not possible to set shm size for now, could you consider that as feature request for future?",thanks think timing possible modify job right job submission unfortunately quite fast pod wait modify job dumb memory exception possible set size could consider feature request future,issue,negative,negative,neutral,neutral,negative,negative
734873175,"It may be possible that there isn't enough memory or disk space. Try increasing the memory allocation (using larger instance types in your cluster configuration and setting `compute.mem` to a higher value in your API spec). To increase disk space, you can set the `instance_volume_size` to a higher value.

After doing some cursory research it looks like the pytorch workers use shared memory (as you've mentioned). At this point Cortex doesn't expose a way to update shared memory. Cortex uses Kubernetes under the hood so we will have to use a different strategy to update the shared memory. I found a way to increase the shared memory size here. https://stackoverflow.com/questions/46085748/define-size-for-dev-shm-on-container-engine/46434614#46434614.

Here is a very high level guess as to how I would try to increase the shm size:
1. setup kubectl and configure it to point to your cluster https://docs.cortex.dev/guides/kubectl-setup
2. Deploy your API
3. If you are deploying a Batch API, run `kubectl edit job api-<your api name here>` and if you are deploying a Realtime API run `kubectl edit deployment api-<your api name here>`. It will open up an editor to edit the relevant resource. Navigate to the `volume` and `volumeMounts` section and update those sections according to the config specified in stackoverflow post above.

Let us know if this works for you.",may possible enough memory disk space try increasing memory allocation instance cluster configuration setting higher value spec increase disk space set higher value cursory research like use memory point cortex expose way update memory cortex hood use different strategy update memory found way increase memory size high level guess would try increase size setup configure point cluster deploy batch run edit job name run edit deployment name open editor edit relevant resource navigate volume section update according post let u know work,issue,positive,positive,positive,positive,positive,positive
734853549,"Closing this issue for now. Specifying log levels is being tracked here #1484. 

Feel free to follow up on this issue.",issue log tracked feel free follow issue,issue,positive,positive,positive,positive,positive,positive
734512647,Closing this issue for now. Feel free to file an issue if this issue is encountered again.,issue feel free file issue issue,issue,positive,positive,positive,positive,positive,positive
734487665,"@dakshvar22 I'll go ahead and close this issue now, feel free to reach out if are still running into this.",go ahead close issue feel free reach still running,issue,positive,positive,positive,positive,positive,positive
734487251,@balakrishna222111 @Mahi-debug I'll go ahead and close this issue since we have improved the networking stack in recent releases. Feel free to reach out if you encounter this problem again!,go ahead close issue since stack recent feel free reach encounter problem,issue,negative,positive,positive,positive,positive,positive
734486293,"I'll go ahead and close this, since https://github.com/cortexlabs/cortex/issues/1540 and https://github.com/cortexlabs/cortex/issues/1541 have been created. Feel free to reach out if you have any other questions!",go ahead close since feel free reach,issue,positive,positive,positive,positive,positive,positive
734484747,"@WasiqMalik We have added [these docs](https://docs.cortex.dev/running-cortex-on-aws/uninstall#troubleshooting) for troubleshooting any issues that arise when running `cortex cluster down`. I'll go ahead and close this issue, feel free to reach out if you have run into any other problems.",added arise running cortex cluster go ahead close issue feel free reach run,issue,positive,positive,positive,positive,positive,positive
734471686,"@mutal I think I have discovered an approach that will work for you.

This example that works for me:

#### cortex.yaml

```yaml
- name: test
  kind: RealtimeAPI
  predictor:
    type: python
    image: quay.io/cortexlabsdev/python-predictor-cpu-async:0.23.0
    path: predictor.py
```

#### predictor.py

```python
import asyncio
from sse_starlette.sse import EventSourceResponse


async def run_command():
    command = ""echo test1 && sleep 2 && echo test2 && sleep 2 && echo test3 && echo done""

    proc = await asyncio.create_subprocess_shell(
        command, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
    )

    while True:
        output = await proc.stdout.readline()
        if not output:
            return
        yield output.decode().strip()


class PythonPredictor:
    def __init__(self, config):
        pass

    async def predict(self, payload):
        return EventSourceResponse(run_command())
```

#### requirements.txt

```python
sse-starlette
```

Note that I built a new version of the predictor image (specified in the `image` API configuration field), because it required a [small change](https://github.com/cortexlabs/cortex/pull/1565/files) to one of our source files. It should be straightforward for you to build a custom image with this change for yourself, depending on the version of cortex you want do use. The easiest way would be to use the official Cortex image as a base image in your Dockerfile, and then replace the file that needs to be updated using a `COPY` command. [Here](https://docs.cortex.dev/advanced/system-packages#custom-docker-image) are instructions for using a custom docker image.

Also note that with this approach, `threads_per_process` will not be enforced.

We are not ready to merge https://github.com/cortexlabs/cortex/pull/1565, because there are some known consequences of this that we would need to address (e.g. `threads_per_process` not being enforced).

Let me know if that works for you!",think discovered approach work example work name test kind predictor type python image path python import import command echo test sleep echo test sleep echo test echo done await command true output await output return yield class self pas predict self return python note built new version predictor image image configuration field small change one source straightforward build custom image change depending version cortex want use easiest way would use official cortex image base image replace file need copy command custom docker image also note approach enforced ready merge known would need address enforced let know work,issue,positive,positive,neutral,neutral,positive,positive
733787699,"Mainly because of this intermittent problem, and we had already used EB for serving some of our models/services.

@nbeuchat ",mainly intermittent problem already used serving,issue,negative,positive,positive,positive,positive,positive
733785184,"@cristianmtr understandable. Since you were in the company at the time when you decided to switch away from Cortex, it would help us a lot if you could share some feedback about Cortex:

* Why did you transition from Cortex to AWS EB?
* What features did AWS EB offer that Cortex didn't?
* What would have kept you with Cortex?
* What things do you think we should address?
* What do you personally feel/think of Cortex?

I feel like this is an opportunity for us to improve by hearing your reasons. You'd be helping us a lot!",understandable since company time decided switch away cortex would help u lot could share feedback cortex transition cortex offer cortex would kept cortex think address personally cortex feel like opportunity u improve hearing helping u lot,issue,positive,neutral,neutral,neutral,neutral,neutral
733767851,"Unfortunately the program that I use doesn‚Äôt have that functionality.
Perhaps it is possible to manually implement a websocket within a current
architecture?

On Tue, 24 Nov 2020 at 23:38, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> I understand, thanks for explaining.
> That said, we still cannot commit to supporting this yet. It might be best
> to try to get it working using the current architecture; this should be
> possible to do (possibly with a minor modification to your inference
> program). My understanding is that right now, you send the input text to
> your program, and then the result is printed out as it's generated. Is it
> instead possible to implement your inference program so that when you run
> ./mycommand, the program doesn't exit, and instead accepts text input on
> stdin? If so, I think we can make it work in the current architecture.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1506#issuecomment-733274370>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIVPIWHLFYJI2CYFO4LSRQYYBANCNFSM4TDKZLAA>
> .
>
",unfortunately program use functionality perhaps possible manually implement within current architecture tue wrote understand thanks explaining said still commit supporting yet might best try get working current architecture possible possibly minor modification inference program understanding right send input text program result printed instead possible implement inference program run program exit instead text input think make work current architecture reply directly view,issue,positive,positive,neutral,neutral,positive,positive
733747827,"@nellaG that's awesome! Let us know if you have any other questions/issues down the line. In the meantime, I'll close this ticket.",awesome let u know line close ticket,issue,positive,positive,positive,positive,positive,positive
733548987,"@deliahu Sorry, I no longer work at that company. And even before I left, we had switched over from cortex unfortunately. to AWS EB",sorry longer work company even left switched cortex unfortunately,issue,negative,negative,negative,negative,negative,negative
733510709,"@deliahu   I just updated `max_instances` value using `cortex cluster configure` command, and now everything works fine. Thanks for your help! ",value cortex cluster configure command everything work fine thanks help,issue,positive,positive,positive,positive,positive,positive
733462103,"@nellaG yes that is correct: because there are a few places where the max instances must be updated, it should be done via the cortex CLI. I don't think you'll have to spin down your cluster to fix it, you should be able to do one of the following:

If you have a [cluster configuration file](https://docs.cortex.dev/install#configure-cortex), update `max_instances` in the file, and run:

`cortex cluster configure --config cluster.yaml`

If you don't have a cluster configuration file, it will prompt you to specify max instances when you run:

`cortex cluster configure`

Let us know if that resolves it for you!",yes correct must done via cortex think spin cluster fix able one following cluster configuration file update file run cortex cluster configure cluster configuration file prompt specify run cortex cluster configure let u know,issue,negative,positive,positive,positive,positive,positive
733422251,"@vishalbollu Yes.`max_instances` of my cluster was set to 5 initially as you said, and after `max_instances` was set to 200 manually via AWS EC2 auto scaling group console. When I checked cluster info using `cortex cluster info` , I checked that `max_instances` was set to 200 correctly. Should I not have set the `max_instances` value using AWS Console?
",cluster set initially said set manually via auto scaling group console checked cluster cortex cluster checked set correctly set value console,issue,negative,neutral,neutral,neutral,neutral,neutral
733378823,"@cristianmtr I just wanted to follow up on this; has this error been persisting, and if so, have you tried on v0.22+?",follow error persisting tried,issue,negative,neutral,neutral,neutral,neutral,neutral
733274370,"@mutal I understand, thanks for explaining. That said, we still cannot commit to supporting this yet. It might be best to try to get it working using the current architecture; this should be possible to do (possibly with a minor modification to your inference program). My understanding is that right now, you send the input text to your program, and then the result is printed out as it's generated. Is it instead possible to implement your inference program so that when you run `./mycommand`, the program doesn't exit, and instead accepts text input on stdin? If so, I think we can make it work in the current architecture.",understand thanks explaining said still commit supporting yet might best try get working current architecture possible possibly minor modification inference program understanding right send input text program result printed instead possible implement inference program run program exit instead text input think make work current architecture,issue,positive,positive,positive,positive,positive,positive
733212010,"Following up on this, the fix for the `cortex cluster configure` #1567 was included in the 0.22.1 patch release. So the previous scenario may not apply to you.

I tried to reproduce the issue using your configuration, a g4dn.xlarge cluster on us-west-2 but it worked as expected for me. When I submitted a job requesting 10 workers (each worker requiring 1 gpu), my autoscaling group desired capacity changed to 10.

Given that I can't reproduce the issue, you can help me investigate this bug by running the command `cortex cluster info -c cluster.yaml --debug` the next time you encounter this issue. This command will export information about the current state of your cluster to a zip file. It would be great if you can email the zip file to me at vishal@cortexlabs.com so that I can investigate the issue in greater detail.",following fix cortex cluster configure included patch release previous scenario may apply tried reproduce issue configuration cluster worked job worker group desired capacity given ca reproduce issue help investigate bug running command cortex cluster next time encounter issue command export information current state cluster zip file would great zip file investigate issue greater detail,issue,positive,positive,positive,positive,positive,positive
733097945,"Was your `max_instances` setting initially set to 5 and then you used the `cortex cluster configure` command to raise the `max_instances` to 200? If this is the case, then you've encountered the bug https://github.com/cortexlabs/cortex/issues/1567. This bug affects `cortex cluster configure` command and doesn't update the maximum cluster size correctly. The fix for this bug will in v0.23 (the next release).

In the meantime, it would be great if you could spin up a new cluster `cortex cluster up -c cluster.yaml` using cortex 0.22.1 with the min/max set to the desired values and try submitting the job again. Let us know how it goes.",setting initially set used cortex cluster configure command raise case bug bug cortex cluster configure command update maximum cluster size correctly fix bug next release would great could spin new cluster cortex cluster cortex set desired try job let u know go,issue,positive,positive,positive,positive,positive,positive
732343874,"I would be really grateful if you could try prioritizing this issue. I‚Äôm
working on project with a group and websockets are a key element in our
platform and we would really like to use Cortex in our implementation
instead of developing a whole new scalable infrastructure system. Hope you
understand. I‚Äôll be waiting.

On Mon, 23 Nov 2020 at 06:07, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> at this point, we aren't able to
> predict; we usually prioritize 2-3 weeks an advance, so we'd probably have
> a clearer picture in a few weeks.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1506#issuecomment-731928625>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIV3PKOVD4CHAC5AOHDSRHUX3ANCNFSM4TDKZLAA>
> .
>
",would really grateful could try issue working project group key element platform would really like use cortex implementation instead whole new scalable infrastructure system hope understand waiting mon wrote point able predict usually advance probably clearer picture reply directly view,issue,positive,positive,positive,positive,positive,positive
732330151,Autoscaling might be better based on the average age of objects in the queue (a metric available by default in SQS).,might better based average age queue metric available default,issue,negative,positive,positive,positive,positive,positive
731928625,"@mutal at this point, we aren't able to predict; we usually prioritize 2-3 weeks an advance, so we'd probably have a clearer picture in a few weeks.",point able predict usually advance probably clearer picture,issue,negative,positive,positive,positive,positive,positive
731928134,"@G-Sheridan Yes, I think this could be a good first issue. There are a few details that have not been scoped out, so I think if someone wanted to contribute this, we would want to have a quick chat about implementation details. But that is certainly doable, so I'll go ahead an add the label. Thanks!",yes think could good first issue think someone contribute would want quick chat implementation certainly doable go ahead add label thanks,issue,positive,positive,positive,positive,positive,positive
731722809,"Oh, I see. Would it be reasonable to expect #1508 to be resolved before
January?

On Sat, 21 Nov 2020 at 22:28, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> we use github issues as a list of
> features that we can potentially work on next. Once we decide to prioritize
> it for a particular release, we'll add a label marking the release we're
> targeting. Once we start working on it, we'll move it to ""In progress"" in
> the GitHub project. Since #1508
> <https://github.com/cortexlabs/cortex/issues/1508> has not been labeled
> with a release tag and is still in the ""To prioritize"" category in the
> GitHub project, that means that we have not yet decided when we will start
> working on this feature. Every two weeks we look at the ""To prioritize""
> list and decide what to work on next. There are many features in the ""To
> prioritize"" list, and currently there are a few I can think of that we will
> probably prioritize higher than #1508
> <https://github.com/cortexlabs/cortex/issues/1508>, like GCP support
> (although this is a team decision, not just mine).
>
> ‚Äî
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1506#issuecomment-731639284>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIRJDXNNQPUIUER35C3SRAWF7ANCNFSM4TDKZLAA>
> .
>
",oh see would reasonable expect resolved sat wrote use list potentially work next decide particular release add label marking release start working move progress project since release tag still category project yet decided start working feature every two look list decide work next many list currently think probably higher like support although team decision mine reply directly view,issue,positive,positive,positive,positive,positive,positive
731639284,"@mutal we use github issues as a list of features that we can potentially work on next. Once we decide to prioritize it for a particular release, we'll add a label marking the release we're targeting. Once we start working on it, we'll move it to ""In progress"" in the GitHub project. Since https://github.com/cortexlabs/cortex/issues/1508 has not been labeled with a release tag and is still in the ""To prioritize"" category in the GitHub project, that means that we have not yet decided when we will start working on this feature. Every two weeks we look at the ""To prioritize"" list and decide what to work on next. There are many features in the ""To prioritize"" list, and currently there are a few I can think of that we will probably prioritize higher than https://github.com/cortexlabs/cortex/issues/1508, like GCP support (although this is a team decision, not just mine).
",use list potentially work next decide particular release add label marking release start working move progress project since release tag still category project yet decided start working feature every two look list decide work next many list currently think probably higher like support although team decision mine,issue,positive,positive,positive,positive,positive,positive
731520234,"I see. What is the usual time for Cortex feature to be implemented after
it‚Äôs been announced? (This ticket was opened about 3 weeks ago)

On Sat, 21 Nov 2020 at 04:27, David Eliahu <notifications@github.com> wrote:

> Thanks for following up. We have not yet made progress on #1508
> <https://github.com/cortexlabs/cortex/issues/1508>, and haven't decided
> yet on how to prioritize it compared to our other tickets.
>
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1506#issuecomment-731500466>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXISMLRIH5NZCQCAVOKDSQ4XSBANCNFSM4TDKZLAA>
> .
>
",see usual time cortex feature ticket ago sat wrote thanks following yet made progress decided yet thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
731500466,"Thanks for following up. We have not yet made progress on #1508, and haven't decided yet on how to prioritize it compared to our other tickets.",thanks following yet made progress decided yet,issue,positive,positive,neutral,neutral,positive,positive
731493987,"@miggymigz thanks for following up. I'll go ahead and mark this as closed for the time being, feel free to reach out if you run into it again!",thanks following go ahead mark closed time feel free reach run,issue,positive,positive,positive,positive,positive,positive
731490182,@deliahu I'm sorry but I can't reproduce the error now since we changed our backend to fastAPI instead (for easier CORS configuration).,sorry ca reproduce error since instead easier configuration,issue,negative,negative,negative,negative,negative,negative
731334635,"> Yes, this makes sense, thanks for your suggestion. I have created this ticket (#1508) to track the feature.

sorry for being intrusive, but how is progress going on that ticket?",yes sense thanks suggestion ticket track feature sorry intrusive progress going ticket,issue,positive,negative,negative,negative,negative,negative
728652276,"@miggymigz Do you mind sharing the message from the CORS error you're getting, as well as your browser type and version, so I can try to reproduce it?",mind message error getting well browser type version try reproduce,issue,negative,neutral,neutral,neutral,neutral,neutral
728468744,"@deliahu Hi. I tried doing what RobertLucian suggested but it was too much work for me. And yes, the goal is to run cortex on several cloud instances (huawei cloud, to be specific) with load balancing. 

We do this since we want to prepare for a LAN deployment (the user of our models is a hospital and they don't want to expose it to the public internet).

Also, the CORS is needed since we access the API with a frontend app running on a different port.",hi tried much work yes goal run cortex several cloud cloud specific load balancing since want prepare lan deployment user hospital want expose public also since access running different port,issue,negative,positive,neutral,neutral,positive,positive
728321726,"> @ariel-frischer Do you see any useful info in `cortex logs <api_name>`? If not, the next time this happens, do you mind running `cortex cluster info --debug`, and sending the resulting zip file (which contains the full cluster state) to [dev@cortex.dev](mailto:dev@cortex.dev)? We'd be happy to take a look to see what's going on!

@deliahu Usually the logs just stop updating, or don't show anything at all. I will send you guys the zip file when I come across this again. Thank you for the support! ",see useful cortex next time mind running cortex cluster sending resulting zip file full cluster state dev dev happy take look see going usually stop show anything send zip file come across thank support,issue,positive,positive,positive,positive,positive,positive
728222478,"@da-source I'm closing this issue due to inactivity, feel free to reach out if you have additional questions.",issue due inactivity feel free reach additional,issue,positive,positive,positive,positive,positive,positive
728222065,"@miggymigz I wanted to follow up on this, were you able to get things working as you expected? If not, we can try to reproduce the issue. Are you accessing your APIs locally (e.g. via `http://localhost:8888`), or via the internet (i.e. you are running Cortex locally on a cloud instance)?",follow able get working try reproduce issue locally via via running cortex locally cloud instance,issue,negative,positive,positive,positive,positive,positive
728198489,"@AkashDharani I'll go ahead and close this issue, feel free to reach out if you have additional questions",go ahead close issue feel free reach additional,issue,positive,positive,positive,positive,positive,positive
728174364,"@imagine3D-ai I'll go ahead and close this issue, feel free to reach out if you have any follow up questions",go ahead close issue feel free reach follow,issue,positive,positive,positive,positive,positive,positive
728172393,"@grzeczko I'm closing this issue due to inactivity, feel free to reach out if it's still causing you problems",issue due inactivity feel free reach still causing,issue,positive,positive,positive,positive,positive,positive
728171425,"@ydennisy We've started working on supporting GCP clusters, and we're hoping to have it ready for our v0.23 or v0.24 release.

I'll go ahead and close this issue (since we have https://github.com/cortexlabs/cortex/issues/114)",working supporting ready release go ahead close issue since,issue,positive,positive,positive,positive,positive,positive
728168741,"@ariel-frischer Do you see any useful info in `cortex logs <api_name>`? If not, the next time this happens, do you mind running `cortex cluster info --debug`, and sending the resulting zip file (which contains the full cluster state) to dev@cortex.dev? We'd be happy to take a look to see what's going on!",see useful cortex next time mind running cortex cluster sending resulting zip file full cluster state dev happy take look see going,issue,positive,positive,positive,positive,positive,positive
727761957,"I have turned off rolling updates with documentation recommendation of `max_surge: 0` and I seem to keep getting into `compute unavailable` statuses. The only way it seems to resolve this is having to go through `cortex cluster down` then `cortex cluster up` then it seems to start working again... This is obviously not ideal for deploying some updates I'm unsure why this is happening with my system, I'm using python, distilgpt-2 with m5.large, min/max instances set to 1... ",turned rolling documentation recommendation seem keep getting compute unavailable way resolve go cortex cluster cortex cluster start working obviously ideal unsure happening system python set,issue,positive,positive,positive,positive,positive,positive
725791507,"Oh, fantastic, thank you very much! I misunderstood the manpage, I guess, and thought it was equivalent to `--data` with the corresponding Content-Type header.",oh fantastic thank much misunderstood guess thought equivalent data corresponding header,issue,positive,positive,positive,positive,positive,positive
725561226,"@cristianmtr in our [v0.22 release](https://github.com/cortexlabs/cortex/releases/tag/v0.22.0) today, we redesigned part of our networking stack. We were able to confirm that it fixed https://github.com/cortexlabs/cortex/issues/814. I know that this ticket is unrelated, but it's possible that this issue is also resolved by v0.22 (we have not been able to reproduce it, although we weren't able to reproduce it on v0.21 either).

Have you continued to experience the 503s, and if so, do you mind trying on cortex v0.22?",release today part stack able confirm fixed know ticket unrelated possible issue also resolved able reproduce although able reproduce either continued experience mind trying cortex,issue,negative,positive,positive,positive,positive,positive
725547205,"@da-source we released v0.22 today, which will throw an error during `cortex cluster up` if an ARM-based instance is requested. Thanks for bringing this to our attention!",today throw error cortex cluster instance thanks attention,issue,negative,positive,positive,positive,positive,positive
725515173,"@DragonDuck I just tried this as well and it looks like, for binary data, the `--data-binary` option is necessary instead of the `-d` option. Running `curl http://localhost:8890 -X POST -H ""Content-Type: application/octet-stream"" --data-binary @input_image.jpg` will send all 9792 bytes to the API.

From the [man](https://curl.se/docs/manpage.html) page:
```text
--data-binary <data>

This posts data exactly as specified with no extra processing whatsoever.

If you start the data with the letter @, the rest should be a filename. Data is posted in a similar manner as -d, --data does, except that newlines and carriage returns are preserved and conversions are never done.

Like -d, --data the default content-type sent to the server is application/x-www-form-urlencoded. If you want the data to be treated as arbitrary binary data by the server then set the content-type to octet-stream: -H ""Content-Type: application/octet-stream"".

If this option is used several times, the ones following the first will append data as described in -d, --data.
```

```text
...
To post data purely binary, you should instead use the --data-binary option.
...
```

I also tried this with the `ab` GNU utility, and out of the box, it does seem to send all bytes correctly:
`ab -n 1 -c 1 -p input_image.jpg -T 'application/octet-stream' -rks 120 'http://localhost:8890/'`",tried well like binary data option necessary instead option running curl post send man page text data data exactly extra whatsoever start data letter rest data posted similar manner data except carriage never done like data default sent server want data arbitrary binary data server set option used several time following first append data data text post data purely binary instead use option also tried gnu utility box seem send correctly,issue,positive,positive,neutral,neutral,positive,positive
725256147,"> @mutal @da-source It appears that you have some urgency with regards to this feature.
> 
> Unfortunately, this feature is not a priority for Cortex for the next few weeks.
> 
> If I were in your position and wanted to ship something in the next month or so, I would try the workaround suggested [here](https://github.com/cortexlabs/cortex/issues/1529#issuecomment-721874025) to use Cortex for your project.
> 
> Feel free to watch for notifications on this ticket. When the team has decided to prioritize this ticket, it will be moved from the `to prioritize` column to `current sprint`. If it remains in the `to prioritize` column it means that the team has decided that other features are a higher priority than this feature.

The workoaround that you have suggested doesn't work for me, because it means restarting the process (something I'm trying to avoid) on each call. In the meantime, I'll try to find a way to create a websockets on the Cortex instances myself. It shouldn't be too hard. Based on [this](https://websockets.readthedocs.io/en/stable/intro.html), I'll have to replace `localhost` with the IP of the Cortex's AWS instance. Any ideas on how to get the IP of the instances which Cortex spins up?",urgency feature unfortunately feature priority cortex next position ship something next month would try use cortex project feel free watch ticket team decided ticket column current sprint remains column team decided higher priority feature work process something trying avoid call try find way create cortex hard based replace cortex instance get cortex,issue,negative,negative,neutral,neutral,negative,negative
724724306,"Theoretically, there's no limit to this, as long as the `max_instances` field is updated to a higher value and as long as the AWS' pool of instances is big enough. And there's no soft limit applied to the `max_instances` field in your [`cluster.yaml`](https://docs.cortex.dev/cluster-management/config) config.",theoretically limit long field higher value long pool big enough soft limit applied field,issue,negative,positive,neutral,neutral,positive,positive
724215679,"> @imagine3D-ai were you able to spin down that cluster? If you run into any trouble, we just added this guide on our master branch (which should be released this week): https://docs.cortex.dev/v/master/troubleshooting/cluster-down

yes, I‚Äôve closed it.",able spin cluster run trouble added guide master branch week yes closed,issue,negative,positive,neutral,neutral,positive,positive
724210934,"@imagine3D-ai were you able to spin down that cluster? If you run into any trouble, we just added this guide on our master branch (which should be released this week): https://docs.cortex.dev/v/master/troubleshooting/cluster-down",able spin cluster run trouble added guide master branch week,issue,negative,positive,positive,positive,positive,positive
723485155,"@deliahu I made a few changes that I think are relevant. Please review them before merging, happy to discuss live as well.",made think relevant please review happy discus live well,issue,positive,positive,positive,positive,positive,positive
723484433,"@mutal @da-source It appears that you have some urgency with regards to this feature.

Unfortunately, this feature is not a priority for Cortex for the next few weeks.

If I were in your position and wanted to ship something in the next month or so, I would try the workaround suggested [here](https://github.com/cortexlabs/cortex/issues/1529#issuecomment-721874025) to use Cortex for your project.

Feel free to watch for notifications on this ticket. When the team has decided to prioritize this ticket, it will be moved from the `to prioritize` column to `current sprint`. If it remains in the `to prioritize` column it means that the team has decided that other features are a higher priority than this feature.",urgency feature unfortunately feature priority cortex next position ship something next month would try use cortex project feel free watch ticket team decided ticket column current sprint remains column team decided higher priority feature,issue,negative,positive,neutral,neutral,positive,positive
723457311,"> @mutal we haven't come up with a timeline for it yet. We'll keep this ticket updated as we go along. Is this urgent to you?
> And to re-iterate what @deliahu has mentioned before, we usually plan about two weeks at a time.

It would be very helpful for me if this feature would become available. When you say two weeks at a time, does that mean you plan to add it the week after the next one?",come yet keep ticket go along urgent usually plan two time would helpful feature would become available say two time mean plan add week next one,issue,positive,negative,neutral,neutral,negative,negative
723416865,"Until this is added, Another way is to create a directory in `/mnt/project` and save files in it.",added another way create directory save,issue,positive,neutral,neutral,neutral,neutral,neutral
723399735,"> @mutal we haven't come up with a timeline for it yet. We'll keep this ticket updated as we go along. Is this urgent to you?
> And to re-iterate what @deliahu has mentioned before, we usually plan about two weeks at a time.

 I was hoping to implement a project with scalable infrastructure and websockets this month, so it would be nice if you could add this feature as soon as possible.",come yet keep ticket go along urgent usually plan two time implement project scalable infrastructure month would nice could add feature soon possible,issue,positive,positive,positive,positive,positive,positive
723399211,"There was one cluster left running indeed.


On Sat, 7 Nov 2020 at 03:53, David Eliahu <notifications@github.com> wrote:

> @imagine3D-ai <https://github.com/imagine3D-ai> Can you confirm that
> there are no EKS clusters running right now, in any regions that you may
> have used? You can check this in your AWS console:
> https://console.aws.amazon.com/eks/home
>
> If there aren't any still running (assuming that you don't have any Cortex
> clusters running at the moment), then I'm not sure what would cause this.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1542#issuecomment-723379782>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQQZJ3FNGMUUNULAIWUWJ4LSOSZBFANCNFSM4TNCFV3A>
> .
>
",one cluster left running indeed sat wrote confirm running right may used check console still running assuming cortex running moment sure would cause reply directly view,issue,negative,positive,positive,positive,positive,positive
723386033,"@mutal we haven't come up with a timeline for it yet. We'll keep this ticket updated as we go along. Is this urgent to you?
And to re-iterate what @deliahu has mentioned before, we usually plan about two weeks at a time.",come yet keep ticket go along urgent usually plan two time,issue,negative,negative,negative,negative,negative,negative
723379782,"@imagine3D-ai Can you confirm that there are no EKS clusters running right now, in any regions that you may have used? You can check this in your AWS console: https://console.aws.amazon.com/eks/home

If there aren't any still running (assuming that you don't have any Cortex clusters running at the moment), then I'm not sure what would cause this.",confirm running right may used check console still running assuming cortex running moment sure would cause,issue,negative,positive,positive,positive,positive,positive
723256892,"> #### Motivation
> * Reduce latency when multiple requests are required
> * Stream output from the predictor as it's generated

Hi! Are there any updates on when this will be coming out?",motivation reduce latency multiple stream output predictor hi coming,issue,negative,neutral,neutral,neutral,neutral,neutral
723195669,"The Batch API currently only keeps track of the status of the overall job and does monitor the status of each individual batch in the job. Failed batches are currently discarded, making it difficult to do perform retries at the batch level.

I have created these two tickets https://github.com/cortexlabs/cortex/issues/1540 and https://github.com/cortexlabs/cortex/issues/1541 to address these issues. 

Support for Batch is a recent addition to Cortex so there is a lot of room for improvement. I would be happy to jump on a call to discuss workarounds for these issues and other potential improvements that can be made to Cortex. You can reach me at vishal@cortexlabs.com if you are interested.
",batch currently track status overall job monitor status individual batch job currently making difficult perform batch level two address support batch recent addition cortex lot room improvement would happy jump call discus potential made cortex reach interested,issue,positive,positive,neutral,neutral,positive,positive
723119003,"We have filed an issue recently https://github.com/cortexlabs/cortex/issues/1484 to provide the ability to control log levels. 

In the meantime, all workers logs are streamed to Cloudwatch. If I were in your situation, here is how I would attempt to debug a big job with a large number of workers:

1. Navigate to your [AWS cloudwatch console](https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logsV2:log-groups) and select your cluster's region
2. Find your API's log group which should take the form `<cortex cluster name>/<api name>`
3. You should notice that there are multiple log streams that take the form: `api-<api name>-<job-id>-<internal id>_[api|downloader]`
4. Click on search all streams
<img width=""1102"" alt=""Screen Shot 2020-11-06 at 9 31 37 AM"" src=""https://user-images.githubusercontent.com/4365343/98377679-201a6400-2013-11eb-9e3e-b20e85fc1638.png"">
5. Click on view as text and search for keywords such as Exception or error
<img width=""1096"" alt=""Screen Shot 2020-11-06 at 9 32 53 AM"" src=""https://user-images.githubusercontent.com/4365343/98377801-42ac7d00-2013-11eb-93d3-892657e305be.png"">

Alternatively, if you are attempting to debug errors in a job and you are okay with stopping the job, then you can stop the job with `cortex delete <api_name> <job_id>` and then run `cortex logs <api_name> <job_id>`. Getting the logs for a completed job should be less error prone. You can search for keywords like `exception` or `error`.",issue recently provide ability control log situation would attempt big job large number navigate console select cluster region find log group take form cortex cluster name name notice multiple log take form name internal id click search screen shot click view text search exception error screen shot alternatively job stopping job stop job cortex delete run cortex getting job le error prone search like exception error,issue,negative,positive,neutral,neutral,positive,positive
722576154,"Check this page, for more information.
[On this page](https://docs.cortex.dev/troubleshooting/stuck-updating) and more specifically [here](https://docs.cortex.dev/troubleshooting/stuck-updating#check-your-aws-auto-scaling-group-activity-history).

Updating here as the link mentioned in the above gist has been removed and shifted to the docs.",check page information page specifically link gist removed,issue,negative,neutral,neutral,neutral,neutral,neutral
722436782,"Could you specify the Cortex created S3 folders in which you would like to reduce the number of objects?

Also is it possible that you meant to file this issue in https://github.com/cortexproject/cortex? This GitHub repo is for Cortex, an open source platform for deploying machine learning models to production.",could specify cortex would like reduce number also possible meant file issue cortex open source platform machine learning production,issue,negative,neutral,neutral,neutral,neutral,neutral
722199075,"Thanks Robert for your guidance, btw I am also looking into Nvidia Trition Inference Server, it seems quite promising as well in comparision with Tensorflow Serving Inference. You guys can look into this as well.",thanks guidance also looking inference server quite promising well serving inference look well,issue,positive,positive,positive,positive,positive,positive
722004120,"@da-source here is the documentation for [cluster configuration](https://docs.cortex.dev/cluster-management/config), and here is the documentation for the [`spot_config` section](https://docs.cortex.dev/cluster-management/spot-instances). Keep in mind that if you set `on_demand_backup` to `false`, you won't be guaranteed to get an instance from AWS, since spot instances are not guaranteed to always be available.",documentation cluster configuration documentation section keep mind set false wo get instance since spot always available,issue,negative,negative,neutral,neutral,negative,negative
721990329,"> @da-source The most straightforward way to accomplish this with Cortex is to generate only one word (or a small number of words) at a time, and to make multiple prediction requests.
> 
> For example, if you're generating text and the initial input is ""word1, word2, word3"", you could send a request with ""word1, word2, word3"" to generate a single word. When the response comes back with ""word4"", you can show that to the user, and make another request to your model with ""word1, word2, word3, word4"", repeating this process one word (or a few words) at a time. Would that work for you / your model?

I‚Äôll try that. Meanwhile how can I configure my cluster to be spot-instance only? I can‚Äôt find cluster.yaml file mentioned in documentation.",straightforward way accomplish cortex generate one word small number time make multiple prediction example generating text initial input word word word could send request word word word generate single word response come back word show user make another request model word word word word process one word time would work model try meanwhile configure cluster find file documentation,issue,negative,positive,neutral,neutral,positive,positive
721880324,"@imagine3D-ai each model behaves a bit differently, so some lend themselves to machines with more memory compared to CPU, and others lend themselves to more/faster CPU compared to memory. The latest ""Compute Optimized"" non-ARM instances would be the c5 or c5a series. ""large"" is the smallest size for those (as opposed to ""medium""), but since you can serve multiple API or multiple replicas of the same API on a single instance, using a larger instance type will not be more expensive if you have multiple APIs or multiple replicas in a single API.",model bit differently lend memory lend memory latest compute would ca series large size opposed medium since serve multiple multiple single instance instance type expensive multiple multiple single,issue,negative,positive,neutral,neutral,positive,positive
721874025,"@da-source The most straightforward way to accomplish this with Cortex is to generate only one word (or a small number of words) at a time, and to make multiple prediction requests.

For example, if you're generating text and the initial input is ""word1, word2, word3"", you could send a request with ""word1, word2, word3"" to generate a single word. When the response comes back with ""word4"", you can show that to the user, and make another request to your model with ""word1, word2, word3, word4"", repeating this process one word (or a few words) at a time. Would that work for you / your model?",straightforward way accomplish cortex generate one word small number time make multiple prediction example generating text initial input word word word could send request word word word generate single word response come back word show user make another request model word word word word process one word time would work model,issue,negative,positive,neutral,neutral,positive,positive
721873535,Cost is not my only motivation (although `c6g.medium` is cheaper than `t3.medium` and more powerful) since Compute Optimized instances seem to be more powerful and more suitable for machine learning inference applications than T3 instances,cost motivation although powerful since compute seem powerful suitable machine learning inference,issue,positive,positive,positive,positive,positive,positive
721871562,"@imagine3D-ai we don't currently have a timeline for ARM instance support. Which instance type are you hoping to use, and is cost reduction your only motivation for using it (and if so, how much would it save you)?",currently arm instance support instance type use cost reduction motivation much would save,issue,positive,positive,neutral,neutral,positive,positive
721869263,"> #### Notes
> Just the containers that run on worker nodes need to be compiled for ARM:
> 
> * [fluentd](https://hub.docker.com/r/fluent/fluentd-kubernetes-daemonset) has an ARM build
> * [cloudwatch-agent](https://hub.docker.com/r/amazon/cloudwatch-agent/tags) doesn't seem to have an ARM build
> * image-downloader containers will need to be updated
> * Modifications will likely need to be made to the API pod containers

What is the timeline on these enhacements?",run worker need arm arm build seem arm build need likely need made pod,issue,negative,neutral,neutral,neutral,neutral,neutral
721786717,"> @da-source we haven't scheduled this one yet; we usually plan about two weeks at a time.
> 
> Would it be possible to change your API implementation so that you can make a single HTTP request to the API (or multiple distinct requests if necessary), rather than relying on streaming the results?

I‚Äôm using compressed* large GPT-2 model: https://bellard.org/nncp/gpt2tc.html",one yet usually plan two time would possible change implementation make single request multiple distinct necessary rather streaming compressed large model,issue,negative,positive,neutral,neutral,positive,positive
721733567,"> @da-source we haven't scheduled this one yet; we usually plan about two weeks at a time.
> 
> Would it be possible to change your API implementation so that you can make a single HTTP request to the API (or multiple distinct requests if necessary), rather than relying on streaming the results?

I would like to deploy a large finetuned GPT-2 model. Since it is so large it takes a while to get the whole output and I would like to stream partial outputs instead of waiting for the whole thing. Something like AI Dungeon 2",one yet usually plan two time would possible change implementation make single request multiple distinct necessary rather streaming would like deploy large model since large get whole output would like stream partial instead waiting whole thing something like ai dungeon,issue,positive,positive,neutral,neutral,positive,positive
721405094,"@da-source we haven't scheduled this one yet; we usually plan about two weeks at a time.

Would it be possible to change your API implementation so that you can make a single HTTP request to the API (or multiple distinct requests if necessary), rather than relying on streaming the results?",one yet usually plan two time would possible change implementation make single request multiple distinct necessary rather streaming,issue,negative,negative,neutral,neutral,negative,negative
721404955,"@da-source When I tried to reproduce this with your configuration (c6g.medium instances in us-east-1), the cluster was able to spin up successfully. According to https://github.com/weaveworks/eksctl/issues/2201, this could be a transient issue with AWS.

That said, ARM instances aren't yet supported by Cortex, so you will run into issues down the road. I just created https://github.com/cortexlabs/cortex/issues/1528 to track support for ARM, and we'll add a validation check in our next release that will block ARM-based instances for now.

In the meantime, can you try again with a non-ARM instance type? Here's a [list of common instance types](https://docs.cortex.dev/cluster-management/ec2-instances). t3.small/t3a.small is probably the most similar to c6g.medium (and it's cheaper), or t3.medium/t3a.medium is good if you need a bit more memory.",tried reproduce configuration cluster able spin successfully according could transient issue said arm yet cortex run road track support arm add validation check next release block try instance type list common instance probably similar good need bit memory,issue,positive,positive,positive,positive,positive,positive
721268500,"> #### Motivation
> * Reduce latency when multiple requests are required
> * Stream output from the predictor as it's generated

When will this feature become available?",motivation reduce latency multiple stream output predictor feature become available,issue,negative,positive,positive,positive,positive,positive
721264764,"> @da-source It is possible to install Node.js on the API replicas by following these instructions: https://docs.cortex.dev/advanced/system-packages
> 
> What is your motivation for installing Node.js? I'm just asking in case there is an alternate solution that would be better for your use case, or if there is a feature that we should consider supporting.

I was looking into creating a websocket on an instance and then streaming the output to a website",possible install following motivation case alternate solution would better use case feature consider supporting looking instance streaming output,issue,positive,positive,positive,positive,positive,positive
720550049,"@da-source the output you are referring to is only there for debugging purposes and making sure the API is running. Your `predict` method has to return something, which will then be a part of the response payload. Here's a list of return examples:
https://docs.cortex.dev/deployments/realtime-api/predictors#api-responses

Also, to make sure this is all clear, when you say
```python
import time
def predict():
      for i in range(100):
         print(i)
         time.sleep(3)
```

I take you were referring to something like:
```python
import time

class PythonPredictor:
    def __init__(self, config):
        # your init stuff

    def predict(self, payload):
       for i in range(3):
         # your operations/predictions here

        return myresult
```

Even more so, I would advise against the use of `time.sleep`s because that will only eat into your throughput and latency.

I think our documentation might need improvement. Was there anything that was confusing with regards to the documentation? Your feedback is important.",output making sure running predict method return something part response list return also make sure clear say python import time predict range print take something like python import time class self stuff predict self range return even would advise use eat throughput latency think documentation might need improvement anything documentation feedback important,issue,positive,positive,positive,positive,positive,positive
720512676,"> @da-source you would do it like in this example (give or take):
> 
> ```js
> // Example POST method implementation:
> async function postData(url = '', data = {}) {
>   // Default options are marked with *
>   const response = await fetch(url, {
>     method: 'POST', // *GET, POST, PUT, DELETE, etc.
>     mode: 'cors', // no-cors, *cors, same-origin
>     cache: 'no-cache', // *default, no-cache, reload, force-cache, only-if-cached
>     credentials: 'same-origin', // include, *same-origin, omit
>     headers: {
>       'Content-Type': 'application/json'
>       // 'Content-Type': 'application/x-www-form-urlencoded',
>     },
>     redirect: 'follow', // manual, *follow, error
>     referrerPolicy: 'no-referrer', // no-referrer, *no-referrer-when-downgrade, origin, origin-when-cross-origin, same-origin, strict-origin, strict-origin-when-cross-origin, unsafe-url
>     body: JSON.stringify(data) // body data type must match ""Content-Type"" header
>   });
>   return response.json(); // parses JSON response into native JavaScript objects
> }
> ```
> 
> This one above is from https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch.
> 
> Here's a list of examples of API requests using the curl utility:
> https://docs.cortex.dev/deployments/realtime-api/predictors#api-requests

My predictor doesn‚Äôt exactly return anything, it rather prints out the parts of output every 3 secs. Will that be a problem since all the predictor examples I‚Äôve seen return something and don‚Äôt display realtime output?",would like example give take example post method implementation function data default marked response await fetch method get post put delete mode cache default reload include omit redirect manual follow error origin body data body data type must match header return response native one list curl utility predictor exactly return anything rather output every problem since predictor seen return something display output,issue,negative,positive,positive,positive,positive,positive
720485385,"@da-source you would do it like in this example (give or take):

```javascript
// Example POST method implementation:
async function postData(url = '', data = {}) {
  // Default options are marked with *
  const response = await fetch(url, {
    method: 'POST', // *GET, POST, PUT, DELETE, etc.
    mode: 'cors', // no-cors, *cors, same-origin
    cache: 'no-cache', // *default, no-cache, reload, force-cache, only-if-cached
    credentials: 'same-origin', // include, *same-origin, omit
    headers: {
      'Content-Type': 'application/json'
      // 'Content-Type': 'application/x-www-form-urlencoded',
    },
    redirect: 'follow', // manual, *follow, error
    referrerPolicy: 'no-referrer', // no-referrer, *no-referrer-when-downgrade, origin, origin-when-cross-origin, same-origin, strict-origin, strict-origin-when-cross-origin, unsafe-url
    body: JSON.stringify(data) // body data type must match ""Content-Type"" header
  });
  return response.json(); // parses JSON response into native JavaScript objects
}
```

This one above is from https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API/Using_Fetch.

Here's a list of examples of API requests using the curl utility:
https://docs.cortex.dev/deployments/realtime-api/predictors#api-requests",would like example give take example post method implementation function data default marked response await fetch method get post put delete mode cache default reload include omit redirect manual follow error origin body data body data type must match header return response native one list curl utility,issue,negative,positive,neutral,neutral,positive,positive
720482571,"Closing this once, since @deliahu already responded on this matter on Gitter. For clarity, this is caused by an OOM - the instance needs more memory in this case.",since already matter clarity instance need memory case,issue,negative,neutral,neutral,neutral,neutral,neutral
720393267,"> Cortex Predictors are deployed as REST APIs (under the hood, Cortex uses FastAPI instead of Flask), meaning you can query them from your frontend the same way you would any other web service. In your specific example, you could use [Fetch](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) to grab information from your endpoint, and then update the page.
> 
> To get a better sense of how Cortex deployments work, I'd recommend going through our tutorial: https://docs.cortex.dev/v/0.21/deployments/realtime-api/text-generator

Could clarify how to call Fetch on an endpoint? Also is there a need to use websockets in this case to stream the data?",cortex rest hood cortex instead flask meaning query way would web service specific example could use fetch grab information update page get better sense cortex work recommend going tutorial could clarify call fetch also need use case stream data,issue,positive,positive,positive,positive,positive,positive
720304807,"Thank you for the pointers. I am not sure what was wrong with S3 access. I solved the problem by spinning down the cluster, updating the cortex version, and setting up the cluster and API with the same configurations as before. ",thank sure wrong access problem spinning cluster cortex version setting cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
720144555,"Cortex Predictors are deployed as REST APIs (under the hood, Cortex uses FastAPI instead of Flask), meaning you can query them from your frontend the same way you would any other web service. In your specific example, you could use [Fetch](https://developer.mozilla.org/en-US/docs/Web/API/Fetch_API) to grab information from your endpoint, and then update the page.

To get a better sense of how Cortex deployments work, I'd recommend going through our tutorial: https://docs.cortex.dev/v/0.21/deployments/realtime-api/text-generator",cortex rest hood cortex instead flask meaning query way would web service specific example could use fetch grab information update page get better sense cortex work recommend going tutorial,issue,positive,positive,positive,positive,positive,positive
720113463,"Let's say my predictor.py is like this:

```
import time
def predict():
      for i in range(100):
         print(i)
         time.sleep(3)

```

And my simple webpage:

```
<html>
  <head>
    <title>DYNAMIC UPDATE</title>
  </head>
  <body>
   <p> UPDATE HERE <p>
  </body>
</html>
```


How can I update `<p>` element using Cortex API?",let say like import time predict range print simple head title dynamic update body update update element cortex,issue,positive,neutral,neutral,neutral,neutral,neutral
719859229,"I can identify 2 separate issues here. Both of them appear to be unrelated to one another.

1. Getting `""/src/cortex/serve/run.sh: line 42: 24 Segmentation fault (core dumped) /usr/bin/python3.6`.
1. Getting `botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden` exception.

The first one is generally caused by a C extension that a Python library may use - the search space is pretty big and considering that it only happened now, months later, suggests that it might be hard to debug. Whichever the case may be, there's not much we can do about it, because we don't have control over that. 

As for the 2nd issue, `botocore.exceptions.ClientError: An error occurred (403) when calling the HeadObject operation: Forbidden` seems to suggest that your S3 object(s) is/are no longer accessible. I recommend you test locally (with the `aws` CLI) if you can still download the objects from the S3 bucket with the same AWS credentials the API has access to.

Waiting to hear back from you!",identify separate appear unrelated one another getting line segmentation fault core getting error calling operation forbidden exception first one generally extension python library may use search space pretty big considering later might hard whichever case may much control issue error calling operation forbidden suggest object longer accessible recommend test locally still bucket access waiting hear back,issue,negative,positive,neutral,neutral,positive,positive
719858648,"@da-source It is possible to install Node.js on the API replicas by following these instructions: https://docs.cortex.dev/advanced/system-packages

What is your motivation for installing Node.js? I'm just asking in case there is an alternate solution that would be better for your use case, or if there is a feature that we should consider supporting.",possible install following motivation case alternate solution would better use case feature consider supporting,issue,positive,positive,positive,positive,positive,positive
718934349,"@balakrishna222111 @Mahi-debug I wanted to follow up on this: we didn't see any obvious issues in the information that you sent, and we have tried and been unable to reproduce it.

Are you still seeing `error: Unexpected response from operator (status code 405)`? If it's still a problem for you, do you mind trying on the latest release (0.21)?",follow see obvious information sent tried unable reproduce still seeing error unexpected response operator status code still problem mind trying latest release,issue,negative,positive,neutral,neutral,positive,positive
718928972,"Yes, this makes sense, thanks for your suggestion. I have created this ticket (https://github.com/cortexlabs/cortex/issues/1508) to track the feature.",yes sense thanks suggestion ticket track feature,issue,positive,positive,positive,positive,positive,positive
718888275,"It still not as quick as running the command from command line though (1
word/sec). I‚Äôm looking into setting up a websocket on each instance using
the predictor‚Äôs init function and then streaming the output to the client
side. Cortex could then spin down the instance when client side sends
signal to it. What do you think?

On Thu, 29 Oct 2020 at 16:18, Robert Lucian Chiriac <
notifications@github.com> wrote:

> @mutal <https://github.com/mutal> to avoid having to run subprocess.Popen
> multiple times, you can just run it once in the constructor of the Python
> predictor. This way, you don't have to wait for those ""3 to 4 seconds""
> every single time.
>
> Once the subprocess.Popen is run in the constructor, you can then use
> subprocess.Popen.communicate
> <https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate>
> to interact with your process. You can read from sdout/stderr, you can
> write to stdin and so on. The constructor only runs once. The predict
> method runs for each request.
>
> With this in, it means that for a request that asks for 5 words, it will
> take 5 seconds to process that request - not 5 + 3/4 seconds = 8/9 seconds.
>
> Does this answer your question?
>
> ‚Äî
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1500#issuecomment-718823054>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIUVIMVXJ5KKG3OMOYTSNGBUPANCNFSM4TCIV5QA>
> .
>
",still quick running command command line though looking setting instance predictor function streaming output client side cortex could spin instance client side signal think wrote avoid run multiple time run constructor python predictor way wait every single time run constructor use interact process read write constructor predict method request request take process request answer question reply directly view,issue,negative,positive,neutral,neutral,positive,positive
718860384,"Until this is added, one workaround is to run a cron job or filesystem monitor on your host which checks for new files in the cortex project directory, and moves them to another location.",added one run job monitor host new cortex project directory another location,issue,negative,positive,positive,positive,positive,positive
718823054,"@mutal to avoid having to run `subprocess.Popen` multiple times, you can just run it once in the constructor of the Python predictor. This way, you don't have to wait for those `""3 to 4 seconds""` every single time.

Once the `subprocess.Popen` is run in the constructor, you can then use [`subprocess.Popen.communicate`](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate) to interact with your process. You can read from `sdout`/`stderr`, you can write to `stdin` and so on. The constructor only runs once. The `predict` method runs for each request.

With this in, it means that for a request that asks for 5 words, it will take 5 seconds to process that request - not 5 + 3/4 seconds = 8/9 seconds.

Does this answer your question? ",avoid run multiple time run constructor python predictor way wait every single time run constructor use interact process read write constructor predict method request request take process request answer question,issue,negative,negative,neutral,neutral,negative,negative
718785912,"@AkashDharani That difference from 5-6 to 8-9 predictions/s is probably caused by the network delays and the concurrency level you have set on both cases. Since a request takes more time due to the round-trip delays caused by the network, you will have to increase the concurrency level (aka more concurrent requests per sec) when the Cortex cluster is used.

When using the Cortex cluster, you should probably increase the `predictor.threads_per_replica` in your API config and then make more concurrent requests.

I also assumed that the requests for `AWS Cluster (Tesla T4): 5-6 FPS` and `AWS EC2 Instance (Telsa T4) : 8-9 FPS` were done from your local machine.",difference probably network concurrency level set since request time due network increase concurrency level aka concurrent per sec cortex cluster used cortex cluster probably increase make concurrent also assumed cluster instance done local machine,issue,positive,negative,neutral,neutral,negative,negative
718775616,"There's currently no user-configurable option to modify the allowed hosts (for CORS) when using Cortex.

That being said, the part of the code that would have to be changed is:
https://github.com/cortexlabs/cortex/blob/06f6d7dd2f05d0119a35a23b22b1fa2af1019195/pkg/workloads/cortex/serve/serve.py#L56-L59
At the time of speaking, you'd need to re-build your predictor image, then push it to your registry (Docker Hub most likely), and then specify the newly uploaded image using the `predictor.image` field in your [`cortex.yaml`](https://docs.cortex.dev/deployments/realtime-api/api-configuration) API spec. The docs on how to set up your environment are found [here](https://docs.cortex.dev/contributing/development).

Let us know if you have any other questions/uncertainties.
",currently option modify cortex said part code would time speaking need predictor image push registry docker hub likely specify newly image field spec set environment found let u know,issue,negative,positive,neutral,neutral,positive,positive
718713857,"Deployed a model license-plate-reader via cortex_lite.yaml in g4dn.xlarge, Telsa T4 and got around 8-9 FPS.

Locally (RTX 2080 Ti): 10-11 FPS, used cortex to deploy a model on local machin without cluster.
AWS Cluster (Tesla T4): 5-6 FPS
AWS EC2 Instance (Telsa T4) : 8-9 FPS, used cortex to deploy a model on ec2 instance without cluster.",model via got around locally ti used cortex deploy model local machin without cluster cluster instance used cortex deploy model instance without cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
718688512,"Deployed same model on g4dn.xlarge, on Tesla T4, and got around 25 FPS.

Locally (RTX 2080 Ti) : 30 FPS, used cortex to deploy a model on local machine without cluster.
AWS Cluster(Tesla T4) : 10 FPS
AWS EC2 instance (Tesla T4) : 25 FPS, used cortex to deploy a model on ec2 instance without cluster.",model got around locally ti used cortex deploy model local machine without cluster cluster instance used cortex deploy model instance without cluster,issue,negative,neutral,neutral,neutral,neutral,neutral
718680613,"> I would like to create websockets on the instances that Cortex automatically spins up. A user mentioned that Cortex doesn‚Äôt yet support sockets. I figure that, AWS EC2 instances that cortex spins up run on Linux machines and that they should be able to run as websocket servers. Is this correct?

Let's say in my `predictor.py`s ` init ` function I would create a websocket and populate it with my model. I then could pass websocket's credentials to the client side and which then could communicate with the websocket server running on Cortex EC2  instance. Once the user completes the process client side could notify the Cortex API instance that it is over and Cortex would then spin it down. Could this be done?",would like create cortex automatically user cortex yet support figure cortex run able run correct let say function would create populate model could pas client side could communicate server running cortex instance user process client side could notify cortex instance cortex would spin could done,issue,positive,positive,positive,positive,positive,positive
718414438,"> Running `subprocess.Popen` multiple times is a valid concern. Using sockets could be a way to stream partial responses. Unfortunately, Cortex doesn't support sockets at the moment.
> 
> Rather than running `subprocess.Popen` in the `predict` function, you can run `subprocess.Popen` in the constructor (`__init__`) to initialize the process. Then you can use [process.communicate](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate) to send text to your process in your predict function.
> 
> Out of curiosity, have you considered loading your model in Python rather than using `subprocess.Popen` to run a binary?

I run binary, since it is compressed version of a large model which otherwise require a lot more memory to run. You mentioned that cortex doesn‚Äôt yet support sockets. But cortex spins up instances that run Linux, and those instances could run websocket servers in normal circumstances. I‚Äôm thinking that I just need to somehow communicate to EC2 AWS instance and launch a websocket which then could display partial output in realtime instead of using subprocess. What do you think?",running multiple time valid concern could way stream partial unfortunately cortex support moment rather running predict function run constructor initialize process use send text process predict function curiosity considered loading model python rather run binary run binary since compressed version large model otherwise require lot memory run cortex yet support cortex run could run normal thinking need somehow communicate instance launch could display partial output instead think,issue,negative,negative,neutral,neutral,negative,negative
718242540,"Running `subprocess.Popen` multiple times is a valid concern. Using sockets could be a way to stream partial responses. Unfortunately, Cortex doesn't support sockets at the moment.

Rather than running `subprocess.Popen` in the `predict` function, you can run `subprocess.Popen` in the constructor (`__init__`) to initialize the process. Then you can use [process.communicate](https://docs.python.org/3/library/subprocess.html#subprocess.Popen.communicate) to send text to your process in your predict function.

Out of curiosity, have you considered loading your model in Python rather than using `subprocess.Popen` to run a binary?",running multiple time valid concern could way stream partial unfortunately cortex support moment rather running predict function run constructor initialize process use send text process predict function curiosity considered loading model python rather run binary,issue,negative,negative,negative,negative,negative,negative
718197764,"> Is it possible for your model to generate a single word at a time rather than a sentence? If it is, rather than having your API respond with 20 words per request, set up your API to respond with a single word based on the provided prompt. In your client, you can iteratively query your API for the next word until you have a text with the desired length.

Correct me if I'm wrong but if I would query my API iteratively that would mean that I would have to use `subprocess.Popen` multiple times, which would be costly. I would like to keep the subprocess open and just print out the output in realtime. Opening new subrocess takes about 3 to 4 seconds + 1 second to actually generate output makes for about 5 seconds/word which is quite slow",possible model generate single word time rather sentence rather respond per request set respond single word based provided prompt client iteratively query next word text desired length correct wrong would query iteratively would mean would use multiple time would costly would like keep open print output opening new second actually generate output quite slow,issue,negative,negative,neutral,neutral,negative,negative
718036511,"Is it possible for your model to generate a single word at a time rather than a sentence? If it is, rather than having your API respond with 20 words per request, set up your API to respond with a single word based on the provided prompt. In your client, you can iteratively query your API for the next word until you have a text with the desired length.",possible model generate single word time rather sentence rather respond per request set respond single word based provided prompt client iteratively query next word text desired length,issue,negative,negative,neutral,neutral,negative,negative
717970164,"@AkashDharani right, so that's it. The RTX 2080 Ti is a lot faster than a Tesla T4:
1. On FP16 TFLOPS, the RTX 2080 Ti is 2.4 times faster than the T4.
1. On FP32 TFLOPS, the RTX 2080 Ti is ~1.7 times faster than the T4.
1. On FP64 TFLOPS, the RTX 2080 Ti is ~1.7 times faster than the T4.

Here's where you can see that:
https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305
https://www.techpowerup.com/gpu-specs/tesla-t4.c3316

So getting 10-11 predictions/s on the RTX and 5-6 predictions/s on the T4 sounds just about right. If you need more performance, I either suggest you increase your `max_replicas` field to something bigger (in your `cortex.yaml`) or go for a more powerful GPU on AWS (like the V100), but keep in mind that the best bang for the buck is the T4. Spots are also desirable.",right ti lot faster ti time faster ti time faster ti time faster see getting right need performance either suggest increase field something bigger go powerful like keep mind best bang buck also desirable,issue,positive,positive,positive,positive,positive,positive
717408032,"@mutal I just wanted to give you a heads up that this fix got released with v0.21.0 today. Please let us know if you continue to run into issues on `eu-north-1`, or if you‚Äôd like us to take a look at the issue you were having on `eu-central-1`.",give fix got today please let u know continue run like u take look issue,issue,positive,neutral,neutral,neutral,neutral,neutral
717337660,"@AkashDharani this should not happen unless the GPUs are different. What GPUs do you have on both your local machine and on the AWS instance?

The following is a list of GPUs having a compute capability of 7.5. A T4 will have a significantly different performance compared to an MX450 or an RTX 2080 Ti.

<img width=""1098"" alt=""Screenshot 2020-10-27 at 17 47 55"" src=""https://user-images.githubusercontent.com/26958764/97326315-8ddcc800-187c-11eb-81f7-46dc4e4c6a22.png"">
",happen unless different local machine instance following list compute capability significantly different performance ti,issue,negative,neutral,neutral,neutral,neutral,neutral
717266285,"@mutal the output/prediction has to be returned by the `predict` method. The response content will then be present in your response. Here is the documentation on this: https://docs.cortex.dev/deployments/realtime-api/predictors.

Since you have to return the output you get from your command, the results you get from `process.stdout.readline()` have to be returned. Here's how you could do it:

```python
import os
import stat

class PythonPredictor:
    def __init__(self, config):
        # equivalent of chmod +x
        st = os.stat('./mycommand')
        os.chmod(""./mycommand"", st.st_mode | stat.S_IEXEC)

    def predict(self, payload):
      command = ['./mycommand',  payload]
      process = subprocess.Popen(command, stdout=subprocess.PIPE, universal_newlines=True)
      output_lines = """"
      while True:
          output = process.stdout.readline()
          if output == '' and process.poll() is not None:
              break
          if output:
              output_lines += output
      return output_lines
```

Let us know if you have any other questions down the line.
",returned predict method response content present response documentation since return output get command get returned could python import o import class self equivalent st predict self command process command true output output none break output output return let u know line,issue,negative,positive,positive,positive,positive,positive
717049192,"I also deployed one of the example given by cortex dev on both local environment and aws environment, license-plate-reader via cortex_lite.yaml.

Local FPS: 10-11
AWS FPS: 5-6

Almost half fps dropped.",also one example given cortex dev local environment environment via local almost half,issue,negative,negative,neutral,neutral,negative,negative
716982763,"@RobertLucian 

1. Well I used same cortex.yaml file for both local environment and aws environment, so threads are same both time.
2. Both GPU have compute capacity 7.5, and I have made sure that on instance gpu is utilized by keeping an eye on utilization of gpu via nvidia-smi.
3. As instances are deployed on aws, both server(model) and client(where i am hitting api) in same region, so network should not be any issue.


And #1426 was related to GPU Ram and this is related to Performance.

I am attaching my model cortex.yaml file as well.

`

- name: detection  
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    path: predict-2.py  
    model_path: s3://cortex-api-sight/detection
    signature_key: serving_default 
    processes_per_replica: 1
    threads_per_process: 4  
    config:
      classes: None
      input_shape: [None, None]
      input_key: image_arrays:0
      output_key: detections:0
  compute:
    cpu: 3
    gpu: 1  
    mem: 8G

  networking:
    endpoint: detection 
    local_port: 9999 
    api_gateway: public`",well used file local environment environment time compute capacity made sure instance keeping eye utilization via server model client region network issue related ram related performance model file well name detection kind predictor type path class none none none compute mem detection public,issue,positive,positive,positive,positive,positive,positive
716583676,"@AkashDharani a couple of factors could be at play here:

1. Low single-thread/multi-thread performance of the instance type you've selected compared to your local setup.
1. Maybe you're using the GPU locally and for AWS clusters you only have CPU.
1. Low network bandwidth (and/or high latency) between your machine and the AWS cluster.

> Such performance decrease is expected on aws environment?

No, it isn't. As long as you're having the same compute capacity on both your local machine and on your AWS cluster, there should not be any drop in performance.

> Locally: 30 FPS
AWS: 10 FPS

When you say locally, I presume you're referring to your local deployment which got deployed with `cortex deploy -e local`, right?

---

Could you provide us with more information regarding your AWS/local setup? It would be great if we could see your `cortex.yaml` API config.

Also, is this ticket related to https://github.com/cortexlabs/cortex/issues/1426#issuecomment-713432792?

 ",couple could play low performance instance type selected local setup maybe locally low network high latency machine cluster performance decrease environment long compute capacity local machine cluster drop performance locally say locally presume local deployment got cortex deploy local right could provide u information regarding setup would great could see also ticket related,issue,negative,positive,neutral,neutral,positive,positive
716200300,"@imagine3D-ai I believe that this is happening because there is not enough available memory to load and run your model. This is shown in the output of `cortex get`:

```
cortex get --env aws
realtime api   status                  up-to-date   requested   failed   last update   avg request   2XX
myapi          error (out of memory)   0            1           1        6m            -             -
```

If you run `cortex cluster info`, it will show you how much memory capacity is on each instance. You can increase your memory request by updating the [`compute` field in your API configuration](https://docs.cortex.dev/advanced/compute). If you aren't able to get it working with your current instance type, you may need to spin up a new cluster using an instance type that has more memory.

Regarding the bucket: Cortex creates a bucket when the cluster is created, but this is only meant to be used internally by Cortex. You can keep your model stored in any bucket you own, as long as the AWS credentials that you used when you created the cluster have access to download the model from your bucket.",believe happening enough available memory load run model shown output cortex get cortex get status last update request error memory run cortex cluster show much memory capacity instance increase memory request compute field configuration able get working current instance type may need spin new cluster instance type memory regarding bucket cortex bucket cluster meant used internally cortex keep model bucket long used cluster access model bucket,issue,negative,positive,positive,positive,positive,positive
716139647,"> @imagine3D-ai Do you mind sharing the output of `cortex logs <your_api_name>`, if you run it about 10 seconds after seeing the ""connection termination"" error?

Just reran it again, and got 'sevice unavailable' error at first:

```
curl myendppint -X POST
{""message"":""Service Unavailable""}

cortex get --env aws

realtime api   status   up-to-date   requested   last update   avg request   2XX
myapi           live     1            1           5m           
 -             -

curl myendpoint -X POST
upstream connect error or disconnect/reset before headers. reset reason: connection termination

cortex get --env aws
realtime api   status                  up-to-date   requested   failed   last update   avg request   2XX
myapi          error (out of memory)   0            1           1        6m            -             -


 cortex logs myapi --env aws
fetching logs ...
2020-10-25 12:16:34.588409:cortex:pid-1:INFO:downloading the project code
2020-10-25 12:16:34.676927:cortex:pid-1:INFO:downloading the python serving image
2020-10-25 12:16:37.534526:cortex:pid-24:INFO:loading the predictor from predictor.py
done
2020-10-25 12:17:07.956450:cortex:pid-24:INFO:Started server process [24]
2020-10-25 12:17:07.957159:cortex:pid-24:INFO:Waiting for application startup.
2020-10-25 12:17:07.958819:cortex:pid-24:INFO:Application startup complete.
2020-10-25 12:17:07.961822:cortex:pid-24:INFO:Uvicorn running on http://0.0.0.0:8888 (Press CTRL+C to quit)
2020-10-25 12:19:33.962669:cortex:pid-1:INFO:downloading the project code
2020-10-25 12:19:34.083881:cortex:pid-1:INFO:downloading the python serving image
2020-10-25 12:19:36.653259:cortex:pid-22:INFO:loading the predictor from predictor.py
done
2020-10-25 12:20:06.080715:cortex:pid-22:INFO:Started server process [22]
2020-10-25 12:20:06.081522:cortex:pid-22:INFO:Waiting for application startup.
2020-10-25 12:20:06.082224:cortex:pid-22:INFO:Application startup complete.
2020-10-25 12:20:06.082636:cortex:pid-22:INFO:Uvicorn running on http://0.0.0.0:8888 (Press CTRL+C to quit)

```",mind output cortex run seeing connection termination error got unavailable error first curl post message service unavailable cortex get status last update request live curl post upstream connect error reset reason connection termination cortex get status last update request error memory cortex fetching cortex project code cortex python serving image cortex loading predictor done cortex server process cortex waiting application cortex application complete cortex running press quit cortex project code cortex python serving image cortex loading predictor done cortex server process cortex waiting application cortex application complete cortex running press quit,issue,negative,positive,neutral,neutral,positive,positive
716134214,"> @imagine3D-ai Do you mind sharing the output of `cortex logs <your_api_name>`, if you run it about 10 seconds after seeing the ""connection termination"" error?

I've noticed that when running `cortex cluster up` it automatically creates a new bucket. Model that my `predictor.py `is referencing is not located on the newly created bucket, it is in the other bucket that I've created and uploaded my model to. When I open the bucket that cortex auto-created in AWS Console, I can't find my `model.bin` file there. Is this the expected behaviour?",mind output cortex run seeing connection termination error running cortex cluster automatically new bucket model newly bucket bucket model open bucket cortex console ca find file behaviour,issue,negative,positive,neutral,neutral,positive,positive
716130944,"> @imagine3D-ai Do you mind sharing the output of `cortex logs <your_api_name>`, if you run it about 10 seconds after seeing the ""connection termination"" error?

```
cortex log myapi --env aws

fetching logs ...
2020-10-25 11:06:38.815431:cortex:pid-1:INFO:downloading the project code
2020-10-25 11:06:38.904597:cortex:pid-1:INFO:downloading the python serving image
2020-10-25 11:06:41.309539:cortex:pid-22:INFO:loading the predictor from predictor.py
done
2020-10-25 11:07:12.687999:cortex:pid-22:INFO:Started server process [22]
2020-10-25 11:07:12.688600:cortex:pid-22:INFO:Waiting for application startup.
2020-10-25 11:07:12.690125:cortex:pid-22:INFO:Application startup complete.
2020-10-25 11:07:12.692149:cortex:pid-22:INFO:Uvicorn running on http://0.0.0.0:8888 (Press CTRL+C to quit)
2020-10-25 11:07:47.647481:cortex:pid-1:INFO:downloading the project code
2020-10-25 11:07:47.750914:cortex:pid-1:INFO:downloading the python serving image
2020-10-25 11:07:50.689710:cortex:pid-22:INFO:loading the predictor from predictor.py
done
2020-10-25 11:08:19.849269:cortex:pid-22:INFO:Started server process [22]
2020-10-25 11:08:19.849825:cortex:pid-22:INFO:Waiting for application startup.
2020-10-25 11:08:19.850430:cortex:pid-22:INFO:Application startup complete.
2020-10-25 11:08:19.850862:cortex:pid-22:INFO:Uvicorn running on http://0.0.0.0:8888 (Press CTRL+C to quit)
^C

cortex get myapi --env aws

status                  up-to-date   requested   failed   last update   avg request   2XX
error (out of memory)   0            1           2        5m            -             -

```",mind output cortex run seeing connection termination error cortex log fetching cortex project code cortex python serving image cortex loading predictor done cortex server process cortex waiting application cortex application complete cortex running press quit cortex project code cortex python serving image cortex loading predictor done cortex server process cortex waiting application cortex application complete cortex running press quit cortex get status last update request error memory,issue,negative,positive,neutral,neutral,positive,positive
716054464,"@imagine3D-ai Do you mind sharing the output of `cortex logs <your_api_name>`, if you run it about 10 seconds after seeing the ""connection termination"" error?",mind output cortex run seeing connection termination error,issue,negative,neutral,neutral,neutral,neutral,neutral
715468243,"> > @imagine3D-ai a correction on my part. We've tested the Cortex, and it actually looks like payload-less requests work. It's just that when `curl`ing, you need to have this format: `curl <api-endpoint> -X POST)`.
> > Apologies for the inaccuracy.
> 
> Thanks for looking into it! But now I'm getting `upstream connect error or disconnect/reset before headers. reset reason: connection termination` followed by `no healthy upstream`:
> 
> ```
> curl <api-endpoint> -X POST
> upstream connect error or disconnect/reset before headers. reset reason: connection termination
> curl <api-endpoint> -X POST
> no healthy upstream
> ```

Why could that be?",correction part tested cortex actually like work curl ing need format curl post inaccuracy thanks looking getting upstream connect error reset reason connection termination healthy upstream curl post upstream connect error reset reason connection termination curl post healthy upstream could,issue,positive,positive,positive,positive,positive,positive
715467975,"> @imagine3D-ai a correction on my part. We've tested the Cortex, and it actually looks like payload-less requests work. It's just that when `curl`ing, you need to have this format: `curl <api-endpoint> -X POST)`.
> 
> Apologies for the inaccuracy.

Thanks for looking into it! But now I'm getting `upstream connect error or disconnect/reset before headers. reset reason: connection termination` followed by `no healthy upstream`:

```
curl <api-endpoint> -X POST
upstream connect error or disconnect/reset before headers. reset reason: connection termination
curl <api-endpoint> -X POST
no healthy upstream
```
",correction part tested cortex actually like work curl ing need format curl post inaccuracy thanks looking getting upstream connect error reset reason connection termination healthy upstream curl post upstream connect error reset reason connection termination curl post healthy upstream,issue,positive,positive,positive,positive,positive,positive
715416056,"Thanks for bringing this to our attention. After investigating this issue, we believe that this bug could be caused by some non-deterministic behaviour during the API Gateway route setup. We are continuing to investigate this issue. In the meantime, you could try to deploy your BatchAPI by explicitly disabling API Gateway:
```
- name: api:
  kind: BatchAPI
  networking:
    api_gateway: none # the default is public
```",thanks attention investigating issue believe bug could behaviour gateway route setup investigate issue could try deploy explicitly gateway name kind none default public,issue,positive,positive,positive,positive,positive,positive
714676862,Just an update here - I tried one of the batch demos in us-east-1 and it worked as expected.,update tried one batch demo worked,issue,negative,neutral,neutral,neutral,neutral,neutral
714309426,"@RobertLucian I have calculated FPS in predictor file as well to get idea if there is any issue with model compute, but FPS is fine in predictor no loss, it means there isn't any issue with compute. Network can't be issue as well because both client and sever are on aws in same region.",calculated predictor file well get idea issue model compute fine predictor loss issue compute network ca issue well client sever region,issue,negative,positive,positive,positive,positive,positive
714223665,"@RobertLucian there isn't any bottleneck one dev box, in internet both client and server are in aws ec2 instances and compute capacity is also not an issue. and yes FPS mean the number of inferences per second.

1. I am only deploying one model in one ec2 instance for now to compute inferences per second, not using multi-model deployment for now.
2. I will use `process_per_replica` in future, but for now I just want to know the capacity of single model locally, on ec2 and their difference.
",bottleneck one dev box client server compute capacity also issue yes mean number per second one model one instance compute per second deployment use future want know capacity single model locally difference,issue,positive,negative,neutral,neutral,negative,negative
713778437,"@AkashDharani 

Thought about this for a bit and I have 2 more questions for you:
1. Are you just looking to load multiple models on the same API? Because we already support that for the TensorFlow and ONNX predictors [here](https://docs.cortex.dev/guides/multi-model#tensorflow-predictor). We are also currently working on https://github.com/cortexlabs/cortex/pull/1428 which will give you more granular control over your deployed models.
1. Do you want to load the same model multiple times on the same API replica? If so, then just setting `processes_per_replica` to anything greater than 1 will do the job.

Waiting for feedback from you.",thought bit looking load multiple already support also currently working give granular control want load model multiple time replica setting anything greater job waiting feedback,issue,positive,positive,positive,positive,positive,positive
713696777,"Yes, I see the custom namespace and the same screenshot you show:

[image: image.png]

I'll reach out to you privately about live debugging.  Thanks Vishal.

-Ryan

On Wed, Oct 21, 2020 at 11:08 AM Vishal Bollu <notifications@github.com>
wrote:

> I tried reproducing the issue by replicating your cluster state on Cortex
> version 20 and running the image classifier example and I was unable to
> reproduce your bug.
>
> I would like to get to the bottom of this, but given that I can't
> reproduce this, it would be great if you would be open to jumping on call
> to debug the issue interactively. You can email me your contact information
> at vishal@cortexlabs.com.
>
> If jumping on a call doesn't work for you, here is the first thing I would
> try to verify that your workers are pushing metrics to cloudwatch.
>
>    1. deploy the image-classifier example and submit a job
>    2. navigate to cloudwatch metrics in your AWS Web Console
>    <https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2>
>    3. search for the job id in the search bar
>    4. can you share a screenshot of what you see?
>    5. under the custom namespaces section, can you see <your cluster
>    name> > APIName, JobID, metric_type?
>    Here is what I see (the values don't matter, but do you see data? you
>    may need to play with time period):
>
> [image: Screen Shot 2020-10-21 at 11 05 28 AM]
> <https://user-images.githubusercontent.com/4365343/96739096-6cb43d00-138d-11eb-80b5-e4d446bca994.png>
>
> Let me know if you have any questions along the way.
>
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1466#issuecomment-713646210>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AINLMMMAYXXZM5Y3FSBQ2RLSL32QFANCNFSM4SWNB3HQ>
> .
>


-- 

*Ryan Frenz*

Co-Founder

+1.412.346.4147 | www.allvision.io | Request a demo <info@allvision.io>
*‚ÄúWe see what others can't.‚Äù*


    <https://www.linkedin.com/company/allvision-io>
<https://twitter.com/allvision_io?lang=en>  [image:
https://www.facebook.com/AllvisionIO/]
<https://www.facebook.com/AllvisionIO/>

The information contained in this transmission and any attachments are for
the sole use of the intended recipient(s) and may be confidential,
privileged, copyrighted or may constitute intellectual property. Any
unauthorized review, use, disclosure or distribution of this transmission
and any attachments is strictly prohibited. If you have received this
transmission in error, please contact the sender and destroy all paper
and/or electronic copies of this transmission.
",yes see custom show image reach privately live thanks wed wrote tried issue cluster state cortex version running image classifier example unable reproduce bug would like get bottom given reproduce would great would open call issue contact information call work first thing would try verify pushing metric deploy example submit job navigate metric web console search job id search bar share see custom section see cluster name see matter see data may need play time period image screen shot let know along way thread reply directly view request see image information transmission sole use intended recipient may confidential privileged may constitute intellectual property unauthorized review use disclosure distribution transmission strictly received transmission error please contact sender destroy paper electronic transmission,issue,positive,positive,positive,positive,positive,positive
713646210,"I tried reproducing the issue by replicating your cluster state on Cortex version 20 and running the image classifier example and I was unable to reproduce your bug.

I would like to get to the bottom of this, but given that I can't reproduce this, it would be great if you would be open to jumping on call to debug the issue interactively. You can email me your contact information at vishal@cortexlabs.com.

If jumping on a call doesn't work for you, here is the first thing I would try to verify that your workers are pushing metrics to cloudwatch.
1. deploy the image-classifier example and submit a job
2. navigate to cloudwatch metrics in your [AWS Web Console](https://us-east-2.console.aws.amazon.com/cloudwatch/home?region=us-east-2)
3. search for the job id in the search bar
4. can you share a screenshot of what you see?
5. under the custom namespaces section, can you see `<your cluster name> > APIName, JobID, metric_type`?
  Here is what I see when I follow these instructions: (the values don't matter, you may need to play with the time window)
<img width=""822"" alt=""Screen Shot 2020-10-21 at 11 05 28 AM"" src=""https://user-images.githubusercontent.com/4365343/96739096-6cb43d00-138d-11eb-80b5-e4d446bca994.png"">

Let me know if you have any questions along the way.
 



",tried issue cluster state cortex version running image classifier example unable reproduce bug would like get bottom given ca reproduce would great would open call issue contact information call work first thing would try verify pushing metric deploy example submit job navigate metric web console search job id search bar share see custom section see cluster name see follow matter may need play time window screen shot let know along way,issue,positive,positive,positive,positive,positive,positive
713567825,"@imagine3D-ai a correction on my part. We've tested the Cortex, and it actually looks like payload-less requests work. It's just that when `curl`ing, you need to have this format: `curl <api-endpoint> -X POST)`.

Apologies for the inaccuracy. ",correction part tested cortex actually like work curl ing need format curl post inaccuracy,issue,negative,neutral,neutral,neutral,neutral,neutral
713565622,"> Thank you, now gpu memory is not fully utilized on instance as well, but can't deploy more than one model in same ec2 instance, working on it, if I can find any way out.

Yes, that's definitely the issue we are tracking here https://github.com/cortexlabs/cortex/issues/1390. By more than one model, I take you are referring to having multiple pods (on the same instance) having access to the same GPU unit. The official device plugin from Nvidia can't split the GPU to different pods. A 3rd-party one may be required. Looking to see if you find a way around this.

> There is another issue I am facing is FPS issue, locally a model gives around 100 FPS with cortex but while deploying that same model using cortex in us-east-1 in aws env and hitting that model from us-east-1 gives around 30 fps.

If it's the same GPU and there's no bottleneck on your dev box (internet connection, compute capacity) and the model is also loaded on the GPU's memory, then this shouldn't happen at all. The performance should be the same. Also, by FPS I suppose you mean the number of inferences per second.

Looking forward to hearing more about this.",thank memory fully instance well ca deploy one model instance working find way yes definitely issue one model take multiple instance access unit official device ca split different one may looking see find way around another issue facing issue locally model around cortex model cortex model around bottleneck dev box connection compute capacity model also loaded memory happen performance also suppose mean number per second looking forward hearing,issue,positive,negative,neutral,neutral,negative,negative
713432792,"@RobertLucian Thank you, now gpu memory is not fully utilized on instance as well, but can't deploy more than one model in same ec2 instance, working on it, if I can find any way out.

There is another issue I am facing is FPS issue, locally a model gives around 100 FPS with cortex but while deploying that same model using cortex in us-east-1 in aws env and hitting that model from us-east-1 gives around 30 fps.",thank memory fully instance well ca deploy one model instance working find way another issue facing issue locally model around cortex model cortex model around,issue,positive,neutral,neutral,neutral,neutral,neutral
713214432,"Thanks for the info Ryan. At a glance nothing obvious jumps out to me. Using your cluster info and the cortex yaml, I will try to reproduce your setup and see if I encounter the issue.",thanks glance nothing obvious cluster cortex try reproduce setup see encounter issue,issue,negative,positive,neutral,neutral,positive,positive
713045085,"Hi @vishalbollu, as you expected, the BatchAPI tutorial example gives the same exact problem.  Below is the info you asked for:

Cortex version: 0.20
Cluster info:
`cloudformation stack name                                   status
eksctl-avio-eldiablo-cluster                                CREATE_COMPLETE
eksctl-avio-eldiablo-nodegroup-ng-cortex-operator           CREATE_COMPLETE
eksctl-avio-eldiablo-nodegroup-ng-cortex-worker-on-demand   CREATE_COMPLETE

syncing cluster configuration ...

endpoints:
operator:         ***.elb.us-east-2.amazonaws.com
api load balancer: ***.elb.us-east-2.amazonaws.com
api gateway:       https://***.execute-api.us-east-2.amazonaws.com

fetching cluster status ...

aws access key id:             ****************2F7T
cluster version:               0.20.0
cluster name:                  avio-eldiablo
aws region:                    us-east-2
availability zones:            [us-east-2a, us-east-2b, us-east-2c]
s3 bucket:                     avio-eldiablo-***
instance type:                 m5.large
min instances:                 1
max instances:                 2
tags:                          {""cortex.dev/cluster-name"": ""avio-eldiablo""}
instance volume size (Gi):     50
instance volume type:          gp2
instance volume iops:          <null>
use spot instances:            no
cloudwatch log group:          cortex
subnet visibility:             public
nat gateway:                   none
api load balancer scheme:      internet-facing
operator load balancer scheme: internet-facing
api gateway:                   public
telemetry:                     true
operator image:                cortexlabs/operator:0.20.0
manager image:                 cortexlabs/manager:0.20.0
downloader image:              cortexlabs/downloader:0.20.0
request monitor image:         cortexlabs/request-monitor:0.20.0
cluster autoscaler image:      cortexlabs/cluster-autoscaler:0.20.0
metrics server image:          cortexlabs/metrics-server:0.20.0
inferentia image:              cortexlabs/inferentia:0.20.0
neuron rtd image:              cortexlabs/neuron-rtd:0.20.0
nvidia image:                  cortexlabs/nvidia:0.20.0
fluentd image:                 cortexlabs/fluentd:0.20.0
statsd image:                  cortexlabs/statsd:0.20.0
istio proxy image:             cortexlabs/istio-proxy:0.20.0
istio pilot image:             cortexlabs/istio-pilot:0.20.0
istio citadel image:           cortexlabs/istio-citadel:0.20.0
istio galley image:            cortexlabs/istio-galley:0.20.0

your cluster currently costs $0.29 per hour

aws resource                            cost per hour
1 eks cluster                           $0.10
1 instance for your apis                $0.096 total
1 50gb ebs volume for your apis         $0.007 total
1 t3.medium instance for the operator   $0.0416
1 20gb ebs volume for the operator      $0.003
2 network load balancers                $0.045 total

your cluster has 0 API replicas running across 1 instance

instance type   lifecycle   replicas   CPU (requested / total allocatable)   memory (requested / total allocatable)
m5.large        on-demand   0          0m / 1290m                            0 / 6738608Ki
`

Batch API Configuration:
`
# cortex.yaml

- name: image-classifier
  kind: BatchAPI
  predictor:
    type: python
    path: predictor.py
  compute:
    cpu: 1
`

I have only specified AWS credentials when calling 'cortex cluster up'.  I have not modified the cortex cluster in any way.

Thanks,
-Ryan
",hi tutorial example exact problem cortex version cluster stack name status cluster configuration operator load balancer gateway fetching cluster status access key id cluster version cluster name region availability bucket instance type min instance volume size instance volume type instance volume null use spot log group cortex visibility public nat gateway none load balancer scheme operator load balancer scheme gateway public telemetry true operator image manager image image request monitor image cluster image metric server image image neuron image image image image proxy image pilot image citadel image galley image cluster currently per hour resource cost per hour cluster instance total volume total instance operator volume operator network load total cluster running across instance instance type total allocatable memory total allocatable batch configuration name kind predictor type python path compute calling cluster cortex cluster way thanks,issue,positive,positive,positive,positive,positive,positive
712856134,"@AkashDharani I see what you want to do - basically you don't want `compute.gpu` to take fractional values (as in it's okay if it takes integers), but you want the GPU's memory to not get fully occupied when a model is loaded. For that, you'll have to add that `--per_process_gpu_memory_fraction` option right in here https://github.com/cortexlabs/cortex/blob/master/pkg/operator/operator/k8s.go#L668.

But keep in mind that for AWS deployments, even though a GPU's memory is not fully utilized, that GPU unit will still be allocated to a single API replica - meaning that the rest of the GPU's available memory can't be shared with other replicas - but only within the processes of a single API replica.

For that, a device plugin that supports fractional values for the GPU is required - as far as we know, the official device plugin for Nvidia GPUs doesn't support that, but there may be 3rd party ones that do this - you can check out the links from the ticket's description in here https://github.com/cortexlabs/cortex/issues/1390.",see want basically want take fractional want memory get fully model loaded add option right keep mind even though memory fully unit still single replica meaning rest available memory ca within single replica device fractional far know official device support may party check link ticket description,issue,positive,positive,positive,positive,positive,positive
712821825,@imagine3D-ai you still need to add a payload - the minimum you have to add to your payload if the content type is set to JSON is `{}`. Here is a [guide](https://docs.cortex.dev/deployments/realtime-api/predictors#api-requests) to walk you through all content types supported by Cortex.,still need add minimum add content type set guide walk content cortex,issue,negative,neutral,neutral,neutral,neutral,neutral
712758126,"> @da-source to make it clear, `no healthy upstream` is returned when no payload is sent to the server.
> 
> At the very least, your command has to look like this:
> 
> ```shell
> curl https://s8u99tpqa2.execute-api.us-west-2.amazonaws.com/iris-classifier -X POST -H ""Content-Type: application/json"" -d '{}'
> ```
> 
> where `https://s8u99tpqa2.execute-api.us-west-2.amazonaws.com/iris-classifier` is the API's endpoint.
> 
> When running `cortex get <api-name>`, you will be encountered with a curl example for your specific API deployment. You can copy-paste it and then provide a `sample.json` to it to make a prediction.

What if my api doesn‚Äôt need a payload?",make clear healthy upstream returned sent server least command look like shell curl post running cortex get curl example specific deployment provide make prediction need,issue,positive,positive,neutral,neutral,positive,positive
712752546,"@RobertLucian I am facing this gpu ram issue while deploying the model in cluster on ec2 instance. Locally it is acquiring only 0.25 percent of ram but on ec2 instance it acquires whole available ram.

https://github.com/cortexlabs/cortex/blob/c6a3894009dae55de5cec68deb97c7d832514da0/cli/local/docker_spec.go#L337-L340

updated it to

> cmdArgs := []string{
		""--port="" + _tfServingPortStr,
		""--per_process_gpu_memory_fraction=0.25"",
		""--model_config_file="" + _tfServingEmptyModelConfig,
	}


But it is effecting the local deployment not on cloud deployment.
",facing ram issue model cluster instance locally percent ram instance whole available ram string local deployment cloud deployment,issue,negative,positive,positive,positive,positive,positive
712445822,"You are correct, publishing the metrics is done behind the scenes, so your code doesn't need to handle that. It would also be helpful if you could run the BatchAPI tutorial and submit a job. I expect you would run into the same issue, but it would be great to double check.",correct metric done behind code need handle would also helpful could run tutorial submit job expect would run issue would great double check,issue,positive,positive,positive,positive,positive,positive
712432467,"Are these metrics something I need to handle in my code?  I'm simply following the BatchAPI tutorial with my own PythonPredictor implementation.  I specify AWS keys each time I spin up the cluster with 'cortex cluster up'.

I will send over the other info tonight, but I've done nothing custom.",metric something need handle code simply following tutorial implementation specify time spin cluster cluster send tonight done nothing custom,issue,negative,neutral,neutral,neutral,neutral,neutral
712408751,"It looks like the error you encountered happens when Cortex is trying to cleanup job resources after a job has completed. In this scenario, Cortex has realized that the queue is empty so it the job is likely complete and it should clean up the job. It double checks the metrics to make sure that the job is indeed complete, but the metrics indicate that the job is still in progress. For some reason Cortex is getting 0 for the number of batches completed when it should be equal to the total of number of batches in the job.

There are two areas where this can go wrong:
* the workers are not to pushing metrics to the expected location
* the cortex operator is unable to read metrics from the expected location

To help debug, it would be great if you could provide the following:
* What version of Cortex are you using?
* The output for `cortex cluster info` (mask any sensitive values with ""***"")
* The yaml configuration for your batch api (`cortex get <batch api name>`) (mask any sensitive values with ""***"")
* Have you specified a different set of credentials for the cortex operator (e.g. cortex_aws_access_key_id, cortex_aws_secret_access_key)
* Have you modified the Cortex cluster in some way (edited config maps, redirected statsd agent, etc.)




",like error cortex trying cleanup job job scenario cortex queue empty job likely complete clean job double metric make sure job indeed complete metric indicate job still progress reason cortex getting number equal total number job two go wrong pushing metric location cortex operator unable read metric location help would great could provide following version cortex output cortex cluster mask sensitive configuration batch cortex get batch name mask sensitive different set cortex operator cortex cluster way agent,issue,positive,positive,neutral,neutral,positive,positive
712294964,"@da-source to make it clear, `no healthy upstream` is returned when no payload is sent to the server.

At the very least, your command has to look like this:
```bash
curl https://s8u99tpqa2.execute-api.us-west-2.amazonaws.com/iris-classifier -X POST -H ""Content-Type: application/json"" -d '{}'
```
where `https://s8u99tpqa2.execute-api.us-west-2.amazonaws.com/iris-classifier` is the API's endpoint.

When running `cortex get <api-name>`, you will be encountered with a curl example for your specific API deployment. You can copy-paste it and then provide a `sample.json` to it to make a prediction.",make clear healthy upstream returned sent server least command look like bash curl post running cortex get curl example specific deployment provide make prediction,issue,positive,positive,neutral,neutral,positive,positive
712269443,"> You can use the command `cortex logs <API_NAME>` to print logs for your API which could help you debug the errors you are running into.
> 
> As for why your Predictor is working in Colab and not in Cortex, Jupyter notebooks and Colabs support convenience features that are not native to Python such as being able to run bash commands with `!<bash command>`.
> 
> When you run `cortex deploy`, Cortex runs your predictor code in a vanilla Python runtime (python 3.6.9 to be precise) and therefore does not support additional convenience features provided by Jupyter notebooks and Colab.
> 
> You can write `!chmod +rx ./mymodel` in python. For example:
> 
> ```python
> import subprocess
> import os
> import boto3
> import sys
> import stat
> 
> class PythonPredictor:
>     def __init__(self, config):
>         s3 = boto3.client('s3', aws_access_key_id='mykey',
>         aws_secret_access_key='secretkey')
>         s3.download_file(config['bucket'], config['key'], ""./mymodel.bin"")
>         st = os.stat('./mymodel')
>         os.chmod('./mymodel', st.st_mode | stat.S_IEXEC ) # this should be the equivalent to chmod +x ./mymodel and make your model.bin executable
>         print(""done"")
> 
>     def predict(self):
>         command = ['./mymodel','-m', 'M', '-l', '20', 'g', 'hi']
>         process = subprocess.Popen(command, stdout=subprocess.PIPE, universal_newlines=True)
>         while True:
>             output = process.stdout.readline()
>             if output == '' and process.poll() is not None:
>                 break
>             if output:
>                 print(output.strip())
>         rc = process.poll()
>         return rc
> ```
> 
> The code above should work in your Colab and in a Python-only runtime such as Cortex.

Thanks a lot, that solved it!
But now I'm getting `no healthy upstream` error when I try to access and get output from my endpoint URL. Since my code doesn't need a payload I just did:
`curl https://myendpoint/mymodel`
`no healthy upstream`

API is live and up-to-date though. What can be causing this message? ",use command cortex print could help running predictor working cortex support convenience native python able run bash bash command run cortex deploy cortex predictor code vanilla python python precise therefore support additional convenience provided write python example python import import o import import import class self st equivalent make executable print done predict self command process command true output output none break output print return code work cortex thanks lot getting healthy upstream error try access get output since code need curl healthy upstream live though causing message,issue,positive,positive,positive,positive,positive,positive
711518612,"Turns out the problem was a Spot Instance Request that kept running. I
fixed it by going to Spot Instances and terminating the request from there.

On Sun, 18 Oct 2020 at 21:31, David Eliahu <notifications@github.com> wrote:

> @imagine3D-ai <https://github.com/imagine3D-ai> If you go to the Cloud
> Formation dashboard in the AWS console (e.g. for us-west-2 it would be:
> https://console.aws.amazon.com/cloudformation/home?region=us-west-2), do
> you see an stacks that are associated with cortex? If so, delete them one
> at a time by selecting a stack and then clicking ""Delete"" at the top.
>
> Please let us know if that doesn't fix it.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1463#issuecomment-711407561>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQQZJ3BMKQWHZJTIIA6BK7LSLM7B3ANCNFSM4SVAWTYQ>
> .
>
",turn problem spot instance request kept running fixed going spot request sun wrote go cloud formation dashboard console would see associated cortex delete one time stack delete top please let u know fix reply directly view,issue,negative,positive,positive,positive,positive,positive
711410486,"@da-source Without having the full context of your application, to me this seems like a good use case for a [batch API](https://docs.cortex.dev/deployments/batch-api) (instead of a realtime API), since with batch APIs in cortex, no instances are required until a request comes in, and the instance(s) spin down after the job is completed. There will still be a delay of a few minutes for the instance to spin down.",without full context application like good use case batch instead since batch cortex request come instance spin job still delay instance spin,issue,negative,positive,positive,positive,positive,positive
711407561,"@imagine3D-ai If you go to the Cloud Formation dashboard in the AWS console (e.g. for us-west-2 it would be: https://console.aws.amazon.com/cloudformation/home?region=us-west-2), do you see an stacks that are associated with cortex? If so, delete them one at a time by selecting a stack and then clicking ""Delete"" at the top.

Please let us know if that doesn't fix it.",go cloud formation dashboard console would see associated cortex delete one time stack delete top please let u know fix,issue,negative,positive,positive,positive,positive,positive
711156100,"> I've deployed a cluster using `cortex cluster up` and even after shutting it down, 1 EC2 instance keeps coming back. I tried terminating it but it restarts a few minutes later, and I'm being billed this whole time. How can I shut this instance down for good?

I try to select Environments in elastic beanstalk but it keeps throwing me back to the welcome page, so I cant open it:https://console.aws.amazon.com/elasticbeanstalk/home?region=us-east-1#/welcome",cluster cortex cluster even shutting instance coming back tried later billed whole time shut instance good try select elastic beanstalk throwing back welcome page cant open,issue,positive,positive,positive,positive,positive,positive
711019690,"You can use the command `cortex logs <API_NAME>` to print logs for your API which could help you debug the errors you are running into.

As for why your Predictor is working in Colab and not in Cortex, Jupyter notebooks and Colabs support convenience features that are not native to Python such as being able to run bash commands with  `!<bash command>`. 

When you run `cortex deploy`, Cortex runs your predictor code in a vanilla Python runtime (python 3.6.9 to be precise) and therefore does not support additional convenience features provided by Jupyter notebooks and Colab.

You can write `!chmod +rx ./mymodel` in python. For example:
```python
import subprocess
import os
import boto3
import sys
import stat

class PythonPredictor:
    def __init__(self, config):
        s3 = boto3.client('s3', aws_access_key_id='mykey',
        aws_secret_access_key='secretkey')
        s3.download_file(config['bucket'], config['key'], ""./mymodel.bin"")
        st = os.stat('./mymodel')
        os.chmod('./mymodel', st.st_mode | stat.S_IEXEC ) # this should be the equivalent to chmod +x ./mymodel and make your model.bin executable
        print(""done"")

    def predict(self):
        command = ['./mymodel','-m', 'M', '-l', '20', 'g', 'hi']
        process = subprocess.Popen(command, stdout=subprocess.PIPE, universal_newlines=True)
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                print(output.strip())
        rc = process.poll()
        return rc
```

The code above should work in your Colab and in a Python-only runtime such as Cortex.",use command cortex print could help running predictor working cortex support convenience native python able run bash bash command run cortex deploy cortex predictor code vanilla python python precise therefore support additional convenience provided write python example python import import o import import import class self st equivalent make executable print done predict self command process command true output output none break output print return code work cortex,issue,positive,positive,positive,positive,positive,positive
709656688,@dakshvar22 got it. I'll close this ticket for now as we already have #927 where we can track this.,got close ticket already track,issue,negative,neutral,neutral,neutral,neutral,neutral
709559537,"@RobertLucian Thanks a lot for the answers. I would definitely prefer having the requests re-routed automatically, or atleast making that option configurable.",thanks lot would definitely prefer automatically making option,issue,positive,positive,neutral,neutral,positive,positive
709476443,"Thanks. Do you mind sending the output of:

```
aws service-quotas list-service-quotas --service-code ec2 --region eu-central-1 | grep -B 14 -A 7 Standard/OnDemand
```",thanks mind sending output region,issue,negative,positive,positive,positive,positive,positive
709462349,"I checked via EC2 dashboard and it says that my limit for ALL Standard Spot
instance is 36vCPUs and for On-Demand All Standard the limit is 42vCPUs.

On Wed, 14 Oct 2020 at 19:59, David Eliahu <notifications@github.com> wrote:

> @mutal <https://github.com/mutal> I'm glad to hear us-east-1 is working
> for you.
>
> Regarding eu-central-1, this should also work, so it would be good to
> figure out why it's not. Do you mind checking your service quota for the
> Standard/OnDemand in eu-central-1? You can do this from the AWS console by
> changing your region, clicking on ""Limits"" in the EC2 dashboard, and then
> searching for ""standard"" (see above for a screen shot). You can also check
> it by running this command:
>
> aws service-quotas list-service-quotas --service-code ec2 --region eu-central-1 | grep -B 14 -A 7 Standard/OnDemand
>
> If the value is 0, then the error that cortex shows is expected, but it
> it's not 0, then there is some other issue which we can look into.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1438#issuecomment-708566354>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ADFOXIRC5YOMOPPIKUHGGATSKXRJLANCNFSM4SPMIFPA>
> .
>
",checked via dashboard limit standard spot instance standard limit wed wrote glad hear working regarding also work would good figure mind service quota console region dashboard searching standard see screen shot also check running command region value error cortex issue look reply directly view,issue,positive,positive,positive,positive,positive,positive
709359864,"@AkashDharani I see. In that case, you'd probably want to change this to a float.

https://github.com/cortexlabs/cortex/blob/c6a3894009dae55de5cec68deb97c7d832514da0/pkg/types/spec/validations.go#L339-L345",see case probably want change float,issue,negative,neutral,neutral,neutral,neutral,neutral
709327390,"@RobertLucian I got your point, but as I am building the cortex from source, so I can change the source code and make able the cortex to work with float values in `compute.gpu` as well, thats what I was asking, can you exactly tell me the name of the file where `cortex.yaml` is getting loaded into our code so that I can remove this exception from `compute.gpu` of being integer.",got point building cortex source change source code make able cortex work float well thats exactly tell name file getting loaded code remove exception integer,issue,negative,positive,positive,positive,positive,positive
709323237,"@AkashDharani when you do `cortex deploy`, it looks in the current directory and checks if there's a config called `cortex.yaml`. Otherwise, if you manually specify a config like `cortex deploy this_config.yaml`, then this `this_config.yaml` config will still have to be found in the same directory from which the CLI command is run.

The `cortex.yaml` config template for each predictor type can be found [here](https://docs.cortex.dev/deployments/realtime-api/api-configuration).

Aside from the error you are currently getting, for the `compute.gpu` field you can only assign integer values. Fractional values are not yet handled. We have a ticket for it to track it https://github.com/cortexlabs/cortex/issues/1390.

And also, to speed up the process, in case you didn't get it to work, could you also post your `cortex.yaml` API config here?",cortex deploy current directory otherwise manually specify like cortex deploy still found directory command run template predictor type found aside error currently getting field assign integer fractional yet handled ticket track also speed process case get work could also post,issue,negative,neutral,neutral,neutral,neutral,neutral
709289581,"@RobertLucian  where exactly cortex.yaml file is getting read and loaded into api variable in go files, because I am facing error while reading `gpu: 0.25`, it is not supported `compute: key ""gpu"" is not supported` 
I want to update the `cortex.yaml` file and go files which are loading` cortex.yaml` file into api variable as well.",exactly file getting read loaded variable go facing error reading compute key want update file go loading file variable well,issue,negative,positive,positive,positive,positive,positive
709185191,"@deliahu Yes, I have already worked on tensorflow/serving, it does work and in cortex dev `--per_process_gpu_memory_fraction` is also working, but I am trying to adjust this functionality in `cortex.yaml` file.",yes already worked work cortex dev also working trying adjust functionality file,issue,negative,neutral,neutral,neutral,neutral,neutral
708881076,"@dakshvar22 here are the answers to your questions:

1. In-flight requests that are being processed will hang indefinitely and in-flight requests that are in the queue will return 503 errors, but not necessarily immediately. Unfortunately, they are not being redirected to other API replicas residing on other instances. The remedy to this is to implement retries and timeouts for your application.
1. I think 20% is still okay as long as there are enough spare instances in your region(s). When selecting the spot instance type, there are a couple of criteria: the region, the interruption rate, the cost savings, and the available resources on that instance type. AWS has an advisor for that [here](https://aws.amazon.com/ec2/spot/instance-advisor/). You can use this advisor to judge which instances (`inf1.*`, `g4dn.*`) make more sense (economically, performance-wise, etc) in the region(s) you are considering.

Does this answer your question? Would you much prefer having the requests re-routed/re-tried automatically?

Also, we do have a ticket to improve the transitioning process of in-flight requests to healthy instances here https://github.com/cortexlabs/cortex/issues/927.",indefinitely queue return necessarily immediately unfortunately remedy implement application think still long enough spare region spot instance type couple criterion region interruption rate cost available instance type advisor use advisor judge make sense economically region considering answer question would much prefer automatically also ticket improve process healthy,issue,negative,positive,positive,positive,positive,positive
708858958,"@dakshvar22 I have a few follow up questions:

1. Would you be able to post your API's logs? If they include sensitive information, you can also email them to us at dev@cortex.dev.
1. How are you determining how many processes are running?
1. If you set `processes_per_replica` to 2, do you get 2 processes?
1. If you run a different model, e.g. [examples/pytorch/iris-classifier](https://github.com/cortexlabs/cortex/tree/0.20/examples/pytorch/iris-classifier), are you able to get 3 processes?",follow would able post include sensitive information also u dev many running set get run different model able get,issue,negative,positive,positive,positive,positive,positive
708699318,"No problem, keep us posted on whether this fixes it.

Also, we are tracking this in https://github.com/cortexlabs/cortex/issues/936, which when resolved, will have cortex surface this kind of issue directly to the user.",problem keep u posted whether also resolved cortex surface kind issue directly user,issue,negative,positive,positive,positive,positive,positive
708696427,Exactly! I've requested more vCPU limit now. Thanks for your time. i'll test this again once I have the limits approved.,exactly limit thanks time test,issue,negative,positive,positive,positive,positive,positive
708696067,"@dakshvar22 my understanding is that this is just poorly worded by AWS, and that it actually is the vCPU limit. Since your limit is 5, this is in line with the behavior we are seeing (since 1 g4dn.xlarge has 4 vCPU)",understanding poorly worded actually limit since limit line behavior seeing since,issue,negative,negative,negative,negative,negative,negative
708696066,"Are the instance limits that I enter in the form actually vCPU limits? I guess one G4DN.xlarge machine takes up 4vCPU, so my current limit is being reached whenever a new instance is to be spawned.",instance enter form actually guess one machine current limit whenever new instance,issue,negative,positive,neutral,neutral,positive,positive
708694334,"@deliahu I think I now spot the problem in the activity history of the autoscaling group. I see the following description in a lot of `FAILED` events - 
```
Launching a new EC2 instance. Status Reason: You have requested more vCPU capacity than your current vCPU limit of 5 allows for the instance bucket that the specified instance type belongs to. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed.
```
However, I am not sure what to do now, bc when i go to `http://aws.amazon.com/contact-us/ec2-request` and select G type instances, it only lets me configure the instance limit which is set to 5 right now. How do I increase the vCPU limit?

In the meantime, I have also sent you the cluster logs via email.",think spot problem activity history group see following description lot new instance status reason capacity current limit instance bucket instance type please visit request adjustment limit instance however sure go select type configure instance limit set right increase limit also sent cluster via,issue,negative,positive,positive,positive,positive,positive
708689655,"@dakshvar22 Depending on a few factors, it is possible that it could take > 5 minutes for the new instances to be ready and for the replicas to be live (see [here](https://docs.cortex.dev/deployments/realtime-api/autoscaling#a-note-about-autoscaling-responsiveness)).

In your AWS EC2 dashboard, do are there multiple instances that are initializing (or running)? If not, the other thing I would check is the activity history of the autoscaling group (see [here](https://docs.cortex.dev/troubleshooting/stuck-updating#check-your-aws-auto-scaling-group-activity-history) for instructions)",depending possible could take new ready live see dashboard multiple running thing would check activity history group see,issue,negative,positive,neutral,neutral,positive,positive
708687771,"@deliahu Thanks for the quick response. I am attaching logs of the command `cortex get` command - 
```
$ cortex get paraphraser --env aws                                                                                                   12:10:54am CEST

status   up-to-date   requested   last update   avg request   2XX
live     1            3           11m           35.2452 s     70

metrics dashboard: https://us-east-1.console.aws.amazon.com/cloudwatch/home#dashboards:name=test

endpoint: https://g0v3cy2m69.execute-api.us-east-1.amazonaws.com/paraphraser
example curl: curl https://g0v3cy2m69.execute-api.us-east-1.amazonaws.com/paraphraser -X POST -H ""Content-Type: application/json"" -d @sample.json

configuration
name: paraphraser
kind: RealtimeAPI
predictor:
  type: python
  path: predictor.py
  processes_per_replica: 3
  threads_per_process: 1
  image: dakshvar22/cortex_custom:hub
networking:
  endpoint: /paraphraser
  api_gateway: public
compute:
  cpu: 200m
  gpu: 1
  mem: 14G
autoscaling:
  min_replicas: 1
  max_replicas: 4
  init_replicas: 1
  target_replica_concurrency: 3.0
  max_replica_concurrency: 1024
  window: 10s
  downscale_stabilization_period: 5m0s
  upscale_stabilization_period: 10s
  max_downscale_factor: 0.75
  max_upscale_factor: 2.0
  downscale_tolerance: 0.05
  upscale_tolerance: 0.05
update_strategy:
  max_surge: 0
  max_unavailable: 25%

```
> how long was / has been the cluster been in the state where it requests 3 API replicas but has only 1 instance?

it stays the same for the whole 5 mins of when the requests are triggered.",thanks quick response command cortex get command cortex get paraphraser cest status last update request live metric dashboard example curl curl post configuration name paraphraser kind predictor type python path image hub public compute mem window long cluster state instance stay whole triggered,issue,positive,positive,positive,positive,positive,positive
708686806,"@dakshvar22 I'd also be happy to jump on a call to research this if you're interested, feel free to email me at david@cortex.dev",also happy jump call research interested feel free,issue,positive,positive,positive,positive,positive,positive
708686280,"@dakshvar22 Thank you for reaching out.

I will try to reproduce this. In the meantime, do you mind sending the zip file which is generated by running `cortex cluster info --debug` to us at dev@cortex.dev? This will include the cluster autoscaler logs and some AWS logs, which will be helpful while we're debugging.

Also, I am assuming that this is true, but would like to confirm: If you run `cortex get` do you see that there are 3 replicas requested (but only 1 is ""live"")?

Lastly, how long was / has been the cluster been in the state where it requests 3 API replicas but has only 1 instance?",thank reaching try reproduce mind sending zip file running cortex cluster u dev include cluster helpful also assuming true would like confirm run cortex get see live lastly long cluster state instance,issue,positive,positive,positive,positive,positive,positive
708575899,"Yes, $0.24/hour will be the cost of the cluster if there is a single instance running (your minimum). If 5 instances are running (your maximum), it will be $0.43/hour.

There is not a way to shut down instances based on request load. There are a few things that you can do to help reduce costs:

* If you delete an API with `cortex delete <api name>`, then it will allow the API instances to scale down. You could set min instances to 0, and then the fixed cost of the cluster if no APIs are deployed should be ~$0.20 per hour.
* If you have APIs running which cannot be deleted, you can consider using [spot instances](https://docs.cortex.dev/cluster-management/spot-instances). This will reduce the cost of running your API instances.
* If you don't have a high request load and have multiple APIs running, you can ""fit"" multiple APIs on a single instance by adjusting the compute request for each API. It is also possible to serve multiple models from a single API if desired (here is [an example](https://github.com/cortexlabs/cortex/blob/master/examples/onnx/multi-model-classifier)).

In any case, there is not currently a way to reduce the fixed cost of the cluster below ~$0.20/hour, without spinning down the cluster via `cortex cluster down`.",yes cost cluster single instance running minimum running maximum way shut based request load help reduce delete cortex delete name allow scale could set min fixed cost cluster per hour running consider spot reduce cost running high request load multiple running fit multiple single instance compute request also possible serve multiple single desired example case currently way reduce fixed cost cluster without spinning cluster via cortex cluster,issue,positive,positive,neutral,neutral,positive,positive
708571012,"@imagine3D-ai yes you can deploy multiple APIs in a single cluster. There are three options:

1. You can have multiple APIs in a single `cortex.yaml` file, like in [this example](https://github.com/cortexlabs/cortex/blob/master/examples/traffic-splitter/cortex.yaml) (ignore the API of `kind: TrafficSplitter` if that is not relevant to you)
1. You can have two separate files, e.g. `api1.yaml` and `api2.yaml`, and then use `cortex deploy api1.yaml --env aws` and `cortex deploy api2.yaml --env aws`
1. If your models don't share dependencies, it might be cleaner to have two directories, each with a single `cortex.yaml` file (e.g. `model1/cortex.yaml` and `model2/cortex.yaml`. Then you can `cd` into each directory and deploy the API with `cortex deploy --env aws`, or from the parent directory you can use `cortex deploy model1/cortex.yaml --env aws` and `cortex deploy model2/cortex.yaml --env aws`",yes deploy multiple single cluster three multiple single file like example ignore kind relevant two separate use cortex deploy cortex deploy share might cleaner two single file directory deploy cortex deploy parent directory use cortex deploy cortex deploy,issue,positive,positive,positive,positive,positive,positive
708566354,"@mutal I'm glad to hear us-east-1 is working for you.

Regarding eu-central-1, this should also work, so it would be good to figure out why it's not. Do you mind checking your service quota for the Standard/OnDemand in eu-central-1? You can do this from the AWS console by changing your region, clicking on ""Limits"" in the EC2 dashboard, and then searching for ""standard"" (see above for a screen shot). You can also check it by running this command:

```bash
aws service-quotas list-service-quotas --service-code ec2 --region eu-central-1 | grep -B 14 -A 7 Standard/OnDemand
```

If the value is 0, then the error that cortex shows is expected, but it it's not 0, then there is some other issue which we can look into.",glad hear working regarding also work would good figure mind service quota console region dashboard searching standard see screen shot also check running command bash region value error cortex issue look,issue,positive,positive,positive,positive,positive,positive
708509670,"> @mutal thanks for reaching out.
> 
> It seems that there is a bug/inconsistency in the AWS service quotas API (which cortex uses behind the scenes) in eu-north-1, but I would like to confirm on your end too.
> 
> For me, when I check my AWS EC2 dashboard in eu-north-1, I see that I have 5 vCPUs of quota for ""standard"" instances:
> 
> ![image](https://user-images.githubusercontent.com/808475/95909674-be3c4680-0d53-11eb-9112-1d4ed30850ec.png)
> 
> However, when I run this command, it comes up empty:
> 
> ```
> $ aws service-quotas list-service-quotas --service-code ec2 --region eu-north-1 | grep -B 14 -A 7 ""Standard/OnDemand""
> ```
> 
> For comparison, in us-west-2, I see that I have 1280 vCPU:
> 
> ![image](https://user-images.githubusercontent.com/808475/95910103-66520f80-0d54-11eb-81e3-005005ab2825.png)
> 
> Which agrees with what the CLI command says:
> 
> ```shell
> $ aws service-quotas list-service-quotas --service-code ec2 --region us-west-2 | grep -B 14 -A 7 Standard/OnDemand
> 
>         {
>             ""ServiceCode"": ""ec2"",
>             ""ServiceName"": ""Amazon Elastic Compute Cloud (Amazon EC2)"",
>             ""QuotaArn"": ""arn:aws:servicequotas:us-west-2:764403040460:ec2/L-1216C47A"",
>             ""QuotaCode"": ""L-1216C47A"",
>             ""QuotaName"": ""Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances"",
>             ""Value"": 1280.0,
>             ""Unit"": ""None"",
>             ""Adjustable"": true,
>             ""GlobalQuota"": false,
>             ""UsageMetric"": {
>                 ""MetricNamespace"": ""AWS/Usage"",
>                 ""MetricName"": ""ResourceCount"",
>                 ""MetricDimensions"": {
>                     ""Class"": ""Standard/OnDemand"",
>                     ""Resource"": ""vCPU"",
>                     ""Service"": ""EC2"",
>                     ""Type"": ""Resource""
>                 },
>                 ""MetricStatisticRecommendation"": ""Maximum""
>             }
>         },
> ```
> 
> Can you confirm that you notice the same discrepancy on your account?
> 
> If so, one workaround could be to try a different region, for example eu-central-1 (which I have used before, so I expect will work)

I tried eu-central-1 but it threw the same error but us-east-1 worked. I wonder why could that be?",thanks reaching service cortex behind would like confirm end check dashboard see quota standard image however run command come empty region comparison see image command shell region elastic compute cloud arn running standard value unit none adjustable true false class resource service type resource maximum confirm notice discrepancy account one could try different region example used expect work tried threw error worked wonder could,issue,negative,negative,neutral,neutral,negative,negative
708031417,@AkashDharani were you able to get one of the two approaches we mentioned working?,able get one two working,issue,negative,positive,positive,positive,positive,positive
707988686,"Note: I confirmed that in regions where the `list-service-quotas` API is fully supported, there is an entry for instance types for which the user has zero quota",note confirmed fully entry instance user zero quota,issue,negative,positive,positive,positive,positive,positive
707978409,"@mutal thanks for reaching out.

It seems that there is a bug/inconsistency in the AWS service quotas API (which cortex uses behind the scenes) in eu-north-1, but I would like to confirm on your end too.

For me, when I check my AWS EC2 dashboard in eu-north-1, I see that I have 5 vCPUs of quota for ""standard"" instances:

![image](https://user-images.githubusercontent.com/808475/95909674-be3c4680-0d53-11eb-9112-1d4ed30850ec.png)

However, when I run this command, it comes up empty:

```
$ aws service-quotas list-service-quotas --service-code ec2 --region eu-north-1 | grep -B 14 -A 7 ""Standard/OnDemand""
```

For comparison, in us-west-2, I see that I have 1280 vCPU:

![image](https://user-images.githubusercontent.com/808475/95910103-66520f80-0d54-11eb-81e3-005005ab2825.png)

Which agrees with what the CLI command says:

```bash
$ aws service-quotas list-service-quotas --service-code ec2 --region us-west-2 | grep -B 14 -A 7 Standard/OnDemand

        {
            ""ServiceCode"": ""ec2"",
            ""ServiceName"": ""Amazon Elastic Compute Cloud (Amazon EC2)"",
            ""QuotaArn"": ""arn:aws:servicequotas:us-west-2:764403040460:ec2/L-1216C47A"",
            ""QuotaCode"": ""L-1216C47A"",
            ""QuotaName"": ""Running On-Demand Standard (A, C, D, H, I, M, R, T, Z) instances"",
            ""Value"": 1280.0,
            ""Unit"": ""None"",
            ""Adjustable"": true,
            ""GlobalQuota"": false,
            ""UsageMetric"": {
                ""MetricNamespace"": ""AWS/Usage"",
                ""MetricName"": ""ResourceCount"",
                ""MetricDimensions"": {
                    ""Class"": ""Standard/OnDemand"",
                    ""Resource"": ""vCPU"",
                    ""Service"": ""EC2"",
                    ""Type"": ""Resource""
                },
                ""MetricStatisticRecommendation"": ""Maximum""
            }
        },
```

Can you confirm that you notice the same discrepancy on your account?

If so, one workaround could be to try a different region, for example eu-central-1 (which I have used before, so I expect will work)",thanks reaching service cortex behind would like confirm end check dashboard see quota standard image however run command come empty region comparison see image command bash region elastic compute cloud arn running standard value unit none adjustable true false class resource service type resource maximum confirm notice discrepancy account one could try different region example used expect work,issue,positive,negative,neutral,neutral,negative,negative
707930611,"@dakshvar22 In addition the the information requested by @deliahu, it would also be helpful if you can share the output `docker info`.
 ",addition information would also helpful share output docker,issue,positive,neutral,neutral,neutral,neutral,neutral
707494217,"@BogdanDarius I'm glad to hear that it's working for you now.

I've created https://github.com/cortexlabs/cortex/pull/1436 so that in the future, choosing an instance type that is not supported by AWS NLB's target groups will be blocked.

Thanks for bringing this to our attention!",glad hear working future choosing instance type target blocked thanks attention,issue,positive,positive,positive,positive,positive,positive
707480092,"@grzeczko Another thing you can try is to uninstall docker on your Mac, and re-install it. That seems to have helped some people on this thread: https://github.com/docker/for-win/issues/5763",another thing try docker mac people thread,issue,negative,neutral,neutral,neutral,neutral,neutral
707192425,"@AkashDharani I confirm the following two things:

1. That indeed the TFS server reserves all of the GPU's memory.
1. That passing the `--per_process_gpu_memory_fraction` inside the indicated `run.sh` script does indeed limit the memory usage.

I've tried this using the [iris-classifier](https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/iris-classifier).",confirm following two indeed server memory passing inside script indeed limit memory usage tried,issue,negative,neutral,neutral,neutral,neutral,neutral
707176494,"@grzeczko yeah, it seems like on Mac, that directory is within the user's home directory - like I read here on SO https://stackoverflow.com/questions/19234831/where-are-docker-images-stored-on-the-host-machine/37642236#37642236.

I think that the best way to reset Docker is to go through this: https://docs.docker.com/docker-for-mac/#reset.

Let us know if that worked for you!",yeah like mac directory within user home directory like read think best way reset docker go let u know worked,issue,positive,positive,positive,positive,positive,positive
707165259,"@RobertLucian The /var/lib/docker directory does not exist on my Mac.  I've restarted my Docker and tried again with no luck. 
I'm currently on MacOS Catalina v10.15.7
MacBook Pro 2.3GHz Quad-Core Intel Core i5
Memory: 16GB 2133 MHz LPDDR3
Graphics: Intel Iris Plus Graphics 655 1536 MB

The above is the graphic card I have installed. I don't have an NVidia card, therefore I set gpu to 0.
Thank you.",directory exist mac docker tried luck currently catalina pro core memory graphic iris plus graphic graphic card card therefore set thank,issue,positive,neutral,neutral,neutral,neutral,neutral
707160675,"@AkashDharani in that case, you could add in the `--per_process_gpu_memory_fraction` flag to [run.sh](https://github.com/cortexlabs/cortex/blob/0.20/images/tensorflow-serving-gpu/run.sh#L31), which is the entry-point that is run when the TFS server is brought up. The associated Dockerfile for the TFS for the GPU is [this one](https://github.com/cortexlabs/cortex/blob/0.20/images/tensorflow-serving-gpu/Dockerfile) (which is residing in the same directory as `run.sh`). For building this image, make sure you do this from the root directory of the repo.

Once your custom image is built, add it to the `tensorflow_serving_image` field in your API's `cortex.yaml` config:
```yaml
- name: <your-API>
  kind: RealtimeAPI
  predictor:
    type: tensorflow
    # ...
    tensorflow_serving_image: <string> # docker image to use for the TensorFlow Serving container (default: cortexlabs/tensorflow-serving-gpu or cortexlabs/tensorflow-serving-cpu based on compute)
    # ...
```

For a bit more context, all of the available flags that can be passed to `tensorflow_model_server` are documented here https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc#L59.

I haven't tried this, but I reckon this would work. I'm curious to see if it limits the memory usage on the GPU. Please, let us know.",case could add flag run server brought associated one directory building image make sure root directory custom image built add field name kind predictor type string docker image use serving container default based compute bit context available tried reckon would work curious see memory usage please let u know,issue,positive,positive,positive,positive,positive,positive
707146451,"@imagine3D-ai to deploy to AWS (using Cortex), you'll have to first spin up a cluster and then deploy to it your API.

To spin up a cluster, you can either use the CLI's prompt for the `cortex cluster up` command to feed in your cluster's specs (name, instance type, region, etc) or you can provide your own `cluster.yaml` config that offers more granular control over your cluster's specs. Here's the [template](https://docs.cortex.dev/cluster-management/config) for the `cluster.yaml` config. And [this is](https://docs.cortex.dev/miscellaneous/cli#cluster-up) the spec for the `cortex cluster up` command. If you want to provide the `-e` option to the command to give a specific name to your environment, you might also want to go through the [environments](https://docs.cortex.dev/miscellaneous/environments) docs.

Once the cluster is up, you can then just go to an example and deploy it to the AWS cluster - when deploying to AWS, you'll need to specify the environment to use (the default one for clusters is the `aws` one, so you'll have to specify that).

We also have a [simple tutorial](https://docs.cortex.dev/v/0.20/deployments/realtime-api/text-generator#deploy-your-model-to-aws) involving the iris classifier API here that shows you how to deploy to AWS.

",deploy cortex first spin cluster deploy spin cluster either use prompt cortex cluster command feed cluster spec name instance type region provide granular control cluster spec template spec cortex cluster command want provide option command give specific name environment might also want go cluster go example deploy cluster need specify environment use default one one specify also simple tutorial iris classifier deploy,issue,negative,positive,neutral,neutral,positive,positive
707136049,"@imagine3D-ai thanks for letting us know. We also have a ticket to support the CLI to run on Windows https://github.com/cortexlabs/cortex/issues/715, so this is on our radar.",thanks u know also ticket support run radar,issue,positive,positive,positive,positive,positive,positive
707124443,"@grzeczko regarding the `failed to register layer: error creating overlay mount to /var/lib/docker/overlay2/b5437b769fe6aeb2e2a3684ae861ae6b8e64abb1692cfb89da268cfd151c8f9b/merged: too many levels of symbolic links` error, I tried to replicate it too, but without success - the example worked fine for me.

What I think it's happening is that the docker service may have gotten stuck in a broken state - I'd recommend stopping the docker service, clearing the `/var/lib/docker` directory, and then restarting the service.

---

As for the original issue, the questions are:

1. Does your machine have access to at least 1x Nvidia GPU (with all of its drivers installed, aka `please make sure that you've set up nvidia-container-runtime or nvidia-container-toolkit for your Docker Engine correctly (https://docs.docker.com/config/containers/resource_constraints/#gpu)`)?
1. What OS is this running on? MacOS/Linux/Linux Subsystem on Windows?",regarding register layer error overlay mount many symbolic link error tried replicate without success example worked fine think happening docker service may gotten stuck broken state recommend stopping docker service clearing directory service original issue machine access least aka please make sure set docker engine correctly o running subsystem,issue,negative,positive,positive,positive,positive,positive
707107016,"@deliahu You are right, everything works with t3 instance.
Maybe add some notice/documentation that it doesnt work with m3 instance right now?",right everything work instance maybe add doesnt work instance right,issue,negative,positive,positive,positive,positive,positive
706907472,"@deliahu Thanks for your advice, will try PythonPredictor.
@RobertLucian It is accessing full gpu ram while using TensorflowPredictor instead of what model needs. I have also used tensorflow-serving docker image separately and deployed models, it also acquire full gpu ram which i limited by giving a parameter --per_process_gpu_memory_fraction.",thanks advice try full ram instead model need also used docker image separately also acquire full ram limited giving parameter,issue,positive,positive,positive,positive,positive,positive
706897815,"I‚Äôll try deploying to AWS, where can I find instructions to do that?

On Sun, 11 Oct 2020 at 22:36, David Eliahu <notifications@github.com> wrote:

> My best guess is that the root of the issue is that the docker daemon is
> running in the windows environment, but the cortex CLI is running in
> ubuntu. Therefore, the docker daemon does not have access to the ubuntu
> file system (which the cortex CLI is operating on), since it's running in
> windows.
>
> If this is the case, there are two possibilities to resolve this:
>
>    1. Install docker in the ubuntu environment instead of in windows
>    2. Use cortex to deploy to AWS instead of deploying locally
>
> We are hoping to support windows natively (#715
> <https://github.com/cortexlabs/cortex/issues/715>), so you wouldn't have
> to run in ubuntu, but we don't yet know exactly when that will be ready.
>
> In the meantime, would one of the above options work for you?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1434#issuecomment-706764880>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AQQZJ3FCGQ3J2UPM3N66GKDSKIJNZANCNFSM4SLYXACQ>
> .
>
",try find sun wrote best guess root issue docker daemon running environment cortex running therefore docker daemon access file system cortex operating since running case two resolve install docker environment instead use cortex deploy instead locally support natively would run yet know exactly ready would one work reply directly view,issue,positive,positive,positive,positive,positive,positive
706896290,"> My best guess is that the root of the issue is that the docker daemon is running in the windows environment, but the cortex CLI is running in ubuntu. Therefore, the docker daemon does not have access to the ubuntu file system (which the cortex CLI is operating on), since it's running in windows.
> 
> If this is the case, there are two possibilities to resolve this:
> 
> 1. Install docker in the ubuntu environment instead of in windows
> 2. Use cortex to deploy to AWS instead of deploying locally
> 
> We are hoping to support windows natively (#715), so you wouldn't have to run in ubuntu, but we don't yet know exactly when that will be ready.
> 
> In the meantime, would one of the above options work for you?

Where can I find tutorials on deploying cortex to AWS?",best guess root issue docker daemon running environment cortex running therefore docker daemon access file system cortex operating since running case two resolve install docker environment instead use cortex deploy instead locally support natively would run yet know exactly ready would one work find cortex,issue,positive,positive,positive,positive,positive,positive
706764880,"My best guess is that the root of the issue is that the docker daemon is running in the windows environment, but the cortex CLI is running in ubuntu. Therefore, the docker daemon does not have access to the ubuntu file system (which the cortex CLI is operating on), since it's running in windows.

If this is the case, there are two possibilities to resolve this:

1. Install docker in the ubuntu environment instead of in windows
2. Use cortex to deploy to AWS instead of deploying locally

We are hoping to support windows natively (https://github.com/cortexlabs/cortex/issues/715), so you wouldn't have to run in ubuntu, but we don't yet know exactly when that will be ready.

In the meantime, would one of the above options work for you?",best guess root issue docker daemon running environment cortex running therefore docker daemon access file system cortex operating since running case two resolve install docker environment instead use cortex deploy instead locally support natively would run yet know exactly ready would one work,issue,positive,positive,positive,positive,positive,positive
706763382,"In that case, the only workarounds that I can think of are the ones I listed above (https://github.com/cortexlabs/cortex/issues/1432#issuecomment-706640493)

We'll keep you posted as we make progress on https://github.com/cortexlabs/cortex/issues/1113",case think listed keep posted make progress,issue,negative,neutral,neutral,neutral,neutral,neutral
706762944,"@dakshvar22 do you mind sharing your `cortex.yaml` file, as well as a simple `Dockerfile` and `predictor.py` which can be used to reproduce it?

For example, the Dockerfile might just show:

```Dockerfile
FROM cortexlabs/python-predictor-gpu-slim:0.20.0-cuda10.1
RUN ... # install your dependencies
```

And your `predictor.py` might just show:

```python
class PythonPredictor:
    def __init__(self, config):
        print(is_gpu_visible())  # replace is_gpu_visible() with the the appropriate function call

    def predict(self, payload):
        return ""ok""
```",mind file well simple used reproduce example might show run install might show python class self print replace appropriate function call predict self return,issue,negative,positive,positive,positive,positive,positive
706720425,"I am using my own API which is just a Python Predictor API since my model
is trained in fairseq lib. I am using the default GPU docker image which is
mentioned in the docs for python predictor API.

On Sun, Oct 11, 2020, 16:59 David Eliahu <notifications@github.com> wrote:

> @dakshvar22 <https://github.com/dakshvar22> are you running an example
> from the cortex repo (if so, which one?), or your own API (if so, which
> predictor type?). Also, what is the base image you're using for the API
> container, or are you using the default?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1223#issuecomment-706717521>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGKPSW2W6HDZKJMGKX3SKHB6LANCNFSM4O4SS2UA>
> .
>
",python predictor since model trained default docker image python predictor sun wrote running example cortex one predictor type also base image container default reply directly view,issue,negative,negative,negative,negative,negative,negative
706720221,"Yes I am trying to avoid the startup/scale up time.

On Sun, Oct 11, 2020, 17:01 David Eliahu <notifications@github.com> wrote:

> @dakshvar22 <https://github.com/dakshvar22> downloading and unpacking the
> model when a new instance comes up shouldn't add to the prediction request
> latency, but will add to the startup / scale up time; is that what you are
> trying to avoid?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1432#issuecomment-706717781>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGOIMI26EKHLIAFG3YTSKHCEHANCNFSM4SLEFORA>
> .
>


On Sun, Oct 11, 2020, 17:01 David Eliahu <notifications@github.com> wrote:

> @dakshvar22 <https://github.com/dakshvar22> downloading and unpacking the
> model when a new instance comes up shouldn't add to the prediction request
> latency, but will add to the startup / scale up time; is that what you are
> trying to avoid?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1432#issuecomment-706717781>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGOIMI26EKHLIAFG3YTSKHCEHANCNFSM4SLEFORA>
> .
>
",yes trying avoid time sun wrote model new instance come add prediction request latency add scale time trying avoid reply directly view sun wrote model new instance come add prediction request latency add scale time trying avoid reply directly view,issue,negative,positive,positive,positive,positive,positive
706719109,"> @imagine3D-ai can you describe your setup, for example, which operating system you're on, how you are running the cortex CLI, and how you have installed docker so that the corex cli can access it?

and I've installed Docker just by using the official installer: https://hub.docker.com/editions/community/docker-ce-desktop-windows/",describe setup example operating system running cortex docker access docker official installer,issue,negative,neutral,neutral,neutral,neutral,neutral
706718746,"> @imagine3D-ai can you describe your setup, for example, which operating system you're on, how you are running the cortex CLI, and how you have installed docker so that the corex cli can access it?

Yes of course. I'm running this on Ubuntu for Windows 10. I've installed cortex cli using this command (found in cortex installation guide) in Ubuntu :
`bash -c ""$(curl -sS https://raw.githubusercontent.com/cortexlabs/cortex/0.20/get-cli.sh)""`",describe setup example operating system running cortex docker access yes course running cortex command found cortex installation guide bash curl,issue,negative,neutral,neutral,neutral,neutral,neutral
706718307,"> @imagine3D-ai just out of curiosity, were you able to get this working, and if so, was it by installing docker in your linux subsystem?

I've installed Ubuntu on my machine and installed docker there.",curiosity able get working docker subsystem machine docker,issue,negative,positive,positive,positive,positive,positive
706718136,"@imagine3D-ai can you describe your setup, for example, which operating system you're on, how you are running the cortex CLI, and how you have installed docker so that the corex cli can access it?",describe setup example operating system running cortex docker access,issue,negative,neutral,neutral,neutral,neutral,neutral
706717932,"@imagine3D-ai just out of curiosity, were you able to get this working, and if so, was it by installing docker in your linux subsystem?",curiosity able get working docker subsystem,issue,negative,positive,positive,positive,positive,positive
706717781,"@dakshvar22 downloading and unpacking the model when a new instance comes up shouldn't add to the prediction request latency, but will add to the startup / scale up time; is that what you are trying to avoid?",model new instance come add prediction request latency add scale time trying avoid,issue,negative,positive,positive,positive,positive,positive
706717521,"@dakshvar22 are you running an example from the cortex repo (if so, which one?), or your own API (if so, which predictor type?). Also, what is the base image you're using for the API container, or are you using the default?
",running example cortex one predictor type also base image container default,issue,negative,negative,negative,negative,negative,negative
706703714,"@BogdanDarius I don't yet know for sure what the issue is, however I wanted to update you that I was able to spin up a cluster by changing the instance type and leaving the rest of the configuration the same.

Are you able to try again with a different instance type? I tried with m5.large, but if you want a smaller one, I expect that the t3 or t3a series will work.",yet know sure issue however update able spin cluster instance type leaving rest configuration able try different instance type tried want smaller one expect ta series work,issue,negative,positive,positive,positive,positive,positive
706668415,"So, the use case is that we would like to bake the model inside the docker
image because downloading and unpacking it everytime when a new instance
comes up adds to the latency. But since this is an inhouse model, we cannot
make it part of a public docker image.
Can you suggest something here?

On Sun, Oct 11, 2020, 04:37 David Eliahu <notifications@github.com> wrote:

> @dakshvar22 <https://github.com/dakshvar22> currently this is not
> supported, but we have #1113
> <https://github.com/cortexlabs/cortex/issues/1113> to track this. Here
> are some potential workarounds:
>
>    - use a public registry
>    - use a private ECR registry
>    - install required dependencies via one of these approaches
>    <https://docs.cortex.dev/advanced/python-packages> for Python
>    dependencies and dependencies.sh
>    <https://docs.cortex.dev/advanced/system-packages#bash-script> for
>    system dependencies.
>
> Is one of these an option for you?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1432#issuecomment-706640493>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGPO6TOUVSLNAFAZ6STSKEK5XANCNFSM4SLEFORA>
> .
>
",use case would like bake model inside docker image new instance come latency since model make part public docker image suggest something sun wrote currently track potential use public registry use private registry install via one python system one option reply directly view,issue,negative,positive,neutral,neutral,positive,positive
706668208,"I don't see any error, but the GPU isn't visible inside the container and
hence not used by the inference API.

On Sun, Oct 11, 2020, 04:30 David Eliahu <notifications@github.com> wrote:

> @dakshvar22 <https://github.com/dakshvar22> yes, it should fall back on
> nvidia-docker2 if nvidia-container-toolkit is not found. What is the error
> message that you see when you try?
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1223#issuecomment-706639943>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ACCOBGJGWTNB4LMD7IM3FD3SKEKD3ANCNFSM4O4SS2UA>
> .
>
",see error visible inside container hence used inference sun wrote yes fall back found error message see try reply directly view,issue,negative,positive,neutral,neutral,positive,positive
706643770,"@BogdanDarius Thank you for providing all of the information, that is very helpful!

I have been able to reproduce this, and I don't know yet exactly what is causing it, but I will keep you posted",thank providing information helpful able reproduce know yet exactly causing keep posted,issue,positive,positive,positive,positive,positive,positive
706640535,"Potential workaround until this is implemented:

* use a public registry
* use a private ECR registry
* install required dependencies via one of [these approaches](https://docs.cortex.dev/advanced/python-packages) for Python dependencies and [`dependencies.sh`](https://docs.cortex.dev/advanced/system-packages#bash-script) for system dependencies.",potential use public registry use private registry install via one python system,issue,negative,neutral,neutral,neutral,neutral,neutral
706640493,"@dakshvar22 currently this is not supported, but we have https://github.com/cortexlabs/cortex/issues/1113 to track this. Here are some potential workarounds:

* use a public registry
* use a private ECR registry
* install required dependencies via one of [these approaches](https://docs.cortex.dev/advanced/python-packages) for Python dependencies and [`dependencies.sh`](https://docs.cortex.dev/advanced/system-packages#bash-script) for system dependencies.

Is one of these an option for you?",currently track potential use public registry use private registry install via one python system one option,issue,negative,neutral,neutral,neutral,neutral,neutral
706639943,"@dakshvar22 yes, it should fall back on nvidia-docker2 if nvidia-container-toolkit is not found. What is the error message that you see when you try?",yes fall back found error message see try,issue,negative,neutral,neutral,neutral,neutral,neutral
706620215,"Hi, is nvidia-docker2 still supported? I have docker version < 19.03 and nvidia-docker2 install and I would like to leverage GPU without having to upgrade docker. Currently with v0.20 it doesn't seem like GPU is made available inside the service. Can you please clarify?",hi still docker version install would like leverage without upgrade docker currently seem like made available inside service please clarify,issue,positive,positive,positive,positive,positive,positive
706460533,"@AkashDharani I would also like to add: if you would prefer to implement your inference in Python rather than using the `TensorFlowPredictor` type (which automatically loads your SavedModel into a sidecar TF Serving container), you can use the `PythonPredictor` predictor type ([docs](https://docs.cortex.dev/deployments/realtime-api/predictors#python-predictor)), which will give you access to a Python environment with the GPU available.",would also like add would prefer implement inference python rather type automatically sidecar serving container use predictor type give access python environment available,issue,negative,positive,positive,positive,positive,positive
706186973,"@AkashDharani when using the TensorFlow predictor, you only have to provide the model(s) to the predictor via the `cortex.yaml` config file. All the rest is handled by Cortex already. And since this is running a TFS server, loading a model into the GPU's memory will only use as much memory as it needs.

The suggestions mentioned in https://docs.cortex.dev/advanced/gpus#tips are for when the `PythonPredictor` predictor is used. It doesn't apply to the `TensorFlowPredictor` predictor.

You can check out [this example](https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/multi-model-classifier) that loads multiple models into the memory while using the `TensorFlowPredictor` predictor.",predictor provide model predictor via file rest handled cortex already since running server loading model memory use much memory need predictor used apply predictor check example multiple memory predictor,issue,negative,positive,neutral,neutral,positive,positive
706157849,"Yes I am using the `TensorFlowPredictor` predictor, and I am running it locally on RTX-2080 Ti. When I import tensorflow in `TensorflowPredictor` and test the availability of gpu it returns False `tf.test.is_gpu_available()`. But the models is getting loaded in gpu ram, it is occupying whole gpu ram.",yes predictor running locally ti import test availability false getting loaded ram whole ram,issue,negative,negative,neutral,neutral,negative,negative
706154915,"@AkashDharani the `tensorflow_model_server`? Are you using the `TensorFlowPredictor` predictor? In either case, could we see the predictor's implementation? And is this run locally?",predictor either case could see predictor implementation run locally,issue,negative,neutral,neutral,neutral,neutral,neutral
705998111,"GPU is not available in this class, tf.test.is_gpu_available() is giving false in the constructor of tensorflow predictor class. But tensorflow_model_server is loaded in my gpu memory and occupying full gpu ram.",available class giving false constructor predictor class loaded memory full ram,issue,negative,positive,positive,positive,positive,positive
705570385,"@AkashDharani this has to be done in the predictor's constructor. It should follow this pattern:

https://github.com/cortexlabs/cortex/blob/889627755582365123f086c673466aa7bcbcc574/examples/tensorflow/license-plate-reader/predictor_crnn.py#L11-L19

Likewise for your specific code snippet. It will take the same place in the predictor's constructor.",done predictor constructor follow pattern likewise specific code snippet take place predictor constructor,issue,negative,neutral,neutral,neutral,neutral,neutral
703631547,"@imagine3D-ai that's because you don't have `docker` installed on your Linux Subsystem. The Cortex CLI requires Docker to be installed. I think that for Windows machines running with the Linux Subsystem, you can follow [these instructions](https://medium.com/faun/docker-running-seamlessly-in-windows-subsystem-linux-6ef8412377aa) to get it installed on yours. I haven't tried this though.",docker subsystem cortex docker think running subsystem follow get tried though,issue,negative,neutral,neutral,neutral,neutral,neutral
703628549,"@imagine3D-ai to make things clear, we don't support the Cortex CLI on Windows yet - I think the only option is to use the Linux Subsystem on Windows, which is what I think you did.

As for why it didn't work for you, maybe [this can help](https://unix.stackexchange.com/questions/27054/bin-bash-no-such-file-or-directory) explain the cause. Also, running `which bash` in your terminal will tell you where `bash` is installed.",make clear support cortex yet think option use subsystem think work maybe help explain cause also running bash terminal tell bash,issue,positive,positive,positive,positive,positive,positive
703222634,"> @imagine3D-ai we don't yet officially support running on Windows. That said, it might be possible by using the Windows Subsystem for Linux. Is this the approach you are trying?
> 
> Also, I noticed that first you ran `bash`, and then you ran `cortex deploy`. If you run `bash` and then run `docker run hello-world` does docker work?

Actually not.. 
`bash: docker: command not found`

So it's not running from bash for some reason",yet officially support running said might possible subsystem approach trying also first ran bash ran cortex deploy run bash run docker run docker work actually bash docker command found running bash reason,issue,negative,positive,neutral,neutral,positive,positive
703197468,"@imagine3D-ai we don't yet officially support running on Windows. That said, it might be possible by using the Windows Subsystem for Linux. Is this the approach you are trying?

Also, I noticed that first you ran `bash`, and then you ran `cortex deploy`. If you run `bash` and then run `docker run hello-world` does docker work?",yet officially support running said might possible subsystem approach trying also first ran bash ran cortex deploy run bash run docker run docker work,issue,negative,positive,positive,positive,positive,positive
703007012,"FYI, `hydra-core` will be a required dependency for using fairseq via `torch.hub` going forward.",dependency via going forward,issue,negative,neutral,neutral,neutral,neutral,neutral
701334033,"@AkashDharani we have instructions on how to set up the development environment here https://docs.cortex.dev/contributing/development. Once those instructions are executed, you will be able to run the make file. 

Let us know if you still encounter any issues along the way.",set development environment executed able run make file let u know still encounter along way,issue,negative,positive,positive,positive,positive,positive
701036785,"With https://github.com/cortexlabs/cortex/pull/1378 and https://github.com/cortexlabs/cortex/pull/1403 in, the order for all `cortex cluster` commands is:
1. Cortex CLI flags.
1. Cortex CLI cache.
1. Environment variables (`AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`).
1. AWS config (default credentials `~/.aws/`).
1. Cortex CLI Prompt.",order cortex cluster cortex cortex cache environment default cortex prompt,issue,negative,neutral,neutral,neutral,neutral,neutral
700819998,"Looks like you are getting this dependency from fairseq, can you file an issue with them?
They should either declare Hydra as a dependency or make sure that their code does not fail if it's not installed.",like getting dependency file issue either declare dependency make sure code fail,issue,negative,neutral,neutral,neutral,neutral,neutral
700718360,"@omry Yes. We were also surprised to see needing these 2 packages, but after adding them, the example started working again. It wasn't this case before.

Here are the logs that we'd get without these packages in:
```text
The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/src/cortex/serve/serve.py"", line 314, in start_fn
    predictor_impl = api.predictor.initialize_impl(project_dir, client, raw_api_spec, None)
  File ""/src/cortex/lib/type/predictor.py"", line 113, in initialize_impl
    raise UserRuntimeException(self.path, ""__init__"", str(e)) from e
cortex.lib.exceptions.UserRuntimeException: error: predictor.py: __init__: No module named 'omegaconf': runtime exception
Requirement already satisfied: torch in /opt/conda/envs/env/lib/python3.6/site-packages (from -r /mnt/project/requirements.txt (line 1)) (1.6.0+cu101)
Requirement already satisfied: regex in /opt/conda/envs/env/lib/python3.6/site-packages (from -r /mnt/project/requirements.txt (line 2)) (2020.9.27)
Requirement already satisfied: tqdm in /opt/conda/envs/env/lib/python3.6/site-packages (from -r /mnt/project/requirements.txt (line 3)) (4.50.0)
Requirement already satisfied: numpy in /opt/conda/envs/env/lib/python3.6/site-packages (from torch->-r /mnt/project/requirements.txt (line 1)) (1.19.1)
Requirement already satisfied: future in /opt/conda/envs/env/lib/python3.6/site-packages (from torch->-r /mnt/project/requirements.txt (line 1)) (0.18.2)
2020-09-29 13:45:22.801359:cortex:pid-29:INFO:loading the predictor from predictor.py
Downloading: ""https://github.com/pytorch/fairseq/archive/master.zip"" to /root/.cache/torch/hub/master.zip
2020-09-29 13:45:23.717412:cortex:pid-29:ERROR:failed to start api
Traceback (most recent call last):
  File ""/src/cortex/lib/type/predictor.py"", line 111, in initialize_impl
    return class_impl(**args)
  File ""/mnt/project/predictor.py"", line 10, in __init__
    roberta = torch.hub.load(""pytorch/fairseq"", ""roberta.large"")
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py"", line 349, in load
    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py"", line 71, in import_module
    spec.loader.exec_module(module)
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/hubconf.py"", line 8, in <module>
    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py"", line 17, in <module>
    import fairseq.criterions  # noqa
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/__init__.py"", line 26, in <module>
    importlib.import_module('fairseq.criterions.' + module)
  File ""/opt/conda/envs/env/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/adaptive_loss.py"", line 12, in <module>
    from fairseq.dataclass.data_class import DDP_BACKEND_CHOICES
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/dataclass/data_class.py"", line 12, in <module>
    from fairseq.tasks import TASK_DATACLASS_REGISTRY
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/__init__.py"", line 73, in <module>
    importlib.import_module('fairseq.tasks.' + task_name)
  File ""/opt/conda/envs/env/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/tasks/language_modeling.py"", line 36, in <module>
    from omegaconf import II
ModuleNotFoundError: No module named 'omegaconf'
```
And when only `omegaconf` is added, this is what it outputs:
```text
2020-09-29 13:49:34.321221:cortex:pid-29:ERROR:failed to start api
Traceback (most recent call last):
  File ""/src/cortex/lib/type/predictor.py"", line 111, in initialize_impl
    return class_impl(**args)
  File ""/mnt/project/predictor.py"", line 10, in __init__
    roberta = torch.hub.load(""pytorch/fairseq"", ""roberta.large"")
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py"", line 349, in load
    hub_module = import_module(MODULE_HUBCONF, repo_dir + '/' + MODULE_HUBCONF)
  File ""/opt/conda/envs/env/lib/python3.6/site-packages/torch/hub.py"", line 71, in import_module
    spec.loader.exec_module(module)
  File ""<frozen importlib._bootstrap_external>"", line 678, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/hubconf.py"", line 8, in <module>
    from fairseq.hub_utils import BPEHubInterface as bpe  # noqa
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/__init__.py"", line 17, in <module>
    import fairseq.criterions  # noqa
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/__init__.py"", line 26, in <module>
    importlib.import_module('fairseq.criterions.' + module)
  File ""/opt/conda/envs/env/lib/python3.6/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/criterions/adaptive_loss.py"", line 12, in <module>
    from fairseq.dataclass.data_class import DDP_BACKEND_CHOICES
  File ""/root/.cache/torch/hub/pytorch_fairseq_master/fairseq/dataclass/data_class.py"", line 18, in <module>
    from hydra.core.config_store import ConfigStore
ModuleNotFoundError: No module named 'hydra'
```

Do you have any idea why this might be the case?",yes also see needing example working case get without text exception direct cause following exception recent call last file line client none file line raise error module exception requirement already satisfied torch line requirement already satisfied line requirement already satisfied line requirement already satisfied line requirement already satisfied future line cortex loading predictor cortex error start recent call last file line return file line file line load file line module file frozen line file frozen line file line module import file line module import file line module module file line return name level package level file line module import file line module import file line module file line return name level package level file line module import module added text cortex error start recent call last file line return file line file line load file line module file frozen line file frozen line file line module import file line module import file line module module file line return name level package level file line module import file line module import module idea might case,issue,positive,positive,positive,positive,positive,positive
700433927,"Looking at the code of the example (very simple) example, I am not seeing any usage of OmegaConf or Hydra there.
Are you sure those are needed dependencies?",looking code example simple example seeing usage sure,issue,negative,positive,positive,positive,positive,positive
700395067,"@deliahu yes, I migrated to 0.19 and it worked fine. Thanks for following up.",yes worked fine thanks following,issue,positive,positive,positive,positive,positive,positive
699520997,"@mrafieej have you tried running cortex v0.19, and if so, does CORS work without you having to manually update the API Gateway CORS configuration?",tried running cortex work without manually update gateway configuration,issue,negative,neutral,neutral,neutral,neutral,neutral
698999286,Happy to hear that! Thanks! I'm gonna close this now.,happy hear thanks gon na close,issue,positive,positive,positive,positive,positive,positive
698997981,"Great, that worked perfectly, it was exactly what i was looking for.

Thanks @RobertLucian , and thank you for such a great tool",great worked perfectly exactly looking thanks thank great tool,issue,positive,positive,positive,positive,positive,positive
698382434,@BogdanDarius thanks for filing the issue. I did quick scan of the [docker network docs](https://docs.docker.com/engine/reference/commandline/network_connect/) and I was wondering if `docker network connect` could be used in your use case to add the Cortex API container to your user defined network in the mean time.,thanks filing issue quick scan docker network wondering docker network connect could used use case add cortex container user defined network mean time,issue,negative,positive,neutral,neutral,positive,positive
698381748,"@francis967 I suppose you are referring to the `max_instances` field available in `cluster.yaml` config (or as provided to `cortex cluster up` via the prompt if run without a config). The `max_instances` field is documented [here](https://docs.cortex.dev/cluster-management/config).

> Is it possible to increase the number of instances of a cluster already running, without having to turn off the cluster?
and if so, how?

> My intention is to be able to increase the number of machines in the cluster as I need more computing capacity, without having to lose the models already deployed.

Yes, that is possible. You just need to use the `cortex cluster configure` command to do that. If using a `cluster.yaml` config, then update the `max_instances` field to the higher value and then run `cortex cluster configure -c cluster.yaml`. Otherwise, use the CLI's prompt to update the value (by not running it with the `-c` option).",suppose field available provided cortex cluster via prompt run without field possible increase number cluster already running without turn cluster intention able increase number cluster need capacity without lose already yes possible need use cortex cluster configure command update field higher value run cortex cluster configure otherwise use prompt update value running option,issue,negative,positive,positive,positive,positive,positive
697161955,"> @Mahi-debug thank you. Were you observing the 405s at the time you created the cluster info zip file? Also, does the request fail 100% of the time or is it intermittent?

yes it fails 100% of the time ",thank observing time cluster zip file also request fail time intermittent yes time,issue,negative,negative,negative,negative,negative,negative
694826380,"@RobertLucian yes it does make sense! Thanks for taking the time to explain.

I think I will give the single node version a try over the weekend and report back.

I have seen the GCP ticket, but looking at the history it keeps getting pushed back.",yes make sense thanks taking time explain think give single node version try weekend report back seen ticket looking history getting back,issue,positive,positive,neutral,neutral,positive,positive
694414095,"@javithe7 We think we've identified the cause of this issue, and are working on fixing it in time for our next release",think cause issue working fixing time next release,issue,negative,neutral,neutral,neutral,neutral,neutral
694389990,"@ydennisy I see. Yes, the GCP is a popular request - as a matter of fact, we have a ticket here for it https://github.com/cortexlabs/cortex/issues/114.

With regards to the `local` provider (aka the type of environment used in the single node guide), we try to keep it as close as possible to what the `aws` provider offers. The major features you don't get are the rolling updates, the autoscaling, and the performance monitoring of deployments (i.e. AWS CloudWatch).

We added the support for local deployments with the thought of simplifying the development process - and not necessarily for production-related reasons. That being said, using the `local` provider for production workloads is still totally okay, especially for smaller projects.

> Out of interest, why are the containers not portable - I thought this what a container was designed for, that it could be run anywhere?

What do you mean by portable? If you mean running them standalone, then the answer is that the containers are deeply integrated with AWS and Cortex internals. For example, the container has an internal API which is more complicated than the API that Cortex exposes to the user, it has multiple sidecar containers and daemonsets which must be run alongside it (e.g. to run TensorFlow Serving, expose GPU and Inferentia hardware, track in-flight requests, etc), it uploads logs/metrics to CloudWatch, etc. In addition, the deployment specs which are passed into the serving container are built dynamically when an API is deployed.

I think that from the user's perspective, he/she should only be concerned with the following:
* The API configuration (`cortex.yaml`).
* The predictor implementation (i.e. `predictor.py`).
* Optionally the cluster configuration when the `aws` provider is used (`cluster.yaml`).

Let me know if this makes sense to you.",see yes popular request matter fact ticket local provider aka type environment used single node guide try keep close possible provider major get rolling performance added support local thought development process necessarily said local provider production still totally especially smaller interest portable thought container designed could run anywhere mean portable mean running answer deeply cortex internals example container internal complicated cortex user multiple sidecar must run alongside run serving expose hardware track addition deployment spec serving container built dynamically think user perspective concerned following configuration predictor implementation optionally cluster configuration provider used let know sense,issue,positive,negative,neutral,neutral,negative,negative
694365319,"Hey @RobertLucian thanks for your reply!

I was hoping to deploy on Cloud Run (GCP), where the input is a docker container, this is why I thought after using the cortex cli locally it would be possible to find the container you guys create to deploy on run.

I have seen the single node guide, what features still available when deploying in this manner?

Out of interest, why are the containers not portable - I thought this what a container was designed for, that it could be run anywhere?",hey thanks reply deploy cloud run input docker container thought cortex locally would possible find container create deploy run seen single node guide still available manner interest portable thought container designed could run anywhere,issue,positive,positive,positive,positive,positive,positive
693683839,"For anyone who have the same issue: the problem turned out to be from the SimpleTransformers library (specifically, the way it handles multi-threading).
Using `train_args.use_multiprocessing = False` when creating/loading the model has solved the issue for me.",anyone issue problem turned library specifically way false model issue,issue,negative,negative,negative,negative,negative,negative
693607596,"@ydennisy the Cortex Docker images (i.e. `cortexlabs/python-predictor-cpu`) are designed to be used within the context of Cortex and aren't easily adaptable to be run on their own as standalone containers.

You can deploy outside of AWS with Cortex though, albeit with fewer features than on AWS (autoscaling, rolling updates, etc). For that, you'll need to use the local Cortex environment. You can use that to deploy a single-node API on the same machine as where your CLI is installed on. You can follow the instructions in this [guide](https://docs.cortex.dev/guides/single-node-deployment) to deploy one on yours.

Does this answer your question, or are you referring to something else? And on what infrastructure are you hoping to run Cortex?",cortex docker designed used within context cortex easily adaptable run deploy outside cortex though albeit rolling need use local cortex environment use deploy machine follow guide deploy one answer question something else infrastructure run cortex,issue,negative,positive,positive,positive,positive,positive
692438949,"I'll go ahead and close this issue, feel free to reach out here or on our gitter ([gitter.im/cortexlabs/cortex](https://gitter.im/cortexlabs/cortex)) if you have any more questions",go ahead close issue feel free reach,issue,positive,positive,positive,positive,positive,positive
692438609,@javithe7 I just wanted to follow up on this; were you able to resolve your issue?,follow able resolve issue,issue,negative,positive,positive,positive,positive,positive
692333272,"@Mahi-debug I have one more question: Do you mind sending the output of `cortex version`, `cortex cluster info`, and `cortex env list`?",one question mind sending output cortex version cortex cluster cortex list,issue,negative,neutral,neutral,neutral,neutral,neutral
692181109,"@Mahi-debug Also, do you have a `cluster.yaml` file, or did you create your cluster without it (answering the cli prompts instead)?",also file create cluster without instead,issue,negative,neutral,neutral,neutral,neutral,neutral
692179406,"@Mahi-debug thank you. Were you observing the 405s at the time you created the cluster info zip file? Also, does the request fail 100% of the time or is it intermittent?",thank observing time cluster zip file also request fail time intermittent,issue,negative,negative,negative,negative,negative,negative
691820421,Thank you @deliahu for your response. I have shared the details at dev@cortex.dev. Hope to hear from you guys soon,thank response dev hope hear soon,issue,positive,neutral,neutral,neutral,neutral,neutral
690793389,"This doesn't seem to actually work, so I'll close this PR without merging

<img width=""1149"" alt=""Screen Shot 2020-09-10 at 4 59 09 PM"" src=""https://user-images.githubusercontent.com/808475/92827705-051ad300-f387-11ea-867b-e4fd23ed1f1f.png"">

For reference, here is the eksctl PR that is supposed to implement this: https://github.com/weaveworks/eksctl/pull/2487",seem actually work close without screen shot reference supposed implement,issue,negative,neutral,neutral,neutral,neutral,neutral
690210855,"Got it Thanks again!
Best regards!
Abdoulaye Faye
Computer Scientist Engineer (ESP)
Data Scientist at Baamtu SARL
Cit√© Castors Municipaux Dalifort
+221 77 285 77 56 - +221 78 216 97 88


Le jeu. 10 sept. 2020 √† 11:05, mossadhelalitalentbait <
notifications@github.com> a √©crit :

> Closed #1346 <https://github.com/cortexlabs/cortex/issues/1346>.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1346#event-3750389880>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AIJ3EPVTAU4C3TXNPD4NGD3SFCXILANCNFSM4RER7PPA>
> .
>
",got thanks best computer scientist engineer data scientist closed reply directly view,issue,positive,positive,positive,positive,positive,positive
690159051,"@Bams2011 

I have created an issue on the SimpleTransformers repo [here](https://github.com/ThilinaRajapakse/simpletransformers/issues/705). 

Thanks for your support. I will go ahead and close the issue here myself.",issue thanks support go ahead close issue,issue,positive,positive,positive,positive,positive,positive
690151130,"Yes you might forward the bug to simple transformers developers!
How  can i close the issue ?
Abdoulaye Faye
Computer Scientist Engineer (ESP)
Data Scientist at Baamtu SARL
Cit√© Castors Municipaux Dalifort
+221 77 285 77 56 - +221 78 216 97 88


Le jeu. 10 sept. 2020 √† 10:37, mossadhelalitalentbait <
notifications@github.com> a √©crit :

> @Bams2011 <https://github.com/Bams2011>
> Thank you very much for the detailed explanation. Using the custom Uvicorn
> build has indeed fixed the issue (at least for now). I will forward this
> bug to the SimpleTransformers developers.
> You can close this issue.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1346#issuecomment-690144741>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AIJ3EPVA45RULWO2INJDX43SFCT7JANCNFSM4RER7PPA>
> .
>
",yes might forward bug simple close issue computer scientist engineer data scientist thank much detailed explanation custom build indeed fixed issue least forward bug close issue reply directly view,issue,positive,positive,neutral,neutral,positive,positive
690144741,"@Bams2011 
Thank you very much for the detailed explanation. Using the custom Uvicorn build has indeed fixed the issue (at least for now). I will forward this bug to the SimpleTransformers developers. 
You can close this issue.",thank much detailed explanation custom build indeed fixed issue least forward bug close issue,issue,negative,positive,neutral,neutral,positive,positive
690134897,"Hi  mossadhelalitalentbait, after investigation with roberlucian ,below is
the conclusion he made:

The problem is with the simpletransformers package. The symptoms are:

   1. When signals (i.e. SIGINT, SIGTERM) are handled, then when the
   self.model.predict(inputs) method is called, it will send a SIGTERM
   signal. Uvicorn comes with signal handlers enabled, which means that the
   web server will always shut down when the signal is sent. The method should
   never send a SIGTERM signal.
   2. Another issue, which explains why the server doesn‚Äôt fully shut down,
   and thus stop the container, is that when the SIGTERM is sent, the
   self.model.predict(inputs) method doesn‚Äôt exit as it should and instead
   it just keeps on executing. This is what I have noticed while running a
   standalone Python script. While running this with Uvicorn (aka Cortex), it
   just hangs.
   3. Another issue, which may not be related, but it‚Äôs still there is that
   the simpletransformers.classification.ClassificationModel class has a
   memory leak bug. The model is never released from memory. My gut tells me
   that this bug might have something to do with the SIGTERM bug because when
   this happens with Cortex, the model is never released from memory

A custom un-official version of uvicorn package has been built to disable
the signal handlers. The package that has to be added to the
requirements.txt file is git+
https://github.com/RobertLucian/uvicorn.git@0.11.9. Use this as a temporary
quick-n-dirty fix.

The solution where the signal handlers are disabled is discouraged,
especially in production, due to simpletransformers package being buggy.
The recommended solution is to find an alternative to the simpletransformers
 package.

The following is a snippet of code that describes the above SIGTERM bug of
simpletransformers.


Hope this will help you !

Best regards!
Abdoulaye Faye
Computer Scientist Engineer (ESP)
Data Scientist at Baamtu SARL
Cit√© Castors Municipaux Dalifort
+221 77 285 77 56 - +221 78 216 97 88


Le jeu. 10 sept. 2020 √† 08:42, mossadhelalitalentbait <
notifications@github.com> a √©crit :

> Version
>
> cli version: 0.19.0
> Description
>
> Hi Everyone,
> Thanks for the cool work you are doing. We have benefited a lot from it.
> Now to the bug:
> I am deploying a *local cortex API* for a BERT model from the
> SimpleTransformers library.
>
> Here is the code for my predictor:
>
> class PythonPredictor:
>     def __init__(self, config):
>         print('INITALIZING')
>         self.classifier = JobPostingClassifier()
>         self.classifier.load_saved_model()
>
>     def predict(self, payload):
>         """"""
>             expected payload:
>             {""html"": ""<html><body> ... etc.""}
>         """"""
>
>         print('recieved response. extracting text from html'.upper())
>         text = get_text(payload['html'])
>
>         print('TEXT EXTRACTED. LENGTH:', len(text),  'PREDICTING USING BERT')
>
>         predictions = self.classifier.predict_categories([text])
>
>         print('PREDICTED', predictions[0])
>
>         return predictions[0]
>
> Notice that it prints the lines before but not after the model prediction
> call.
>
> And here is the response I get in the logs pretty much *instantly* after
> I make the request:
>
> RECIEVED RESPONSE. EXTRACTING TEXT FROM HTML
> TEXT EXTRACTED. LENGTH: 0 PREDICTING USING BERT
> 100% 1/1 [00:00<00:00, 32.93it/s]
> 2020-09-10 07:55:12.456679:cortex:pid-202:INFO:Shutting down
> 2020-09-10 07:55:12.562235:cortex:pid-202:INFO:Waiting for connections to close. (CTRL+C to force quit)
> 2020-09-10 08:00:30.450488:cortex:pid-202:INFO:Waiting for background tasks to complete. (CTRL+C to force quit)
>
> The request contains an empty HTML document to make sure the model doesn't
> take time in predicting it. This request works locally without deploying to
> cortex.
> I noticed someone else had the same issue (also with the
> SimpleTransformers library) in this thread
> <https://gitter.im/cortexlabs/cortex?at=5f3ee5eba05e464346da2c87>
> (Abdoulaye Faye @Bams2011 <https://github.com/Bams2011>). But they never
> reported back on what worked for them to solve the issue.
> I also tried the debugging step mentioned in the same thread (by
> @RobertLucian <https://github.com/RobertLucian>) in which I call the
> predictor from within the docker and it also worked (just like it worked
> for @Bams2011 <https://github.com/Bams2011> in the thread)
> Configuration
>
> cortex.yaml:
>
>   kind: RealtimeAPI
>   predictor:
>     type: python
>     path: cortex_predictor.py
>     threads_per_process: 16
>
> I tried it with and without the threads_per_process: 16 because I noticed
> the predictor makes the predictions in another thread. Both give the same
> response.
> Steps to reproduce
>
>    1. Deploy a SimpleTransformers predictor via cortex
>    2. Make a request to the API (e.g. for my case: curl
>    http://localhost:8888 -X POST -H ""Content-Type: application/json"" -d
>    '{""html"": ""<html><body></body></html>""}')
>
> Expected behavior
>
> The API should return the predictions of the request.
> Actual behavior
>
> The API doesn't return a response (curl just hangs there waiting) and in
> the logs it says the API has crashed.
>
> If you need more documentation for the bug, please tell me what
> outputs/files are desired.
>
> Thanks again.
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/cortexlabs/cortex/issues/1346>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AIJ3EPQ6MVV56NJPXQF3DRDSFCGOJANCNFSM4RER7PPA>
> .
>
",hi investigation conclusion made problem package handled method send signal come signal web server always shut signal sent method never send signal another issue server fully shut thus stop container sent method exit instead running python script running aka cortex another issue may related still class memory leak bug model never memory gut bug might something bug cortex model never memory custom version package built disable signal package added file use temporary fix solution signal disabled especially production due package buggy solution find alternative package following snippet code bug hope help best computer scientist engineer data scientist version version description hi everyone thanks cool work lot bug local cortex model library code predictor class self print predict self body print response text text print extracted length text text print return notice model prediction call response get pretty much instantly make request response text text extracted length cortex shutting cortex waiting close force quit cortex waiting background complete force quit request empty document make sure model take time request work locally without cortex someone else issue also library thread never back worked solve issue also tried step thread call predictor within docker also worked like worked thread configuration kind predictor type python path tried without predictor another thread give response reproduce deploy predictor via cortex make request case curl post body behavior return request actual behavior return response curl waiting need documentation bug please tell desired thanks reply directly view,issue,positive,positive,positive,positive,positive,positive
688957322,"@Mahi-debug thank you for the info.

Since we haven't been able to reproduce this yet, the next time that you see this error, do you mind sending us an email to dev@cortex.dev which includes the following information?

* The zip file generated by `cortex cluster info --debug`
* Your cluster.yaml` config (please hide or omit any AWS credentials)
* Your `cortex.yaml` config",thank since able reproduce yet next time see error mind sending u dev following information zip file cortex cluster please hide omit,issue,negative,positive,positive,positive,positive,positive
688617238,"This error is also returned when we try it with cortex version 0.17.1 
Command: `cortex deploy --env aws`
Error: **error: unexpected response from operator (status code 405):**",error also returned try cortex version command cortex deploy error error unexpected response operator status code,issue,negative,positive,neutral,neutral,positive,positive
688003136,"Typically 1 model == 1 api entry point, and this is the case in the [tutorial](https://docs.cortex.dev/tutorial). However it is possible to serve multiple models in a single api entry point, see here: https://docs.cortex.dev/guides/multi-model",typically model entry point case tutorial however possible serve multiple single entry point see,issue,negative,negative,neutral,neutral,negative,negative
687997776,"Ok, not clear how the de-coupling
is done on the code level.

1 model == 1 api entry point ?


> On Sep 7, 2020, at 11:44, David Eliahu <notifications@github.com> wrote:
> 
> Ôªø
> I'm not sure if I understand your question, but you can have multiple API containers running on a single instance. If you have a very big model, you will want to choose an instance that is large, and only run a single API container per instance.
> 
> In Cortex, we call the API container a ""replica"" (since it's actually a kubernetes pod which contains a few containers).
> 
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",clear done code level model entry point wrote sure understand question multiple running single instance big model want choose instance large run single container per instance cortex call container replica since actually pod reply directly view,issue,positive,positive,neutral,neutral,positive,positive
687991619,"I'm not sure if I understand your question, but you can have multiple API containers running on a single instance. If you have a very big model, you will want to choose an instance that is large, and only run a single API container per instance.

In Cortex, we call the API container a ""replica"" (since it's actually a kubernetes pod which contains a few containers).",sure understand question multiple running single instance big model want choose instance large run single container per instance cortex call container replica since actually pod,issue,negative,positive,neutral,neutral,positive,positive
687665917,"
It seems that scaling is done
at AWS instance level :  
   1 Web + 1 backend compute.

Can you change to have :
   Scaling Unit ;
       one docker (instead of one instance).

In the case of Aws, you can keep 1 to 1 mapping.


In case of very big model,
You need also big instance....






> On Sep 6, 2020, at 3:30, David Eliahu <notifications@github.com> wrote:
> 
> Ôªø
> In the local environment, there will be one web server per API endpoint, each running in a different container and on a different port. It is also possible to have multiple models in a single API endpoint, so the user could select which one e.g. with a query parameter (like http://localhost:8888?model=model1).
> 
> In the AWS environment, each API can have multiple replicas running. Autoscaling will determine how many replicas are running. A replica can be thought of as a single container running a web server. Like with local, it is possible to have multiple models in a single API endpoint, or you can deploy a separate API endpoint for each model.
> 
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",scaling done instance level web compute change scaling unit one docker instead one instance case keep case big model need also big instance wrote local environment one web server per running different container different port also possible multiple single user could select one query parameter like environment multiple running determine many running replica thought single container running web server like local possible multiple single deploy separate model reply directly view,issue,positive,positive,neutral,neutral,positive,positive
687645753,"In the local environment, there will be one web server per API endpoint, each running in a different container and on a different port. It is also possible to have multiple models in a single API endpoint, so the user could select which one e.g. with a query parameter (like `http://localhost:8888?model=model1`).

In the AWS environment, each API can have multiple replicas running. Autoscaling will determine how many replicas are running. A replica can be thought of as a single container running a web server. Like with local, it is possible to have multiple models in a single API endpoint, or you can deploy a separate API endpoint for each model.",local environment one web server per running different container different port also possible multiple single user could select one query parameter like environment multiple running determine many running replica thought single container running web server like local possible multiple single deploy separate model,issue,positive,positive,neutral,neutral,positive,positive
687616835,"Thanks.
Just checking how the volume is handle :
Per instance:
    1 web server + 1 model running ?

Loading balancing is done by increskng the scale.


While not using docker as a reference node ?




> On Sep 5, 2020, at 22:37, David Eliahu <notifications@github.com> wrote:
> 
> Ôªø
> Cortex does not yet natively support running on Azure with multiple instances (the steps in the guide I sent above can be used to run on Azure with a single instance). It might be possible to manually set up a load balancer outside of cortex to make it work, but it is not something that we have tried yet.
> 
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",thanks volume handle per instance web server model running loading balancing done scale docker reference node wrote cortex yet natively support running azure multiple guide sent used run azure single instance might possible manually set load balancer outside cortex make work something tried yet reply directly view,issue,positive,positive,neutral,neutral,positive,positive
687612646,"Cortex does not yet natively support running on Azure with multiple instances (the steps in the guide I sent above can be used to run on Azure with a single instance). It might be possible to manually set up a load balancer outside of cortex to make it work, but it is not something that we have tried yet.",cortex yet natively support running azure multiple guide sent used run azure single instance might possible manually set load balancer outside cortex make work something tried yet,issue,negative,negative,neutral,neutral,negative,negative
687508409,"Thanks.
My point is can we run on Azure ?
How the load balancer works ?

Does every node contains the
HTTP server and the model ?



> On Sep 5, 2020, at 10:21, David Eliahu <notifications@github.com> wrote:
> 
> Ôªø
> @arita37 Yes, it is possible to run Cortex outside of AWS by using the ""local"" environment. You would just need to expose the port that your API is running on to make it available to external clients. Here is a guide for how to run on a single AWS node, and the instructions should be fairly similar for other cloud providers: https://docs.cortex.dev/guides/single-node-deployment
> 
> As you mentioned, features such as autoscaling and rolling updates are not available when running locally.
> 
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",thanks point run azure load balancer work every node server model wrote yes possible run cortex outside local environment would need expose port running make available external guide run single node fairly similar cloud rolling available running locally reply directly view,issue,positive,positive,neutral,neutral,positive,positive
687507852,"@arita37 Yes, it is possible to run Cortex outside of AWS by using the ""local"" environment. You would just need to expose the port that your API is running on to make it available to external clients. Here is a guide for how to run on a single AWS node, and the instructions should be fairly similar for other cloud providers: https://docs.cortex.dev/guides/single-node-deployment

As you mentioned, features such as autoscaling and rolling updates are not available when running locally.",yes possible run cortex outside local environment would need expose port running make available external guide run single node fairly similar cloud rolling available running locally,issue,negative,positive,neutral,neutral,positive,positive
686837051,"@javithe7 I was not able to reproduce the error, do you mind replying with `my_cluster.yaml` (with any sensitive information hidden), a minimal version of your Dockerfile that will reproduce the error, the command that you are using to run your Docker image, and anything else that might be helpful as I try to reproduce it? Also, how many clusters are running on your AWS account when you run the `cortex cluster info` command?",able reproduce error mind sensitive information hidden minimal version reproduce error command run docker image anything else might helpful try reproduce also many running account run cortex cluster command,issue,negative,positive,positive,positive,positive,positive
686818376,"@javithe7 I will see if I can reproduce this, thank you for creating the issue.

In the meantime, one thing you can try (although I have not tried this so I don't know for sure that it will work), is that you can run `cortex env configure` if you have access to all of the necessary information. You can either let it prompt you for the information it needs, or you can pass it in as flags, e.g. 

```
cortex env configure aws --provider aws --operator-endpoint <> --aws-access-key-id <> --aws-secret-access-key <>
```",see reproduce thank issue one thing try although tried know sure work run cortex configure access necessary information either let prompt information need pas cortex configure provider,issue,positive,positive,positive,positive,positive,positive
683763751,"Thanks for bringing this to our attention. The `cortex get` command may take a while because it tries to get deployed APIs from all environments listed in `cortex env list`. `cortex cluster down` deletes the environment automatically so this shouldn't happen typically, however, because you've encountered issues while spinning down the cluster and deleted the cluster manually, the environment to your cluster may not have been deleted. Therefore when you run `cortex get`, it will try to get APIs from a deleted cluster and hang. 

You can verify that this is your scenario by using `cortex env list`. If you still find the environment for your cluster (it is usually named `aws` be default), you can delete the environment with `cortex env delete`. Let us know if this works for you.

Following up, do you have more information on the error you've encountered while spinning down the cluster?

As for your suggestions, I have filed a ticket https://github.com/cortexlabs/cortex/issues/1319 to add documentation for `cortex cluster down` cleanup.

Furthermore, maybe a timeout should be added to CLI requests to prevent the CLI from hanging https://github.com/cortexlabs/cortex/issues/1320.",thanks attention cortex get command may take get listed cortex list cortex cluster environment automatically happen typically however spinning cluster cluster manually environment cluster may therefore run cortex get try get cluster verify scenario cortex list still find environment cluster usually default delete environment cortex delete let u know work following information error spinning cluster ticket add documentation cortex cluster cleanup furthermore maybe added prevent hanging,issue,negative,negative,neutral,neutral,negative,negative
683753211,This looks interesting and we can definitely see its use-case. Thanks for filling in this feature request. We'll have a better look at it.,interesting definitely see thanks filling feature request better look,issue,positive,positive,positive,positive,positive,positive
682281505,"To check what the latest tag is currently pointing to, run `aws ecr list-images --region us-west-2 --registry-id 790709498068 --repository-name neuron-device-plugin`, and then see which version has the same imageDigest as `latest`. See [here](https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-container-tools/k8s-neuron-device-plugin.yml) (or its parent directory) to determine the ecr repository id/name.",check latest tag currently pointing run region see version latest see parent directory determine repository,issue,negative,positive,positive,positive,positive,positive
682156740,Closing now that we build and push Python Predictor images for multiple CUDA versions,build push python predictor multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
681139422,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/cortexlabs/cortex?pullRequest=1308) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/cortexlabs/cortex?pullRequest=1308) before we can accept your contribution.<br/>**2** out of **3** committers have signed the CLA.<br/><br/>:white_check_mark: RobertLucian<br/>:white_check_mark: ismaelc<br/>:x: Chris Ismael<br/><hr/>**Chris Ismael** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/cortexlabs/cortex?pullRequest=1308) it.</sub>",assistant check thank submission really appreciate like many open source ask sign contributor license agreement accept user need account able sign already account please add address used commit account already status still pending let u recheck,issue,positive,positive,positive,positive,positive,positive
680875678,"Thanks for bringing this up! 

There appears to be a typo in the link. It should link to https://www.youtube.com/watch?v=aUdKzb4LGJI rather than https://www.youtube.com/watch?v=aUdKzb4LGJ.",thanks typo link link rather,issue,negative,positive,positive,positive,positive,positive
677441847,"@RobertLucian Thanks for looking into this

- installed CLI
- implemented Python predictor
- spun up cluster using the config above
- configured the API Gateway to serve using SSL. Added route to Route53
- the endpoint gets ~30 req/min from our services. The data is just text, and the payload size isn't large
- the service returns `503 upstream connect error or disconnect/reset before headers. reset reason: connection failure` after a random amount of time (we haven't been able to pinpoint what's the underlying reason)
- a `refresh` or `deploy` (with a dummy change) always fixes the issue

A few notes about the predictor:

- in the `__init__` the class downloads and loads three fairly large models (based on Spacy)
- a request takes about 350ms to process on a `t3.medium`


Lmk if you need more specific details",thanks looking python predictor spun cluster gateway serve added route route data text size large service upstream connect error reset reason connection failure random amount time able pinpoint underlying reason refresh deploy dummy change always issue predictor class three fairly large based spacy request process need specific,issue,negative,positive,neutral,neutral,positive,positive
676556987,"@cristianmtr we have reasons to think that this is no longer related to #814 (just like you have mentioned in one of your previous responses).

Our course of action would be to first reproduce this. In this case, could you tell us all of the steps you've taken so far that got you to this point (or at least to the first time when you encountered this)? (i.e. installed CLI, spun-up the cluster, etc). The more details there are, the faster/easier should be for us to reproduce this. Anything you think is useful, please let us know.",think longer related like one previous course action would first reproduce case could tell u taken far got point least first time cluster u reproduce anything think useful please let u know,issue,positive,positive,neutral,neutral,positive,positive
676222072,"It just happened again. From the custom logs we aggregate in Grafana, we can see that the last action were to set up the predictor class (downloading and loading models). My theory is that it was trying to spin up a new instance, but, due to the failing health check, the LB doesn't detect them as running instances anymore.",custom aggregate see last action set predictor class loading theory trying spin new instance due failing health check detect running,issue,negative,positive,neutral,neutral,positive,positive
675697909,Please re-run the versions script to update the warnings for new examples.,please script update new,issue,negative,positive,positive,positive,positive,positive
675656099,"@mrafieej I'm glad to hear that you were able to get it working! Ideally it would not be necessary for you to manually update the API Gateway CORS configuration, and instead it should work out-of-the-box. It seems to work out-of-the-box for me now on the master branch, which we can confirm with your application once we release 0.19",glad hear able get working ideally would necessary manually update gateway configuration instead work work master branch confirm application release,issue,positive,positive,positive,positive,positive,positive
675453569,"I investigated the error message from the load balancer health checks, and I ended up at this issue on the kubernetes github page: https://github.com/kubernetes/kubernetes/issues/61486

Seems like it's a problem with the VPC setup in AWS. Not sure this is related to the problem you referenced above.",error message load balancer health ended issue page like problem setup sure related problem,issue,negative,positive,positive,positive,positive,positive
675304235,"> How often does this surge in 503s happen to you? Every hour? Every few days?

Last time it happened it was after 2 days since the deployment. It's usually within a 12 hour window.

> When this happens, what percentage of the total number of requests get 503s?

It's a complete shutdown. No requests reach the instances themselves. Our theory is that they are stopped at the LB level (due to the 'unhealthy' issue I mentioned)

> When this happens, how long would you say this persists for? 1 minute, 5 minutes, 1 hour, etc?

I've seen it twice when it automatically ""fixed"" itself almost immediately. Otherwise it's permanent.

> Are you using the API Gateway? The API Gateway is enabled by default and its field is networking:api_gateway in cortex.yaml.

Yes, we are. We have also enabled SSL at that level, if that helps.

> When you say redeployment, do you mean cortex delete <api-name> && cortex deploy or just cortex deploy?

It's enough to `deploy` (though we need to make a dummy change to be able to, otherwise we are told the API is up to date)
",often surge happen every hour every day last time day since deployment usually within hour window percentage total number get complete shutdown reach theory stopped level due issue long would say minute hour seen twice automatically fixed almost immediately otherwise permanent gateway gateway default field yes also level say redeployment mean cortex delete cortex deploy cortex deploy enough deploy though need make dummy change able otherwise told date,issue,negative,negative,neutral,neutral,negative,negative
675233314,Thanks @deliahu we resolved the issue with @vishalbollu 's help. He created a guide on how to add the required header for the LB in AWS console.,thanks resolved issue help guide add header console,issue,positive,positive,positive,positive,positive,positive
675219376,"@mrafieej we have not been able to reproduce this issue on the master branch; once we release v0.19 (which we expect to late this week), do you mind trying your application with v0.19 to check if this has been resolved?",able reproduce issue master branch release expect late week mind trying application check resolved,issue,negative,positive,neutral,neutral,positive,positive
675051005,"@cristianmtr we haven't tested for that yet, but we will look into this as we investigate #814 to see if it's related. Thanks for the heads up. We'll dig into this soon and update you on this matter.

Also, we do have a couple of questions regarding this:
1. How often does this surge in 503s happen to you? Every hour? Every few days?
1. When this happens, what percentage of the total number of requests get 503s?
1. When this happens, how long would you say this persists for? 1 minute, 5 minutes, 1 hour, etc?
1. Are you using the API Gateway? The API Gateway is enabled by default and its field is `networking:api_gateway` in `cortex.yaml`.
1. When you say redeployment, do you mean `cortex delete <api-name> && cortex deploy` or just `cortex deploy`?",tested yet look investigate see related thanks dig soon update matter also couple regarding often surge happen every hour every day percentage total number get long would say minute hour gateway gateway default field say redeployment mean cortex delete cortex deploy cortex deploy,issue,positive,negative,neutral,neutral,negative,negative
674942459,"@RobertLucian Thanks for answering. Are you also seeing the ""unhealthy"" status on the target instances for the load balancer? Or is that a separate issue I should look into? ",thanks also seeing unhealthy status target load balancer separate issue look,issue,negative,negative,neutral,neutral,negative,negative
674931664,"@cristianmtr thanks for reporting this here.

We already have a ticket for it #814.
This may be valuable in debugging this issue so we're gonna keep this one open.",thanks already ticket may valuable issue gon na keep one open,issue,positive,positive,neutral,neutral,positive,positive
674078433,"@codecraf8 thanks for reaching out! Yes, I agree; in fact we just recently decided that we should update our [tutorial](https://docs.cortex.dev/iris-classifier) to use Hugging Face's GPT-2 (https://github.com/cortexlabs/cortex/issues/1256). We should have that ready for the next release, which is expected to come late next week.

In the meantime, the existing tutorial still gives a good sense for how to use Cortex, and we have a few examples which use transformers that should serve as a good starting point:

* [pytorch/sentiment-analyzer](https://github.com/cortexlabs/cortex/tree/0.18/examples/pytorch/sentiment-analyzer)
* [pytorch/text-summarizer](https://github.com/cortexlabs/cortex/tree/0.18/examples/pytorch/text-summarizer)
* [pytorch/answer-generator](https://github.com/cortexlabs/cortex/tree/0.18/examples/pytorch/answer-generator)

In addition, on the master branch, we have [pytorch/text-generator](https://github.com/cortexlabs/cortex/tree/master/examples/pytorch/text-generator), which is what we'll use for the tutorial. The example should be able to run on v0.18 with a minor modification (removing the `kind: SyncAPI` from `cortex.yaml`).

Let us know if you have any questions!",thanks reaching yes agree fact recently decided update tutorial use hugging face ready next release come late next week tutorial still good sense use cortex use serve good starting point addition master branch use tutorial example able run minor modification removing kind let u know,issue,positive,positive,positive,positive,positive,positive
672261262,"@dsuess do you mind sharing how you approached installing CUDA 10.2? The way I was thinking it could work would be to clone the cortex repo and modify the base cuda image used in the first line of `images/python-predictor-gpu/Dockerfile`. You also might as well re-purpose [this section](https://github.com/cortexlabs/cortex/blob/0.18/images/python-predictor-gpu/Dockerfile#L42-L78) to install whichever packages you need (remove the `test ""${SLIM}"" = ""true""` part as well), or remove the section entirely if you plan on using a `requirements.txt`. You can then run `docker build -f images/python-predictor-gpu/Dockerfile` from the root of the repo to build the image.

I also spoke with @RobertLucian, and we came up with plan where we could build and publish a base image of `python-predictor-gpu-slim` for each CUDA version. I updated https://github.com/cortexlabs/cortex/issues/923 accordingly.",mind way thinking could work would clone cortex modify base image used first line also might well section install whichever need remove test slim true part well remove section entirely plan run docker build root build image also spoke came plan could build publish base image version accordingly,issue,positive,negative,negative,negative,negative,negative
670680100,"@deliahu I added an example deployment for the ResNet50 model for server-side batching for Inferentia. This is the link to the compiled model (with a fixed batch size of 5):
https://www.dropbox.com/s/yra52y2gqi8fm7f/rn50_fp16_compiled_b5_nc1.zip?dl=0

This means that for `examples/tensorflow/image-classifier-resnet50/cortex_inf_server_side_batching.yaml` config file, the model path has to be changed.",added example deployment model link model fixed batch size file model path,issue,negative,positive,neutral,neutral,positive,positive
670270328,"Thanks @zouyee, yes this one is up for grabs; let us know if you have any questions about this or if you'd be interested in discussing any of the design.",thanks yes one let u know interested design,issue,positive,positive,positive,positive,positive,positive
669103659,"Hey there,
this came up in a GitHub search for Server-Side Apply.
If you encounter problems or need help with implementing Server-Side Apply, feel free to get in touch with us (wg-api-expression) on the Kubernetes Slack or ping me here.
https://kubernetes.slack.com/archives/C0123CNN8F3",hey came search apply encounter need help apply feel free get touch u slack ping,issue,positive,positive,positive,positive,positive,positive
668183848,"Closing due to inactivity, feel free to follow up if you have additional questions",due inactivity feel free follow additional,issue,positive,positive,positive,positive,positive,positive
667616624,"@sladyn98 Thanks for reaching out!

We are planning on implementing https://github.com/cortexlabs/cortex/issues/1198 in the next week or so. I think it might be good to hold off until that change gets in, since it will lay the groundwork, and will also probably touch most of the places in the codebase that will be modified again for supporting a pre-existing API Gateway.

We actually aren't yet 100% sure we want to support the ability to use a pre-existing API Gateway. I think it could make sense in a world where we also support REST API Gateway (https://github.com/cortexlabs/cortex/issues/1197) (right now we only support HTTP), since there is a lot more configuration that a user might associate with a REST gateway (HTTP is not super configurable beyond routing). We plan on discussing this internally soon, since we have to make sure that the feature is worth the implementation and maintenance cost.

Does that make sense to you? Is there a specific reason that this feature will be useful for you, or that you think this will be useful for others?",thanks reaching next week think might good hold change since lay groundwork also probably touch supporting gateway actually yet sure want support ability use gateway think could make sense world also support rest gateway right support since lot configuration user might associate rest gateway super beyond routing plan internally soon since make sure feature worth implementation maintenance cost make sense specific reason feature useful think useful,issue,positive,positive,positive,positive,positive,positive
667531480,"@deliahu I would like to give this a shot, is it open for opening PR's",would like give shot open opening,issue,negative,neutral,neutral,neutral,neutral,neutral
667424817,Part of the error message is indeed contradictory/recursive: `please run cortex deploy to redeploy the api or cortex delete model to delete the api`. Running `cortex deploy` or `cortex delete` would return the same error.,part error message indeed please run cortex deploy redeploy cortex delete model delete running cortex deploy cortex delete would return error,issue,negative,neutral,neutral,neutral,neutral,neutral
663523124,Thanks for filing the issue with the reproduction steps and so much detail. We will look into it and try to surface a meaningful error when image download fails.,thanks filing issue reproduction much detail look try surface meaningful error image,issue,negative,positive,positive,positive,positive,positive
663174503,"@lezwon sounds good, thank you, keep us posted!",good thank keep u posted,issue,positive,positive,positive,positive,positive,positive
662768901,@deliahu Thank you for the help. I'll look into the issue you mentioned with eksctl. :),thank help look issue,issue,positive,neutral,neutral,neutral,neutral,neutral
662636766,"@lezwon thanks for your interest!

I think the first step is to figure out how to create an eks cluster with instances that have elastic inference attached. Currently, Cortex uses eksctl to create the cluster, and based on https://github.com/weaveworks/eksctl/issues/643, it looks like eksctl might not support elastic inference yet. But I am not sure if that's the case, or if there is a workaround; it could be worth reaching out to the eksctl team to inquire.

@RobertLucian or @vishalbollu, do you have any additional context on this?",thanks interest think first step figure create cluster elastic inference attached currently cortex create cluster based like might support elastic inference yet sure case could worth reaching team inquire additional context,issue,positive,positive,positive,positive,positive,positive
662539425,"Hi, I'd like to look into this issue if anyone can help me get started.",hi like look issue anyone help get,issue,positive,neutral,neutral,neutral,neutral,neutral
660222572,"To solve this kind of problem, it is necessary to modify the compute section of the cortex.yaml file by increasing the variables cpu and mem.",solve kind problem necessary modify compute section file increasing mem,issue,negative,positive,positive,positive,positive,positive
660069791,It seems like I didn't have the right permissions after all. AWS is very confusing :angry: Sorry for posting here,like right angry sorry posting,issue,negative,negative,negative,negative,negative,negative
659033531,"Unfortunately, at this point, there is no easy way to make the development cycle that much faster. I presume you are already using Cortex to deploy locally (to run a docker image) and use Cortex CLI to fetch the logs.

The predictor behaviour is fairly predictable. I tend to iterate on the business logic in a typical python development environment using virtual environments and then import the python modules into the Cortex predictor.

Thanks for bringing this up. There may be opportunities to set up live reloading to improve iteration speed. That improvement is being tracked by this issue https://github.com/cortexlabs/cortex/issues/983.",unfortunately point easy way make development cycle much faster presume already cortex deploy locally run docker image use cortex fetch predictor behaviour fairly predictable tend iterate business logic typical python development environment virtual import python cortex predictor thanks may set live improve iteration speed improvement tracked issue,issue,positive,positive,neutral,neutral,positive,positive
657633278,I think for this to usable (at least for gpu deployments) users need to be able to specify the set of instances available to the cluster. This is especially important if I have a model compiled with TensorRT and need a specific GPU arch.,think usable least need able specify set available cluster especially important model need specific arch,issue,negative,positive,positive,positive,positive,positive
656734326,"I 100% agree that not everyone uses AWS and it shouldn't be the case :).

Perhaps I should rephrase my thoughts. I was wondering in what scenarios would prefer Cortex local over Cortex cluster? Is it just because Cortex cluster isn't supported for other cloud providers/on-prem? Is it because Cortex local is easier to plug into existing infrastructure? How is Cortex local being used in existing infrastructure? This information would help us improve Cortex local by folding in more out of the box functionality and gives insights into how we can make Cortex cluster more flexible.

It sounds like a part of this conversation may overlap with this https://github.com/cortexlabs/cortex/issues/1092 thread.",agree everyone case perhaps rephrase wondering would prefer cortex local cortex cluster cortex cluster cloud cortex local easier plug infrastructure cortex local used infrastructure information would help u improve cortex local folding box functionality make cortex cluster flexible like part conversation may overlap thread,issue,positive,neutral,neutral,neutral,neutral,neutral
656671311,"@rdwrt if you go with a 3rd-party solution, then the common denominator is in implementing the access token validation mechanism on your Cortex API. 

In the `predict` method of your predictor class, you would first make a request to your auth server to validate your access token. If it checks out, make the prediction and return a response, otherwise, just return 403 in case the token is invalid or 401 if it's not present. You do this by returning a [starlette.responses.Response](https://www.starlette.io/responses/) object in your `predict` method.

The only downside to this is that you have to initiate the validation process directly on your resource server (your Cortex APIs) as opposed to doing it before that like it's the case with Amazon Cognito.

Your auth server is responsible for issuing you an access token (+ optional refresh token) if your authorization code checks you, and then you embed the access token in your request to your resource server (Cortex API).

There are multiple OAuth providers. 2 that I can think of are:
* https://auth0.com
* And Okta. Check this out: https://developer.okta.com/code/python/.

> There are valid reasons not to use AWS for this (existing contracts, funding for a specific cloud provider, company policies, existing on-premise infrastructure, etcetera)

Then it really depends on what your existing infrastructure already looks like. How is your authentication/authorization infrastructure configured?",go solution common denominator access token validation mechanism cortex predict method predictor class would first make request server validate access token make prediction return response otherwise return case token invalid present object predict method downside initiate validation process directly resource server cortex opposed like case server responsible issuing access token optional refresh token authorization code embed access token request resource server cortex multiple think check valid use specific cloud provider company infrastructure really infrastructure already like infrastructure,issue,positive,positive,neutral,neutral,positive,positive
656524336,"Hi Vishal, not everyone uses AWS! There are valid reasons for that (existing contracts, company policy, etc.)",hi everyone valid company policy,issue,negative,neutral,neutral,neutral,neutral,neutral
656523546,"The problem is this requires AWS. How do I set this up without using AWS?

There are valid reasons not to use AWS for this (existing contracts, funding for a specific cloud provider, company policies, existing on-premise infrastructure, etcetera)",problem set without valid use specific cloud provider company infrastructure,issue,negative,neutral,neutral,neutral,neutral,neutral
656323690,"@rdwrt got it. When you say local setup for Cortex, do you mean deploying Cortex APIs locally? (with the `-e local` option).

Regarding setting up OAuth to perform authorization, you can use AWS Cognito for that. This only works when the API is deployed using the `aws` provider. It integrates nicely with the AWS API Gateway. It's also managed, so that's great. You can check out the following resources on this matter:
* https://stackoverflow.com/questions/32236568/how-to-set-up-an-oauth2-authentication-provider-with-aws-api-gateway
* The AWS documentation on setting up AWS Cognito for API Gateway can be found here: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-integrate-with-cognito.html
* And a nice tutorial showing how AWS Cognito is set up can be found [here](https://medium.com/@awskarthik82/part-1-securing-aws-api-gateway-using-aws-cognito-oauth2-scopes-410e7fb4a4c0) and [here](https://medium.com/@awskarthik82/part-2-securing-aws-api-gateway-using-aws-cognito-oauth2-scopes-and-openid-connect-f20fd1392e97).

I really liked the following diagram, because it visually explains how OAuth2 authorization is performed:

![0*vXjRjS4vzOV9TFBh](https://user-images.githubusercontent.com/26958764/87085116-01a88580-c238-11ea-93ab-b13adc504847.png)",got say local setup cortex mean cortex locally local option regarding setting perform authorization use work provider nicely gateway also great check following matter documentation setting gateway found nice tutorial showing set found really following diagram visually authorization,issue,positive,positive,positive,positive,positive,positive
656260370,"> Hi Vishal, the use case is that also local networking traffic should go over https, also outside of AWS integration :)

I may not have interpreted this correctly, any particular reason for not deploying APIs using a Cortex cluster in the AWS environment? A lot of the common networking and TLS are provided out of the box when you use the cluster.",hi use case also local traffic go also outside integration may correctly particular reason cortex cluster environment lot common provided box use cluster,issue,negative,negative,neutral,neutral,negative,negative
656230660,Got it. TLS via an nginx reverse proxy would work for sure. Setting up nginx in front of Cortex local has other benefits like allowing access to multiple Cortex APIs via a single endpoint.,got via reverse proxy would work sure setting front cortex local like access multiple cortex via single,issue,positive,positive,positive,positive,positive,positive
656197891,"Hi Robert, I am thinking of a local setup for cortex, where something like OAuth is used to perform the authentication. It'll look something like:

![](http://blog.geveo.com/img/Tharindu1.png)

(example from just a random hit from Google: http://blog.geveo.com/OAuth-Implementation-for-WebAPI2 )
",hi thinking local setup cortex something like used perform authentication look something like example random hit,issue,positive,negative,negative,negative,negative,negative
656195395,"> SSL is not supported for Cortex local at te moment. Could you explain your use case for SSL enabled Cortex local?

Hi Vishal, the use case is that also local networking traffic should go over https, also outside of AWS integration :) 

I guess TLS via nginx reverse proxy can be applied?",cortex local te moment could explain use case cortex local hi use case also local traffic go also outside integration guess via reverse proxy applied,issue,negative,neutral,neutral,neutral,neutral,neutral
656162108,"@rdwrt https://www.cortex.dev/v/master/guides/api-gateway has been removed because the functionality for creating API Gateway for Cortex APIs has been automated as of Cortex v0.18. APIs will automatically be created with HTTPS (provided by AWS Rest API Gateway).

The API Gateway setup documentation can be found in docs v0.17: https://docs.cortex.dev/v/0.17/guides/api-gateway

SSL is not supported for Cortex local at the moment. Could you explain your use case for SSL enabled Cortex local?",removed functionality gateway cortex cortex automatically provided rest gateway gateway setup documentation found cortex local moment could explain use case cortex local,issue,negative,neutral,neutral,neutral,neutral,neutral
656153118,"@rdwrt we're gonna assign a higher priority to this, and we'll try to implement it. Thanks for bringing this up to us!

In the meantime, it's worth mentioning that the Cortex CLI creates an HTTP API Gateway when `cluster`ing up. **One way** for you is to do the auth with JWT. More on this here:
https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-jwt-authorizer.html

**Another workaround** would be to create a REST API Gateway just like it's shown [in here (this is an old guide for 0.17 btw)](https://docs.cortex.dev/v/0.17/guides/api-gateway) and then you'd set up API keys for your paths. When an API key doesn't match what the API GW has, then it's gonna respond with error code 403. You can either import or create API keys with the AWS API Gateway. More on this here:
https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-usage-plans.html

**A third** option would be to implement the auth yourself:
1. By using the `.env` file or the `predictor:env` field in the `cortex.yaml` config to export the API keys.
1. Using an S3 bucket to store the API keys. Your `predict` method would have to cross-reference it.
1. Using the [AWS Secrets Manager](https://docs.aws.amazon.com/secretsmanager/latest/apireference/Welcome.html).

Also, can you tell us more about the kind of auth you'd be more interested in? ",gon na assign higher priority try implement thanks u worth cortex gateway cluster ing one way another would create rest gateway like shown old guide set key match gon na respond error code either import create gateway third option would implement file predictor field export bucket store predict method would manager also tell u kind interested,issue,positive,positive,positive,positive,positive,positive
656138742,"Oh yes, I see now. My bad",oh yes see bad,issue,negative,negative,negative,negative,negative,negative
656131846,"@cristianmtr The `kind` key is a new key in the config that is on the master branch but has not been released yet. The examples you've mentioned contain this key because those examples are on the master branch and apply to Cortex version master. Since you are using Cortex 0.18, the sklearn iris-classifier document can be found here: https://github.com/cortexlabs/cortex/blob/0.18/examples/sklearn/iris-classifier/README.md.

At the top of the examples and docs on the master branch, we've added a warning `WARNING: you are on the master branch, please refer to the examples on the branch that matches your cortex version` to help subvert this. 

However, it looks like more work needs to be done to make it harder for users to accidentally view unreleased versions of docs and examples.

Thanks for bringing this to our attention. ",kind key new key master branch yet contain key master branch apply cortex version master since cortex document found top master branch added warning warning master branch please refer branch cortex version help subvert however like work need done make harder accidentally view unreleased thanks attention,issue,positive,positive,positive,positive,positive,positive
656128630,"This page doesn't exist: https://www.cortex.dev/v/master/guides/api-gateway

Any info on how to deploy SSL-enabled cortex API in general (also locally?)",page exist deploy cortex general also locally,issue,negative,positive,neutral,neutral,positive,positive
656125452,"+1 for this. Not having API authorization is a clear blocker for many deployments. 
Any projected release date for cortex with api authorization? ",authorization clear blocker many release date cortex authorization,issue,negative,positive,positive,positive,positive,positive
655605897,"@MrRace thank you a lot for reporting this. Indeed, this doesn't appear to be the right behavior for the webserver:

1. When a post request with a malformed JSON is made, a 400 error code should be returned. Instead, it gets a 500.
1. When the content-type is not set (i.e.`""Content-Type: ""`), your `payload` parameter should be a `bytes` object.

Also, when the user doesn't set the content-type for a curl request, curl will automatically set it to `multipart/form`. This can explain why you are getting a `starlette.datastructures.FormData` object. I presume you were using the `curl` utility for this. Check this out:
https://stackoverflow.com/questions/37718308/what-is-default-header-content-type-for-http-post-method-in-curl-form",thank lot indeed appear right behavior post request malformed made error code returned instead set parameter object also user set curl request curl automatically set explain getting object presume curl utility check,issue,negative,positive,positive,positive,positive,positive
655511116,"@MrRace, when an API is locally deployed with Cortex, the logs will get streamed and aggregated from the docker containers that are associated with the said API.

This effectively means that the aggregated logs from `cortex logs <api-name>` are not saved anywhere, they are only streamed from their respective containers - keep in mind that an API deployment may be composed of multiple containers. The end-user doesn't usually care about that.

Now, each Docker container actually stores the logs to a specific file `/var/lib/docker/containers/<container_id>/<container_id>-json.log`. The full IDs of each container can be extracted with `docker ps --no-trunc` command. 
For the argument's sake, let's say you deploy a `PythonPredictor` API - this API type only requires a single container for it to work. You then determine what's the container's full ID and then you `tail -f /var/lib/docker/containers/<container_id>/<container_id>-json.log`.

A better option is to just redirect the stdout of `cortex logs <api-name>` to a file. You can do it this way `cortex get <api-name> | tee <api-name>.log >/dev/null` or this way to put it in the background `cortex get <api-name> | tee <api-name>.log >/dev/null &`. And the good thing with running it in the background is that as soon as the API has been deleted, the command ends too, which means you don't have to do any cleanup. Then, to get the logs in real-time from your log file, just do a `tail -f <api-name>.log` or `less`, etc.",locally cortex get docker associated said effectively cortex saved anywhere respective keep mind deployment may composed multiple usually care docker container actually specific file full container extracted docker command argument sake let say deploy type single container work determine container full id tail better option redirect cortex file way cortex get tee way put background cortex get tee good thing running background soon command cleanup get log file tail le,issue,positive,positive,positive,positive,positive,positive
654399787,I'll go ahead and close this issue since we have https://github.com/cortexlabs/cortex/issues/715,go ahead close issue since,issue,negative,neutral,neutral,neutral,neutral,neutral
654399462,"I'll go ahead and close this now, feel free to comment here or create a new issue if you run into any issues!",go ahead close feel free comment create new issue run,issue,positive,positive,positive,positive,positive,positive
653787896,"@RobertLucian @dsuess I've uploaded the model to `s3://cortex-examples/onnx/yolov5-youtube/yolov5s.onnx` for now, and I can move it depending on what we decide regarding the API name",model move depending decide regarding name,issue,negative,neutral,neutral,neutral,neutral,neutral
653721220,"@dsuess thanks for the link. As for the write access, that was a false alarm (an old peculiarity of my setup). It now works!",thanks link write access false alarm old peculiarity setup work,issue,negative,negative,neutral,neutral,negative,negative
653720177,"The box is ticked as far as I can tell, so not sure what's going on.

For the ONNX file: https://drive.google.com/file/d/1p0nbSHUFpZhp6RxR2scNFaMk0ANHu_1e/view?usp=sharing

I've also included the steps how to create that ONNX file from the original repo, so you could also try to reproduce those setps",box ticked far tell sure going file also included create file original could also try reproduce,issue,negative,positive,positive,positive,positive,positive
653718204,"@dsuess Thank you a lot for adding this!

I tried pushing to your branch, but apparently, I don't have write permissions. I think this might be because `Allow edits from maintainer` hasn't been checked. This option should be somewhere on the right-hand side of the PR I think.

Also, I'd want to test this over the weekend. For that, do you have a public link to your ONNX model I could use?",thank lot tried pushing branch apparently write think might allow maintainer checked option somewhere side think also want test weekend public link model could use,issue,positive,positive,neutral,neutral,positive,positive
653715602,"OK, I've send through the ONNX file and fixed the things that needed fixing. So if someone with the necessary permissions can a) upload the ONNX file and change the path in the config and b) fix the failing test, we're good to go IMO.",send file fixed fixing someone necessary file change path fix failing test good go,issue,negative,positive,positive,positive,positive,positive
653694154,"> Does this help improve latency or just throughput? Are you running the API with `processes_per_replica=1` and `threads_per_process=1`? (note that `processes_per_replica` refers just to the uvicorn processes; ffmpeg will spawn additional processes). I'd be curious to understand how all of the threading/parallelism interplays, but also, there's no need to spend a lot of time on it, since it's just an example and not running in production :)

To be honest, the main reason for using ffmpeg-python was that I didn't want to implement the resize-padding for the hundredth time üòÑ 

I think it improves both (especially when you run on a GPU) since it allows you to run model inference and data preprocessing (decoding, resizing) in parallel. But I haven't done any tests and there are definitely better solutions if you actually need to optimize either performance metric.

> > async the video-download?
> 
> This is also an interesting one, similar to the discussion about ffmpeg. My intuition says that downloading the video async could increase throughput (and not latency) assuming `processes_per_replica>1` and/or `threads_per_process>1`. Like before, there's no need to spend a lot of time on this for the example's sake.

You're right on this one, this would only benefit throughput. And just increasing either `process_per_replica` or `threads_per_replica will probably be the better solution anyway
",help improve latency throughput running note spawn additional curious understand also need spend lot time since example running production honest main reason want implement hundredth time think especially run since run model inference data parallel done definitely better actually need optimize either performance metric also interesting one similar discussion intuition video could increase throughput latency assuming like need spend lot time example sake right one would benefit throughput increasing either probably better solution anyway,issue,positive,positive,positive,positive,positive,positive
653603698,"@andriy-tsvikla there are a couple of things that should probably be better explained:

1. `max_replica_concurrency` specifies the maximum number of connections a replica can hold at any given time. Increasing this number will avoid making the client receive 503s. The downside to having it set to a very big value is that if there are lots of requests coming in, then your clients might have to wait much longer to get a prediction back. In your case, since a prediction only takes <50 ms, you could set your `max_replica_concurrency` to something higher - like 100, 200. Increase it as much as you can until the latency starts going up. `max_replica_concurrency` should be higher than your concurrency level `processes_per_replica * threads_per_process` because you want to leave some headroom for when other nodes are being spun up - so that your service can still serve your clients and not drop them, albeit with higher latency. This field is set by default to 1024.

1. `target_replica_concurrency` specifies the number of in-flight requests the replica should strive to hold. If it goes over this threshold, an up-scale event is triggered, meaning that it's gonna spin up a new node for your API. This is set by default to `processes_per_replica * threads_per_process`. In your case, I'd probably let this be set to the default number (and not 1).

1. `max_replicas` this is just the number of nodes that can be associated with your API - since an API replica of yours occupies an entire node, then the maximum number of replicas it can have is exactly the number of nodes you can spin up (which is `max_replicas`).

To achieve higher levels of concurrency, you'd also have to set `processes_per_replica` and `threads_per_replica` to higher values. If your API replica gets 4 vCPU cores, then you should set `processes_per_replica` to the same value. From there, if you want to increase the concurrency level, just increase `threads_per_replica`. If each of them both is set to 1, then you can only process one request at a time, no matter what `max_replica_concurrency` and `target_replica_concurrency` is set to.

The documentation for all these fields can be found here:
https://docs.cortex.dev/v/master/deployments/api-configuration

Also, you can monitor your metrics by checking the CloudWatch dashboard we set for each API. You can get it by running `cortex get <API-name>`. You can find out more here:
https://docs.cortex.dev/guides/metrics

Also, could you share with us your `cortex.yaml` and `cluster.yaml` configs?
",couple probably better maximum number replica hold given time increasing number avoid making client receive downside set big value lot coming might wait much longer get prediction back case since prediction could set something higher like increase much latency going higher concurrency level want leave headroom spun service still serve drop albeit higher latency field set default number replica strive hold go threshold event triggered meaning gon na spin new node set default case probably let set default number number associated since replica entire node maximum number exactly number spin achieve higher concurrency also set higher replica set value want increase concurrency level increase set process one request time matter set documentation found also monitor metric dashboard set get running cortex get find also could share u,issue,positive,positive,positive,positive,positive,positive
653596873,"@dsuess this is really awesome!! I like how simple the API is: send a youtube link, and get back and annotated video!

> I am using ffmpeg-python (instead of the more standard opencv/scikkit-video) since it allows you to do large parts of the pre-processing (resizing & padding) in a separate process.

Does this help improve latency or just throughput? Are you running the API with `processes_per_replica=1` and `threads_per_process=1`? (note that `processes_per_replica` refers just to the uvicorn processes; ffmpeg will spawn additional processes). I'd be curious to understand how all of the threading/parallelism interplays, but also, there's no need to spend a lot of time on it, since it's just an example and not running in production :)

> The reason I am using youtube-dl (instead of passing in a video directly) is that we don't have to provide an example video ourselves.

Yes, I agree, this makes for a nice example!

> Would this be a good example? Any suggestions for improvements? 

Yes, I think this is a great example! I can't think of any suggestions for improvements at the moment.

> How do you want to store the ONNX file?

I will add it to our `cortex-examples` bucket; feel free to send me the model when it's ready (via google drive, dropbox, etc), and I can upload it there.

> async the video-download?

This is also an interesting one, similar to the discussion about ffmpeg. My intuition says that downloading the video async could increase throughput (and not latency) assuming `processes_per_replica>1` and/or `threads_per_process>1`. Like before, there's no need to spend a lot of time on this for the example's sake.

Thanks again for adding this!
",really awesome like simple send link get back video instead standard since large padding separate process help improve latency throughput running note spawn additional curious understand also need spend lot time since example running production reason instead passing video directly provide example video yes agree nice example would good example yes think great example ca think moment want store file add bucket feel free send model ready via drive also interesting one similar discussion intuition video could increase throughput latency assuming like need spend lot time example sake thanks,issue,positive,positive,positive,positive,positive,positive
653595103,"In the end all nodes are destroyed except initial one and it is processing requests from 2 sources, max_replicas: 2 didnt help.
I will continue on Monday and try to reproduce and log if possible.
Thanks for quick answer.",end except initial one didnt help continue try reproduce log possible thanks quick answer,issue,negative,positive,positive,positive,positive,positive
653591740,"At the time when you have multiple nodes and only one replica, do you mind running `cortex cluster info --debug` and sending the resulting zip file? This won't include any of your code, but if there is other sensitive information in there, feel free to email it to me at david@cortex.dev",time multiple one replica mind running cortex cluster sending resulting zip file wo include code sensitive information feel free,issue,positive,positive,positive,positive,positive,positive
653589719,"1. For me - yes, but i had to change some configuration values in order for cortex to spawn node, now i have 3 and 2 of them without replicas
```
g4dn.xlarge     on-demand   0          3190m / 3190m        14893404Ki / 14893404Ki   1 / 1
g4dn.xlarge     on-demand   1          0 / 3190m            1221529Ki / 14893404Ki    0 / 1
g4dn.xlarge     on-demand   0          3190m / 3190m        14893404Ki / 14893404Ki   1 / 1
```
2. Client stream video to server and server sends frames to model. Could be no gaps so thats why i try to spawn second instance at least for now,  have to handle it later.
3. 46-55ms with cortex
4. I second node spawned only once and without replica. On next check second node was down.
5.
```
env   api        status   up-to-date   requested   last update   avg request   2XX      5XX
aws   estimator live     1            1           10m           33.1385 ms    100775   2334
```",yes change configuration order cortex spawn node without client stream video server server model could thats try spawn second instance least handle later cortex second node without replica next check second node status last update request estimator live,issue,negative,negative,neutral,neutral,negative,negative
653588385,"update: Thanks to @RobertLucian, who pointed this out to me since I totally missed it: currently your API configuration has `max_replicas: 1`; in order to allow your API to scale to 2 replicas, try setting `max_replicas: 2` (or higher)",update thanks pointed since totally currently configuration order allow scale try setting higher,issue,positive,positive,positive,positive,positive,positive
653583378,"update: I see that you just closed this issue, feel free to ignore my previous message if it's been resolved üëç ",update see closed issue feel free ignore previous message resolved,issue,negative,positive,neutral,neutral,positive,positive
653583099,"@andriy-tsvikla What exactly is being measured by the timing data? Is it the total time the request takes, as seen by the client? (e.g. is it `time curl http://...`? If so, can you share where the client is running and where the VM is running (if they are different)? Also, are the VM and Cortex in the same AWS region?",exactly measured timing data total time request seen client time curl share client running running different also cortex region,issue,negative,positive,neutral,neutral,positive,positive
653582046,after additional testing from different request source increase was only ~8%,additional testing different request source increase,issue,negative,neutral,neutral,neutral,neutral,neutral
653579836,"@andriy-tsvikla thanks for reaching out, I can definitely help you debug this (it's not clear yet exactly what is causing it).

I have a few questions to start with:

1. Is this behavior reproducible? 
2. What is the request pattern of the two users? Is it fairly consistent, or are there long gaps between each request?
3. How long does a single prediction request take to fulfil?
4. When the second node is requested, does the replica ever run on it, or does the node remain empty the whole time and then get taken down?
5. Can you show the output of `cortex get` at the time when the second node is in the cluster, but no replica has been added to it? Ideally, run `cortex get --watch`, and see if the requested number of replicas changes at all during the time that the new node is added and then eventually removed.",thanks reaching definitely help clear yet exactly causing start behavior reproducible request pattern two fairly consistent long request long single prediction request take second node replica ever run node remain empty whole time get taken show output cortex get time second node cluster replica added ideally run cortex get watch see number time new node added eventually removed,issue,positive,positive,positive,positive,positive,positive
653006801,"You are correct, Cortex is not supported for Windows at the moment. The ticket for tracking Windows CLI support can be found here #715.
",correct cortex moment ticket support found,issue,negative,neutral,neutral,neutral,neutral,neutral
652750307,"Ok, I'll just start with overlaying the video/image first and add tracking later.",start first add later,issue,negative,positive,positive,positive,positive,positive
652749781,"Thanks for your quick reply!

@RobertLucian Not the highest priority on my list, so no need to rush it
@deliahu I think that's a good idea, even independently from this issue. We've used nginx to add https-support to a local deployment. But as means to debugging a complex deployment locally, a solution that behaves similar to AWS would be very useful",thanks quick reply highest priority list need rush think good idea even independently issue used add local deployment complex deployment locally solution similar would useful,issue,positive,positive,positive,positive,positive,positive
652592605,"@timsoraro Even with `max_instances` set to 1, the replica should restart if it fails. Feel free to reach out again if you notice it not restarting!",even set replica restart feel free reach notice,issue,positive,positive,positive,positive,positive,positive
652518114,"I'm sorry, I guess it was because I set the maximum_instances to 1. My bad. I'll report if that happens again when I set it to more than 1!",sorry guess set bad report set,issue,negative,negative,negative,negative,negative,negative
652482688,"Yes, the replica should automatically restart. Have you seen a case where it failed but did not restart?",yes replica automatically restart seen case restart,issue,negative,neutral,neutral,neutral,neutral,neutral
652102502,"@dsuess yes, that is correct, thank you for linking to #961; I'll go ahead and close this issue given that we have #961 to track this",yes correct thank linking go ahead close issue given track,issue,positive,neutral,neutral,neutral,neutral,neutral
651953775,"@dsuess Another thing we could try, if you think it would be helpful, is to see if we could create a guide that shows how to use e.g. NGINX (possibly also running in a docker container) to act as a reverse proxy for locally deployed Cortex APIs. With this guide-based approach, it wouldn't be ""built-in"" to Cortex, so it would require running NGINX separately, as well as manually updating the NGINX config when APIs are created or deleted. I have not tried this, but it seems to me like it should be possible.",another thing could try think would helpful see could create guide use possibly also running docker container act reverse proxy locally cortex approach would cortex would require running separately well manually tried like possible,issue,positive,neutral,neutral,neutral,neutral,neutral
651927735,"@dsuess yes, you can go ahead with this one. If you've got any questions along the way, please don't hesitate to give us a shout. I think going with the tracks (bounding boxes) overlayed over the actual video should be more interesting to the end-user when it's run. Otherwise, a client would be needed to draw the bounding boxes. What do you think?

I think a good template would be this example:
https://github.com/cortexlabs/cortex/tree/master/examples/keras/document-denoiser

Thanks!



",yes go ahead one got along way please hesitate give u shout think going bounding actual video interesting run otherwise client would draw bounding think think good template would example thanks,issue,positive,positive,positive,positive,positive,positive
651880596,@dsuess we could bump up the priority for this one and try to get it out of the door. Is this lack of feature a deal-breaker for you?,could bump priority one try get door lack feature,issue,negative,neutral,neutral,neutral,neutral,neutral
651657616,"I am happy to give this a shot as I've got some experience with both this particular model in ONNX and video-predictions in Cortex. 

What kind of output did you envision for the video part? Should we add a simple tracker and return tracks? ",happy give shot got experience particular model cortex kind output envision video part add simple tracker return,issue,positive,positive,positive,positive,positive,positive
651654601,"I think it's not possible at the moment with local deployments. Good news is that it's been raised before, see #961",think possible moment local good news raised see,issue,negative,positive,positive,positive,positive,positive
651653661,"I am interested in this feature as well. Our use case is that we have multiple models (where the output of one model is used for another model) and the deployments reference each other just using a different endpoint. Everything works fine in AWS, but we can't test this setup locally since we can't have multiple endpoints locally without setting a separate port.",interested feature well use case multiple output one model used another model reference different everything work fine ca test setup locally since ca multiple locally without setting separate port,issue,positive,positive,neutral,neutral,positive,positive
651444738,"@MrRace just to be sure, I tried the above too (using my suggestions) on the [TensorFlow Iris example](https://github.com/cortexlabs/cortex/tree/master/examples/tensorflow/iris-classifier) with the same `ab` concurrency levels and the same number of samples.

Out of 10,000 samples, only 25 got 503s, which is 0.25%. Reducing the concurrency level down to 990 for the benchmarking program got me no 503s, which is really great! That 0.25% is a result of the uneven process of distributing requests to each worker - so one worker might temporarily get more requests than it can handle and thus, it may throw some 503s. That's why, internally, we are adding 100 to what the user sets for the `max_replica_concurrency` field in the `cortex.yaml` config - to account for those small imbalances.",sure tried iris example concurrency number got reducing concurrency level program got really great result uneven process worker one worker might temporarily get handle thus may throw internally user field account small,issue,positive,positive,positive,positive,positive,positive
651409258,"@vishalbollu as per our conversation, it has been confirmed that the `limit_concurrency` param of the `uvicorn.run` function represents the maximum number of concurrent connections a worker can handle.",per conversation confirmed param function maximum number concurrent worker handle,issue,negative,positive,positive,positive,positive,positive
651242987,"@MrRace I looked through the source code and it appears a few things have changed. Therefore, the variables you have to tinker with are `CORTEX_MAX_PROCESS_CONCURRENCY` and `CORTEX_SO_MAX_CONN` and not `CORTEX_MAX_REPLICA_CONCURRENCY`, which was made redundant in one of the recent releases. Basically `CORTEX_MAX_PROCESS_CONCURRENCY` has been renamed to `CORTEX_SO_MAX_CONN`.

One thing to keep in mind is that when setting `CORTEX_MAX_PROCESS_CONCURRENCY`, you have to calculate it by taking `CORTEX_SO_MAX_CONN` and dividing it by the number of processes `processes_per_replica`. They are both integers.

Assuming you want to achieve a `max_replica_concurrency` of 10000 and you demand 4 `processes_per_replica`, you will have to set the following 2 variables in the `.env` file this way:
```bash
CORTEX_SO_MAX_CONN=10000
CORTEX_MAX_PROCESS_CONCURRENCY=2500
```

Does this make sense? Hopefully, it does. It can be a bit harder to get it since this is a workaround that tinkers with the internals of Cortex. With https://github.com/cortexlabs/cortex/issues/1182 solved, this workaround will no longer be required. ",source code therefore tinker made redundant one recent basically one thing keep mind setting calculate taking dividing number assuming want achieve demand set following file way bash make sense hopefully bit harder get since internals cortex longer,issue,negative,negative,neutral,neutral,negative,negative
651035773,"@RobertLucian  I have put a `.env` file(`CORTEX_MAX_REPLICA_CONCURRENCY=10000`) in the project directory, and restart the serving. However  `Exceeded concurrency limit` and `Service Unavailable POST` still happen.  My concurrency level is just  1000 and the total requests is 10000.  The `CORTEX_MAX_REPLICA_CONCURRENCY` setting seems not workÔºü
",put file project directory restart serving however concurrency limit service unavailable post still happen concurrency level total setting,issue,negative,neutral,neutral,neutral,neutral,neutral
651009755,"@MrRace yes, you are correct. `max_replica_concurrency` is only available for AWS deployments. I have created ticket https://github.com/cortexlabs/cortex/issues/1182 to address this.

Until then, a workaround is to drop into the root directory of your project a `.env` file within which you can set the `CORTEX_MAX_REPLICA_CONCURRENCY` variable, which basically overrides the default value of the `max_replica_concurrency` field (currently sitting at 1024). The `.env` file could have the following content:
```bash
CORTEX_MAX_REPLICA_CONCURRENCY=4096
```

You can read more about the `.env` file here:
https://docs.cortex.dev/deployments/predictors#project-files",yes correct available ticket address drop root directory project file within set variable basically default value field currently sitting file could following content bash read file,issue,negative,positive,positive,positive,positive,positive
650926994,"@RobertLucian  The `max_replica_concurrency` seems only for aws, how to modify the value in my local machine ? ",modify value local machine,issue,negative,neutral,neutral,neutral,neutral,neutral
650896212,"@MrRace that's really great! I'm delighted to hear it's working.

As for the error you are getting (`Service Unavailable POST` and `Exceeded concurrency limit`), that's because your throughput tester is exceeding the maximum number of in-flight requests the API can handle at any single point in time. Now, this maximum limit of which we are talking about `max_replica_concurrency` does not represent the maximum number of concurrent requests the API can immediately start processing, but the number of connections the API can hold concurrently. To avoid getting into this error again, just increase the value of `max_replica_concurrency` to something bigger - currently, it sits at 1024, so I may suggest you increase it to something more manageable, like 4096 or 8192. Bear in mind, there's a soft limit imposed by [Linux which is 65536](https://serverfault.com/a/769555). 

The downside to setting the concurrency limit to something higher is that if the cost of making a prediction is high enough, your API replica won't have enough compute power to keep up with your demand, which means, at some point (in seconds or minutes, depends on the traffic you get), your clients will start receiving 503s (aka `Service Unavailable POST` error), even if you've set the concurrency level at its highest. Setting `max_replica_concurrency` is generally set higher when you expect to:
1. Have a high number of concurrent requests going on at any point in time and still be able to process them over time.
1. Or if you expect fair bursts of traffic and you need a buffer to prevent the service from retrieving 503s.

Also, it is generally recommended to set `processes_per_replica` to the number of cores you've got available for your API replica. To scale the number of permissible concurrent requests, only tweak `threads_per_processes`. The concurrency level you can achieve on your API replica is determined by `processes_per_replica * threads_per_processes`.

You can find out more here:
https://docs.cortex.dev/deployments/api-configuration

Let me know if there are any other lingering questions.",really great delighted hear working error getting service unavailable post concurrency limit throughput tester exceeding maximum number handle single point time maximum limit talking represent maximum number concurrent immediately start number hold concurrently avoid getting error increase value something bigger currently may suggest increase something manageable like bear mind soft limit downside setting concurrency limit something higher cost making prediction high enough replica wo enough compute power keep demand point traffic get start aka service unavailable post error even set concurrency level highest setting generally set higher expect high number concurrent going point time still able process time expect fair traffic need buffer prevent service also generally set number got available replica scale number permissible concurrent tweak concurrency level achieve replica determined find let know,issue,positive,positive,positive,positive,positive,positive
650888272,"@RobertLucian It can work now!
In my local machine,  I  send concurrent requests to the server ,the server display message like `Service Unavailable POST` and `Exceeded concurrency limit.` By the way the stress test tool I used is `ab`, the command likeÔºö`ab -n 10000 -c 1000 -p single_task.json -T 'application/json' -r -k -s 120 http://localhost:8888/`. The result is:
```
Concurrency Level:      1000
Time taken for tests:   1.742 seconds
Complete requests:      10000
Failed requests:        5608
   (Connect: 0, Receive: 0, Length: 5608, Exceptions: 0)
Write errors:           0
Non-2xx responses:      5608
Keep-Alive requests:    0
Total transferred:      21738592 bytes
Total body sent:        2560000
HTML transferred:       20182584 bytes
Requests per second:    5740.69 [#/sec] (mean)
Time per request:       174.195 [ms] (mean)
Time per request:       0.174 [ms] (mean, across all concurrent requests)
Transfer rate:          12186.96 [Kbytes/sec] received
                        1435.17 kb/s sent
                        13622.14 kb/s total

```
We can see the number of `Failed requests` is too much. 
I set `threads_per_process : 4` and  `processes_per_replica : 8`.  Choose higher `processes_per_replica` sometimes seems work better,but it cost much more resources like memory and cpu. The `threads_per_process` seem no work when more concurrent requests come. I mean in my sense, more threads in a process can handle more requests, maybe it will cost more time(e.t. `Time per request` will higher). Therefore how and when should I set the value of  `threads_per_process`?
",work local machine send concurrent server server display message like service unavailable post concurrency way stress test tool used command result concurrency level time taken complete connect receive length write total transferred total body sent transferred per second mean time per request mean time per request mean across concurrent transfer rate received sent total see number much set choose higher sometimes work better cost much like memory seem work concurrent come mean sense process handle maybe cost time time per request higher therefore set value,issue,positive,positive,neutral,neutral,positive,positive
650739297,"@RobertLucian OK , I will try build a new IMAGE based on cortexlabs/python-predictor-cpu-slim:0.18.0. And try again, thanks for your prompt reply.",try build new image based try thanks prompt reply,issue,negative,positive,positive,positive,positive,positive
650738783,"@MrRace I cannot reproduce this with version 0.18 of the CLI and with the default images set. Therefore, I believe you must have used a different image for the `image` field in the `predictor` section. Version 0.18 has changed bits of the serving images, so if you use a custom version of the said images, then you'll have to rebuild them using version 0.18 as the base image.

Does this make sense? Waiting to see if this has been the issue for you.",reproduce version default set therefore believe must used different image image field predictor section version serving use custom version said rebuild version base image make sense waiting see issue,issue,negative,negative,negative,negative,negative,negative
650678997,@deliahu @RobertLucian  Great!  Thanks a lot! Can't wait to try!,great thanks lot ca wait try,issue,positive,positive,positive,positive,positive,positive
650580428,"@timsoraro I'm glad to hear that it's working well for you! I'd love to hear about your use case, and if you have any other suggestions or feature requests; if you're open to chatting feel free to email me at david@cortex.dev",glad hear working well love hear use case feature open chatting feel free,issue,positive,positive,positive,positive,positive,positive
650577060,"Gotcha, thanks! I miss that info in the docs.

Cortex is really awesome. I've been looking for something like that for so long, and not until I got really frustrated with Algorithmia that I searched for alternatives, found Cortex, and in 1 day set up everything to work in production (:",thanks miss cortex really awesome looking something like long got really found cortex day set everything work production,issue,positive,positive,positive,positive,positive,positive
650576086,"@timsoraro 

> is it because it is trying to have no downtime yet I set the number of max instances to 1?

Yes, that is exactly right! (in combination with the fact that the CPU / GPU / memory compute request for your API was likely too big to be able to fit two replica on a single instance)

> How can I force it to change?

See our documentation [here](https://docs.cortex.dev/troubleshooting/stuck-updating) (this page also has some info describing the behavior that you saw)
",trying yet set number yes exactly right combination fact memory compute request likely big able fit two replica single instance force change see documentation page also behavior saw,issue,positive,positive,positive,positive,positive,positive
650497246,Now everything is clear. Thanks a lot!,everything clear thanks lot,issue,positive,positive,positive,positive,positive,positive
650436712,"1. I can see how my explanation could have been confusing with my misuse of the word server. A replica is a copy of your application. You are correct that the aws resource associated with replicas EC2. However, multiple replicas can be scheduled on a single EC2 instance depending on the compute requirements of a replica and the compute available on an EC2 instance. For example, if 1 copy of your application such as the Cortex Predictor requires 1 CPU and 1 GB memory and you had an instance with 4 CPU and 4 GB memory available, you would be able to run 4 replicas on that instance.

2. The Cortex operator requires 1 t3.medium instance and is responsible for tasks such as responding to your CLI requests, performing API autoscaling, and other housekeeping processes. The instance allocated for the Cortex operator is always a single t3.medium and is separate from the pool of EC2 instances dedicated to your API. It just so happened that the example used t3.mediums for the APIs. If I wanted to serve models on GPU instances, my Cortex operator would still run on a t3.medium, not on one of the GPU instances.

I hope this provides some clarification. Thanks for the docs suggestions, there is definitely room for improvement :)",see explanation could misuse word server replica copy application correct resource associated however multiple single instance depending compute replica compute available instance example copy application cortex predictor memory instance memory available would able run instance cortex operator instance responsible housekeeping instance cortex operator always single separate pool example used serve cortex operator would still run one hope clarification thanks definitely room improvement,issue,positive,positive,positive,positive,positive,positive
650400508,"Thanks for your elaborative response, @vishalbollu! I really like the way you break down the price by different resources, very useful.

Just to clarify:
1. So what exactly a replica is? What aws resource is it? From what you describe it seems to be an instance for my apps (t3.medium, in your example).
2. Why do we need to use the same instance used for prediction (t3.medium, in your example) for the operator as well? Does it mean that if I want to spin only 1 GPU instance I would need to spin another one?

For the docs, I would explain better what I asked about above (I couldn't find a definition for replica and esk). And I'd suggest to give an example of was resources used like above with hyperlinks to their definition and their role.",thanks elaborative response really like way break price different useful clarify exactly replica resource describe instance example need use instance used prediction example operator well mean want spin instance would need spin another one would explain better could find definition replica suggest give example used like definition role,issue,positive,positive,positive,positive,positive,positive
650223152,"@timsoraro No worries! A replica is 1 copy of a server/worker. In the context of Cortex, it is one copy of a Cortex predictor that serves your model. You can specify how much [compute resources](https://docs.cortex.dev/deployments/compute) each replica needs in the `compute` section of your [API configuration](https://docs.cortex.dev/deployments/api-configuration). Multiple replicas can be scheduled onto an EC2 instance depending on the compute requirements per replica and the EC2 instance chosen for your Cortex cluster.

Cortex autoscales replicas based on incoming traffic and the autoscaling configuration and Cortex autoscales the number of EC2 instances based on the number of replicas that need to be scheduled and their compute requirements.

You can get a breakdown of the different AWS resources used by Cortex and their price by defining a [cluster configuration](https://docs.cortex.dev/cluster-management/config) file and running `cortex cluster up -c cluster.yaml`. You will be prompted for confirmation after the pricing information has been displayed so you can try different configurations and get their prices without actually spinning up a cluster.

You should get an output like this:
```
aws resource                              cost per hour
1 eks cluster                             $0.10
0 - 5 t3.medium instances for your apis   $0.0125 - $0.0416 each (varies based on spot price)
0 - 5 50gb ebs volumes for your apis      $0.007 each
1 t3.medium instance for the operator     $0.0416
1 20gb ebs volume for the operator        $0.003
2 network load balancers                  $0.0225 each

your cluster will cost $0.19 - $0.43 per hour based on cluster size and spot instance pricing/availability
...
would you like to continue? (y/n): 
```

You can find an overview of the high-level Cortex infrastructure [here](https://docs.cortex.dev/miscellaneous/architecture).

Feel free to let us know if there are any sections of the documentation that can be improved to provide more clarity.",replica copy context cortex one copy cortex predictor model specify much compute replica need compute section configuration multiple onto instance depending compute per replica instance chosen cortex cluster cortex based incoming traffic configuration cortex number based number need compute get breakdown different used cortex price cluster configuration file running cortex cluster confirmation information displayed try different get without actually spinning cluster get output like resource cost per hour cluster based spot price instance operator volume operator network load cluster cost per hour based cluster size spot instance would like continue find overview cortex infrastructure feel free let u know documentation provide clarity,issue,positive,positive,positive,positive,positive,positive
650199584,"Hi! @vishalbollu, sorry for my quite basic understanding, I'm just digging into your docs and I'm not knowledgable in Devop. What does a replica mean? Is it the sort of basic CPU that manages the (let's say) powerful GPU instances that run the model? Or is it the ec2 instances that run the model? Or am I not getting the structure right?

If so, I was mostly curious, and secondly, because my app is still under the radar I was wondering about it since I'm not getting many requests yet.

Also, I can't quite grasp the entire infrastructure I'd use and, much more importantly, how much I'm going to pay for everything. Can you break it down for me approximately? From my understanding there's:
1. The basic instance that manages the entire thing (or are there more than 1? Is that EKS?)
2. EC2 instance/s, either spot/on-demand.
3. ?

Please correct me if I'm wrong, I'm trying to understand this better.

Thanks!",hi sorry quite basic understanding digging replica mean sort basic let say powerful run model run model getting structure right mostly curious secondly still radar wondering since getting many yet also ca quite grasp entire infrastructure use much importantly much going pay everything break approximately understanding basic instance entire thing either please correct wrong trying understand better thanks,issue,positive,positive,neutral,neutral,positive,positive
650195549,"Hi @timsoraro Currently scaling a replica down to 0 is not possible. You can configure the minimum number of replicas for your API to minimize cold starts. The lowest you can configure the min value is 1. As you've mentioned, in order to scale down to 0, a mechanism is required to queue the request until the API becomes available.

It would be helpful if you could share your use case for APIs scaling down to 0 replicas in a production setting. Do you expect there to be a long gap between requests such as a request for batch inference once every 2 hours? Or perhaps your API only receives requests during certain time periods like from 9 AM to 5 PM so you don't need an API running at night.",hi currently scaling replica possible configure minimum number minimize cold configure min value order scale mechanism queue request becomes available would helpful could share use case scaling production setting expect long gap request batch inference every perhaps certain time like need running night,issue,positive,negative,neutral,neutral,negative,negative
649887659,"@zouyee I'll close this PR for now, feel free to reach out in the future if you plan on working on gRPC for users' APIs (see my comment above).",close feel free reach future plan working see comment,issue,positive,positive,positive,positive,positive,positive
649887162,"@zouyee I have incorporated your change into https://github.com/cortexlabs/cortex/pull/1167 (and https://github.com/cortexlabs/cortex/commit/239316ae7cb0596d87abe90419c86ea2edd9014d), so I will go ahead and close this one. I mostly just moved the logic you added into the operator, and added a new API status for it. Thanks for your contribution!",incorporated change go ahead close one mostly logic added operator added new status thanks contribution,issue,negative,positive,positive,positive,positive,positive
649788291,"If `CacheModels` function is ever to be called with no models, then the newline would mess up the output a bit. It's just better to check if any models have been printed to stdout and then print the newline as well. 

Currently, `CacheModels` is only called when there are models to be cached, but still, it's better to be safe than sorry.",function ever would mess output bit better check printed print well currently still better safe sorry,issue,positive,positive,positive,positive,positive,positive
649762679,"We haven't been using a YAML formatter, but maybe we should! Which one do you use?

Regardless, I think we should make a separate PR after this one if we do decide to reformat, to keep the diff in the PR more relevant",maybe one use regardless think make separate one decide keep relevant,issue,negative,positive,positive,positive,positive,positive
649730737,"Since we now support multi-model TensorFlow APIs, it is possible to chain multiple TensorFlow models in a single predictor.

In addition, it is possible to chain models of multiple different types (TensorFlow, Python, or ONNX) this by using a ""wrapper"" API. This wrapper API would be a [Python Predictor type](https://docs.cortex.dev/deployments/predictors#python-predictor), and would make prediction requests and perform the reduction in the `predict()` function. All inference APIs would autoscale independently.

If anyone has questions about either of these approaches, we can provide an example implementation.",since support possible chain multiple single predictor addition possible chain multiple different python wrapper wrapper would python predictor type would make prediction perform reduction predict function inference would independently anyone either provide example implementation,issue,negative,negative,neutral,neutral,negative,negative
649728555,"It is now possible to implement this by using a ""wrapper"" API. This wrapper API would be a [Python Predictor type](https://docs.cortex.dev/deployments/predictors#python-predictor), and would make prediction requests (in parallel if desired) and perform the reduction in the `predict()` function. All inference APIs would autoscale independently. If anyone has questions about this, we can provide an example implementation.",possible implement wrapper wrapper would python predictor type would make prediction parallel desired perform reduction predict function inference would independently anyone provide example implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
649725582,"@Magdiel3 v0.18 has now been released, which supports local images (here is the full [changelog](https://github.com/cortexlabs/cortex/releases/tag/v0.18.0)).

I'll go ahead and close this issue, feel free to reach out if you have additional questions!",local full go ahead close issue feel free reach additional,issue,positive,positive,positive,positive,positive,positive
649701598,I'm closing this question since we already have a GitHub issue to track our progress on on-premise deployments: https://github.com/cortexlabs/cortex/issues/150,question since already issue track progress,issue,negative,neutral,neutral,neutral,neutral,neutral
649638116,"@javithe7 sounds good, thanks!

I'll go ahead and close this question, since we have filed https://github.com/cortexlabs/cortex/issues/1161",good thanks go ahead close question since,issue,positive,positive,positive,positive,positive,positive
648947898,"Especially `cortex get` and  `cortex get model-name` could be good options, also the `cortex cluster info` command, when you want to take a look in the configuration of a cluster could be useful. 
Yes, json is a really good election for this task.
@deliahu Thanks a lot for considering the implementation of this functionality. ",especially cortex get cortex get could good also cortex cluster command want take look configuration cluster could useful yes really good election task thanks lot considering implementation functionality,issue,positive,positive,positive,positive,positive,positive
648932492,@MrRace I have great news for you. The option to set concurrency fields for local deployments is going to be possible with `0.18`. This feature is enabled by https://github.com/cortexlabs/cortex/pull/1158.,great news option set concurrency local going possible feature,issue,positive,positive,positive,positive,positive,positive
648931304,"I'll close this ticket now, feel free to reach out if you have additional questions",close ticket feel free reach additional,issue,positive,positive,positive,positive,positive,positive
648931211,"A possible alternative to mentioning the output format is: have an `-o/--output` option, with the allowed values being `json`, `yaml`, and `text`. `text` would be the default value so that the current output format is being kept.",possible alternative output format output option text text would default value current output format kept,issue,negative,neutral,neutral,neutral,neutral,neutral
648925152,"@javithe7 There is not currently a way of doing this, but it's something we can look into! I created https://github.com/cortexlabs/cortex/issues/1161 to track this.

Which command(s) other than `cortex get` would you want to support this option? Also, is json your preferred format?",currently way something look track command cortex get would want support option also preferred format,issue,negative,neutral,neutral,neutral,neutral,neutral
648517223,"@RobertLucian Yes I think that is a good point. It probably makes sense to have a few different IDs; right now, API ID is probably what we would end up calling ""predictor ID"": it is an ID that represents everything about your API container, and nothing outside of it (which is why things like `networking` and `autoscaling` are not included in that ID). This ID is useful for example if you want stats about your API; simply changing the endpoint or the autoscaling configuration should not e.g. create a new cloudwatch dashboard. In addition, as you said, having an ID that incorporates everything would be useful for seeing if anything has changed.

> This implementation could also be used until we revisit this at a later time.

Yes, I think we can go forward with this PR and discuss this matter in the future.",yes think good point probably sense different right id probably would end calling predictor id id everything container nothing outside like included id id useful example want simply configuration create new dashboard addition said id everything would useful seeing anything implementation could also used revisit later time yes think go forward discus matter future,issue,positive,positive,positive,positive,positive,positive
648476866,"Tested this with various examples. In these examples, it has been tested with different values for `processes_per_replica` and `threads_per_process`. This also included verifications that the CloudWatch dashboard, the endpoints and the multi-model feature work.",tested various tested different also included dashboard feature work,issue,negative,neutral,neutral,neutral,neutral,neutral
648250737," Yes. If we have the title **endpoint** it makes sense to remove endpoint. 

You're right with the newline. I removed the `fmt.Println()` because I thought it was me who put it there for testing lol. So no need for extra new line",yes title sense remove right removed thought put testing need extra new line,issue,negative,positive,positive,positive,positive,positive
648248666,"I think it looks great!

What do you think about removing ""endpoints"" from the items?

```text
**Endpoints**
operator:          ***
api load balancer: ***
api gateway:       ***
```

Also, I see a new line before `fetching cluster status ...` on my end, I wonder why it doesn't show up on yours (I think it comes from [here](https://github.com/cortexlabs/cortex/blob/info-api-gateway/cli/cmd/cluster.go#L428)).",think great think removing text operator load balancer gateway also see new line fetching cluster status end wonder show think come,issue,positive,positive,positive,positive,positive,positive
647891704,"@MrRace I see. In this case, knowing it's an urgency for you, we can assign a higher priority to this ticket and try to get it into 0.18. We are planning to cut the 0.18 release candidate tomorrow, so we can't make any promises that this will make it in.",see case knowing urgency assign higher priority ticket try get cut release candidate tomorrow ca make make,issue,negative,positive,positive,positive,positive,positive
647883546,"@RobertLucian  Do you have any advise for concurrent requests in local deployment current? It is a immediate emergency, I can not just wait.........",advise concurrent local deployment current immediate emergency wait,issue,negative,neutral,neutral,neutral,neutral,neutral
647879828,"@MrRace as far as I am aware, this is gonna land in `0.19`.",far aware gon na land,issue,negative,positive,positive,positive,positive,positive
647875559,@deliahu Looking forward to  concurrent requests  for local deployment!  Next versionÔºü,looking forward concurrent local deployment next,issue,negative,neutral,neutral,neutral,neutral,neutral
647717623,"`cpu: 50` configures how much CPU to allocate to each replica of the API. Since you are running locally, there will only be one replica, and the actual CPU available will be less (it will be limited by your machine and Docker configuration). The amount of CPU allocated does not affect the concurrency of the API.

Concurrent requests are not currently supported for local deployment (here is the issue we have to add it: https://github.com/cortexlabs/cortex/issues/960). It is supported on AWS, via `workers_per_replica` and `threads_per_worker` in the `autoscaling` field (see [here](https://docs.cortex.dev/deployments/autoscaling#replica-parallelism)).",much allocate replica since running locally one replica actual available le limited machine docker configuration amount affect concurrency concurrent currently local deployment issue add via field see,issue,negative,positive,neutral,neutral,positive,positive
647549152,"@Magdiel3 yes, that's correct. v0.18 will support local images for local Cortex deployments.",yes correct support local local cortex,issue,positive,neutral,neutral,neutral,neutral,neutral
647528182,"Thanks for all the help. Everything seems to be working locally. There was just an issue with trying to use the local Docker image and got this error: 

```
error: ...: image: docker_images/starspace_image is not accessible
Error response from daemon: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
```

The `cortex.yaml` contained  
```
predictor:
    type: python
    path: pexpect_test.py
    image: docker_images/starspace_image:v1
```

How we managed to work that out was using DockerHub and updating the `cortex.yaml` to:
```
  predictor:
    type: python
    path: pexpect_test.py
    image: magdiel3/recommender-cortex:v1
```

As I read from [this comment](https://github.com/cortexlabs/cortex/issues/1094#issuecomment-640693855) on #1094, there will be support for using local images then v0.18 is released, isn't it?",thanks help everything working locally issue trying use local docker image got error error image accessible error response daemon access resource unauthorized authentication predictor type python path image work predictor type python path image read comment support local,issue,negative,positive,positive,positive,positive,positive
647015289,"@Magdiel3 I found the installation instructions for Boost in the [Starspace readme](https://github.com/facebookresearch/StarSpace#requirements).

We can add the instructions to install boost in the Dockerfile. It looks like we need wget to download boost so for ease of use let's install wget as well. Your Dockerfile might look something like this:

```
FROM cortexlabs/python-predictor-cpu-slim:<your cortex version>

RUN <install libraries such as cmake and make to compile the library>

# install wget
RUN apt-get update \
  && apt-get install -y wget

# download boost
RUN wget https://dl.bintray.com/boostorg/release/1.63.0/source/boost_1_63_0.zip \
  && unzip boost_1_63_0.zip \
  && mv boost_1_63_0 /usr/local/bin

RUN git clone https://github.com/facebookresearch/StarSpace

RUN cd StarSpace/python && ./build.sh
...
```",found installation boost add install boost like need boost ease use let install well might look something like cortex version run install make compile library install run update install boost run run git clone run,issue,positive,neutral,neutral,neutral,neutral,neutral
647011611,"This looks like an interesting idea. I have a few points:

If each thread runs its own predictor constructor, does this mean that each thread loads a model into memory separately? At that point, it may be safer to use processes instead of threads. If that is the case, we can also only initialize the threadpool if the thread is of size 1 (to avoid OOM). At least this way, running TensorFlow sessions should work by default when threads_per_worker is set to 1.

Another consideration, which isn't mutually exclusive to the above, is to import the threadpool from serve.py in the predictor implementation and run initializations in the predictor's constructor. We can have a guide showcasing this pattern if it works (and I am not sure that it does work).

",like interesting idea thread predictor constructor mean thread model memory separately point may use instead case also initialize thread size avoid least way running session work default set another consideration mutually exclusive import predictor implementation run predictor constructor guide pattern work sure work,issue,positive,positive,neutral,neutral,positive,positive
647008396,@vishalbollu Indeed I was working with a binary compiled for Mac OSX. I get the idea of compiling in the Dockerfile and appreciate the help for starting the Dockerfile. I have not yet been able to make it work as I'm new with Docker images and I am not sure how to download and place [Boost](https://www.boost.org/) which is another requirement for Starspace as described [here](https://github.com/facebookresearch/StarSpace#requirements). ,indeed working binary mac get idea appreciate help starting yet able make work new docker sure place boost another requirement,issue,positive,positive,positive,positive,positive,positive
646926006,"Ran the following tests to double-check everything is still okay:

1. Tested every Cortex deployment that uses Inferentia ASICs. 
1. Tested the multi-model examples.
1. Tested some generic single-model examples (PyTorch, TensorFlow, ONNX).
1. Verified that the CloudWatch dashboard works.
1. Converted an Inferentia-provisioned example to use multiple models with the TF Predictor and tested it. This example is not present in the PR per se.
1. Checked that the Autoscaler spins up/down new/old nodes when the situation asks for it.",ran following everything still tested every cortex deployment tested tested generic dashboard work converted example use multiple predictor tested example present per se checked situation,issue,negative,neutral,neutral,neutral,neutral,neutral
646898167,"I'll close this issue now, feel free to reach out if you have additional questions about this",close issue feel free reach additional,issue,positive,positive,positive,positive,positive,positive
646882012,We appreciate contributions but we can't commit to accepting any pull request. I'd be happy to set up some time to chat to make sure we're on the same page. Feel free to email me at omer@cortexlabs.com,appreciate ca commit pull request happy set time chat make sure page feel free omer,issue,positive,positive,positive,positive,positive,positive
646878479,"@ospillinger Glad to hear your feedback! Did I get it right, that you could accept our pull request, if we do? ",glad hear feedback get right could accept pull request,issue,positive,positive,positive,positive,positive,positive
646876305,"Thanks for reaching out, puzl.ee sounds cool! We are focused on AWS for now but we'd be happy to advise if you want to build the integration on your end.",thanks reaching cool happy advise want build integration end,issue,positive,positive,positive,positive,positive,positive
646814623,"@MrRace to do this in a Dockerfile, as opposed to changing the Python version by dropping a different one in a [`conda-packages.txt`](https://docs.cortex.dev/deployments/python-packages#conda-packages), you can do it like in the following:

```Dockerfile
FROM cortexlabs/python-predictor-cpu-slim:0.17.0

# to install the new Python version
# pip packages have to be reinstalled for the new version
RUN pip uninstall -yr /src/cortex/serve/requirements.txt && \
    conda install -y conda-forge::python=3.8 && \
    conda clean -a && \
    pip --no-cache-dir install -r /src/cortex/serve/requirements.txt

# you now have Python 3.8 installed at this point
# your stuff here
```

Just keep in mind that the Cortex team cannot guarantee the stability of the system when a different version of Python is used - the reason is that we haven't tested it against other versions.",opposed python version dropping different one like following install new python version pip new version run pip install clean pip install python point stuff keep mind cortex team guarantee stability system different version python used reason tested,issue,positive,positive,positive,positive,positive,positive
646671796,"@Magdiel3  Is it possible that your binary is compiled on an architecture that is different from the run time. What OS is running on your machine? The Docker runtime is Linux based. If you compiled your binary on Mac OSX, you may run into issues.

To be completely safe, instead of compiling the binary on your machine, you can try compiling it in the Dockerfile to ensure that the binary is compatible with the Docker runtime.

You can find the instructions for creating your own Dockerfile here: https://docs.cortex.dev/deployments/system-packages#custom-docker-image

As a rough starting point:
```
FROM cortexlabs/python-predictor-cpu-slim:<your cortex version>

RUN <install libraries such as cmake and make to compile the library>

RUN git clone https://github.com/facebookresearch/StarSpace

RUN cd StarSpace/python && make
```",possible binary architecture different run time o running machine docker based binary mac may run completely safe instead binary machine try ensure binary compatible docker find rough starting point cortex version run install make compile library run git clone run make,issue,positive,positive,neutral,neutral,positive,positive
646591587,"@RobertLucian I have already tried with the `/bin/bash` but still dows'nt works. Now I'm getting an error at the execution of the CLI command:
```
./query_predict_placement_id: ./query_predict_placement_id: cannot execute binary file
```

So basically my goal is to run the c++ application (compiled in my machine as specified [here](https://github.com/facebookresearch/StarSpace/tree/master/python#how-to-build) and all my documents are in the same directory as well",already tried still work getting error execution command execute binary file basically goal run application machine directory well,issue,negative,neutral,neutral,neutral,neutral,neutral
646437750,"@Charlitos96 I appreciate your interest in contributing!

Yes, we will need to add `post_predict()` as an optional function in `PYTHON_CLASS_VALIDATION`, `TENSORFLOW_CLASS_VALIDATION`, and `ONNX_CLASS_VALIDATION` in [predictor.py](https://github.com/cortexlabs/cortex/blob/master/pkg/workloads/cortex/lib/type/predictor.py#L166) (you can add a new top-level key to the dictionary, similar to ""required"", but called ""optional""). Then the validation should be automatically handed.

I think a reasonable schema is:

```python
***_CLASS_VALIDATION = {
    ""required"": [
        # ...
    ],
    ""optional"": [
        {
            ""name"": ""predict"",
            ""required_args"": [""self""],
            ""optional_args"": [""response"", ""payload"", ""query_params"", ""headers""],
        },
    ],
}
```

Where the args are the same as `predict()`, plus it also includes the return value from `predict()` (called `response`). Do you think that makes sense?

Then, the user's `post_predict()` function will be added to the [background tasks](https://www.starlette.io/background/) (if it's defined) in [serve.py](https://github.com/cortexlabs/cortex/blob/master/pkg/workloads/cortex/serve/serve.py#L184) at some point after `predictor_impl.predict(**args)` (since we need to pass in the result). We currently only add background tasks if monitoring is enabled (see [here](https://github.com/cortexlabs/cortex/blob/master/pkg/workloads/cortex/serve/serve.py#L210-L213)), so the logic of setting the background tasks in `predict(request: Request)` will have to be updated a little, but not much. 

Finally, we will need to update the docs, specifically [predictors.md](https://github.com/cortexlabs/cortex/blob/master/docs/deployments/predictors.md).

Does that all make sense? Let us know if you have additional questions!",appreciate interest yes need add optional function add new key dictionary similar optional validation automatically handed think reasonable schema python optional name predict self response predict plus also return value predict response think sense user function added background defined point since need pas result currently add background see logic setting background predict request request little much finally need update specifically make sense let u know additional,issue,positive,positive,neutral,neutral,positive,positive
646347650,"@Magdiel3 thank you for coming back with more information. Your goal appears to be suited for cortex, after all, you're trying to make predictions, so let's see how we can fix this for you.

I see that this is failing at

```python
child = pexpect.spawn(""./query_predict_placement_id models/model_v02 300 datasets/dataset_extended.txt datasets/labels.txt 9086"",timeout=600)
```

Could you change that to:

```python
child = pexpect.spawn(""/bin/bash ./query_predict_placement_id models/model_v02 300 datasets/dataset_extended.txt datasets/labels.txt 9086"",timeout=600)
```

And then see if it's working?

If this isn't working, could you share your environment with us (make, executable, and all afferent files)? This is for the purpose of reproducing your error and then that of finding a solution.",thank coming back information goal cortex trying make let see fix see failing python child could change python child see working working could share environment u make executable afferent purpose error finding solution,issue,negative,neutral,neutral,neutral,neutral,neutral
646330368,"What I'm doing is starting the application within the python script (when `__init__` is called) and then just input and listen with the `predict` function.

I have not tried to start the application from dependencies.sh and then just keep listening, which I'm not sure how I could do that. Here it is how the `pexpect_test.py`:

```
import pexpect

class PythonPredictor:
    def __init__(self,config):
        print(""getting it to run"")
        child = pexpect.spawn(""./query_predict_placement_id models/model_v02 300 datasets/dataset_extended.txt datasets/labels.txt 9086"",timeout=600)
        child.expect(""STARSPACE-2018-2"")
        # print(child.before.decode(""utf-8""))
        child.expect("".*Enter some text:"")
        # print(child.before)
        self.child = child
        print(""Succesfully initialized"")

    def predict(self,payload):
        print(f""Getting predictions for {payload}"")
        self.child.sendline(payload)
        self.child.expect(payload)
        self.child.expect(f""Enter some text:"")
        output = self.child.before.decode(""utf-8"")
        # print(output)
        return output
```

I'm running an executable compiled with make on my local machine. And also, it's just one application that I'm running and want to keep it running and serving. The loading of the model (the command that I'm spawning) takes arround 7 to 8 min to start listening for an input. I really appreciate the intention of a call, and maybe my goal is not very suited for cortex, but I find it super cool to just deploy and serve which such an ease. ",starting application within python script input listen predict function tried start application keep listening sure could import class self print getting run child print enter text print child print predict self print getting enter text output print output return output running executable make local machine also one application running want keep running serving loading model command spawning min start listening input really appreciate intention call maybe goal cortex find super cool deploy serve ease,issue,positive,positive,positive,positive,positive,positive
646319764,"@Magdiel3 I think that might be because the shebang hasn't been set for your executable script (which I assume it's a script from what I can tell) or if you haven't added the `/bin/bash` shell to your command.

There's already a way to execute scripts and that's by adding a script called `dependencies.sh` to the root directory of your Cortex project. That script is spawned with the `/bin/bash` shell, so there's no need for you to add the shebang. The documentation for this can be found at https://docs.cortex.dev/deployments/system-packages#bash-script.

Also, if this isn't what you're looking for, could you then share more about your `pexpect_test.py` script? I could have a look at it and see what's wrong with it. If this is confidential, I could jump on a call or you could email me the specifics at robert.lucian.chiriac@gmail.com.
",think might shebang set executable script assume script tell added shell command already way execute script root directory cortex project script shell need add shebang documentation found also looking could share script could look see wrong confidential could jump call could,issue,negative,negative,negative,negative,negative,negative
646298424,"Hi @deliahu, I'm looking for an Issue to do my first contribution, can you give me more details about what you want to do in `post_predict()`??
Also, can you confirm me that the file to be modified is this [predictor.py](https://github.com/cortexlabs/cortex/blob/dd9d8b3967f1046489253dfeb82ed0b408ef2f71/pkg/workloads/cortex/lib/type/predictor.py) ?

",hi looking issue first contribution give want also confirm file,issue,negative,positive,positive,positive,positive,positive
646013610,"@MrRace It sounds like you are using PythonPredictor to deploy your models locally. You have correctly noticed that local models can be used with the PythonPredictor by placing the model in the same folder or in a subdirectory of the folder containing your `predictor.py`. The mounting behaviour isn't exactly associated with your predictor.py, but rather the location of your `cortex.yaml`. Before Cortex runs your API locally, it mounts the directory containing your `cortex.yaml` to `/mnt/project`. 

If you multiple APIs that share models you could have a `models` directory in the same directory as `cortex.yaml` (or in a subdirectory of the folder containing your cortex.yaml).

```
my_project
‚îú‚îÄ‚îÄ cortex.yaml
‚îú‚îÄ‚îÄ predictor_a.py
‚îú‚îÄ‚îÄ predictor_b.py
‚îî‚îÄ‚îÄ models/
    ‚îú‚îÄ‚îÄ model1
    ‚îú‚îÄ‚îÄ model2
    ‚îî‚îÄ‚îÄ model3
```

The model folder would be mounted into all APIs defined in that `cortex.yaml` file.
In your predictor implementations, you could load your models using `load('./models/model1')`, `load('./models/model2')`.",like deploy locally correctly local used model folder folder mounting behaviour exactly associated rather location cortex locally directory multiple share could directory directory folder model model model model folder would mounted defined file predictor could load load load,issue,positive,positive,neutral,neutral,positive,positive
646001474,"> I suppose it is normal and that the only way would be to access it from a cli of the old version, or to create a new updated cluster, right?

@javithe7, yes you are correct!",suppose normal way would access old version create new cluster right yes correct,issue,positive,positive,positive,positive,positive,positive
645879644,"Hi @deliahu , thanks a lot for your help, this method worked for me.
Although I know that the issue is closed, I have a little final doubt. When a cluster was created using an old version of cortex (e.g 0.14), you can't access it using an updated cortex cli (e.g 0.17).
I suppose it is normal and that the only way would be to access it from a cli of the old version, or to create a new updated cluster, right?

 `error: cluster_cortex_1.yaml: image_manager: the specified image (cortexlabs/manager:0.14.0) has a tag (0.14.0) which does not match your Cortex version (0.17.1); please update the image tag, remove the image registry path from your configuration file (to use the default value), or update your CLI `",hi thanks lot help method worked although know issue closed little final doubt cluster old version cortex ca access cortex suppose normal way would access old version create new cluster right error image tag match cortex version please update image tag remove image registry path configuration file use default value update,issue,positive,positive,neutral,neutral,positive,positive
645693750,"@sachaarbonel Thanks for reaching out! Yes, this will mean that you'll be able to add any container you'd like to a Cortex API. Once this feature is implemented, you'll be able to use rust-bert by building a docker image with rust-bert installed and a server which receives a prediction request, runs rust-bert, and responds with the result.",thanks reaching yes mean able add container like cortex feature able use building docker image server prediction request result,issue,positive,positive,positive,positive,positive,positive
645691711,@javithe7 I'll close this issue since I believe it's been addressed. Feel free to reach out if you have more questions.,close issue since believe feel free reach,issue,positive,positive,positive,positive,positive,positive
645657462,"I was about to open an issue when I saw this one. Does it mean we'll be able to use [rust-bert](https://github.com/guillaume-be/rust-bert) to run inferences with rust for example? I don't understand the difference between this and contributing new predictor types through PRs? Just discovered cortex, great work.",open issue saw one mean able use run rust example understand difference new predictor discovered cortex great work,issue,positive,positive,positive,positive,positive,positive
645643916,"We are using https://github.com/sportradar/aws-azure-login to generate a token with azure AD.
You would run `aws-azure-login --profile <profile_name>` which would generate a token that would be valid for 10 hours. Having to also run `cortex configure` as well would not be ideal.",generate token azure ad would run profile would generate token would valid also run cortex configure well would ideal,issue,positive,positive,positive,positive,positive,positive
645642142,"@sp-davidpichler the deployed docker image uses the credentials that you set up the cluster with (or [`cortex_aws_access_key_id`](https://docs.cortex.dev/miscellaneous/security#iam-permissions) if you specified that during cluster creation), so it should be possible to support `aws_session_token` for the CLI commands.

You are correct that it might be nice check for credentials in `~/.aws` and/or in your environment variables to support this, since updating `~/.cortex/cli.yaml` (or running `cortex env configure`) each time the session token changes could be cumbersome. Does the session token in `~/.aws/credentials` automatically get regenerated over time, and if so, how does that process work?",docker image set cluster cluster creation possible support correct might nice check environment support since running cortex configure time session token could cumbersome session token automatically get time process work,issue,positive,positive,positive,positive,positive,positive
645637482,"Another thing: when you do cortex deploy, does the deployed docker image use your currently configured credentials or the credentials you set up the cluster with? If it's your current credentials this probably can't be supported.",another thing cortex deploy docker image use currently set cluster current probably ca,issue,negative,neutral,neutral,neutral,neutral,neutral
645626519,It seems this might be problematic as the credentials are hard coded into ~/.cortex/cli.yaml. Would it be better to support a reference to an aws profile in ~/.aws/credentials?,might problematic hard would better support reference profile,issue,negative,positive,positive,positive,positive,positive
644961578,"> Is it currently necessary to convert fastapi to golang implementation? I found that no matter tf or onnx support grpc.

That's correct, I think converting fastai to golang would create a new issue, which is how to run the user's Python code.

One option could be to add a sidecar gRPC service which forwards requests to the FastAPI container, do you think that would work? Do you know how much overhead this would introduce?",currently necessary convert implementation found matter support correct think converting would create new issue run user python code one option could add sidecar service forward container think would work know much overhead would introduce,issue,positive,positive,positive,positive,positive,positive
644957150,"@zouyee Yes, there is currently tight coupling to aws. This is because in order to implement some of our features, like spot instances, we had to use many aws-specific tools. Adding support for additional cloud providers is something we will eventually want to do, but is not on our immediate roadmap because it will be a large effort. We have GitHub issues for supporting [GCP](https://github.com/cortexlabs/cortex/issues/114), [Azure](https://github.com/cortexlabs/cortex/issues/811), and [on-premise](https://github.com/cortexlabs/cortex/issues/150).",yes currently tight coupling order implement like spot use many support additional cloud something eventually want immediate large effort supporting azure,issue,positive,positive,positive,positive,positive,positive
644888788,@HowJMay absolutely; feel free to follow up on this thread if you have any questions,absolutely feel free follow thread,issue,positive,positive,positive,positive,positive,positive
644886248,Could you give me about one week to examine myself and ensure my ability to implement this requirement? ,could give one week examine ensure ability implement requirement,issue,positive,neutral,neutral,neutral,neutral,neutral
644881934,"@HowJMay yes this is still open, and as far as I know, it is not currently being worked on",yes still open far know currently worked,issue,negative,positive,neutral,neutral,positive,positive
644754235,"> @zouyee Thank you for taking a stab at this!
> 
> Unfortunately the title that we wrote for #1056 was not clear; it was our intention for this to refer to the user's APIs that are generated with `cortex deploy`, not the Cortex operator's API (I have now updated the title from ""Add support for gRPC requests"" to ""Add support for gRPC requests to user's APIs""). It is not a high priority for us to convert the Cortex operator's API to gRPC, since it seems to be working well enough as implemented today.
> 

If we need user API to support grpc (as mentioned above, convert the implementation languageÔºü),  it can also be implemented, but operator API is also required for grpc support.

",thank taking stab unfortunately title wrote clear intention refer user cortex deploy cortex operator title add support add support user high priority u convert cortex operator since working well enough today need user support convert implementation also operator also support,issue,positive,negative,neutral,neutral,negative,negative
644640482,Currently the code shows tight coupling to aws ‚ù§Ô∏è,currently code tight coupling,issue,negative,negative,neutral,neutral,negative,negative
644632798,"

> Feel free to unassign this issue, or if you are still interested in adding gRPC support to the user's APIs, let me know and we can discuss how we might want to approach this (it's tricky since right now, we use FastAPI for the user's APIs, and FastAPI doesn't seem to support gRPC).

Is it currently necessary to convert fastapi to golang implementation ? I found that no matter tf or onnx support grpc.

",feel free issue still interested support user let know discus might want approach tricky since right use user seem support currently necessary convert implementation found matter support,issue,positive,positive,positive,positive,positive,positive
644540417,"@javithe7 I have created https://github.com/cortexlabs/cortex/pull/1127 to add this information to the documentation ([here it is in markdown](https://github.com/cortexlabs/cortex/blob/34c26948b8e95620369a998c9537b0018157989e/docs/miscellaneous/environments.md#example-configuring-the-cortex-cli-on-a-new-machine-after-your-cluster-is-running)). Thanks for bringing it to our attention!

Also, when adding this section, I realized that I made a mistake in my previous message: I wrote `cortex configure`, but it should have been `cortex env configure`. The `cortex cluster info` command should work as I wrote it.

Were you able to configure the CLI on your new machine?",add information documentation markdown thanks attention also section made mistake previous message wrote cortex configure cortex configure cortex cluster command work wrote able configure new machine,issue,negative,positive,positive,positive,positive,positive
644534130,"I see what you're saying... however there is a nuance here, which is that the status we show in `cortex get` is a combination of the statuses of all the API replicas. This is computed in [`getStatusCode()`](https://github.com/cortexlabs/cortex/blob/master/pkg/operator/operator/status.go#L163), and is why we took the approach of wrapping the k8s statuses (since we eventually have to end up with a single descriptive status for the entire k8s deployment). Does that make sense?",see saying however nuance status show cortex get combination took approach wrapping since eventually end single descriptive status entire deployment make sense,issue,negative,negative,neutral,neutral,negative,negative
644532930,"@zouyee Thank you for taking a stab at this!

Unfortunately the title that we wrote for https://github.com/cortexlabs/cortex/issues/1056 was not clear; it was our intention for this to refer to the user's APIs that are generated with `cortex deploy`, not the Cortex operator's API (I have now updated the title from ""Add support for gRPC requests"" to ""Add support for gRPC requests to user's APIs""). It is not a high priority for us to convert the Cortex operator's API to gRPC, since it seems to be working well enough as implemented today.

Feel free to unassign this issue, or if you are still interested in adding gRPC support to the user's APIs, let me know and we can discuss how we might want to approach this (it's tricky since right now, we use FastAPI for the user's APIs, and FastAPI doesn't seem to support gRPC).",thank taking stab unfortunately title wrote clear intention refer user cortex deploy cortex operator title add support add support user high priority u convert cortex operator since working well enough today feel free issue still interested support user let know discus might want approach tricky since right use user seem support,issue,positive,positive,neutral,neutral,positive,positive
644408821,"https://github.com/kubernetes/autoscaler/issues/3109 has been resolved (by https://github.com/kubernetes/autoscaler/pull/3185), so perhaps we can resolve this with the next cluster autoscaler release",resolved perhaps resolve next cluster release,issue,positive,neutral,neutral,neutral,neutral,neutral
643724034,"@fabrizzioalco yes, absolutely you can work on it! As far as I know there has not been any progress on this yet.",yes absolutely work far know progress yet,issue,positive,positive,positive,positive,positive,positive
643351106,"@javithe7 yes this is possible, and we will add it to the environments documentation soon!

You can use the [`cortex configure`](https://docs.cortex.dev/miscellaneous/cli#env-configure) command on your other machine, and it will prompt you for the information necessary to connect to your cluster.

You could also copy over `cluster1.yaml` and run `cortex cluster info --config cluster1.yaml --env cluster1` which will automatically configure the ""cluster1"" environment in your cli .",yes possible add documentation soon use cortex configure command machine prompt information necessary connect cluster could also copy run cortex cluster cluster automatically configure cluster environment,issue,negative,neutral,neutral,neutral,neutral,neutral
643156770,"@deliahu @RobertLucian  Thank very much to both of you, that was exactly what i was asking for.
But after knowing this, a new doubt persists. Is it possible to deploy models in a cortex cluster that was launched with the cortex cli from a different machine ?. 
Imagine that I launch my cortex cluster with a cluster1.yaml and then deploy a model, all of it using my computer. If my computer suddenly dies, can I somehow access that same cluster from another cli on a different computer?",thank much exactly knowing new doubt possible deploy cortex cluster cortex different machine imagine launch cortex cluster deploy model computer computer suddenly somehow access cluster another different computer,issue,negative,positive,neutral,neutral,positive,positive
643060237,"I'll go ahead and close this issue, feel free to re-open it or create a new one if you have any more questions",go ahead close issue feel free create new one,issue,positive,positive,positive,positive,positive,positive
643001118,"@deliahu version numbers are still in `images/neuron-rtd/Dockerfile`. These will be taken out after the merge conflicts are addressed. The reason is that if there's something wrong with always pulling the latest, then potential bugs originating from it might exhibit very odd behavior that would be quite troublesome to debug.",version still taken merge reason something wrong always latest potential might exhibit odd behavior would quite troublesome,issue,negative,negative,neutral,neutral,negative,negative
642906605,"@javithe7 there's one assumption I have to make about both of your questions and that is: by same machine, I take you're referring to the place where the Cortex CLI is installed and not where the actual cluster is running. 

With that out of the way, let me answer your questions here:

1. Yes, that's correct, you can spin-up multiple clusters. There's no limit on how many you can have. For that to be possible, you have to come up with unique names for your environments - each environment can manage one cluster. This environment is created upon spinning up the cluster by adding `--env` option to your command. When spinning up a cluster, the default name of the environment is `aws`.

1. Assuming you have deployed 2 clusters in 2 environments called `cluster1` and `cluster2`, you can make it such that APIs are deployed to `cluster1` cluster by running `cortex deploy --env cluster1` and respectively, `cortex deploy --env cluster2` for `cluster2`.

My suggestion is to go through this [page that describes Cortex environments](https://docs.cortex.dev/miscellaneous/environments) and give it a read.",one assumption make machine take place cortex actual cluster running way let answer yes correct multiple limit many possible come unique environment manage one cluster environment upon spinning cluster option command spinning cluster default name environment assuming cluster cluster make cluster cluster running cortex deploy cluster respectively cortex deploy cluster cluster suggestion go page cortex give read,issue,positive,positive,positive,positive,positive,positive
642906572,"Hi @javithe7, thanks for reaching out!

Yes, it is possible to launch multiple cortex clusters from the same machine using a different `cluster.yaml`, and you can specify which one to use when deploying. Here is an [example](https://docs.cortex.dev/miscellaneous/environments#example-multiple-clusters) (or see [this example](https://docs.cortex.dev/miscellaneous/environments#example-multiple-clusters-if-you-omitted-the-env-on-cortex-cluster-up) if you already have multiple clusters running).",hi thanks reaching yes possible launch multiple cortex machine different specify one use example see example already multiple running,issue,positive,positive,neutral,neutral,positive,positive
642855499,"Closing because separate tickets have been made for API auth, and it is now possible to configure the cluster to be private.",separate made possible configure cluster private,issue,negative,neutral,neutral,neutral,neutral,neutral
642618092,"According to the design concept of k8s, it is not recommended to summarize the status when pull image or oom into the state, but the reason why the pod presents the pending state",according design concept summarize status pull image state reason pod pending state,issue,negative,neutral,neutral,neutral,neutral,neutral
642334456,"@vishalbollu reducing the timeout for the process of loading big TF models works (that take more than the said timeout), but it can bring an avalanche of logs to `cortex logs <api-name>` coming from TFS. 

Basically, for each new gRPC call, the list of requested models gets logged to stdout as a string representation of `ModelServerConfig` [object](https://www.tensorflow.org/tfx/serving/serving_config). 

I think the easiest thing is to just set the timeout to something big anyway (i.e. 600 seconds) and while the gRPC call is waiting for a response,  a message like _""model(s) are still loading ...""_ would get repeatedly printed every 5 seconds to stdout.",reducing process loading big work take said bring avalanche cortex coming basically new call list logged string representation object think easiest thing set something big anyway call waiting response message like model still loading would get repeatedly printed every,issue,positive,positive,neutral,neutral,positive,positive
641999537,Great! @zouyee let us know if you have any questions along the way!,great let u know along way,issue,positive,positive,positive,positive,positive,positive
641613860,That's interesting @hammacktony. I'll have a look at this. Thanks for letting us know about it!,interesting look thanks u know,issue,positive,positive,positive,positive,positive,positive
641610417,"Tested this the following ways assuming a local provider:

1. With the image present both locally and on the upstream (i.e. DockerHub/ECR) -> worked as expected.
1. With local image present and none available on the upstream (i.e. DockerHub/ECR) -> proceeded to use the local image.
1. With local image not present and none available on the upstream (i.e. DockerHub/ECR) -> failed as expected.
1. With local image not present and one available on the upstream (i.e. DockerHub/ECR) -> proceeed to download the upstream image.
1. With an API already running using an `xyz` image, build a different image locally and then hit `cortex deploy` -> proceeded to use the newly updated local image.",tested following way assuming local provider image present locally upstream worked local image present none available upstream use local image local image present none available upstream local image present one available upstream upstream image already running image build different image locally hit cortex deploy use newly local image,issue,negative,positive,neutral,neutral,positive,positive
641511974,[Ray](https://github.com/ray-project/ray) solves this by using the Plasma object store. Maybe something like this could be used.,ray plasma object store maybe something like could used,issue,negative,neutral,neutral,neutral,neutral,neutral
641328586,"@MrRace The reason all of the APIs are updated is because when Cortex deploys an API, it sends all files and directories (at and below the location of `cortex.yaml`) to the API container. If any of these files are changed, then it's possible that the API has changed, so it needs to redeploy the API.

One option is to create a separate directory for each API, each with a `cortex.yaml`, and deploy them separately. If your APIs share code and you would like to keep them in the same directory, you can also create a separate yaml file for each API (e.g. `api1.yaml`, `api2.yaml`, ...), and deploy them separately (e.g. `cortex deploy api1.yaml`, `cortex deploy api2.yaml`, ...).",reason cortex location container possible need redeploy one option create separate directory deploy separately share code would like keep directory also create separate file deploy separately cortex deploy cortex deploy,issue,positive,neutral,neutral,neutral,neutral,neutral
641008670,"> @MrRace with #1114 you will be able to use local Docker images with your locally-deployed Cortex API(s). This will most likely land in `0.18`.

Great! 
By the way, I find that when set multiple classifiers(e.t. by set multiple `-name`  in `cortex.yaml` ). Each time update one of them by `cortex deploy`, all the classifiers' service will update, but I just want to update the target classifier.  How can I do to just update the specific classifier? Maybe by some parametersÔºü",able use local docker cortex likely land great way find set multiple set multiple time update one cortex deploy service update want update target classifier update specific classifier maybe,issue,positive,positive,positive,positive,positive,positive
640693855,@MrRace with #1114 you will be able to use local Docker images with your locally-deployed Cortex API(s). This will most likely land in `0.18`.,able use local docker cortex likely land,issue,negative,positive,positive,positive,positive,positive
639236556,"@deliahu the models used in this PR can be [found here](https://www.dropbox.com/s/hsqv87ogwptnltr/multi-model-endpoint-models.zip?dl=0). They have to be uploaded to the public Cortex S3 bucket. Inside that zip, you will find the following models:

1. ResNet50 model in SavedModel and ONNX format.
1. MobileNet model in ONNX format.
1. ShuffleNet model in ONNX format.

Next, there was a bug with the ONNX predictor where if you used a model with a dynamically sized input signature, the `predict` method would then try to reshape the model input to `(1,1,1)` - where each 1 was the replacement value for an unknown shape (that would otherwise be permissible). This is now fixed. This is the commit https://github.com/cortexlabs/cortex/pull/1107/commits/ad6ec6f1f40168b6a150de046e9c243b40840b82 in question.

Following this, here's my answer to @vishalbollu 's question: assuming there are at least 2 threads per worker and a single worker, when a request for a time-consuming model is being processed (say 5-10 seconds), if a new request lands on the other thread and it's for a model that takes little time to run the inference on, the new request will be parallelized and hence, will finish first.",used found public cortex bucket inside zip find following model format model format model format next bug predictor used model dynamically sized input signature predict method would try reshape model input replacement value unknown shape would otherwise permissible fixed commit question following answer question assuming least per worker single worker request model say new request thread model little time run inference new request hence finish first,issue,positive,negative,neutral,neutral,negative,negative
638481030,"@zouyee thanks for looking into this!

Since we don't require the CLI user to have kubectl configured, it would be better to implement this in the Cortex operator.

A better approach would be to add additional status code(s) in the Cortex operator to capture additional error cases. All of the status logic is implemented in [operator/status.go](https://github.com/cortexlabs/cortex/blob/master/pkg/operator/operator/status.go) and [GetPodStatus()](https://github.com/cortexlabs/cortex/blob/master/pkg/lib/k8s/pod.go#L140) in k8s/pod.go. It should be possible to add new statuses [here](https://github.com/cortexlabs/cortex/blob/master/pkg/lib/k8s/pod.go#L43), add them to [SubReplicaCounts](https://github.com/cortexlabs/cortex/blob/master/pkg/types/status/status.go#L32), and then update [addPodToReplicaCounts()](https://github.com/cortexlabs/cortex/blob/master/pkg/operator/operator/status.go#L126) and [getStatusCode()](https://github.com/cortexlabs/cortex/blob/master/pkg/operator/operator/status.go#L163) accordingly.

Does that make sense? Let us know if you have any questions!",thanks looking since require user would better implement cortex operator better approach would add additional status code cortex operator capture additional error status logic possible add new add update accordingly make sense let u know,issue,positive,positive,positive,positive,positive,positive
638425952,"@kmrabhay I just wanted to follow up on this, were you able to get it working? Let me know if you'd like any help.",follow able get working let know like help,issue,positive,positive,positive,positive,positive,positive
638292837,"The behaviour you encountered with `cortex deploy` on a local machine is expected because update operations on a local machine take down the previous API and then spin up the latest API. Unfortunately, rolling update strategy is only supported in cluster deploys (on AWS) and is not supported in local mode.",behaviour cortex deploy local machine update local machine take previous spin latest unfortunately rolling update strategy cluster local mode,issue,negative,negative,neutral,neutral,negative,negative
638103496,"Thanks for your prompt reply.  Do you mean the rolling-update strategy will keep my service accessible when run `cortex deploy`? I try it in my local machine, it seems not work as my expectation. After running `cortex deploy`,   the service is unaccessible over a period of time.
If I want to update my service hot restart(like nginx‚Äòs `reload`‚Äô), how should I set my configuration?
Thanks a lot!",thanks prompt reply mean strategy keep service accessible run cortex deploy try local machine work expectation running cortex deploy service unaccessible period time want update service hot restart like reload set configuration thanks lot,issue,positive,positive,positive,positive,positive,positive
637609057,"@deliahu with #1103 in, it no longer fails. Since this is no longer an issue, I'm closing this PR.",longer since longer issue,issue,negative,neutral,neutral,neutral,neutral,neutral
637416322,I can confirm that it's fixed on the `master` branch for me. Looking forward to 0.17.1,confirm fixed master branch looking forward,issue,negative,positive,neutral,neutral,positive,positive
637291426,"@RobertLucian after merging https://github.com/cortexlabs/cortex/pull/1103, I was not able to reproduce the error you mentioned. Also I see this which seems to imply that it should work: https://www.gnu.org/software/make/manual/html_node/Choosing-the-Shell.html

Do you mind pulling the latest from master, and checking if it's still failing for you?",able reproduce error also see imply work mind latest master still failing,issue,negative,positive,positive,positive,positive,positive
637181708,"@MrRace with #1104, you will be able to get rid of `/bin/bash -c` before the pip command. Have a look at this example:
```Dockerfile
# test base image, can be any of the other 4
FROM cortexlabs/python-predictor-cpu-slim:0.17.0

COPY requirements.txt /etc/pip/requirements.txt
RUN pip install --no-cache-dir -r /etc/pip/requirements.txt
```",able get rid pip command look example test base image copy run pip install,issue,negative,negative,negative,negative,negative,negative
637036196,"Also, in case it's relevant to you, this issue only seems to happen locally (it doesn't get stuck on AWS)",also case relevant issue happen locally get stuck,issue,negative,positive,positive,positive,positive,positive
637035658,"@dsuess thanks for bringing this to our attention, and for contributing the fix!

I just spoke with the team, and we think it does make sense to make a patch release for this. There is one other small fix that we're working on which we may finish soon, so we'll hold off for a day to see if we can get that in too, and then we'll release 0.17.1.",thanks attention fix spoke team think make sense make patch release one small fix working may finish soon hold day see get release,issue,negative,negative,neutral,neutral,negative,negative
636940534,"This makes a lot of sense, thank you for addressing! The most relevant thing for me was to see the `endpoint` information show up in the `cortex get ...` command. üëç ",lot sense thank relevant thing see information show cortex get command,issue,negative,positive,positive,positive,positive,positive
636812398,"That's okay, if the docker images are rebuilt, I can work off master for testing. Given the previous release cycle, a new release should come out in the next 1--2 weeks?",docker rebuilt work master testing given previous release cycle new release come next,issue,negative,negative,neutral,neutral,negative,negative
636802722,@dsuess I'm able to reproduce this. Would you say this bug is a blocker for you? We can create a patch release specifically for this issue in case you need it ASAP.,able reproduce would say bug blocker create patch release specifically issue case need,issue,negative,positive,positive,positive,positive,positive
636509523,"@ssami thanks for creating a ticket for the documentation!

That makes sense. I think it might also be cool to have a higher level application consume that API, but I'm not sure exactly what that would look like (maybe a chrome extension that tracks what kinds of things you are reading?), and I wouldn't want you to do extra work that you weren't planning on doing anyways.

We'd be happy to consider adding your project to our [examples](https://github.com/cortexlabs/cortex/tree/master/examples) folder, and also consider creating a guide or a blog post as well. Keep us posted on our progress, and let us know if you have any questions/feedback with Cortex!",thanks ticket documentation sense think might also cool higher level application consume sure exactly would look like maybe chrome extension reading would want extra work anyways happy consider project folder also consider guide post well keep u posted progress let u know cortex,issue,positive,positive,positive,positive,positive,positive
636508132,"Thanks @ssami, I agree that your additions make it much more clear!

I incorporated them in this PR: https://github.com/cortexlabs/cortex/pull/1099. I didn't include as much of the exact CLI output as your proposal did, since we change that output frequently so it may be hard to keep up to date. Everything you suggested makes perfect sense, let me know if you think I've addressed it in the PR, or if I should make any additional changes.",thanks agree make much clear incorporated include much exact output proposal since change output frequently may hard keep date everything perfect sense let know think make additional,issue,positive,positive,positive,positive,positive,positive
636505194,"Thanks @deliahu, I've submitted an issue to enhance documentation [here](https://github.com/cortexlabs/cortex/issues/1098). For the sample application, I am planning to classify the trainng data that isn't labeled with any genre by calling the local deployed classifier API. ",thanks issue enhance documentation sample application data genre calling local classifier,issue,negative,positive,neutral,neutral,positive,positive
636412810,"> When we use the specified version of the interface, such as `k8s.io/api/core/v1`, then the version obtained `TypeMeta` must be v1, I think it may not be necessary to define it.

Yes I think you are correct, although for some reason at some point it was necessary for me, and since I don't think it hurts, I'm inclined to leave it as it was.

Using the labels package is definitely an improvement, so I'll go ahead and merge that now! Thanks again!",use version interface version must think may necessary define yes think correct although reason point necessary since think leave package definitely improvement go ahead merge thanks,issue,positive,positive,neutral,neutral,positive,positive
636411519,"@ssami Yes I think it could work well! Is there a sample application from which you plan to call the API?

Also, yes, please share any suggestions you have for improving our docs, thank you! Feel free to email me at david@cortex.dev if you'd like to chat live, or you can create a GitHub issue, whichever you prefer.",yes think could work well sample application plan call also yes please share improving thank feel free like chat live create issue whichever prefer,issue,positive,positive,positive,positive,positive,positive
636396450,"@deliahu I've been [working](https://github.com/ssami/genre-detector) with a fastText genre-classification model built on Kaggle's Google books reviews dataset, and wanted to use Cortex to deploy the model. I had some suggestions about improving the doc, as well. Would this sort of project be appropriate for this issue? ",working model built use cortex deploy model improving doc well would sort project appropriate issue,issue,positive,positive,positive,positive,positive,positive
636351462,"@RobertLucian can correct me if I'm wrong here, but I believe currently, the image must be pushed to a public repo in Dockerhub, or a private repo in ECR. We added this check because it is necessary to ensure that the image is accessible when running Cortex in AWS. However, I created https://github.com/cortexlabs/cortex/issues/1094 for us to look into whether we can allow local docker images (only when running Cortex locally).

In the mean time, the easiest solution is to push your image to Dockerhub, something like this (assuming you own the `ljp` repository in Dockerhub):

```text
docker push ljp/cortex-python-predictor-cpu
```",correct wrong believe currently image must public private added check necessary ensure image accessible running cortex however u look whether allow local docker running cortex locally mean time easiest solution push image something like assuming repository text docker push,issue,positive,negative,neutral,neutral,negative,negative
636348347,"@kmrabhay currently this is not incorporated into Cortex, but configuring a single port as an entrypoint for all APIs can be done outside of Cortex. There are a few ways this could be accomplished, I'd be happy to advise; do you mind sharing what cloud platform you're running on (if applicable) and your use case (including how many APIs you will have, whether you'll be continuously adding APIs or if it's a static set, etc)?",currently incorporated cortex single port done outside cortex way could accomplished happy advise mind cloud platform running applicable use case many whether continuously static set,issue,positive,positive,positive,positive,positive,positive
636347447,"@MrRace The cortex docker images are designed only to be used through the cortex CLI, so that cortex can help with some additional tasks. You can put your [Predictor implementation](https://docs.cortex.dev/deployments/predictors) in a directory, then add a `cortex.yaml` file which contains the [API configuration](https://docs.cortex.dev/deployments/api-configuration#python-predictor) for your API (including the path to the docker image that you built), and then run `cortex deploy`. Here is a [tutorial](https://docs.cortex.dev/iris-classifier).",cortex docker designed used cortex cortex help additional put predictor implementation directory add file configuration path docker image built run cortex deploy tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
636324742,"Hey, This would be great . @deliahu  May I know if this enhancement is incorporated? Can you guide me on nginx set up with cortex? ",hey would great may know enhancement incorporated guide set cortex,issue,positive,positive,positive,positive,positive,positive
636291355,"@RobertLucian  Thanks for your reply.  After building a new image, I set predictor->image value as the new builded image in the `cortex.yaml` . When running  `cortex deploy`  comes the error below:
```
error: /home/liujiepeng/IntentClassification/logistic_regression/cortex/cortex.yaml: gbm-classifier: predictor: image: ljp/cortex-python-predictor-cpu is not accessible
Error response from daemon: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
```
What should I do to fix it?",thanks reply building new image set image value new image running cortex deploy come error error predictor image accessible error response daemon access resource unauthorized authentication fix,issue,negative,positive,positive,positive,positive,positive
636277687,"> If I understand correctly, you are saying that the user would be responsible for calculating which percentage of requests to send to each model, and that Cortex would only provide the API to update these percentages, is that correct?

Yes, it's absolutely correct!",understand correctly saying user would responsible calculating percentage send model cortex would provide update correct yes absolutely correct,issue,positive,positive,positive,positive,positive,positive
636225498,"@puhoshville Thanks for the kind words, I'm glad you're liking Cortex!

Yes, I think this would be a useful feature. Our infrastructure is already mostly set up to support this, so it may not be too much effort; with Istio, we can route to multiple deployments from a single endpoint based on weights.

If I understand correctly, you are saying that the user would be responsible for calculating which percentage of requests to send to each model, and that Cortex would only provide the API to update these percentages, is that correct?

I'd be happy to jump on a call if you'd like to discuss, feel free to email me at david@cortex.dev if you are interested.",thanks kind glad liking cortex yes think would useful feature infrastructure already mostly set support may much effort route multiple single based understand correctly saying user would responsible calculating percentage send model cortex would provide update correct happy jump call like discus feel free interested,issue,positive,positive,positive,positive,positive,positive
636050695,"@deliahu oh, didn't mention that HTTP API doesn't support caching. 
Maybe using [AWS SAM](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/what-is-sam.html) syntax can simplify/unify API Gateway usage? It supports both [REST API](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-api.html) and [HTTP API](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-resource-httpapi.html). Not very familiar with cortex architecture but probably AWS SAM can be used.",oh mention support maybe sam syntax gateway usage rest familiar cortex architecture probably sam used,issue,negative,positive,positive,positive,positive,positive
636034892,"@puhoshville Yes that's a great idea, I'll add a separate ticket for that!

One thing that comes to mind is the distinction between REST and HTTP API Gateways. HTTP is the newer product, and from what I've seen it seems that AWS is recommending that new applications start with HTTP instead of REST. However, at least as of today, HTTP APIs [don't support caching](https://docs.aws.amazon.com/apigateway/latest/developerguide/http-api-vs-rest.html) (only REST APIs do). Since it's still possible for users to manually create a REST API gateway and configure caching, we are considering implementing HTTP API Gateways out-of-the-box to support typical use cases. Hopefully AWS brings HTTP APIs to feature parity with REST APIs soon, or if not, once we've implemented HTTP API Gateway out-of-the-box, we could add an option to use REST instead (and then expose the cache configurations in the cortex API configuration).",yes great idea add separate ticket one thing come mind distinction rest product seen new start instead rest however least today support rest since still possible manually create rest gateway configure considering support typical use hopefully feature parity rest soon gateway could add option use rest instead expose cache cortex configuration,issue,positive,positive,neutral,neutral,positive,positive
636012240,"Also, API Gateway supports response caching which can be extremely helpful in case of high load. It will be nice to see this feature out of the box! üëç",also gateway response extremely helpful case high load nice see feature box,issue,positive,positive,positive,positive,positive,positive
635923067,"The Cortex APIs are managed declaratively. This means that with each new modification brought to your API config (`cortex.yaml`), running `cortex deploy` will update your deployed API.

Please keep in mind that a rolling-update approach is employed with the update process of a deployed API, so if you don't have enough compute capacity around (either because you've hit the ceiling capacity of your cluster or because your local machine is close to being maxed out), then your API may never get updated - just keep that in mind.

The rolling-update strategy is used to ensure zero downtime of your service.

On the other hand, if you just want to get rid of the old API replica(s) and spawn a new one instead, just hit `cortex delete <api> && cortex deploy`. You'd probably want to go with this one when there isn't much compute capacity available (or when your API is using a substantial amount of resources relative to the cluster's capacity).",cortex declaratively new modification brought running cortex deploy update please keep mind approach employed update process enough compute capacity around either hit ceiling capacity cluster local machine close may never get keep mind strategy used ensure zero service hand want get rid old replica spawn new one instead hit cortex delete cortex deploy probably want go one much compute capacity available substantial amount relative cluster capacity,issue,positive,positive,positive,positive,positive,positive
635913877,"The `requirements.txt` file has been added to make it easy for users to install Python packages without dealing with the hassle of creating custom Dockerfiles for each new modification. 

With that in mind, here's how you can create a custom Dockerfile image with your preferred Python packages. Before anything else, check what's your current image in `predictor:image` field - you can do that with `cortex get <my-api>`. To choose from one of the base images, check this out [here](https://docs.cortex.dev/deployments/system-packages#custom-docker-image).
```Dockerfile
# test base image, can be any of the other 4
FROM cortexlabs/python-predictor-cpu-slim:0.17.0

COPY requirements.txt /etc/pip/requirements.txt

# /bin/bash is required
RUN /bin/bash -c ""pip install --no-cache-dir -r /etc/pip/requirements.txt""
```

With this, you only have to install your Python packages once. The resulting image can then be utilized like any of our images. Will this make it easier for you?",file added make easy install python without dealing hassle custom new modification mind create custom image preferred python anything else check current image predictor image field cortex get choose one base check test base image copy run pip install install python resulting image like make easier,issue,positive,negative,negative,negative,negative,negative
635617071,Thanks for your suggestion @deliahu. I am currently using Option 1 to deploy and have got a DevOps engineer on board who would take care of the security issues for now. ,thanks suggestion currently option deploy got engineer board would take care security,issue,positive,positive,neutral,neutral,positive,positive
635028321,"> Regarding setting `TypeMeta` for the k8s objects, I remember having to add this for some reason. This was a while ago and now I forget (I should have added a comment!), but for it was necessary at least at the time that I added it, so I am hesitant to remove it. Since the client version matches the `TypeMeta` that we are setting, is it safe to leave it as is? If that's the case that would be my preference for now.

When we use the specified version of the interface, such as `k8s.io/api/core/v1`, then the version obtained `TypeMeta` must be v1, I think it may not be necessary to define it.
",regarding setting remember add reason ago forget added comment necessary least time added hesitant remove since client version setting safe leave case would preference use version interface version must think may necessary define,issue,negative,positive,neutral,neutral,positive,positive
633157582,"Closing since with the current architecture, there is no request time limit",since current architecture request time limit,issue,negative,neutral,neutral,neutral,neutral,neutral
633001869,"Thanks!!!!!
And thank you for this great tool!!!",thanks thank great tool,issue,positive,positive,positive,positive,positive,positive
631572532,"@ug911 Thanks for reaching out. Administrator access is required to create a cluster. If I understand correctly, does your EC2 instance have an admin IAM role? If so, it may be possible to support your use case (although we do not today, I can look into what this might involve).

It will still be necessary to have an AWS Key and Secret Key to give to the Cortex operator, however this can have a much narrower set of permissions. Please see https://www.cortex.dev/miscellaneous/security#iam-permissions

As we explore if this will be possible, in the short term there are two options:

1. The DevOps team can spin up the cluster, and then you can connect your Cortex CLI to the cluster by having an AWS Key/Secret Key for any IAM user in the AWS account (no special permissions are required). You would connect your CLI by running `cortex env configure aws` once the cluster is running.

2. The DevOps team can give you two sets of keys, one with admin access and one with the permissions listed [here](https://www.cortex.dev/miscellaneous/security#operator). You can set the appropriate cluster configuration / environment variables when spinning up your cluster (described [here](https://www.cortex.dev/miscellaneous/security#cluster-spin-up)). Then once the cluster is running, you can revoke the admin credentials (you may need new admin credentials when you want to spin the cluster down).

Would one of these options work for you?",ug thanks reaching administrator access create cluster understand correctly instance role may possible support use case although today look might involve still necessary key secret key give cortex operator however much narrower set please see explore possible short term two team spin cluster connect cortex cluster key user account special would connect running cortex configure cluster running team give two one access one listed set appropriate cluster configuration environment spinning cluster cluster running revoke may need new want spin cluster would one work,issue,positive,positive,neutral,neutral,positive,positive
630429711,"@tthebst In that case, yes I think we should build the url ourselves, is this possible? We could do it in the CLI, or in the operator and send it through in `GetAPIsResponse` (similar to how we send `BaseURL`), whichever you think makes more sense.",case yes think build possible could operator send similar send whichever think sense,issue,negative,neutral,neutral,neutral,neutral,neutral
630405182,"@deliahu I looked into the thing with the dashboard link. The only thing the SDK provides is the ARN. So if you want to add a link we would have to build the URL by ourselves. 

",thing dashboard link thing arn want add link would build,issue,negative,neutral,neutral,neutral,neutral,neutral
629763504,"Thank you for the feedback. It really helps me to learn go and write more readable code. 

I'm happy to discuss if you want to change the layout or something like the bucket period. 
",thank feedback really learn go write readable code happy discus want change layout something like bucket period,issue,positive,positive,positive,positive,positive,positive
629566412,"I will check it out later tonight or this weekend, thanks again for your work on this!

No, there is not a style guide that we follow, although maybe there should be! We just have some conventions that have formed organically. Sorry for the ""nit"" comments regarding whitespace, I was mostly interested in keeping it consistent with the rest of the code that was in that file, otherwise I am not too opinionated on these matters (which is one of the reasons I love `go`, since it comes with `gofmt`!)",check later tonight weekend thanks work style guide follow although maybe formed organically sorry nit regarding mostly interested keeping consistent rest code file otherwise opinionated one love go since come,issue,positive,positive,positive,positive,positive,positive
629388012,"Hi David,

I included the changes you can check it out I think. 

Also is there a special style guide for this project or go in general that you follow?

Tim

",hi included check think also special style guide project go general follow,issue,negative,positive,positive,positive,positive,positive
629276235,Grabbing this one. Part of the trick in making this work will be in reloading and unloading the model configs for the Tensorflow Predictor on-the-fly and reliably. I reckon things will be simpler for the ONNX and Python Predictors. This one goes hand-in-hand with #890.,one part trick making work model predictor reliably reckon simpler python one go,issue,negative,neutral,neutral,neutral,neutral,neutral
629274122,"@deliahu taking this one alongside https://github.com/cortexlabs/cortex/issues/619. Finally, guides for each predictor type will be required as described in https://github.com/cortexlabs/cortex/issues/986. ",taking one alongside finally predictor type,issue,negative,neutral,neutral,neutral,neutral,neutral
628751206,"@vlinden-cs I believe you intended to create an issue over at [cortexproject/cortex](https://github.com/cortexproject/cortex), so I'll close this issue (let me know if I am mistaken!)",believe intended create issue close issue let know mistaken,issue,negative,neutral,neutral,neutral,neutral,neutral
626356114,@deliahu just tested this with the latest and it works. It's a go from me.,tested latest work go,issue,negative,positive,positive,positive,positive,positive
626268159,"@GiedriusS No worries, I hope you caught up on sleep last night :)",hope caught sleep last night,issue,negative,neutral,neutral,neutral,neutral,neutral
626264019,"This is great @RobertLucian, thanks for adding this!

It looks good to me, and I've added @vishalbollu as a reviewer since he has been more involved with this part of the code, and he can go ahead and merge if it looks good to him",great thanks good added reviewer since involved part code go ahead merge good,issue,positive,positive,positive,positive,positive,positive
626241166,"> @GiedriusS thank you for creating this ticket, and for describing your use case and motivation so clearly. This is something that we will definitely look into.
> 
> Right now, logs are sent to Cloudwatch as JSON with just a single ""log"" key (which contains all of the text from the log). Does the current architecture work for you, except that you would like the json to include more fields (i.e. log level, timestamp, ...), and the ""log"" field would contain only the actual log message? (we could also rename the ""log"" field to e.g. ""text"" if that makes more sense)
> 
> Here is a log in Cloudwatch today:
> 
> ```json
> {
>     ""log"": ""2020-04-03 00:19:37.667433:cortex:pid-1:INFO:pulling the python serving image""
> }
> ```
> 
> Would you want this?
> 
> ```json
> {
>     ""timestamp"": ""2020-04-03 00:19:37.667433"",
>     ""pid"": 1,
>     ""level"": ""INFO"",
>     ""log"": ""pulling the python serving image""
> }
> ```
> 
> In cases where users call `print()` from their implementations, would we just leave all of the metadata fields blank and only have the single ""log"" key (like it is today)?
> 
> Also, would you want access to the Cortex logger so that you could log in the same JSON format? Or is there a standard format you can point us to that if we follow it, it's easy to replicate it in your predictor code?
> 
> Finally, are you able to easily ingest the json logs from CloudWatch to Elasticsearch?

Oh lord, I just realized that I have opened this issue in a wrong repository. :smile: :smile: I wanted to open it up here: https://github.com/cortexproject/cortex. Please ignore this message. :smile: I clearly didn't have enough sleep yesterday.",thank ticket use case motivation clearly something definitely look right sent single log key text log current architecture work except would like include log level log field would contain actual log message could also rename log field text sense log today log cortex python serving image would want level log python serving image call print would leave blank single log key like today also would want access cortex logger could log format standard format point u follow easy replicate predictor code finally able easily ingest oh lord issue wrong repository smile smile open please ignore message smile clearly enough sleep yesterday,issue,positive,positive,positive,positive,positive,positive
626068292,"@GiedriusS thank you for creating this ticket, and for describing your use case and motivation so clearly. This is something that we will definitely look into.

Right now, logs are sent to Cloudwatch as JSON with just a single ""log"" key (which contains all of the text from the log). Does the current architecture work for you, except that you would like the json to include more fields (i.e. log level, timestamp, ...), and the ""log"" field would contain only the actual log message? (we could also rename the ""log"" field to e.g. ""text"" if that makes more sense)

Here is a log in Cloudwatch today:

```json
{
    ""log"": ""2020-04-03 00:19:37.667433:cortex:pid-1:INFO:pulling the python serving image""
}
```

Would you want this?

```json
{
    ""timestamp"": ""2020-04-03 00:19:37.667433"",
    ""pid"": 1,
    ""level"": ""INFO"",
    ""log"": ""pulling the python serving image""
}
```

In cases where users call `print()` from their implementations, would we just leave all of the metadata fields blank and only have the single ""log"" key (like it is today)?

Also, would you want access to the Cortex logger so that you could log in the same JSON format? Or is there a standard format you can point us to that if we follow it, it's easy to replicate it in your predictor code?

Finally, are you able to easily ingest the json logs from CloudWatch to Elasticsearch?",thank ticket use case motivation clearly something definitely look right sent single log key text log current architecture work except would like include log level log field would contain actual log message could also rename log field text sense log today log cortex python serving image would want level log python serving image call print would leave blank single log key like today also would want access cortex logger could log format standard format point u follow easy replicate predictor code finally able easily ingest,issue,positive,positive,positive,positive,positive,positive
624840275,"@lzmisscc it is possible to deploy Cortex on a since instance locally or on any cloud provider, here is our documentation for [single-node deployments](https://www.cortex.dev/guides/single-node-deployment).

If you are asking about deploying to an on-premise cluster, currently that is not supported. We support deploying to a cluster on AWS, and we are currently exploring adding GCP support.",possible deploy cortex since instance locally cloud provider documentation cluster currently support cluster currently exploring support,issue,positive,neutral,neutral,neutral,neutral,neutral
624370931,"@iPalash I am working on adding this now; does this file work on your system? You should be able to copy-paste this into a file named `_cortex` in your `fpath`, or paste it into any file (e.g. `cortex-completions.sh`) and run `source cortex-completions.sh`.

```text
#compdef _cortex cortex


function _cortex {
  local -a commands

  _arguments -C \
    ""1: :->cmnds"" \
    ""*::arg:->args""

  case $state in
  cmnds)
    commands=(
      ""deploy:create or update apis""
      ""refresh:restart all replicas for an api (witout downtime)""
      ""get:get information about apis""
      ""logs:stream logs from an api""
      ""predict:make a prediction request using a json file""
      ""delete:delete an api""
      ""cluster:manage a cluster""
      ""version:print the cli and cluster versions""
      ""env:manage environments""
      ""completion:generate shell completion scripts""
      ""help:Help about any command""
    )
    _describe ""command"" commands
    ;;
  esac

  case ""$words[1]"" in
  deploy)
    _cortex_deploy
    ;;
  refresh)
    _cortex_refresh
    ;;
  get)
    _cortex_get
    ;;
  logs)
    _cortex_logs
    ;;
  predict)
    _cortex_predict
    ;;
  delete)
    _cortex_delete
    ;;
  cluster)
    _cortex_cluster
    ;;
  version)
    _cortex_version
    ;;
  env)
    _cortex_env
    ;;
  completion)
    _cortex_completion
    ;;
  help)
    _cortex_help
    ;;
  esac
}

function _cortex_deploy {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:' \
    '(-f --force)'{-f,--force}'[override the in-progress api update]' \
    '(-y --yes)'{-y,--yes}'[skip prompts]'
}

function _cortex_refresh {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:' \
    '(-f --force)'{-f,--force}'[override the in-progress api update]'
}

function _cortex_get {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:' \
    '(-w --watch)'{-w,--watch}'[re-run the command every second]'
}

function _cortex_logs {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:'
}

function _cortex_predict {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:'
}

function _cortex_delete {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:' \
    '(-f --force)'{-f,--force}'[delete the api without confirmation]' \
    '(-c --keep-cache)'{-c,--keep-cache}'[keep cached data for the api]'
}


function _cortex_cluster {
  local -a commands

  _arguments -C \
    ""1: :->cmnds"" \
    ""*::arg:->args""

  case $state in
  cmnds)
    commands=(
      ""up:spin up a cluster""
      ""info:get information about a cluster""
      ""update:update a cluster""
      ""down:spin down a cluster""
    )
    _describe ""command"" commands
    ;;
  esac

  case ""$words[1]"" in
  up)
    _cortex_cluster_up
    ;;
  info)
    _cortex_cluster_info
    ;;
  update)
    _cortex_cluster_update
    ;;
  down)
    _cortex_cluster_down
    ;;
  esac
}

function _cortex_cluster_up {
  _arguments \
    '(-c --config)'{-c,--config}'[path to a cluster configuration file]:filename:_files -g ""yaml"" -g ""yml""' \
    '(-e --env)'{-e,--env}'[environment to configure]:' \
    '(-y --yes)'{-y,--yes}'[skip prompts]'
}

function _cortex_cluster_info {
  _arguments \
    '(-c --config)'{-c,--config}'[path to a cluster configuration file]:filename:_files -g ""yaml"" -g ""yml""' \
    '(-e --env)'{-e,--env}'[environment to configure]:' \
    '(-d --debug)'{-d,--debug}'[save the current cluster state to a file]' \
    '(-y --yes)'{-y,--yes}'[skip prompts]'
}

function _cortex_cluster_update {
  _arguments \
    '(-c --config)'{-c,--config}'[path to a cluster configuration file]:filename:_files -g ""yaml"" -g ""yml""' \
    '(-e --env)'{-e,--env}'[environment to configure]:' \
    '(-y --yes)'{-y,--yes}'[skip prompts]'
}

function _cortex_cluster_down {
  _arguments \
    '(-c --config)'{-c,--config}'[path to a cluster configuration file]:filename:_files -g ""yaml"" -g ""yml""' \
    '(-y --yes)'{-y,--yes}'[skip prompts]'
}

function _cortex_version {
  _arguments \
    '(-e --env)'{-e,--env}'[environment to use]:'
}


function _cortex_env {
  local -a commands

  _arguments -C \
    ""1: :->cmnds"" \
    ""*::arg:->args""

  case $state in
  cmnds)
    commands=(
      ""configure:configure an environment""
      ""list:list all configured environments""
      ""default:set the default environment""
      ""delete:delete an environment configuration""
    )
    _describe ""command"" commands
    ;;
  esac

  case ""$words[1]"" in
  configure)
    _cortex_env_configure
    ;;
  list)
    _cortex_env_list
    ;;
  default)
    _cortex_env_default
    ;;
  delete)
    _cortex_env_delete
    ;;
  esac
}

function _cortex_env_configure {
  _arguments \
    '(-p --provider)'{-p,--provider}'[set the provider without prompting]:' \
    '(-o --operator-endpoint)'{-o,--operator-endpoint}'[set the operator endpoint without prompting]:' \
    '(-k --aws-access-key-id)'{-k,--aws-access-key-id}'[set the aws access key id without prompting]:' \
    '(-s --aws-secret-access-key)'{-s,--aws-secret-access-key}'[set the aws secret access key without prompting]:' \
    '(-r --aws-region)'{-r,--aws-region}'[set the aws region without prompting]:'
}

function _cortex_env_list {
  _arguments
}

function _cortex_env_default {
  _arguments
}

function _cortex_env_delete {
  _arguments
}

function _cortex_completion {
  _arguments \
    '(-h --help)'{-h,--help}'[help for completion]'
}

function _cortex_help {
  _arguments
}

alias cx='cortex'

if compquote '' 2>/dev/null; then _cortex; else compdef _cortex cortex; fi
```",working file work system able file paste file run source text cortex function local case state deploy create update refresh restart get get information stream predict make prediction request file delete delete cluster manage cluster version print cluster manage completion generate shell completion help help command command case deploy refresh get predict delete cluster version completion help function environment use force force override update yes yes skip function environment use force force override update function environment use watch watch command every second function environment use function environment use function environment use force force delete without confirmation keep data function local case state spin cluster get information cluster update update cluster spin cluster command case update function path cluster configuration file environment configure yes yes skip function path cluster configuration file environment configure save current cluster state file yes yes skip function path cluster configuration file environment configure yes yes skip function path cluster configuration file yes yes skip function environment use function local case state configure configure environment list list default set default environment delete delete environment configuration command case configure list default delete function provider provider set provider without set operator without set access key id without set secret access key without set region without function function function function help help help completion function alias else cortex fi,issue,positive,positive,neutral,neutral,positive,positive
624275215,Closing since request-based scaling is supported and should work well for GPU-based workloads,since scaling work well,issue,negative,neutral,neutral,neutral,neutral,neutral
624240530,@tharun208 I was just wondering if you are currently working on this? I would just like to know in case other first-time contributors might be interested in implementing it.,wondering currently working would like know case might interested,issue,positive,positive,positive,positive,positive,positive
624175127,"We don't have immediate plans to support Azure. That being said, we plan to support GCP within the next few releases so that should help inform our timeline for Azure support.",immediate support azure said plan support within next help inform azure support,issue,positive,neutral,neutral,neutral,neutral,neutral
624163024,"@iPalash thanks for creating this issue! Currently we only support bash completion, but we will look into supporting zsh for our next release",thanks issue currently support bash completion look supporting next release,issue,positive,positive,positive,positive,positive,positive
624142168,"@iborko With our [TensorFlow predictor type](https://www.cortex.dev/deployments/predictors#tensorflow-predictor), there are two containers running: one (we can call it ""API server"") receives prediction requests and does pre/post processing (this is where your `predict()` function runs), and we run TensorFlow Serving in a separate container to actually run the inference (the API server makes the TF Serving request when the user calls `self.client.predict(payload)`). The TF Serving container is the one that has access to the GPU since it runs the inference, and the API server does not have GPU.

The API server can be configured to run with multiple threads and/or workers, allowing incoming requests to be processed concurrently (here is the [documentation](https://www.cortex.dev/deployments/autoscaling#replica-parallelism) on that, which has changed since I explained how to configure it above). Therefore TF Serving requests can be sent concurrently. This is helpful if there are any preprocessing or postprocessing steps (which happen in the API server), especially if they involve network requests. So this is what I meant by ""multiple in-flight requests can be sent through to the GPU concurrently"".

Whether multiple requests can be processed by TF Serving concurrently is a different matter. My understanding is that unless [server-side batching](https://www.tensorflow.org/tfx/serving/serving_advanced#batching) is enabled, requests will be processed sequentially. We have https://github.com/cortexlabs/cortex/issues/152 to add support for this.

This means that leveraging concurrency in the API server can have major benefits when there is significant pre/post processing (especially involving network requests), and smaller benefits if the prediction request goes straight through to TF Serving without pre/post processing.

With our [Python predictor type](https://www.cortex.dev/deployments/predictors#python-predictor), the discussion above pretty much applies the same. In this setup, there is only one container: the API server which receives the request and calls your `predict()` function (which runs the actual inference). In this case, concurrency can also be used to allow multiple `predict()` calls to be running concurrently. Like with the TensorFlow predictor, the benefit will depend on how much pre/post processing is done outside of the actual inference and whether this step is I/O bound.

Does that make sense? Let me know if you still have questions!",predictor type two running one call server prediction predict function run serving separate container actually run inference server serving request user serving container one access since inference server server run multiple incoming concurrently documentation since configure therefore serving sent concurrently helpful happen server especially involve network meant multiple sent concurrently whether multiple serving concurrently different matter understanding unless sequentially add support concurrency server major significant especially network smaller prediction request go straight serving without python predictor type discussion pretty much setup one container server request predict function actual inference case concurrency also used allow multiple predict running concurrently like predictor benefit depend much done outside actual inference whether step bound make sense let know still,issue,positive,positive,neutral,neutral,positive,positive
623940858,"> the Python container will not be the limiting factor, since multiple in-flight requests can be sent through to the GPU concurrently.

I don't see how multiple in-flight requests can be sent through the GPU concurrently using Python. Can you provide an example of that? In theory perhaps, but it certainly can't be done using Tensorflow or PyTorch.",python container limiting factor since multiple sent concurrently see multiple sent concurrently python provide example theory perhaps certainly ca done,issue,negative,positive,neutral,neutral,positive,positive
623773005,"@nicmer your understanding is correct: currently this is possible, but we have not added a guide or built-in solution for it. We (and some of our users) have successfully deployed multi-model endpoints without any caching, i.e. by simply loading multiple models in `__init__()` and then selecting the model based on the request body. However we have not explored what using a cache would look like.

I envisioned that we would try it out and add a guide with some sample code as a first step. Then we would have a better sense of how to build it into the product. I'm thinking maybe it would be a library we ship with that can be imported and used in the Predictor implementation. For example, in `__init__()` you could do something like:

```python
self.model_cache = cortex.cache.init(""path/to/s3/prefix"", max_models=100, cache_to_disk=true, preload=true, ttl=timedelta(hours=24))
```

And then in `predict()` you could do
```python
model = self.model_cache.get(model_path)
```

Another option could be to fold this into the API configuration. I think we'll have a better sense of the best way to build it in once we have a working example.

One of the questions that we have not yet researched is how easy is it to unload models from memory? Does it depend on the model framework? Is there anything special we'd have to do on GPU, and is it generalizable or would we have to rely on the user to provide an `unload()` implementation?

Let us know if you make any progress on this, we'd love to hear if you have any ideas/tips, as well as take a look at anything that you think would be useful!",understanding correct currently possible added guide solution successfully without simply loading multiple model based request body however cache would look like would try add guide sample code first step would better sense build product thinking maybe would library ship used predictor implementation example could something like python predict could python model another option could fold configuration think better sense best way build working example one yet easy unload memory depend model framework anything special generalizable would rely user provide unload implementation let u know make progress love hear well take look anything think would useful,issue,positive,positive,positive,positive,positive,positive
623369868,"I am just wondering whether that is already possible and ""only"" requires documentation or whether the issue suggests the enhancement. That would be a really great feature. I was thinking whether it is possible to implement a multi model endpoint using lru.cache or something in that direction, but would be certainly more happy if there is already some solution.
 ",wondering whether already possible documentation whether issue enhancement would really great feature thinking whether possible implement model something direction would certainly happy already solution,issue,positive,positive,positive,positive,positive,positive
623263613,"Sounds good; in that case I'm closing this issue, and I created https://github.com/cortexlabs/cortex/issues/1016.

Thanks for bringing this up!",good case issue thanks,issue,positive,positive,positive,positive,positive,positive
623070364,"Yes, I think Triton Inference Server covers more use cases and it is useful to optimize both TF and ONNX models.

It also has the makes sharing models between GPUs of the same machine more efficient.",yes think triton inference server use useful optimize also machine efficient,issue,positive,positive,positive,positive,positive,positive
623037904,"@miguelvr yes that could be a good option. Currently my understanding is that it's possible to serve TensorRT models in TF Serving using our existing stack, but we have not attempted this before. Assuming that is the case, do you think it makes sense to change the title of this ticket to something along the lines of ""Support Triton Inference Server""? Since perhaps TensorRT is already supported, and adding support for Triton can be a separate matter (to optimize performance)?",yes could good option currently understanding possible serve serving stack assuming case think sense change title ticket something along support triton inference server since perhaps already support triton separate matter optimize performance,issue,positive,positive,positive,positive,positive,positive
622937723,"The Triton Inference Server (fka TensorRT Inference Server) might be a good option as an alternative to Tensorflow Serving, and a more efficient on to serve traced pytorch models with libtorch (no python GIL)

https://github.com/NVIDIA/triton-inference-server",triton inference server inference server might good option alternative serving efficient serve python,issue,positive,positive,positive,positive,positive,positive
622521424,"@tthebst we've decided to not close this ticket, but rather update the title/description and de-prioritize it for now",decided close ticket rather update,issue,negative,neutral,neutral,neutral,neutral,neutral
622190518,@tthebst yes that is correct. I just created a pull request for this: https://github.com/cortexlabs/cortex/pull/992; we'll merge it soon and it will be in our next release!,yes correct pull request merge soon next release,issue,negative,neutral,neutral,neutral,neutral,neutral
621948724,"@tthebst Yes you are correct; to chain models across providers, users will have to implement it on the client.

Since we have not yet heard a feature request for this, and since there is a reasonable work-around which makes it possible (implementing it on the client), I think for now supporting server-side model chaining across predictor types is not a high priority, do you think that makes sense?",yes correct chain across implement client since yet feature request since reasonable possible client think supporting model across predictor high priority think sense,issue,positive,positive,positive,positive,positive,positive
621926118,"Yes, that's true, but what if someone wants to mix TensorFlow with python predictor. For example, if one could use pretrained tf models, which outputs then get used in a for example python model. I don't know how common this workflow is but as of now one would have to stick to one of the deployment options to do model chaining. Or implement the chaining on the client-side.",yes true someone mix python predictor example one could use get used example python model know common one would stick one deployment model implement,issue,positive,positive,neutral,neutral,positive,positive
621917095,"Okay I understand. So you would like just a slim image with just the dependencies needed for cortex, but still based on ubuntu and conda?",understand would like slim image cortex still based,issue,negative,neutral,neutral,neutral,neutral,neutral
621793862,"This is not a blocker yet, but it adds extra time when I send json instead of bytes.
Thanks for clarification.",blocker yet extra time send instead thanks clarification,issue,negative,positive,neutral,neutral,positive,positive
621790883,"@andriy-tsvikla at the moment, that's not possible. You still have to serialize your payload in order to be able to send it for inference. Will this be a blocker for you or are you okay with having it to serialize/deserialize the payload for the moment?

The situation won't stay this way too long. There's already a ticket on this matter. Check it out: https://github.com/cortexlabs/cortex/issues/917.

The silver lining here is that your API can respond with a bytes object instead. This means you won't have to deserialize on your client once you get the response. Check this out: https://github.com/cortexlabs/cortex/pull/915.",moment possible still serialize order able send inference blocker moment situation wo stay way long already ticket matter check silver lining respond object instead wo client get response check,issue,negative,positive,positive,positive,positive,positive
621537390,"@tthebst thanks for reaching out on this! @vishalbollu and I just discussed, and we think that for now, it's easy enough to use multiple models in a single endpoint for the Python predictor; you can load both models in `__init__()`, and then call them sequentially in `predict()`.

We do not yet support this for the TensorFlow Predictor type (you'd have to use the Python Predictor with TensorFlow models). I just updated the description on https://github.com/cortexlabs/cortex/issues/890 to be more clear, which now covers this topic.

Therefore, since we think it's supported well enough for the Python Predictor, and we now have an issue to track TensorFlow/ONNX, I'm thinking of closing this issue, what do you think? Let me know if we're missing a big opportunity here to improve it though!",thanks reaching think easy enough use multiple single python predictor load call sequentially predict yet support predictor type use python predictor description clear topic therefore since think well enough python predictor issue track thinking issue think let know missing big opportunity improve though,issue,positive,positive,neutral,neutral,positive,positive
621525496,"Hi @tthebst, the intention of this is provide a base image that has all of the Cortex code in it, but without any of the ""user"" dependencies installed. Right now our containers ship with PyTorch, TensorFlow, Scikit Learn, XGboost installed, whereas a user probably only needs one of these. So if they are interested in building their own image from our ""base"" image and using that, or if they are ok `pip install`ing their dependencies every time an API replica is created, they would want to use the ""slim"" base image. It would still be based on Ubuntu and still have conda, just wouldn't have the extra pip packages.

We are planning to add these images soon!",hi intention provide base image cortex code without user right ship learn whereas user probably need one interested building image base image pip install ing every time replica would want use slim base image would still based still would extra pip add soon,issue,negative,negative,negative,negative,negative,negative
621394954,"I gave this some thought because it sounds useful. 
One possible option would be to create another predictor for example Chaining preditor:

 ```

labels = [""setosa"", ""versicolor"", ""virginica""]

class ChainPredictor:
    def __init__(self, chain_client, config):
        self.client = chain_client
        self.models =[""GPT,""Tacotron""]

    def predict(self, payload):
        model_input = [
            payload[""sepal_length""],
            payload[""sepal_width""],
            payload[""petal_length""],
            payload[""petal_width""],
        ]

        prediction = self.chain.predict(model_input)
        predicted_class_id = prediction[0][0]
        return labels[predicted_class_id]
 ```

The `self.models` would then contain the names of already deployed models and one would then implement the calls to all models in the `chain_client.predict `.

One possible addition would also be to add transformation function to apply between models.

Tim",gave thought useful one possible option would create another predictor example versicolor class self predict self prediction prediction return would contain already one would implement one possible addition would also add transformation function apply,issue,positive,positive,neutral,neutral,positive,positive
621338958,"Hi,

What's the reason for using conda inside ubuntu and not a python docker image?

Do I understand correctly that you want a seperate image? Or just a slimmer version of the existing?",hi reason inside python docker image understand correctly want image version,issue,negative,neutral,neutral,neutral,neutral,neutral
620579770,@deliahu Sorry I didn't notice that there was a 50:1 ratio restriction,sorry notice ratio restriction,issue,negative,negative,negative,negative,negative,negative
620294666,"Update: I was able to reproduce the issue from the EBS console, and there was a better error message this time :)

![image](https://user-images.githubusercontent.com/808475/80431729-11563600-88a7-11ea-9e96-69559b369db6.png)

I'll add a validation",update able reproduce issue console better error message time image add validation,issue,negative,positive,positive,positive,positive,positive
620293111,"@tthebst Thanks for addressing my comments!

I merged master in, and made a few small updates to the branch.

I tried creating a cluster with `io1` volume types, but it wasn't able to start up. Have you been able to create a cluster with `io1` volumes?

I found this error message in my autoscaling group activity history:

`Client.InvalidParameterCombination: Could not create volume with size 50GiB and iops 4000 from snapshot 'snap-05048ef88fd5fb0db'`

Here's a screenshot:

<img width=""1679"" alt=""Screen Shot 2020-04-27 at 4 41 42 PM"" src=""https://user-images.githubusercontent.com/808475/80431448-55950680-88a6-11ea-875f-8f9dfc629d18.png"">
",thanks master made small branch tried cluster io volume able start able create cluster io found error message group activity history could create volume size gib snapshot screen shot,issue,positive,positive,positive,positive,positive,positive
619475336,"Currently only AWS is supported. We are adding support for local deployments in the next release (hopefully this week); it will be possible to spin up a single VM in Alibaba, SSH into it, deploy models in the VM, and expose the port to the internet. However, all of the ""cluster"" features would not be supported when using this approach (autoscaling, managing spot instances, etc).",currently support local next release hopefully week possible spin single deploy expose port however cluster would approach spot,issue,positive,negative,neutral,neutral,negative,negative
619417766,Excited to see what comes out @deliahu! Good luck!,excited see come good luck,issue,positive,positive,positive,positive,positive,positive
619417674,"Just to post an update: we've been heads down for the last couple weeks on this, and we're hoping to release local support this week.",post update last couple release local support week,issue,negative,neutral,neutral,neutral,neutral,neutral
619417283,"@codeghees Yes, the local feature will allow you to code and deploy APIs on your local machine, without AWS.

@mrciolino Thank you for the kind words, we really appreciate hearing feedback from our users! Yes, that is one of the main reasons we decided to implement local deployment. We also thought it would be a nice option for evaluating Cortex before committing to running on AWS. There is a third use case, which will probably be less common, but if you are using it in a dev or test environment and you don't need Cortex's ""cluster"" features (e.g. autoscaling, spot instance management, etc), then you could spin up a single VM, SSH in, run Cortex locally, and expose the port to the internet.",yes local feature allow code deploy local machine without thank kind really appreciate hearing feedback yes one main decided implement local deployment also thought would nice option cortex running third use case probably le common dev test environment need cortex cluster spot instance management could spin single run cortex locally expose port,issue,positive,positive,positive,positive,positive,positive
619415994,"@tthebst Awesome, I'm looking forward to checking it out!

Yes, let's stick with gp2 for the manager node.

Go tests are in the same directory as the source files, and in our case we've been keeping them in the same package too. We don't have full test coverage (especially on the relevant code for this PR), but here is an example of one of our tests if you'd like to add one: https://github.com/cortexlabs/cortex/blob/master/pkg/lib/zip/zip_test.go",awesome looking forward yes let stick manager node go directory source case keeping package full test coverage especially relevant code example one like add one,issue,positive,positive,positive,positive,positive,positive
619398271,I would say my main reason to want to test the deployment is to just verify everything is working before connecting the service up to aws. It aligns with my current workflow of local development and then deployment. Thanks to the devs for writing this amazing software!,would say main reason want test deployment verify everything working service current local development deployment thanks writing amazing,issue,positive,positive,positive,positive,positive,positive
619391300,"@deliahu with this new update, I will be able to run cortex without AWS, right? I would love to contribute to your code but can't afford the cost of AWS servers as I am just a graduating student.",new update able run cortex without right would love contribute code ca afford cost graduating student,issue,negative,positive,positive,positive,positive,positive
619385572,"@deliahu I'm now finalizing the changes. Just one more thing. The manager node I assume should still use ""gp2"" as storgage type. Is this correct?

Also where do you write your tests?

Tim",one thing manager node assume still use type correct also write,issue,negative,neutral,neutral,neutral,neutral,neutral
619238987,Going with the *and* situation: removing the autofilling process of compatible instances & the instance distribution length is removed (aka allowing instance distribution length = 1). ,going situation removing process compatible instance distribution length removed aka instance distribution length,issue,negative,neutral,neutral,neutral,neutral,neutral
618694623,"@tthebst I'm glad you reached out! I've given it some thought, and I think this could make sense regarding the fields:

```yaml
instance_volume_size: 50  # already exists
instance_volume_type: general_purpose  # or gp2 if you prefer the short hand
instance_volume_iops: 3000  # only applicable for instance_volume_type: provisioned_iops
```

On the go side of things, `instance_volume_type` should be an ""enum"", which we have implemented like [this](https://github.com/cortexlabs/cortex/blob/private-networking/pkg/types/clusterconfig/clusterconfig.go#L229-L238) (which refers to [this file](https://github.com/cortexlabs/cortex/blob/private-networking/pkg/types/clusterconfig/subnet_visibility.go), which you can copy-paste with minor modifications).

`instance_volume_iops` should be an int pointer, which is optional for the user to pass in. In [`clusterconfig.Validate()`](https://github.com/cortexlabs/cortex/blob/master/pkg/types/clusterconfig/clusterconfig.go#L403), you can add a check: if the volume type is `provisioned_iops` and `instance_volume_iops` is nil, you can set it to 3000; if the volume type is not `provisioned_iops` and `instance_volume_iops` is not nil, you return an error.

Does that make sense?",glad given thought think could make sense regarding already prefer short hand applicable go side like file minor pointer optional user pas add check volume type nil set volume type nil return error make sense,issue,positive,positive,positive,positive,positive,positive
618673505,"I'm currently encountering an issue. I generated a new resource metadata file, which works. I'm now working on the config validations. I generated the metadata file such that the IOPS pricing is zero for all storages expect io1 and standard. Now when i want to validate volume_iops this depends on volume_type. With the current validation system, it is not possible to reference other config values in the validation except the one to be validated. I don't really know where the best place to validate volume_iops. Do you have any opinion on this?
You can also look at my fork to see the my changes.
I thought maybe one could create a top level volume config option like:

```
volume_conf:
   - volume_type
   - volume_iops

```

Tim",currently issue new resource file work working file zero expect io standard want validate current validation system possible reference validation except one really know best place validate opinion also look fork see thought maybe one could create top level volume option like,issue,positive,positive,positive,positive,positive,positive
617466077,"@tthebst Sorry, I just assumed that the pricing data would be there, but I did not confirm!

At first I couldn't find it, but after digging in a bit more, I may have found it (I have not cross-referenced with the AWS web console).

I found this in the JSON:

```json
""ZS6E9ESZZKGW2ZVG"" : {
  ""sku"" : ""ZS6E9ESZZKGW2ZVG"",
  ""productFamily"" : ""System Operation"",
  ""attributes"" : {
    ""servicecode"" : ""AmazonEC2"",
    ""location"" : ""US West (Oregon)"",
    ""locationType"" : ""AWS Region"",
    ""provisioned"" : ""Yes"",
    ""group"" : ""EBS IOPS"",
    ""groupDescription"" : ""IOPS"",
    ""usagetype"" : ""USW2-EBS:VolumeP-IOPS.piops"",
    ""operation"" : """",
    ""servicename"" : ""Amazon Elastic Compute Cloud"",
    ""volumeApiName"" : ""io1""
  }
},
```

The SKU can be used to find this in the json:

```json
""ZS6E9ESZZKGW2ZVG"" : {
  ""ZS6E9ESZZKGW2ZVG.JRTCKXETXF"" : {
    ""offerTermCode"" : ""JRTCKXETXF"",
    ""sku"" : ""ZS6E9ESZZKGW2ZVG"",
    ""effectiveDate"" : ""2020-04-01T00:00:00Z"",
    ""priceDimensions"" : {
      ""ZS6E9ESZZKGW2ZVG.JRTCKXETXF.6YS6EN2CT7"" : {
        ""rateCode"" : ""ZS6E9ESZZKGW2ZVG.JRTCKXETXF.6YS6EN2CT7"",
        ""description"" : ""$0.065 per IOPS-month provisioned - US West (Oregon)"",
        ""beginRange"" : ""0"",
        ""endRange"" : ""Inf"",
        ""unit"" : ""IOPS-Mo"",
        ""pricePerUnit"" : {
          ""USD"" : ""0.0650000000""
        },
        ""appliesTo"" : [ ]
      }
    },
    ""termAttributes"" : { }
  }
},
```

Is that the data you are looking for?

If so, feel free to add whichever attribute filters you think make sense to find the SKU, similar to how we filter for the other products.",sorry assumed data would confirm first could find digging bit may found web console found system operation location u west region yes group operation elastic compute cloud io used find description per u west unit data looking feel free add whichever attribute think make sense find similar filter,issue,positive,positive,neutral,neutral,positive,positive
617392718,"@deliahu 

Yes makes sense. And I think I know the general approach. But I encountered one problem with Provisioned IOPS SSD (io1) ,  which is priced by GB an IOPS. The AWS pricing API doesn't provide this pricing I think (which is strange ,  [see](https://forums.aws.amazon.com/thread.jspa?threadID=258259)). Do you have any idea to resolve this?",yes sense think know general approach one problem io priced provide think strange see idea resolve,issue,negative,positive,neutral,neutral,positive,positive
616944449,"@tthebst awesome! Yes, we would like to update the cluster pricing depending on the storage type if that's feasible. Good catch!

Here is where we add it to the CLI output: https://github.com/cortexlabs/cortex/blob/master/cli/cmd/lib_cluster_config.go#L296

That pulls from this file:
https://github.com/cortexlabs/cortex/blob/master/pkg/lib/aws/resource_metadata.go#L3358

Which is generated by running this file:
https://github.com/cortexlabs/cortex/blob/master/pkg/lib/aws/gen_resource_metadata.py#L128
(you can run it with `go generate` from the `pkg/lib/aws/` directory)

So the path to take would probably be to modify `gen_resource_metadata.py` to add the necessary pricing data to `resource_metadata.go`, and then consume that data in `lib_cluster_config.go` (based on the user's configuration).

Does that all make sense? Let us know if you have any other questions!
",awesome yes would like update cluster depending storage type feasible good catch add output file running file run go generate directory path take would probably modify add necessary data consume data based user configuration make sense let u know,issue,positive,positive,positive,positive,positive,positive
616787594,"hi,

I'm currently looking at this. Do you also want to update the pricing depending on the storage type or is the price just a rough guidance?",hi currently looking also want update depending storage type price rough guidance,issue,negative,negative,neutral,neutral,negative,negative
616720615,"@tthebst sounds great! All of the features/improvements/bugs that are on our radar are listed in our [GitHub Project](https://github.com/cortexlabs/cortex/projects/1). That list may be a bit overwhelming, so (as you probably noticed when you searched for this issue) we have created the [""good first issue"" label](https://github.com/cortexlabs/cortex/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) and added it to some of our issues. That said, the other ones are all fair game too, and some of them may be good first issues as well - we plan on making a pass on all of our issues and seeing if we can add the label to more of them.

Feel free to let us know which ones sound interesting to you. We'd be able to advise on which one might be best to start with based on your interests and the scope of the task, as well as point you in the right direction (since some of our descriptions are vague and and some of our issues need additional design thought).",great radar listed project list may bit overwhelming probably issue good first issue label added said fair game may good first well plan making pas seeing add label feel free let u know sound interesting able advise one might best start based scope task well point right direction since vague need additional design thought,issue,positive,positive,positive,positive,positive,positive
616144926,"@deliahu Thank you for your feedback. These are my first real contributions to an opensource project. I like the project and will try to find some good other addressable issues

",thank feedback first real project like project try find good,issue,positive,positive,positive,positive,positive,positive
615947530,"@tthebst Thanks for addressing my comments! I'll try this out on my end soon, and then I'll merge it in.",thanks try end soon merge,issue,negative,positive,positive,positive,positive,positive
615875467,"@tthebst regarding the error you are getting, yes that's correct. Version `0.15` of Cortex still uses those fields. 

On the `master` version though, you'll see that they've been removed (related to https://github.com/cortexlabs/cortex/pull/948). Check out the `master` version of the [documentation](https://www.cortex.dev/v/master/contributing/development).",regarding error getting yes correct version cortex still master version though see removed related check master version documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
615721554,"Hi, 
I will try to work on this one.
Just one question don't know if its an error. In the development [docs](https://www.cortex.dev/contributing/development) there is an instruction on how to update cluster.yaml.
I think the following aren't an available option for clusterconfig.
```
image_python_serve:
image_python_serve_gpu: 
image_tf_serve: 
image_tf_serve_gpu: 
image_tf_api:
image_onnx_serve: 
```

Tim",hi try work one one question know error development instruction update think following available option,issue,negative,positive,positive,positive,positive,positive
614252621,"Closing due to inactivity, as well as existing issue https://github.com/cortexlabs/cortex/issues/782.

Our upcoming v0.16 release will include support for running Cortex locally or on any VM (this will only run on a single instance, and will not support features such as autoscaling, load balancing, etc). Feel free to follow up here or on [Gitter](https://gitter.im/cortexlabs/cortex) if you have any questions.",due inactivity well issue upcoming release include support running cortex locally run single instance support load balancing feel free follow,issue,positive,positive,neutral,neutral,positive,positive
614250471,Closing due to inactivity. Feel free to follow up here or on [Gitter](https://gitter.im/cortexlabs/cortex) if you have any questions.,due inactivity feel free follow,issue,positive,positive,positive,positive,positive,positive
614219432,"@tharun208 we'd welcome this contribution, thank you! I just updated the description to be more clear, let us know if you have any questions. Since this will touch code that's in the operator, you'll probably want to set up a [development environment](https://www.cortex.dev/contributing/development).",welcome contribution thank description clear let u know since touch code operator probably want set development environment,issue,positive,positive,positive,positive,positive,positive
613732133,"Hi @gautamchitnis, thank you for offering to contribute! We still don't know exactly when we'll have time to work on GCP support. If you're interested in contributing in other ways please feel free to email me at omer@cortexlabs.com and we can set up some time to chat.",hi thank offering contribute still know exactly time work support interested way please feel free omer set time chat,issue,positive,positive,positive,positive,positive,positive
613274501,I am interested in helping cortex support GCP as I am currently planning to use GCP for my product deployment. Do let me know how I can contribute.,interested helping cortex support currently use product deployment let know contribute,issue,positive,positive,positive,positive,positive,positive
612481574,"Thanks @cabhijith!

Cortex takes ML models and turns them into scalable web services that you can use in production. Streamlit seems to be focused on helping you create visualizations.",thanks cortex turn scalable web use production helping create,issue,positive,positive,positive,positive,positive,positive
612274161,"Useful resources:

- https://github.com/aws/aws-neuron-sdk/blob/master/docs/neuron-container-tools/tutorial-k8s.md - device plugin to expose inf chips to k8s.
- https://github.com/aws/aws-neuron-sdk/blob/master/FAQ.md - faq on aws neuron sdk.
- https://aws.amazon.com/blogs/machine-learning/reduce-inference-costs-on-amazon-ec2-for-pytorch-models-with-amazon-elastic-inference/ - for pytorch stuff.",useful device expose chip neuron stuff,issue,negative,positive,positive,positive,positive,positive
612203139,@deliahu letting you know I'm going back to this ticket (as we have previously discussed). Been busy with #948 so far. Hope that's okay with you.,know going back ticket previously busy far hope,issue,negative,positive,neutral,neutral,positive,positive
611665793,"@deliahu 
This will also close #950. Here's what the errors can look like now:
```
# robertlucian/sample-image is a private image
error: cpy_pred.yaml: iris-classifier-2: predictor: image: robertlucian/sample-image is not accessible: docker client:
Error response from daemon: errors:
denied: requested access to the resource is denied
unauthorized: authentication required

# cluster has no read permissions on ECR
error: cpy_pred.yaml: iris-classifier-2: predictor: image: XXXXXXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/mock-repository/hello-world is not accessible: docker client:
Error response from daemon: denied: User: arn:aws:iam::XXXXXXXXXXXX:user/cloud-user is not authorized to perform: ecr:BatchGetImage on resource: arn:aws:ecr:eu-central-1:XXXXXXXXXXXX:repository/mock-repository/hello-world with an explicit deny

# docker image does not exist
error: cpy_pred.yaml: iris-classifier-2: predictor: image: cortexlabs/python-serve-non-existent:master is not accessible: docker client:
Error response from daemon: errors:
denied: requested access to the resource is denied
unauthorized: authentication required

# ECR image does not exist
error: cpy_pred.yaml: iris-classifier-2: predictor: image: XXXXXXXXXXXX.dkr.ecr.eu-central-1.amazonaws.com/cortexlabs/python-serve-non-existent is not accessible: docker client:
Error response from daemon: manifest unknown: Requested image not found

# Non-existent registry (checkout ddkr vs dkr)
error: cpy_pred.yaml: iris-classifier-2: predictor: image: XXXXXXXXXXXX.ddkr.ecr.eu-central-1.amazonaws.com/cortexlabs/python-serve:custom is not accessible: docker client:
Error response from daemon: Get https://XXXXXXXXXXXX.ddkr.ecr.eu-central-1.amazonaws.com/v2/: dial tcp: lookup XXXXXXXXXXXX.ddkr.ecr.eu-central-1.amazonaws.com: no such host

# Non-matching registry account IDs
error: cpy_pred.yaml: iris-classifier-2: predictor: image: AWS registry account ID YYYYYYYYYYYY doesn't match operator's account ID XXXXXXXXXXXX

# Version mismatch (at the tag level)
error: cpy_pred.yaml: iris-classifier-2: predictor: image: the specified image (cortexlabs/python-serve:0.15) has a tag (0.15) which does not match the version of your CLI (master); please update the image tag, remove the image from the API config file (to use the default value), or update your CLI by following the instructions at https://www.cortex.dev/install

# Unsupported field for Python/ONNX Predictors
error: cpy_pred.yaml: iris-classifier-2: predictor: tf_serve_image is not a supported field for the python predictor type

# Should never happen
error: cpy_pred.yaml: iris-classifier-2: predictor: image: failed to encode docker login credentials: json: <insert error>

# Should never happen
error: cpy_pred.yaml: iris-classifier-2: predictor: image: failed to retrieve ECR auth token: <insert error>

# Should never happen
error: cpy_pred.yaml: iris-classifier-2: predictor: image: unable to extract ECR credentials: <insert error>
```

One thing to keep in mind: when trying to inspect images from other registries other than ECR, the docker client can't tell whether the image simply does not exist or it's private. I think this stems from the fact that the docker client has to be authenticated in the said registry. This does work with ECR though, because the client has the authentification token. 

One a different note, there's no need to use the AWS ECR client to inspect images. As it appears, it's generally expected to use AWS ECR API to get the authentification token and then use the docker client instead. The AWS ECR API can also be used to modify policies and stuff like that, but  not things pertaining to the docker client. 

Here are the validations done for a generic docker image:
1. Check tag version.
1. Inspect image w/ docker client. If error, propagate.

Here are the validations done for an ECR image:
1. Check tag version.
1. Match account IDs.
1. Inspect image w/ docker client. If error, propagate.

Regarding the errors returned upon inspecting the image, I saw the errors can get quite large (verbosy like in the examples above) and so I decided to place them on a new line right after adding the `docker client: ` message to all the other wrapped messages. I'm also thinking that this is a way to put emphasis on the docker client error and separate it from other cortex errors.",also close look like private image error predictor image accessible docker client error response daemon access resource unauthorized authentication cluster read error predictor image accessible docker client error response daemon user arn authorized perform resource arn explicit deny docker image exist error predictor image master accessible docker client error response daemon access resource unauthorized authentication image exist error predictor image accessible docker client error response daemon manifest unknown image found registry error predictor image custom accessible docker client error response daemon get dial host registry account error predictor image registry account id match operator account id version mismatch tag level error predictor image image tag match version master please update image tag remove image file use default value update following unsupported field error predictor field python predictor type never happen error predictor image encode docker login insert error never happen error predictor image retrieve token insert error never happen error predictor image unable extract insert error one thing keep mind trying inspect docker client ca tell whether image simply exist private think fact docker client said registry work though client token one different note need use client inspect generally use get token use docker client instead also used modify stuff like pertaining docker client done generic docker image check tag version inspect image docker client error propagate done image check tag version match account inspect image docker client error propagate regarding returned upon image saw get quite large like decided place new line right docker client message wrapped also thinking way put emphasis docker client error separate cortex,issue,negative,positive,positive,positive,positive,positive
611110456,"Oh yeah, definitely. Both of them are gonna be closed at the same time. ",oh yeah definitely gon na closed time,issue,positive,negative,neutral,neutral,negative,negative
611102708,Keeping it as a separate ticket is totally ok! I was just thinking https://github.com/cortexlabs/cortex/issues/900 should close both üëç (which it sounds like your were too),keeping separate ticket totally thinking close like,issue,negative,neutral,neutral,neutral,neutral,neutral
610675678,"@deliahu Hmm, so you're saying this ticket should be closed and have the description added to #900? I agree - this ticket is tied to #900. ",saying ticket closed description added agree ticket tied,issue,negative,negative,neutral,neutral,negative,negative
610637367,"@RobertLucian I think we may want to fold this into https://github.com/cortexlabs/cortex/issues/900, what do you think?",think may want fold think,issue,negative,neutral,neutral,neutral,neutral,neutral
609462301,"@wise-east yes, as long as you haven't changed the name of your API and you haven't run `cortex cluster down`, the endpoint will remain the same",yes long name run cortex cluster remain,issue,negative,negative,neutral,neutral,negative,negative
609365534,"I've tried `cortex delete` and `cortex deploy` and was able to see that the endpoint remained the same, at least for the one time that I tried it. Is it guaranteed to be consistent if the config file, i.e. `cortex.yaml` file, remains the same? ",tried cortex delete cortex deploy able see least one time tried consistent file file remains,issue,negative,positive,positive,positive,positive,positive
609332273,"> However, my guess is that if you terminate the instance, a replacement will be immediately created, since there will be an unscheduled API replica and the cluster autoscaler will notice that and request a new instance to schedule it on.

I'm glad I asked and thank you for getting back to me so soon! I would've ended up wasting a lot of time, thank you. 

> I think the best option would be to run cortex delete <api_name> and cortex deploy when desired (either on a schedule or based on other events/metrics). If you set min_instances in your cluster to 0, then when you cortex delete your API, the instance will spin down (after a short delay). When you then run cortex deploy it will request a new instance for the API again.

This makes a lot of sense. I'd be looking forward to when this can be done natively as mentioned in #445. 

I'm wondering if this option is also free from the networking issue that I am concerned about. If the API is deleted and then redeployed, wouldn't it be given a new url endpoint that I have to configure with the API Gateway service? ",however guess terminate instance replacement immediately since unscheduled replica cluster notice request new instance schedule glad thank getting back soon would ended wasting lot time thank think best option would run cortex delete cortex deploy desired either schedule based set cluster cortex delete instance spin short delay run cortex deploy request new instance lot sense looking forward done natively wondering option also free issue concerned would given new configure gateway service,issue,positive,positive,positive,positive,positive,positive
609151920,"I understand your motivation for this since your traffic is low (at least for now, hopefully that changes soon :) ). We have https://github.com/cortexlabs/cortex/issues/445 to track our progress on supporting this natively.

Until then, there are a few possibilities I can think of. I have not tried using the AWS InstanceScheduler before; I think the networking would actually be ok, since the request enters through the ELB (which won't go down with the instance), and kubernetes automatically maintains routing configurations as nodes come in and out. However, my guess is that if you terminate the instance, a replacement will be immediately created, since there will be an unscheduled API replica and the cluster autoscaler will notice that and request a new instance to schedule it on.

I think the best option would be to run `cortex delete <api_name>` and `cortex deploy` when desired (either on a schedule or based on other events/metrics). If you set `min_instances` in your cluster to 0, then when you `cortex delete` your API, the instance will spin down (after a short delay). When you then run `cortex deploy` it will request a new instance for the API again.

Another option is to programmatically modify the AWS autoscaling groups (for the workloads) to set max instances to 0, and then back to 1 (or more) as desired. The reason this isn't as good as the first option is that it's better for cortex to manage the autoscaling groups, otherwise some features like on-demand backup for spot instances may not work.

Let us know if you'd like any help setting this up!",understand motivation since traffic low least hopefully soon track progress supporting natively think tried think would actually since request elb wo go instance automatically routing come however guess terminate instance replacement immediately since unscheduled replica cluster notice request new instance schedule think best option would run cortex delete cortex deploy desired either schedule based set cluster cortex delete instance spin short delay run cortex deploy request new instance another option programmatically modify set back desired reason good first option better cortex manage otherwise like backup spot may work let u know like help setting,issue,positive,positive,positive,positive,positive,positive
609107959,"> That's awesome! Has the cluster been fully functional thus far? I'm considering adding your list to the docs, however my main concern is that over time as we add new features, this list may change and we'll have to invest in keeping it up to date (and in reality it may not be much ""lighter"" than the AdministratorAccess anyways).

It has been fully functional so far, and if I recall correctly these will also allow for these steps to be taken with the IAM account as well: https://www.cortex.dev/guides/api-gateway 

I agree on your concern and it might not be that useful in the longer run, but maybe useful for the short term. In addition, the action configuration `iam:*` actually defeats the purpose of setting up an IAM because then the IAM user can change his/her policy to allow for `AdministratorAccess`. 

> Yes, this is a known behavior. We try to mention how to clean this up manually in the error message, since in some scenarios it's difficult to do in an automated way. We improved the error messages a bit in v0.15, and we added a check to see if the IAM user has AdministratorAccess to help avoid this situation. Should we make any of the error messages or warnings more clear? Perhaps the AdministratorAccess warning could include instructions on how to add the policy to your IAM user?

I think the error messages were pretty clear even in v0.14, and I haven't checked the error messages that would appear in v0.15 so I can't give the best opinion on this. For those who are not familiar with setting policies for IAM users may find the instructions useful, but my concern is that it would lead to an iterative and cumbersome process where you add individual policies such as ""iam:CreateRole"", delete the stack and log group, and then rerun `cortex cluster up` just to find out that you have to add ""iam:<some other function>"" and repeat the same process. Maybe an error message saying ""You're likely getting this error message because you don't have the full list of permissions for spinning up this cluster. Please make sure that the IAM user you are using to spin up the cluster has `AdministratorAccess` or all of the following permissions: `<link to the configuration that I provided above or instruction on adding it>`"" will save the user from the pain I had to go through. ",awesome cluster fully functional thus far considering list however main concern time add new list may change invest keeping date reality may much lighter anyways fully functional far recall correctly also allow taken account well agree concern might useful longer run maybe useful short term addition action configuration actually purpose setting user change policy allow yes known behavior try mention clean manually error message since difficult way error bit added check see user help avoid situation make error clear perhaps warning could include add policy user think error pretty clear even checked error would appear ca give best opinion familiar setting may find useful concern would lead iterative cumbersome process add individual delete stack log group rerun cortex cluster find add function repeat process maybe error message saying likely getting error message full list spinning cluster please make sure user spin cluster following link configuration provided instruction save user pain go,issue,positive,positive,positive,positive,positive,positive
608716023,"@wise-east thanks for sharing this!

> I'm wondering if there is a minimal AWS permission policy that you can provide for setting up IAM users that can run the cortex cluster up command without failing due to permission issues...  I think providing a minimal set of permissions in the format above can be useful for users like me who don't have access to the root account, as it took me a while to able to list out all the requirements.

We have not identified this ""minimal"" policy since the CLI requires many permissions (as you noticed), and it's difficult to create an exhaustive list since the requirements may change as Cortex adds new features. Is it possible for you to add the `AdministratorAccess` policy to your IAM user? Also in case it helps (as mentioned in the docs you referenced), you can use credentials with fewer permissions for the cluster itself.

> After multiple trial-and-errors and some research, I set up a custom AWS permission policy with JSON that allows me to do all the things I need

That's awesome! Has the cluster been fully functional thus far? I'm considering adding your list to the docs, however my main concern is that over time as we add new features, this list may change and we'll have to invest in keeping it up to date (and in reality it may not be much ""lighter"" than the `AdministratorAccess` anyways).

> However, I am certain that this is not the minimal permissions that can be given

Yes you are correct

> Just FYI, I also realized that when I fail on certain levels, towards the beginning of setting up the cluster, I had to manually delete the stack on CloudFormation and the CloudWatch log group to be able to run `cortex cluster up` again.

Yes, this is a known behavior. We try to mention how to clean this up manually in the error message, since in some scenarios it's difficult to do in an automated way. We improved the error messages a bit in v0.15, and we added a check to see if the IAM user has `AdministratorAccess` to help avoid this situation. Should we make any of the error messages or warnings more clear? Perhaps the `AdministratorAccess` warning could include instructions on how to add the policy to your IAM user?",thanks wondering minimal permission policy provide setting run cortex cluster command without failing due permission think providing minimal set format useful like access root account took able list minimal policy since many difficult create exhaustive list since may change cortex new possible add policy user also case use cluster multiple research set custom permission policy need awesome cluster fully functional thus far considering list however main concern time add new list may change invest keeping date reality may much lighter anyways however certain minimal given yes correct also fail certain towards beginning setting cluster manually delete stack log group able run cortex cluster yes known behavior try mention clean manually error message since difficult way error bit added check see user help avoid situation make error clear perhaps warning could include add policy user,issue,positive,positive,positive,positive,positive,positive
608278672,"@deliahu I understand. 

That is a very thorough guide! Thank you for adding it :) ",understand thorough guide thank,issue,negative,neutral,neutral,neutral,neutral,neutral
608230303,Thank you! I suspected the --env flag to be of use here but it wasn't clear with the current docs. Thank you for the steps! ,thank suspected flag use clear current thank,issue,positive,positive,neutral,neutral,positive,positive
608177963,"@wise-east Yes we support the `--env` flag on most `cortex` commands to allow the CLI to connect with multiple clusters. I will create a proper guide on this, but I have to head out now, so in the meantime here is a quick version (to make it simpler I'll assume one CPU cluster and one GPU cluster):

If you are spinning up new clusters, the easiest way would look something like this:

1) `cortex cluster up --env gpu --config cluster-gpu.yaml`
2) `cortex cluster up --env cpu --config cluster-cpu.yaml`
3) `cortex deploy --env gpu` (will deploy the API to the gpu cluster)
4) `cortex deploy --env cpu` (will deploy the API to the cpu cluster)

If you already have a cluster running, or forget the `--env` flag on `cortex cluster up`, you can do something like this:

1) `cortex cluster info --config cluster-gpu.yaml` (will show you the operator url for the gpu cluster)
2) `cortex configure --env gpu` (use the operator URL from step 1)
3) `cortex cluster info --config cluster-cpu.yaml` (will show you the operator url for the cpu cluster)
4) `cortex configure --env cpu` (use the operator URL from step 3)
5) `cortex deploy --env gpu` (will deploy the API to the gpu cluster)
6) `cortex deploy --env cpu` (will deploy the API to the cpu cluster)",yes support flag cortex allow connect multiple create proper guide head quick version make simpler assume one cluster one cluster spinning new easiest way would look something like cortex cluster cortex cluster cortex deploy deploy cluster cortex deploy deploy cluster already cluster running forget flag cortex cluster something like cortex cluster show operator cluster cortex configure use operator step cortex cluster show operator cluster cortex configure use operator step cortex deploy deploy cluster cortex deploy deploy cluster,issue,positive,positive,positive,positive,positive,positive
608176006,"@wise-east that makes sense. My main concern with the previous version was that it's hard to generalize use cases (that has become more apparent as we've seen different people prefer different instance types for similar applications), and I didn't want to mislead anyone :)

Also, I've added this page to the docs based on our discussion: https://www.cortex.dev/troubleshooting/stuck-updating",sense main concern previous version hard generalize use become apparent seen different people prefer different instance similar want mislead anyone also added page based discussion,issue,negative,negative,neutral,neutral,negative,negative
608161027,"@deliahu  Thank you, the changes are useful! I didn't think the previous format was confusing. In fact, I thought it was quite helpful showing what instances may be useful for specific use-cases, but I see that the downside is lazy people like me would not think further if one seems to match my use-case and just take the doc's word for it. ",thank useful think previous format fact thought quite helpful showing may useful specific see downside lazy people like would think one match take doc word,issue,positive,positive,neutral,neutral,positive,positive
608157407,"@wise-east thanks for pointing that out, I see how those docs can be confusing. I've updated them on master: https://www.cortex.dev/v/master/cluster-management/ec2-instances",thanks pointing see master,issue,negative,positive,positive,positive,positive,positive
608130066,"@wise-east sounds good!

We just release v0.15.1 in case you want to upgrade, just letting you know",good release case want upgrade know,issue,negative,positive,positive,positive,positive,positive
608123555,"@deliahu  That is a great tip, I'll try using the `g4dn.xlarge` instance and compare performance. I blindly assumed the `p2` instance is better given the description here: https://www.cortex.dev/cluster-management/ec2-instances. (The model I'm using is a variant of GPT-2 for text generation) ",great tip try instance compare performance blindly assumed instance better given description model variant text generation,issue,positive,positive,positive,positive,positive,positive
608120672,I see. I was actually able to bypass the issues for now with the guide provided to me at https://www.cortex.dev/v/master/guides/api-gateway by @deliahu ,see actually able bypass guide provided,issue,negative,positive,positive,positive,positive,positive
608120169,"Thank you @deliahu 

I was able to bypass both the SSL certificates and CORS issue by following the guide and I believe it will really help future cortex users. üëç ",thank able bypass issue following guide believe really help future cortex,issue,positive,positive,positive,positive,positive,positive
608106010,@wise-east I just wanted to follow up and let you know that the API Gateway guide has now been promoted to our docs: https://www.cortex.dev/v/master/guides/api-gateway,follow let know gateway guide,issue,negative,neutral,neutral,neutral,neutral,neutral
608038429,"@wise-east Apologies, what I meant by `addressed by returning a blanket` was that the CORS that we had in <0.14.0 returned ""*"" or equivalent for CORS headers like `Access-Control-Allow-Origin` and `Access-Control-Allow-Headers`.

You should be able to get it working with Cortex version <0.14.0. If it is not too urgent, I would recommend waiting because we are looking to release 0.15.1 that should address the CORS issue and a CLI bug fix either today or tomorrow. 

`cortex cluster down`, install cortex CLI with your desired version and `cortex cluster up` is the safest upgrade path at the moment because mismatched versions may cause compatibility issues.",meant blanket returned equivalent like able get working cortex version urgent would recommend waiting looking release address issue bug fix either today tomorrow cortex cluster install cortex desired version cortex cluster upgrade path moment may cause compatibility,issue,positive,positive,positive,positive,positive,positive
607990324,"Forgive my ignorance, I'm not sure what you mean by `addressed by returning a blanket`.  Can you elaborate? 

If the problem is with versions >=0.14.0, and if I want to be able to handle CORS now, is it possible to do so by using a previous version (<0.14.0)? If so, can a newly installed cortex pick up the cluster I have set up with the current installation of cortex after I uninstall my current installation? Or will I have to do `cortex cluster down` and then uninstall and reinstall and do `cortex cluster up` again? I hope that since my AWS credentials will be the same that I won't have to do this and be able to use a previous version. ",forgive ignorance sure mean blanket elaborate problem want able handle possible previous version newly cortex pick cluster set current installation cortex current installation cortex cluster reinstall cortex cluster hope since wo able use previous version,issue,negative,positive,positive,positive,positive,positive
607961131,"@wise-east Also I thought of something last night but forgot to mention it in my previous response: unless there is a specific reason your model requires the `p2` instances, many of our users have found that the `g4dn.xlarge` have better price/performance for inference. Just wanted to let you know in case it's relevant!",also thought something last night forgot mention previous response unless specific reason model many found better inference let know case relevant,issue,negative,positive,positive,positive,positive,positive
607959624,"We're happy to help any time; thanks for you suggestions, and feel free to keep the questions coming!",happy help time thanks feel free keep coming,issue,positive,positive,positive,positive,positive,positive
607958990,"@wise-east Thanks for reaching out! We have plans (https://github.com/cortexlabs/cortex/issues/326) to leverage AWS API Gateway to support custom certificates, as well as just using AWS's out-of-the-box certificate (that will make things even easier).

In the mean time, the easiest way to get https working without the `-k` flag is to manually add API Gateway in front of the Cortex APIs. We'll be adding this to the docs soon, but in the mean time, you can see the instructions [here](https://gist.github.com/vishalbollu/9af99c5bb0f892374058d8ff3b50e34e).

The CORS issue has been fixed in https://github.com/cortexlabs/cortex/pull/942, and will be included in the 0.15.1 release we are planning for today. We have not yet tested API Gateway with CORS together, but will do so as part of our release testing process.",thanks reaching leverage gateway support custom well certificate make even easier mean time easiest way get working without flag manually add gateway front cortex soon mean time see issue fixed included release today yet tested gateway together part release testing process,issue,positive,negative,neutral,neutral,negative,negative
607785693,"@wise-east Issue #539 was addressed by returning a blanket
```
Access-Control-Allow-Origin: *
Access-Control-Allow-Headers: * # can be configured if request sends Access-Control-Request-Headers 
```
 The ability to specify custom CORS headers as described in this ticket hasn't been implemented yet. Do you think returning responses with the headers above will satisfy your current requirements in the meantime?

Thanks for bringing this up. I just looked through our serving code and I think I found a bug in Cortex versions >= 0.14.0 that prevents the server from handling CORS correctly. Specifically, I don't think the server responds correctly to OPTIONS requests. We will investigate this issue more thoroughly.",issue blanket request ability specify custom ticket yet think satisfy current thanks serving code think found bug cortex server handling correctly specifically think server correctly investigate issue thoroughly,issue,positive,positive,neutral,neutral,positive,positive
607635184,"Thank you so much for the detailed response, now everything makes a lot more sense and I hope I can get around these issue to make my particular use-case working. I really admire this effort in making scalable deployments easier ü•á ",thank much detailed response everything lot sense hope get around issue make particular working really admire effort making scalable easier,issue,positive,positive,positive,positive,positive,positive
607631886,Tagging @vishalbollu who has spent the most time on CORS. I believe @wise-east is running v0.14 (please correct me if I'm wrong),spent time believe running please correct wrong,issue,negative,negative,negative,negative,negative,negative
607631364,"@wise-east that is helpful, thank you for including that output.

> it has to do with reaching the AWS limit for my instance type

Yes that is correct. The way AWS enforces EC2 limits is purely based on vCPU within an ""instance family"", no matter the instance type or the GPU/memory on that instance. Your current limit in the ""P"" family for your region seems to be 4 vCPU. This means that you can only run one `p2.xlarge` in your AWS account (in that region), unrelated to Cortex, since a `p2.xlarge` has 4 vCPU. Once your cluster already had one `p2.xlarge` instance running, your were at your limit of 4 vCPU, and AWS rejected Cortex's request for another instance. You can request a limit increase from the [EC2 dashboard](https://console.aws.amazon.com/ec2/v2/home?#Limits:); search ""P instances"", select ""Running On-Demand All P instances"", and click ""Request limit increase"" in the top right. I know this can be a bit confusing... let me know if that wasn't clear and I can try to explain differently, or feel free to email me at david@cortex.dev if you'd like to set up a call to discuss further. Also here is an [AWS blog post ](https://aws.amazon.com/blogs/compute/preview-vcpu-based-instance-limits/) which explains the vCPU limits.

> Does this message mean that I should be using an instance with more CPUs, such as p2.8xlarge?

As mentioned above, currently AWS will not allow you to run a `p2.8xlarge` instance, since it has 32 vCPU but your limit is 4.

> As a side note, it would be nice if there was a way to see this AWS error message through the cortex CLI or have this mentioned in the docs as something to check when deploying a model takes too long.

Yes I agree 100%. We have this GitHub issue https://github.com/cortexlabs/cortex/issues/936 to address this, and I think adding something in the docs now would go a long way, so I just added it to our list of docs changes to consider for our next release (https://github.com/cortexlabs/cortex/issues/914). Thanks for the suggestion!

> as I changed the code and tried to update the additional API, it has been held with the updating status since

This is another scenario which we'll add to the same docs you suggested above. Here is what happened: By default, cortex performs rolling updates on all APIs. Your API requested 1 GPU and was running on an instance with 1 GPU. When you update your API, cortex deploys the updated version, and only takes down the old version once the new one has deployed without error (this is to ensure that traffic can continue to be served during updates, and that there is no downtime if there's an error in the new version). In your case, since there was no GPU available on your instance, the new version of your API requested another instance to run on. Normally this will be ok (it might just take a few minutes since a new instance has to spin up): the new instance will become live, the new API replica will run on it, once it starts up successfully the old replica will be terminated, and eventually the old instance should spin down. In your case, it got stuck because you had reached your AWS account limit, and the second instance could not be created. We'll definitely add this to the ""troubleshooting"" section you suggested above.

It is possible to disable rolling updates in cortex: in your API configuration (e.g. in `cortex.yaml`), set `max_surge` to 0 (in the `update_strategy` configuration). Here are [the docs](https://www.cortex.dev/deployments/api-configuration#python-predictor). E.g.:

```yaml
- name: iris-classifier
  predictor:
    type: python
    ...
  update_strategy:
    max_surge: 0
```",helpful thank output reaching limit instance type yes correct way purely based within instance family matter instance type instance current limit family region run one account region unrelated cortex since cluster already one instance running limit cortex request another instance request limit increase dashboard search select running click request limit increase top right know bit let know clear try explain differently feel free like set call discus also post message mean instance currently allow run instance since limit side note would nice way see error message cortex something check model long yes agree issue address think something would go long way added list consider next release thanks suggestion code tried update additional status since another scenario add default cortex rolling running instance update cortex version old version new one without error ensure traffic continue error new version case since available instance new version another instance run normally might take since new instance spin new instance become live new replica run successfully old replica eventually old instance spin case got stuck account limit second instance could definitely add section possible disable rolling cortex configuration set configuration name predictor type python,issue,positive,positive,positive,positive,positive,positive
607627309,How can we configure CORS for the current version? I see #539 is closed. ,configure current version see closed,issue,negative,negative,neutral,neutral,negative,negative
607621548,"I just pushed the fix; v0.15.1 is building now, and we plan on testing and releasing it tomorrow.

Thanks again for the heads up!",fix building plan testing tomorrow thanks,issue,negative,positive,positive,positive,positive,positive
607618677,"This is what I found according to the instruction you provided: 

> Status: Failed / Description: Launching a new EC2 instance 
> 
> Description: 
> Launching a new EC2 instance. Status Reason: You have requested more vCPU capacity than your current vCPU limit of 4 allows for the instance bucket that the specified instance type belongs to. Please visit http://aws.amazon.com/contact-us/ec2-request to request an adjustment to this limit. Launching EC2 instance failed.
> 
> Cause: 
> At 2020-04-02T04:04:10Z a user request explicitly set group desired capacity changing the desired capacity from 3 to 2. At 2020-04-02T04:04:28Z an instance was started in response to a difference between desired and actual capacity, increasing the capacity from 1 to 2.


My `max_instances` is set to 5, so I don't think that was the issue and as the message says above, it has to do with reaching the AWS limit for my instance type. I'm using a p2.xlarge instance and I can't quite get my head around why 4 CPUs were not enough for accommodating my API. Does this message mean that I should be using an instance with more CPUs, such as p2.8xlarge?

As a side note, it would be nice if there was a way to see this AWS error message through the cortex CLI or have this mentioned in the docs as something to check when deploying a model takes too long.  

`Also, were your previously-deployed APIs still running in your cluster when you deployed the one that got stuck?` --> Yes, I had a `iris-classifier` API running when I added a new API, but the new API worked initially when I first deployed it while the iris-classifier was already running, but as I changed the code and tried to update the additional API, it has been held with the `updating` status since. 



",found according instruction provided status description new instance description new instance status reason capacity current limit instance bucket instance type please visit request adjustment limit instance cause user request explicitly set group desired capacity desired capacity instance response difference desired actual capacity increasing capacity set think issue message reaching limit instance type instance ca quite get head around enough accommodating message mean instance side note would nice way see error message cortex something check model long also still running cluster one got stuck yes running added new new worked initially first already running code tried update additional status since,issue,positive,positive,neutral,neutral,positive,positive
607610106,"@wise-east There are a few things that could be causing this, do you mind checking your autoscaling group activity history by following [these instructions](https://gist.github.com/deliahu/b9bdb4055473c2719cc7aa19e6c8f00e), and seeing if there is any useful information there?

Also, were your previously-deployed APIs still running in your cluster when you deployed the one that got stuck?

One guess is that when you deployed the 2nd API, it didn't fit on your existing instance(s) (e.g. if the other API was still running), and when your cluster requested an additional instance from AWS, for some reason AWS was not able to fulfill that request. One possible reason is that you have reached your `max_instances` you configured when you spun up the cluster, or you have reached your AWS limits for your instance type (there is a separate limit for spot and for on-demand). The AWS limit issues would show up in the auto scaling group activity history, and you can check your cluster `max_instances` by running `cortex cluster info`.",could causing mind group activity history following seeing useful information also still running cluster one got stuck one guess fit instance still running cluster additional instance reason able fulfill request one possible reason spun cluster instance type separate limit spot limit would show auto scaling group activity history check cluster running cortex cluster,issue,positive,positive,positive,positive,positive,positive
607595776,"Has this been updated? I've modified some code for the `text-generator` example in the repo and currently waiting for it to deploy, but it's been saying `updating` for over an hour now. I have also deployed the given `text-generator` example but it is also taking a long time. If I try to do `cortex logs <api that is taking a long time>`, I just get `fetching logs...`.  I am wondering what the usual wait time is for deployment of a GPT-2 model. FYI, I was able to get the `iris-classifier` example deployed and working in less than 10 seconds. ",code example currently waiting deploy saying hour also given example also taking long time try cortex taking long time get fetching wondering usual wait time deployment model able get example working le,issue,negative,positive,neutral,neutral,positive,positive
607574314,"Yes this is super helpful, thank you! It explains why I was unable to reproduce it: I am using an IAM user, and this issue only happens with the root user.

I will work on a fix tonight, and we'll try to have it released tomorrow. In the mean time you can continue using 0.14, or you can create an IAM user (give it the ""AdministratorAccess"" policy) and use that instead of your root user.",yes super helpful thank unable reproduce user issue root user work fix tonight try tomorrow mean time continue create user give policy use instead root user,issue,positive,negative,negative,negative,negative,negative
607567509,"My pleasure. 

1.  Here's my output for `aws iam get-user` 
```
{
    ""User"": {
        ""UserId"": ""493676065566"",
        ""Arn"": ""arn:aws:iam::493676065566:root"",
        ""CreateDate"": ""2018-06-07T08:09:37Z"",
        ""PasswordLastUsed"": ""2020-04-01T17:59:10Z""
    }
}
```

2.   Here's the output I get for the commands you've asked me to run. 
```
(interface {}) <nil>
---------------------
(*iam.GetUserOutput)(({
  User: {
    Arn: ""arn:aws:iam::493676065566:root"",
    CreateDate: 2018-06-07 08:09:37 +0000 UTC,
    PasswordLastUsed: 2020-04-01 17:59:10 +0000 UTC,
    UserId: ""493676065566""
  }
}) {
  _: (struct {}) {
  },
  User: (*iam.User)(({
  Arn: ""arn:aws:iam::493676065566:root"",
  CreateDate: 2018-06-07 08:09:37 +0000 UTC,
  PasswordLastUsed: 2020-04-01 17:59:10 +0000 UTC,
  UserId: ""493676065566""
}) {
    _: (struct {}) {
    },
    Arn: (*string)((len=30) ""arn:aws:iam::493676065566:root""),
    CreateDate: (*time.Time)((2018-06-07 08:09:37 +0000 UTC) {
      wall: (uint64) 0,
      ext: (int64) 63663955777,
      loc: (*time.Location)(<nil>)
    }),
    PasswordLastUsed: (*time.Time)((2020-04-01 17:59:10 +0000 UTC) {
      wall: (uint64) 0,
      ext: (int64) 63721360750,
      loc: (*time.Location)(<nil>)
    }),
    Path: (*string)(<nil>),
    PermissionsBoundary: (*iam.AttachedPermissionsBoundary)(<nil>),
    Tags: ([]*iam.Tag) <nil>,
    UserId: (*string)((len=12) ""493676065566""),
    UserName: (*string)(<nil>)
  })
})
=====================
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
	panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x8 pc=0x1b5bf0b]

goroutine 1 [running]:
github.com/cortexlabs/cortex/pkg/lib/exit.Panic(0x2e88a80, 0xc00007f680, 0x0, 0x0, 0x0)
	/home/ubuntu/src/github.com/cortexlabs/cortex/pkg/lib/exit/exit.go:60 +0x1c0
github.com/cortexlabs/cortex/pkg/lib/exit.RecoverAndExit(0x0, 0x0, 0x0)
	/home/ubuntu/src/github.com/cortexlabs/cortex/pkg/lib/exit/exit.go:66 +0x9d
panic(0x27f3be0, 0x3e06950)
	/usr/local/go/src/runtime/panic.go:967 +0x166
github.com/cortexlabs/cortex/pkg/lib/aws.(*Client).IsAdmin(0xc00030a070, 0x15)
	/home/ubuntu/src/github.com/cortexlabs/cortex/pkg/lib/aws/iam.go:104 +0x7b
github.com/cortexlabs/cortex/cli/cmd.promptIfNotAdmin(0xc00030a070)
	/home/ubuntu/src/github.com/cortexlabs/cortex/cli/cmd/lib_aws_creds.go:60 +0x59
github.com/cortexlabs/cortex/cli/cmd.getInstallClusterConfig(0xc0000b6f80, 0x14, 0xc0000ac420, 0x28, 0xc0000b6f80, 0x14, 0xc0000ac420, 0x28, 0xc0000ac420, 0x28, ...)
	/home/ubuntu/src/github.com/cortexlabs/cortex/cli/cmd/lib_cluster_config.go:143 +0x10c
github.com/cortexlabs/cortex/cli/cmd.glob..func1(0x3e18540, 0x3e55938, 0x0, 0x0)
	/home/ubuntu/src/github.com/cortexlabs/cortex/cli/cmd/cluster.go:89 +0x10c
github.com/spf13/cobra.(*Command).execute(0x3e18540, 0x3e55938, 0x0, 0x0, 0x3e18540, 0x3e55938)
	/home/ubuntu/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830 +0x29d
github.com/spf13/cobra.(*Command).ExecuteC(0x3e1a0c0, 0xc00065ff40, 0x1, 0x1)
	/home/ubuntu/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914 +0x2fb
github.com/spf13/cobra.(*Command).Execute(...)
	/home/ubuntu/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864
github.com/cortexlabs/cortex/cli/cmd.Execute()
	/home/ubuntu/src/github.com/cortexlabs/cortex/cli/cmd/root.go:132 +0x300
main.main()
	/home/ubuntu/src/github.com/cortexlabs/cortex/cli/main.go:24 +0x20
```

Hope this helps!",pleasure output user arn arn root output get run interface nil user arn arn root user arn arn root arn string arn root wall nil wall nil path string nil nil nil string string nil panic error invalid memory address nil pointer panic error invalid memory address nil pointer signal segmentation violation running panic client command command command hope,issue,negative,neutral,neutral,neutral,neutral,neutral
607533145,"@wise-east Thank you for reporting this!

I believe this is an issue with a new IAM check we added for 0.15. Do you mind trying two things:

1\. What does `aws iam get-user` show for you (using the same credentials you are using for the cluster)? There shouldn't be sensitive information in there, but feel free to obscure anything if you'd like.

2\.  I have built a slightly modified version of the CLI, which will print stack traces for panics (this change https://github.com/cortexlabs/cortex/pull/921 has been merged into master, but not yet released). I also added extra debug statements to try to see what is going on. Do you mind trying this CLI and sending the output?

```bash
# make a temporary directory for the test
mkdir cli-test
cd cli-test

# download the modified CLI

# on Mac:
curl -o cortex-test https://cortex-cli-david.s3-us-west-2.amazonaws.com/0.15.0/cli/darwin/cortex
# on Linux:
curl -o cortex-test https://cortex-cli-david.s3-us-west-2.amazonaws.com/0.15.0/cli/linux/cortex

chmod +x cortex-test

# run the command that was breaking for you
./cortex-test cluster up -c <path to cluster.yaml>

# ctrl+c if it asks ""do you want to continue"", but it will probably crash before then

# clean up
cd ..
rm -rf cli-test
```

Feel free to email me at david@cortex.dev if you prefer, I'd also be happy to jump on a call if you'd like to go through it together, since this is on the critical path and I'd like to get it fixed as soon as possible.

Thanks again for reporting this!",thank believe issue new check added mind trying two show cluster sensitive information feel free obscure anything like built slightly version print stack change master yet also added extra try see going mind trying sending output bash make temporary directory test mac curl curl run command breaking cluster path want continue probably crash clean feel free prefer also happy jump call like go together since critical path like get fixed soon possible thanks,issue,positive,positive,positive,positive,positive,positive
606874017,"@deliahu Oh yeah, that's perfect. That shines light on both sides of the coin. ",oh yeah perfect light side coin,issue,positive,positive,positive,positive,positive,positive
606855380,"@RobertLucian Thanks for improving this! I made an edit (to mention client retries), does it make sense to you?",thanks improving made edit mention client make sense,issue,positive,positive,positive,positive,positive,positive
606825641,"@RobertLucian yes that is correct, @nabeeltahir12 we are planning to release the local environment in our next release, which will probably be in ~2 weeks. We haven't yet researched how GPUs work with docker locally, but we will look into it, and we'll include GPU support in this release if it's relatively straightforward for us to implement it",yes correct release local environment next release probably yet work docker locally look include support release relatively straightforward u implement,issue,positive,positive,neutral,neutral,positive,positive
606514613,"@nabeeltahir12 as far as I know, a local version of `cortex` intended for development is in the works and could be released in the next version (which means a few weeks give or take). I'm not part of the development team, so I might be wrong. ",far know local version cortex intended development work could next version give take part development team might wrong,issue,negative,negative,neutral,neutral,negative,negative
606491906,@deliahu Have you guys incorporated this feature in current release of cortex. I have a GPU server available locally and would like to use your platform for development to cut down the cost. ,incorporated feature current release cortex server available locally would like use platform development cut cost,issue,negative,positive,positive,positive,positive,positive
605458786,Over-heating GPUs sounds like a bad node. Better to work around this with a powerlimit on the GPU rather than trying to limit requests.,like bad node better work around rather trying limit,issue,negative,negative,neutral,neutral,negative,negative
605357887,"@ismaelc I had a chance to look at this, and it's great! Thank you for sharing!

I decided to keep only the ""new"" summarizer (since on GPU it only takes ~1 second). Also, I felt like it should replace our existing text summarizer example, so I moved it to that directory.",chance look great thank decided keep new summarizer since second also felt like replace text summarizer example directory,issue,positive,positive,positive,positive,positive,positive
605352555,I agree with this change. I had concerns about this cleanup making the dev commands take too much time. ,agree change cleanup making dev take much time,issue,negative,positive,positive,positive,positive,positive
605342129,"This is great, thanks for adding it! This definitely helps keep ECR clean, and beats what I have been doing (which is to delete all the repositories once every month or two and re-create them).

Since it takes some time to list the repositories, `make registry-all` / `make registry-dev` are run frequently in dev, and ECR storage isn't too expensive, I split this out into a separate make command (`make registry-clean`). Does that sound reasonable to you?",great thanks definitely keep clean delete every month two since time list make make run frequently dev storage expensive split separate make command make sound reasonable,issue,positive,positive,positive,positive,positive,positive
605313466,"Per our last conversation, I'm grabbing this ticket. Thanks @vishalbollu !

Useful articles:
* http://blog.shippable.com/setting-permissions-on-aws-ec2-ecr-repositories",per last conversation ticket thanks useful,issue,positive,positive,positive,positive,positive,positive
604772798,"@RobertLucian good call on the `text/plain` media type

This PR looks good to me, I will allow @vishalbollu to do a final pass, weigh in on the error message, and LGTM",good call medium type good allow final pas weigh error message,issue,positive,positive,positive,positive,positive,positive
604630007,"What has to be kept in mind in the back of the mind is that with this method, other protocols for pickle can be used too: 1, 2, 3, and 4 (with the exclusion of 5). 5 cannot be used because the serving container is still based on Python 3.6.x).

Anyhow, the speedup over protocol 0 (in my experiments) is this:

- Protocol 1: ~0.5x speedup.
- Protocol 2: ~0.5x speedup.
- Protocol 3: ~3x speedup.
- Protocol 4: ~3x speedup.
- Protocol 5: ~6x speedup - but is not accessible yet.

So assuming protocol 4, which is 3 times as fast as protocol 0, and no `json.dumps` for it, the serialization process for a ~27 MB `DataFrame` takes about 58 ms. That's a very good improvement over the 820 ms using protocol 0 & `dumps.json`. And even much better if it weren't for pickle (aka using `df.to_dict` and `json.dumps`): ~8000 ms.",kept mind back mind method pickle used exclusion used serving container still based python anyhow protocol protocol protocol protocol protocol protocol accessible yet assuming protocol time fast protocol serialization process good improvement protocol even much better pickle aka,issue,positive,positive,positive,positive,positive,positive
604600197,"@deliahu Hmm, I see the logic behind that - eliminating yet another step when it's about strings. I think that makes sense, although, I think it still has to be decoded on the receiving end. I'd give it a `text/plain` media type. I'm implementing this then.  ",see logic behind yet another step think sense although think still end give medium type,issue,negative,negative,negative,negative,negative,negative
604549650,"@RobertLucian also, I just pushed a few small changes to the docs and the error message, feel free to edit them back or further if you'd like",also small error message feel free edit back like,issue,negative,positive,neutral,neutral,positive,positive
604543828,"Yes, I definitely agree with using the `application/octet-stream` media type, thanks for pointing that out!

Our thinking regarding passing strings through is that the json dumping/loading is unnecessary, since it's already a string (you might as well just read it from the body directly). E.g. currently, if you return `""string""`, the body will be `'""string""'`, and you will need to `json.loads()` to remove the quotes, which didn't seem to be necessary in the first place. Also, as a user, if I return a string, it may be weird to me to get an encoded string on the other end. Although I'm not fully sold on all this, what do you think?",yes definitely agree medium type thanks pointing thinking regarding passing unnecessary since already string might well read body directly currently return string body string need remove seem necessary first place also user return string may weird get string end although fully sold think,issue,positive,negative,neutral,neutral,negative,negative
604530386,"@deliahu I just saw your comment here. So, let me iterate what #915 already has:

1. *""If the user returns an instance of [starlette.response](https://www.starlette.io/responses/#response), send that through unmodified""*
1. If the user returns a `bytes` object, send that through to the user, and set `media_type` to `application/octet-stream`.
1. *""Otherwise attempt to encode as JSON using the default json package, and set the `application/json` media type header""*

And here's an example of the error's message in case the JSON serialization fails:
```
cortex.lib.exceptions.UserRuntimeException: error: consider passing a bytes object or a custom starlette.response object instead: Object of type 'DataFrame' is not JSON serializable: runtime exception
```

Out of all these 4 things (the first 3 points and the error message), the only difference is in that `string` objects are not sent to the user directly and that the `media_type` is set for `bytes` objects. 

---

Here are my thoughts:

1. Returning a string as-is with #915 ""works"" because `json.dumps` is there. On the other end, the user has to `json.loads(response.json())`. Do you think returning a string without passing it through `json.dumps` is necessary? I can do it if asked for. The thinking goes like ""well, I still get a string if I `json.loads(json.dumps(""generic string""))`"".

1. It might give more clarity to the user on the other end if the media type is set (for `bytes` [and `string`]), I'm thinking. I don't have a strong opinion here. 

What do you think?",saw comment let iterate already user instance send unmodified user object send user set otherwise attempt encode default package set medium type header example error message case serialization error consider passing object custom object instead object type exception first error message difference string sent user directly set string work end user think string without passing necessary thinking go like well still get string generic string might give clarity user end medium type set string thinking strong opinion think,issue,negative,positive,positive,positive,positive,positive
604507165,"@RobertLucian I spoke with Vishal, and here's what we're thinking:

1. If the user returns an instance of [`starlette.response`](https://www.starlette.io/responses/#response), send that through unmodified
1. If the user returns a `string` or `bytes`, send that through to the user, and do not set the `media_type` header
1. Otherwise attempt to encode as JSON using the default `json` package, and set the ""application/json"" media type header

The error message if encoding fails should list the user's three options.

We decided to close https://github.com/cortexlabs/cortex/issues/910 for now, since using the built-in json makes it easier for users to understand the behavior, and search for error messages.

Does this behavior make sense to you?",spoke thinking user instance send unmodified user string send user set header otherwise attempt encode default package set medium type header error message list user three decided close since easier understand behavior search error behavior make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
604505703,Closing since using the built-in `json` makes it easier for users to search for and understand the behavior,since easier search understand behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
604169372,"Absolutely! @vishalbollu and I will discuss tomorrow whether we want to also pass strings through or continue encoding them as json, so we will update you on our thoughts there. But passing the bytes through definitely makes sense since right now that isn't even possible.",absolutely discus tomorrow whether want also pas continue update passing definitely sense since right even possible,issue,negative,positive,positive,positive,positive,positive
603618134,"@ismaelc thank you for your contribution, this is an awesome example!

@vishalbollu and I will review it soon; we're working on getting the next release out (hopefully tomorrow), and we'll take a look at this more carefully after",thank contribution awesome example review soon working getting next release hopefully tomorrow take look carefully,issue,positive,positive,positive,positive,positive,positive
602277679,"@RobertLucian Go for it! We can schedule a call to decide on a potential interface when you get to that stage.

Some options:
```bash
cortex configure --env # -e
cortex configure --list # -l
cortex configure --remove # -rm

cortex configure env
cortex configure list
cortex configure remove
```

I moved one of the requirements (remove associated CLI environments after a `cortex cluster down`) to #894  because it isn't related to the core of this issue.",go schedule call decide potential interface get stage bash cortex configure cortex configure list cortex configure remove cortex configure cortex configure list cortex configure remove one remove associated cortex cluster related core issue,issue,negative,neutral,neutral,neutral,neutral,neutral
602249397,@vishalbollu I'd like to work on this issue. What do you think? ,like work issue think,issue,negative,neutral,neutral,neutral,neutral,neutral
601457635,@vishalbollu totally. Take as much time as necessary to sort out things for `0.15.`.,totally take much time necessary sort,issue,negative,positive,neutral,neutral,positive,positive
601243453,"@RobertLucian Thanks for the PR! The features improvements addressed in this PR will make customizing Cortex runtime a lot easier and minimize the need for users to create custom Docker images.

We are currently focused on bug fixes and messaging improvements this week. Do you mind if we place the PR on the backburner for the next few days while we focus on this release?",thanks make cortex lot easier minimize need create custom docker currently bug week mind place next day focus release,issue,positive,positive,neutral,neutral,positive,positive
600691584,@KalivarapuReshma Thank you for your suggestion! I have updated the PR to include the AWS CLI version.,thank suggestion include version,issue,negative,neutral,neutral,neutral,neutral,neutral
600394989,"@RobertLucian @deliahu 
Hello, Recently I was exploring this project and wanted to play with it, so started installing them. One pain point I observed is there are two versions of awscli available. The `make registry-all` [step](https://github.com/cortexlabs/cortex/blob/master/dev/registry.sh#L27-L35) uses `aws ecr get-login` which is deprecated in V2 of aws-cli. So it started failing at this point, had to uninstall v2 and install V1. It will be helpful if we can call out in the doc that V1 needs to be installed.

Thanks @RobertLucian for updating the doc, all other pain points are covered. Now someone new like me can also set it up seamlessly.
",hello recently exploring project play one pain point two available make step failing point install helpful call doc need thanks doc pain covered someone new like also set seamlessly,issue,negative,positive,positive,positive,positive,positive
599578057,"As David has mentioned, `rerun` is a go package and it doesn't coincide with what I had in mind. So the flag discrepancies are explained by this. I rolled it back to use the go's version of this package.",rerun go package coincide mind flag rolled back use go version package,issue,negative,neutral,neutral,neutral,neutral,neutral
598674863,"@deliahu 
So, I had to revert to only 1 worker/instance for the CRNN API (https://github.com/cortexlabs/cortex/pull/845). 

Apparently, the CRNN API models need cumulatively about 8119 MiB. The T4 GPU only has 15079 MiB. This means it's not possible to fit 2 workers on a single T4 GPU. And there's no GPU memory leak nor there is an issue with the GPU growth setting - we're okay on that regard.

I looked into ways of reducing the memory need of the models within Keras to be able to fit in 2 workers, and without a significant change to the used models (for instance, inside [faustomorales/keras-ocr](https://github.com/faustomorales/keras-ocr)'s source code), there isn't an easy way out of this.

---

> Also, I am still seeing GPU out of memory issues when I tried running it. Now the API did become ready without crashing, but when I tried to hit the API with concurrent requests, it seemed to crash due to GPU OOM.

I looked into this and I found out the memory requirements of loading a model are lower than those of loading a model and running predictions. This explains the above situation.

> I updated the code a little, and moved the documentation you added to gpus.md (which we just added today). Please let me know what you think.

Yes, I like this. Specifically the headline. It's succinct. 

",revert apparently need cumulatively mib mib possible fit single memory leak issue growth setting regard way reducing memory need within able fit without significant change used instance inside source code easy way also still seeing memory tried running become ready without tried hit concurrent crash due found memory loading model lower loading model running situation code little documentation added added today please let know think yes like specifically headline succinct,issue,positive,positive,positive,positive,positive,positive
598507955,"@RobertLucian thank you for looking into this!

I updated the code a little, and moved the documentation you added to `gpus.md` (which we just added today). Please let me know what you think.

Also, I am still seeing GPU out of memory issues when I tried running it. Now the API did become ready without crashing, but when I tried to hit the API with concurrent requests, it seemed to crash due to GPU OOM.

I changed from `tf.config.experimental.list_physical_devices` to `tf.config.list_physical_devices`, although I don't think that is the issue. I ran concurrent requests by running `sample_inference.py` in three terminals as close together as I could (I also commented out the `yolov3` API request and instead loaded `boxes_raw` from a pickled file that I saved).

Perhaps the model, once loaded into the GPU, is too big? Or perhaps limiting the GPU growth isn't working for some reason? Or perhaps there is a GPU memory leak somehow?

Here is the error I saw:

```text
2020-03-13 01:30:24.565193: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-03-13 01:30:26.381819: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.382882: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 921.60M (966367744 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.383895: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 829.44M (869731072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.384796: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 746.50M (782758144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.385776: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 671.85M (704482304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.385815: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 662.60MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.386715: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.386738: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 662.60MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.386967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-03-13 01:30:26.759981: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.760021: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.40MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.760857: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.760887: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.40MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.761677: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.761697: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 168.26MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.762491: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.762513: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 168.26MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.763358: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.763385: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.39MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.764299: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.764321: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 16.39MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.765089: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.765111: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 34.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.765944: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 1.00G (1073741824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.765967: W tensorflow/core/common_runtime/bfc_allocator.cc:243] Allocator (GPU_0_bfc) ran out of memory trying to allocate 34.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2020-03-13 01:30:26.766829: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.767941: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.803854: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.804850: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.819843: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.820687: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.826803: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.827651: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.828450: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.829320: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.849865: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.850845: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.851695: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.852582: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.880788: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.881711: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.882618: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.883480: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.906586: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.907449: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.908214: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.909076: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.940342: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.941290: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.942502: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.943345: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 2.00G (2147483648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.955865: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.956705: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.959952: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.960889: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.961669: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.962642: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.977870: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.978717: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.979915: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.980725: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.981720: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.982729: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.983500: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.984214: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.992417: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:26.993443: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.001266: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.002539: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.011531: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.012398: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.015864: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.016839: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.017998: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.019095: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.035904: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.036739: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.039700: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.040621: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.041605: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.042594: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.058893: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.059838: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.068041: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.068988: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.076160: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.076993: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.085423: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.086386: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.094381: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.095314: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.102756: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.103944: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.110515: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.111342: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.118190: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.119024: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.125368: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.126410: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.132345: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:27.133607: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.269306: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.270174: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.275340: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.276114: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.294826: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.295732: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.301980: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.303037: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.304003: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.304907: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.306188: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.307439: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.308545: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.309411: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.310587: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.311738: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.329960: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.331054: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.331924: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.332697: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.344471: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.345405: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.350423: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.351409: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.352685: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.353698: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.372790: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.374071: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.374988: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.375985: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.390418: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-03-13 01:30:28.391255: I tensorflow/stream_executor/cuda/cuda_driver.cc:801] failed to allocate 4.00G (4294967296 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
```",thank looking code little documentation added added today please let know think also still seeing memory tried running become ready without tried hit concurrent crash due although think issue ran concurrent running three close together could also request instead loaded file saved perhaps model loaded big perhaps limiting growth working reason perhaps memory leak somehow error saw text successfully dynamic library allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available successfully dynamic library allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocator ran memory trying allocate caller failure may mean could performance gain memory available allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory allocate device memory,issue,positive,negative,neutral,neutral,negative,negative
597087073,"Thank you @deliahu.

Yes, for now, I'd could use these tags to identify cost and add them manually to the rest of the resources used in a project.

I close this question and I'll follow the issue opened.",thank yes could use identify cost add manually rest used project close question follow issue,issue,positive,neutral,neutral,neutral,neutral,neutral
596881018,@deliahu I'm thinking the users will most likely look at the examples directory in cortex and see how https://github.com/cortexlabs/cortex/pull/834 loads the model with Keras or they'll see this ticket. I think it's okay to close this issue. ,thinking likely look directory cortex see model see ticket think close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
596831354,"@adrianicv yes, your use case makes sense, I've created issue https://github.com/cortexlabs/cortex/issues/856 to track this.

In the mean time, there may be a few existing tags you could use. The tag `alpha.eksctl.io/cluster-name` is set to the cluster name on all of the instances in the cluster. In addition, there is a tag for each cluster called `kubernetes.io/cluster/<cluster_name>` which has value `owned ` and is set on the load balancers in addition to the instances, so that should pick up more of the total cost of the cluster.

The main factor leading to cost differences between clusters is the cost of the instances, which should be captured with either of these tags.

Do these tags help provide a picture of your cluster cluster costs?",yes use case sense issue track mean time may could use tag set cluster name cluster addition tag cluster value set load addition pick total cost cluster main factor leading cost cost either help provide picture cluster cluster,issue,positive,negative,neutral,neutral,negative,negative
596822175,"@RobertLucian thank you for checking. Should we close this issue since `from tensorflow.keras.models import load_model` is working, or is the a change that you would suggest to be implemented in Cortex?",thank close issue since import working change would suggest cortex,issue,negative,neutral,neutral,neutral,neutral,neutral
596511889,"@vishalbollu In order to assign the clusters associated costs to different projects and use the AWS billing dashboard to explore the costs by project.

For example, if I have a project, called A, where I'm using a Cortex cluster and an API using API Gateway and Lambdas, and another project, called B, with a Cortex cluster and Cloudfront with a frontend interface hosted in S3. I want to be able to see the costs I have in project A and in project B independently. To accomplish this, I have a tag called Project and I assign the value A or B depending on the resource.
",order assign associated different use billing dashboard explore project example project cortex cluster gateway another project cortex cluster interface want able see project project independently accomplish tag project assign value depending resource,issue,positive,positive,positive,positive,positive,positive
596495632,"@adrianicv Currently, there isn't a way to tag all of AWS resources created by Cortex. Why do you want to tag resources?",currently way tag cortex want tag,issue,negative,neutral,neutral,neutral,neutral,neutral
595927094,Thanks guys! I will fork now and also look into the options that you guys suggested.,thanks fork also look,issue,negative,positive,positive,positive,positive,positive
595920566,"I think the most straightforward example would be to run the ""Building the Model"" section of the [demo of TensorFlow for R](https://tensorflow.rstudio.com/deploy/docker/), then instead of serving it with TF-serving via R via docker, as in the demo, follow [the cortex example](https://www.cortex.dev/packaging-models/tensorflow) for uploading a saved TF model to S3 and using it in a predictor.

Just a thought",think straightforward example would run building model section instead serving via via docker follow cortex example saved model predictor thought,issue,negative,positive,positive,positive,positive,positive
595917401,"There are no blockers at the moment and no one has started work on it yet. Go for it! As the ticket describes, a good place to start is to have an R script that trains a small model such as an iris-classifier. From there, it may be a good idea to see if it is possible to export an R model to a model format that can be run in Python. I am not sure if these are relevant but maybe you can export an R model to one of these formats: TensorFlow saved model, ONNX or PMML. Once you get to the model formats, it should be possible to run them on Cortex fairly easily. Alternatively, is it possible to run R in Python? I wonder what the performance impacts are.

Let us know how it goes.",moment one work yet go ticket good place start script small model may good idea see possible export model model format run python sure relevant maybe export model one saved model get model possible run cortex fairly easily alternatively possible run python wonder performance let u know go,issue,positive,positive,positive,positive,positive,positive
595904010,I would like to start working on this. Please let me know if there are any blockers.,would like start working please let know,issue,positive,neutral,neutral,neutral,neutral,neutral
595593178,@deliahu just tried this on `0.14` and it still doesn't work. Same error. Using `from tensorflow.keras.models import load_model` still seems to be the best alternative.,tried still work error import still best alternative,issue,negative,positive,positive,positive,positive,positive
595581022,@RobertLucian I just created https://github.com/cortexlabs/cortex/issues/852 to track our research into a conda environment separately from this (which can be use primarily for e.g. `apt-get install`),track research environment separately use primarily install,issue,negative,positive,positive,positive,positive,positive
595501801,"@deliahu yeah, I'm adding this to my list of things to do. I will be back with an answer.",yeah list back answer,issue,negative,neutral,neutral,neutral,neutral,neutral
595485320,"> Would this support running in a conda environment?

If conda/pip packages are to be installed in this script, then I think this should run by-default in a conda environment. My 2 cents.",would support running environment script think run environment,issue,negative,neutral,neutral,neutral,neutral,neutral
595452149,"@RobertLucian I wonder if this has been resolved now that 0.14 is released, is it easy for you to check this?",wonder resolved easy check,issue,positive,positive,positive,positive,positive,positive
593659444,Consider having a process in request-monitor.go to delete request files older than the request timeout configuration to ensure that any mistakes in deleting request files in API pod are cleaned up.,consider process delete request older request configuration ensure request pod,issue,negative,positive,positive,positive,positive,positive
593236737,"This looks really awesome, thank you for contributing this!

I did a quick pass, and everything looks good on first glance. I will do a more thorough review in the next day or two, and I'll upload the the model to the cortex examples bucket and try running the API. I'll keep you posted on how it goes.",really awesome thank quick pas everything good first glance thorough review next day two model cortex bucket try running keep posted go,issue,positive,positive,positive,positive,positive,positive
592912548,"Thanks for bringing this to our attention! We will be switching from Flask to FastAPI soon (hopefully merging into master in the next few days, and releasing shortly thereafter). Let‚Äôs see what happens when that gets in, hopefully the issue will just go away",thanks attention switching flask soon hopefully master next day shortly thereafter let see hopefully issue go away,issue,positive,positive,neutral,neutral,positive,positive
592214376,"Yes I agree. The generator I wrote Is general enough to be used with most cobra-based cli, so in theory I can also create a similar API for `istioctl`. I'll have a look; for now I think you can close this PR and I will write a new one once I will update my fork it with istio. Thanks!",yes agree generator wrote general enough used theory also create similar look think close write new one update fork thanks,issue,positive,positive,neutral,neutral,positive,positive
592180594,"@fbertola This is really cool, we always hypothesized this would be possible, but haven't had the chance to try!

As you mentioned, this is an essential component for eliminating the docker container that the cortex CLI uses to manage the cluster, and building it all directly into the CLI. This would be great because then users wouldn't need docker to run the CLI.

There is one component of the CLI docker container which probably needs to be updated before we can move everything out: istio. Right now, we use helm to install istio. This is no longer the recommended way (there is `istioctl`), and there is a chance we will remove istio soon, but we aren't sure yet.

Until we are able to move everything into the CLI, I'm not sure if it would make sense to merge this into master, since for testing purposes we try to use the `eksctl` CLI like we do in the docker container. So it might make sense to merge it into a different branch on Cortex, or keep it in your repo for now until we can remove the CLI's docker container. What are your thoughts?",really cool always would possible chance try essential component docker container cortex manage cluster building directly would great would need docker run one component docker container probably need move everything right use helm install longer way chance remove soon sure yet able move everything sure would make sense merge master since testing try use like docker container might make sense merge different branch cortex keep remove docker container,issue,positive,positive,positive,positive,positive,positive
592069940,"@chrisranderson Yes, your use case makes sense since it's just to try out Cortex :)

Until we are able to add support for this, one potential work around for you to test out Cortex would be to ask your organization for a separate AWS account where you can have long-term credentials. Alternatively, you could use a personal account; as long as you spin down your cluster when you're done trying it out it won't be too expensive. Another option would be to ask for a set of long-term credentials in your organization, with the assumption that they can manually revoke them after 10 hours.

Do you think one of these options would work for you?

In the meantime, we will definitely improve that error message for the next release, thanks for bringing it to our attention!",yes use case sense since try cortex able add support one potential work around test cortex would ask organization separate account alternatively could use personal account long spin cluster done trying wo expensive another option would ask set organization assumption manually revoke think one would work definitely improve error message next release thanks attention,issue,positive,positive,neutral,neutral,positive,positive
592042849,"I'm brand new to Cortex - I just tried installing, couldn't, and found this issue. I don't know what the operator is, or know hardly anything about Cortex :) Maybe you could mention this issue in the error message if it will be a while before it is fixed.

My organization set the token expiration to 10 hours. My goal is to try Cortex out on a very short-term scale. If I have a good experience with it, I can probably request a new user / IAM role or whatever and avoid this problem entirely. For me, it doesn't matter if the cluster breaks at the 10 hour mark, as long as I can keep fiddling the next morning.",brand new cortex tried could found issue know operator know hardly anything cortex maybe could mention issue error message fixed organization set token expiration goal try cortex scale good experience probably request new user role whatever avoid problem entirely matter cluster hour mark long keep fiddling next morning,issue,negative,positive,neutral,neutral,positive,positive
591738002,"@chrisranderson there is not currently a temporary way around this that I am aware of. Updating the CLI to support session tokens sounds like it would be straightforward, however the Cortex operator requires continued access to AWS, so it would have to implement logic to refresh session tokens when they expire. My understanding is the session tokens expire relatively quickly (based on a quick search 36 hours seems to be the max), and that the long-term credentials are required to refresh the token. If that's all true, it doesn't sound feasible to rely on session tokens in the operator.

That said, if it would be valuable to support session tokens only in the CLI (and not the operator), that is definitely something we can do. You would need to have access to the long-term credentials during cluster spin up, and thereafter the session token could be used to run all of the CLI commands. Would this address your concern?",currently temporary way around aware support session like would straightforward however cortex operator continued access would implement logic refresh session expire understanding session expire relatively quickly based quick search refresh token true sound feasible rely session operator said would valuable support session operator definitely something would need access cluster spin thereafter session token could used run would address concern,issue,positive,positive,positive,positive,positive,positive
591696747,"+1. Is there a way to get around this in the meantime, that doesn't involve creating a new user or changing permissions?",way get around involve new user,issue,negative,positive,positive,positive,positive,positive
590514702,"Yes, using a GPU will help with throughput, assuming your model is configured properly to utilize it (some of the TensorFlow APIs provide GPU support out of the box, like the [pre-made estimators](https://www.tensorflow.org/guide/estimator#pre-made_estimators)).

If you are referring to TensorFlow Serving's [server-side batching feature](https://www.tensorflow.org/tfx/serving/serving_advanced#batching) (code [here](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/batching)), we have https://github.com/cortexlabs/cortex/issues/152 to add support for this. Still, even without this feature, as long as you run the Python container with enough threads (i.e. `waitress_threads` from above), the Python container will not be the limiting factor, since multiple in-flight requests can be sent through to the GPU concurrently.",yes help throughput assuming model properly utilize provide support box like serving feature code add support still even without feature long run python container enough python container limiting factor since multiple sent concurrently,issue,positive,negative,neutral,neutral,negative,negative
590180451,"@timothywangdev this is not actively being worked on, although it is on our roadmap. Do you mind sharing what is your motivation for this? E.g. is it to save time and/or money during development, or to serve a small-scale API from one machine in production, or something else?",actively worked although mind motivation save time money development serve one machine production something else,issue,positive,negative,negative,negative,negative,negative
590166049,is this till being worked on? It's going to be quite helpful for local testings,till worked going quite helpful local,issue,negative,neutral,neutral,neutral,neutral,neutral
590153971,"hi, thanks for the detailed reply. motivation is mostly optimize for throughput but keep a reasonable latency. I guess gpu will also help to optimize for throughput?

how is it when I use batching in TensorFlow Serving - will the python docker container not limit the batching - and be the limiting factor to achieve high throughput?",hi thanks detailed reply motivation mostly optimize throughput keep reasonable latency guess also help optimize throughput use serving python docker container limit limiting factor achieve high throughput,issue,positive,positive,positive,positive,positive,positive
588386231,"Hey Sam, I'd be happy to chat. I just emailed the address on your profile.",hey sam happy chat address profile,issue,positive,positive,positive,positive,positive,positive
588308424,"I too would love to have offline installation capability for the framework and I am willing to implement the features and make pull requests to get it. 

**Background**: I built a machine learning framework for video processing centered around Tensorflow Serving for a private application with my company.  The solution as it stands has rather limited support for other frameworks such as PyTorch and scikit-learn. 

**Reasoning**: Having the ability to install Cortex on bare metal servers would increase the adoption rate significantly in certain communities in my opinion.  Most of the clients utilizing the application have more than enough funding to self-support hosting but are extremely evasive when it comes to utilizing AWS because of what they see as long term cost.  

For instance, they have already purchased multiple server racks with Nvidia GPUs which are currently utilizing Nvidia GRID / vGPU with the application I made.  They will not want to backtrack and waste all the money they used for purchasing those server racks/GPUs by transitioning to a cloud-only solution.  

If this is a feature you all would like to have available for Cortex please get in touch with me, we can have a conversation somewhere beyond this ticket if it is convenient. Otherwise, just point me where I am needed and I will get started this weekend. 

Looking forward to hearing from you. 
-Sam 

",would love installation capability framework willing implement make pull get background built machine learning framework video centered around serving private application company solution rather limited support reasoning ability install cortex bare metal would increase adoption rate significantly certain opinion application enough hosting extremely evasive come see long term cost instance already multiple server currently grid application made want backtrack waste money used server solution feature would like available cortex please get touch conversation somewhere beyond ticket convenient otherwise point get weekend looking forward hearing,issue,positive,positive,neutral,neutral,positive,positive
587339370,"@deliahu thank you very much for your detailed response. This answers my question and brings some additional details to my consideration that I was not aware of before.

I fully understand your design choices.The initial motivation for my question was to minimize excess capacity.",thank much detailed response question additional consideration aware fully understand design initial motivation question minimize excess capacity,issue,positive,positive,positive,positive,positive,positive
587257627,"@Arnold1 your diagram is correct :) Also, yes, a replica is the scalable unit, and a replica contains both the Python and TensorFlow Serving container, so both containers scale up together. This approach makes autoscaling a bit simpler and easier to configure (we're hoping to release v0.14 soon, which updates our autoscaling to be request-based rather than CPU-based). In addition, keeping both containers on the same pod ensures that the request from the Python container to the TensorFlow Serving container always has low latency.

If you are concerned about the TensorFlow Serving container being idle while the request is in the pre/post processing phase, once we release v0.14 (which we hope to do this week), you will be able to control the on-replica parallelism via the `workers_per_replica` and `threads_per_worker` configuration (each worker has it's own process, and each has `threads_per_worker` threads). I'll be writing up an explanation of these configuration parameters soon for the v0.14 release, but in the mean time you can see a short description of them [here](https://www.cortex.dev/v/master/deployments/tensorflow).

If you would like to control the parallelism now (i.e. on v0.13), you can do so like this:

```yaml
- name: iris-classifier
  predictor:
    type: tensorflow
    path: predictor.py
    model: s3://cortex-examples/tensorflow/iris-classifier/nn
    config:
      waitress_threads: 4
```

I just did a quick check on the latency between the Python container and the TensorFlow Serving container. TensorFlow Serving doesn't seem to have a health check API (https://github.com/tensorflow/serving/issues/671), so I called an actual API (`GetModelStatus()`), and the total round trip time was ~0.2 milliseconds. I am not sure how much of this time was spent in the TensorFlow Serving container versus in-transit.

The max queries per second will depend on how long a single inference takes and how parallelizable it is (e.g. if it is not CPU/GPU bound, running multiple threads and/or workers will increase the throughput). How long the inference takes depends on the the pre/post processing, the model itself, and the resources allocated to the replica (e.g. if it runs on a GPU or CPU, what type of GPU, etc). For GPU workloads, we have seen the best performance per dollar with the `g4dn.xlarge` instances (using spot instance saves a lot of money too of course).

We currently do not have plans to re-write the Python container in c++/go. Is your motivation for this mostly about reducing latency?",diagram correct also yes replica scalable unit replica python serving container scale together approach bit simpler easier configure release soon rather addition keeping pod request python container serving container always low latency concerned serving container idle request phase release hope week able control parallelism via configuration worker process writing explanation configuration soon release mean time see short description would like control parallelism like name predictor type path model quick check latency python container serving container serving seem health check actual total round trip time sure much time spent serving container versus per second depend long single inference bound running multiple increase throughput long inference model replica type seen best performance per dollar spot instance lot money course currently python container motivation mostly reducing latency,issue,positive,positive,positive,positive,positive,positive
587214297,"@deliahu yeah understood. so that python container is some sort of proxy right? the tf serving and python docker come as a pair - scaling up to handle more traffic requires both to scale up!?
```
                                                (model from s3)
client -> python container (preprocessing) -> tf serving container -> python container (post processing) -> client
```

did you measure about how much overhead it has forwarding to tf serving? what's the max ups (queries per second) you have tested this?

are there any plans to re-write the python container in c++/go?",yeah understood python container sort proxy right serving python docker come pair scaling handle traffic scale model client python container serving container python container post client measure much overhead forwarding serving per second tested python container,issue,negative,positive,positive,positive,positive,positive
587148739,"@sebastianschramm currently we do not support deploying to an existing Kubernetes cluster, however technically speaking I believe it could be supported somewhat easily. That said, there are some advantages with running a separate cluster for Cortex, which is why we decided to design it that way initially:

* Cortex requires a number of daemonsets to be running on the nodes, which could be intrusive for non-Cortex workloads (e.g. the Nvidia device plugin for GPUs, fluentd and cloudwatch for logging, the Istio CNI for networking, etc).
* Cortex supports fine grain cluster management, e.g. spot instances, instance types/distributions, etc (here are the docs for [cluster configuration](https://www.cortex.dev/cluster-management/config) and [spot instances](https://www.cortex.dev/cluster-management/spot-instances)). This should make it easy to configure the cluster tailored to the specific use case.
* Having a separate cluster helps with isolation, which can make some things more straightforward, like securing the cluster or tracking spend. It also helps ensure that other workloads don't interfere with Cortex, and vice versa.
* Since Cortex supports pod and node autoscaling, there shouldn't be much excess capacity on the cluster, so there shouldn't be too much overhead (I agree that the $0.10 per hour for EKS is a bit annoying, however depending on the size of your workload and you instance types this cost could be relatively small).

What is your motivation to install Cortex on an existing cluster?",currently support cluster however technically speaking believe could somewhat easily said running separate cluster cortex decided design way initially cortex number running could intrusive device logging cortex fine grain cluster management spot instance cluster configuration spot make easy configure cluster specific use case separate cluster isolation make straightforward like cluster spend also ensure interfere cortex vice since cortex pod node much excess capacity cluster much overhead agree per hour bit annoying however depending size instance cost could relatively small motivation install cortex cluster,issue,positive,positive,neutral,neutral,positive,positive
587141330,"@Arnold1 it is currently not possible to bypass the python container, however the actual model inference is performed on the official TensorFlow serving docker container (here is the Cortex `Dockerfile` for the serving container for [CPUs](https://github.com/cortexlabs/cortex/blob/master/images/tf-serve/Dockerfile) and for [GPUs](https://github.com/cortexlabs/cortex/blob/master/images/tf-serve-gpu/Dockerfile)). In the code above, `self.client.predict(payload)` sends the request to the TensorFlow serving container running on the same Kubernetes pod. Since the Python and TensorFlow serving containers are on the same Kubernetes pod, the latency of the network request between the two is very small (they are always guaranteed to be on the same instance).

The Python container is just a wrapper, so that pre- and post-inference processing can be handled easily. If you don't require any request processing, you can have a simple implementation for `predictor.py` like the iris one above (even simpler since that one has a little bit of post-processing).

Does that make sense? Is there a certain use case you are trying to implement which doesn't work well with this design?",currently possible bypass python container however actual model inference official serving docker container cortex serving container code request serving container running pod since python serving pod latency network request two small always instance python container wrapper handled easily require request simple implementation like iris one even simpler since one little bit make sense certain use case trying implement work well design,issue,positive,positive,neutral,neutral,positive,positive
587137124,@deliahu ok. how can i avoid the use of python is my question? I dont want to use a container with python.,avoid use python question dont want use container python,issue,negative,neutral,neutral,neutral,neutral,neutral
587078800,"@Arnold1 When using the [TensorFlow runtime](https://www.cortex.dev/deployments/tensorflow), you can specify the s3 path to your exported TensorFlow model (in the `model` key within the `predictor` configuration). Here is an example:

```yaml
- name: iris-classifier
  predictor:
    type: tensorflow
    path: predictor.py
    model: s3://cortex-examples/tensorflow/iris-classifier/nn
```

At runtime, there will be two containers (per replica): one is a TensorFlow serving container which downloads the model from the path you specified and serves it, the other is a Python container which receives the prediction request, does any necessary transformations, and sends the transformed sample to the TensorFlow Serving container by calling `tensorflow_client.predict()`. Here is an example:

```python
labels = [""setosa"", ""versicolor"", ""virginica""]

class TensorFlowPredictor:
    def __init__(self, tensorflow_client, config):
        self.client = tensorflow_client

    def predict(self, payload):
        prediction = self.client.predict(payload)
        predicted_class_id = int(prediction[""class_ids""][0])
        return labels[predicted_class_id]
```

The full example can be seen [here](https://github.com/cortexlabs/cortex/tree/0.13/examples/tensorflow/iris-classifier).

Does this address your question?",specify path model model key within predictor configuration example name predictor type path model two per replica one serving container model path python container prediction request necessary sample serving container calling example python versicolor class self predict self prediction prediction return full example seen address question,issue,negative,positive,positive,positive,positive,positive
586844690,@vishalbollu thanks for quick reply. it looks like I have to use python for tensorflow serving. is there a way to use the docker container instead - and a way to load the model? if not when will it be available?,thanks quick reply like use python serving way use docker container instead way load model available,issue,positive,positive,positive,positive,positive,positive
586525431,"As agreed, the client has been moved to a separate repository here:
https://github.com/RobertLucian/cortex-license-plate-reader-client

For verifying that the deployed models work as expected on AWS w/ cortex, this half of the project has gotten a `sample_inference.py` script to which a URL link of an image is passed on and a prediction is outputted in the form of a JPEG image. More on that in the README.

This verification is useful before going heavy with the client app from the other repo (check the above link).  ",agreed client separate repository work cortex half project gotten script link image prediction form image verification useful going heavy client check link,issue,positive,negative,neutral,neutral,negative,negative
586399581,"@Arnold1 Given a TensorFlow model, Cortex will serve your model using TensorFlow serving and make it available as a web service. The documentation for how to deploy TensorFlow models with Cortex can be found [here](https://www.cortex.dev/deployments/tensorflow). You can configure Cortex to run on your desired instance type including spot instances by specifying your own [cluster configuration](https://www.cortex.dev/cluster-management/config). Cortex will automatically spin up more replicas to meet the needs of traffic to your web service. Cortex uses Kubernetes (AWS EKS) under the hood for workload orchestration across multiple ec2 instances and it can scale up the number instances in your cluster when necessary.

Feel free to provide more information so that we can explore your use case further.",given model cortex serve model serving make available web service documentation deploy cortex found configure cortex run desired instance type spot cluster configuration cortex automatically spin meet need traffic web service cortex hood orchestration across multiple scale number cluster necessary feel free provide information explore use case,issue,positive,positive,positive,positive,positive,positive
586090830,"Thanks for letting us know. It makes sense to lean towards existing infra but still be on the lookout for other tools that may increase productivity.

Support for GCP is being tracked in https://github.com/cortexlabs/cortex/issues/114 and Azure in https://github.com/cortexlabs/cortex/issues/811.

Let us know if you have any questions, feel free to close this ticket otherwise.",thanks u know sense lean towards infra still lookout may increase productivity support tracked azure let u know feel free close ticket otherwise,issue,positive,positive,positive,positive,positive,positive
585779502,"Sure. We are mainly based on Microsoft Azure, but starting to explore options at AWS and GCP! I would rather keep our services for now in one place, our primary provider Azure, running the API, containers etc. to have easier control over our resources, others access to GCP and AWS is there too. 

I will have a look at the detection example, thanks!",sure mainly based azure starting explore would rather keep one place primary provider azure running easier control access look detection example thanks,issue,positive,positive,positive,positive,positive,positive
585468640,"@bcjordan I've made a PR to address this: https://github.com/cortexlabs/cortex/pull/807

Is this what you had in mind? Let us know if you'd like us to change or add something!

(feel free to check just the .md files and ignore the diff in the go file, I just rearranged the cost breakdown table a bit)",made address mind let u know like u change add something feel free check ignore go file cost breakdown table bit,issue,positive,positive,positive,positive,positive,positive
585350488,"@bcjordan yes I think this is a great idea, we'll work on adding this today",yes think great idea work today,issue,positive,positive,positive,positive,positive,positive
585301227,"If you don't mind me asking, which cloud provider(s) are you using for your model serving? As we set our roadmap, it's helpful for us to know which clouds our users prefer and why.

If you are interested in object detection, you may want to check out https://github.com/cortexlabs/cortex/pull/803 showcasing a real-time video-based license plate detection based on models like yolov3 and how to deploy with Cortex.",mind cloud provider model serving set helpful u know prefer interested object detection may want check license plate detection based like deploy cortex,issue,positive,positive,positive,positive,positive,positive
585039439,"@vishalbollu Thanks for the quick reply. Nothing special, simple functionalities like you do provide for AWS for deployment would be a great start, things like launching/terminating instances, Autoscaling, Remote debugging, Docker, Kubernetes, packages you have a great list I am currently focused on a containerized TF Object Detection model. ",thanks quick reply nothing special simple like provide deployment would great start like remote docker great list currently object detection model,issue,positive,positive,positive,positive,positive,positive
584949889,"@RobertLucian this is really awesome! I'll go through it now and let you know if I have any feedback, but on first glance it looks great.",really awesome go let know feedback first glance great,issue,positive,positive,positive,positive,positive,positive
584763981,"Great! I'll send it to @vishalbollu for review, and then we'll merge it in",great send review merge,issue,positive,positive,positive,positive,positive,positive
584661065,"@mmortazavi Adding support for other cloud providers is on the roadmap with GCP being our next priority. Are there particular features from other cloud providers that you would like to see Cortex support when we add support for other cloud providers?

I'd be happy to jump on a call to discuss if you'd like, feel free to email me at vishal@cortexlabs.com.",support cloud next priority particular cloud would like see cortex support add support cloud happy jump call discus like feel free,issue,positive,positive,positive,positive,positive,positive
584475469,"> Thanks for addressing the comments!
> 
> For 1, I realized the root of the issue was that were were matching against the relative paths, but when `ListDirRecursive()` was called with `relative` set to `false` (as it is in `cli/cmd/deploy.go`), then it was not working as expected.
> 
> For 2, I did see this logic, however it seems like this is just for parsing the patterns, but the `!` was not being interpreted by the matcher.
> 
> It seemed like even according to [Docker's documentation](https://docs.docker.com/engine/reference/builder/#dockerignore-file), they had to do some workarounds on top of Go's `filepath.Match()`. So I searched for a replacement to the matcher and found this one: https://godoc.org/github.com/denormal/go-gitignore. I switch to it in to your PR, and also tried using the `files.IgnoreFn()` pattern, what do you think of it?

I like this approach. It looks more cleaner!",thanks root issue matching relative relative set false working see logic however like matcher like even according docker documentation top go replacement matcher found one switch also tried pattern think like approach cleaner,issue,positive,positive,neutral,neutral,positive,positive
584371440,"Thanks for addressing the comments!

For 1, I realized the root of the issue was that were were matching against the relative paths, but when `ListDirRecursive()` was called with `relative` set to `false` (as it is in `cli/cmd/deploy.go`), then it was not working as expected.

For 2, I did see this logic, however it seems like this is just for parsing the patterns, but the `!` was not being interpreted by the matcher.

It seemed like even according to [Docker's documentation](https://docs.docker.com/engine/reference/builder/#dockerignore-file), they had to do some workarounds on top of Go's `filepath.Match()`. So I searched for a replacement to the matcher and found this one: https://godoc.org/github.com/denormal/go-gitignore. I switch to it in to your PR, and also tried using the `files.IgnoreFn()` pattern, what do you think of it?",thanks root issue matching relative relative set false working see logic however like matcher like even according docker documentation top go replacement matcher found one switch also tried pattern think,issue,positive,positive,neutral,neutral,positive,positive
584307469,"Hey @sam-qordoba, we have some initial thoughts, but we have not fully scoped it out yet. I'd be happy to jump on a call to discuss if you'd like, feel free to email me at omer@cortexlabs.com",hey initial fully yet happy jump call discus like feel free omer,issue,positive,positive,positive,positive,positive,positive
584231828,Is it possible to know which tickets need to be done to support GKE? Or does that work still need to be identified?,possible know need done support work still need,issue,negative,neutral,neutral,neutral,neutral,neutral
584193002,"It looks like https://github.com/catboost/catboost has a python interface. You can try using the [Python Predictor](https://www.cortex.dev/deployments/python) interface to serve your models. The python catboost library [can be imported](https://www.cortex.dev/dependency-management/python-packages) in your predictor implementation by specifying `catboost` and other python dependencies in a `requirements.txt` file on the same level as your `cortex.yaml` file.

Feel free to reach out to us on [Gitter](https://gitter.im/cortexlabs/cortex) if you would like to exchange messages in real-time.",like python interface try python predictor interface serve python library predictor implementation python file level file feel free reach u would like exchange,issue,positive,positive,positive,positive,positive,positive
584075483,"> @wingkwong thank you for your contribution, this is a great feature!
> 
> I made a few comments on the pull request, but nothing major, and most of them are up to you if you want to make the changes I mentioned. I also pushed a few very small changes to your branch.
> 
> I had two questions regarding the behavior of `.cortexignore`:
> 
> 1. As I was running the code, it seemed to me that it didn't work quite as I expected. I was in the `examples/tensorflow/iris-classifier` directory, I created `.cortexignore` with `*.md`, but `README.md` was still included in the project files (I tested this by printing out `projectPaths` in `cli/cmd/deploy.go`).
> 2. I wasn't able to determine by reading the code where a leading `!` is handled (I didn't see mention of it in [filepath.Match](https://golang.org/pkg/path/filepath/#Match)).
> 
> Please let me know your thoughts!

1.  If *.md is in cortexignore.md
https://github.com/wingkwong/cortex/blob/ebfd26301609042f9b3b6c52e2cc3ee00d66813e/pkg/lib/files/files_test.go#L188

and I expect there is a README.md. The test case will be failed 
![image](https://user-images.githubusercontent.com/35857179/74144875-81560800-4c38-11ea-9412-3bb57ac4bdf5.png)

so it should be able to ignore it.

2. the logic comes from 
https://github.com/wingkwong/cortex/blob/b0546616882bc9d17380c551b106a515ca91896e/pkg/lib/files/files.go#L615",thank contribution great feature made pull request nothing major want make also small branch two regarding behavior running code work quite directory still included project tested printing able determine reading code leading handled see mention please let know expect test case image able ignore logic come,issue,positive,positive,positive,positive,positive,positive
577913321,"Cortex uses functionality from EKS, EC2, CloudWatch, and S3 so it would be possible to have some of the key features working on minikube without a huge amount of effort, but full compatibility would be a lot harder which is why we haven't done it yet.",cortex functionality would possible key working without huge amount effort full compatibility would lot harder done yet,issue,negative,positive,positive,positive,positive,positive
577876089,"I see, but do you think it would be difficult to make it run on minikube, for instance?",see think would difficult make run instance,issue,negative,negative,negative,negative,negative,negative
577856872,"We'd love to support this use case but it might take some time given that we're a small team right now and our primary focus is AWS. Regarding lock-in, supporting other cloud providers like GCP is a high priority for us. As far as cost goes, it's easy to spin cortex clusters up when you're experimenting and down when you're done. You can also take advantage of the fact that Cortex can autoscale instances down to 0 when no models are deployed. We'd be happy to help you set that up if you are interested.",love support use case might take time given small team right primary focus regarding supporting cloud like high priority u far cost go easy spin cortex done also take advantage fact cortex happy help set interested,issue,positive,positive,positive,positive,positive,positive
577796377,"Hi @vishalbollu , basically we need to run the models on premises/other VMs in order to avoid vendor lock-in and costs (until, of course, the model costs are covered by revenues or fundings). 
And also because ours is highly experimental stuff most of the times. 
I thought cortex would be a good scaffolding tool for putting our models in production without having to do it manually all the times.",hi basically need run order avoid vendor course model covered also highly experimental stuff time thought cortex would good scaffolding tool production without manually time,issue,negative,positive,positive,positive,positive,positive
577782485,"Hi @lamuertepeluda, Cortex currently only runs on AWS. If you are looking to test Cortex, other users in the past have spun up a tiny single worker node cluster to play with Cortex.

Out of curiosity, what functionality of Cortex are you looking to use when you run Cortex locally or on a VM? If you can explain your use-case, we may be able to improve our roadmap.",hi cortex currently looking test cortex past spun tiny single worker node cluster play cortex curiosity functionality cortex looking use run cortex locally explain may able improve,issue,positive,positive,neutral,neutral,positive,positive
576480951,"Currently, Cortex only runs on AWS because we take advantage of services like EKS, S3, and CloudWatch. However, we plan to support running Cortex locally and on other cloud providers in the future.",currently cortex take advantage like however plan support running cortex locally cloud future,issue,positive,neutral,neutral,neutral,neutral,neutral
576024990,You should be able to deploy Spark models by exporting to ONNX first: https://github.com/onnx/onnxmltools/tree/master/onnxmltools/convert/sparkml.,able deploy spark first,issue,negative,positive,positive,positive,positive,positive
576024774,Cortex is designed for production environments. A Cortex cluster can scale to handle large request volumes (some users are running clusters of 100s of GPU instances in production) and it also runs on your AWS account so you have full visibility and control over security policies.,cortex designed production cortex cluster scale handle large request running production also account full visibility control security,issue,negative,positive,positive,positive,positive,positive
575915265,Hi @HowJMay this ticket refers to the preventing Cortex from assigning labels that are longer than 63 characters to Kubernetes pods. Some of the labels are being generated are based on users' deployment configuration in their cortex.yaml. This ticket is being addressed by PR #751. There are other tickets that we can use your help with which we labeled as [good first issue](https://github.com/cortexlabs/cortex/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22). Feel free to message us on [gitter](https://gitter.im/cortexlabs/cortex) if you have any questions.,hi ticket cortex longer based deployment configuration ticket use help good first issue feel free message u,issue,positive,positive,positive,positive,positive,positive
575907204,"Does the name here mean CONFIG_FILE name? 
If so, may I tty to take this issue?",name mean name may take issue,issue,negative,negative,negative,negative,negative,negative
573445905,"Ah I see, sorry for the confusion. Merged!",ah see sorry confusion,issue,negative,negative,negative,negative,negative,negative
573443223,"Hey David,

Thanks! However, I cannot merge since it says I do not have ""write access"" to the repository",hey thanks however merge since write access repository,issue,negative,positive,positive,positive,positive,positive
572264446,Cortex only runs on AWS right now because we use EKS as our underlying kubernetes cluster as well as CloudWatch for logging and metrics collection. We aim to support other cloud providers and on-premise deployments in the future.,cortex right use underlying cluster well logging metric collection aim support cloud future,issue,positive,positive,positive,positive,positive,positive
572093930,A temporary way to find relevant loadbalancers  is to search for them by tags e.g. `tag:kubernetes.io/cluster/<cluster-name> : All values`,temporary way find relevant search tag,issue,negative,positive,positive,positive,positive,positive
570454334,Could potentially be done by calling `du -sh` in bash and checking if this value exceeds 50MB. Just checked with a 21GB folder and value is returned in under half a second.,could potentially done calling bash value checked folder value returned half second,issue,positive,negative,neutral,neutral,negative,negative
564704161,This is super critical for us. So if this can be done as soon as possible would be extremely helpful. ,super critical u done soon possible would extremely helpful,issue,positive,positive,neutral,neutral,positive,positive
562867426,"[Starlette](https://www.starlette.io) would be a nice alternative too. It seems faster than Fast-API:

https://fastapi.tiangolo.com/benchmarks/
",would nice alternative faster,issue,negative,positive,positive,positive,positive,positive
559053742,"Hi @nikkkkhil that example was renamed from sentiment-analysis in version 0.10 to sentiment-analyzer in version 0.11 which is going to be released soon. 

Can you describe how you ran into that link?

In the meantime, that example can be accessed in the link: https://github.com/cortexlabs/cortex/tree/0.10/examples/tensorflow/sentiment-analysis",hi example version version going soon describe ran link example link,issue,negative,neutral,neutral,neutral,neutral,neutral
558268056,"Cortex doesn't run on GKE yet, I'm sorry if I mislead you. The latest version of Cortex can autoscale based on load up to the maximum number of nodes that you set. Our next release will include the ability to autoscale the inference nodes to 0 if no deployments are running though there will always be a Cortex operator (server) node unless you spin down the entire cluster. The operator resource requests are small so an inexpensive instance is sufficient. When we add GCP support, all this functionality will be ported over.",cortex run yet sorry mislead latest version cortex based load maximum number set next release include ability inference running though always cortex operator server node unless spin entire cluster operator resource small inexpensive instance sufficient add support functionality ported,issue,positive,negative,neutral,neutral,negative,negative
557760607,With Cortex on GKE will I still have to reason about capacity?  Selecting number of nodes for peak load?  Will I be able to scale to zero nodes when there is no load?,cortex still reason capacity number peak load able scale zero load,issue,negative,positive,positive,positive,positive,positive
555673971,"Hey Andrew, thanks for your feedback! I agree that Knative and GCR are super relevant and we just spent some time evaluating them. That being said, we built Cortex on top of Kubernetes to simplify supporting all cloud providers so our current plan is to run on GKE but that could definitely change. Our technology decisions are primarily focused on optimizing developer experience and minimizing the cost for running inference at scale so we try to avoid managed services that cost more as usage increases. We also try to abstract the underlying infrastructure services to allow users to focus on ML, so ideally whether we run on GKE or GCR shouldn't matter too much. Are there any features in GCR that you think Cortex is missing?",hey thanks feedback agree super relevant spent time said built cortex top simplify supporting cloud current plan run could definitely change technology primarily developer experience cost running inference scale try avoid cost usage also try abstract underlying infrastructure allow focus ideally whether run matter much think cortex missing,issue,positive,positive,positive,positive,positive,positive
555534257,"When GCP support is added, is there any intention of supporting deploying models to Google Cloud Run?

It is their hosted Knative and is dead simple serverless containers. We use GCR to evaluate predictions in production and are extremely happy with it.",support added intention supporting cloud run dead simple use evaluate production extremely happy,issue,positive,positive,positive,positive,positive,positive
548589894,"So, the problem somehow has gone away. Perhaps, there is a time lag on AWS. ",problem somehow gone away perhaps time lag,issue,negative,neutral,neutral,neutral,neutral,neutral
547761091,"Hey Nikita, `cortex deploy ` will not work without a cluster running the operator. You can make sure that all resources from your previous cluster have been cleaned up by looking at the CloudFormation console. Then, you'll be able to start a new cluster (www.cortex.dev/install), run `cortex configure` to reconnect your CLI to the new operator, and `cortex deploy` should work again.",hey cortex deploy work without cluster running operator make sure previous cluster looking console able start new cluster run cortex configure reconnect new operator cortex deploy work,issue,negative,positive,positive,positive,positive,positive
531954196,"@nickwalton I am marking this issue as resolved, please follow up if you encounter any more issues.",marking issue resolved please follow encounter,issue,negative,neutral,neutral,neutral,neutral,neutral
531913748,Note: this got split into separate commits,note got split separate,issue,negative,neutral,neutral,neutral,neutral,neutral
530169750,"Sorry about that! We are preparing for our release of v0.8 (which does not require zipping the model), and in our haste we deleted the old models when we uploaded the new, non-zipped ones.

We have added back both the small and large GPT-2 models:
* s3://cortex-examples/gpt-2/124M/1539234124.zip
* s3://cortex-examples/gpt-2/774M/1539234541.zip

Please let me know if this fixes the issue!",sorry release require zipping model haste old new added back small large please let know issue,issue,negative,negative,neutral,neutral,negative,negative
530044091,"Thanks @nickwalton, I agree and we'll definitely look into it! According to https://github.com/kubernetes/kubernetes/issues/69687 it should be possible if we use custom metrics for autoscaling instead of CPU utilization.",thanks agree definitely look according possible use custom metric instead utilization,issue,positive,positive,neutral,neutral,positive,positive
529988843,Gotcha. Well if you are able to enable it then that would be a great feature. I'll add it as a feature request #445,well able enable would great feature add feature request,issue,positive,positive,positive,positive,positive,positive
529601750,@deliahu corrected me that we can only autoscale down to 1 replica right now because the autoscaler is based on CPU which is undefined when there are no replicas. I'm sorry about the miscommunication and we'll look into how we can support autoscaling to 0.,corrected replica right based undefined sorry look support,issue,negative,negative,negative,negative,negative,negative
529559886,"Yes, you would need to set [min_replicas](https://github.com/cortexlabs/cortex/blob/master/docs/deployments/apis.md) on each API to 0. I'll clarify this in the docs.",yes would need set clarify,issue,negative,neutral,neutral,neutral,neutral,neutral
529552834,It should be fixed now. Thanks for letting us know!,fixed thanks u know,issue,negative,positive,positive,positive,positive,positive
528602858,"In this case, the status should be ""error""

Dupe of #408 
",case status error dupe,issue,negative,neutral,neutral,neutral,neutral,neutral
526325036,We can't delete cloudwatch metrics. They expire and get deleted automatically. I think we should maintain our current decision to help users Cortex resources but not delete them even if they are Cortex created.,ca delete metric expire get automatically think maintain current decision help cortex delete even cortex,issue,negative,neutral,neutral,neutral,neutral,neutral
523054272,"When an error is encountered while initializing flask container (e.g. exception in python handler), `kubectl logs ... --previous` fails during the transition of pod status from Error to CrashLoopBackOff `Unable to retrieve container logs for docker://<container_id>`",error flask container exception python handler previous transition pod status error unable retrieve container docker,issue,negative,negative,negative,negative,negative,negative
520028712,"Thanks for submitting your PR!

Yes, configuring Go environments can be tricky. Since we switched to Go modules, `go` should download the dependencies automatically at build time. For example, I ran the Go tests in a clean docker image with Go v1.12.6:

```
$ docker run --rm -it golang:1.12.6
root@9c1fd142baf1:/go# git clone https://github.com/cortexlabs/cortex.git
Cloning into 'cortex'...
...
root@9c1fd142baf1:/go# cd cortex/
root@9c1fd142baf1:/go/cortex# make test-go
go: finding github.com/pkg/errors v0.8.1
...
go tests passed
```

Note that the Python tests won't run here since they run in Docker (and I didn't configure ""Docker in Docker""), however they should run on your local environment if you have Docker installed. `make test` runs all of the tests.

We do have a guide for setting up a development environment, which we will make more prominent soon: https://github.com/cortexlabs/cortex/blob/master/docs/cluster/development.md

Also, CI runs the tests and makes sure all of the images build on each push. You can get a link to the job on CircleCI at the bottom of [your PR](https://github.com/cortexlabs/cortex/pull/303) in the ""All checks have passed"" section (in this case: https://circleci.com/workflow-run/f54fa603-c361-469e-b795-728a138f54c6).",thanks yes go tricky since switched go go automatically build time example ran go clean docker image go docker run root git clone root root make go finding go note python wo run since run docker configure docker docker however run local environment docker make test guide setting development environment make prominent soon also sure build push get link job bottom section case,issue,positive,positive,positive,positive,positive,positive
519925142,"Hi @deliahu ,

Got the idea too. Here is a change to `./cortex.sh` to take AWS credentials into account if they are not specifically exported in the shell. Existence of credentials in `~/.aws/credentials` assumes the installation of `aws-cli`, which offers the way access used credentials.

Now I admit I tested manually but not with `make test`. I'm not a good Go developer (just have the basics), so I was getting a lot of errors for missing packages (cortexlabs/yml, GoogleCloudPlatform/spark-on-k8s-operator,...) How can I install them all at once and not manually? Also do you have a `CONTRIBUTION.md` file for external contributors like me?

Thanks",hi got idea change take account specifically shell existence installation way access used admit tested manually make test good go developer getting lot missing install manually also file external like thanks,issue,positive,positive,positive,positive,positive,positive
485453962,"- showcases a collaborative filtering approach to recommendation system using deep learning
- uses [movielens 100k dataset](https://grouplens.org/datasets/movielens/) dataset as input
- dataset is a zip file  containing ratings.csv, movies.csv and other information
- our pipeline will use ratings.csv to group similar users to each other and similar movies to eachother and predict the rating that a user may give to a movie
- dataset contains the reviews of 600 users for 9000 movies with the schema: userid,movieid,rating,timestamp
- the userid range is [1, 610] and movieid range from [1, 193609]
- movieid range is large compared to the number of unique movies in dataset so shrink the range by creating a new index of only the movies that are present in the dataset
- index userid and movieid columns by treating them as string and using index_string
- embed indexed userid and movieid columns
- train a regression model to minimize RMSE for rating predictions
- predict the rating of a user with a userid that is mentioned in the input dataset to an unrated movie with movieid that is in the input dataset",collaborative filtering approach recommendation system deep learning input zip file information pipeline use group similar similar predict rating user may give movie schema rating range range range large number unique shrink range new index present index treating string embed indexed train regression model minimize rating predict rating user input unrated movie input,issue,negative,positive,positive,positive,positive,positive
482764861,"The landing page is live at cortex.dev

Have a good weekend!",landing page live good weekend,issue,negative,positive,positive,positive,positive,positive
482243244,Thank you for your feedback! I am working on a landing page and will reply here once it's live.,thank feedback working landing page reply live,issue,negative,positive,positive,positive,positive,positive
476705630,Can be recreated by adding a sum_distinct_float in the normalize template of fraud example,normalize template fraud example,issue,negative,neutral,neutral,neutral,neutral,neutral
471662996,"> Hey @rbromley10, thanks again for your contribution! Our CLA agreement and process is finalized now, do you mind signing the updated CLA and then merging?

Signed! I can't merge as I don't seem to have write access though.",hey thanks contribution agreement process mind ca merge seem write access though,issue,positive,positive,positive,positive,positive,positive
471653472,"Hey @rbromley10, thanks again for your contribution! Our CLA agreement and process is finalized now, do you mind signing the updated CLA and then merging?",hey thanks contribution agreement process mind,issue,positive,positive,positive,positive,positive,positive
470989149,@rbromley10 apologies for the delay. We are working towards setting up and following open source best practices. We appreciate your patience in the meanwhile.,delay working towards setting following open source best appreciate patience meanwhile,issue,positive,positive,positive,positive,positive,positive
465116929,"Currently, PySpark code can be used to create custom transformers and aggregators for data processing. You can view the Cortex built-in aggregators and transformers as examples:

* [cortex built-in aggregators](https://github.com/cortexlabs/cortex/tree/master/pkg/aggregators ""https://github.com/cortexlabs/cortex/tree/master/pkg/aggregators"") 
*  [cortex built-in transformers](https://github.com/cortexlabs/cortex/tree/master/pkg/transformers ""https://github.com/cortexlabs/cortex/tree/master/pkg/transformers"") 

Also the [mnist](https://github.com/cortexlabs/cortex/blob/master/examples/mnist/implementations/transformers/decode_and_normalize.py ""https://github.com/cortexlabs/cortex/blob/master/examples/mnist/implementations/transformers/decode_and_normalize.py"") and [reviews]( https://github.com/cortexlabs/cortex/blob/master/examples/reviews/implementations/aggregators/vocab.py ""https://github.com/cortexlabs/cortex/blob/master/examples/reviews/implementations/aggregators/vocab.py"") examples contain more advanced implementations.

If there are any additional aggregators or transformers that you think would be useful to provide out of the box,  let us know and we can implement them, or feel free submit a pull request.

You can find a detailed explanation on how you can create and use your own custom [transformers](https://docs.cortexlabs.com/cortex/resources/transformers ""https://docs.cortexlabs.com/cortex/resources/transformers"") and [aggregators](https://docs.cortexlabs.com/cortex/resources/aggregators ""https://docs.cortexlabs.com/cortex/resources/aggregators"") in these docs.

Hope this helps!",currently code used create custom data view cortex cortex cortex also contain advanced additional think would useful provide box let u know implement feel free submit pull request find detailed explanation create use custom hope,issue,positive,positive,positive,positive,positive,positive
