id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2004263084,"Thanks for the reply. Being relatively new to Anaconda and Python didn't give me any insight to troubleshooting. It all came down to dependencies and flaky paths and inexperience. All works great now. Very good learning experience. Outstanding work on your part.
Best regards.
bk

On March 17, 2024 9:51:34 PM EDT, Jason Antic ***@***.***> wrote:
>I haven't run into this error myself. Have you seen this?  Seems like a good start:  https://www.reddit.com/r/linuxquestions/comments/1054cju/libtiffso5_is_broken_how_to_fix_it/
>
>-- 
>Reply to this email directly or view it on GitHub:
>https://github.com/jantic/DeOldify/issues/498#issuecomment-2002737511
>You are receiving this because you authored the thread.
>
>Message ID: ***@***.***>",thanks reply relatively new anaconda python give insight came flaky inexperience work great good learning experience outstanding work part best march antic wrote run error seen like good start reply directly view thread message id,issue,positive,positive,positive,positive,positive,positive
2002738934,"Thanks! There isn't a way to do this unfortunately. DeOldify was designed to be a purely automatic colorization process, as opposed to one allowing for manual interventions like that. It wouldn't be trivial to redesign it for that either.",thanks way unfortunately designed purely automatic colorization process opposed one manual like would trivial redesign either,issue,negative,negative,neutral,neutral,negative,negative
2002737511,I haven't run into this error myself. Have you seen this?  Seems like a good start:  https://www.reddit.com/r/linuxquestions/comments/1054cju/libtiffso5_is_broken_how_to_fix_it/,run error seen like good start,issue,negative,positive,positive,positive,positive,positive
1986946031,"Wow, I went through that readme- very interesting results you got there. Great read, and I like the project! Thanks for doing this.",wow went interesting got great read like project thanks,issue,positive,positive,positive,positive,positive,positive
1937833264,"Hmm yeah I'm having trouble with Flickr links too. Not sure what's up there. Video/gif downloads are handled through youtube-dl (https://github.com/ytdl-org/youtube-dl), so that's beyond the scope of the DeOldify project to try to address. I wish I could help more on that!",yeah trouble link sure handled beyond scope project try address wish could help,issue,positive,positive,positive,positive,positive,positive
1937828426,"My best guess is that if you open in a text editor the stable image generator weights file you downloaded, that you'll see that it's actually the host html page that got saved as a file and not the actual weights.  That actually happens! Got to this link and hit File->download to get the weights:  https://www.dropbox.com/s/axsd2g85uyixaho/ColorizeStable_gen.pth?e=2&dl=0",best guess open text editor stable image generator file see actually host page got saved file actual actually got link hit get,issue,positive,positive,positive,positive,positive,positive
1891060216,"> @jantic Has this issue been resolved?

It hasn't. I don't think I'll be getting to this any time soon.",issue resolved think getting time soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1852242628,"Works great on Apple Silicon (Apple M1 Max) on macOS Sonoma (14.1.2). The only thing I had to change was the PyTorch version. In file `requirements.text` change to `torch==2.0.0` (note PyTorch 2.0.0 doesn't support HW acceleration on  Apple Silicon - so I run everything on CPU.). PyTorch 2.0.0 is the oldest version that I managed to run on Apple Silicon.

I haven't tried training anything yet. 

If I succeed in getting DeOldify to run on Metal, I'll share my findings in another comment. 

Thanks for sharing this awesome project.",work great apple silicon apple thing change version file change note support acceleration apple silicon run everything version run apple silicon tried training anything yet succeed getting run metal share another comment thanks awesome project,issue,positive,positive,positive,positive,positive,positive
1825725306,I use colab to run and it work fine.,use run work fine,issue,negative,positive,positive,positive,positive,positive
1822722932,"> I try to use ColorizeTrainingStable.ipynb to train a model, When I run to
> 
> `learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))`
> 
> i get following warning
> 
> `---------------------------------------------------------------------------
> Empty                                     Traceback (most recent call last)
> File F:\Anaconda\envs\deoldify\lib\site-packages\torch\utils\data\dataloader.py:1011, in _MultiProcessingDataLoaderIter._try_get_data(self, timeout)
> 1010 try:
> -> 1011     data = self._data_queue.get(timeout=timeout)
> 1012     return (True, data)
> File F:\Anaconda\envs\deoldify\lib\queue.py:179, in Queue.get(self, block, timeout)
> 178 if remaining <= 0.0:
> --> 179     raise Empty
> 180 self.not_empty.wait(remaining)
> Empty:
> The above exception was the direct cause of the following exception:
> RuntimeError                              Traceback (most recent call last)
> Cell In[9], line 1
> ----> 1 learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))
> File F:\image\project\DeOldify-ImageColorizer\fastai\train.py:23, in fit_one_cycle(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch, batch_multiplier)
> 20 callbacks = listify(callbacks)
> 21 callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,
> 22                                    final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))
> ---> 23 learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks, batch_multiplier=batch_multiplier)
> File F:\image\project\DeOldify-ImageColorizer\fastai\basic_train.py:202, in Learner.fit(self, epochs, lr, wd, callbacks, batch_multiplier)
> 200 if not getattr(self, 'opt', False): self.create_opt(lr, wd)
> 201 else: self.opt.lr,self.opt.wd = lr,wd
> --> 202 callbacks = [cb(self) for cb in self.callback_fns + listify(defaults.extra_callback_fns)] + listify(callbacks)
> 203 if defaults.extra_callbacks is not None: callbacks += defaults.extra_callbacks
> 204 fit(epochs, self, metrics=self.metrics, callbacks=self.callbacks+callbacks, batch_multiplier=batch_multiplier)
> File F:\image\project\DeOldify-ImageColorizer\fastai\basic_train.py:202, in (.0)
> 200 if not getattr(self, 'opt', False): self.create_opt(lr, wd)
> 201 else: self.opt.lr,self.opt.wd = lr,wd
> --> 202 callbacks = [cb(self) for cb in self.callback_fns + listify(defaults.extra_callback_fns)] + listify(callbacks)
> 203 if defaults.extra_callbacks is not None: callbacks += defaults.extra_callbacks
> 204 fit(epochs, self, metrics=self.metrics, callbacks=self.callbacks+callbacks, batch_multiplier=batch_multiplier)
> File F:\image\project\DeOldify-ImageColorizer\fastai\callbacks\tensorboard.py:178, in ImageGenTensorboardWriter.**init**(self, learn, base_dir, name, loss_iters, hist_iters, stats_iters, visual_iters)
> 176 def **init**(self, learn:Learner, base_dir:Path, name:str, loss_iters:int=25, hist_iters:int=500, stats_iters:int=100,
> 177              visual_iters:int=100):
> --> 178     super().**init**(learn=learn, base_dir=base_dir, name=name, loss_iters=loss_iters, hist_iters=hist_iters,
> 179                      stats_iters=stats_iters)
> 180     self.visual_iters = visual_iters
> 181     self.img_gen_vis = ImageTBWriter()
> File F:\image\project\DeOldify-ImageColorizer\fastai\callbacks\tensorboard.py:38, in LearnerTensorboardWriter.**init**(self, learn, base_dir, name, loss_iters, hist_iters, stats_iters)
> 36 self.data = None
> 37 self.metrics_root = '/metrics/'
> ---> 38 self._update_batches_if_needed()
> File F:\image\project\DeOldify-ImageColorizer\fastai\callbacks\tensorboard.py:50, in LearnerTensorboardWriter._update_batches_if_needed(self)
> 48 if not update_batches: return
> 49 self.data = self.learn.data
> ---> 50 self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)
> 51 self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)
> File F:\image\project\DeOldify-ImageColorizer\fastai\callbacks\tensorboard.py:42, in LearnerTensorboardWriter._get_new_batch(self, ds_type)
> 40 def _get_new_batch(self, ds_type:DatasetType)->Collection[Tensor]:
> 41     ""Retrieves new batch of DatasetType, and detaches it.""
> ---> 42     return self.learn.data.one_batch(ds_type=ds_type, detach=True, denorm=False, cpu=False)
> File F:\image\project\DeOldify-ImageColorizer\fastai\basic_data.py:168, in DataBunch.one_batch(self, ds_type, detach, denorm, cpu)
> 166 w = self.num_workers
> 167 self.num_workers = 0
> --> 168 try:     x,y = next(iter(dl))
> 169 finally: self.num_workers = w
> 170 if detach: x,y = to_detach(x,cpu=cpu),to_detach(y,cpu=cpu)
> File F:\image\project\DeOldify-ImageColorizer\fastai\basic_data.py:75, in DeviceDataLoader.**iter**(self)
> 73 def **iter**(self):
> 74     ""Process and returns items from `DataLoader`.""
> ---> 75     for b in self.dl: yield self.proc_batch(b)
> File F:\Anaconda\envs\deoldify\lib\site-packages\torch\utils\data\dataloader.py:530, in _BaseDataLoaderIter.**next**(self)
> 528 if self._sampler_iter is None:
> 529     self._reset()
> --> 530 data = self._next_data()
> 531 self._num_yielded += 1
> 532 if self._dataset_kind == _DatasetKind.Iterable and 
> 533         self._IterableDataset_len_called is not None and 
> 534         self._num_yielded > self._IterableDataset_len_called:
> File F:\Anaconda\envs\deoldify\lib\site-packages\torch\utils\data\dataloader.py:1207, in _MultiProcessingDataLoaderIter._next_data(self)
> 1204     return self._process_data(data)
> 1206 assert not self._shutdown and self._tasks_outstanding > 0
> -> 1207 idx, data = self._get_data()
> 1208 self._tasks_outstanding -= 1
> 1209 if self._dataset_kind == _DatasetKind.Iterable:
> 1210     # Check for _IterableDatasetStopIteration
> File F:\Anaconda\envs\deoldify\lib\site-packages\torch\utils\data\dataloader.py:1163, in _MultiProcessingDataLoaderIter._get_data(self)
> 1161 elif self._pin_memory:
> 1162     while self._pin_memory_thread.is_alive():
> -> 1163         success, data = self._try_get_data()
> 1164         if success:
> 1165             return data
> File F:\Anaconda\envs\deoldify\lib\site-packages\torch\utils\data\dataloader.py:1024, in _MultiProcessingDataLoaderIter._try_get_data(self, timeout)
> 1022 if len(failed_workers) > 0:
> 1023     pids_str = ', '.join(str(w.pid) for w in failed_workers)
> -> 1024     raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
> 1025 if isinstance(e, queue.Empty):
> 1026     return (False, None)
> RuntimeError: DataLoader worker (pid(s) 14696, 5216, 4312) exited unexpectedly`
> 
> how should I do?

Turn on virtual memory can fix it. But I meet another issue
`RuntimeError: Given input size: (512x1x1). Calculated output size: (512x0x0). Output size is too small`

Any advice?",try use train model run get following warning empty recent call last file self try data return true data file self block raise empty empty exception direct cause following exception recent call last cell line file learn learn file self self false else self none fit self file self false else self none fit self file self learn name self learn learner path name super file self learn name none file self return file self self collection tensor new batch return file self detach try next iter finally detach file iter self iter self process yield file next self none data none file self return data assert data check file self success data success return data file self raise worker return false none worker unexpectedly turn virtual memory fix meet another issue given input size calculated output size output size small advice,issue,negative,positive,neutral,neutral,positive,positive
1821157326,"Change the code to
> if not path_lr.exists():
>     il = ImageList.from_folder(path_hr)
>     for i in range(len(il.items)):
>         create_training_images(il.items[i],1)
> print(il.items)

can solve problems",change code range print solve,issue,negative,neutral,neutral,neutral,neutral,neutral
1820554520,"> I ran the notebook to try training the model with imagenet images. Put 20k images under the path. The images were found after this `il = ImageList.from_folder(path_hr)`. But when I ran `data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)`, I got the exact same error as [#35](https://github.com/jantic/DeOldify/issues/35). I tried to create the folders manually with no luck.

I have same error right now. Have you figured out how to solve it?

",ran notebook try training model put path found ran got exact error tried create manually luck error right figured solve,issue,negative,positive,positive,positive,positive,positive
1793811602,"I see this:

Solving environment: - Processus arrêté

To me that reads as if the conda environment install was stopped prematurely with either ctrl+c or something similar....Anyway I'd try that again but also I'd say that this primarily tested on Ubuntu 18.04 and above so I'm not sure what kind of problems you may be running into on Debian.",see environment environment install stopped prematurely either something similar anyway try also say primarily tested sure kind may running,issue,positive,positive,positive,positive,positive,positive
1793809176,Believe it or not I've moved on from the DeOldify project and startup myself.  The startup still exists but I exited earlier this year. So I can't speak to their plans but I don't plan on doing updates to this repo other than maintenance.  I've moved on to weather forecasting modeling. ,believe project still year ca speak plan maintenance weather forecasting modeling,issue,negative,neutral,neutral,neutral,neutral,neutral
1773918876,"There wasn't anything done to facilitate this in the project, no. It could be modified for sure to do that but that's beyond the scope of what I intended to do with this project.  The core code that does inference ultimately calls a function predict_batch in fastai with a single image to predict, which is a good place to start.

https://github.com/jantic/DeOldify/blob/be725ca6c5f411c47550df951546537d7202c9bc/fastai/basic_train.py#L374",anything done facilitate project could sure beyond scope intended project core code inference ultimately function single image predict good place start,issue,positive,positive,positive,positive,positive,positive
1773918242,It was never designed to be used like that- I was just sharing research.  But the code can be modified pretty easily to do that. Thanks!,never designed used like research code pretty easily thanks,issue,positive,positive,positive,positive,positive,positive
1746757392,"I am an <a href=""https://www.dakshi.in/"">Bangalore Escort</a> and I can offer you the most exciting escort services. You can contact me for booking without any hesitation because I will provide you unforgettable time with me. It doesn't matter if you are in Bangalore or outside this city, my service is available 24 hours a day 7 days a week for your convenience.",escort offer exciting escort contact booking without hesitation provide unforgettable time matter outside city service available day day week convenience,issue,positive,positive,positive,positive,positive,positive
1746756312,"Many people might still view <a href=""https://www.ctgirls.in/"">Mumbai Escort</a> in a red light. However, with so many firms offering their services, it opens up opportunities for people to indulge in their fantasies. With the right person by your side, you'll be able to enjoy everything from exotic dinners and movie screenings to a day at the spa or even on an ocean cruise.",many people might still view escort red light however many offering people indulge right person side able enjoy everything exotic movie day spa even ocean cruise,issue,negative,positive,positive,positive,positive,positive
1746754923,"If you are looking for an <a href=""https://www.nightangels.in/escorts/jaipur"">Escort Service in Jaipur</a> then you have come to the right place, VIP Escorts is one of the trusted escort service providers in Jaipur. We specialize in providing high-profile escort services to our elite clientele. You can book our escorts for an outcall or incall on any day of your choice and at any time of your choice.
<a href=""https://www.nightangels.in/escorts/mumbai"">Mumbai Escorts Services</a>
<a href=""https://www.nightangels.in/escorts/bangalore"">Bangalore Escort</a>",looking service come right place one escort service specialize providing escort elite clientele book day choice time choice escort,issue,negative,positive,positive,positive,positive,positive
1737738892,UPDATE: problem solved. its cool to be able to finally run deoldify on windows ,update problem cool able finally run,issue,negative,positive,positive,positive,positive,positive
1721382606,"I understand. It might be good to add a check and error message otherwise the decision does not seem too deliberate. 
Anyway I will keep using my fork then, I completely understand not wanting to put effort into this.",understand might good add check error message otherwise decision seem deliberate anyway keep fork completely understand wanting put effort,issue,negative,positive,positive,positive,positive,positive
1694488369,"> Done! I have to tell you, this is really really awesome. Thank you!

Thank you for your recognition that Deoldify is a great project. It's my honour to contribute to it.",done tell really really awesome thank thank recognition great project contribute,issue,positive,positive,positive,positive,positive,positive
1694449049,"Done! I have to tell you, this is really really awesome. Thank you!",done tell really really awesome thank,issue,positive,positive,positive,positive,positive,positive
1694428268,"Yeah that's a deliberate decision. As the aim is to just make research freely available to the public, I made a few simplifying decisions to keep maintenance and bugs to a minimum. This is one of them. ",yeah deliberate decision aim make research freely available public made keep maintenance minimum one,issue,positive,positive,positive,positive,positive,positive
1667809663,there's no paid subscription on this stuff btw.... I'm not sure who you're paying but it's not me!,subscription stuff sure paying,issue,negative,positive,positive,positive,positive,positive
1667807725,It appears it was just a temporary issue with the hosting site being down.  It's working now. ,temporary issue hosting site working,issue,negative,neutral,neutral,neutral,neutral,neutral
1667002358,Seems that deepai link is either down or no longer being used.,link either longer used,issue,negative,neutral,neutral,neutral,neutral,neutral
1627379667,"Training set is used for all the training.  I used ImageNet, but I'd suggest trying OpenImages these days (more diverse and larger dataset).",training set used training used suggest trying day diverse,issue,negative,neutral,neutral,neutral,neutral,neutral
1627378867,"> I found it really makes a difference in quality when you have a better quality video to start

Yeah that's definitely something I've found and have advised users in the past to consider. And unfortunately this is even true with my later and more fancy commercial versions of this stuff. It was improved upon quite a bit, but ultimately it's just inherently harder for the AI to work with poor quality images/video.

>  I have 2 GPU's running 24/7 colorizing. lol

WOW! This is great to hear.
",found really difference quality better quality video start yeah definitely something found advised past consider unfortunately even true later fancy commercial stuff upon quite bit ultimately inherently harder ai work poor quality running wow great hear,issue,positive,positive,neutral,neutral,positive,positive
1627377359,"> Firstly,learn_gen.freeze() only freezes the conv layer, not the BN layer.Do I need to freeze the BN layer separately?

That was a deliberate design decision by FastAI. I honestly forget what the exact justification was for it, but it was deliberate. You can modify the source code there of the FastAI fork there if you want to experiment with it of course!

> Secondly,after training the 192px portion, the resulting image was then put into the GAN for training, but why was it clean when it was put in and ended up with a lot of dirty data?

So what I'm seeing there is evidence of instability of training.  Especially the blue blobs and the b/w images. Top candidates for causing this are: 
1. Changing the batch size without modifying the learning rate accordingly, or vice versa. Generally you want to halve the learning rate if you halve the batch size, and the other way around if you double it.
2. Using fp16 (mixed precision) training naively. I've never tried mixed precision with this particular repo, but I did with my commercial stuff and I can tell you there's an art to doing it.
3. Changing anything with the architecture without experimenting with updates to learning rate via learning rate finder.
4. Changing the optimizer.

Hope that helps!

1. ",firstly layer need freeze layer separately deliberate design decision honestly forget exact justification deliberate modify source code fork want experiment course secondly training portion resulting image put gan training clean put ended lot dirty data seeing evidence instability training especially blue top causing batch size without learning rate accordingly vice generally want halve learning rate halve batch size way around double mixed precision training naively never tried mixed precision particular commercial stuff tell art anything architecture without learning rate via learning rate finder hope,issue,positive,positive,positive,positive,positive,positive
1597293937,"Hey Jason,

Just wanted to give you an update.  I was trying to use the artistic feature for video which I did do successfully but it seems to have more issues with leaving gray parts.. like the nose... It's too bad because it seems to give more yellows and possibly better reds.  Deciding on reds and creating that red fog seems to be the biggest issue.  And second biggest would be the parts that still have a gray tone to them.. usually things like hands or shaded faces.  I found it really makes a difference in quality when you have a better quality video to start.  Some shows the color is amazing.  Others with lower film resolution and quality (like 360p) are not as good.  If you are looking for more results to compare, I have the B&W and color versions for 35 shows I am colorizing.  I believe you already subscribed to my channel on YouTube (@jdc4429 - Jeff's Classics + The Bird Cam)  Getting some really nice comments from people on the colorization.  You made it possible to bring back a generation of B&W media so that people will might actually watch it again.  So many shows that the big studio's never colorized.  Thanks again for such a great tool.  I have 2 GPU's running 24/7 colorizing. lol",hey give update trying use artistic feature video successfully leaving gray like nose bad give yellow possibly better red fog biggest issue second biggest would still gray tone usually like shaded found really difference quality better quality video start color amazing lower film resolution quality like good looking compare color believe already channel jeff bird cam getting really nice people colorization made possible bring back generation medium people might actually watch many big studio never thanks great tool running,issue,positive,positive,positive,positive,positive,positive
1582421398,"It certainly could be done (pretty trivial) but it's beyond the scope/ambitions of the project. It's strictly for sharing research. One of the things I try to be disciplined about is not venture into ""productization"" territory with this (there's no end to what different people want!).",certainly could done pretty trivial beyond project strictly research one try venture territory end different people want,issue,positive,positive,positive,positive,positive,positive
1568813000,Yeah I guess we're out of luck here then. There are better ways to do video that avoid the flickering and make the color more vibrant- I improved upon the tech in our closed source models for the DeOldify business. But that's not an option here.,yeah guess luck better way video avoid flickering make color upon tech closed source business option,issue,positive,positive,positive,positive,positive,positive
1567632832,"> I'm a bit confused honestly. That .NET program (assuming you're talking about [this](https://github.com/ColorfulSoft/DeOldify.NET) ) is for images only. You'd want to use the video specific model for DeOldify (there's a link to the Colab on the readme. The reason why the video specific model was developed was specifically to address inconsistency of frames, at the expense of lowering the range of colors (deliberate tradeoff).

I have also tried that but it is still not ideal.",bit confused honestly program assuming talking want use video specific model link reason video specific model specifically address inconsistency expense lowering range color deliberate also tried still ideal,issue,positive,positive,positive,positive,positive,positive
1567334293,"I'm a bit confused honestly. That .NET program (assuming you're talking about [this](https://github.com/ColorfulSoft/DeOldify.NET) ) is for images only. You'd want to use the video specific model for DeOldify (there's a link to the Colab on the readme. The reason why the video specific model was developed was specifically to address inconsistency of frames, at the expense of lowering the range of colors (deliberate tradeoff).",bit confused honestly program assuming talking want use video specific model link reason video specific model specifically address inconsistency expense lowering range color deliberate,issue,negative,positive,neutral,neutral,positive,positive
1567140903,Sorry but that's a very big ask and I simply won't have the time nor the inclination to work on such a thing!,sorry big ask simply wo time inclination work thing,issue,negative,negative,negative,negative,negative,negative
1566179289,"Did you use the requirements.txt file to install the requirements (via pip)? And is the install isolated from other things with either a python environment or a conda environment?  If you do those two things you should avoid this error as what you're seeing here is that the Python version used here (3.10) is a mismatch from what should be installed (3.6). 

If that doesn't work- It's very possible that you did those two things and that there's a bug in the fastai install process that's allowing for 3.10 to be install so if that's the case, I'd just put in an explicit line in requirements.txt:

`python==3.6.15`

If that's the case I can see about doing a pull request for that project but honestly it appears they're not making updates to that project lately.",use file install via pip install isolated either python environment environment two avoid error seeing python version used mismatch possible two bug install process install case put explicit line case see pull request project honestly making project lately,issue,negative,positive,neutral,neutral,positive,positive
1564610537,"Nevermind, I got it working.  removed the .name from the visualize.py and
it works.

Thanks a lot for your time.  Will let you know the results if you like.

Regards,

Jeff

On Fri, May 26, 2023 at 11:25 AM Jeff Carleton ***@***.***> wrote:

> Thanks for the info, I did not know that the _ meant not public facing or
> have forgotten. I actually tried that way but I got an error. :(
>
> Traceback (most recent call last):
>   File ""/home/jeff/DeOldify/colorize_it.py"", line 27, in <module>
>     colorizer._save_result_image(input_path, result)
>   File ""/home/jeff/DeOldify/deoldify/visualize.py"", line 164, in
> _save_result_image
>     result_path = results_dir / source_path.name
> AttributeError: 'str' object has no attribute 'name'
>
>
> In case you're curious, I basically want to be able to do this so I can
> try using the Artistic method on a video.  Problems with the red getting
> washed out and still get frames with hands that are gray or part of the
> head when wearing a hat.
> I want to see if I can get better results.. Plus I can add upscalers and
> such... There is a really good upscaler now that has some really good face
> restoration.  Unfortunately it's not good on the side or back of the head
> yet.  But if the face is head on the results are amazing. It's called
> GFPGAN and can be found on github. Version 1.4 is the best so far. You can
> take a jpg of a whole person that's only around 200x250 and crop the head
> only to 512x512.  It's scary good.
> Any idea on the error?
>
> On Fri, May 26, 2023 at 11:05 AM Jason Antic ***@***.***>
> wrote:
>
>> This is all said with the caveat that the colorizer._save_result_image
>> function isn't a public facing function and wasn't meant to be (hence
>> following the convention of being prefixed with _ ).
>>
>> That being said , colorizer._save_result_image has this function
>> definition:
>>
>> def _save_result_image(self, source_path: Path, image: Image, results_dir
>> = None) -> Path:
>>
>> You'll actually pass the original input_path as the first parameter
>> ""source_path"" because it'll derive the file name from that. And then you
>> pass the ""result"" image as second parameter (""image""). The third parameter
>> is optional and it specifies the directory you want to put the images in-
>> the default behavior is to just use the normal result folder.
>>
>> So you'd call it like this:
>> colorizer._save_result_image(input_path, result)
>>
>> Hope that helps!
>>
>> —
>> Reply to this email directly, view it on GitHub
>> <https://github.com/jantic/DeOldify/issues/472#issuecomment-1564532050>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AEVP2RB57FBWRUPUON6H4U3XIDBDFANCNFSM6AAAAAAYHFS64Y>
>> .
>> You are receiving this because you authored the thread.Message ID:
>> ***@***.***>
>>
>
",got working removed work thanks lot time let know like jeff may jeff wrote thanks know meant public facing forgotten actually tried way got error recent call last file line module result file line object attribute case curious basically want able try artistic method video red getting washed still get gray part head wearing hat want see get better plus add really good really good face restoration unfortunately good side back head yet face head amazing found version best far take whole person around crop head scary good idea error may antic wrote said caveat function public facing function meant hence following convention prefixed said function definition self path image image none path actually pas original first parameter derive file name pas result image second parameter image third parameter optional directory want put default behavior use normal result folder call like result hope reply directly view id,issue,positive,positive,positive,positive,positive,positive
1564558659,"Thanks for the info, I did not know that the _ meant not public facing or
have forgotten. I actually tried that way but I got an error. :(

Traceback (most recent call last):
  File ""/home/jeff/DeOldify/colorize_it.py"", line 27, in <module>
    colorizer._save_result_image(input_path, result)
  File ""/home/jeff/DeOldify/deoldify/visualize.py"", line 164, in
_save_result_image
    result_path = results_dir / source_path.name
AttributeError: 'str' object has no attribute 'name'


In case you're curious, I basically want to be able to do this so I can try
using the Artistic method on a video.  Problems with the red getting washed
out and still get frames with hands that are gray or part of the head when
wearing a hat.
I want to see if I can get better results.. Plus I can add upscalers and
such... There is a really good upscaler now that has some really good face
restoration.  Unfortunately it's not good on the side or back of the head
yet.  But if the face is head on the results are amazing. It's called
GFPGAN and can be found on github. Version 1.4 is the best so far. You can
take a jpg of a whole person that's only around 200x250 and crop the head
only to 512x512.  It's scary good.
Any idea on the error?

On Fri, May 26, 2023 at 11:05 AM Jason Antic ***@***.***>
wrote:

> This is all said with the caveat that the colorizer._save_result_image
> function isn't a public facing function and wasn't meant to be (hence
> following the convention of being prefixed with _ ).
>
> That being said , colorizer._save_result_image has this function
> definition:
>
> def _save_result_image(self, source_path: Path, image: Image, results_dir
> = None) -> Path:
>
> You'll actually pass the original input_path as the first parameter
> ""source_path"" because it'll derive the file name from that. And then you
> pass the ""result"" image as second parameter (""image""). The third parameter
> is optional and it specifies the directory you want to put the images in-
> the default behavior is to just use the normal result folder.
>
> So you'd call it like this:
> colorizer._save_result_image(input_path, result)
>
> Hope that helps!
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/472#issuecomment-1564532050>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AEVP2RB57FBWRUPUON6H4U3XIDBDFANCNFSM6AAAAAAYHFS64Y>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks know meant public facing forgotten actually tried way got error recent call last file line module result file line object attribute case curious basically want able try artistic method video red getting washed still get gray part head wearing hat want see get better plus add really good really good face restoration unfortunately good side back head yet face head amazing found version best far take whole person around crop head scary good idea error may antic wrote said caveat function public facing function meant hence following convention prefixed said function definition self path image image none path actually pas original first parameter derive file name pas result image second parameter image third parameter optional directory want put default behavior use normal result folder call like result hope reply directly view id,issue,positive,positive,positive,positive,positive,positive
1564532050,"This is all said with the caveat that the colorizer._save_result_image function isn't a public facing function and wasn't meant to be (hence following the convention of being prefixed with _ ).


That being said , colorizer._save_result_image has this function definition:

`def _save_result_image(self, source_path: Path, image: Image, results_dir = None) -> Path:`

You'll actually pass the original input_path as the first parameter ""source_path"" because it'll derive the file name from that. And then you pass the ""result"" image as second parameter (""image""). The third parameter is optional  and it specifies the directory you want to put the images in- the default behavior is to just use the normal result folder.  

So you'd call it like this:
`colorizer._save_result_image(input_path, result)`

Hope that helps!
",said caveat function public facing function meant hence following convention prefixed said function definition self path image image none path actually pas original first parameter derive file name pas result image second parameter image third parameter optional directory want put default behavior use normal result folder call like result hope,issue,positive,positive,neutral,neutral,positive,positive
1564503416,"Thanks Jason.  I am still having an issue saving the file.  I don't see any
documentation on how to call these things and no example I can find that
uses this function. The examples just plot the image and say to right click
and save as..

Here is sample code..

input_path = '/home/jeff/DeOldify/test_images/00104.jpg'
result = colorizer.get_transformed_image(input_path, render_factor)  #
Original line, idea to try below line
output_file = 'home/jeff/DeOldify/result_images/'
colorizer._save_result_image(output_file, image = result)   #<--- Don't
know what I need to pass.
print(""Passed the single image"")

How do I pass to colorizer._save_result_image() ?

Thanks again.

Regards,
Jeff

On Thu, May 25, 2023 at 6:07 PM Jason Antic ***@***.***>
wrote:

> Closed #472 <https://github.com/jantic/DeOldify/issues/472> as completed.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/472#event-9344821854>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AEVP2RHD7H5S6D5MG3PR4GTXH7J3XANCNFSM6AAAAAAYHFS64Y>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",thanks still issue saving file see documentation call example find function plot image say right click save sample code result original line idea try line image result know need pas print single image pas thanks jeff may antic wrote closed reply directly view id,issue,positive,positive,positive,positive,positive,positive
1563565681,environment.yml files aren't for pip.  They're for conda.  You'll want to install anaconda and use that to create the environment.,pip want install anaconda use create environment,issue,negative,neutral,neutral,neutral,neutral,neutral
1563561867,"Sorry about the delay.  This one should be a pretty easy fix- don't pass in input_image to colorizer.get_transformed_image.  Pass input_path instead. The function takes a path, not an image.  Hope that didn't suck up too much more time!",sorry delay one pretty easy pas pas instead function path image hope suck much time,issue,positive,positive,neutral,neutral,positive,positive
1553882950,"I think this is from another path I have to set.. ie result_path=None.Source_url=None

",think another path set ie,issue,negative,neutral,neutral,neutral,neutral,neutral
1552032313,It works for me.  Seems like it must be a temporary or local issue for you....,work like must temporary local issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1552030230,"The docker functionality isn't supported anymore, chiefly because docker has all sorts of potential support issues. I can tell just looking at this that this is probably a bit of a rabbit hole. Really...your best bet might be to try to get ChatGPT-4 to get you to walk through it. It might take a few back and forths but I've generally had a lot of success with it.",docker functionality chiefly docker potential support tell looking probably bit rabbit hole really best bet might try get get walk might take back generally lot success,issue,positive,positive,positive,positive,positive,positive
1548581017,"Thank you for your reply.  I kinda figured that would be the case regarding the GPU.  I may look into lightning for multiple GPUs. Said it was easy to set up.  But only so many hours in a day and so many amazing AI projects now.  Basically any black and white show that is allowed on YouTube I'm going to colorize. There are a bunch!  Going to take me a lot of time.  Once I get the K80 installed I will be able to do 3 conversions at a time so that will speed it up even if I can't run them in parallel. I have also been downloading some of the image upscalers.  Upscayl shows some amazing results. It actually fixes some of the blurriness in the colorized videos (as well as 4x the resolution) but was only made to do individual frames.  Since DeOldify already splits the video into images, I may try adding upscayl to process each frame after the colorization.  I did a test on a single frame and it was impressive. Thanks again for such a great project. Unfortunately it seems I can't attach pictures here to show the difference. ",thank reply figured would case regarding may look lightning multiple said easy set many day many amazing ai basically black white show going colorize bunch going take lot time get able time speed even ca run parallel also image amazing actually well resolution made individual since already video may try process frame colorization test single frame impressive thanks great project unfortunately ca attach show difference,issue,positive,positive,positive,positive,positive,positive
1548242351,"> Can you set multiple GPU's? I actually just ordered a K80 to go along with my RTX2070.

I didn't set up the code to do this unfortunately. You could do it theoretically but it would require extra work and research that you would have to do.

> I also noticed the image conversions seem to have better quality then the
video conversions. 

Yes, this is generally going to be the case due to the design of the models. The image models are going to be more colorful, and the video model is going to be more conservative (trade offs of interestingness of colors vs stability). 

>Not really seeing any difference with changing from the default setting of 21.

There's two ranges to try here depending on what you're looking for. If you go down to 12-15, you may see better colors; If you go to about 28-30, you'll probably get more stable results.

> For some reason I can't colorize from a file locally instead of using a YouTube URL.

You'll want to run the VideoColorizer.ipynb notebook for this in Jupyter with a local installation. That has the ability to point to local files..  Specifically you set source_url to None and go with the convention described in the comment:

```
#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification
source_url=None
file_name = 'DogShy1926'
```


",set multiple actually ordered go along set code unfortunately could theoretically would require extra work research would also image seem better quality video yes generally going case due design image going colorful video model going conservative trade interestingness color stability really seeing difference default setting two try depending looking go may see better color go probably get stable reason ca colorize file locally instead want run notebook local installation ability point local specifically set none go convention comment note make none read file directly without modification,issue,positive,positive,neutral,neutral,positive,positive
1546898594,"Hey Jason,

I ran into another issue. For some reason I can't colorize from a file
locally instead of using a YouTube URL. It runs, creates the colorized
version but then right when it's done or sometime after the no audio file
is created anyways it deletes the file and then gives an error message
stating the file does not exist!  It's really annoying because I have to
wait several hours for the colorization before it gives me the error.
Looking through the code I'm not seeing where it's being deleted from.  I
believe the issue is from the visualizer.py code it checks for the file and
then states it's not there.  Which it's not because it deleted it.  No
messages stating the encode failed.

If you need more information please let me know. I would have to wait for
it to fail again to get a copy of the message.  Basically though it appears
to be deleting the file.

Regards,

Jeff

On Fri, May 12, 2023 at 9:47 AM Jeff Carleton ***@***.***> wrote:

> Hey Jason,
>
> I can't seem to find any documentation on whether you can run on multiple
> GPU's when running DeOldify.
> Is it possible to set up DeOldify to run from more than one GPU? Possibly
> running lightning?
>
> I also noticed the image conversions seem to have better quality then the
> video conversions.  I do see at times during the videos where the face, or
> top of head with a hat or hand still show in gray through some of the
> frames.
> Are there any other things that I can do to get a better video quality?
> Not really seeing any difference with changing from the default setting of
> 21.  It takes quite a bit of time to convert 200 videos so I would like to
> get the best speed and quality possible before I spend another 200 hours
> converting Three Stooges videos.
>
> Regards,
>
> Jeff
>
>
> On Mon, May 8, 2023 at 6:59 PM Jason Antic ***@***.***>
> wrote:
>
>> Awesome that you're doing that project!
>>
>> Well, I can tell you that you'll definitely need to change the ordering
>> of the imports. This is admittedly tricky. Here's the imports, corrected.
>> Having this ordered incorrectly may be causing it to go to CPU instead of
>> GPU.
>>
>> from deoldify import device
>> from deoldify.device_id import DeviceId
>> #choices:  CPU, GPU0...GPU7
>> device.set(device=DeviceId.GPU0)
>> import fastai
>> from deoldify.visualize import *
>> import torch
>>
>> As far as whether it'll work in Windows- it can work but it's not
>> ""officially"" supported. Meaning enter at your own risk. I've used it on
>> Windows myself as is.
>>
>> —
>> Reply to this email directly, view it on GitHub
>> <https://github.com/jantic/DeOldify/issues/468#issuecomment-1539032605>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AEVP2RESCDH6Y4QRQMUMWW3XFF3EXANCNFSM6AAAAAAXYK4F7Y>
>> .
>> You are receiving this because you modified the open/close state.Message
>> ID: ***@***.***>
>>
>
",hey ran another issue reason ca colorize file locally instead version right done sometime audio file anyways file error message file exist really annoying wait several colorization error looking code seeing believe issue code file encode need information please let know would wait fail get copy message basically though file jeff may jeff wrote hey ca seem find documentation whether run multiple running possible set run one possibly running lightning also image seem better quality video see time face top head hat hand still show gray get better video quality really seeing difference default setting quite bit time convert would like get best speed quality possible spend another converting three jeff mon may antic wrote awesome project well tell definitely need change admittedly tricky corrected ordered incorrectly may causing go instead import device import import import import torch far whether work work officially meaning enter risk used reply directly view id,issue,positive,positive,positive,positive,positive,positive
1545779540,"Hey Jason,

I can't seem to find any documentation on whether you can run on multiple
GPU's when running DeOldify.
Is it possible to set up DeOldify to run from more than one GPU? Possibly
running lightning?

I also noticed the image conversions seem to have better quality then the
video conversions.  I do see at times during the videos where the face, or
top of head with a hat or hand still show in gray through some of the
frames.
Are there any other things that I can do to get a better video quality?
Not really seeing any difference with changing from the default setting of
21.  It takes quite a bit of time to convert 200 videos so I would like to
get the best speed and quality possible before I spend another 200 hours
converting Three Stooges videos.

Regards,

Jeff


On Mon, May 8, 2023 at 6:59 PM Jason Antic ***@***.***> wrote:

> Awesome that you're doing that project!
>
> Well, I can tell you that you'll definitely need to change the ordering of
> the imports. This is admittedly tricky. Here's the imports, corrected.
> Having this ordered incorrectly may be causing it to go to CPU instead of
> GPU.
>
> from deoldify import device
> from deoldify.device_id import DeviceId
> #choices:  CPU, GPU0...GPU7
> device.set(device=DeviceId.GPU0)
> import fastai
> from deoldify.visualize import *
> import torch
>
> As far as whether it'll work in Windows- it can work but it's not
> ""officially"" supported. Meaning enter at your own risk. I've used it on
> Windows myself as is.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/468#issuecomment-1539032605>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AEVP2RESCDH6Y4QRQMUMWW3XFF3EXANCNFSM6AAAAAAXYK4F7Y>
> .
> You are receiving this because you modified the open/close state.Message
> ID: ***@***.***>
>
",hey ca seem find documentation whether run multiple running possible set run one possibly running lightning also image seem better quality video see time face top head hat hand still show gray get better video quality really seeing difference default setting quite bit time convert would like get best speed quality possible spend another converting three jeff mon may antic wrote awesome project well tell definitely need change admittedly tricky corrected ordered incorrectly may causing go instead import device import import import import torch far whether work work officially meaning enter risk used reply directly view id,issue,positive,positive,positive,positive,positive,positive
1539134523,"Have one more question... In the code you provided above... 

from deoldify import device
from deoldify.device_id import DeviceId
#choices:  CPU, GPU0...GPU7
device.set(device=DeviceId.GPU0)
import fastai
from deoldify.visualize import *
import torch

Can you set multiple GPU's? I actually just ordered a K80 to go along with my RTX2070.",one question code provided import device import import import import torch set multiple actually ordered go along,issue,negative,neutral,neutral,neutral,neutral,neutral
1539065242,"I would consider it a benefit, not a bug. Takes forever, but someone with just a CPU and no GPU can at least try it if they wish. :)
Maybe add some comment to the code letting people know they can run CPU. The single image conversion would be no biggie time wise even for CPU as well.  Hmm.. When I tried to run Windows, I got some error window from windows pop up like from a library, not from python... Maybe I will investigate a little more.  I have almost everything working under Ubuntu now anyways since I was sorta forced to run Ubuntu to do the conversions.  If I could just figure out the permissions to get Jellyfin to see my external drives I would be running everything I am via Win 10. :)",would consider benefit bug forever someone least try wish maybe add comment code people know run single image conversion would time wise even well tried run got error window pop like library python maybe investigate little almost everything working anyways since forced run could figure get see external would running everything via win,issue,positive,positive,neutral,neutral,positive,positive
1539032605,"Awesome that you're doing that project!

Well, I can tell you that you'll definitely need to change the ordering of the imports.  This is admittedly tricky. Here's the imports, corrected.  Having this ordered incorrectly may be causing it to go to CPU instead of GPU.


```
from deoldify import device
from deoldify.device_id import DeviceId
#choices:  CPU, GPU0...GPU7
device.set(device=DeviceId.GPU0)
import fastai
from deoldify.visualize import *
import torch
```


As far as whether it'll work in Windows- it can work but it's not ""officially"" supported. Meaning enter at your own risk. I've used it on Windows myself as is.",awesome project well tell definitely need change admittedly tricky corrected ordered incorrectly may causing go instead import device import import import import torch far whether work work officially meaning enter risk used,issue,positive,positive,positive,positive,positive,positive
1538944940,"I believe it was this script.. I made a few..  Since I was copying from the notebook I am thinking I just missed something or the notebook does some initialization you don't get from terminal.  I quite frankly was just happy I finally got it working. :) Thanks for the great tool. If your curious as to my use, right now I am colorizing all The Three Stooges episodes and some old B&W movies for my YouTube channel.  

You can check it out on my channel at: https://www.youtube.com/channel/UC_KcaNqHfYGedpSdsEmi6tw  

The Three Stooges playlist for colorized episodes: https://www.youtube.com/playlist?list=PLJ3OakWWbaTEU193JRdSi2DWKLSrMvSYf

Script attached as yt.txt.  Curious why it doesn't work in Windows. I did get some Windows error after executing once I got everything setup correctly.  Seems the library is not functioning under Windows correctly?

[yt.txt](https://github.com/jantic/DeOldify/files/11424474/yt.txt)


",believe script made since notebook thinking something notebook get terminal quite frankly happy finally got working thanks great tool curious use right three old channel check channel three script attached curious work get error got everything setup correctly library correctly,issue,positive,positive,positive,positive,positive,positive
1538860407,"Thanks for the update. I am curious though, can you post the script you ran where it ran on CPU instead of GPU? I suspect I may be able to identify the problem quite readily.",thanks update curious though post script ran ran instead suspect may able identify problem quite readily,issue,negative,positive,positive,positive,positive,positive
1537519593,"Sorry, Saw the comment about Windows not being supported.  I resolved my issue by installing on Ubuntu 22.04.
Also a note for people on Ubuntu, I found the GPU did not work outside the notebook.  Would run from CPU and take around 18 hours!  If I ran jupyter lab then the GPU was recognized.  Takes about 62-65 minutes for same 15 minute video on RTX2070
",sorry saw comment resolved issue also note people found work outside notebook would run take around ran lab minute video,issue,negative,negative,negative,negative,negative,negative
1514877998,"I'd strongly recommend using the Colab notebooks linked to on the readme, but if you want to do a local install, follow [this](https://github.com/jantic/DeOldify#your-own-machine-not-as-easy). You may have success on Windows if that's your target but it's not -officially- supported.",strongly recommend linked want local install follow may success target,issue,positive,positive,positive,positive,positive,positive
1513878680,"where can I find instructions on how to run your program on a computer on my own? I have little experience, but I can do it if there is an instruction",find run program computer little experience instruction,issue,negative,negative,negative,negative,negative,negative
1513272980,"If you're asking with respect to Ariel Replicate, it appears not.  If you're talking about DeOldify calls themselves (colorizer.plot_transformed_image, colorizer.plot_transformed_image_from_url, etc), you can pass watermarked=False.",respect ariel replicate talking pas,issue,negative,neutral,neutral,neutral,neutral,neutral
1512955925,is it possible to call the program and pass it a parameter to remove the logo?,possible call program pas parameter remove,issue,negative,neutral,neutral,neutral,neutral,neutral
1512226269,"This is a tricky issue for sure. The reason why it's there to begin with is to convey that the colorization is made by AI and not representative of real colors. It depends on who you talk to, but this is a controversial subject. I err on the side of caution on this ethically, so it defaults to using the watermark. On the other-hand, I know full well that frankly most users don't want it showing up on their renders, so you can toggle it off.

As far as it showing up without being able to toggle it off on Ariel Replicate renders- that's the designer's choice really, and out of my hands.

So I'm going to keep the code as is on this. ",tricky issue sure reason begin convey colorization made ai representative real color talk controversial subject err side caution ethically watermark know full well frankly want showing toggle far showing without able toggle ariel replicate designer choice really going keep code,issue,negative,positive,positive,positive,positive,positive
1511693288,"> Всем привет!Я вообще не программист,мучался 2 дня с такой же проблемой,но нашёл для себя решение.Даже на github зарегистрировался.Может кому поможет)))
> 
> 1. Файл - Сохранить копию на диске
> 2. После строки   !pip install -r colab_requirements.txt   добавить ОТДЕЛЬНЫХ 3 кода.
>    pip uninstall ffmpeg
>    pip uninstall ffmpeg-python
>    pip install ffmpeg-python
>    ![1](https://user-images.githubusercontent.com/87704893/126325084-2a096b0f-1bee-45b2-a3ce-81d617da5198.jpg)
> 
> 3.После подключения согласиться 2 раза на удаление ""pip uninstall ffmpeg"" и ""pip uninstall ffmpeg-python"" ![2](https://user-images.githubusercontent.com/87704893/126326228-bbe1cde9-62c4-4b12-a7a8-efc5fe719667.jpg) ![3](https://user-images.githubusercontent.com/87704893/126326254-71079d7d-7a53-4511-b847-ed28c5c99054.jpg)
> 
> 4.Установить ""pip install ffmpeg-python"" ![4](https://user-images.githubusercontent.com/87704893/126327012-cd08f83e-104c-4a74-a231-5d0c6661a09c.jpg)
> 
> 5.Дальше как обычно.
> 
> И ещё нашёл немного модифицированный код,который добавляет возможность загружать видеофайлы прямо с компьютера,надеюсь автор тоже добавит такую функцию )))

WOW, I do not know why this worked but it did, so thank you!",pip install pip pip pip install pip pip pip install wow know worked thank,issue,positive,positive,neutral,neutral,positive,positive
1510428275,I just made a small fix and I think you should be good to go now.  Unfortunately I'm not in a position to formally test this currently but I'm pretty confident in the change. My apologies for having to deal with this!,made small fix think good go unfortunately position formally test currently pretty confident change deal,issue,positive,positive,positive,positive,positive,positive
1506202284,"@jantic 
AttributeError Traceback (most recent call last)
Cell In[53], line 1
----> 1 do_fit(‘v1.1’, slice(lr*10))

Cell In[52], line 4, in do_fit(save_name, lrs, pct_start)
2 learn.fit_one_cycle(1, lrs, pct_start=pct_start)
3 learn.save(save_name)
----> 4 learn.show_results()

File /fastai/basic_train.py:407, in Learner.show_results(self, ds_type, rows, **kwargs)
405 preds = self.pred_batch(ds_type)
406 *self.callbacks,rec_cpu = self.callbacks
→ 407 x,y = rec_cpu.input,rec_cpu.target
408 norm = getattr(self.data,‘norm’,False)
409 if norm:

AttributeError: ‘RecordOnCPU’ object has no attribute ‘input’

I get the above error using learn.show_results()",recent call last cell line slice cell line file self norm norm false norm object attribute input get error,issue,negative,negative,negative,negative,negative,negative
1505389273,"Unfortunately I won't be able to dig into this without further context/information.  What would help is to paste a stack trace, and to provide some information on what you're doing. In particular anything you've done differently, even if it sounds trivial. ",unfortunately wo able dig without would help paste stack trace provide information particular anything done differently even trivial,issue,negative,positive,positive,positive,positive,positive
1499345577,"It's been quite a while since I made the decision to explicitly do that. But if I were to guess my exact intentions:  It's two fold:

1. Be explicit about the requirement to unfreeze for each training step for readability and clarity (even if it's technically redundant).
2. Make the code robust to stops/resumes, esp since this is a Jupyter notebook. That is, it's making it so that if you just want to run a subset of cells to get caught up to your checkpoint, the necessary steps are mostly self-contained at the current training step. 

I think that's pretty much it. ",quite since made decision explicitly guess exact two fold explicit requirement unfreeze training step readability clarity even technically redundant make code robust since notebook making want run subset get caught necessary mostly current training step think pretty much,issue,positive,positive,neutral,neutral,positive,positive
1491035664,"I should mention too- any changes in batch size should be accompanied by a proportionate change in learning rate.  For example- if 2x lower batch size, then 2x lower learning rate. Just doing that if you haven't already might address your issue but I will say that going lower than the already very low batch sizes I used here will probably not be great.",mention batch size proportionate change learning rate lower batch size lower learning rate already might address issue say going lower already low batch size used probably great,issue,negative,positive,positive,positive,positive,positive
1491032996,"Lowering the batch size will cause this sort of issue too, actually. You could try to lower the learning rate proportionately but really I'd recommend either trying [mixed precision training ](https://docs.fast.ai/callback.fp16.html) (learner.to_fp16()) or securing a larger video card (11GB+ gpu memory. Note that I haven't tried mixed precision here on this particular project so you may or may not have success with that.",lowering batch size cause sort issue actually could try lower learning rate proportionately really recommend either trying mixed precision training video card memory note tried mixed precision particular project may may success,issue,positive,positive,positive,positive,positive,positive
1489592459,"@jantic Thanks for your prompt reply.I only changed the batch size due to low memory.Others I'm sure that I followed the training steps verbatim.Does the small batch size affect the training results?
",thanks prompt batch size due low sure training small batch size affect training,issue,negative,positive,neutral,neutral,positive,positive
1488826042,That's basically the sort of thing you see when you've busted your model weights during training. Did you do anything different or are you following the training steps verbatim? I can tell you you're more likely to run into this if you're attempting to train using mixed precision.,basically sort thing see busted model training anything different following training verbatim tell likely run train mixed precision,issue,negative,neutral,neutral,neutral,neutral,neutral
1488820617,"Years ago I saw this and figured it wasn't a big deal and just left it be, even though it's weird. My reasoning was that it's normalizing luminance of an already b/w image (so the channel ordering shouldn't matter much), and the chroma conversion is effectively idempotent. I swear I tested it, but I just did again, and boy was I wrong!

Before/After:
![BGRC_RGBL](https://user-images.githubusercontent.com/179759/228584548-1f36aebe-dfb2-49a1-a7cf-2f50e968cf8c.jpg)
![RGBC_RGBL](https://user-images.githubusercontent.com/179759/228584568-0a4dcbe3-dc66-4f6e-ab81-807e4d88402c.jpg)

I honestly don't know how I let this go for so long but thank you for pointing this out! Fix is in.",ago saw figured big deal left even though weird reasoning luminance already image channel matter much chroma conversion effectively idempotent swear tested boy wrong honestly know let go long thank pointing fix,issue,positive,positive,neutral,neutral,positive,positive
1468476812,"This thread should help:  [https://stackoverflow.com/questions/71248521/why-numexpr-defaulting-to-8-threads-warning-message-shown-in-python](https://stackoverflow.com/questions/71248521/why-numexpr-defaulting-to-8-threads-warning-message-shown-in-python)

I wouldn't necessarily worry about it but you can certainly experiment with it.

",thread help would necessarily worry certainly experiment,issue,negative,positive,positive,positive,positive,positive
1456326458,"Unfortunately I got rid of this functionality last year as it was a bit much to maintain and it diverged from the vision of keeping this repo strictly oriented around sharing research.

The last commit before I got rid of it was here. It wasn't working 100% at that point, but I'm sure it can be fixed: https://github.com/jantic/DeOldify/commit/43a12a1d7755f09919a5c558a3d4f22af5b66a40

You can get that specific commit following these instructions:  https://stackoverflow.com/questions/31462683/git-pull-till-a-particular-commit",unfortunately got rid functionality last year bit much maintain vision keeping strictly around research last commit got rid working point sure fixed get specific commit following,issue,positive,positive,neutral,neutral,positive,positive
1455259314,"You actually don't need to worry about that.  It's purposefully empty to create an empty dataset. It was a quick hack on my part, admittedly.",actually need worry purposefully empty create empty quick hack part admittedly,issue,negative,positive,neutral,neutral,positive,positive
1448220625,"> This isn't my repo, but someone did anime using a fork of DeOldify here: https://github.com/Dakini/AnimeColorDeOldify. Not sure if that'll be useful but basically what I can tell you is that I don't have good instructions for you on this other than to start with what I did and tweak from there. If it's a much smaller dataset than Imagenet you're going to probably have to increase the number of epochs accordingly. But ultimately it'll really boil down to trial and error and I'd just consider both mine and Dakini's repos a good start.

Thank you for your recommendation! 
Hope you have a good day! ",someone anime fork sure useful basically tell good start tweak much smaller going probably increase number accordingly ultimately really boil trial error consider mine good start thank recommendation hope good day,issue,positive,positive,positive,positive,positive,positive
1446956883,"> Well I can save you a bit of time here: DeOldify won't help to improve that sort of video. It'll just try to re-colorize it and the results won't look right. I'd recommend looking into something like Topaz for what you're asking for, here: https://www.topazlabs.com

thank you. sad :/",well save bit time wo help improve sort video try wo look right recommend looking something like topaz thank sad,issue,positive,negative,negative,negative,negative,negative
1446410258,"Well I can save you a bit of time here:  DeOldify won't help to improve that sort of video. It'll just try to re-colorize it and the results won't look right. I'd recommend looking into something like Topaz for what you're asking for, here: [https://www.topazlabs.com](https://www.topazlabs.com)",well save bit time wo help improve sort video try wo look right recommend looking something like topaz,issue,positive,positive,positive,positive,positive,positive
1445213197,"This isn't my repo, but someone did anime using a fork of DeOldify here:  [https://github.com/Dakini/AnimeColorDeOldify](https://github.com/Dakini/AnimeColorDeOldify ).  Not sure if that'll be useful but basically what I can tell you is that I don't have good instructions for you on this other than to start with what I did and tweak from there. If it's a much smaller dataset than Imagenet you're going to probably have to increase the number of epochs accordingly. But ultimately it'll really boil down to trial and error and I'd just consider both mine and Dakini's repos a good start.",someone anime fork sure useful basically tell good start tweak much smaller going probably increase number accordingly ultimately really boil trial error consider mine good start,issue,positive,positive,positive,positive,positive,positive
1445155210,"> @Ssakura-go This section in the readme should help: https://github.com/jantic/DeOldify#pretrained-weights

Sorry for disturbing you again. I have learned from all the sections via the link you shared.  But I found that I haven't solved the problem yet. I want to share with you my situation and wonder if there exists any solution you know: My task requires me to train the model you shared on my cartoon images (which is different from natural images in the ImageNet dataset). ",section help sorry disturbing learned via link found problem yet want share situation wonder solution know task train model cartoon different natural,issue,negative,negative,negative,negative,negative,negative
1443270431,"> Oops! I've added this back to the project. It's in the root folder as ./models . Sorry about that.

Hi, dear contributor, 

I met with the same problem and I only found the file [.gitkeep] in the root folder [./models]. Could you please tell me how to get the code?",added back project root folder sorry hi dear contributor met problem found file root folder could please tell get code,issue,negative,negative,negative,negative,negative,negative
1437222765,"If you're using Colab, you'll basically have an effective practical limit of about a minute or less in length. Otherwise you'll tend to run into reliability issues (Colab limitations). With local installs, you can go as long as you want.

The format DeOldify is expecting for videos is mp4. ",basically effective practical limit minute le length otherwise tend run reliability local go long want format,issue,positive,positive,positive,positive,positive,positive
1436042609,Well if you're changing the dataset then basically it's going to be a trial and error exercise for you. I can't really help you there beyond just saying that if it's a smaller dataset your probably going to have to have more epochs and the opposite if it's a bigger dataset. I doubt you'd have to change other parameters but ultimately this is all an empirically driven craft and I wouldn't take anything I've said or done as anything other than tentative!,well basically going trial error exercise ca really help beyond saying smaller probably going opposite bigger doubt change ultimately driven craft would take anything said done anything tentative,issue,negative,positive,neutral,neutral,positive,positive
1436041543,"Yeah unfortunately there are some cases where DeOldify just totally falls on its face and this is one of them. The best results you can get with it are attained by lowering the render_factor all the way down to 8 or 10. Generally if you're lacking in color that's your best bet- lower the render_factor. But even then in this case it just renders the floor red.
![download](https://user-images.githubusercontent.com/179759/219963476-df5ff322-5dcb-4311-b895-b655774dd9e4.png)
",yeah unfortunately totally face one best get lowering way generally color best lower even case floor red,issue,positive,positive,positive,positive,positive,positive
1435807281,"Thank you for your attention. And in the ColorizeTrainingStable.ipynb, if the parameters in the function of '.fit_one_cycle()' need to be adjusted, such as the epochs or max_lr, when the training dataset changes.",thank attention function need training,issue,negative,neutral,neutral,neutral,neutral,neutral
1435418660,The training notebooks should have the correct settings built in already. Just make sure to read the instructions on the GAN training part in particular.  Is there a specific notebook/cell you're wondering about?,training correct built already make sure read gan training part particular specific wondering,issue,negative,positive,positive,positive,positive,positive
1430858599,"Totally understandable.
Feel free to merge only the link to the demo here #452 if you're interested.",totally understandable feel free merge link interested,issue,positive,positive,positive,positive,positive,positive
1430382744,"There's not too much information to go by here so far so it'd help to know what you're running and the larger error message you're getting (stack trace). But at first glance this looks like it could be an empty/invalid/non existent image it's trying to run on. If it's a url you're using make sure it's the direct link to the photo as opposed to just the page hosting the photo.  In Imgur, for example, you'd have to right click and ""copy image address"" as opposed to just using the url of the page with the image on it.  e.g. for this page,[https://imgur.com/t/old_photo/LMkLq87]( https://imgur.com/t/old_photo/LMkLq87), you'd wind up getting this direct url that you would actually use to colorize:  
[https://i.imgur.com/v5t4V6V.png](https://i.imgur.com/v5t4V6V.png)",much information go far help know running error message getting stack trace first glance like could existent image trying run make sure direct link photo opposed page hosting photo example right click copy image address opposed page image page wind getting direct would actually use colorize,issue,positive,positive,positive,positive,positive,positive
1427223035,"Yeah just add this to the video colorization call:

watermarked=False",yeah add video colorization call,issue,negative,neutral,neutral,neutral,neutral,neutral
1427064604,"> Good to know!

Hey, I found there is a small watermark like a palette in the lower left corner of the colorized video. Can I remove it?
![image](https://user-images.githubusercontent.com/125037628/218321255-dfeddd63-e470-4509-bf3f-78dd58fa53c6.png)
",good know hey found small watermark like palette lower left corner video remove image,issue,negative,positive,positive,positive,positive,positive
1426924572,"> You should be able to resolve this by following these directions for the D://DeOldify folder: https://answers.microsoft.com/en-us/windows/forum/all/permissionerror-errno-13-permission-denied-how-to/1b61d264-4df4-4bfc-b1a9-1e4808f23ee5 .

Hey!! I think I've known the reason, which is my hard disk. I just copy the folder to another hard disk, it worked! However, I still didn't find why the old hard disk could not write. Whatever, now the program is running well, and colorizing the video. Awesome!!",able resolve following folder hey think known reason hard disk copy folder another hard disk worked however still find old hard disk could write whatever program running well video awesome,issue,positive,positive,positive,positive,positive,positive
1426917016,"> You should be able to resolve this by following these directions for the D://DeOldify folder: https://answers.microsoft.com/en-us/windows/forum/all/permissionerror-errno-13-permission-denied-how-to/1b61d264-4df4-4bfc-b1a9-1e4808f23ee5 .

Still not worked, should I re-install the environment and try it again?",able resolve following folder still worked environment try,issue,negative,positive,positive,positive,positive,positive
1426896148,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1426896141,I'm going to hold off on this and wait until 2.0 is stable. These releases take a bit of testing and often require fixes to use it (particularly for the fork of fastai I'm maintaining).,going hold wait stable take bit testing often require use particularly fork,issue,negative,positive,positive,positive,positive,positive
1426894503,"> Oops sorry I forgot the re-downloading check. Hold fire, I'll deal with that ASAP.

There hasn't been any activity after this. Want to go ahead with this or do you want me to close this out?",sorry forgot check hold fire deal activity want go ahead want close,issue,negative,negative,negative,negative,negative,negative
1426894021,"I finally got settled after the move and can now review these things. Since you won't have the time to do the likely back and forth here, I'll close this. Thanks!",finally got settled move review since wo time likely back forth close thanks,issue,negative,positive,neutral,neutral,positive,positive
1426892935,"Hello @ArielReplicate . @alexandrevicenzi is correct (thanks Alexandre!).  This is of course a tough call because it's not easy to say ""no"" to something you clearly put thought and time into. But as a general rule I'm trying to keep this repo as simple as possible and not add any additional features, straying beyond the ""sharing research"" objective. It's a fuzzy line for sure. Making this decision means less for the users to navigate and it keeps maintenance simple. Regardless thanks for creating this and I do encourage you to host this elsewhere!",hello correct thanks course tough call easy say something clearly put thought time general rule trying keep simple possible add additional beyond research objective fuzzy line sure making decision le navigate maintenance simple regardless thanks encourage host elsewhere,issue,positive,positive,neutral,neutral,positive,positive
1426891989,You should  be able to resolve this by following these directions for the D://DeOldify folder:  https://answers.microsoft.com/en-us/windows/forum/all/permissionerror-errno-13-permission-denied-how-to/1b61d264-4df4-4bfc-b1a9-1e4808f23ee5 . ,able resolve following folder,issue,negative,positive,positive,positive,positive,positive
1426553064,"Thank you for your contribution, but consider the following first.

What is wrong with Colab? The easiest way to run DeOldifty.

Docker support existed before and it was removed recently, see commit https://github.com/jantic/DeOldify/commit/d5a716bd5ad88661d7637f066c674d40f736566b. It seems to me it is very unlikely to be added back.

You can create a Docker image and publish it to Docker Hub or any other place without adding any sources to this project.

Adding support for Docker and other platforms in this project means that @jantic will need to maintain in case there are bugs. Review the section [A Statement on Open Source Support](https://github.com/jantic/DeOldify#a-statement-on-open-source-support).

The final decision on this is on @jantic, but consider my thoughts.",thank contribution consider following first wrong easiest way run docker support removed recently see commit unlikely added back create docker image publish docker hub place without project support docker project need maintain case review section statement open source support final decision consider,issue,positive,negative,neutral,neutral,negative,negative
1426354098,"Hey guys, it's been over a month since I worked on this and I have moved on to other things. I understand this is difficult to merge so would understand if you just closed this since its unlikely I'll have time to go back and break this out to separate commits.",hey month since worked understand difficult merge would understand closed since unlikely time go back break separate,issue,negative,negative,negative,negative,negative,negative
1425809774,"Hello, I have an issue when I run cell2 in local, there are Errors: Permission Error:13
`PermissionError                           Traceback (most recent call last)
Cell In[2], line 1
----> 1 from deoldify.visualize import *
      2 plt.style.use('dark_background')
      3 import warnings

File D:\Deoldify\DeOldify-master\deoldify\visualize.py:8
      6 from PIL import Image
      7 import ffmpeg
----> 8 import yt_dlp as youtube_dl
      9 import gc
     10 import requests

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\__init__.py:19
     17 from .compat import compat_shlex_quote
     18 from .cookies import SUPPORTED_BROWSERS, SUPPORTED_KEYRINGS
---> 19 from .downloader.external import get_external_downloader
     20 from .extractor import list_extractor_classes
     21 from .extractor.adobepass import MSO_INFO

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\downloader\__init__.py:26
     24 from .common import FileDownloader
     25 from .dash import DashSegmentsFD
---> 26 from .external import FFmpegFD, get_external_downloader
     27 from .f4m import F4mFD
     28 from .fc2 import FC2LiveFD

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\downloader\external.py:12
     10 from .fragment import FragmentFD
     11 from ..compat import functools
---> 12 from ..postprocessor.ffmpeg import EXT_TO_OUT_FORMATS, FFmpegPostProcessor
     13 from ..utils import (
     14     Popen,
     15     RetryManager,
   (...)
     29     traverse_obj,
     30 )
     33 class Features(enum.Enum):

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\postprocessor\__init__.py:38
     35 from .xattrpp import XAttrMetadataPP
     36 from ..plugins import load_plugins
---> 38 _PLUGIN_CLASSES = load_plugins('postprocessor', 'PP')
     41 def get_postprocessor(key):
     42     return globals()[key + 'PP']

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\plugins.py:135, in load_plugins(name, suffix)
    132 def load_plugins(name, suffix):
    133     classes = {}
--> 135     for finder, module_name, _ in iter_modules(name):
    136         if any(x.startswith('_') for x in module_name.split('.')):
    137             continue

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\plugins.py:119, in iter_modules(subpackage)
    117 fullname = f'{PACKAGE_NAME}.{subpackage}'
    118 with contextlib.suppress(ModuleNotFoundError):
--> 119     pkg = importlib.import_module(fullname)
    120     yield from pkgutil.iter_modules(path=pkg.__path__, prefix=f'{fullname}.')

File C:\ProgramData\Anaconda3\envs\deoldify\lib\importlib\__init__.py:127, in import_module(name, package)
    125             break
    126         level += 1
--> 127 return _bootstrap._gcd_import(name[level:], package, level)

File <frozen importlib._bootstrap>:1014, in _gcd_import(name, package, level)

File <frozen importlib._bootstrap>:991, in _find_and_load(name, import_)

File <frozen importlib._bootstrap>:971, in _find_and_load_unlocked(name, import_)

File <frozen importlib._bootstrap>:914, in _find_spec(name, path, target)

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\plugins.py:96, in PluginFinder.find_spec(self, fullname, path, target)
     93 if fullname not in self.packages:
     94     return None
---> 96 search_locations = self.search_locations(fullname)
     97 if not search_locations:
     98     return None

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\plugins.py:88, in PluginFinder.search_locations(self, fullname)
     86     elif path.name and any(path.with_suffix(suffix).is_file() for suffix in {'.zip', '.egg', '.whl'}):
     87         with contextlib.suppress(FileNotFoundError):
---> 88             if parts in dirs_in_zip(path):
     89                 locations.add(str(candidate))
     90 return locations

File C:\ProgramData\Anaconda3\envs\deoldify\lib\site-packages\yt_dlp\plugins.py:37, in dirs_in_zip(archive)
     35 @functools.cache
     36 def dirs_in_zip(archive):
---> 37     with ZipFile(archive) as zip:
     38         return set(itertools.chain.from_iterable(
     39             Path(file).parents for file in zip.namelist()))

File C:\ProgramData\Anaconda3\envs\deoldify\lib\zipfile.py:1251, in ZipFile.__init__(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)
   1249 while True:
   1250     try:
-> 1251         self.fp = io.open(file, filemode)
   1252     except OSError:
   1253         if filemode in modeDict:

PermissionError: [Errno 13] Permission denied: 'D:\\Deoldify\\DeOldify-master'`",hello issue run cell local permission error recent call last cell line import import file import image import import import import file import import import import import file import import import import import file import import import import class file import import key return key file name suffix name suffix class finder name continue file subpackage subpackage yield file name package break level return name level package level file frozen name package level file frozen name file frozen name file frozen name path target file self path target return none return none file self suffix suffix path candidate return file archive archive archive zip return set path file file file self file mode compression true try file except permission,issue,negative,positive,neutral,neutral,positive,positive
1424634390,Oops! I've added this back to the project. It's in the root folder as ./models . Sorry about that.,added back project root folder sorry,issue,negative,negative,negative,negative,negative,negative
1423966786,"Too many changes and just 3 commits. Would be nice to split a few things and have at least 7 commits to reflect your changes.

For formatting, you can also use Black and have everything consistent.",many would nice split least reflect also use black everything consistent,issue,negative,positive,positive,positive,positive,positive
1421280901,@seidnerj it's a bit difficult to review the changes in this PR because it contains both formatting fixes as well as functional code changes. It would be much easier to review and merge these changes if this large PR was split into several smaller PRs.,bit difficult review well functional code would much easier review merge large split several smaller,issue,negative,negative,neutral,neutral,negative,negative
1413970601,"Well, thanks for that! Made my day.",well thanks made day,issue,positive,positive,positive,positive,positive,positive
1413967593,"Oh yes if that was actually the first line in the script then that would indeed cause the first issue you were having. 

Now on the slow loading into GPU and memory usage you describe- yeah that sounds pretty normal for the open source version. I'm just telling you based on experience (don't have my local setup here to verify definitively currently). There are ways to speed things up that I didn't incorporate into the open source version- making pickled versions of the loaded models and using those as ""cached"" loaded models is a great way to get a speed up but it comes of course with the pitfalls of pickling.",oh yes actually first line script would indeed cause first issue slow loading memory usage yeah pretty normal open source version telling based experience local setup verify definitively currently way speed incorporate open source making loaded loaded great way get speed come course,issue,positive,positive,positive,positive,positive,positive
1411696932,"This was indeed printed. Actually the first line in my script was 
`from deoldify.visualize import *
`
and moving to the last import removed the print and now the script seems to use the GPU.

However there seem to bee a slow loading into the GPU and memory usage increases slowly towards ~5G where the inference seem to happen?
Is this behaviour normal?

",indeed printed actually first line script import moving last import removed print script use however seem bee slow loading memory usage slowly towards inference seem happen behaviour normal,issue,negative,negative,neutral,neutral,negative,negative
1411423819,"**Your software is fantastic and UNIQUE**
Huge Thanks 

I asked ChatGPT and ChatGPT too shared same **UNIQUE** opinion
Peek [https://www.youtube.com/playlist?list=PLNM_Hi4WMpTZXm2_L2BjFY-kAtkAbV5Vz](https://www.youtube.com/playlist?list=PLNM_Hi4WMpTZXm2_L2BjFY-kAtkAbV5Vz)
I made first 12 and last 3 from few  others

I managed download by patient wait
Then entered and executed cell
```
    from google.colab import files
    files.download('video/result/video.mp4') 
```

I am 72 and a Elvis Presley fan and suddenly saw Jailhouse Rock and Don't be Cruel in so calla AI5K Color!
This made me curious.....

Thanks Again",fantastic unique huge thanks unique opinion peek made first last patient wait executed cell import fan suddenly saw jailhouse rock cruel color made curious thanks,issue,positive,positive,neutral,neutral,positive,positive
1411012794,"It's really not jumping out to me what's wrong here actually. Though I am curious, do you get a print out as expected if you change the code for detecting cuda like this?  I just want to make sure it's -actually- detecting cuda here. If not that could narrow down the issue quite a bit.

```
if torch.cuda.is_available():
    print('GPU detected.')
```",really wrong actually though curious get print change code like want make sure could narrow issue quite bit print,issue,negative,negative,neutral,neutral,negative,negative
1411002383,"Sorry for the very late response- was very busy the last few months. Unfortunately the free Google Colabs will probably continue to be increasingly nerfed as they move from drawing in users to trying to actually make money. So I'm not inclined to create work arounds for this as it's just research code I'm sharing, and the answer to these problems is to either get the paid version or install locally.",sorry late busy last unfortunately free probably continue increasingly move drawing trying actually make money create work research code answer either get version install locally,issue,negative,negative,neutral,neutral,negative,negative
1410993905,Sorry for the latest response. If the video doesn't show up embedded in the notebook as a playable video- which happens often with any clips much longer than 30-60 seconds- then there's instructions there in the Colab on how to navigate to that path it specifies. Did you see that part?,sorry latest response video show notebook playable often clip much longer navigate path see part,issue,negative,positive,neutral,neutral,positive,positive
1410988694,Sorry for the late response. I'd say the only compelling reason to upgrade would be to run on Windows (which doesn't sound like your use case). Otherwise you won't see a difference in quality of outputs and I wouldn't expect much better performance.,sorry late response say compelling reason upgrade would run sound like use case otherwise wo see difference quality would expect much better performance,issue,positive,positive,neutral,neutral,positive,positive
1410985030,Sorry about the wait. I've finally been able to sit down and address this.  You should be good to go now.,sorry wait finally able sit address good go,issue,negative,positive,positive,positive,positive,positive
1384908222,"No worries, just wanted to make sure that this was actually seen by someone. Cheers!",make sure actually seen someone,issue,negative,positive,positive,positive,positive,positive
1384792044,"Hey, sorry about not saying this earlier- I'm in the middle of a move and my pc has been packed away for a while. I won't be able to properly review and test this stuff until another month or so. Thanks!",hey sorry saying middle move away wo able properly review test stuff another month thanks,issue,negative,positive,neutral,neutral,positive,positive
1373118039,"I am just using the included models. So I am not training. Colorizing is
working fine.

On Thu, Jan 5, 2023, 10:38 PM stphtan94117 ***@***.***> wrote:

> hi sir
> may i ask you,how was your train working?
> i always failure on training.
> my computer is windows 10.
> but it can work on colorize.
>
> —
> Reply to this email directly, view it on GitHub
> <https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fjantic%2FDeOldify%2Fissues%2F442%23issuecomment-1373103178&data=05%7C01%7C%7C5cdf56c4bebc492a118408daef978066%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C638085731225537490%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=uzEH19LXLinbOdri%2FHeEkoWWYBnfV3S8rsfPWwkcptM%3D&reserved=0>,
> or unsubscribe
> <https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAXMCGUZLN427LNRNA4NLCFLWQ6HT7ANCNFSM6AAAAAATLQP5IU&data=05%7C01%7C%7C5cdf56c4bebc492a118408daef978066%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C638085731225693721%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=lgHRBVJyIpTg51u7lrvT0Df4FSZ9xR51iKL7w4jZhDM%3D&reserved=0>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
",included training working fine wrote hi sir may ask train working always failure training computer work colorize reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1373103178,"hi sir
may i ask you,how was your train working?
i always failure on training.
my computer is windows 10.
but it can work on colorize.",hi sir may ask train working always failure training computer work colorize,issue,negative,negative,negative,negative,negative,negative
1365043797,Turns out it is the external that decreases the performance of the GPU so I will just lower the render factor.,turn external performance lower render factor,issue,negative,neutral,neutral,neutral,neutral,neutral
1330292613,"Here is attachment!

Sent from Yahoo Mail on Android 
 
  On Tue, Nov 29, 2022 at 3:47, Tom ***@***.***> wrote:   Hello again:
Waited 2 hrs 45 minutes. I just ask to download and it looked like it was not going to do it. It did not but got details on could not fetch. Meaning it could not find the file to download?
A few days ago i had problem with Google chrome not updated. It was really quick on 4:30 files.But was using Mozzila. 
I am attaching details on could not fetch. Can you tell from this what is happening.
Hope you had a good thanksgiving meal and time.
Tom

Sent from Yahoo Mail on Android 
 
  On Thu, Nov 10, 2022 at 14:22, Tom ***@***.***> wrote:   Tried running file at 27 factor versus 31. It was really awful but it gave output. They said 21 is best. Then i will need to go higher factors. Maybe 35?

Sent from Yahoo Mail on Android 
 
  On Tue, Nov 8, 2022 at 16:23, Tom ***@***.***> wrote:   Yes and yes. I would like the two steps included. 
I dont think even with your instructions i can get there. But very kind of you to reply.
Is attach drive and mount drive the same. I might be able to do that?
But command code to copy file to drive would take a lot of learning.
I tried some right clicks on results where it had an upload arrow. Bombed it good!
Thanks.
Tom




Sent from Yahoo Mail on Android 
 
  On Mon, Nov 7, 2022 at 22:02, Gaz ***@***.***> wrote:   


You need to:
   
   - first attach your google drive using the file browser on the left, before you run anything.
   - Then you need to run it on a very small file
   - Then you need to go to the bottom of the file and write the commands that copies the output video into your Google drive (This is awkward without shell access, which you only get on a paid Colab environment)
   - After you've tested that the script works, and your file appears in drive, then you put your big video in and queue up all the steps.
   - If you're lucky, it'll run the process, copy the output file to Google Drive, then your VM will get deleted. If you're unlucky it'll delete your VM halfway through.

Yes it's a complete pain. I've got a pull request with some changes in there to make it easier, but it won't magically fix this issue - it'll make it easier to continue if you manually run a backup part-way through and then manually copy the files back in to continue, so you don't have to start again. But I've not supplied those commands as changes to the notebook.

But as it says in the README, this isn't really a polished product for end-users. You need to understand what it's doing, where the files are, how to debug the problems and so on. This is what this issue that you're posting to is about, it's painful to run on colab for free even if you're a developer who knows what they're doing.

That said, your experience with it make me think that it'd be better to just have the ""attach google drive"" and ""copy to drive"" steps in there. When I have time I'll add them to my pull request and you can try it out if you like

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
  
  
  
",attachment sent yahoo mail android tue wrote hello ask like going got could fetch meaning could find file day ago problem chrome really quick could fetch tell happening hope good thanksgiving meal time sent yahoo mail android wrote tried running file factor versus really awful gave output said best need go higher maybe sent yahoo mail android tue wrote yes yes would like two included dont think even get kind reply attach drive mount drive might able command code copy file drive would take lot learning tried right arrow bombed good thanks sent yahoo mail android mon gaz wrote need first attach drive file browser left run anything need run small file need go bottom file write output video drive awkward without shell access get environment tested script work file drive put big video queue lucky run process copy output file drive get unlucky delete halfway yes complete pain got pull request make easier wo magically fix issue make easier continue manually run backup manually copy back continue start notebook really polished product need understand issue posting painful run free even developer said experience make think better attach drive copy drive time add pull request try like reply directly view id,issue,positive,positive,positive,positive,positive,positive
1330283962,"Hello again:
Waited 2 hrs 45 minutes. I just ask to download and it looked like it was not going to do it. It did not but got details on could not fetch. Meaning it could not find the file to download?
A few days ago i had problem with Google chrome not updated. It was really quick on 4:30 files.But was using Mozzila. 
I am attaching details on could not fetch. Can you tell from this what is happening.
Hope you had a good thanksgiving meal and time.
Tom

Sent from Yahoo Mail on Android 
 
  On Thu, Nov 10, 2022 at 14:22, Tom ***@***.***> wrote:   Tried running file at 27 factor versus 31. It was really awful but it gave output. They said 21 is best. Then i will need to go higher factors. Maybe 35?

Sent from Yahoo Mail on Android 
 
  On Tue, Nov 8, 2022 at 16:23, Tom ***@***.***> wrote:   Yes and yes. I would like the two steps included. 
I dont think even with your instructions i can get there. But very kind of you to reply.
Is attach drive and mount drive the same. I might be able to do that?
But command code to copy file to drive would take a lot of learning.
I tried some right clicks on results where it had an upload arrow. Bombed it good!
Thanks.
Tom




Sent from Yahoo Mail on Android 
 
  On Mon, Nov 7, 2022 at 22:02, Gaz ***@***.***> wrote:   


You need to:
   
   - first attach your google drive using the file browser on the left, before you run anything.
   - Then you need to run it on a very small file
   - Then you need to go to the bottom of the file and write the commands that copies the output video into your Google drive (This is awkward without shell access, which you only get on a paid Colab environment)
   - After you've tested that the script works, and your file appears in drive, then you put your big video in and queue up all the steps.
   - If you're lucky, it'll run the process, copy the output file to Google Drive, then your VM will get deleted. If you're unlucky it'll delete your VM halfway through.

Yes it's a complete pain. I've got a pull request with some changes in there to make it easier, but it won't magically fix this issue - it'll make it easier to continue if you manually run a backup part-way through and then manually copy the files back in to continue, so you don't have to start again. But I've not supplied those commands as changes to the notebook.

But as it says in the README, this isn't really a polished product for end-users. You need to understand what it's doing, where the files are, how to debug the problems and so on. This is what this issue that you're posting to is about, it's painful to run on colab for free even if you're a developer who knows what they're doing.

That said, your experience with it make me think that it'd be better to just have the ""attach google drive"" and ""copy to drive"" steps in there. When I have time I'll add them to my pull request and you can try it out if you like

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
  
  
",hello ask like going got could fetch meaning could find file day ago problem chrome really quick could fetch tell happening hope good thanksgiving meal time sent yahoo mail android wrote tried running file factor versus really awful gave output said best need go higher maybe sent yahoo mail android tue wrote yes yes would like two included dont think even get kind reply attach drive mount drive might able command code copy file drive would take lot learning tried right arrow bombed good thanks sent yahoo mail android mon gaz wrote need first attach drive file browser left run anything need run small file need go bottom file write output video drive awkward without shell access get environment tested script work file drive put big video queue lucky run process copy output file drive get unlucky delete halfway yes complete pain got pull request make easier wo magically fix issue make easier continue manually run backup manually copy back continue start notebook really polished product need understand issue posting painful run free even developer said experience make think better attach drive copy drive time add pull request try like reply directly view id,issue,positive,positive,positive,positive,positive,positive
1310901795,"Strange that this hasn't come up before, but it'd just be missing this import:

from deoldify.augs import noisify

I'll quickly add that fix now.",strange come missing import import quickly add fix,issue,negative,positive,neutral,neutral,positive,positive
1310787198,"Tried running file at 27 factor versus 31. It was really awful but it gave output. They said 21 is best. Then i will need to go higher factors. Maybe 35?

Sent from Yahoo Mail on Android 
 
  On Tue, Nov 8, 2022 at 16:23, Tom ***@***.***> wrote:   Yes and yes. I would like the two steps included. 
I dont think even with your instructions i can get there. But very kind of you to reply.
Is attach drive and mount drive the same. I might be able to do that?
But command code to copy file to drive would take a lot of learning.
I tried some right clicks on results where it had an upload arrow. Bombed it good!
Thanks.
Tom




Sent from Yahoo Mail on Android 
 
  On Mon, Nov 7, 2022 at 22:02, Gaz ***@***.***> wrote:   


You need to:
   
   - first attach your google drive using the file browser on the left, before you run anything.
   - Then you need to run it on a very small file
   - Then you need to go to the bottom of the file and write the commands that copies the output video into your Google drive (This is awkward without shell access, which you only get on a paid Colab environment)
   - After you've tested that the script works, and your file appears in drive, then you put your big video in and queue up all the steps.
   - If you're lucky, it'll run the process, copy the output file to Google Drive, then your VM will get deleted. If you're unlucky it'll delete your VM halfway through.

Yes it's a complete pain. I've got a pull request with some changes in there to make it easier, but it won't magically fix this issue - it'll make it easier to continue if you manually run a backup part-way through and then manually copy the files back in to continue, so you don't have to start again. But I've not supplied those commands as changes to the notebook.

But as it says in the README, this isn't really a polished product for end-users. You need to understand what it's doing, where the files are, how to debug the problems and so on. This is what this issue that you're posting to is about, it's painful to run on colab for free even if you're a developer who knows what they're doing.

That said, your experience with it make me think that it'd be better to just have the ""attach google drive"" and ""copy to drive"" steps in there. When I have time I'll add them to my pull request and you can try it out if you like

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
  
",tried running file factor versus really awful gave output said best need go higher maybe sent yahoo mail android tue wrote yes yes would like two included dont think even get kind reply attach drive mount drive might able command code copy file drive would take lot learning tried right arrow bombed good thanks sent yahoo mail android mon gaz wrote need first attach drive file browser left run anything need run small file need go bottom file write output video drive awkward without shell access get environment tested script work file drive put big video queue lucky run process copy output file drive get unlucky delete halfway yes complete pain got pull request make easier wo magically fix issue make easier continue manually run backup manually copy back continue start notebook really polished product need understand issue posting painful run free even developer said experience make think better attach drive copy drive time add pull request try like reply directly view id,issue,positive,positive,positive,positive,positive,positive
1307843723,"Yes and yes. I would like the two steps included. 
I dont think even with your instructions i can get there. But very kind of you to reply.
Is attach drive and mount drive the same. I might be able to do that?
But command code to copy file to drive would take a lot of learning.
I tried some right clicks on results where it had an upload arrow. Bombed it good!
Thanks.
Tom




Sent from Yahoo Mail on Android 
 
  On Mon, Nov 7, 2022 at 22:02, Gaz ***@***.***> wrote:   


You need to:
   
   - first attach your google drive using the file browser on the left, before you run anything.
   - Then you need to run it on a very small file
   - Then you need to go to the bottom of the file and write the commands that copies the output video into your Google drive (This is awkward without shell access, which you only get on a paid Colab environment)
   - After you've tested that the script works, and your file appears in drive, then you put your big video in and queue up all the steps.
   - If you're lucky, it'll run the process, copy the output file to Google Drive, then your VM will get deleted. If you're unlucky it'll delete your VM halfway through.

Yes it's a complete pain. I've got a pull request with some changes in there to make it easier, but it won't magically fix this issue - it'll make it easier to continue if you manually run a backup part-way through and then manually copy the files back in to continue, so you don't have to start again. But I've not supplied those commands as changes to the notebook.

But as it says in the README, this isn't really a polished product for end-users. You need to understand what it's doing, where the files are, how to debug the problems and so on. This is what this issue that you're posting to is about, it's painful to run on colab for free even if you're a developer who knows what they're doing.

That said, your experience with it make me think that it'd be better to just have the ""attach google drive"" and ""copy to drive"" steps in there. When I have time I'll add them to my pull request and you can try it out if you like

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
",yes yes would like two included dont think even get kind reply attach drive mount drive might able command code copy file drive would take lot learning tried right arrow bombed good thanks sent yahoo mail android mon gaz wrote need first attach drive file browser left run anything need run small file need go bottom file write output video drive awkward without shell access get environment tested script work file drive put big video queue lucky run process copy output file drive get unlucky delete halfway yes complete pain got pull request make easier wo magically fix issue make easier continue manually run backup manually copy back continue start notebook really polished product need understand issue posting painful run free even developer said experience make think better attach drive copy drive time add pull request try like reply directly view id,issue,positive,positive,positive,positive,positive,positive
1306556432,"You need to:
* first attach your google drive using the file browser on the left, before you run anything.
* Then you need to run it on a very small file
* Then you need to go to the bottom of the file and write the commands that copies the output video into your Google drive (This is awkward without shell access, which you only get on a paid Colab environment, and you need to know how to write shell commands)
* After you've tested that the script works, and your file appears in drive, then you put your big video in and queue up all the steps.
* If you're lucky, it'll run the process, copy the output file to Google Drive, then your VM will get deleted. If you're unlucky it'll delete your VM halfway through.

Yes it's a complete pain. I've got a pull request with some changes in there to make it easier, but it won't magically fix this issue - it'll make it easier to continue if you manually run a backup part-way through and then manually copy the files back in to continue, so you don't have to start again. But I've not supplied those commands as changes to the notebook.

But as it says in the README, this isn't really a polished product for end-users. You need to understand what it's doing, where the files are, how to debug the problems and so on. This is what this issue that you're posting to is about, it's painful to run on colab for free even if you're a developer who knows what they're doing.

That said, your experience with it make me think that it'd be better to just have the ""attach google drive"" and ""copy to drive"" steps in there. When I have time I'll add them to my pull request and you can try it out if you like",need first attach drive file browser left run anything need run small file need go bottom file write output video drive awkward without shell access get environment need know write shell tested script work file drive put big video queue lucky run process copy output file drive get unlucky delete halfway yes complete pain got pull request make easier wo magically fix issue make easier continue manually run backup manually copy back continue start notebook really polished product need understand issue posting painful run free even developer said experience make think better attach drive copy drive time add pull request try like,issue,positive,positive,neutral,neutral,positive,positive
1306381749,"I just noticed, my audio and colorization are completed . Runtime is terminated. You must select output download after colorization icon stops revolving giving time. The icon gives a couple of arrowed circles then it shows yellow progress. Last 4 attemps it has terminated before at the last moment. Jason said there was a workaround in the instructions in the notebook. 
I looked for it but no luck. If you were able to upload to cloud, would you share how?

Sent from Yahoo Mail on Android 
 
  On Sat, Nov 5, 2022 at 12:51, Tom ***@***.***> wrote:   By the way. Seems to be a waste of Computer resources to run for 2 hrs and then another 1 hr for audio then downloading for another then nothing. You start all over again. I suppose the logic was for small files. Then pay for nothing. I have no friends with graphics cards to suite.
Tom

Sent from Yahoo Mail on Android 
 
  On Sat, Nov 5, 2022 at 12:35, Tom ***@***.***> wrote:   I have researched moving files to drive but I find no clear instructions how to do that. I get a log of could not fetch. Greek to me! I was getting downloads of 9 min files. Now i have been shut down totally. No output. They claim it free but you dont get anything. I miss the red colors. But still quite amazing to me.
Thanks for all.

Sent from Yahoo Mail on Android 
 
  On Fri, Nov 4, 2022 at 20:46, Gaz ***@***.***> wrote:   


@pitt6


If you run out of time before the output video is combined, you lose all your data. Even after every frame has been colourized, you need to start again cause Google crushed your nuts with their dark marketing pattern


Your colab VM can disappear at any time, so probably got deleted. Move your files to Google Drive and download them from there. Or buy colab access. Or don't, because they'll still delete your files.

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
  
  
",audio colorization must select output colorization icon revolving giving time icon couple arrowed yellow progress last last moment said notebook luck able cloud would share sent yahoo mail android sat wrote way waste computer run another audio another nothing start suppose logic small pay nothing graphic suite sent yahoo mail android sat wrote moving drive find clear get log could fetch getting min shut totally output claim free dont get anything miss red color still quite amazing thanks sent yahoo mail android gaz wrote run time output video combined lose data even every frame need start cause crushed dark marketing pattern disappear time probably got move drive buy access still delete reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1304902588,"Oops sorry I forgot the re-downloading check. Hold fire, I'll deal with that ASAP.",sorry forgot check hold fire deal,issue,negative,negative,negative,negative,negative,negative
1304586664,"By the way. Seems to be a waste of Computer resources to run for 2 hrs and then another 1 hr for audio then downloading for another then nothing. You start all over again. I suppose the logic was for small files. Then pay for nothing. I have no friends with graphics cards to suite.
Tom

Sent from Yahoo Mail on Android 
 
  On Sat, Nov 5, 2022 at 12:35, Tom ***@***.***> wrote:   I have researched moving files to drive but I find no clear instructions how to do that. I get a log of could not fetch. Greek to me! I was getting downloads of 9 min files. Now i have been shut down totally. No output. They claim it free but you dont get anything. I miss the red colors. But still quite amazing to me.
Thanks for all.

Sent from Yahoo Mail on Android 
 
  On Fri, Nov 4, 2022 at 20:46, Gaz ***@***.***> wrote:   


@pitt6


If you run out of time before the output video is combined, you lose all your data. Even after every frame has been colourized, you need to start again cause Google crushed your nuts with their dark marketing pattern


Your colab VM can disappear at any time, so probably got deleted. Move your files to Google Drive and download them from there. Or buy colab access. Or don't, because they'll still delete your files.

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
  
",way waste computer run another audio another nothing start suppose logic small pay nothing graphic suite sent yahoo mail android sat wrote moving drive find clear get log could fetch getting min shut totally output claim free dont get anything miss red color still quite amazing thanks sent yahoo mail android gaz wrote run time output video combined lose data even every frame need start cause crushed dark marketing pattern disappear time probably got move drive buy access still delete reply directly view id,issue,positive,positive,neutral,neutral,positive,positive
1304582773,"I have researched moving files to drive but I find no clear instructions how to do that. I get a log of could not fetch. Greek to me! I was getting downloads of 9 min files. Now i have been shut down totally. No output. They claim it free but you dont get anything. I miss the red colors. But still quite amazing to me.
Thanks for all.

Sent from Yahoo Mail on Android 
 
  On Fri, Nov 4, 2022 at 20:46, Gaz ***@***.***> wrote:   


@pitt6


If you run out of time before the output video is combined, you lose all your data. Even after every frame has been colourized, you need to start again cause Google crushed your nuts with their dark marketing pattern


Your colab VM can disappear at any time, so probably got deleted. Move your files to Google Drive and download them from there. Or buy colab access. Or don't, because they'll still delete your files.

—
Reply to this email directly, view it on GitHub, or unsubscribe.
You are receiving this because you were mentioned.Message ID: ***@***.***>
  
",moving drive find clear get log could fetch getting min shut totally output claim free dont get anything miss red color still quite amazing thanks sent yahoo mail android gaz wrote run time output video combined lose data even every frame need start cause crushed dark marketing pattern disappear time probably got move drive buy access still delete reply directly view id,issue,positive,positive,positive,positive,positive,positive
1304360268,"@pitt6 

> If you run out of time before the output video is combined, you lose all your data. Even after every frame has been colourized, you need to start again cause Google crushed your nuts with their dark marketing pattern

Your colab VM can disappear at any time, so probably got deleted. Move your files to Google Drive and download them from there. Or buy colab access. Or don't, because they'll still delete your files.",run time output video combined lose data even every frame need start cause crushed dark marketing pattern disappear time probably got move drive buy access still delete,issue,negative,negative,negative,negative,negative,negative
1304356087,"Sorry it took so long! I've started a new contract and it's been pretty intense over the first month. Finally got around to fixing the parameter changes though, and also added a few more logging things. I've not been using it recently so might have missed some things. I did give it a quick manual test, but I'm a feeble bag of flesh and water; it could do with some test automation!

I'll hopefully have time to do some more old movies soon and look at some perf improvements :)",sorry took long new contract pretty intense first month finally got around fixing parameter though also added logging recently might give quick manual test feeble bag flesh water could test hopefully time old soon look,issue,positive,positive,neutral,neutral,positive,positive
1287297435,"Since it's a certificate issue on DeepAI's end, I'm going to wait this out for them to resolve it. It'll probably be a relatively short term issue.",since certificate issue end going wait resolve probably relatively short term issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1287265432,"I'm also getting the same error, ran everything in order and tried from different accounts and different computers, nothing worked.",also getting error ran everything order tried different different nothing worked,issue,negative,neutral,neutral,neutral,neutral,neutral
1286778372,getting the same error as lashavsdz. Ran all in the order and getting the same error.,getting error ran order getting error,issue,negative,neutral,neutral,neutral,neutral,neutral
1286713472,"hello, I have an issue. when i run: **colorizer = get_image_colorizer(artistic=True)**
I get error like this:

EOFError                                  Traceback (most recent call last)
[<ipython-input-7-d41e8163fe4e>](https://localhost:8080/#) in <module>
----> 1 colorizer = get_image_colorizer(artistic=True)

5 frames
[/usr/local/lib/python3.7/dist-packages/torch/serialization.py](https://localhost:8080/#) in _legacy_load(f, map_location, pickle_module, **pickle_load_args)
    918             ""functionality."")
    919 
--> 920     magic_number = pickle_module.load(f, **pickle_load_args)
    921     if magic_number != MAGIC_NUMBER:
    922         raise RuntimeError(""Invalid magic number; corrupt file?"")

EOFError: Ran out of input

![image](https://user-images.githubusercontent.com/103876658/197165632-c86829ae-6276-4bc0-9204-9f4f267e7ee7.png)
",hello issue run get error like recent call last module functionality raise invalid magic number corrupt file ran input image,issue,negative,neutral,neutral,neutral,neutral,neutral
1259500058,"Hey sorry, just a quick update - I've neglected this for a bit while I'm working on something else, but I will get back to it sometime this week, hopefully tomorrow or thursday",hey sorry quick update bit working something else get back sometime week hopefully tomorrow,issue,negative,negative,neutral,neutral,negative,negative
1249978528,"@bitplane  That all sounds fantastic.  Yeah as far as performance goes- I've had to do a lot of optimization of our more advanced video stuff in our commercial work with my DeOldify startup. Both speed and memory actually- doing everything in memory is quite a challenge, for example.   It's a pretty deep rabbit hole to get ffmpeg pipelines optimized and there's a lot of low hanging fruit. I haven't been in a rush to try optimizing too much for this stuff but please if you want to go ahead!  I can say that the less compression you use on frames, the faster it is and that's potentially an easy win, but you can use up disk space very quickly.  Especially with these Colabs.  Another potential win is using grayscale formats when applicable.

Oh and ""resume"" sounds like great wording.",fantastic yeah far performance lot optimization advanced video stuff commercial work speed memory everything memory quite challenge example pretty deep rabbit hole get lot low hanging fruit rush try much stuff please want go ahead say le compression use faster potentially easy win use disk space quickly especially another potential win applicable oh resume like great wording,issue,positive,positive,positive,positive,positive,positive
1248312630,"@jqueguiner 
`->$:docker build -t deoldify_jupyter -f Dockerfile .`

Any chance you could share your `Dockerfile`?",docker build chance could share,issue,negative,neutral,neutral,neutral,neutral,neutral
1246057375,"Hi Jason, sounds great, thanks for the feedback.

I'll keep hacking away at it in another branch while I'm processing videos and merge here when it's ready for review again, rather than keep force-pushing here and bugging you with notifications! :D

The reason I used restart is because the most natural word to use would be ""continue"", but that's a reserved word in Python. The convention is to stick an underscore on the end of it like `continue_`, but that seems horrible - specially in the notebook world where parameter inspection isn't that great. Maybe calling it ""resume"" instead?

Now I know you're interested in changes I'll have a deeper think about how to make it a bit more beautiful. I'm thinking ~~that since we've got a class I should probably put the flags on the instance rather than pass them as parameters,~~ edit: I misread, scratch that... and add some proper docstrings, fix the unsafe `os.system` calls, use f-strings etc.

I've also had a little hack around with skipping frame extraction but I don't like the way I've implemented it at present, there's a few edge cases to do with restarting after deleting images.

The other things that are bugging me:

1. I'm wasting a lot of time on Colab just building the video itself. I could tar the frames up and push them into drive while using the GPU to process another video, and do the rest on my CPU at home. So I might also make the actual video combination step optional. But it'd need some thought around properly managing the dependencies.
2. There's too many files in a dir to be practical in Jupyter's web UI; it doesn't like more than a thousand and chokes when clicking around to look at the results. I think it'd make sense to break outputs into dirs like ""012345000/012345678.jpg""
3. It feels like the global interpreter lock / single threading is killing me locally. I'm getting about 70% GPU usage according to nvtop while I've got one CPU core running hot - I assume it's processing JPEGs or PIL creating images. If I get round to profiling it (I love performance engineering) I'll raise another PR with changes for that; there could be a +40% perf win there.",hi great thanks feedback keep hacking away another branch merge ready review rather keep reason used restart natural word use would continue reserved word python convention stick underscore end like horrible specially notebook world parameter inspection great maybe calling resume instead know interested think make bit beautiful thinking since got class probably put instance rather pas edit misread scratch add proper fix unsafe use also little hack around skipping frame extraction like way present edge wasting lot time building video could tar push drive process another video rest home might also make actual video combination step optional need thought around properly many practical web like thousand around look think make sense break like like global interpreter lock single killing locally getting usage according got one core running hot assume get round love performance engineering raise another could win,issue,positive,positive,positive,positive,positive,positive
1241273853,"Basically there's two options provided here:  Either you use URLs to sites like YouTube (like you have already done successfully), or you point to a file local to your -server- using the code in the ImageColorizer.ipynb and VideoColorizer.ipynb notebooks as an example. Anything else- like perhaps you'd want to upload to a separate server from a local computer file- that would be beyond the scope of what will be supported here.  That would require work and research on your end. To elaborate- This is, unapologetically, a research repository. Nothing more. Nothing less. I've made a conscious decision to keep this project simple and leave the myriad of potential applications up to others to figure out. ",basically two provided either use like like already done successfully point file local code example anything like perhaps want separate server local computer would beyond scope would require work research end research repository nothing nothing le made conscious decision keep project simple leave myriad potential figure,issue,positive,positive,positive,positive,positive,positive
1230793566,"I'm back from a big trip in Iceland so apologies for the wait. The thought put into this post is certainly appreciated. but what you're speaking of here is a whole other research project far beyond the scope of the DeOldify project. I can tell you though that the most promising direction to incorporate this sort of information was pioneered by Emil Wallner in this newer project:  https://palette.fm/ .  Simply put- building upon language modeling seems to be the key to getting better results, by incorporating more hard earned human knowledge into building the resulting images. That would cover at least a good portion of what you're referring to there with the ""other useful bits of information"" and I think that would go quite far. 

As far as the grayscale information- well unfortunately grayscale can represent multiple color solutions. That's one challenge. And while logical deduction sounds sensible and is to a certain extent, it's the actual implementation of how you'd segment and break that problem up which would be the -very- hard part in my opinion. And it'd still ultimately be at best the work of some heuristics and ""guesses"" due to the grayscale ambiguity problem.

So again- super interesting areas to ponder, but it's definitely a whole other project to tackle all these things. And I think we're just now arriving at potential solutions with the recent multi-modal vision and language modeling stuff that has come out along with diffusion models, etc.",back big trip iceland wait thought put post certainly speaking whole research project far beyond scope project tell though promising direction incorporate sort information project simply building upon language modeling key getting better hard human knowledge building resulting would cover least good portion useful information think would go quite far far well unfortunately represent multiple color one challenge logical deduction sensible certain extent actual implementation segment break problem would hard part opinion still ultimately best work due ambiguity problem super interesting ponder definitely whole project tackle think potential recent vision language modeling stuff come along diffusion,issue,positive,positive,positive,positive,positive,positive
1230769374,"DeOldify is only supported on Linux officially, to keep things simple. If you're to venture into this territory you'll have to modify the source code and get digging on Google/StackOverflow/etc. I haven't tried it but I suspect based on what I've seen from other efforts with M1 that it won't be trivial in practice.",officially keep simple venture territory modify source code get digging tried suspect based seen wo trivial practice,issue,negative,neutral,neutral,neutral,neutral,neutral
1221539037,"*** UPDATE ***

I've run it in local (with the ""run_local.py"" script, it worked somehow, i got the video colorized but i also had the same error code ``` [WinError 2] The system cannot find the specified file ```  at the end of the process. 
But i think i barely know from where the issue is coming from ; **The audio encoding.**
So the issue occur when it's encoding the audio. 
Here is the lines of the processing:

```
Colorizing frame 4791 of 4791
Transferring audio
ffmpeg version 5.1 Copyright (c) 2000-2022 the FFmpeg developers
  built with clang version 14.0.6
  configuration: --prefix=/d/bld/ffmpeg_1660921618181/_h_env/Library --cc=clang.exe --cxx=clang++.exe --nm=llvm-nm --ar=llvm-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libfontconfig --enable-libopenh264 --ld=lld-link --target-os=win64 --enable-cross-compile --toolchain=msvc --host-cc=clang.exe --extra-libs=ucrt.lib --extra-libs=vcruntime.lib --extra-libs=oldnames.lib --strip=llvm-strip --disable-stripping --host-extralibs= --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --pkg-config=/d/bld/ffmpeg_1660921618181/_build_env/Library/bin/pkg-config
  libavutil      57. 28.100 / 57. 28.100
  libavcodec     59. 37.100 / 59. 37.100
  libavformat    59. 27.100 / 59. 27.100
  libavdevice    59.  7.100 / 59.  7.100
  libavfilter     8. 44.100 /  8. 44.100
  libswscale      6.  7.100 /  6.  7.100
  libswresample   4.  7.100 /  4.  7.100
  libpostproc    56.  6.100 / 56.  6.100
Input #0, mov,mp4,m4a,3gp,3g2,mj2, from './test_images/02.mp4':
  Metadata:
    major_brand     : mp42
    minor_version   : 0
    compatible_brands: mp42mp41
    creation_time   : 2022-08-19T21:08:27.000000Z
  Duration: 00:03:19.85, start: 0.000000, bitrate: 20763 kb/s
  Stream #0:0[0x1](eng): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(progressive), 1920x1036 [SAR 1:1 DAR 480:259], 20444 kb/s, 23.98 fps, 23.98 tbr, 24k tbn (default)
    Metadata:
      creation_time   : 2022-08-19T21:08:27.000000Z
      handler_name    : ?Mainconcept Video Media Handler
      vendor_id       : [0][0][0][0]
      encoder         : AVC Coding
  Stream #0:1[0x2](eng): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 317 kb/s (default)
    Metadata:
      creation_time   : 2022-08-19T21:08:27.000000Z
      handler_name    : #Mainconcept MP4 Sound Media Handler
      vendor_id       : [0][0][0][0]
./test_images/temp_30a3f856b36441fd91fc3a5d1615add4.mp3: No such file or directory
Traceback (most recent call last):
  File ""D:\DeOldify\run_local.py"", line 137, in <module>
    model.colorize_video(args)
  File ""D:\DeOldify\run_local.py"", line 102, in colorize_video
    os.remove(tempAudio)
FileNotFoundError: [WinError 2] Le fichier spécifié est introuvable: './test_images/temp_30a3f856b36441fd91fc3a5d1615add4.mp3'
```

**NB: it still work because in the output folder i still get the ""no_audio"" colorized video version.**",update run local script worked somehow got video also error code system find file end process think barely know issue coming audio issue occur audio frame transferring audio version copyright built clang version configuration input duration start stream video main progressive sar dar default video medium handler stream audio stereo default sound medium handler file directory recent call last file line module file line still work output folder still get video version,issue,negative,positive,positive,positive,positive,positive
1221382181,"Okay so at first, after running that script  i got this error: 
ModuleNotFoundError: No module named 'cv2' 
So i tried to run before that:  conda activate deoldify  (which is logical but since i'm a real newbie... ^^ )  and then i run the script, it works like a charm =) 
Thank you very much @prettydeep  i appreciate it !
And thanks for @harisreedhar for this helpful script.",first running script got error module tried run activate logical since real run script work like charm thank much appreciate thanks helpful script,issue,positive,positive,positive,positive,positive,positive
1221371622,"Sorry, i just updated the link.
I just figured out how to download that script, okay i took the RAW form ^^ 
Thank you very much, i'll test it and in a while and tell you how it's going =)
Thank again PrettyDeep ",sorry link figured script took raw form thank much test tell going thank,issue,positive,negative,negative,negative,negative,negative
1221367550,"The URL you list does not show anything.

Just download the python script [here](https://github.com/harisreedhar/DeOldify/blob/run-local/run_local.py) and place it in the root Deoldify folder. Be sure to download it in the ""Raw"" form. Follow the install directions in the readme in the Deoldify repository for the python requirements and the necessary weights. Put your B&W image in the test_images folder. Then from the root Deoldify folder, run...

`python run_local.py --input ./test_images/1.jpg --output ./test_images/ --model artistic`",list show anything python script place root folder sure raw form follow install repository python necessary put image folder root folder run python input output model artistic,issue,negative,positive,positive,positive,positive,positive
1221365576,"Hello Prettydeep =) 
Well, excuse my lack of knowelge  i'm totally new on that and i'm trying to undertand.
So what i understand is that i should process like this:

- Download the zip which is here https://github.com/harisreedhar/DeOldify/tree/run-local and unzip it (let's say in /Downloads folder).
- I put my old B&W footage in the folder /test_images
- In my Ubuntu terminal i run the command: cd ~/Downloads/DeOldify
- Then i run the command: python run_local.py --input ./test_images/1.jpg --output ./test_images/ --model artistic

Is that the good way or if not can you please explain or just give me a lead so i can research on my own if it  somehow bother you, thanks in advance ^^",hello well excuse lack totally new trying understand process like zip let say folder put old footage folder terminal run command run command python input output model artistic good way please explain give lead research somehow bother thanks advance,issue,positive,positive,positive,positive,positive,positive
1219905560,"> Yeah you should be fine with that.

Hello again =) that was long lol but yes It worked ! i just tried it today with an online video hosted on Youtube. Thank you Jason. 
I'm totally noob on Ubuntu so it may be a stupid question but i'm really stucking on it, i would like to know if possible what's the way to put a local video/image, i don't know if i'm wrong but i just pasted the path file on the URL, like in this example: 

`source_url = ''/home/MyNickname/Downloads/Video001.mp4""
render_factor = 21  #@param {type: ""slider"", min: 5, max: 40}
watermarked = True #@param {type:""boolean""}

if source_url is not None and source_url !='':
    video_path = colorizer.colorize_from_url(source_url, 'video.mp4', render_factor, watermarked=watermarked)
    show_video_in_notebook(video_path)
else:
    print('Provide a video url and try again.')`

And i get an error saying that it's not a valid youtube link.
I also tried to drag & drop the video in a new tab of mozilla so i got a path like "" File:///home/MyNickname/Dektop/Video001.mp4 "" that i pasted on the source_url but i got the same issue.

If somebody can help it would be cool.",yeah fine hello long yes worked tried today video thank totally may stupid question really would like know possible way put local know wrong pasted path file like example param type slider min true param type none else print video try get error saying valid link also tried drag drop video new tab got path like file pasted got issue somebody help would cool,issue,positive,positive,neutral,neutral,positive,positive
1200170651,This link doesn't seem to be working for me. Is this still the location of the stable model?,link seem working still location stable model,issue,negative,neutral,neutral,neutral,neutral,neutral
1188507501,I finally got around to addressing this issue.  I've added error handling code for the ffmpeg calls that has been battle tested elsewhere. Closing.,finally got around issue added error handling code battle tested elsewhere,issue,negative,neutral,neutral,neutral,neutral,neutral
1179622002,"@dejanmilivojevic I understand what you're saying, but that argument could easily be extrapolated to suggest that we should support Jax, Tensorflow, Windows, AMD cards, etc.  I'm really advocating for simplicity and strict limits here. The more you support in software, the less likely you'll succeed in keeping everything in working order and correct, especially in the long term.

I really appreciate your offer on the pull request and the offer to maintain it, but I respectfully decline. In practice- life happens and I certainly wouldn't expect you to do free maintenance on something non-trivial like that in the long term.  I've dealt with Docker enough to know that it can become quite the pain in the butt.",understand saying argument could easily suggest support really simplicity strict support le likely succeed keeping everything working order correct especially long term really appreciate offer pull request offer maintain respectfully decline life certainly would expect free maintenance something like long term dealt docker enough know become quite pain butt,issue,positive,positive,positive,positive,positive,positive
1179574060,">     2. It strays far away from the core mission of this repo: To strictly share research.  As opposed to helping others deploy this as a service.

I think to effectively share resarch is to have training setup as easily runnable as possible, so that people can play with the model and data and compare with ""official"" results. At least that is what I'm trying to do :)

I'm willing to make a docker image for training setup and I can make a pull request and potentialy mantain it. If you are not up for it that is also ok I respect your decision. ",far away core mission strictly share research opposed helping deploy service think effectively share training setup easily runnable possible people play model data compare official least trying willing make docker image training setup make pull request also respect decision,issue,positive,positive,positive,positive,positive,positive
1179475478,"This isn't actually true strictly speaking- 4k will output as 4k in terms of dimensions.  **But**:

- It does convert frames to jpg though which may be causing a small visible degradation however- an engineering trade-off for practical space usage and wide ability to deploy.
-  Colors are rendered at much lower resolutions than 4k- default is at 336x336  (render_factor 21) and it  goes no higher than about 640x640 (render_factor 40). This can definitely cause visible degradation in the case of 4k, even after taking the effect of human perception of color into account. Chroma-subsampling has its limits here, after all.  This really can't be practically changed unfortunately without a brand new model (I've done it but only for commercial purposes). Basically that change isn't going to happen soon.

So basically what I'm saying is- Yes, there's limitations to the renders which you are probably seeing here but I don't have a solution for you. Sorry about that.",actually true strictly output convert though may causing small visible degradation engineering practical space usage wide ability deploy color much lower default go higher definitely cause visible degradation case even taking effect human perception color account really ca practically unfortunately without brand new model done commercial basically change going happen soon basically saying yes probably seeing solution sorry,issue,negative,negative,neutral,neutral,negative,negative
1179474683,This is great feedback. I've integrated these suggestions and can confirm that yt-dlp is much faster (have been using it for internal projects for a few months).  Thanks!,great feedback confirm much faster internal thanks,issue,positive,positive,positive,positive,positive,positive
1179474471,"So first thing to point out here:  The Docker stuff was never intended for training. It was strictly made with inference and server deployment in mind. The problem you had here was indeed actually a legit problem in the Docker file (PyTorch version wasn't properly updating to latest version) but doubtless you would have other problems with this anyway after having that solved.

What you should do instead if you're interested in training is use the conda install. That's what is supported, anyway.

Now here's the thing that I'm sure is going to disappoint some people:  I've just removed the the Docker support. I've been wanting to do this for a while but this issue you had here got me to finally do it.  There's a few reasons:

1. It's a lot of extra stuff to support.
2. It strays far away from the core mission of this repo: To strictly share research.  As opposed to helping others deploy this as a service.
3. It was a lot of extra clutter in the repo and readme (and hence more potential for confusion).
",first thing point docker stuff never intended training strictly made inference server deployment mind problem indeed actually legit problem docker file version properly latest version doubtless would anyway instead interested training use install anyway thing sure going disappoint people removed docker support wanting issue got finally lot extra stuff support far away core mission strictly share research opposed helping deploy service lot extra clutter hence potential confusion,issue,positive,positive,positive,positive,positive,positive
1161133416,"Hey everyone-

I've got fixes in for this and a bunch of other stuff (like supporting the latest GPUs with the latest version of PyTorch). Hopefully you guys should be good to go now.",hey got bunch stuff like supporting latest latest version hopefully good go,issue,positive,positive,positive,positive,positive,positive
1161130248,I've just done a bunch of upgrades that may help solve your issue here. Also make sure to use nvidia-docker (just in case).,done bunch may help solve issue also make sure use case,issue,positive,positive,positive,positive,positive,positive
1156166494,"> I made a single python file to run deoldify through CLI. Just copy [run_local.py](https://github.com/harisreedhar/DeOldify/blob/run-local/run_local.py) to the root directory and run:
> 
> ```
> python run_local.py --input ./test_images/1.jpg --output ./test_images/ --model artistic
> ```
> 
> N.B. only tested on linux

running same command gives an error:

  File ""run_local.py"", line 8
    <!DOCTYPE html>
    ^
SyntaxError: invalid syntax",made single python file run copy root directory run python input output model artistic tested running command error file line invalid syntax,issue,negative,positive,positive,positive,positive,positive
1156124871,"Renamed copy of _version. to PILLOW_VERSION. Now stuck on this string:
```
colorizer = get_video_colorizer()
Downloading: ""https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"" to /home/kirill/.torch/models/resnet101-5d3b4d8f.pth
178728960it [02:25, 1227503.15it/s]
```
After downloading nothing... tried in WSL2 and from hard drive different Ubuntu versions",copy stuck string nothing tried hard drive different,issue,negative,negative,negative,negative,negative,negative
1135320329,"Granted, if you're just doing inference- you can achieve that fairly easily. But the FastAI code is there for all the training infrastructure it provides. After all, the code provided here is to share the research.",achieve fairly easily code training infrastructure code provided share research,issue,positive,positive,positive,positive,positive,positive
1135318878,Thanks everyone for digging into this while I was gone. I'll try to build in a fix fairly soon.,thanks everyone digging gone try build fix fairly soon,issue,negative,positive,positive,positive,positive,positive
1135316885,"> I don't have any reason to do that. Why roll my own on all the things that FastAI is doing for me?

Thanks for your reply, I have removed FastAI",reason roll thanks reply removed,issue,negative,positive,positive,positive,positive,positive
1135311560,Thanks!  I found one that PaddlePaddle created (new to me) and I'll replace the link with that.,thanks found one new replace link,issue,negative,positive,positive,positive,positive,positive
1133943543,I found a similar issue being discuss on the Pillow repository page. A fix seems to be installing pillow version 6.1. It worked for me and I'm using Win 10. https://github.com/python-pillow/Pillow/issues/4130#issuecomment-555434800,found similar issue discus pillow repository page fix pillow version worked win,issue,positive,positive,positive,positive,positive,positive
1126797177,@prettydeep Thanks a lot it works! I had almost given up on installing DeOldify before you helped me out. I do appreciate your assistance :),thanks lot work almost given appreciate assistance,issue,positive,positive,positive,positive,positive,positive
1126783422,"Just use ffmpeg to copy audio from one video to another.

ffmpeg -i COLOR.mp4 -i BW.mp4 -map 0:v -map 1:a -c copy videoCOLORaudioBW.mp4",use copy audio one video another copy,issue,negative,neutral,neutral,neutral,neutral,neutral
1126707554,"@prettydeep I had to double post but there is only one error left. I was wondering if you faced the same error. DeOldify successfully colorized the video. When I run it on Colab it produces a video with audio. However, when I run it locally the video has no audio. **And displays the following error.**

Transferring audio
Traceback (most recent call last):
  File ""\DeOldify-master\run_local.py"", line 137, in <module>
    model.colorize_video(args)
  File ""\DeOldify-master\run_local.py"", line 101, in colorize_video
    subprocess.call("" && "".join([command1, command2]), shell=platform.system() != 'Windows')
  File ""\anaconda3\lib\subprocess.py"", line 349, in call
    with Popen(*popenargs, **kwargs) as p:
  File ""anaconda3\lib\subprocess.py"", line 951, in __init__
    self._execute_child(args, executable, preexec_fn, close_fds,
  File ""anaconda3\lib\subprocess.py"", line 1420, in _execute_child
    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,
FileNotFoundError: [WinError 2] System cannot find file specified",double post one error left wondering faced error successfully video run video audio however run locally video audio following error transferring audio recent call last file line module file line command command file line call file line executable file line tid executable system find file,issue,negative,positive,positive,positive,positive,positive
1126688750,"@prettydeep You are amazing, thank you! I can't believe it worked! I colored a short video on Colab using the zip file you provided. Now I need to make it work on my local machine :)",amazing thank ca believe worked colored short video zip file provided need make work local machine,issue,positive,positive,positive,positive,positive,positive
1126587354,"@enessol Everything you need is in the deoldify.zip file in the previous post. Extract the notebook, and try running it in google colab. If you still can't run on a local machine, then I suspect you have larger issues.",everything need file previous post extract notebook try running still ca run local machine suspect,issue,negative,negative,neutral,neutral,negative,negative
1126582300,"@prettydeep I appreciate your support. This is my first time dealing with an AI project so please pardon my questions :) I know that it might be a lot but I have to get this working somehow.
If it is possible could you please elaborate more on how you managed to get DeOldify working on your system? I would love step-by-step instructions. And I wonder what your CLI script looks like after you modified it according to your own directory. If it is possible can you also upload your whole DeOldify folder (without the models ofc)?

I hope I did not cause you any trouble with my overwhelming questions :))",appreciate support first time dealing ai project please pardon know might lot get working somehow possible could please elaborate get working system would love wonder script like according directory possible also whole folder without hope cause trouble overwhelming,issue,positive,positive,positive,positive,positive,positive
1126576483,"@enessol I am running in ubuntu18 wsl, on win11, with python3.7.

You may need to create a 'video/source/' directory within the Deoldify folder before you run the script (if you are using video). When I git clone Deoldify this folder is missing. Incoming video needs to be in this 'source' directory. The CLI script and notebook file need to run from the root Deoldify folder. 

Attached is the notebook I am using. 
[deoldify.zip](https://github.com/jantic/DeOldify/files/8691717/deoldify.zip)


",running win python may need create directory within folder run script video git clone folder missing incoming video need directory script notebook file need run root folder attached notebook,issue,positive,positive,positive,positive,positive,positive
1126556115,"@prettydeep Thanks a lot! I have tried your solution. However, when I run the command I get this error:

AssertionError: dummy is not a valid directory.

What is OS? I have been using Windows 10. I would really be delighted if you could tell me how exactly you are running the script.",thanks lot tried solution however run command get error dummy valid directory o would really delighted could tell exactly running script,issue,positive,positive,positive,positive,positive,positive
1126379932,"@enessol Someone provided a CLI script [here](https://github.com/jantic/DeOldify/issues/395#issuecomment-1026548969) that is working for me. I tried running the same script within a Jupyter notebook, seems to run fine.

",someone provided script working tried running script within notebook run fine,issue,negative,positive,positive,positive,positive,positive
1122346918,"@Atheryl Hey, could you manage to get it working? I have been having the same issue.",hey could manage get working issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1118136828,"There are other broken dependancies, so I guess it's not workable for the moment.",broken guess workable moment,issue,negative,negative,negative,negative,negative,negative
1109025133,"@Archviz360 I have limited experience with VMware but the key thing you'd have to look for is ""gpu passthrough"" and I know with vms that can be a hard thing to come by. Last I checked (like two years ago)that was offered only on a subset of VMware's offerings.",limited experience key thing look know hard thing come last checked like two ago subset,issue,negative,negative,neutral,neutral,negative,negative
1106904206,"@johnowhitaker  Thanks!  That's exactly right.  @zeke800 You'll need to use the conda install per the readme to keep things straight here with the installation. That's where you will get the correct fastai v1 installation (and everything else). I wouldn't try to diverge from that installation process, and I won't be supporting anything else.

As far the rest of your question goes- installation is the easy part. Asking about how to train the model- well...  the best I can tell you is to just follow the notebooks, and searching the many closed issues here for various questions that have been answered already. Beyond that, if you run into difficulties (you will) you'll have to rely on knowing the ropes of Python, fastai and deep learning in general. It's a vague question to answer because I don't know where you are in terms of knowledge base. But there's no short cuts.",thanks exactly right need use install per keep straight installation get correct installation everything else would try diverge installation process wo supporting anything else far rest question installation easy part train well best tell follow searching many closed various already beyond run rely knowing rope python deep learning general vague question answer know knowledge base short,issue,positive,positive,positive,positive,positive,positive
1106235723,"fastai.callbacks is fastai V1, you likely have fastai V2. In V2 you'd likely need something like `from fastai.callback.all import *` but there will sadly be other issues due to all this code being based (afaik, 5 minutes into trying to play with this myself) on version 1 of fast ai.",likely likely need something like import sadly due code based trying play version fast ai,issue,negative,positive,neutral,neutral,positive,positive
1099392240,thanks for the tips will try to see if its possible to make so i just need to start with python dependencies. However do you know if this works on Vmware if I have a Linux distribution with the pre-installed python & conda ?? I plan to build a VMA server for my colorizing project that would be connected to a local network that would let me scan in photos  & import videos and then automatic colorize them ,thanks try see possible make need start python however know work distribution python plan build server project would connected local network would let scan import automatic colorize,issue,negative,positive,neutral,neutral,positive,positive
1097311307,This is really more a Google Colab question which honestly I couldn't tell you for sure if this would actually work. You can try starting [here](https://stackoverflow.com/questions/48376580/google-colab-how-to-read-data-from-my-google-drive) and see if replacing the download steps with mounting the drive and operating on the DeOldify project there works. You'll still have to install the python dependencies for sure.,really question honestly could tell sure would actually work try starting see mounting drive operating project work still install python sure,issue,positive,positive,positive,positive,positive,positive
1094367720,"Thank you very much for the feedback, I didn't even know I was dealing with a 3 years old software. 

Your picture, by the way, is very much better. The building looks duller than in my picture, but the people look OK.",thank much feedback even know dealing old picture way much better building duller picture people look,issue,negative,positive,positive,positive,positive,positive
1094304845,"That's probably about as good as you're going to get with the 3 year old open source model. We have commercial work that we've been feverishly working on since then which at least gets rid of the zombie issues you're citing:  [https://i.imgur.com/NJDaSuR.jpg](https://i.imgur.com/NJDaSuR.jpg) .

But ultimately, you're also dealing with a tech that has severe limitations and I'd still characterize even the latest results as overly dull. You're just not going to get the kind of results that manual colorizers get any time soon with AI.",probably good going get year old open source model commercial work feverishly working since least rid zombie ultimately also dealing tech severe still characterize even latest overly dull going get kind manual get time soon ai,issue,negative,positive,positive,positive,positive,positive
1092015807,@dcaud I'm pretty sure @tonybarnhill is pointing you in the right direction there. Please let me know if that helps.,pretty sure pointing right direction please let know,issue,positive,positive,positive,positive,positive,positive
1092012741,"What I'm asking for though is the actual urls you're using to run these through DeOldify- assuming you're using urls.  It's either that or you're passing in a filename to a local file. In the case of urls, you will get the exact error you're getting if you're not directly linking to an image. For example, you have to get the direct image link on an Imgur image page as opposed to the link to the page it's hosted on, as the latter isn't the image itself.  

This doesn't work as a url:  [https://i.imgur.com/HGsGvPy](https://imgur.com/HGsGvPy)

This does:  [https://i.imgur.com/HGsGvPy.jpg](https://i.imgur.com/HGsGvPy.jpg)

Good way to tell if you're using the right link is to do a wget command on the image url and see if the resulting file you downloaded is actually an image.

If you're using an already downloaded file and are passing a file name in, best bet is to double check whether or not the file being pointed to is actually an image file by opening it.

These are really the only real possibilities I can think of and this seems specific to your use case, so chances are this isn't a bug in DeOldify that hasn't been discovered previously.  Anyway I wish you luck. I'll be closing this issue and not looking into it further unless I see a chorus of the same issues pop up indicating a widespread issue.
",though actual run assuming either passing local file case get exact error getting directly linking image example get direct image link image page opposed link page latter image work good way tell right link command image see resulting file actually image already file passing file name best bet double check whether file pointed actually image file opening really real think specific use case bug discovered previously anyway wish luck issue looking unless see chorus pop widespread issue,issue,positive,positive,positive,positive,positive,positive
1091539950,"Sure here you go :
This one is in png format
![PngItem_1908805](https://user-images.githubusercontent.com/83954824/162180034-68efb045-7e4a-4c6c-ab0d-aa3afd3c4a14.png)

and this one in jpeg
![1209385](https://user-images.githubusercontent.com/83954824/162180061-0dcec091-d2c7-4bf1-bb2b-fa5c44c31bcb.jpg)
)",sure go one format one,issue,negative,positive,positive,positive,positive,positive
1091253086,"Can you give the exact url of the image you used for this?

Initial resolution doesn't matter.",give exact image used initial resolution matter,issue,negative,positive,positive,positive,positive,positive
1091122437,"I'm pretty sure its regular jpeg image downloaded from picsart, I tried it out with the png format but no success.

Just asking, Does the initial resolution of image also matters?",pretty sure regular image tried format success initial resolution image also,issue,positive,positive,positive,positive,positive,positive
1091006177,It seems to be complaining that the file being pointed to isn't actually an image or an image type that PIL can open.  What's the image you're trying to use?,file pointed actually image image type open image trying use,issue,negative,neutral,neutral,neutral,neutral,neutral
1091005070,"The bandw folder is generated by the create_training_images function in the ""ColorizeTraining"" notebooks.

The expected folder format is just that there's a train and val folder in whatever your path winds up being there for ""path"" in the notebook. You don't have any additional requirements for the formatting of the data, as there isn't any additional information needed other than the images themselves. So just take the imagenet data as is.",folder function folder format train folder whatever path path notebook additional data additional information take data,issue,negative,neutral,neutral,neutral,neutral,neutral
1078402046,"Alright. Thank you Jason and sorry if i took some of your time.
I will update this thread once i install Ubuntu, and tell you if it worked.",alright thank sorry took time update thread install tell worked,issue,negative,negative,negative,negative,negative,negative
1075904451,"@dcaud Perhaps my thread might help? I was having a similar issue.

https://github.com/jantic/DeOldify/issues/364",perhaps thread might help similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1075049289,"I understand and that's fair enough.

Just a last question; If i install Ubuntu (let's say v20.05) with Linux Kernel 5, would it work and avoid the windows instability ? (i also have RTX 3060ti)",understand fair enough last question install let say kernel would work avoid instability also ti,issue,negative,positive,positive,positive,positive,positive
1074548953,"To clarify: Windows is not supported in terms of what I'm willing to help out with. Reason being is that when this project was set up a few years ago, the underlying libraries that were crucial only supported Linux. Things have changed since then but in order to properly bring up everything to speed so that Windows is properly supported I'd basically have to do a bunch of code updates and testing of what's already up- not only for running it but the training code as well. And then I'd have to support the resulting questions around it because inevitably as there would be more configurations to support, there would be more things to fail for users. 

That all being said it's possible to hack in Windows support and you can find that discussed in the issues here in the past. But I'm not going to try to help people debug that. I don't work for free and have better things to do, basically.
",clarify willing help reason project set ago underlying crucial since order properly bring everything speed properly basically bunch code testing already running training code well support resulting around inevitably would support would fail said possible hack support find past going try help people work free better basically,issue,positive,positive,neutral,neutral,positive,positive
1074481613,"But in that tutorial, that guy were running it on Windows, is it normal Jason ?

Here is the video: https://youtu.be/NA0GfR3zwb0",tutorial guy running normal video,issue,negative,positive,positive,positive,positive,positive
1061121238,"Yes, that line contradicts the other line about GPU: ""nvidia card found running on GPU.""

I believe I installed the NVIDIA Driver and was able to test that it was connected and no jobs were running on it (which I believe is indicated in the first part of the output I copied above). But now I can't figure out how to test that again. Do you know of another way to test this? I wonder whether the driver I installed needed to be installed inside the docker container that is running the API?

My attempt thus far was on a Google Cloud VM with Ubuntu. Is that a route known to fail?",yes line line card found running believe driver able test connected running believe first part output copied ca figure test know another way test wonder whether driver inside docker container running attempt thus far cloud route known fail,issue,negative,positive,neutral,neutral,positive,positive
1061110874,"This isn't working as expected- you should be getting much faster results. It's saying this which makes me wonder:

```
WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.
   Use 'nvidia-docker run' to start this container; see
   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .
```

Do you have that installed? And this is a native instance of Ubuntu as opposed to hosted on say WSL or a virtual machine?",working getting much faster saying wonder warning driver functionality available use run start container see native instance opposed say virtual machine,issue,negative,positive,positive,positive,positive,positive
1050201506,"I really do appreciate the effort and the contribution, but there's several reasons why I can't accept this pull request:

1. environment.yml has been tailored to the aws use case and gets rid of dependencies needed for other use cases- jupyter notebooks for example.
2. It adds more stuff to support long term, but really we have Google Colabs already and we're not itching for a replacement.  There's a real cost-benefit trade off to inviting new support requests based on this breaking in addition to everything else breaking as AWS etc change things in the future. 

Hope that explains it.  I'd recommend making a fork and supporting it as a separate repo.",really appreciate effort contribution several ca accept pull request use case rid use example stuff support long term really already itching replacement real trade inviting new support based breaking addition everything else breaking change future hope recommend making fork supporting separate,issue,positive,positive,positive,positive,positive,positive
1050193116,"@wonder2025 You'd have to be specifically using nvidia-docker with a supported Nvidia GPU installed, on Linux. There's instructions for this on the readme. Still to this day is only supported on Linux. There's Windows 11 with WSL but that's where you'd be on your own.  

Apparently it's now called Nvidia Container Toolkit. ",wonder specifically still day apparently container,issue,negative,positive,neutral,neutral,positive,positive
1050189307,"Not sure what you guys ran into exactly (temporary issue?) but it's working currently.  This command definitely works (in case you were trying something else):

`wget https://www.dropbox.com/s/usf7uifrctqw9rl/ColorizeStable_gen.pth?dl=0 -O ./models/ColorizeStable_gen.pth`",sure ran exactly temporary issue working currently command definitely work case trying something else,issue,positive,positive,positive,positive,positive,positive
1050121908,Same issue here -- can't download the stable model,issue ca stable model,issue,negative,neutral,neutral,neutral,neutral,neutral
1043698038,It's mean to be used as part of the Docker deployment. It's actually a flask server file. So that's where you should go for the instructions- the Docker section of the readme.,mean used part docker deployment actually flask server file go docker section,issue,negative,negative,negative,negative,negative,negative
1043696824,"There's already an open issue about this actually:  https://github.com/jantic/DeOldify/issues/395 .

It's beyond the scope of what this project is for (sharing research- it's not a product)- but there are others on the case.",already open issue actually beyond scope project product case,issue,negative,neutral,neutral,neutral,neutral,neutral
1030519088,"Well, first, go ahead and solve the problem yourself then, if you want to be consistent in your position here.

Second, part of effective problem solving is limiting the scope of problems in the first place. Windows isn't supported because the underlying key libraries didn't support it when I put this up to share the research. And that's all it is. I'm sharing research, not working for free for people who are quick on the trigger to bash the creator on the baseless grounds that they're entitled to it for....reasons.

Now of course I don't expect any of that to change your mind. But I've said my piece and that's that.",well first go ahead solve problem want consistent position second part effective problem limiting scope first place underlying key support put share research research working free people quick trigger bash creator baseless ground course expect change mind said piece,issue,positive,positive,positive,positive,positive,positive
1030517368,"oh dude yes you are a problem solving guru

can write code that will work on different systems?
rather than simple answers...",oh dude yes problem guru write code work different rather simple,issue,negative,neutral,neutral,neutral,neutral,neutral
1030416977,This one has a pretty easy answer: Windows isn't supported. ,one pretty easy answer,issue,positive,positive,positive,positive,positive,positive
1026548969,"I made a single python file to run deoldify through CLI. Just copy [run_local.py](https://github.com/harisreedhar/DeOldify/blob/run-local/run_local.py) to the root directory and run:
````
python run_local.py --input ./test_images/1.jpg --output ./test_images/ --model artistic
````
N.B. only tested on linux",made single python file run copy root directory run python input output model artistic tested,issue,negative,positive,positive,positive,positive,positive
1024738113,I honestly don't know what's going on then- there's a lot of potential ways you could be getting to this problem it appears. I think your best bet may be to reference this thread and see if you can get things working with the suggestions here.  [https://github.com/pytorch/pytorch/issues/31285](https://github.com/pytorch/pytorch/issues/31285),honestly know going lot potential way could getting problem think best bet may reference thread see get working,issue,positive,positive,positive,positive,positive,positive
1020521709,"Sorry meant it doesn't like the RTX 30 series.
",sorry meant like series,issue,negative,negative,negative,negative,negative,negative
1020521025,"I did the Anaconda install. Same issue with Version 10, so I upgraded the Cuda to 11 thinking that may help. Any other things I might try ? I just tried swapping the card in my machine that had the 2060 super, and had they same issue. For some reason, it doesn't like the 20 series. Any other thoughts on something I could try ?",anaconda install issue version thinking may help might try tried swapping card machine super issue reason like series something could try,issue,positive,positive,positive,positive,positive,positive
1020461891,"DeOldify uses an older version of PyTorch that only supports CUDA 10 or 9. That CUDA version result you're getting (11.,6) seems likely to be the problem. So try going with CUDA 10. I'm not sure how you're installing all this but it sounds like you may not be using the Anaconda install. I'd suggest sticking with that.",older version version result getting likely problem try going sure like may anaconda install suggest sticking,issue,negative,positive,positive,positive,positive,positive
1019650957,The available options are listed there and for some reason they don't go past 8.4.0. I'm not going to try to hunt this down as it's not critical so instead I've made a simple change to the requirements.txt file that should resolve this. Can you try again?,available listed reason go past going try hunt critical instead made simple change file resolve try,issue,negative,positive,neutral,neutral,positive,positive
1018766980,I stumbled across Deoldify just 10 minutes ago so I have no idea about how you could solve the problem. I was wondering if a mobile 3060 super exists.,across ago idea could solve problem wondering mobile super,issue,negative,positive,positive,positive,positive,positive
1016814105,Definitely would like this tool,definitely would like tool,issue,positive,neutral,neutral,neutral,neutral,neutral
1012435409,"Looks like you found the help you needed [here in the correct project](https://github.com/olaviinha/NeuralImageColorization/issues/1) as this notebook isn't actually part of the DeOldify project.  

Closing.",like found help correct project notebook actually part project,issue,positive,neutral,neutral,neutral,neutral,neutral
1009219114,"I won't be doing a video (camera shy and whatnot) but here's a blog that hopefully explains it better:  [blog](https://www.fast.ai/2019/05/03/decrappify/) .

My ultimate suggestion, however, is to simply follow the ColorizeTrainingVideo.ipynb notebook step by step.  It may be tedious to do so but it's also showing exactly what is being done. ",wo video camera shy whatnot hopefully better ultimate suggestion however simply follow notebook step step may tedious also showing exactly done,issue,positive,negative,neutral,neutral,negative,negative
1009211332,WSL is beyond the scope of what we're supporting here (because it's Windows).  You may be able to have success by using this but you're on your own from here:  https://github.com/daddyparodz/AutoDeOldifyLocal,beyond scope supporting may able success,issue,positive,positive,positive,positive,positive,positive
1009207101,"@onlymaureen This appears to be a case of having not successfully downloaded the model weights correctly to /models/ColorizeVideo_gen.pth .  I'd delete the existing ColorizeVideo_gen.pth and reattempt the download from here:  https://data.deepai.org/deoldify/ColorizeVideo_gen.pth .  The file size should end up being 853,581 KB .   ",case successfully model correctly delete reattempt file size end,issue,negative,positive,positive,positive,positive,positive
1009196908,@IveMalfunctioned  Thanks for bringing this to my attention. I've committed a fix and you should be good to go. I noticed the problem in imgur links myself- not sure where you were seeing it. Let me know if you still see an issue.,thanks attention fix good go problem link sure seeing let know still see issue,issue,positive,positive,positive,positive,positive,positive
992804413,No worries man. Just wanted to bring it to your attention. I'll look for the changes when I have a chance. ,man bring attention look chance,issue,negative,neutral,neutral,neutral,neutral,neutral
992725711,"@niutech - Research, persistence, and yet knowing when to quit! Most likely it won't work the first time as it seems almost everything in the current ecosystem breaks when it comes to interoperability between formats for anything that's outside of ""normal""- that is, basic vision networks like resnet and nlp like BERT.  I can already tell you two candidates for likely headaches: The hooks generated by hook_outputs in unet.py, and spectral norm.  The former could potentially be remedied by [tracing](https://pytorch.org/docs/stable/generated/torch.jit.trace.html) the model with the jit and using the resulting torchscript model.  The latter is easily remedied by simply stripping spectral norm off the generator model after loading it and using that instead.  You can use the function I posted [here](https://github.com/jantic/DeOldify/issues/166#issuecomment-540882832) . 

Now more headaches to consider:  Realistically you may have to upgrade to the latest PyTorch (1.10.0) in order to get torchscript compiles working well (it's your best shot anyway).  Yet if you do that you're going to have to fork a custom fastai 1.0.51 and get it working with PyTorch 1.10.0 (we're moving from PyTorch 1.0.1 here).  Another user described that process [here](https://github.com/jantic/DeOldify/issues/354).

tl;dr :  It's non trivial and will require research, persistence, and some luck.
",research persistence yet knowing quit likely wo work first time almost everything current ecosystem come anything outside normal basic vision like like already tell two likely spectral norm former could potentially tracing model resulting model latter easily simply stripping spectral norm generator model loading instead use function posted consider realistically may upgrade latest order get working well best shot anyway yet going fork custom get working moving another user process non trivial require research persistence luck,issue,positive,positive,positive,positive,positive,positive
992711236,"Hey @KeygenLLC , you were totally right.  That was some seriously sloppy and embarrassing code on my part and I'm surprised it hadn't been pointed out sooner.  I've committed fixes not only for what you pointed out but also some sloppiness  in other files as well. ",hey totally right seriously sloppy embarrassing code part pointed sooner pointed also sloppiness well,issue,negative,negative,neutral,neutral,negative,negative
987143965,"> The source images aren't available anymore (hosting costs) but you can see what I used on images in this notebook: https://github.com/jantic/DeOldify/blob/master/ImageColorizerArtisticTests.ipynb .

Flickr, archive.org and Wikimedia Commons are obscenely good resources for getting public domain images to test with. All images are clearly marked and Google Image Search will be much better and faster to search these sites than their built-in search engines. Just append your Google searches with the domain, such as `site:wikimedia.org`. The only downside to Google Image search is that they removed some of the search tools, so you can't specifically search for public domain images anymore. Searching the sites I mentioned you will find mostly PD images anyway.

One exception is Flickr. Flickr's search engine is slower, and can hang up, but it has multiple public domain filters and many galleries are curated by institutions. Millions of images probably.

**Note:** Searching Flickr images without safe search enabled will possibly give you results that make you wish you had no eyes. You have been warned.",source available hosting see used notebook common obscenely good getting public domain test clearly marked image search much better faster search search append domain site downside image search removed search ca specifically search public domain searching find mostly anyway one exception search engine multiple public domain many million probably note searching without safe search possibly give make wish,issue,positive,positive,positive,positive,positive,positive
987061837,"Great summary- it pretty much jibes with my experience. It really depends on what you're looking for. and what kind of images you're dealing with. The source images aren't available anymore (hosting costs) but you can see what I used on images in this notebook: [https://github.com/jantic/DeOldify/blob/master/ImageColorizerArtisticTests.ipynb]( https://github.com/jantic/DeOldify/blob/master/ImageColorizerArtisticTests.ipynb) .

My work ever since this open source release has involved a great deal of effort into making it so that you don't have to figure out the arbitrary magic number and things just work.  I think it's pretty much at that point now but don't expect that to make it to open source any time soon. But I can at least let it be known to those developing their own versions that yes- it's possible to make things fully automatic, but it's a lot of work.",great pretty much experience really looking kind dealing source available hosting see used notebook work ever since open source release involved great deal effort making figure arbitrary magic number work think pretty much point expect make open source time soon least let known possible make fully automatic lot work,issue,positive,positive,positive,positive,positive,positive
986154951,"Just wanted to tack some of my research here for anyone searching for RAM, memory, memory use, memory errors, or running out of memory, as well as ""what is a good render_factor to use?""

I ran batches of small images (300x300 8-bit PNG) just to see how the color varied and how much RAM was used. This was not a scientific test, so my results will be vague, but hopefully will give you a ballpark idea of what to expect and why the creators of DeOldify have said to stay below `render_factor=64`.

These tests were run on macOS 10.14.6 Mojave running on a 10-core CPU. YMMV

Let's start with the insane, `render_factor=100`
---
**Max RAM used:** 48GB
**Time:** ~5 minutes
Not suggested, as most of the image turns blue/cyan. You will see large memory jumps of 2-8GB as time goes on. Using this high of a render factor is pointless, unless you like the unrealistic look and frying your hardware for no reason.

High `render_factor` of 50-75
---
**Max RAM used:** ~2-4GB depending (more for larger images)
**Time:** ~30 seconds - 1 minute
Again, not suggested for most images. The colors are not very good. Whatever `render_factor` does, it's not a ""quality"" setting, so don't treat it like that.

Medium `render_factor` of 35-50
---
**Max RAM used:** ~2GB
**Time:** ~15-30 seconds
Pretty fast, but typically the colors have already gone too far into the blues. Only 1 image so far was sort of okay in this range but it was heavily damaged. I don't recommend using a render_factor this high unless it's a special use case or you are just playing around.

Ideal Settings `render_factor` 1-30
---
**Max RAM used:** Typically less than 1GB for small images. hard to tell because they finish faster than I can monitor
**Time:** ~1-15 seconds
This is my recommendation. The best results I'm getting are sometimes with a `render_factor` in single digits, even as low as 2 or 3. In fact, if you do your renders in increments of 1, you will notice drastic differences in regard to how the colors begin to resolve. You will be able to easily see when you hit the sweet spot _**for that particular image**_ when things just start looking realistic.

Conclusion
---

- `render_factor` is not a quality setting. Don't use it like that.
- Each image is different and requires a unique render factor.
- Testing is your friend.
- If doing 1-offs, use small images to test. It will save you time so you can test more.
- Using anything above a factor of 30 for typical images makes no sense. Only 1 image I tested looked right with the suggested default value of 35.
- Above 50 is getting into goofy territory.
- 75+ is fine if you have a thing for Aquaman.
- There is no reason to tax your CPU or GPU with ridiculously high settings. They won't improve your image. Most people don't have tons of RAM or GPUs with enough RAM to even handle it. This is why the creators suggest at least 11GB of GPU memory if you are using a factor of 64, though I haven't found a case yet where it's warranted. 

Hope this helps someone. If not, it helped me :P",tack research anyone searching ram memory memory use memory running memory well good use ran small see color varied much ram used scientific test vague hopefully give idea expect said stay run running let start insane ram used time image turn see large memory time go high render factor pointless unless like unrealistic look hardware reason high ram used depending time minute color good whatever quality setting treat like medium ram used time pretty fast typically color already gone far blue image far sort range heavily recommend high unless special use case around ideal ram used typically le small hard tell finish faster monitor time recommendation best getting sometimes single even low fact notice drastic regard color begin resolve able easily see hit sweet spot particular image start looking realistic conclusion quality setting use like image different unique render factor testing friend use small test save time test anything factor typical sense image tested right default value getting goofy territory fine thing reason tax ridiculously high wo improve image people ram enough ram even handle suggest least memory factor though found case yet warranted hope someone,issue,positive,positive,positive,positive,positive,positive
982353672,"You may just upload your code into a branch here, or a separate project if you prefer.

I will test this on Ubuntu Linux and see if some changes are needed.",may code branch separate project prefer test see,issue,negative,neutral,neutral,neutral,neutral,neutral
977071810,"> I think the problem with this project is the owner is a jerk and is a waste of anyone's time using this project because he uses it as a way to troll people who try it and realize the project doesn't work.

Simply put, I'm not your Reek from Game of Thrones. 

I don't bend over backwards to stroke the egos of people who refuse to take the time read what I actually wrote a year ago just enough to finally realize ""Oh...there's an actual purpose to this that I missed."" I don't expect you to do that now.  Instead, despite my pointing this out, I fully expect you to double down on bestowing the honor of ""jerk"" on me and perhaps even throwing a few other choice insults my direction.  After all, providing a year's worth of work to you and the rest of the world for free isn't enough. I must go further and embrace ""professionalism"".  That is to say- I must surrender my personality, forgo any actual opinions and standards of basic human conduct expected in any other sphere of life- in order to please my masters.  The CONSUMER*.

Or I can just say ""fuck that"" and move on with my life and feel satisfied that my work has gone far and wide already, instead of making myself miserable by conforming to the insane and completely unreasonable expectations of the increasingly unhinged masses on the internet.

Yeah, let's do that.

*Obviously this is a very liberal use of the term ""consumer"" as this is a completely free project.",think problem project owner jerk waste anyone time project way troll people try realize project work simply put reek game bend backwards stroke people refuse take time read actually wrote year ago enough finally realize oh actual purpose expect instead despite pointing fully expect double bestowing honor jerk perhaps even throwing choice direction providing year worth work rest world free enough must go embrace professionalism must surrender personality forgo actual basic human conduct sphere order please consumer say move life feel satisfied work gone far wide already instead making miserable insane completely unreasonable increasingly yeah let obviously liberal use term consumer completely free project,issue,negative,negative,neutral,neutral,negative,negative
977049377,"> Some days were 90% failures.

P.S. Ask inventors about this one. It's probably higher than 90%. ☝🏻",day ask one probably higher,issue,negative,positive,positive,positive,positive,positive
977031150,"When you work in production, you do tons of tests.  It doesn't matter if you use closed-source software, which can be an even be a bigger nightmare. Trying to meet perishable deadlines, even in a big VFX studio, you need to find workarounds for workarounds, for your workarounds when your 3rd-string fall-backs fail and the client pushed the deadline back by a month. It doesn't matter whether you use After Effects, Maya, or something that has open source code. You need to be a hacker at heart, even if you're an artist and no excuses are allowed when big money is on the line. You have to be like Dustin Hoffman in Wag the Dog. That's my mantra, _""This is nothing. Try a ten AM script meeting, coked to the gills, no sleep and you haven't even read the treatment.""_

The aforementioned closed-source apps can be even worse, because you can't access the source, can't reach the dev team to even pose a question, the customer service people know as much about the what we do as a blind hooker, and you have to hope Adobe or Autodesk decide to patch things, if ever.

This was my first look at jupyter notebooks and it didn't seem very user-friendly, so I worked around it. And yes, this is not the most user-friendly script on Github. Who cares? The developers wrote 8 paragraphs explaining why it is the way it is. They made it for them, not for us, and just let it loose in the wild to a thankless world. Dude's still smiling in his avatar and I'm laughing while smoking on my porch. I'm happy there's even software like this because I'm on a mac and don't have access to CUDA. Be a little greatful, ffs.

If you don't like it, just don't use it. There's plenty of other simpler colorization tools, with different looks. They don't all work with video, but you can just output an image sequence and batch them, even if you just dump the whole script in a function and use a `for` loop, you can build the video with any industry-standard video app or ffmpeg. It will take you like 5 minutes. When you work in production, you work with uncompressed frames, not mp4s, so this is standard practice. I work in 32-bit linear workflow and my next task is to figure out how to make these things work with EXR float files.

Check out colorization by richzhang. It uses 2 siggraph models to color images. Script is like 50 lines of code. Renders nearly instantly. Not the greatest, but not terrible, and you can augment output from different apps and scripts by compositing them to get what you want.

And you can call me whatever you want, kid. After going through the meat grinder of life, being called a jerk is a highlight.

☕️🚬",work production matter use even bigger nightmare trying meet perishable even big studio need find fail client deadline back month matter whether use effect maya something open source code need hacker heart even artist big money line like wag dog mantra nothing try ten script meeting sleep even read treatment even worse ca access source ca reach dev team even pose question customer service people know much blind hooker hope adobe decide patch ever first look seem worked around yes script wrote explaining way made u let loose wild thankless world dude still smiling laughing smoking porch happy even like mac access little like use plenty simpler colorization different work video output image sequence batch even dump whole script function use loop build video video take like work production work uncompressed standard practice work linear next task figure make work float check colorization color script like code nearly instantly terrible augment output different get want call whatever want going meat grinder life jerk highlight,issue,positive,negative,neutral,neutral,negative,negative
976977231,"> Not everything was easy. Some days were 90% failures. Trying to make head or tail of `visualize.py` to add dynamic path 
> Don't get mad at these guys if they aren't handing you the keys to the castle and ushering you through the halls in a diaper. They're nice enough for providing this research tool for free. God only knows how much work they put in to build it, and how many years of learning were behind it. Between Github, StackOverflow, StackExchange and YouTube—not to mention all the free packages you can use in a heartbeat with pip and Anaconda—you can learn plenty and do plenty on your own if you are diligent and put in the time... and it's not even that much time that's needed.

I think the problem with this project is the owner is a jerk and is a waste of anyone's time using this project because he uses it as a way to troll people who try it and realize the project doesn't work. 

I posted in honest faith thinking is a bug. I still think is a bug. It doesn't make sense that you would want to render one video at multiple ratios when you clearly choose one specific ratio in the first function.
 
There was never any need for trolling or rude remarks.  Absolutely no professionalism. 

In reality this is an experimental project that only works for the creator and I didn't want to sink in anymore time only to get more rude and troll remarks instead of genuine help.  

Edit: I am by no means a beginner and I was willing to put in the time and effort but this here was my first interaction with the project put me off. ",everything easy day trying make head tail add dynamic path get mad castle diaper nice enough providing research tool free god much work put build many learning behind mention free use heartbeat pip learn plenty plenty diligent put time even much time think problem project owner jerk waste anyone time project way troll people try realize project work posted honest faith thinking bug still think bug make sense would want render one video multiple clearly choose one specific ratio first function never need trolling rude absolutely professionalism reality experimental project work creator want sink time get rude troll instead genuine help edit beginner willing put time effort first interaction project put,issue,positive,positive,positive,positive,positive,positive
976973059,@jantic  You call yourself a project manager. Look how you talk. ,call project manager look talk,issue,negative,neutral,neutral,neutral,neutral,neutral
976891251,"@KeygenLLC  I have to let you know that both @dana-kelley and I are fans of you and your comments here. I don't know if you realize just how much of a relief it is to hear someone with sense on the internet these days!

Oh and visualize.py is a bit of a hacky nightmare LOL. Sorry about that....",let know know realize much relief hear someone sense day oh bit hacky nightmare sorry,issue,negative,negative,negative,negative,negative,negative
976736868,"I know this is old, but in case anyone stumbles on this (like I did when looking for info on how to grab the render_factor), I'd like to share my 2¢ about this project and others like it on Github.

This software is not hard to use or modify if you have some knowledge of python. Yeah, you can't just type in `make_pretty.py --now` and get what you want. However, just because it's setup with jupyter notebooks doesn't mean you have to use it that way. It's python ffs and they gave us the source code. 😆

As an example, I've only been using python for about 2 weeks, am not a programmer by trade, and only have experience with PHP and markdown and such. With a reasonable amount of research and effort, I was able to convert the `.ipynb` notebook I needed into a `.py` file. I added custom environment variables with `python-dotenv`, input and output paths, turned the `vis.plot_transformed_image` into a `method` with `args` and modified the hell out of the script using `lists`, `while` and `for` loops, added keyboard exceptions, formatted the output so it's human readable with `colorama` and `f-strings` with tons of feedback I need, formatted the output filenames with date stamps and `render_factor`, setup a separate script with `watchdog` to watch folders, am firing off shell commands and controlling the keyboard and mouse programmatically, and even created my own socket server that I can connect to as a client with the script to send it data. 

Now I can batch to my heart's content _and_ output multiple files _and multiple versions_ of them in one go with different `render_factor` by dropping images in a folder and typing one command.

Then I did the same with 15 more scripts.

Not everything was easy. Some days were 90% failures. Trying to make head or tail of `visualize.py` to add dynamic path variables and output the filenames I wanted, took almost a whole day and made me go cross-eyed when trying to grab the `render_factor` from the correct function, but I finally figured it out through trial and error. Still working out some bugs with the socket server because I coded it from scratch, but so what. I'm learning a lot and the tools worked... eventually. 😬

Don't get mad at these guys if they aren't handing you the keys to the castle and ushering you through the halls in a diaper. They're nice enough for providing this research tool for free. God only knows how much work they put in to build it, and how many years of learning were behind it. Between Github, StackOverflow, StackExchange and YouTube—not to mention all the free packages you can use in a heartbeat with pip and Anaconda—you can learn plenty and do plenty on your own if you are diligent and put in the time... and it's not even that much time that's needed.",know old case anyone like looking grab like share project like hard use modify knowledge python yeah ca type get want however setup mean use way python gave u source code example python programmer trade experience markdown reasonable amount research effort able convert notebook file added custom environment input output turned method hell script added keyboard output human readable feedback need output date setup separate script watchdog watch firing shell keyboard mouse programmatically even socket server connect client script send data batch heart content output multiple multiple one go different dropping folder one command everything easy day trying make head tail add dynamic path output took almost whole day made go trying grab correct function finally figured trial error still working socket server scratch learning lot worked eventually get mad castle diaper nice enough providing research tool free god much work put build many learning behind mention free use heartbeat pip learn plenty plenty diligent put time even much time,issue,positive,positive,neutral,neutral,positive,positive
976342195,Thank you for your reply. There will be no problem after using Linux system. Sorry to bother you,thank reply problem system sorry bother,issue,negative,negative,negative,negative,negative,negative
974316139,"I'm sorry to hear that you've been through this so many times.  

Good luck with your project.",sorry hear many time good luck project,issue,positive,positive,positive,positive,positive,positive
974284527,"This is a great opportunity to remind everyone to take a look at the wall of words on the readme and search issues and see if your questions are answered there first. That is to say, Google the internet before you turn to making me your Google.

![image](https://user-images.githubusercontent.com/179759/142669158-3c5141ca-b7f4-4864-b2be-929975c6ad87.png)


I know it's a harsh response but guys...come on.  We've been through this soooo many times.....",great opportunity remind everyone take look wall search see first say turn making image know harsh response come many time,issue,positive,positive,positive,positive,positive,positive
974276593,This is certainly helpful to have documented in the project so this is much appreciated! Unfortunately I'm not in the position to fully test this at this time and I know others are using this Docker deployment commercially. So I'll leave the code as is for now to keep it stable until this can be tested. @jqueguiner  fyi.,certainly helpful project much unfortunately position fully test time know docker deployment commercially leave code keep stable tested,issue,positive,negative,neutral,neutral,negative,negative
974270748,I mean this would certainly be possible but it's not on my agenda. This project is strictly about sharing research and others are free to expand upon it to do things like this.  If somebody makes it and it works well I'll certainly throw a link to it in the readme because it'll probably be useful for a few people.,mean would certainly possible agenda project strictly research free expand upon like somebody work well certainly throw link probably useful people,issue,positive,positive,positive,positive,positive,positive
972440537,"No worries, man. it's understandable. I find most things made for Linux for can work on a mac as long as CUDA isn't required. Just wanted to let others know it is possible with minimal fuss to make it work if you know how to use a few tools.

Thanks for making this research available. It provides a unique look to image restoration that other software doesn't and it's fun to experiment with. Interested to see what it does to video.",man understandable find made work mac long let know possible minimal fuss make work know use thanks making research available unique look image restoration fun experiment interested see video,issue,positive,positive,positive,positive,positive,positive
972324624,"Seems to always happen this way- I found a solution after I thought I gave up. Code is updated, and we're no longer ignoring certificates now.",always happen found solution thought gave code longer,issue,negative,neutral,neutral,neutral,neutral,neutral
972236347,"After a lot of frustration looking at this today I'm going to have to go with @johnbreslin 's solution of doing the ignore on the ssl certificates for now and will accept the pull request.  Adding the ca-certificates install isn't actually fixing this issue (nor is adding update-ca-certificates -f afterwards) and I have yet to figure out what can be done here. I'll still add those to the DockerFile after merging this pull request but honestly I'm just not a Docker expert, nor an ssl expert. 

Suggestions are definitely welcome.",lot frustration looking today going go solution ignore accept pull request install actually fixing issue afterwards yet figure done still add pull request honestly docker expert expert definitely welcome,issue,positive,positive,positive,positive,positive,positive
971777998,"Thanks for documenting your experiences here.  I'm sure there will be people that will find this helpful.  We won't be officially supporting Mac OS or WIndows just because it adds more maintenance issues for something that was really mean to just be a sharing of research. 

You're very much right on the nature of what does well and what doesn't, and what it looks like.",thanks sure people find helpful wo officially supporting mac o maintenance something really mean research much right nature well like,issue,positive,positive,positive,positive,positive,positive
971774470,Have you tried any other videos?  It does make me wonder if it's just that specific video that's having the issue.,tried make wonder specific video issue,issue,negative,neutral,neutral,neutral,neutral,neutral
968290381,Just got to say thank you @dan64 this (and different issue putting model folders locally and changing generators.py) allowed deoldify to work for me. Just to note my card is 3070ti,got say thank dan different issue model locally work note card ti,issue,negative,neutral,neutral,neutral,neutral,neutral
966540452,"Setup on Mojave and ran into some errors during install. Was toward the end when installing dependencies for pip, specifically `opencv-python`. This typically causes hiccups so not surprised. Manually installing with pip after this, making sure `cmake` is installed, upgrading pip with setuptools wheel, then a installing a version of `opencv-python` I know works on this system. I then had to install `ffmpeg`. Then to be sure I ran `pip install -r requirements.txt`. Once that was done, jupyter lab was able to process the images just fine.

This is certainly a problem associated with the packages and OS and not DeOldify as I see it constantly and typically setup env with python only, then use pip to install cmake and opencv to avoid the hassles. I tried setting up the env with the yml file this time just to see what would happen, and... yeah. :)

Anyway, DeOldify runs much faster on desktop (no surprise). Factor of 35 only took 28 seconds. Factor of 64 only took 1 minute 10 seconds. Now I just need to figure out how to batch process folders so I don't have to manually input image names.

Edit: Just a note about what I've noticed processing portraits. Good colorization seems to depend on the quality of the image. Images that are soft I find have odd colorization and may add blues/cyans near shadows. Images with heavy printing halftone that have not been descreened hardly get colored at all. Images with decent detail and gradations in the face produce the best results, even if compression artifacts exist.",setup ran install toward end pip specifically typically manually pip making sure pip wheel version know work system install sure ran pip install done lab able process fine certainly problem associated o see constantly typically setup python use pip install avoid tried setting file time see would happen yeah anyway much faster surprise factor took factor took minute need figure batch process manually input image edit note good colorization depend quality image soft find odd colorization may add near heavy printing hardly get colored decent detail face produce best even compression exist,issue,positive,positive,positive,positive,positive,positive
965943993,"I had same issue; the addresses are https://www.youtube.com/watch?v=Wk4Eq8IcQMk & https://www.youtube.com/watch?v=iXT2E9Ccc8A  Both are old BW 50s Tv show, same file format, size etc. neither worked tried various verisons of the Deoldify Colab, as well as looked into the logs and googled what I could but came up with nothing. I tested a few other videos on youtube using same colab and it worked fine, so I suspect it is the source. I also tested the same url (above) as the OP, and got the same no-go response. 
I know this thread is  year old, but I wanted to at least mention it to maybe get a bugfix or at a better understanding of why some videos work and others don't. ",issue old show file format size neither worked tried various well could came nothing tested worked fine suspect source also tested got response know thread year old least mention maybe get better understanding work,issue,negative,positive,positive,positive,positive,positive
962757964,"Removing `ffmpeg==1.4` is correct. I think it is this issue: https://github.com/jantic/DeOldify/issues/365

@alexandrevicenzi the cert is invalid according to the Docker file in the repository. It is easy to reproduce by running `./quick_start.sh video_api`. The fix is to add `ca-certificates` to the list of `apt-get install` in the docker file. This will force the certs in the container to update and wget will download correctly. ",removing correct think issue invalid according docker file repository easy reproduce running fix add list install docker file force container update correctly,issue,negative,positive,positive,positive,positive,positive
961069455,"#NOTE:  Max is 44 with 11GB video cards.  21 is a good default
render_factor=21
#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification
source_url= None
file_name = './test'
file_name_ext = file_name + '.mp4'
result_path = None

if source_url is not None:
    result_path = colorizer.colorize_from_url(source_url, file_name_ext, render_factor=render_factor)
else:
    result_path = colorizer.colorize_from_file_name(file_name_ext, render_factor=render_factor)

show_video_in_notebook(result_path)",note video good default note make none read file directly without modification none none none else,issue,negative,positive,positive,positive,positive,positive
944832506,Thanks! This is the sort of stuff that's highly appreciated.  I've added this to the readme as an alternative installation method.,thanks sort stuff highly added alternative installation method,issue,negative,positive,positive,positive,positive,positive
944610846,"@tonybarnhill The thing that looks suspect is the

`show_image_in_notebook(result_path) `

part because that doesn't belong there.  

The images do take time to generate.  Have you tried making the process faster by shortening the range of render factors being tried there and seeing if you get a result?",thing suspect part belong take time generate tried making process faster shortening range render tried seeing get result,issue,negative,neutral,neutral,neutral,neutral,neutral
944608120,"That would be beyond the scope of what we're doing here. What the YouTube downloader logic does is it retrieves the best version available on the site with these options:  'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4 and output a video that should reflect that quality (resolution/aspect ratio, etc)',  Similarly, if you're processing a local video, it'll do the same matching of quality as output.

I mean, you could run it through another program that specializes in video editing and artificially upscale the video to that 1920x1080 aspect ratio but it's not going to look any better if that's the intent. If you want to increase the actual video resolution, that would involve using super resolution methods such as those offered by  [Topaz Labs.](https://www.topazlabs.com/video-enhance-ai)",would beyond scope logic best version available site output video reflect quality ratio similarly local video matching quality output mean could run another program video artificially upscale video aspect ratio going look better intent want increase actual video resolution would involve super resolution topaz,issue,positive,positive,positive,positive,positive,positive
944589293,"There's a lot of things that could be going on here. Not to offend but I don't have enough information to go by so I have to start with the basics that amount to ""is your computer plugged in?"" tech support:

1. Did you try these two commands (url and local image based) one after another on the same session? i.e. are you sure the server was running at the time?
2. Are you sure you typed the same exact IP Address both times?
3. Have you seen [this thread](https://stackoverflow.com/questions/41027340/curl-connection-refused)?

I'll have to assume this is purely a local issue as this stuff as been used for three years with this working fine so I'm going to close this and wish the best of luck to your debugging here.",lot could going offend enough information go start amount computer plugged tech support try two local image based one another session sure server running time sure exact address time seen thread assume purely local issue stuff used three working fine going close wish best luck,issue,positive,positive,positive,positive,positive,positive
944552915,"It looks like @alexandrevicenzi is correct:
![image](https://user-images.githubusercontent.com/179759/137538663-374f4219-efb6-4372-87b6-0a83a6692180.png)

I've tried both on Colab and WIndows/Linux boxes with the wget calls to see if I could somehow reproduce this issue but I haven't. Perhaps a temporary hiccup?  @johnbreslin is it still happening on your end? I'm hesitant to incorporate the ignoring of the certificate check as default behavior, though an actual problem resulting from that seems unlikely...",like correct image tried see could somehow reproduce issue perhaps temporary hiccup still happening end hesitant incorporate certificate check default behavior though actual problem resulting unlikely,issue,negative,negative,negative,negative,negative,negative
942756044,"deepai.org certificates are not expired, there's something else wrong with your setup.

Invalid certificate errors can be caused by outdated CA certificates.",something else wrong setup invalid certificate outdated ca,issue,negative,negative,negative,negative,negative,negative
928059876,"@FoxTheMad None of those sources of images would be expected to work. The URL provided needs to be a direct image link that is public. Dropbox and Google Drive require password access and so you can't provide direct image links to the Colab, as it won't actually be logged into those services. You'd have to provide a direct public image link  from such a service as Imgur or Flickr. 

If you have a link to the tutorial you're referring to that would be interesting to see, but I'd be very surprised that there would be a way to do that.

As far as ""local cloud service"" I'm not quite sure what you mean by that, but if it's not providing a publicly accessible -direct- image url then the same thing applies here.

Anyway- I'd suggest rather than trying to figure out how to make this work somehow to instead  go with working with what works.  It'll make life easier. That could mean reuploading to public sites just to get the public link, which I've done myself before many times.  I've deliberately limited the scope of what's supported here because, well, I'm lazy, but also because the point is to cololrize and not to make an awesome image downloader.",none would work provided need direct image link public drive require password access ca provide direct image link wo actually logged provide direct public image link service link tutorial would interesting see would way far local cloud service quite sure mean providing publicly accessible image thing suggest rather trying figure make work somehow instead go working work make life easier could mean public get public link done many time deliberately limited scope well lazy also point make awesome image,issue,positive,positive,positive,positive,positive,positive
927254122,"This is what I get:

---------------------------------------------------------------------------
UnidentifiedImageError                    Traceback (most recent call last)
<ipython-input-36-95c6e7dbdd44> in <module>()
      4 
      5 if source_url is not None and source_url !='':
----> 6     image_path = colorizer.plot_transformed_image_from_url(url=source_url, render_factor=render_factor, compare=True, watermarked=watermarked)
      7     show_image_in_notebook(image_path)
      8 else:

2 frames
/usr/local/lib/python3.7/dist-packages/PIL/Image.py in open(fp, mode)
   2894         warnings.warn(message)
   2895     raise UnidentifiedImageError(
-> 2896         ""cannot identify image file %r"" % (filename if filename else fp)
   2897     )
   2898 

UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f441149b950>",get recent call last module none else open mode message raise identify image file else identify image file object,issue,negative,neutral,neutral,neutral,neutral,neutral
926106673,@groowyCZ  That call where it's plotting to matplotlib simply shouldn't have been there- it's totally unnecessary and will just cause problems.  I was waiting for the go-ahead to fix this from the original author since they were going to be directly affected by the change but that was quite a while ago and I haven't heard anything. So I've decided just to go ahead and fix it myself. Honestly too- I don't see any reason why the images should be saving/reading to disk but I'll just keep that in for now to not upset anybody's setup too much.  I may just eventually correct that too.,call plotting simply totally unnecessary cause waiting fix original author since going directly affected change quite ago anything decided go ahead fix honestly see reason disk keep upset anybody setup much may eventually correct,issue,negative,positive,positive,positive,positive,positive
926099912,"you should look into passing through your nvidia card to the VMware station 

https://communities.vmware.com/t5/VMware-Workstation-Pro/Make-VMWare-Workstation-use-my-Nvidia-card/td-p/2751434
https://communities.vmware.com/t5/VMware-Workstation-Pro/GPU-not-recognized-utilized-in-Workstation-Pro-16/td-p/2819765

it's very likely that you don't have the GPU passed through to the VM.

Their are 2 ways to do it and to my knowledge only 1 is compatible with non-datacenter GPUcard:
1) either you virtualize the GPU
2) or pass it though directly to the VM
",look passing card station likely way knowledge compatible either virtualize pas though directly,issue,negative,positive,neutral,neutral,positive,positive
919735815,Thank you for the high assessment of my work!,thank high assessment work,issue,negative,positive,positive,positive,positive,positive
919687742,"@GlebSBrykin I've been digging more into what you actually did here. I have to just emphasize that it's pretty impressive. This for example- 

[https://github.com/ColorfulSoft/DeOldify.NET/blob/main/Implementation/src/Functional.cs)](https://github.com/ColorfulSoft/DeOldify.NET/blob/main/Implementation/src/Functional.cs)

And this:

[https://github.com/ColorfulSoft/DeOldify.NET/blob/main/Implementation/src/HalfHelper.cs](https://github.com/ColorfulSoft/DeOldify.NET/blob/main/Implementation/src/HalfHelper.cs)",digging actually emphasize pretty impressive,issue,positive,positive,positive,positive,positive,positive
919650326,"There's now a Windows GUI version, courtesy of ColorSoft- [https://github.com/ColorfulSoft/DeOldify.NET](https://github.com/ColorfulSoft/DeOldify.NET)

It works!  Even without a GPU.",version courtesy work even without,issue,negative,neutral,neutral,neutral,neutral,neutral
919646923,"@GlebSBrykin  This is fantastic.  I gave it a spin locally and it strikes a great balance between optimization for the CPU and quality of output. It's super low barrier to entry as a result. And you've made the UI super simple which I like. You're right, people have been asking for this for ages.  I'll spread the word on Twitter and link to this on the readme.  Thank you so much!

![image](https://user-images.githubusercontent.com/179759/133360009-93e1d55b-5523-4346-b3f4-523180840452.png)
",fantastic gave spin locally great balance optimization quality output super low barrier entry result made super simple like right people spread word twitter link thank much image,issue,positive,positive,positive,positive,positive,positive
919587708,"> @galaxyentity904 With a conda install (which is strongly recommended), this would happen if you forgot to activate the deoldify environment before launching Jupyter via the command line.  That is, you'd need to do this:
> 
> `conda activate deoldify`
> 
> or
> 
> `source activate deoldify`
> 
> One of those should work.
> 
> If you're not doing the conda install route, then it could be one of several things going wrong but you'd be on your own for that (not supported).

@jantic 
I did do a conda install. But also I figured out a fix I forgot to share, I switched to the version made for colab instead of whatever I was on before.
Thanks!",install strongly would happen forgot activate environment via command line need activate source activate one work install route could one several going wrong install also figured fix forgot share switched version made instead whatever thanks,issue,negative,positive,neutral,neutral,positive,positive
919368828,"@galaxyentity904 With a conda install (which is strongly recommended), this would happen if you forgot to activate the deoldify environment before launching Jupyter via the command line.  That is, you'd need to do this:

`conda activate deoldify`

or

`source activate deoldify`

One of those should work.

If you're not doing the conda install route, then it could be one of several things going wrong but you'd be on your own for that (not supported).",install strongly would happen forgot activate environment via command line need activate source activate one work install route could one several going wrong,issue,negative,negative,neutral,neutral,negative,negative
918764390,What exactly did you do? im having the same error and cant get it working,exactly error cant get working,issue,negative,positive,positive,positive,positive,positive
914647632,"@srini8080  I'm not quite sure if you're trying to call a notebook from the docker api here- your deviating from the usage instructions in the readme. But you can't call .ipynb notebooks with the docker api like this if that's your intent.  

If you want to use the notebooks you'll want to do a normal jupyter run per instructions.",quite sure trying call notebook docker usage ca call docker like intent want use want normal run per,issue,positive,positive,positive,positive,positive,positive
909459884,"@christian2022  Yes this is certainly doable. You could really just get away with training on a normal dataset like Open Images and knock out the color channels in question with perhaps OpenCV to make the ""before"" images, and then use the original images as the ""after"".  That right there alone would probably be the 90% solution. You'd get additional benefits by ""crappifying"" the images to more closely simulate the kinds of images you'd be dealing with in reality.  We wrote a blog about that general idea [here](https://www.fast.ai/2019/05/03/decrappify/). 

Granted, all this being said, you're not going to get perfect video because it's not going to have explicit temporal modeling constraints (inherent limitation of DeOldify's simplified approach).",yes certainly doable could really get away training normal like open knock color question perhaps make use original right alone would probably solution get additional closely simulate dealing reality wrote general idea said going get perfect video going explicit temporal modeling inherent limitation simplified approach,issue,positive,positive,positive,positive,positive,positive
901422110,"@tasiotas There may be updates to the pretrained models in the future but no, they haven't been updated since the last major release in May 2019. There's been plenty of commercial work done since then but by not giving away that work for free I can pay the bills which is nice in the meantime!

I'm not aware of alternative models being hosted for free of DeOldify other than forks to do different things like anime colorization.",may future since last major release may plenty commercial work done since giving away work free pay nice aware alternative free different like anime colorization,issue,positive,positive,positive,positive,positive,positive
901420281,I'm not sure what you're trying to do here exactly or what's motivating it but that function it's complaining about (apply_tfms) is in the fastai library. We're not going to be making DeOldify installable without installing fastai and its dependencies if that's what you're attempting- we have no motivation to make our lives more difficult like that!,sure trying exactly function library going making without motivation make difficult like,issue,positive,positive,neutral,neutral,positive,positive
901418017,@tonybarnhill It wouldn't be too much code to create a batch process and in fact we have that internally here for our commercial work. But it's not going to be supported here in this project- generally the goal is to keep things as simple as possible and not create 100 more problems in the process of adding a feature that in this case is probably quite niche.,would much code create batch process fact internally commercial work going generally goal keep simple possible create process feature case probably quite niche,issue,negative,positive,neutral,neutral,positive,positive
898710221,"Hi, I just wanted to circle back around after a few weeks with this configuration. It's really fast at converting frames to color, now. 
I was wondering if there is a modification that could be made to allow batch processing with the still image Artistic mode? I ran a few frames through there manually and the color is a bit better than with the video processor.
Thanks for everything.",hi circle back around configuration really fast converting color wondering modification could made allow batch still image artistic mode ran manually color bit better video processor thanks everything,issue,positive,positive,positive,positive,positive,positive
890438307,I guess I cannot run on Virtual machine,guess run virtual machine,issue,negative,neutral,neutral,neutral,neutral,neutral
890437218,"Source file worked, but kernel crashed:
![kernelerror](https://user-images.githubusercontent.com/88035011/127757252-ae5b36ca-2da1-4e42-ac8e-5c5466e4b1ea.png)
",source file worked kernel,issue,negative,neutral,neutral,neutral,neutral,neutral
890435580,"This is covered in the [Easy Install section](https://github.com/jantic/DeOldify#easy-install) of the readme. 
",covered easy install section,issue,negative,positive,positive,positive,positive,positive
890434922,"Keep in mind too...even after you get past this point, I have serious doubts that this is going to actually render using GPU on the VM (there's another recent discussion on this).  CPU will be fine but slow AF. That's where you'll be on your own because that's beyond the scope of support here.",keep mind even get past point serious going actually render another recent discussion fine slow beyond scope support,issue,negative,negative,neutral,neutral,negative,negative
890434427," That's actually failing at the stage where it's trying to download the image and then read the results of the download as a PIL image. I tested the url itself and it's fine so this likely means you were having a connection issue with that url.  That could be internet access itself, or wikimedia was somehow down temporarily... or perhaps the site is some blocked for you (firewall?).  Did you check that on the vm? Perhaps try going to that URL specified in source_url in a browser.",actually failing stage trying image read image tested fine likely connection issue could access somehow temporarily perhaps site blocked check perhaps try going browser,issue,negative,positive,positive,positive,positive,positive
890168295,"@Archviz360 Did you make sure to run each cell in order as discussed above? The specific cell that needs to run successfully to avoid the error you're seeing is this one (but make sure to run all of them in order of course!)

![image](https://user-images.githubusercontent.com/179759/127714169-fd8490cd-f938-4b5e-b45b-30ab6a172d6e.png)
",make sure run cell order specific cell need run successfully avoid error seeing one make sure run order course image,issue,negative,positive,positive,positive,positive,positive
890166877,"I assume you mean VMWare Workstation Pro. I tried looking into this a few years ago because it would have been hella convenient. But it didn't work back then and it appears it still doesn't now.  It's a product distinction- apparently ESXi (never heard of it previously) is what you'd have to use for proper gpu passthrough to do DeOldify model training.

The latest credible discussion I could find on this topic over the past thirty minutes was [here](https://communities.vmware.com/t5/VMware-Workstation-Pro/ML-inside-Workstation-VMs/td-p/2314440) .

l wouldn't 100% take my word for it but that's what I know.

",assume mean pro tried looking ago would convenient work back still product apparently never previously use proper model training latest credible discussion could find topic past thirty would take word know,issue,negative,positive,neutral,neutral,positive,positive
890157928,"That can definitely work at least to a decent degree- I've already done super resolution using the same techniques. But honestly you may be able to get away with not doing the GAN part and just sticking with the generator model you get just training on Feature Loss.  The GAN portion was motivated by wanting to spice up the colors, but here you have a more straightforward target I think. Adding the GAN part might just make it more complicated and perhaps even more unstable than it needs to be.

I say it'll work to a certain degree because without temporal modeling, which DeOldify doesn't have, you're not going to get complete consistency between frames.  I think this will be less noticeable in this application but you'll have to try it to know for sure.

As far as training on a free colab goes- I don't think that's really a feasible option.  I haven't looked into it for a while but last I checked they have it such that it won't be up for more than a few hours-  training took a few days when I did it.   I'm pretty sure getting the amount of data you'll need available on there is going to be a big pain in the butt. Unless I'm lacking imagination on that one (possible..?).  On the plus side, the GPUs have a surprising amount of RAM- I just checked and got 15GB, so that's great because that's more than what I trained on for the open source DeOldify (11GB). 

I'd really really recommend just creating your own local box to do this work on. You don't have to get the latest GPU (they're pricey I know!). I used a 1080TI.  It'll make your life a lot easier to have this.",definitely work least decent already done super resolution honestly may able get away gan part sticking generator model get training feature loss gan portion wanting spice color straightforward target think gan part might make complicated perhaps even unstable need say work certain degree without temporal modeling going get complete consistency think le noticeable application try know sure far training free think really feasible option last checked wo training took day pretty sure getting amount data need available going big pain butt unless imagination one possible plus side surprising amount checked got great trained open source really really recommend local box work get latest know used ti make life lot easier,issue,positive,positive,positive,positive,positive,positive
889581493,"im stuck on this error 

ameError                                 Traceback (most recent call last)
<ipython-input-15-d41e8163fe4e> in <module>
----> 1 colorizer = get_image_colorizer(artistic=True)

NameError: name 'get_image_colorizer' is not defined",stuck error recent call last module name defined,issue,negative,neutral,neutral,neutral,neutral,neutral
887598474,"Yes, I am just talking about the training. Thanks for the instructions and your help that should do the job. Btw, great work :)",yes talking training thanks help job great work,issue,positive,positive,positive,positive,positive,positive
887152690,"Are you talking about inference as opposed to training a model? If inference, then I have no ambitions/plans whatsoever to try make it multi-gpu.  That would add way too much complication for very little benefit- you can't really render images at much higher resolutions decently anyway that would merit throwing resources at this sort of approach.

If you're talking about training- I use multiple GPUs routinely in my day to day work, and based it on these instructions from fastai: [https://fastai1.fast.ai/distributed.html](https://fastai1.fast.ai/distributed.html) . If I do a model upgrade (not this year) for the open source project I'll most likely update the project with the scripting code I have but it's basically mostly based around what I liked to there. You have to go the additional mile of making sure that you're using synched batchnorm though, and GANs don't work out of the box with this stuff in fastai v1.
",talking inference opposed training model inference whatsoever try make would add way much complication little ca really render much higher decently anyway would merit throwing sort approach talking use multiple routinely day day work based model upgrade year open source project likely update project code basically mostly based around go additional mile making sure though work box stuff,issue,positive,positive,positive,positive,positive,positive
887147200,"So I'll replay the sequence of events here so far.  

First this request is made by you:  

> I tried to load the model on streamlit, please help me in doing it as it will be helpful for my research topic but facing some issues with it.

I then answer it with this:

> This request is beyond the scope of what we're supporting for this project. I've never used streamlit and this project wasn't designed with that in mind (not sure what's involved there). But more to the point- it's a personal request for free remote debugging assistance on a specific and unique use case. We just don't do that here.

[Source](https://github.com/jantic/DeOldify/issues/366)

Then you run into a few issues and we see your problem-solving process play out in real time as you gradually figure out what you're doing:

[Here](https://github.com/jantic/DeOldify/issues/368)

And then above same thing apparently:

> Could you please let me know if any dictionary file exist where we have the model and weights saved?

The latter of course is part of the readme.

And then you wind up ignoring the first ""No"" alluded to above on the streamlit thing by this question:

> I am trying to deploy model using streamlit and facing issue?

There's a reason why I'm drawing boundaries here.  You're obviously doing a bunch of things that are outside the normal  use case of DeOldify and I can pretty much guarantee that the problems you've had so far were probably just the natural consequence of trying to do your own project.  It's fine to do your own project! But we're not here as your personal debuggers for the inevitable difficulties that arise. I'm not sure where you're getting this idea that that's what we do but sorry... I'm not particularly fond of doing things for free on demand in the middle of the night here on the west coast of the United States because they are ""urgent"". 

So from here forward, these requests are just going to be ignored and closed ok? 

End of story.",replay sequence far first request made tried load model please help helpful research topic facing answer request beyond scope supporting project never used project designed mind sure involved personal request free remote assistance specific unique use case source run see process play real time gradually figure thing apparently could please let know dictionary file exist model saved latter course part wind first thing question trying deploy model facing issue reason drawing obviously bunch outside normal use case pretty much guarantee far probably natural consequence trying project fine project personal inevitable arise sure getting idea sorry particularly free demand middle night west coast united urgent forward going closed end story,issue,positive,positive,positive,positive,positive,positive
887142240,"There was never a paper written for it but we did write a blog about it [here]( https://www.fast.ai/2019/05/03/decrappify/) .

Hope that helps!

",never paper written write hope,issue,negative,neutral,neutral,neutral,neutral,neutral
886254942,"Could you please let me know the what is the path or variable in which coloured image gets saved in your ""app.py"" file?",could please let know path variable image saved file,issue,positive,neutral,neutral,neutral,neutral,neutral
886245778,"I am trying to deploy model using streamlit and facing issue

![image](https://user-images.githubusercontent.com/80449168/126910724-ac3760e1-29af-4d7d-b8ea-53439fa808b7.png)
",trying deploy model facing issue image,issue,negative,neutral,neutral,neutral,neutral,neutral
885133385,This request is beyond the scope of what we're supporting for this project. I've never used streamlit and this project wasn't designed with that in mind (not sure what's involved there). But more to the point- it's a personal  request for free remote debugging assistance on a specific and unique use case.  We just don't do that here.,request beyond scope supporting project never used project designed mind sure involved personal request free remote assistance specific unique use case,issue,positive,positive,positive,positive,positive,positive
884538625,"Thank you, thank you, thank you many, many times!!! What a great job!",thank thank thank many many time great job,issue,positive,positive,positive,positive,positive,positive
884531743,"Ok guys, really sorry about this bug in the colab_requirements.txt file! I just fixed it and it appears to be solved now. And thanks for doing all this hunting for me so I knew where to look!

@dstef81 I specifically tried your video in that screenshot but I get a separate error complaining that the video is private.  You'll run into the same issue unless you make it something other than private (I think unlisted should work).",really sorry bug file fixed thanks hunting knew look specifically tried video get separate error video private run issue unless make something private think unlisted work,issue,negative,negative,neutral,neutral,negative,negative
884514327,Btw I too was thinking of installing the software locally on my 16GB Mac Mini M1.  The instructions I see are mostly linux based so I think it could be a good match.  I believe there is a need to install the training data though...  but I think you can run the same kind of scripts to get the data like it is in the Colab version of Deoldify.,thinking locally mac see mostly based think could good match believe need install training data though think run kind get data like version,issue,positive,positive,positive,positive,positive,positive
884511928,"Did you look in the Deoldify folder and find the colab_requirements.txt? 

![Screenshot 2021-07-22 at 00 02 57](https://user-images.githubusercontent.com/7872926/126566051-ce737f2e-bc35-4145-8115-69f5a0636a11.png)

I modified that just prior running the step in the image below by taking away the ffmpeg line as I believe the ffmpeg-python library is a wrapper for it.  You have to make sure the file saves before running thestep. 

It seemed to autosave after 5 seconds

![Screenshot 2021-07-22 at 00 02 28](https://user-images.githubusercontent.com/7872926/126566076-53d2fdc8-7519-4407-82ee-825949cfd220.png)

 I also made sure I reset the runtime before doing all this just so I knew it was all clean

<img width=""416"" alt=""Screenshot 2021-07-21 at 23 26 00"" src=""https://user-images.githubusercontent.com/7872926/126562431-23a50913-5c7a-44be-874d-eb29dcdb5362.png"">

I think successive runs of the different parts can cause the Deolldify software to be installed in a nested way... gets confusing.  So I prefer to reset if I need to run from the top.",look folder find prior running step image taking away line believe library wrapper make sure file running also made sure reset knew clean think successive different cause way prefer reset need run top,issue,positive,positive,positive,positive,positive,positive
884505974,"     Hello! The Zzip7's solution works for me. I also tried the orangefunk's one, but I failed 
to accomplish this, I feel I need more precise step by step instructions on this. If somebody 
needs the English translation of the Russian lines, I can give it.
      Interesting, why the Colab version of DeOldify is so unstable. One day it works fine, the next day
 it gives out an error. Probably, the developers are updating the software. I have not found any 
official notice on the ""No Attribute"" issue. I am considering installing DeOldify into my machine, 
but it's current GPU has only two Gb of memory, whereas the recommended one is four to
 six Gb. Besides, I am just a PC user and I am not confident at all of whether I will be able to install
 this thing.
",hello solution work also tried one accomplish feel need precise step step somebody need translation give interesting version unstable one day work fine next day error probably found official notice attribute issue considering machine current two memory whereas one four six besides user confident whether able install thing,issue,positive,positive,positive,positive,positive,positive
884356082,"I think I found another way to get it to run.

If you find colab_requirements.txt in the Deoldify directory and remove the ffmpeg line (it will save after a few seconds) then it emulates what Zzip is doing previously.

mine (after removing ffmpeg) says:

```
fastai==1.0.51
tensorboardX==1.6
ffmpeg-python==0.1.17
youtube-dl>=2019.4.17
opencv-python>=3.3.0.10
pillow
tornado~=5.1.0
```

I'm thnking that there must be some incompatibility going on in respect of versions...   ",think found another way get run find directory remove line save previously mine removing pillow must incompatibility going respect,issue,positive,negative,negative,negative,negative,negative
884295941,I'm still running into problems and I'm using the VideoColorizerColab.ipynb...  will try again later..,still running try later,issue,negative,neutral,neutral,neutral,neutral,neutral
884288497,Ahh maybe I went to the wrong project!  Thanks for being eagle eyed! I was using a new computer!,maybe went wrong project thanks eagle eyed new computer,issue,negative,negative,neutral,neutral,negative,negative
883889791,"> 
> 
>     1. Create a folder named ""models"" inside ""DeOldify"" folder.
> 
>     2. Download these [three](https://github.com/jantic/DeOldify#completed-generator-weights) weights from  **Completed Generator Weights** and paste inside ""models"" folder.
>        It solved my issue.

Thanks a lot, this is the only solution worked for me on Windows 10. Downloading only ""ColorizeArtistic_gen.pth"" was enough for image colorizing with artistic model [ colorizer = get_image_colorizer(artistic=True) ]. ",create folder inside folder three generator paste inside folder issue thanks lot solution worked enough image artistic model,issue,positive,positive,positive,positive,positive,positive
883767845,"I tried for a few hours and the fix didn't seem to work with files I have put in Dropbox...  (they worked before!)

![Screenshot 2021-07-21 at 00 47 51](https://user-images.githubusercontent.com/7872926/126407030-3d316f7e-3b6e-433a-bdc8-50dab90c41fd.png)

I am hoping there is a more general fix coming.  Otherwise I will have to look into hacking to get video files loaded from a directory on the host.
",tried fix seem work put worked general fix coming otherwise look hacking get video loaded directory host,issue,negative,positive,neutral,neutral,positive,positive
883387375,"Hello, I've tried it and it worked! Although i didnt understand a word, its very easy to follow the images, just make sure to run the commands before the import.fastai step. 
Mr 7zip you are amazing. большое тебе спасибо !",hello tried worked although didnt understand word easy follow make sure run step zip amazing,issue,positive,positive,positive,positive,positive,positive
883376997,Very good!  Yes will try this tonight!,good yes try tonight,issue,positive,positive,positive,positive,positive,positive
883369317,"Всем привет!Я вообще не программист,мучался 2 дня с такой же проблемой,но нашёл для себя решение.Даже на github зарегистрировался.Может кому поможет)))
1. Файл - Сохранить копию на диске
2. После строки   !pip install -r colab_requirements.txt   добавить ОТДЕЛЬНЫХ 3 кода.
pip uninstall ffmpeg
pip uninstall ffmpeg-python
pip install ffmpeg-python
![1](https://user-images.githubusercontent.com/87704893/126325084-2a096b0f-1bee-45b2-a3ce-81d617da5198.jpg)

3.После подключения согласиться 2 раза на удаление ""pip uninstall ffmpeg"" и ""pip uninstall ffmpeg-python""
![2](https://user-images.githubusercontent.com/87704893/126326228-bbe1cde9-62c4-4b12-a7a8-efc5fe719667.jpg)
![3](https://user-images.githubusercontent.com/87704893/126326254-71079d7d-7a53-4511-b847-ed28c5c99054.jpg)

4.Установить  ""pip install ffmpeg-python""
![4](https://user-images.githubusercontent.com/87704893/126327012-cd08f83e-104c-4a74-a231-5d0c6661a09c.jpg)

5.Дальше как обычно.

И ещё нашёл немного модифицированный код,который добавляет возможность загружать видеофайлы  прямо с компьютера,надеюсь автор тоже добавит такую функцию ))) 


",pip install pip pip pip install pip pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
883358470,"Not just me then... tried to run it via new Mac mini and thought some problem was occurring there but seems it's also broken via my old computer.

I think its a problem with the ffmpeg library. I have seen mention of using ffmpeg.python instead... ",tried run via new mac thought problem also broken via old computer think problem library seen mention instead,issue,negative,negative,neutral,neutral,negative,negative
883317328,"> The exact same issue has been happening for me. Seems to be across multiple machines and logins, and multiple settings choices in the colourize section.

Thats correct. I've tried with different browsers and google accounts but the problem is still there! Im a bit confused as it worked perfectly since recently. maybe something changed on the code? Who knows...
",exact issue happening across multiple multiple section thats correct tried different problem still bit confused worked perfectly since recently maybe something code,issue,negative,positive,positive,positive,positive,positive
883121186,"The exact same issue has been happening for me. Seems to be across multiple machines and logins, and multiple settings choices in the colourize section.",exact issue happening across multiple multiple section,issue,negative,positive,neutral,neutral,positive,positive
882855525,"@dan64 thank you for that. I created a new deoldify2 enviornment with your file. I needed to install ipywidgets to that environment otherwise was seeing errors. 
Iʻm seeing 8fps doing 1080p at render_factor=24

Thank you again!",dan thank new file install environment otherwise seeing seeing thank,issue,positive,positive,positive,positive,positive,positive
882712865,"In the case it can be useful, this is the **environment.yml** for **Linux**:

[environment_linux.yml.txt](https://github.com/jantic/DeOldify/files/6842699/environment_linux.yml.txt)

The environment for **Windows 10** (it is necessary to install Anaconda for windows)
is the following:

[deoldify_env_w10.yml.txt](https://github.com/jantic/DeOldify/files/6842696/deoldify_env_w10.yml.txt)

The encoding speed for the 2 OS is almost the same. On my hardware the encoding of a 720p movie with a render_factor=21 is 6.67fps on Linux and 6.48fps on Windows.
  



",case useful environment necessary install anaconda following speed o almost hardware movie,issue,negative,positive,neutral,neutral,positive,positive
882181652,"> @tonybarnhill gosh sorry about that. 

I fixed it and it completed successfully. I activated the new environment, and it's working. Thank You.

",gosh sorry fixed successfully new environment working thank,issue,positive,negative,neutral,neutral,negative,negative
882159625,"@tonybarnhill  Just based on a quick look at that error message there, it appears you may have simply introduced a typo inadvertently in the environmentnew.yml file you made there, towards the top.  I'd compare it to the original environment file and see where the difference lies.",based quick look error message may simply typo inadvertently file made towards top compare original environment file see difference,issue,negative,positive,positive,positive,positive,positive
882154104,"> @lkp520
> 
> That would mean commenting out lines that look like this (found in the ColorizeTrainingArtistic notebook):
> ` learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))`

Thank you! I try it.",would mean look like found notebook partial thank try,issue,positive,negative,negative,negative,negative,negative
882147528,"@dan64 thank you for your help!
 Ok, I created a new environmentnew.yml file in the DeOldify folder. But when I try to create the environment, I get an error report that says this:

>  File ""/home/admin/anaconda3/lib/python3.7/site-packages/ruamel_yaml/scanner.py"", line 345, in stale_possible_simple_keys
        ""could not find expected ':'"", self.reader.get_mark())
    ruamel_yaml.scanner.ScannerError: while scanning a simple key
      in ""<unicode string>"", line 4, column 1:
        fastai
        ^ (line: 4)
    could not find expected ':'
      in ""<unicode string>"", line 5, column 1:
        pytorch
        ^ (line: 5)

I'm on Ubuntu 18.04.5 LTS exactly. Is it an issue with the version of python?

The rest of the report:

> environment variables:
                 CIO_TEST=<not set>
  CONDA_AUTO_UPDATE_CONDA=false
        CONDA_DEFAULT_ENV=base
                CONDA_EXE=/home/admin/anaconda3/bin/conda
             CONDA_PREFIX=/home/admin/anaconda3
    CONDA_PROMPT_MODIFIER=(base)
         CONDA_PYTHON_EXE=/home/admin/anaconda3/bin/python
               CONDA_ROOT=/home/admin/anaconda3
              CONDA_SHLVL=1
           CURL_CA_BUNDLE=<not set>
                     PATH=/home/admin/anaconda3/bin:/home/tbadmin/anaconda3/bin:/home/tbadmin/
                          anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/
                          sbin:/bin:/usr/games:/usr/local/games:/snap/bin
       REQUESTS_CA_BUNDLE=<not set>
            SSL_CERT_FILE=<not set>
               WINDOWPATH=2

     active environment : base
    active env location : /home/admin/anaconda3
            shell level : 1
       user config file : /home/admin/.condarc
 populated config files : 
          conda version : 4.10.3
    conda-build version : 3.17.8
         python version : 3.7.3.final.0
       virtual packages : __cuda=11.2=0
                          __linux=5.4.0=0
                          __glibc=2.27=0
                          __unix=0=0
                          __archspec=1=x86_64
       base environment : /home/admin/anaconda3  (writable)
      conda av data dir : /home/admin/anaconda3/etc/conda
  conda av metadata url : None
           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64
                          https://repo.anaconda.com/pkgs/main/noarch
                          https://repo.anaconda.com/pkgs/r/linux-64
                          https://repo.anaconda.com/pkgs/r/noarch
          package cache : /home/admin/anaconda3/pkgs
                          /home/admin/.conda/pkgs
       envs directories : /home/admin/anaconda3/envs
                          /home/admin/.conda/envs
               platform : linux-64
             user-agent : conda/4.10.3 requests/2.21.0 CPython/3.7.3 Linux/5.4.0-77-generic ubuntu/18.04.5 glibc/2.27
                UID:GID : 1000:1000
             netrc file : None
           offline mode : False


",dan thank help new file folder try create environment get error report file line could find scanning simple key string line column line could find string line column line exactly issue version python rest report environment set base set set set active environment base active location shell level user file version version python version final virtual base environment writable data none channel package cache platform generic gid file none mode false,issue,positive,negative,negative,negative,negative,negative
882114579,"@GlebBrykin  I think perhaps this article we wrote a few years ago may answer your questions (?):  https://www.fast.ai/2019/05/03/decrappify/ .  It gets into some of those specifics that you're asking about, at least with video., in terms of how many iterations are optimal, and what that inflection point looks like before, during and after. 

But really I'd caution against doing anything but using your own eyeballs ultimately and only use what's been said so far as a very general starting point.  To this day, for example, I'm not sure how much the behavior of training changes if you increase batch size, for example. I suspect you'd be able to train with more stability for a longer time.

You do seem to have a good grasp of what's going on though with the algorithm- you're right as far as I can tell! ",think perhaps article wrote ago may answer least many optimal inflection point like really caution anything ultimately use said far general starting point day example sure much behavior training increase batch size example suspect able train stability longer time seem good grasp going though right far tell,issue,positive,positive,positive,positive,positive,positive
882114014,"This is great feedback and information for other users, so thank you guys for that.

I'm going to keep this open for now. Upgrading to the latest Pytorch in the code, at least for training purposes, will require either going to the new FastAI 2.0 or making a custom fork- the latter I've already done before successfully. It's just a few small changes as described here:  [https://github.com/jantic/DeOldify/issues/354](https://github.com/jantic/DeOldify/issues/354) . But it'll be quite a while before I sit down and commit to putting this in the repo formally for a number of reasons.",great feedback information thank going keep open latest code least training require either going new making custom latter already done successfully small quite sit commit formally number,issue,positive,positive,positive,positive,positive,positive
882110408,"@lkp520 

That would mean commenting out lines that look like this (found in the ColorizeTrainingArtistic notebook):
`
learn_gen.callback_fns.append(partial(ImageGenTensorboardWriter, base_dir=TENSORBOARD_PATH, name='GenPre'))`",would mean look like found notebook partial,issue,negative,negative,negative,negative,negative,negative
882089272,The problem in upgrading the deoldify environment is mainly due to the conda version used to perform the upgrade. I used an Anaconda version of 2019 both in windows and Ubuntu to perform this upgrade. More recent versions of conda performs more consistency checks and don't allow to apply the upgrade. Moreover my RTX 3060 is supported only with the NVIDIA version 465 (cuda 11.3) which is available by default only with Ubuntu 21.04.  ,problem environment mainly due version used perform upgrade used anaconda version perform upgrade recent consistency allow apply upgrade moreover version available default,issue,negative,positive,neutral,neutral,positive,positive
882080483,"Thank You very much. I had originally installed Ubuntu 20 but couldnʻt get anywhere with it, so attempted with 18.

I updated my Ubuntu 18 install by using:
`conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=11.1.1 -c pytorch -c conda-forge`

Iʻm running the same job as yesterday and itʻs now taking 14hrs instead of 23hrs, so huge improvement there!

Thanks Again",thank much originally get anywhere install install running job yesterday taking instead huge improvement thanks,issue,positive,positive,positive,positive,positive,positive
882052881,"I tested the RTX 3060 also with Ubuntu 21.04.  In this case to use DeOldify with CUDA 11.1 it is necessary to create a new environment following these steps:

1) Create a new  environment.yml with the following text:
`name: deoldify1
channels:
- fastai
- pytorch
- conda-forge
- defaults
dependencies:
- pip
- python>=3.7.3
- fastai=1.0.51
- ffmpeg=4.1.1
- tensorboardX=1.6
- youtube-dl>=2019.4.17
- jupyterlab
- pillow=8.2.0
- pip:
  - ffmpeg-python==0.1.18
  - opencv-python>=3.3.0.10
  - wandb`

2) create the env with command:
  
  conda env create -f environment.yml

3) finally install pytorch with cuda 11.1 support with following command:

conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c nvidia


P.S. With the version 1.10 pytorch should add the support to CUDA 11.3, this should improve the stability and the performance.

",tested also case use necessary create new environment following create new following text name pip python pip create command create finally install support following command install version add support improve stability performance,issue,positive,positive,neutral,neutral,positive,positive
881906881,"I got the same problem with my RTX 3060
The problem is that the pytorch version used by deoldify does not support the architecture Ampere. 
It is necessary to switch to CUDA 11.1.

To solve the problem I installed the last version of pytorch with the following command:

`conda install pytorch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 cudatoolkit=11.1.1 -c pytorch -c conda-forge`

I tested it on Windows 10 and the it is working properly (the encoding speed increased by about 3x respect to my old GTX 1070). 
I need to do more tests because both Windows 10 and pytorch 1.9.0 are not supported by deoldify.",got problem problem version used support architecture ampere necessary switch solve problem last version following command install tested working properly speed respect old need,issue,negative,positive,neutral,neutral,positive,positive
881136251,"> I figured it out, it happens when the tensorboard callback is appended. If I remove it memory leak not happens.

Can you tell me the detail of removing tensorboard callback? I don not know how to do it. Thank you very much!",figured remove memory leak tell detail removing know thank much,issue,negative,positive,positive,positive,positive,positive
874544335,"I do it like this: 

```
source_url=None
img_root='./test_images'
i=0
for img_path in os.listdir(img_root):
    source_path=os.path.join(img_root,img_path)
    i+=1
    print(i,source_path)
    # source_path = 'test_images/1.jpg'
    result_path = None
    if source_url is not None:
        result_path = colorizer.plot_transformed_image_from_url(url=source_url, path=source_path, render_factor=render_factor, compare=True)
    else:
        result_path = colorizer.plot_transformed_image(path=source_path, render_factor=render_factor, compare=True)
    # show_image_in_notebook(result_path)
```

and comment the code in visualize.py. (this code will cause CPU memory to increase)

```
    def plot_transformed_image(
        self,
        path: str,
        results_dir:Path = None,
        figsize: (int, int) = (20, 20),
        render_factor: int = None,
        display_render_factor: bool = False,
        compare: bool = False,
        post_process: bool = True,
        watermarked: bool = True,
    ) -> Path:
        path = Path(path)
        if results_dir is None:
            results_dir = Path(self.results_dir)
        result = self.get_transformed_image(
            path, render_factor, post_process=post_process,watermarked=watermarked
        )
        orig = self._open_pil_image(path)
        ###############
        # if compare:
        #     self._plot_comparison(
        #         figsize, render_factor, display_render_factor, orig, result
        #     )
        # else:
        #     self._plot_solo(figsize, render_factor, display_render_factor, result)
        ###############
        orig.close()
        result_path = self._save_result_image(path, result, results_dir=results_dir)
        result.close()
        return result_path
```

",like print none none else comment code code cause memory increase self path path none none bool false compare bool false bool true bool true path path path path none path result path path compare result else result path result return,issue,positive,negative,neutral,neutral,negative,negative
870399342,Thank you. The Colab set up worked perfectly on a 4 minute video.,thank set worked perfectly minute video,issue,positive,positive,positive,positive,positive,positive
869240565,"Hi @myfta, 

As far as we know, DeepAI has only implemented the still-image colorization of DeOldify. Can you please share the link you are using to colorize video? 

The DeOldify video colab is provided here: https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb. There is a tutorial on how to use DeOldify notebooks here: https://www.youtube.com/watch?v=VaEl0faDw38&t=4s. This information is also available in the readme, and further questions on this topic will be referred there.

Generally speaking for the video colab, if the file won't process, it still might be too long and it's recommended to try a smaller segment. ",hi far know colorization please share link colorize video video provided tutorial use information also available topic generally speaking video file wo process still might long try smaller segment,issue,positive,positive,neutral,neutral,positive,positive
868747545,Please try your experiment in a Linux environment and the problem should resolve. ,please try experiment environment problem resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
864271786,"I guess the website was temporarily down, as everything now works.",guess temporarily everything work,issue,negative,neutral,neutral,neutral,neutral,neutral
861741326,"Hello,
finally, by installing ffmpeg-python from inside the conda env, did the trick for me! 
There was no way to figure it out as its the first time i try something on conda.
Thank you very much!
",hello finally inside trick way figure first time try something thank much,issue,negative,positive,positive,positive,positive,positive
861338400,delete ffmpeg and use default version ffmpeg-python in requirements.txt. It's ok,delete use default version,issue,negative,neutral,neutral,neutral,neutral,neutral
860894427,"@dstef81 Something I noticed there is that it doesn't look like you're using conda in that pip command to reinstall ffmeg-python . I'd start with that- install your environment using the provided conda install, as that'll do a great job of making sure you have your dependencies in order. I've been able to run this code fine myself so this really looks like an environment setup issue on your end.",something look like pip command reinstall start install environment provided install great job making sure order able run code fine really like environment setup issue end,issue,positive,positive,positive,positive,positive,positive
860068100,"Hello!

This is the full screen, hope this one helps, thanks! :
Last Modified

#NOTE:  This must be the first call in order to work properly!

from deoldify import device

from deoldify.device_id import DeviceId

#choices:  CPU, GPU0...GPU7

device.set(device=DeviceId.GPU0)

<DeviceId.GPU0: 0>

from deoldify.visualize import *

plt.style.use('dark_background')

import warnings

warnings.filterwarnings(""ignore"", category=UserWarning, message="".*?Your .*? set is empty.*?"")

colorizer = get_video_colorizer()

Instructions
source_url

Type in a url hosting a video from YouTube, Imgur, Twitter, Reddit, Vimeo, etc. Many sources work! GIFs also work. Full list here: https://ytdl-org.github.io/youtube-dl/supportedsites.html NOTE: If you want to use your own video, you can set source_url to None and just upload the file to video/source/ in Jupyter. Just make sure that the file_name parameter matches the file you uploaded.
file_name

Name this whatever sensible file name you want (minus extension)! It should actually exist in video/source if source_url=None
render_factor

The default value of 21 has been carefully chosen and should work -ok- for most scenarios (but probably won't be the -best-). This determines resolution at which the color portion of the video is rendered. Lower resolution will render faster, and colors also tend to look more vibrant. Older and lower quality film in particular will generally benefit by lowering the render factor. Higher render factors are often better for higher quality videos and inconsistencies (flashy render) will generally be reduced, but the colors may get slightly washed out.
file_name_ext

There's no reason to changes this.
result_path

Ditto- don't change.
How to Download a Copy

Simply shift+right click on the displayed video and click ""Save video as...""!
Pro Tips

    If a video takes a long time to render and you're wondering how well the frames will actually be colorized, you can preview how well the frames will be rendered at each render_factor by using the code at the bottom. Just stop the video rendering by hitting the stop button on the cell, then run that bottom cell under ""See how well render_factor values perform on a frame here"". It's not perfect and you may still need to experiment a bit especially when it comes to figuring out how to reduce frame inconsistency. But it'll go a long way in narrowing down what actually works.

Troubleshooting

The video player may wind up not showing up, in which case- make sure to wait for the Jupyter cell to complete processing first (the play button will stop spinning). Then follow these alternative download instructions

    In the menu to the left, click Home icon.
    By default, rendered video will be in /video/result/

If a video you downloaded doesn't play, it's probably because the cell didn't complete processing and the video is in a half-finished state. If you get a 'CUDA out of memory' error, you probably have the render_factor too high. The max is 44 on 11GB video cards.
Colorize!!

#NOTE:  Max is 44 with 11GB video cards.  21 is a good default

render_factor=21

#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification

source_url='https://www.youtube.com/watch?v=HQXNdcNQWe4'

file_name = 'sevrwn'

file_name_ext = file_name + '.mp4'

result_path = None

​

if source_url is not None:

    result_path = colorizer.colorize_from_url(source_url, file_name_ext, render_factor=render_factor)

else:

    result_path = colorizer.colorize_from_file_name(file_name_ext, render_factor=render_factor)

​

show_video_in_notebook(result_path)

[youtube] HQXNdcNQWe4: Downloading webpage
[youtube] HQXNdcNQWe4: Downloading MPD manifest
[dashsegments] Total fragments: 22
[download] Destination: video/source/sevrwn.f137.mp4
[download] 100% of 52.26MiB in 01:214.84KiB/s ETA 00:00:21
[download] Destination: video/source/sevrwn.mp4.f140
[download] 100% of 2.05MiB in 00:03.76KiB/s ETA 00:00
[ffmpeg] Merging formats into ""video/source/sevrwn.mp4""
Deleting original file video/source/sevrwn.f137.mp4 (pass -k to keep)
Deleting original file video/source/sevrwn.mp4.f140 (pass -k to keep)

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-5-3fa061bb217c> in <module>
      8 
      9 if source_url is not None:
---> 10     result_path = colorizer.colorize_from_url(source_url, file_name_ext, render_factor=render_factor)
     11 else:
     12     result_path = colorizer.colorize_from_file_name(file_name_ext, render_factor=render_factor)

~/DeOldify/DeOldify/deoldify/visualize.py in colorize_from_url(self, source_url, file_name, render_factor, post_process, watermarked)
    338         self._download_video_from_url(source_url, source_path)
    339         return self._colorize_from_path(
--> 340             source_path, render_factor=render_factor, post_process=post_process,watermarked=watermarked
    341         )
    342 

~/DeOldify/DeOldify/deoldify/visualize.py in _colorize_from_path(self, source_path, render_factor, watermarked, post_process)
    356                 'Video at path specfied, ' + str(source_path) + ' could not be found.'
    357             )
--> 358         self._extract_raw_frames(source_path)
    359         self._colorize_raw_frames(
    360             source_path, render_factor=render_factor,post_process=post_process,watermarked=watermarked

~/DeOldify/DeOldify/deoldify/visualize.py in _extract_raw_frames(self, source_path)
    254         bwframes_folder.mkdir(parents=True, exist_ok=True)
    255         self._purge_images(bwframes_folder)
--> 256         ffmpeg.input(str(source_path)).output(
    257             str(bwframe_path_template), format='image2', vcodec='mjpeg', qscale=0
    258         ).run(capture_stdout=True)

AttributeError: module 'ffmpeg' has no attribute 'input'

",hello full screen hope one thanks last note must first call order work properly import device import import import ignore set type hosting video twitter many work also work full list note want use video set none file make sure parameter file name whatever sensible file name want minus extension actually exist default value carefully chosen work probably wo resolution color portion video lower resolution render faster color also tend look vibrant older lower quality film particular generally benefit lowering render factor higher render often better higher quality flashy render generally reduced color may get slightly washed reason change copy simply click displayed video click save video pro video long time render wondering well actually preview well code bottom stop video rendering stop button cell run bottom cell see well perform frame perfect may still need experiment bit especially come reduce frame inconsistency go long way actually work video player may wind showing make sure wait cell complete first play button stop spinning follow alternative menu left click home icon default video video play probably cell complete video state get memory error probably high video colorize note video good default note make none read file directly without modification none none else manifest total destination eta destination eta original file pas keep original file pas keep recent call last module none else self return self path could found self module attribute,issue,positive,positive,positive,positive,positive,positive
852590474,Actually sorry @KarolyPoka I slightly misremembered and thought I needed to change existing repo.  But anyway- one thing I've actually talked about about our work beyond the open source repo here was on the fastai forums-  that I had trouble with the new GraphWriter code that was added and my solution was to make a fork of fastai and comment that graph_writer code out.   [forum post here](https://forums.fast.ai/t/tensorboard-integration/38023/21) . I honestly don't remember what all I saw at the time with that code but it was definitely problematic and kept me from moving forward. ,actually sorry slightly thought change one thing actually work beyond open source trouble new code added solution make fork comment code forum post honestly remember saw time code definitely problematic kept moving forward,issue,negative,negative,neutral,neutral,negative,negative
852565553,@KarolyPoka oh that's a fantastic find! I'll try to see what needs to be changed in order to get things taken care of there then. I think I may of done that....not sure....,oh fantastic find try see need order get taken care think may done sure,issue,positive,positive,positive,positive,positive,positive
852564096,"I appreciate this, I really do! So please don't take what I'm about to say as anything against you or the thought behind it.  

Anyway, the gist is we've taken the stance that we want to keep things as simple as possible for maintenance and so we don't want to add more nodes of potential problems. (i.e. ""HALP MY KAGGLE NOTEBOOK WON'T TRAIN!!!"").  There's that concern and then there's the lack of desire to have our repo look like a NASCAR vehicle plastered with corporate logos.

So with that said I'll have to decline this one.  But again, thank you!",appreciate really please take say anything thought behind anyway gist taken stance want keep simple possible maintenance want add potential notebook wo train concern lack desire look like vehicle corporate logo said decline one thank,issue,positive,negative,neutral,neutral,negative,negative
850884589,"I figured it out, it happens when the tensorboard callback is appended. If I remove it memory leak not happens.",figured remove memory leak,issue,negative,neutral,neutral,neutral,neutral,neutral
850846468,"Thank you for the answer! I made some changes which helped:
1. fastai.vision.transform.py: line 243 torch.gsv replaced with torch.solve
2. fastai.vision.image.py: line 183 tensors converted to float
3. fastai/vision/image.py:
**replace**:
F.interpolate(x[None], scale_factor=1/d, mode='area') 
by
F.interpolate(x[None], scale_factor=1/d, mode='area', recompute_scale_factor=True)
**replace:**
return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode)[0]
by
return F.grid_sample(x[None], coords, mode=mode, padding_mode=padding_mode, align_corners=True)[0]

The ColorizeTrainingStable.ipynb is running until the second epoch, where the system is running out of CPU RAM (64Gb). I read that if num_workers>0 the memory leak problem could happen with PyTorch. I did not find any solution for that problem (even with changing the number of workers), so I wanted to ask if you have or had the same issue with the newer PyTorch version. I checked all of the issue threads but this particular problem was never solved. Could you help me with this? Thanks in advance!",thank answer made line line converted float replace none none replace return none return none running second epoch system running ram read memory leak problem could happen find solution problem even number ask issue version checked issue particular problem never could help thanks advance,issue,negative,positive,positive,positive,positive,positive
850013152,I took a quick look at this but it appears to be more involved than I imagined.  I'll be committing a fix that seems to speed things up a bit when doing CPU inference (about 10%) . But I won't be looking into this further anytime soon. Optimizing for CPU just isn't a focus of this project.,took quick look involved fix speed bit inference wo looking soon focus project,issue,negative,positive,positive,positive,positive,positive
849993152,"So you can indeed update this project to the latest PyTorch and corresponding cuda11.  We did that with our internal fork of DeOldify, which has become our commercial offerings. We're not ready yet to update the open source code with that, but I can tell you that it involves making a few changes to a fork of Fastai 1.0.51. The errors should pretty much guide you without too much ambiguity.",indeed update project latest corresponding internal fork become commercial ready yet update open source code tell making fork pretty much guide without much ambiguity,issue,positive,positive,positive,positive,positive,positive
841105234,"for **AttributeError: module 'ffmpeg' has no attribute 'input'**

I fix `Dockerfile` online 23:
`
RUN pip install --upgrade pip \
        && pip install versioneer==0.18 \
                tensorboardX==1.6 \
                Flask==1.1.1 \
                pillow==6.1 \
                numpy==1.16 \
                scikit-image==0.15.0 \
                requests==2.21.0 \
                # ffmpeg==1.4 \
                # ffmpeg-python==0.1.17 \
                ffmpeg-python \
                youtube-dl>=2019.4.17 \
                jupyterlab==1.2.4 \
                opencv-python>=3.3.0.10 \
                fastai==1.0.51
`

I try on Ubuntu and CentOS; it works！
In container `ffmpeg-python==0.2.0`",module attribute fix run pip install upgrade pip pip install try container,issue,negative,neutral,neutral,neutral,neutral,neutral
840774479,"From the readme:

`Windows is not supported and any issues brought up related to this will not be investigated.`

This hasn't changed and I wouldn't bet on this stance changing anytime soon.  I'll also point out that you're using PyTorch and Python versions that won't work with DeOldify. 

There's detailed instructions on how to install, and many issues opened already about trying to run on Windows where I have to keep swatting them down and pointing them to the same thing in the readme over and over again.

Personalized assistance on the inevitable issues of ignoring all this just isn't something we're willing to do. Sorry!

</rant>",brought related would bet stance soon also point python wo work detailed install many already trying run keep pointing thing assistance inevitable something willing sorry,issue,negative,positive,positive,positive,positive,positive
839042377,Start a patreon for early access my man.,start early access man,issue,negative,positive,neutral,neutral,positive,positive
835186705,"1.We later ran the project through the following command：
 ->$:docker build -t deoldify_jupyter -f Dockerfile .
And the effect looks great！
2.But during the deployment process, we found some problems and modified the code to solve them. I don't know if this problem is usual or independent. Here is a record for you:
  We run DeOldify through step 1, but we found that the CPU utilization rate during the operation is 3000%, while the GPU utilization rate is 0%. This situation is the same in the video slicing and colorize process running.
  We made the process run on the GPU by forcibly modifying the code. We haven’t found a deeper reason yet.
  /deoldify/_device.py  line 12 ： self.set(DeviceId.CPU) -> self.set(DeviceId.GPU0)
  ",later ran project following docker build effect deployment process found code solve know problem usual independent record run step found utilization rate operation utilization rate situation video slicing colorize process running made process run forcibly code found reason yet line,issue,negative,positive,neutral,neutral,positive,positive
832800328,"did you login to docker as request per the error message ?
",login docker request per error message,issue,negative,neutral,neutral,neutral,neutral,neutral
832097631,"@RiverTre This problem you had here was apparently with another project completely, fyi.  This is the DeOldify repo you've commented on, a colorization project. You're talking about an nlp  project here....",problem apparently another project completely colorization project talking project,issue,negative,positive,neutral,neutral,positive,positive
832095641,"> Is there a commercial product I can buy with the most current development efforts that colorizes video any better then deoldify?

Not that I'm aware of, but honestly I really haven't been looking lately, so I could easily be missing something. If anybody else knows of anything, please chime in.

It's a difficult problem and apparently a very niche space to work in.

> How much difference are the new generated weights compared to what has been released?

We don't have a new video model.  We have new images models which we consider to be prerequisite work to making _good_ video, but we don't feel like we're at the point both research-wise and  bandwidth-wise in what we can do with our time to be able to do a true leap in the tech that would be something we could commercialize.  

Keep in mind, people definitely want this and we're well aware of that fact, but going from that to something that can actually work out as a business with just two of us working at DeOldify is a big difference. Seriously, it's just two of us!

To be fair we did experimentation on a ""1.5"" version of sorts back in summer of 2019 but it wasn't the kind of leap we were looking for.  [See here for an example.]( https://www.youtube.com/watch?v=u9Z3lCmIOlg) . But there's a number of reasons why that remained just an experiment which I won't delve into here.",commercial product buy current development video better aware honestly really looking lately could easily missing something anybody else anything please chime difficult problem apparently niche space work much difference new new video model new consider prerequisite work making video feel like point time able true leap tech would something could commercialize keep mind people definitely want well aware fact going something actually work business two u working big difference seriously two u fair experimentation version back summer kind leap looking see example number experiment wo delve,issue,positive,positive,positive,positive,positive,positive
832087501,"So I'll help you here but keep in mind that this is research code that I'm putting out there for others to use.  It's not an easy to use product nor is it intended to be, so I won't be spending the time to do that. I have plenty of other stuff to do that actually pays the bills, to be frank!

Here's a screenshot of where you should be navigating to find the video:

![image](https://user-images.githubusercontent.com/179759/117039471-aa3f3e80-acbd-11eb-8aa9-429e819f5d6b.png)
",help keep mind research code use easy use product intended wo spending time plenty stuff actually frank find video image,issue,positive,positive,positive,positive,positive,positive
829237950,"I'm on the same boat with theman8631.  I too am colorizing old video clips for personal use.  Deoldify has been a magical tool, especially seeing my grandfather in color.  How much difference are the new generated weights compared to what has been released?   I hope you'll consider making a consumer product in the near future. ",boat old video clip personal use magical tool especially seeing grandfather color much difference new hope consider making consumer product near future,issue,negative,positive,positive,positive,positive,positive
828581209,"@jantic  Thank you so much for your reply. And to be brief, the problem was solved. 

Here are some details:
And about the details of the issue. My computer is Ubuntu2004 and has 32 GB RAM. I was using the notebook of ohmeow-blurr to fine tune BART-LAREGE-CNN, after some trying, I found the reason for the overwhelming  memory use is my metrics setting. When I use bert-score along with rouge to fine tune, bert-score uses a lot of memories and rouge uses less. Based on my 370000  full text of scientific articles, it really costs much memory ---- more than 20 G i think. So I tried training on 3700 and 37000 size dataset, it works fine.  ",thank much reply brief problem issue computer ram notebook fine tune trying found reason overwhelming memory use metric setting use along rouge fine tune lot rouge le based full text scientific really much memory think tried training size work fine,issue,negative,positive,positive,positive,positive,positive
827609007,"1，set the learn.model_dir in the generators.py
The source code is showed below:
def gen_inference_wide(
    root_folder: Path, weights_name: str, nf_factor: int = 2, arch=models.resnet101) -> Learner:
    data = get_dummy_databunch()
    learn = gen_learner_wide(
        data=data, gen_loss=F.l1_loss, nf_factor=nf_factor, arch=arch
    )
    learn.path = root_folder
    print(weights_name)
    learn.model_dir=""/home/xc/AIModel/DeOldify/models""
    learn.load(weights_name)
    learn.model.eval()
    return learn

It works for me ",source code path learner data learn print return learn work,issue,negative,neutral,neutral,neutral,neutral,neutral
827063956,"This is actually a thing PyTorch would need to implement, and as far as I can tell, this feature certainly has been requested but they have yet to implement it (don't know if they ever will).

I can tell you though if you're really quite fine with sacrificing speed to be able to render at higher render_factors (which doesn't necessarily imply higher quality, to be clear), you can run the model on CPU by doing this at the top (before calling anything else)

```
from deoldify import device
from deoldify.device_id import DeviceId
device.set(device=DeviceId.CPU)
```

But, fair warning, that will be very slow!",actually thing would need implement far tell feature certainly yet implement know ever tell though really quite fine sacrificing speed able render higher necessarily imply higher quality clear run model top calling anything else import device import fair warning slow,issue,positive,positive,positive,positive,positive,positive
827059097,"@omegavegas1313  There's no plan to update them anytime soon, and the last time there was an update was in April 2019 with the release of DeOldify V2 (DeOldify V1 was in November 2018). We develop commercial models now separate from this open source project. There's a possibility that there will be weight updates benefiting from that work in the more distant future (we're talking years not months), but I wouldn't hold your breath. ",plan update soon last time update release develop commercial separate open source project possibility weight work distant future talking would hold breath,issue,negative,negative,neutral,neutral,negative,negative
827050092,"I can tell you this much:  <DeviceId.GPU0: 0> isn't a red flag per se.  You'll be able to narrow the problem down further by running these two lines and seeing if you get ""True"" as a result (you should):

```
import torch
print(torch.cuda.is_available())
```
And if that evaluates to True, apparently that doesn't necessarily mean you're going to have success with accessing the GPU anyway as far as this WSL stuff is concerned. There's a whole thread about the difficulties really smart people like Sylvain Gugger himself had [dealing with trying to get it to work with Fastai](https://forums.fast.ai/t/platform-windows-10-using-wsl2-w-gpu/73521/31) . So in short, this looks like it's full of pitfalls. [This more recent blog post](https://christianjmills.com/Using-PyTorch-with-CUDA-on-WSL2/) might be good to follow, as they got PyTorch working.

Anyway this is almost certainly a WSL setup issue independent of DeOldify, so I'm going to close this issue.  Good luck!
",tell much red flag per se able narrow problem running two seeing get true result import torch print true apparently necessarily mean going success anyway far stuff concerned whole thread really smart people like dealing trying get work short like full recent post might good follow got working anyway almost certainly setup issue independent going close issue good luck,issue,positive,positive,positive,positive,positive,positive
827041302,"@RiverTre I'll gladly accept pull requests that address this but at this point this repo is up primarily to share research and as I said above, there just hasn't been any activity on this for the past two years. But while we're at it, can you lay out what your system specs are- operating system and amount of RAM?",gladly accept pull address point primarily share research said activity past two lay system spec operating system amount ram,issue,positive,positive,positive,positive,positive,positive
826074221,"So sad, I also encountered this issue. fit_one_cycle() is all doing fine until it reach 100% point and my RAM is rapidly increasing and then out of memory caused python to crashed",sad also issue fine reach point ram rapidly increasing memory python,issue,negative,negative,neutral,neutral,negative,negative
823795727,"Thank you again. Nvidia driver compatibility problem. Replaced 460 server ( proprietary) to 460 ( proprietary, tested). Working great and fast.. thank you",thank driver compatibility problem server proprietary proprietary tested working great fast thank,issue,positive,positive,positive,positive,positive,positive
821890416,Not even I remembered this anymore. Feel free to close this issue if no more work is needed.,even feel free close issue work,issue,positive,positive,positive,positive,positive,positive
821568848,Answers are already provided in notebook and readme. I won't be reiterating what has already been said.,already provided notebook wo already said,issue,negative,neutral,neutral,neutral,neutral,neutral
820834776,@alexandrevicenzi Somehow I totally missed this and just realized that you had made the wiki page.  I can't believe I just now realized this.  Belated thank you!,somehow totally made page ca believe belated thank,issue,negative,neutral,neutral,neutral,neutral,neutral
820834081,Going to close this.  The problem is acknowledged but there's no plans to improve upon this in this repo. ,going close problem acknowledged improve upon,issue,negative,neutral,neutral,neutral,neutral,neutral
820833085,I'm going to close this.  It's been inactive for too long.,going close inactive long,issue,negative,negative,neutral,neutral,negative,negative
820750015,"If you're getting some images failing to colorize, it's very likely that you're running out of GPU memory with them and you should be seeing a message like this if that's the case:

> Warning: render_factor was set too high, and out of memory error resulted. Returning original image.

3GB is a pretty small amount of GPU memory for this application, unfortunately.  I'd recommend more on the order of 8GB+.

Now as far as being slow on the Colab, you'll want to make sure that the runtime type is set to GPU.  Otherwise, it'll be running on CPU and yeah that'll be slow!",getting failing colorize likely running memory seeing message like case warning set high memory error original image pretty small amount memory application unfortunately recommend order far slow want make sure type set otherwise running yeah slow,issue,positive,positive,neutral,neutral,positive,positive
820747799,"This would be a ""batch process"" and is beyond the scope of what we're willing to provide support for here.  What you're trying to do here simply isn't supported in the notebooks.",would batch process beyond scope willing provide support trying simply,issue,negative,positive,positive,positive,positive,positive
820747131,@gitLRD  That's a little strange... Can you copy/paste here the curl command you used verbatim?  I know you're saying it's exactly the same as what I provided but I highly suspect there's something that's going to jump out to me that will help solve this (such as the image file name).,little strange curl command used verbatim know saying exactly provided highly suspect something going jump help solve image file name,issue,negative,positive,neutral,neutral,positive,positive
820744507,"As far as using the Colab for artistic=False, that's the ""stable"" model and there's a separate Colab for that called ImageColorizerColabStable. [Link to Colab](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColabStable.ipynb )

I'm a bit confused as to what you're trying to do running it locally but you can just do it through that link and run it with Google's servers. They do have the option to run it locally too via ""connect to local runtime"" but I honestly haven't tried that (never needed to).",far stable model separate link bit confused trying running locally link run option run locally via connect local honestly tried never,issue,negative,positive,neutral,neutral,positive,positive
820742215,"@Archviz360 That's simply way beyond the scope of what we're willing to provide for free in this open source project. There is the MyHeritage version which is paid and much more advanced, and more generally speaking any of our efforts going forward in bringing this stuff in a user friendly way is going to be in that direction.  

For convenience, I'll repost what we have in the readme at the bottom concerning our stance:

A Statement on Open Source Support
We believe that open source has done a lot of good for the world.  After all, DeOldify simply wouldn't exist without it. But we also believe that there needs to be boundaries on just how much is reasonable to be expected from an open source project maintained by just two developers.

Our stance is that we're providing the code and documentation on research that we believe is beneficial to the world.  What we have provided are novel takes on colorization, GANs, and video that are hopefully somewhat friendly for developers and researchers to learn from and adopt. This is the culmination of well over a year of continuous work, free for you. What wasn't free was shouldered by us, the developers.  We left our jobs, bought expensive GPUs, and had huge electric bills as a result of dedicating ourselves to this.

What we haven't provided here is a ready to use free ""product"" or ""app"", and we don't ever intend on providing that.  It's going to remain a Linux based project without Windows support, coded in Python, and requiring people to have some extra technical background to be comfortable using it.  Others have stepped in with their own apps made with DeOldify, some paid and some free, which is what we want! We're instead focusing on what we believe we can do best- making better commercial models that people will pay for.   Does that mean you're not getting the very best for free?  Of course. We simply don't believe that we're obligated to provide that, nor is it feasible! We compete on research and sell that.  Not a GUI or web service that wraps said research- that part isn't something we're going to be great at anyways. We're not about to shoot ourselves in the foot by giving away our actual competitive advantage for free, quite frankly.

We're also not willing to go down the rabbit hole of providing endless, open ended and personalized support on this open source project.  Our position is this:  If you have the proper background and resources, the project provides more than enough to get you started. We know this because we've seen plenty of people using it and making money off of their own projects with it.  

Thus, if you have an issue come up and it happens to be an actual bug that having it be fixed will benefit users generally, then great- that's something we'll be happy to look into. 

In contrast, if you're asking about something that really amounts to asking for personalized and time consuming support that won't benefit anybody else, we're not going to help. It's simply not in our interest to do that. We have bills to pay, after all. And if you're asking for help on something that can already be derived from the documentation or code?  That's simply annoying, and we're not going to pretend to be ok with that.",simply way beyond scope willing provide free open source project version much advanced generally speaking going forward stuff user friendly way going direction convenience repost bottom concerning stance statement open source support believe open source done lot good world simply would exist without also believe need much reasonable open source project two stance providing code documentation research believe beneficial world provided novel colorization video hopefully somewhat friendly learn adopt culmination well year continuous work free free shouldered u left bought expensive huge electric result provided ready use free product ever intend providing going remain based project without support python people extra technical background comfortable stepped made free want instead believe making better commercial people pay mean getting best free course simply believe provide feasible compete research sell web service said part something going great anyways shoot foot giving away actual competitive advantage free quite frankly also willing go rabbit hole providing endless open ended support open source project position proper background project enough get know seen plenty people making money thus issue come actual bug fixed benefit generally something happy look contrast something really time consuming support wo benefit anybody else going help simply interest pay help something already derived documentation code simply annoying going pretend,issue,positive,positive,positive,positive,positive,positive
820739893,"So what you're really running into here is the fundamental limitation of the approach of  the open source DeOldify's video implementation- it's based purely on colorizing frames one at a time without considering consistency between frames.  This was a sort of ""20/80"" solution in my mind- 20% effort to get 80% results, as this can produce good results.  It's just not the best nor ideal solution.  It's an engineering trade-off.

The -only- way I can tell you to deal with this is:
-  Generally, you simply can't expect good results with rough old videos. The higher the quality, the better.
-  Best results come out of experimentation with render_factor. Generally, the lower the quality the video, the lower the render_factor you should try using.",really running fundamental limitation approach open source video based purely one time without considering consistency sort solution effort get produce good best ideal solution engineering way tell deal generally simply ca expect good rough old higher quality better best come experimentation generally lower quality video lower try,issue,positive,positive,positive,positive,positive,positive
820659076,"bs = ""batch size"" (the number of images trained during a single iteration of training) and sz  is the length of each side of the image that you're resizing them to for training.  sz=64, for example would mean you're resizing to 64x64 pixel squares for the images. 

Generally I try to maximize batch size to fit the gpu's memory constraints.  This version of DeOldify was developed using a single 11GB video card. So if you have something smaller than that you're going to have to adjust it accordingly.",batch size number trained single iteration training length side image training example would mean generally try maximize batch size fit memory version single video card something smaller going adjust accordingly,issue,negative,negative,neutral,neutral,negative,negative
820656813,"@kroslaniec just do what it takes to match the folder structure I described above, link here:  https://github.com/jantic/DeOldify/issues/175#issuecomment-559881554

As to what should go in train/val - if you're really needing to make that manually then just pull a small percentage of random images from train and put them in val. 1% perhaps?

I won't be able to go beyond this though and start helping to debug here though- that will take too much time and I'm a bit busy.",match folder structure link go really needing make manually pull small percentage random train put perhaps wo able go beyond though start helping take much time bit busy,issue,negative,positive,neutral,neutral,positive,positive
818139220,"@Archviz360 as already mentioned, there's a [website](https://www.myheritage.com/incolor) and an [Android/iPhone app](https://www.colorizeimages.com/) ready for use, you can use the website in Windows or install the app on your phone.",already ready use use install phone,issue,negative,positive,positive,positive,positive,positive
817997829,a desktop version would be perfect since I have difficulty getting it to work. i started a few weeks ago to try getting deoldiy working on windows using jupyter notebook. I managed to get it connected to the local machine once but never managed to install the required files. since my pc doesn't have a petabyte or terabyte harddrive I found it frustrating having to install large software and learning a coding language i can't remember even a single hour. for me a desktop app would be appreciated and i would even pay for it. so please bring a window ui verson. ,version would perfect since difficulty getting work ago try getting working notebook get connected local machine never install since found install large learning language ca remember even single hour would would even pay please bring window,issue,negative,positive,positive,positive,positive,positive
817894394,it would be a lot easier to have a graphic UI version. since I would like to run this on an offline server it would be a lot easier for me. i could actually pay if there was a UI version of this cool ai. for now I'm using the online version but hopes to be able to use it offline as software. ,would lot easier graphic version since would like run server would lot easier could actually pay version cool ai version able use,issue,positive,positive,positive,positive,positive,positive
817312888,"> Oh, my bad, I forgot to set the bs to smaller. Problem solved.

I know it's an old thing, but I am searching for the same thing as you - could you try to explain me what bs and sz means and what value have you set?",oh bad forgot set smaller problem know old thing searching thing could try explain value set,issue,negative,negative,negative,negative,negative,negative
817305446,"@jantic 

Could you help me with one thing? I have downloaded files from imagenet. It have different data structure. Can you explaing me which files should be in train and val folders? 

Also i have a problem with line, can you show me what I am doing wrong?
`
learn_gen.fit_one_cycle(1, pct_start=0.8, max_lr=slice(1e-3))`

[https://i.imgur.com/VJ9LLMa.png](url)

",could help one thing different data structure train also problem line show wrong,issue,negative,negative,negative,negative,negative,negative
817269849,"It looks like the `jupyter lab` is downloading wrong/old files, downloaded file has ~83MB, but the file downloaded in Collab from `https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth` is ~243MB.

Downloading and placing the `ColorizeArtistic_gen.pth` in models folder fixed the errors.

Everything is working now. 

EDIT: Just realised this thread is for collab `artictic=False`. I got the same error as OP in jupyter lab. Maybe Collab version is downloading wrong files for `Stable` model?",like lab file file folder fixed everything working edit thread got error lab maybe version wrong stable model,issue,negative,negative,negative,negative,negative,negative
817263319,"I was able to partialy resolve this by placing the models in models directory under the root DeOldify folder.[
![Screenshot_2021-04-11_09-23-48](https://user-images.githubusercontent.com/2306998/114295822-e3fa9c00-9aa7-11eb-8dc2-7e6c629255e7.png)
](url)

But now I am facing another issue when running the `colorizer = get_image_colorizer(artistic=True)` cell:
```
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
<ipython-input-9-d41e8163fe4e> in <module>
----> 1 colorizer = get_image_colorizer(artistic=True)

/mnt/_500GB/Work/Personal/DeOldify/deoldify/visualize.py in get_image_colorizer(root_folder, render_factor, artistic)
    395 ) -> ModelImageVisualizer:
    396     if artistic:
--> 397         return get_artistic_image_colorizer(root_folder=root_folder, render_factor=render_factor)
    398     else:
    399         return get_stable_image_colorizer(root_folder=root_folder, render_factor=render_factor)

/mnt/_500GB/Work/Personal/DeOldify/deoldify/visualize.py in get_artistic_image_colorizer(root_folder, weights_name, results_dir, render_factor)
    418     render_factor: int = 35
    419 ) -> ModelImageVisualizer:
--> 420     learn = gen_inference_deep(root_folder=root_folder, weights_name=weights_name)
    421     filtr = MasterFilter([ColorizerFilter(learn=learn)], render_factor=render_factor)
    422     vis = ModelImageVisualizer(filtr, results_dir=results_dir)

/mnt/_500GB/Work/Personal/DeOldify/deoldify/generators.py in gen_inference_deep(root_folder, weights_name, arch, nf_factor)
     86     )
     87     learn.path = root_folder
---> 88     learn.load(weights_name)
     89     learn.model.eval()
     90     return learn

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in load(self, file, device, strict, with_opt, purge, remove_module)
    275             if with_opt: warn(""Saved filed doesn't contain an optimizer state."")
    276             if remove_module: state = remove_module_load(state)
--> 277             get_model(self.model).load_state_dict(state, strict=strict)
    278         del state
    279         gc.collect()

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load_state_dict(self, state_dict, strict)
    752                     load(child, prefix + name + '.')
    753 
--> 754         load(self)
    755 
    756         if strict:

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load(module, prefix)
    750             for name, child in module._modules.items():
    751                 if child is not None:
--> 752                     load(child, prefix + name + '.')
    753 
    754         load(self)

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load(module, prefix)
    750             for name, child in module._modules.items():
    751                 if child is not None:
--> 752                     load(child, prefix + name + '.')
    753 
    754         load(self)

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load(module, prefix)
    750             for name, child in module._modules.items():
    751                 if child is not None:
--> 752                     load(child, prefix + name + '.')
    753 
    754         load(self)

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load(module, prefix)
    750             for name, child in module._modules.items():
    751                 if child is not None:
--> 752                     load(child, prefix + name + '.')
    753 
    754         load(self)

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in load(module, prefix)
    747             local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})
    748             module._load_from_state_dict(
--> 749                 state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
    750             for name, child in module._modules.items():
    751                 if child is not None:

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/modules/module.py in _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
    679         """"""
    680         for hook in self._load_state_dict_pre_hooks.values():
--> 681             hook(state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
    682 
    683         local_name_params = itertools.chain(self._parameters.items(), self._buffers.items())

~/miniconda3/envs/deoldify/lib/python3.7/site-packages/torch/nn/utils/spectral_norm.py in __call__(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)
    164         if version is None or version < 1:
    165             with torch.no_grad():
--> 166                 weight_orig = state_dict[prefix + fn.name + '_orig']
    167                 weight = state_dict.pop(prefix + fn.name)
    168                 sigma = (weight_orig / weight).mean()

KeyError: 'layers.3.0.0.weight_orig'
```

Have you encountered this error too?",able resolve directory root folder facing another issue running cell recent call last module artistic artistic return else return learn vi arch return learn load self file device strict purge warn saved contain state state state state state self strict load child prefix name load self strict load module prefix name child child none load child prefix name load self load module prefix name child child none load child prefix name load self load module prefix name child child none load child prefix name load self load module prefix name child child none load child prefix name load self load module prefix none else prefix prefix strict name child child none self prefix strict hook hook prefix strict self prefix strict version none version prefix weight prefix sigma weight error,issue,negative,positive,positive,positive,positive,positive
817192259,"1. Create a folder named ""models"" inside ""DeOldify"" folder.
2. Download these [three](https://github.com/jantic/DeOldify#completed-generator-weights) weights from  **Completed Generator Weights** and paste inside ""models"" folder.
It solved my issue.",create folder inside folder three generator paste inside folder issue,issue,negative,neutral,neutral,neutral,neutral,neutral
816866904,"Thanks for the reply @jantic. I tried the exact command you gave me for a local file (with my IP address of course) and although the curl command worked I ended up with a 400 error on the server side. Removing the two -H arguments got it working though - any guidance on what could be doing that please?

",thanks reply tried exact command gave local file address course although curl command worked ended error server side removing two got working though guidance could please,issue,negative,positive,positive,positive,positive,positive
816333334,"@jantic Thanks for your reply, I have solved it.
run: curl -X POST ""http://127.0.0.1:6000/process"" -F ""file=@slave-family-P.jpeg"" -F ""render_factor=35"" --output colorized_image.png
and it work.",thanks reply run curl post output work,issue,negative,positive,positive,positive,positive,positive
816279754,"@gitLRD It'd definitely help to see a stack trace, and there should be one. But let's just start with basics here: The call should look basically like this in the case of a remote url call:

`curl -X POST ""http://MY_SUPER_API_IP:5000/process"" -H ""accept: image/png"" -H ""Content-Type: application/json"" -d ""{\""url\"":\""http://www.afrikanheritage.com/wp-content/uploads/2015/08/slave-family-P.jpeg\"", \""render_factor\"":35}"" --output colorized_image.png
`

And if it's a local file:

`curl -X POST ""http://MY_SUPER_API_IP:5000/process"" -H ""accept: image/png"" -H ""Content-Type: image/jpeg"" -F ""file=@slave-family-P.jpeg"" -F ""render_factor=35"" --output colorized_image.png`

The line you're showing that is failing would be at the point where it's trying to read the url string , and it's expecting it to be formatted as json (hence the first example).  So I'd make sure you pay extra attention to the formatting here to start- it may simply be a matter of missing a curly brace, etc.

@yfq512 I'll need more feedback in your case to help you.",definitely help see stack trace one let start call look basically like case remote call curl post accept output local file curl post accept output line showing failing would point trying read string hence first example make sure pay extra attention may simply matter missing curly brace need feedback case help,issue,positive,positive,neutral,neutral,positive,positive
815728575,"Hey @jantic, I've got the same problem here. My cURL results are the same, but when I open the resulting image my viewer (Preview on my Mac) complains it can't open it.

This is the output of the Docker container on the server's console - let me know if there are other logs that would be useful:

```
<myIPaddress> - - [07/Apr/2021 19:58:15] ""POST /process HTTP/1.1"" 200 -
Traceback (most recent call last):
  File ""app.py"", line 61, in process_image
    url = request.json[""url""]
```",hey got problem curl open resulting image viewer preview mac ca open output docker container server console let know would useful post recent call last file line,issue,negative,positive,neutral,neutral,positive,positive
815312885,"@NathanRichHGDW I'm not sure how you got a hold of that specific url but that's not the official Colab that we provide.  That one is at [this link](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb) . Our officially supported Colabs have the links updated properly already.

What you have there is some else's fork of older code, coming from [here](https://github.com/ojjsaw/video-processing) .

",sure got hold specific official provide one link officially link properly already else fork older code coming,issue,negative,positive,positive,positive,positive,positive
814686310,"You may want to update this then:
https://colab.research.google.com/github/ojjsaw/video-processing/blob/master/Custom_DeOldify_VideoColorizer_Colab.ipynb
It's still trying to use this URL.",may want update still trying use,issue,negative,neutral,neutral,neutral,neutral,neutral
814372847,Thanks for the report. I won't be prioritizing digging into this issue anytime soon- too much on the plate elsewhere currently.  ,thanks report wo digging issue much plate elsewhere currently,issue,negative,positive,positive,positive,positive,positive
803012005,"Can you elaborate on what you mean by ""failed""? Is it that the image doesn't open in a image viewer locally? If so can you attach the failed image to this thread? Also can you post the original image you're trying to colorize?",elaborate mean image open image viewer locally attach image thread also post original image trying colorize,issue,negative,positive,positive,positive,positive,positive
802025752,"There's a requirements.txt already, perhaps the dockerfile could use it.",already perhaps could use,issue,negative,neutral,neutral,neutral,neutral,neutral
802022979,"@jantic can you ban this user from opening PR in this project? He opened a few already and will probably continue to do so, just polluting your project with spammy content.",ban user opening project already probably continue polluting project content,issue,negative,neutral,neutral,neutral,neutral,neutral
801950961,"Do not use someone else's repository to learn git.

Non utilizzare il repository di qualcun altro per imparare git. ",use someone else repository learn git non repository di per git,issue,negative,neutral,neutral,neutral,neutral,neutral
801510258,I've never run into this issue before.  But perhaps [this Stack Overflow thread](https://stackoverflow.com/questions/45993879/matplot-lib-fatal-io-error-25-inappropriate-ioctl-for-device-on-x-server-loc) will help?,never run issue perhaps stack overflow thread help,issue,negative,neutral,neutral,neutral,neutral,neutral
797145654,"Well sign me up for whatever that software ends up becoming!

On Thu, Mar 11, 2021 at 2:18 PM Jason Antic ***@***.***>
wrote:

> @theman8631 <https://github.com/theman8631> I definitely agree with the
> general idea that the way forward for much better results is to take the
> strengths of fully automated colorization and marry that with human
> creativity and intelligence. I think there's a bit of a diminishing returns
> on fully automated at this point as to how far you can go with that due to
> the nature of the problem and the limits of current tech.
>
> That being said I only differ on what I think would be the best way to do
> this- I think just having the users directly suggest the colors with a dot
> or scribble on the object would be more intuitive and productive. This is
> something I plan on exploring pretty soon, but it won't be going into this
> current project. Rather it'll be in the commercial work for my startup
> DeOldify.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/239#issuecomment-797087451>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AFQO4FGQ73EBH5QOXK5KBRLTDEXSJANCNFSM4NGUAEDQ>
> .
>
",well sign whatever becoming mar antic wrote definitely agree general idea way forward much better take fully colorization marry human creativity intelligence think bit fully point far go due nature problem current tech said differ think would best way think directly suggest color dot scribble object would intuitive productive something plan exploring pretty soon wo going current project rather commercial work reply directly view,issue,positive,positive,positive,positive,positive,positive
797087451,"@theman8631 I definitely agree with the general idea that the way forward for much better results is to take the strengths of fully automated colorization and marry that with human creativity and intelligence. I think there's a bit of a diminishing returns on fully automated at this point as to how far you can go with that due to the nature of the problem and the limits of current tech. 

That being said I only differ on what I think would be the best way to do this- I think just having the users directly suggest the colors with a dot or scribble on the object would be more intuitive and productive. This is something I plan on exploring pretty soon, but it won't be going into this current project. Rather it'll be in the commercial work for my startup DeOldify.",definitely agree general idea way forward much better take fully colorization marry human creativity intelligence think bit fully point far go due nature problem current tech said differ think would best way think directly suggest color dot scribble object would intuitive productive something plan exploring pretty soon wo going current project rather commercial work,issue,positive,positive,positive,positive,positive,positive
796001118,"With my limited understanding of what's going on besides the explanations of cheating the test I have an assumption and possible suggestion to possibly have some hands on to direct the cheating.

Assumption:  It seems like purple/blue is a frequent guess to cheat the test, but there seems to be multiple guess hues going on, one example is the green floor.  I assume theres a level of certainty going on here, grass and trees are often green, good guess, high certainty.  Color of dress?  Who knows.

Suggestion: is there a way similar to the render factor that we can tweak the hue on the low certainly color choices?  For example if in seeing the playback it looks like purple guesses should be more blue, you could dial the blue in the low certainty purple choices.

I know easier said then done but thought I'd share the idea.  It moves farther away from the ai model but at some point, unless there's color sources of near same images arn't we going to need some user input?",limited understanding going besides cheating test assumption possible suggestion possibly direct cheating assumption like frequent guess cheat test multiple guess going one example green floor assume there level certainty going grass often green good guess high certainty color dress suggestion way similar render factor tweak hue low certainly color example seeing playback like purple blue could dial blue low certainty purple know easier said done thought share idea farther away ai model point unless color near ar going need user input,issue,negative,positive,neutral,neutral,positive,positive
793282537,"@GlebBrykin  Your best bet to port to C# would probably be to export to ONNX, and then use something like Microsoft's  [onnxruntime](https://github.com/microsoft/onnxruntime) to execute it in C#.  To start, something like this to export it:
```python
    import os
    from deoldify.generators import gen_inference_deep
    import torch.nn as nn
    from pathlib import Path
    from fastai.vision.data import normalize_funcs, imagenet_stats
    
    norm, denorm = normalize_funcs(*imagenet_stats)
    sz =  (560,560) 
    
    class ImageScaleInput(nn.Module):
        def __init__(self): 
            super().__init__()
            self.norm = norm
    
        def forward(self, x): 
            out = (x.div(255.0)).type(torch.float32)
            out,_ = self.norm((out,out), do_x=True)
            #out = out.unsqueeze(0)
            return out
    
    class ImageScaleOutput(nn.Module):
        def __init__(self): 
            super().__init__()
            self.denorm = denorm
    
        def forward(self, x):
            out = self.denorm(x, do_x=True)
            out = out.float().clamp(min=0,max=1) 
            out = self.denorm(out, do_x=False)
            out = (out.mul(255.0)).type(torch.float32)
            return out
    onnx_path = './models/ColorizeArtistic_gen.onnx'
    raw_model = gen_inference_deep(root_folder=Path('./'), weights_name='ColorizeArtistic_gen').model
    
    dummy_input = torch.randn(1, 3, sz[0], sz[1])
    final_pytorch_model = nn.Sequential(ImageScaleInput(), raw_model , ImageScaleOutput())
    torch.onnx.export(final_pytorch_model, dummy_input, onnx_path, input_names=['input'], output_names=['output'], opset_version=9)
```
I didn't test this particular snippet but this should be a good start- I've done this before.  Keep in mind: There's still work to be done on the C# side in terms of the image processing that's done in the Python code.  I won't be providing that but there's quite a bit involved in that including mapping just the hue/saturation of the model output image to the luminance of the original image.  This allows for processing of high definition images without losing image resolution.  It won't be totally easy in summary, but it can be done.
",best bet port would probably export use something like execute start something like export python import o import import import path import norm class self super norm forward self return class self super forward self return test particular snippet good done keep mind still work done side image done python code wo providing quite bit involved model output image luminance original image high definition without losing image resolution wo totally easy summary done,issue,positive,positive,positive,positive,positive,positive
783747646,Can you clarify? The existing scripts could be modified pretty easily to accommodate this but I'm not sure what you're looking for.  A flag in the command line to specify artistic vs stable? ,clarify could pretty easily accommodate sure looking flag command line specify artistic stable,issue,positive,positive,positive,positive,positive,positive
783745551,"Hey guys....readme says this:

> Windows is not supported and any issues brought up related to this will not be investigated.

I'm sticking to that policy. Fastai 1.x doesn't support Windows officially so I'm certainly not going to to try to work around that.

Closing this issue.",hey brought related sticking policy support officially certainly going try work around issue,issue,positive,positive,positive,positive,positive,positive
782595697,"> Deoldify not working on windows system

You should use Ubuntu or alternative Linux dist in WSL2. 
Try to execute `!mkdir 'models'` before `colorizer = get_image_colorizer(...)`",working system use alternative try execute,issue,negative,neutral,neutral,neutral,neutral,neutral
771261403,"@Nickzhangkaiyu So yes, all of imagenet is downloaded and used to train DeOldify.  I should point out though that in the training notebooks that there's a keep_pct variable which ranges from 0.0 to 1.0 (100%) that controls the percentage of the dataset used for each step.  See [here ](https://github.com/jantic/DeOldify/blob/master/ColorizeTrainingArtistic.ipynb)for example.

I will say this:  I think there's a very high likelihood of a mismatch in implementations when attempting to do this on Tensorflow. You'd have to not only reimplement what's in the DeOldify code itself but also the FastAI code that it depends on.  It's a lot of moving parts. I would strongly recommend not trying to go down that path if you can avoid it. ",yes used train point though training variable percentage used step see example say think high likelihood mismatch code also code lot moving would strongly recommend trying go path avoid,issue,positive,positive,positive,positive,positive,positive
769624758,I’ll add related doc today in readme,add related doc today,issue,negative,neutral,neutral,neutral,neutral,neutral
769624273,Awesome ! Thanks for helping us improving the experience with your feedback !,awesome thanks helping u improving experience feedback,issue,positive,positive,positive,positive,positive,positive
769623134,@mewbot97 did you had time to check of behaviour presented fits your need ?,time check behaviour need,issue,negative,neutral,neutral,neutral,neutral,neutral
769622689,"No space left on device is because you don’t have enough space to pull docker image 

docker system is similar to git.

it’s pulling image layers (layer are a bit lit commits) 

so if you don’t have enough space on your laptop/desktop it will not be able to download all docker layer and therefore not able to run it.

yiu have also to keep in mind that docker as a limited space on disk that you can adjust and might be also the source of the issue


I suggested looking into this resolution 
https://stackoverflow.com/questions/30604846/docker-error-no-space-left-on-device",space left device enough space pull docker image docker system similar git image layer bit lit enough space able docker layer therefore able run also keep mind docker limited space disk adjust might also source issue looking resolution,issue,negative,positive,positive,positive,positive,positive
769621130,"@CodeClouds-Guru can you try following the new QuickStart doc in read me and let me know if the problem persist. Sorry for the doc glitch.

also results are not being saved permanently on the API docker it’s a temporary file being saved then served through the API and then removed form the API server in order to avoid disk usage and preserve data privacy.",try following new doc read let know problem persist sorry doc also saved permanently docker temporary file saved removed form server order avoid disk usage preserve data privacy,issue,negative,negative,negative,negative,negative,negative
769619403,Hi @covercash2 would it be possible to have the URL used to reproduce the error ?,hi would possible used reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
767157190,"I took a quick look. Good news is that it's not all videos on YouTube that are affected.  Bad news is that it's not the usual suspects that I'd look for in diagnosing issues, and I don't have the time currently to dig deep- lots of other work going on here currently at DeOldify in our business efforts. It does appear though that this is an ffmpeg issue that needs to be addressed on their end. I'll keep this issue open for now in the meantime.",took quick look good news affected bad news usual look time currently dig lot work going currently business appear though issue need end keep issue open,issue,negative,positive,neutral,neutral,positive,positive
767137449,"@TheVincinator Sure, just update the code under the header ""Colorize!!"" to look like this.  Only thing I've changed here is adding the ""watermarked=False"" parts.


```
#NOTE:  Max is 44 with 11GB video cards.  21 is a good default
render_factor=21
#NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification
source_url='https://twitter.com/silentmoviegifs/status/1116751583386034176'
file_name = 'DogShy1926'
file_name_ext = file_name + '.mp4'
result_path = None

if source_url is not None:
    result_path = colorizer.colorize_from_url(source_url, file_name_ext, render_factor=render_factor, watermarked=False)
else:
    result_path = colorizer.colorize_from_file_name(file_name_ext, render_factor=render_factor, watermarked=False)

show_video_in_notebook(result_path)
```",sure update code header colorize look like thing note video good default note make none read file directly without modification none none else,issue,positive,positive,positive,positive,positive,positive
767134133,"@TaktakTaktouk   I'll start with the easiest option that's already built in:  Lower render_factor values tend to not only make the colors more saturated but also more colorful. There's a cost however and that is you tend to get less detail accuracy.

Otherwise, If you literally mean increasing the saturation value of the frames or photos, that would involve code that I don't have built into the open source version currently.  If you're comfortable with Python, you can increase saturation by taking the pil images that result and running this function on them.  A saturation value of 1.1 to 1.3 would be a good start. Note that Image in this case is PIL.Image .  

def saturate(filtered_image:Image, saturation:float)->Image:
    converter = PIL.ImageEnhance.Color(filtered_image)
    filtered_image = converter.enhance(saturation)
    return filtered_image",start easiest option already built lower tend make color saturated also colorful cost however tend get le detail accuracy otherwise literally mean increasing saturation value would involve code built open source version currently comfortable python increase saturation taking result running function saturation value would good start note image case saturate image saturation float image converter saturation return,issue,positive,positive,positive,positive,positive,positive
765859504,I was also wondering if it was my GPU holding me back. I am currently using the Radeon 5700 XT. Does it require an NVIDIA GPU? I found this article on it: https://www.fast.ai/2017/11/16/what-you-need/,also wondering holding back currently require found article,issue,negative,neutral,neutral,neutral,neutral,neutral
762493543,"@xuancao19 I have no control over that. You'll need to click that unsubscribe link that should be at the bottom right of your email.  
![image](https://user-images.githubusercontent.com/179759/104967289-df868180-5997-11eb-8fdc-c2f24051c3f0.png)
",control need click link bottom right image,issue,negative,positive,positive,positive,positive,positive
762492849,"Easy:  Just grab a lot of images- a million+ from [open images](https://storage.googleapis.com/openimages/web/index.html) or imagenet, and use PIL to change the images to black and white to get your before images:

Assuming img is a PIL image:

img = img.convert('L').convert('RGB')

That's it!",easy grab lot open use change black white get assuming image,issue,negative,positive,neutral,neutral,positive,positive
762491254,"@TheVincinator You have a few things possibly going on there:

1. The directory you install deoldify to needs recursive read/write permissions open for the program.  And you need the models directory created and populated with models. See [here](https://github.com/jantic/DeOldify/issues/250 ).
2. The errors you are getting running it are suggesting that you either don't have everything installed or forgot to activate the conda environment. 

Bottom line:  The readme has instructions covering all this.  The conda install route is the way to go to keep your sanity.",possibly going directory install need recursive open program need directory see getting running suggesting either everything forgot activate environment bottom line covering install route way go keep sanity,issue,negative,neutral,neutral,neutral,neutral,neutral
762489433,"@CodeClouds-Guru What I'm saying is this is really @jqueguiner 's baby and I don't want to change anything without his input first, as it's a pretty big behavior change. What I'd suggest to you though is 

- debug assuming there's something missed in the deployment on your end.  This is a pretty safe assumption because there haven't been reports of this behavior previous and things haven't changed much with the api. 
- if you're up to it/can do it that you modify the code according to what I suggested. ",saying really baby want change anything without input first pretty big behavior change suggest though assuming something deployment end pretty safe assumption behavior previous much modify code according,issue,negative,positive,positive,positive,positive,positive
762476461,"
> That being said- I personally don't see why you'd want to save the images in the first place with an api like this. There is an alternative method: get_transformed_image that doesn't plot the image nor save it as side effects- it just returns the processed image. That seems to me to be the real solution here.

This is the default behavior when running the application using ```python3 app.py``` without doing any extra modifications, is there a starting parameter that needs set when doing this? None of the recommended ways of running the API are working, only by manually running ```python3 app.py``` can I get the API to partly work

",personally see want save first place like alternative method plot image save side image real solution default behavior running application python without extra starting parameter need set none way running working manually running python get partly work,issue,positive,positive,neutral,neutral,positive,positive
761721818,"![Jupyter Lab DeOldify Issue part  2](https://user-images.githubusercontent.com/68995370/104829839-e9986b00-5845-11eb-8183-d25ca8c9e860.png)

I was also having trouble with the other video scripts.",lab issue part also trouble video,issue,negative,negative,negative,negative,negative,negative
759913421,"I want to unsubscribe from this email, thank you------------------&nbsp;原始邮件&nbsp;------------------
发件人:&nbsp;&quot;Jason&amp;nbsp;Antic&quot;<notifications@github.com&gt;
发送时间:&nbsp;2021年1月14日(星期四) 凌晨4:35
收件人:&nbsp;&quot;jantic/DeOldify&quot;<DeOldify@noreply.github.com&gt;;
抄送:&nbsp;&quot;Subscribed&quot;<subscribed@noreply.github.com&gt;;
主题:&nbsp;Re: [jantic/DeOldify] [DOC] Fix old doc + Add doc on how to push local img/video to API (#306)",want thank quot antic quot quot quot quot quot doc fix old doc add doc push local,issue,negative,positive,neutral,neutral,positive,positive
759123187,"So I'll have to add this disclaimer: I personally don't use the docker image so I tend to defer to the creator @jqueguiner  on design questions, because they're the one who will be affected most.

That being said- I personally don't see why you'd want to save the images in the first place with an api like this. There is an alternative method:  get_transformed_image that doesn't plot the image nor save it as side effects- it just returns the processed image.  That seems to me to be the real solution here.

@jqueguiner , what do you think?",add disclaimer personally use docker image tend defer creator design one affected personally see want save first place like alternative method plot image save side image real solution think,issue,positive,positive,positive,positive,positive,positive
759110734,Ooof.  I can't believe I made that mistake.  Thank you!,ca believe made mistake thank,issue,negative,neutral,neutral,neutral,neutral,neutral
753219823,"I'm guessing its a problem with youtube videos now, uploaded videos to dropbox and it worked fine. Sorry guess I didn't test everything... ",guessing problem worked fine sorry guess test everything,issue,negative,negative,neutral,neutral,negative,negative
753020463,"It looks like FileNotFoundError: [Errno 2] No such file or directory: '/home/testing/DeOldify/data/result_images/a4ccf03a-c25b-4e17-aae6-88300a42a7a3.jpeg'

Is because its looking for the images under /data/result_images however its creating the folders in the main directory so the real path should be '/home/testing/DeOldify/result_images/a4ccf03a-c25b-4e17-aae6-88300a42a7a3.jpeg'
",like file directory looking however main directory real path,issue,negative,positive,positive,positive,positive,positive
752990669,I can see images are being created in the result_images folder however the application is still returning the error message and no response from the api call. I can only get the API to run at all by manually typing ```python3 app.py``` but this does not appear to be the correct way to run the program according to the documentation. No method of running the program seems to work properly.,see folder however application still error message response call get run manually python appear correct way run program according documentation method running program work properly,issue,negative,neutral,neutral,neutral,neutral,neutral
752975819,"Hey guys. Great tool! I'm using this to colorize some old photos of my grandfather right now.

I made this patch to alleviate some difficulty in controlling Deoldify from a custom script that may reside in a different directory than Deoldify. Currently it seems Deoldify isn't flexible with respect to letting you set the output directory of modified images.

I simply added some arguments that allow the user to customize the ""root directory"" and ""results directory"". If those arguments aren't provided, they will default to what you have set now.",hey great tool colorize old grandfather right made patch alleviate difficulty custom script may reside different directory currently flexible respect set output directory simply added allow user root directory directory provided default set,issue,positive,positive,positive,positive,positive,positive
750536073,"I made the changes to `quick_start.sh` but it made no difference to the error. In fact, I don't think the behaviour changed at all, as I never installed any nvidia libs or tools on the system - I'm pretty sure `IPC_HOST=""""` was the code path on the original attempt as the `nvidia-smi` binary was never present on my system.

I think the root cause of `No space left on device` is unrelated.

However it's good to know that it should run on the CPU.

I'd be happy to provide any troubleshooting info.",made made difference error fact think behaviour never system pretty sure code path original attempt binary never present system think root cause space left device unrelated however good know run happy provide,issue,negative,positive,positive,positive,positive,positive
748725125,"Yes, if you want to run this against a GPU you'll need an Nvidia GPU.  It'll still run on CPU otherwise. So it looks like you'll either want to uninstall anything nvidia that was installed. Or just change this  in the quick_start:

```
if nvidia-smi; then

	IPC_HOST=""--ipc=host""

	echo ""nvidia card found running on GPU""

else

	IPC_HOST=""""

	echo ""No nvidia card found running on CPU""

fi
```


To this:

```
IPC_HOST=""""
```",yes want run need still run otherwise like either want anything change echo card found running else echo card found running fi,issue,positive,neutral,neutral,neutral,neutral,neutral
743385130,"If you're just interested in getting results faster, DataParallel would work if you can get it to run with a GAN.  I tried briefly and realized it wasn't trivial.  And since then I haven't touched the subject because I haven't been using GANs for some time. 

But really I'd recommend trying to get DistributedParallel to work, along with incorporating SyncBatchNorm, so that you get the benefit of better batch norm statistics with bigger batch sizes. But there's definitely issues with the version of fastai used here when attempting to do DistributedDataParallel on GANs.  There was a patch for one of them [earlier this year](https://github.com/fastai/fastai/issues/2496).  That's not included in the version currently used by DeOldify. 

Also note that the newer version of fastai (quite different) appears to have better support for this.  That's another route- upgrade the code in a fork and try [this](https://docs.fast.ai/distributed.html).  

But I won't be adding support for this myself to the code base.  It's too much work that goes way beyond the scope of what I'm willing to do for free.

",interested getting faster would work get run gan tried briefly trivial since touched subject time really recommend trying get work along get benefit better batch norm statistic bigger batch size definitely version used patch one year included version currently used also note version quite different better support another upgrade code fork try wo support code base much work go way beyond scope willing free,issue,positive,positive,neutral,neutral,positive,positive
742205041,"Maybe you need DataParallel (if you use pytorch), just see https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html",maybe need use see,issue,negative,neutral,neutral,neutral,neutral,neutral
736251481,"Yep that's intentional.  I got booted off of hosting this and the video model on Dropbox because I was getting too many downloads. Thankfully DeepAI is now providing hosting for these links.

So update your links accordingly (documented in the GitHub repo) or pull the latest source code.  The last remaining piece that I just merged in was updates to the links for the Docker code.

One thing I ask particularly of those making services or scripts is please don't keep downloading the same file if you don't need to.  Cache it!",yep intentional got booted hosting video model getting many thankfully providing hosting link update link accordingly pull latest source code last piece link docker code one thing ask particularly making please keep file need cache,issue,positive,positive,positive,positive,positive,positive
736246174,"@mdtanrikulu  Look above in the comment thread.  Replace the links in the code with those (comment starts with"" DeepAI kindly provided these links"").

I'm assuming you're having this trouble with the docker image.  I'm about to merge a pull request that resolves these issues.  ",look comment thread replace link code comment kindly provided link assuming trouble docker image merge pull request,issue,negative,positive,positive,positive,positive,positive
735990177,"Hello there, I have this issue again.

```
--2020-11-30 19:20:40--  https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0
Resolving www.dropbox.com (www.dropbox.com)... 162.125.66.1, 2620:100:6022:1::a27d:4201
Connecting to www.dropbox.com (www.dropbox.com)|162.125.66.1|:443... connected.
HTTP request sent, awaiting response... 301 Moved Permanently
Location: /s/raw/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth [following]
--2020-11-30 19:20:40--  https://www.dropbox.com/s/raw/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth
Reusing existing connection to www.dropbox.com:443.
HTTP request sent, awaiting response... 404 Not Found
2020-11-30 19:20:41 ERROR 404: Not Found.
```",hello issue ad connected request sent response permanently location following connection request sent response found error found,issue,negative,neutral,neutral,neutral,neutral,neutral
735039601,"@isty1974  Any links have to be direct to the files when it comes to images. It looks like Google Drive simply doesn't provide those.  We won't be writing special code to accommodate this special use case. It's simply not worth the effort/maintenance relative to the more straight forward options (imgur, etc).",link direct come like drive simply provide wo writing special code accommodate special use case simply worth relative straight forward,issue,positive,positive,positive,positive,positive,positive
731841301,"Hey @jqueguiner can't we add these steps in Dockerfile-api as well?
`RUN [[ ! -f /data/models/ColorizeArtistic_gen.pth ]] && wget -O /data/models/ColorizeArtistic_gen.pth https://data.deepai.org/deoldify/ColorizeArtistic_gen.pth
RUN [[ ! -f /data/models/ColorizeVideo_gen.pth ]] && wget -O /data/models/ColorizeVideo_gen.pth https://data.deepai.org/deoldify/ColorizeVideo_gen.pth
`",hey ca add well run run,issue,negative,neutral,neutral,neutral,neutral,neutral
731080993,"I think that the most feasible option would be something like [MyHeritage](https://www.myheritage.com/incolor), but for video.",think feasible option would something like video,issue,negative,neutral,neutral,neutral,neutral,neutral
731055284,"@alexandrevicenzi  Thanks- that's well put.  

I'm not aware of any ""free"" apps for Windows, and putting free in quotes there is quite appropriate because it's certainly not free to develop! I can tell you guys this much:  We've even looked at doing a commercial desktop version just for our proprietary (much better) image models, and we've decided to shelve that for now because it's just too much of a technical challenge currently.  Mostly that has to do with the fact that we're talking about rather large models, and that the support in PyTorch on the C++ end is just very lacking quite frankly.  

Believe me, I've tried.  I have the code in our private repo that I put quite a bit of work into.

And if anybody thinks I'm full of shit they can go right ahead and prove me wrong by making it themselves!",well put aware free free quite appropriate certainly free develop tell much even commercial version proprietary much better image decided shelve much technical challenge currently mostly fact talking rather large support end quite frankly believe tried code private put quite bit work anybody full go right ahead prove wrong making,issue,positive,positive,positive,positive,positive,positive
731033576,"@hammerheaddown Windows support is not planned, you would have better luck hiring a developer to implement the app for you.

@jantic are you aware of any ""free"" apps for Windows already?",support would better luck developer implement aware free already,issue,positive,positive,positive,positive,positive,positive
730813304,how much donation for a win desktop version for video? ,much donation win version video,issue,positive,positive,positive,positive,positive,positive
730502308,"@1997wzk I did this:

```
$ pip uninstall ffmpeg
$ pip uninstall ffmpeg-python
$ pip install ffmpeg-python
``` 
Make sure to be in the `deoldify` env when you run these commands (`conda activate deoldify`).",pip pip pip install make sure run activate,issue,negative,positive,positive,positive,positive,positive
730148283," @jtn7, hello，excuse me, I encountered the same problem as you, but reinstalling ffmpeg-python did not solve the problem. May I ask where ffmpeg-python should be installed, or can you tell me the details of the solution?",problem solve problem may ask tell solution,issue,negative,neutral,neutral,neutral,neutral,neutral
729161225,"Wow, great stuff, thanks @jqueguiner ! I'll wait for @mewbot97 to review.",wow great stuff thanks wait review,issue,positive,positive,positive,positive,positive,positive
729159537,"I just did a fresh conda install of DeOldify and couldn't reproduce what you experienced here.  This to me suggests that there's something that was done incorrectly on your end. Perhaps you forgot to do this before running Jupyter:

`conda activate deoldify`

Anyway, unless it's reproducible I'm not going to try to hunt down this issue any further. Thanks!",fresh install could reproduce experienced something done incorrectly end perhaps forgot running activate anyway unless reproducible going try hunt issue thanks,issue,positive,positive,positive,positive,positive,positive
728877381,"- quickstart added

```bash
$ ./quick_start.sh
usage : ./quick_start.sh notebook password -- to start the notebook with password
             leave empty for no password (not recommended)
usage : ./quick_start.sh image_api  -- to start image api
usage : ./quick_start.sh video_api  -- to start video api

you can add non mandatory arguments
usage : ./quick_start.sh image_api port host -- for custom port or host
```


- quickstart auto detect the presence of GPU including when running the docker with ipc option as well as in the pytorch implementation.

- Fixed also the download of models

- made a pseudo local cache management of models

- api_cmd_example added 
```bash
$ api_cmd_example.sh
usage : ./api_cmd_example.sh image  -- to test image api
usage : ./api_cmd_example.sh video  -- to test video api

you can add non mandatory arguments
usage : ./api_cmd_example.sh image port host -- for custom port or host
```
",added bash usage notebook password start notebook password leave empty password usage start image usage start video add non mandatory usage port host custom port host auto detect presence running docker option well implementation fixed also made pseudo local cache management added bash usage image test image usage video test video add non mandatory usage image port host custom port host,issue,negative,neutral,neutral,neutral,neutral,neutral
727978168,"Now it seems to work. I indeed made an error in installing the wrong requirements, as I did it by using the python3-venv module installing the requirements given in the `requirements.txt` file.
Using the approach with conda seems to work properly.

Thank you for the hint!",work indeed made error wrong module given file approach work properly thank hint,issue,negative,negative,negative,negative,negative,negative
727379979,"Please ignore my premature question 
I simply missed to set ffmpeg\bin in environment variables in Win10 .. and now all set and working perfect 
Sorry for bothering ",please ignore premature question simply set environment win set working perfect sorry,issue,positive,positive,positive,positive,positive,positive
727119828,"Thank you very very much!

Στις Παρ, 13 Νοε 2020 στις 8:58 μ.μ., ο/η Jason Antic <
notifications@github.com> έγραψε:

> This should be good to go now. The model links were moved to a new
> location and I just now merged the code for that to work again. Shout out
> to DeepAI for hosting them!
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/285#issuecomment-726973373>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARXULQIRABJYZWSKKQTINYLSPV6VBANCNFSM4TUJZSSA>
> .
>
",thank much antic good go model link new location code work shout hosting thread reply directly view,issue,positive,positive,positive,positive,positive,positive
726973373,This should be good to go now.  The model links were moved to a new location and I just now merged the code for that to work again.  Shout out to DeepAI for hosting them!,good go model link new location code work shout hosting,issue,negative,positive,positive,positive,positive,positive
726972526,The model links were moved to a new location by DeepAI and they provided the necessary changes to update the links.  I've just now merged them in and thing should be good to go now.,model link new location provided necessary update link thing good go,issue,negative,positive,positive,positive,positive,positive
726913537,"@Derik555 Having the same issue here as well, today in Colab. It appears the Mkdir Models directory isn't loading, and as a result the rest of the process fails. ",issue well today directory loading result rest process,issue,negative,neutral,neutral,neutral,neutral,neutral
726791021,"@Derik555 I have been using DeOldify for nearly a year now. It is nice that there is a Colabs solution and Jason Antic is doing great stuff, but Colabs is far from perfect. Over time I have ran into many problems. About half of my colorization attempts failed for some reason (too large file, errors, time-out while downloading the result etc.). **Since today I am getting the same error as you: ""UnpicklingError: invalid key, 'm'.  I have already written to Jason to inform him about this problem.**
If it doesn't work, there is a fall back by using ""PixBim color surprise AI"". But then you need to split the footage into png or bmp frames, use their batch photo colorizer and then re-assemble everything into a video. It takes 4 times as long.",nearly year nice solution antic great stuff far perfect time ran many half colorization reason large file result since today getting error invalid key already written inform problem work fall back color surprise ai need split footage use batch photo everything video time long,issue,positive,positive,positive,positive,positive,positive
726371292,"Based on how you're describing then installation it sounds like you did it manually, and I'm seeing evidence of this: FastAI version appears to be different than the required 1.0.51 because I'm comparing the line numbers in that stacktrace there and they don't match up to the 1.0.51 fastai version of that data_block.py file I have here.  I can tell you this much:  The fastai version matters a lot.  There were breaking changes made after that version, which is why I froze it there.

The better approach here is to use the conda install, via command line in the root DeOldify folder:

`conda env create -f environment.yml`

Then run

`conda activate deoldify `

and you should be set.

That'll take care of your dependencies for you so that you don't run into a potential mismatch.  That's really what this looks like at this point.
",based installation like manually seeing evidence version different line match version file tell much version lot breaking made version froze better approach use install via command line root folder create run activate set take care run potential mismatch really like point,issue,positive,positive,positive,positive,positive,positive
726361252,No response...gonna close.,response gon na close,issue,negative,neutral,neutral,neutral,neutral,neutral
724962059,"Yes, this was intentional. With the current repo you shouldn't need that anyway.  Can you let me know what your use case is here?",yes intentional current need anyway let know use case,issue,negative,neutral,neutral,neutral,neutral,neutral
724319773,This just happened again.  I'm using the aws links in the Colabs now (already committed) and I'll address the readme links soon.  ,link already address link soon,issue,negative,neutral,neutral,neutral,neutral,neutral
724291703,"This would be way beyond the scope of what we're willing to do for this project.  You're basically talking about creating a product! 

Your best bet for images is to go here if you want to use a good free version:  https://deepai.org/machine-learning-model/colorizer

Or the best and latest version:  https://www.myheritage.com/incolor",would way beyond scope willing project basically talking product best bet go want use good free version best latest version,issue,positive,positive,positive,positive,positive,positive
723696432,This is now fixed.  Yay!  Apparently Colabs now have stricter checking on the format of the notebook markup and it was failing on duplicate ids that somehow wound up in the notebooks.,fixed apparently format notebook markup failing duplicate somehow wound,issue,negative,positive,neutral,neutral,positive,positive
720186864,"This is working now, because Dropbox's 24 hour time limit on the suspension has lapsed.  I haven't heard from them so who knows if they'll just do it again.  But in the meantime, party on.",working hour time limit suspension lapsed party,issue,negative,neutral,neutral,neutral,neutral,neutral
720091062,"@CaiqueCoelho, @jantic Everything works without any problems now. Thank you for your kindness!",everything work without thank kindness,issue,negative,neutral,neutral,neutral,neutral,neutral
720065912,"@kugyo5  DeepAI kindly provided these links (you can run these ca

[Image (artistic)](https://deepai-opensource-codebases-models.s3-us-west-2.amazonaws.com/deoldify/ColorizeArtistic_gen.pth)

[Video](https://deepai-opensource-codebases-models.s3-us-west-2.amazonaws.com/deoldify/ColorizeVideo_gen.pth)",kindly provided link run ca image artistic video,issue,negative,positive,positive,positive,positive,positive
720064380,"I'd like to download ColorizeVideo_gen.pth. However, I cannot find it anywhere. 
Where can I download it?",like however find anywhere,issue,negative,neutral,neutral,neutral,neutral,neutral
720018756,"Hi @jantic I had a similar problem with the dropbox these times and the only solution I found was to move to the google drive on a public link using the gdown library to download the files. I hope you can find a solution, I look forward to trying your project, anything I can help just let me know",hi similar problem time solution found move drive public link library hope find solution look forward trying project anything help let know,issue,positive,neutral,neutral,neutral,neutral,neutral
720009984,"@jantic Hi, thank you for your reply. I see. I'll wait for a while.

",hi thank reply see wait,issue,negative,neutral,neutral,neutral,neutral,neutral
719996375,@kugyo5  Yeah I got a notification from Dropbox that they've shutdown the links temporarily because of high traffic. I'm waiting to hear back from their customer service on this.,yeah got notification shutdown link temporarily high traffic waiting hear back customer service,issue,negative,positive,neutral,neutral,positive,positive
718232848,"Most likely cells weren't run in order or were skipped- that's how this issue normally happens.  I just tried the Colab and it worked fine for me on this video:  http://i.imgur.com/zaYFEwq.mp4

Try that yourself and see if you can get that to work.",likely run order issue normally tried worked fine video try see get work,issue,negative,positive,positive,positive,positive,positive
704042974,"Thank you very much for the feedback! So it sounds like I have to wait then and hope 😉

",thank much feedback like wait hope,issue,positive,positive,positive,positive,positive,positive
703931120,"Yes, Jason. Thanks for your help. I reloaded the page and started
from scratch and now it is working fine.

On Mon, Oct 5, 2020 at 5:26 PM Jason Antic <notifications@github.com> wrote:

> So I'll try to figure out what could have gone wrong here. The most likely
> scenario seems to be that you're trying to run it on your own computer. The
> Colab is strictly meant to be ran online, here:
> link
> <https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb>
>
> And make sure to run each cell in order.
>
> Does that help?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/274#issuecomment-703898197>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ARH7GK6EQSXQ2TEUGFEDRK3SJI2ZLANCNFSM4SDX57OA>
> .
>
",yes thanks help page scratch working fine mon antic wrote try figure could gone wrong likely scenario trying run computer strictly meant ran link make sure run cell order help thread reply directly view,issue,positive,positive,positive,positive,positive,positive
703906694,"@alexandrevicenzi  Thanks.  I agree, these are pretty big changes and I'll have to wait for @jqueguiner to weigh in on this.",thanks agree pretty big wait weigh,issue,positive,positive,positive,positive,positive,positive
703905307,Seems like there's a fairly easy answer here:  If you're running DeOldify locally use VideoColorizer.ipynb instead.  This Colab notebook you're using is strictly meant to be ran [here](https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb): ,like fairly easy answer running locally use instead notebook strictly meant ran,issue,positive,positive,positive,positive,positive,positive
703904005,That makes sense but this would be something for @jqueguiner to weigh in on (original author and primary user). Thoughts @jqueguiner ?,sense would something weigh original author primary user,issue,negative,positive,positive,positive,positive,positive
703902121,"Training from beginning to end will take 3-5 days. 

Your best bet in getting generally better results is to train on a bunch of images from a more diverse training set such as open images.  BUT the gains from this will be marginal relative to what you're trying to achieve here, and ultimately you're just not going to get the level of control you're hoping for here, even if you were to add very specific training examples. I call it a very long tail problem: The reality is that it's quite literally an impossible task to get perfection in this sort of task, especially when it comes to things like clothing.

Really for people who want this level of control the path I foresee that I want to explore is blending manual interventions with automatic colorization.  Say for example, you'd have a user place a dot of color on the hat to indicate that it should be red, and the model runs with that hint accordingly.  That's what I think should be done but I just haven't made it yet.",training beginning end take day best bet getting generally better train bunch diverse training set open gain marginal relative trying achieve ultimately going get level control even add specific training call long tail problem reality quite literally impossible task get perfection sort task especially come like clothing really people want level control path foresee want explore blending manual automatic colorization say example user place dot color hat indicate red model hint accordingly think done made yet,issue,positive,positive,neutral,neutral,positive,positive
703898197,"So I'll try to figure out what could have gone wrong here.  The most likely scenario seems to be that you're trying to run it on your own computer. The Colab is strictly meant to be ran online, here: 
 [link](https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb)

And make sure to run each cell in order.

Does that help?",try figure could gone wrong likely scenario trying run computer strictly meant ran link make sure run cell order help,issue,negative,neutral,neutral,neutral,neutral,neutral
699584818,"Hello! There are none of the paths you indicated:

./data/imagenet/ILSVRC/Data/CLS-LOC/models
path = Path ('data / imagenet / ILSVRC / Data / CLS-LOC')

---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
<ipython-input-6-81c07a854ecc> in <module>
----> 1 colorizer = get_video_colorizer()

~/DeOldify/deoldify/visualize.py in get_video_colorizer(render_factor)
    357 
    358 def get_video_colorizer(render_factor: int = 21) -> VideoColorizer:
--> 359     return get_stable_video_colorizer(render_factor=render_factor)
    360 
    361 

~/DeOldify/deoldify/visualize.py in get_stable_video_colorizer(root_folder, weights_name, results_dir, render_factor)
    378     render_factor: int = 21
    379 ) -> VideoColorizer:
--> 380     learn = gen_inference_wide(root_folder=root_folder, weights_name=weights_name)
    381     filtr = MasterFilter([ColorizerFilter(learn=learn)], render_factor=render_factor)
    382     vis = ModelImageVisualizer(filtr, results_dir=results_dir)

~/DeOldify/deoldify/generators.py in gen_inference_wide(root_folder, weights_name, nf_factor, arch)
     13     )
     14     learn.path = root_folder
---> 15     learn.load(weights_name)
     16     learn.model.eval()
     17     return learn

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in load(self, file, device, strict, with_opt, purge, remove_module)
    259              with_opt:bool=None, purge:bool=True, remove_module:bool=False):
    260         ""Load model and optimizer state (if `with_opt`) `file` from `self.model_dir` using `device`. `file` can be file-like (file or buffer)""
--> 261         if purge: self.purge(clear_opt=ifnone(with_opt, False))
    262         if device is None: device = self.data.device
    263         elif isinstance(device, int): device = torch.device('cuda', device)

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in purge(self, clear_opt)
    299     def purge(self, clear_opt:bool=True):
    300         ""Purge the `Learner` of all cached attributes to release some GPU memory.""
--> 301         self._test_writeable_path()
    302         attrs_all = [k for k in self.__dict__.keys() if not k.startswith(""__"")]
    303         attrs_pkl = ['bn_wd', 'callback_fns', 'layer_groups', 'loss_func', 'metrics', 'model',

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in _test_writeable_path(self)
    175         try: tmp_file = get_tmp_file(path)
    176         except OSError as e:
--> 177             raise Exception(f""{e}\nCan't write to '{path}', set `learn.model_dir` attribute in Learner to a full libpath path that is writable"") from None
    178         os.remove(tmp_file)
    179 

Exception: [Errno 2] No such file or directory: 'models/tmpsm5w3cv9'
Can't write to 'models', set `learn.model_dir` attribute in Learner to a full libpath path that is writable",hello none path path data exception recent call last module return learn vi arch return learn load self file device strict purge purge load model state file device file file buffer purge false device none device device device device purge self purge self purge learner release memory self try path except raise exception write path set attribute learner full path writable none exception file directory ca write set attribute learner full path writable,issue,negative,positive,neutral,neutral,positive,positive
691528567,"There's sample code in visualize.py that shows converting raw bytes into PIL, similar to what you're describing:

```
def _get_image_from_url(self, url: str) -> Image:
        response = requests.get(url, timeout=30)
        img = PIL.Image.open(BytesIO(response.content)).convert('RGB')
        return img
```
From there you colorize it.  Granted, the code that people keep trying to use is the visualize.py stuff that is oriented around testing and jupyter notebooks, so they read and write to file paths which is not what you want here.  But within there you'll see this:

   ```
 def get_transformed_image(
        self, path: Path, render_factor: int = None, post_process: bool = True,
        watermarked: bool = True,
    ) -> Image:
        self._clean_mem()
        orig_image = self._open_pil_image(path)
        filtered_image = self.filter.filter(
            orig_image, orig_image, render_factor=render_factor,post_process=post_process
        )

        if watermarked:
            return get_watermarked(filtered_image)

        return filtered_image
```

The relevant call for you is just the self.filter.filter part:

`        filtered_image = self.filter.filter(
            orig_image, orig_image, render_factor=render_factor,post_process=post_process
        )`

That way you can directly pass an image in and get an image out.

Hope that helps.",sample code converting raw similar self image response return colorize code people keep trying use stuff around testing read write file want within see self path path none bool true bool true image path return return relevant call part way directly pas image get image hope,issue,negative,positive,positive,positive,positive,positive
683937802,"@AlexanderKozhevin It really depends on what kind of data you're talking about.  I've actually fine tuned superresolution functionality out of the model but that was using a lot of data in the process. And that consisted of training the both the generator and critic a bit on the pretraining end first- I forgot how much exactly but the point being you didn't have to do nearly as much training there.  Then I did the normal GAN training stage.

If you're talking about a small amount of data (100s to a few 1000s of images) to finetune on- I haven't tried anything like that yet and honestly I just can't say anything other than ""try it!""",really kind data talking actually fine tuned functionality model lot data process training generator critic bit pretraining end forgot much exactly point nearly much training normal gan training stage talking small amount data tried anything like yet honestly ca say anything try,issue,positive,positive,positive,positive,positive,positive
683425916," I just moved it to regular python scripts because broweser with jupyter notebook getting broken because of RAM shortage. More or less I was able to complete it.
@jantic Sorry, I've another stupid question. Is there a way to fine-tune neural network. 
I mean there is already great result which you created. 

It it possible just pick a bunch of extra images and fine-tune on them?
If the answer too big, you can give me just general direct to google. ",regular python notebook getting broken ram shortage le able complete sorry another stupid question way neural network mean already great result possible pick bunch extra answer big give general direct,issue,negative,negative,neutral,neutral,negative,negative
683149494,@AlexanderKozhevin  That's new to me. I can't go ahead and try to reproduce this myself yet because my GPUs are all occupied currently and will be for the next two weeks. So I guess I have to start with:  Have you changed -anything-?  Just as an example- I can tell you I know the noise augmentation (not used by the critic trainer here by default) has a nasty memory leak and I haven't gotten around updating it yet.  I use something else internally now.,new ca go ahead try reproduce yet currently next two guess start tell know noise augmentation used critic trainer default nasty memory leak gotten around yet use something else internally,issue,negative,negative,negative,negative,negative,negative
678508522,"@hiterjoshua  You're using the critic in a weird way here- using generator oriented code to try to examine the output of the critic. That's why you're getting these results. It's a bit hairy but it boils down to this:  You're trying to use an image as a target in this case for the critic as the ""ground truth"" when really your target is ultimately 1 or 0 for real or fake.  AdaptiveLoss changes the length of this target vector adaptively to match the dimensions of the critic output:

```
class AdaptiveLoss(Module):
    ""Expand the `target` to match the `output` size before applying `crit`.""
    def __init__(self, crit): self.crit = crit
    def forward(self, output, target):
        return self.crit(output, target[:,None].expand_as(output).float())
```

So if you really wanted to look at the output of the critic in an informative way, do it this way, because this reflects how it's actually used.  As you can see, the output of the critic here winds up being just sized (1, 484), as opposed to that being repeated 3 times to match the number of channels in an RBG image as you ran into above.

![image](https://user-images.githubusercontent.com/179759/90935514-358ad500-e3b8-11ea-9178-9df9ad2d236a.png)
",critic weird way generator code try examine output critic getting bit hairy trying use image target case critic ground truth really target ultimately real fake length target vector adaptively match critic output class module expand target match output size self forward self output target return output target output really look output critic informative way way actually used see output critic sized opposed repeated time match number image ran image,issue,negative,negative,negative,negative,negative,negative
675046996,We have a solution for this in our commercial models but it's not going to be released anytime soon in open source for a number of reasons. Competitive advantage being just one of them.,solution commercial going soon open source number competitive advantage one,issue,positive,neutral,neutral,neutral,neutral,neutral
675046161,Yes that's intentional. It's more flexible and robust to future changes in this way.,yes intentional flexible robust future way,issue,positive,neutral,neutral,neutral,neutral,neutral
674334081,"@jantic Thank you for your reply, now I understand how to transform the list to a float between 0 and 1 by BCEWithLogitsLoss , and make it perform as a loss function with  gan_loss_from_func.

but why the output is a list of which  the length is 3? According to the discriminator model, the pic input of size  (400,400,3) would  have a flatten output of (1,484), while I receive a output of [(1,484), (1,484), (1,484)].

My code is below:
The model part:
```
def colorize_crit_learner(
    data: ImageDataBunch,
    loss_critic=AdaptiveLoss(nn.BCEWithLogitsLoss()),
    nf: int = 256,
) -> Learner:
    return Learner(
        data,
        custom_gan_critic(nf=nf),
        metrics=accuracy_thresh_expand,
        loss_func=loss_critic,
        wd=1e-3,
    )

# Weights are implicitly read from ./models/ folder
def critic_inference_wide(
    root_folder: Path, weights_name: str, nf_factor: int = 256) -> Learner:
    data = get_dummy_databunch()
    learn = colorize_crit_learner(data=data, loss_critic=AdaptiveLoss(nn.BCEWithLogitsLoss()), nf=256)
    learn.path = root_folder
    learn.model_dir = os.path.dirname(os.path.abspath(__file__))
    learn.load(weights_name)
    #print(learn)
    learn.model.eval()
    return learn
```

The inference part:

        model_image= PIL.Image.fromarray(np.zeros((400,400),np.uint8)).convert('RGB')
        print(numpy.asarray(model_image))
        
        x = pil2tensor(model_image, np.float32)
        x = x.to(self.device)
        x.div_(255)
        x, y = self.norm((x, x), do_x=True)

        try:
            result = self.learn.pred_batch(
                ds_type=DatasetType.Valid, batch=(x[None], y[None]), reconstruct=True
            )
        except RuntimeError as rerr:
            if 'memory' not in str(rerr):
                raise rerr
            print('Warning: render_factor was set too high, and out of memory error resulted. Returning original image.')
            return model_image
        
        print(len(result), type(result), result)`

The two line print output is:
`(400, 400, 3)`

`3 <class 'list'> [Image (1, 484), Image (1, 484), Image (1, 484)]`

Can you give me some tips why the output of discriminator is  [(1,484), (1,484), (1,484)] instead of [(1,484)]? Hope your answer, Thank you!

",thank reply understand transform list float make perform loss function output list length according discriminator model pic input size would flatten output receive output code model part data learner return learner data implicitly read folder path learner data learn print learn return learn inference part print try result none none except raise print set high memory error original image return print result type result result two line print output class image image image give output discriminator instead hope answer thank,issue,positive,positive,positive,positive,positive,positive
674021505,"@zhouhana I guess the short answer is that the colorizations look significantly better in every way :).  Unfortunately I can't give you specific details as to what has changed in the models over the past 1 1/2 years since development of the open source version.  I can just say what I've already said publicly:  Attention is greatly improved upon leading to better details/skin, etc; GAN training is no longer used and results are much more stable; And basically the current models are a culmination of two years of staring at comparison images and slowly having correct hunches as to what's actually going on :). Sometimes.  Most of the time I'm just plain wrong. Rinse and repeat.

But you can try for yourself [here](https://www.myheritage.com/incolor) for free over the next month actually (they're running a promotion currently).

We'll continue to provide them with the latest polished models we have. You can see what I've been posting on Twitter in my media posts here as I continue development: https://twitter.com/citnaj/media",guess short answer look significantly better every way unfortunately ca give specific past since development open source version say already said publicly attention greatly upon leading better gan training longer used much stable basically current culmination two staring comparison slowly correct actually going sometimes time plain wrong rinse repeat try free next month actually running promotion currently continue provide latest polished see posting twitter medium continue development,issue,positive,positive,neutral,neutral,positive,positive
673123250,"I guess the easiest way to think of it is that the burden of turning this into a single score shifts to the critic loss function itself- [BCEWithLogitsLoss](https://pytorch.org/docs/master/generated/torch.nn.BCEWithLogitsLoss.html). If you look at the source code in FastAI AdaptiveLoss wraps this loss to make sure that the loss's targets are fed with the appropriate length tensor filled with zeros for the ""fake"" predictions and ones for the ""real"" predictions.  It's a bit hard to follow but the code for that is [here](https://github.com/fastai/fastai/blob/master/fastai/vision/gan.py) and perhaps starting at gan_loss_from_func would help in understanding.  

Now as to why they decided to do it this way in the FastAI code?  I actually never asked! I would suspect it's more numerically robust and stable (that has been my assumption), but I never actually tested that theory because I moved away from GANs in my DeOldify work soon after I completed the initial rollout of the current open source version here.   But basically you can think of it as a round about way of achieving the same exact thing that you're looking for here.",guess easiest way think burden turning single score critic loss function look source code loss make sure loss fed appropriate length tensor filled fake real bit hard follow code perhaps starting would help understanding decided way code actually never would suspect numerically robust stable assumption never actually tested theory away work soon initial current open source version basically think round way exact thing looking,issue,negative,positive,neutral,neutral,positive,positive
673103078,"@jorgealbertogomezgomez77  Yes, if you install DeOldify locally you can colorize images and video from your local drive.  In the ImageColorizer.ipynb notebook, for example, you just set the source_url to None and put the local file path in for source_path .  Admittedly the current implementations in the notebooks are a bit goofy.

![image](https://user-images.githubusercontent.com/179759/90066102-27470580-dca2-11ea-9532-3d47017c87f6.png)
",yes install locally colorize video local drive notebook example set none put local file path admittedly current bit goofy image,issue,negative,positive,neutral,neutral,positive,positive
672473243,"> @miaoqiz How did you exactly restructure the training set? I'm currently having this problem as well (though I'm using a bs=44).

Hi, it has been a long time. You can try various batch size. 

BTW, using the latest ""Pytorch"" may help.",exactly training set currently problem well though hi long time try various batch size latest may help,issue,negative,positive,positive,positive,positive,positive
671919812,The model can be used with images provided from the computer and not from the internet?,model used provided computer,issue,negative,neutral,neutral,neutral,neutral,neutral
671540015,If hunting down Imagenet proves to be impossible you should be able to get excellent results using a few million images using [Open Images](https://storage.googleapis.com/openimages/web/index.html). It's one of the main datasets we use now for our closed source version. In fact I suspect if you use it in isolation to replace Imagenet in this case you may in fact get better results.,hunting impossible able get excellent million open one main use closed source version fact suspect use isolation replace case may fact get better,issue,negative,positive,positive,positive,positive,positive
671349963,@miaoqiz How did you exactly restructure the training set? I'm currently having this problem as well (though I'm using a bs=44). ,exactly training set currently problem well though,issue,negative,positive,positive,positive,positive,positive
671270650,"It is not available in Kaggle anymore. Some people suggested the [original imagenet download site](http://image-net.org/download) but I'm trying it it works out with [imagenette](https://github.com/fastai/imagenette) first. 
![image](https://user-images.githubusercontent.com/1536515/89772495-60487400-db02-11ea-8895-667405c12226.png)
",available people original site trying work first image,issue,negative,positive,positive,positive,positive,positive
671216161,"Perfect. Thank you for your help with this, Jason. I'll look into Deep Exemplar now.

",perfect thank help look deep exemplar,issue,positive,positive,positive,positive,positive,positive
670656277,"@gitLRD That's great to hear, and really helpful to get this sort of feedback documented in the issues log.  Thank you!",great hear really helpful get sort feedback log thank,issue,positive,positive,positive,positive,positive,positive
670655905,"@mbittraining Answers:

1. What you're really describing here is what [Deep Exemplar-based Colorization](https://github.com/msracver/Deep-Exemplar-based-Colorization) tackles best.  DeOldify would realistically require a lot more examples than 10, and even if you had the data it's questionable as to how well it would work relative to expectations.  Reason: DeOldify leans heavily on more generalized material/physical properties to determine color rather than object recognition.  This makes it robust but also limited in accuracy.
2. DeOldify already does this with the data you feed it and much more.  This is what is termed ""augmentation"" and is a standard practice to get more mileage out of your training data. It definitely helps but it won't be the magic bullet in your scenario here.  ",really deep colorization best would realistically require lot even data questionable well would work relative reason heavily generalized determine color rather object recognition robust also limited accuracy already data feed much augmentation standard practice get mileage training data definitely wo magic bullet scenario,issue,positive,positive,positive,positive,positive,positive
670207947,"Hey Jason,

Looks like a newer version of Fedora has solved this for me; I deleted all the existing DeOldify environments, recreated them and it worked flawlessly. Thanks for the help 👍 ",hey like version worked flawlessly thanks help,issue,positive,positive,positive,positive,positive,positive
669475626,"Hi Jason. I hope you are well.

Let's reset the conversation as I have an important question for you:

The training catalogue requires millions of images. This is not reasonable when you have a limited pallette and limited source images which are almost identical.

1. How do I limit the number of training images and get an optimal result when colourising only one image and have similar images taken at the same time (e.g. photoshoot of 10 identical images with the one b/w image to be colourised slightly offset)?

2. To help with the quantity can I mirror and flip the images so I have 4 rotations of each image? Would that help or does it matter at all? Does the algorithm boil down the colour neighbouring associations therefore it doesn't matter to rotate them?

I'd value your ideas on the above.

Best regards,

Matthew",hi hope well let reset conversation important question training catalogue million reasonable limited pallette limited source almost identical limit number training get optimal result one image similar taken time identical one image slightly offset help quantity mirror flip image would help matter algorithm boil colour therefore matter rotate value best,issue,positive,positive,positive,positive,positive,positive
668863942,"Yeah you're absolutely right that there's no indication in the directions that this will come up as an issue.  So the api is a bit confusing as a result and so are the directions. Really what we need I think is a change in the api so that it's clean and you don't have edit the files for this.  

@jqueguiner  Since you'd be affected by this, what do you think of making the video and image Docker-api deployments 100% cleanly separated so that no editing of the Dockerfile-api file is required to deploy them?  Perhaps just making a separate Dockerfile-video-api and Dockerfile-image-api would suffice?",yeah absolutely right indication come issue bit result really need think change clean edit since affected think making video image cleanly file deploy perhaps making separate would suffice,issue,positive,positive,positive,positive,positive,positive
668610026,GOLD. Thank you Jason. Learning lots.,gold thank learning lot,issue,negative,neutral,neutral,neutral,neutral,neutral
668566286,Well its funny you are saying that im using the image processing im following your directions for video. Based off the directions in this repo you cant run it locally. Have you tried?,well funny saying image following video based cant run locally tried,issue,positive,positive,neutral,neutral,positive,positive
668243096,"Thanks Jason. Conda was a brand new install so I would have thought it was the most up to date, but this box was running Fedora 29 when I tried it which is pretty old by now, so I just upgraded it to 32 so let me have another go and see what happens when everything is a bit newer. 

Will get back to you in a bit.",thanks brand new install would thought date box running tried pretty old let another go see everything bit get back bit,issue,positive,positive,positive,positive,positive,positive
668169594,"That's a bit strange.  I'm not reproducing it myself.  Based on what I'm seeing elsewhere researching this issue, my best guess as to now is that you may just need to update conda itself and retry. Update conda by running this command: 

`conda update conda`

If that works, great- please let me know. If not, please provide complete conda environment install output for this.  Thanks!",bit strange based seeing elsewhere issue best guess may need update retry update running command update work please let know please provide complete environment install output thanks,issue,positive,positive,positive,positive,positive,positive
668156693,"So there's a few things going on here.  First, you're running image processing docker so it's expecting an image, not a video.  Hence it's complaining about not being able to identify the ""image file"". You'd want to run app-video.py for video and to do that you'll have to change CMD in https://github.com/jantic/DeOldify/blob/master/Dockerfile-api accordingly. I don't use the docker functionality personally so I can't 100% say if that's sufficient without digging further.

But really more than anything what I'd recommend if you simply want to run this stuff locally is just use the Jupyter notebooks and run jupyter lab, rather than trying to use docker.  My argument is that it's a lot more interactive and gives you more useful feedback, and is easier to identify issues. It gives you a nice UI to work with.",going first running image docker image video hence able identify image file want run video change accordingly use docker functionality personally ca say sufficient without digging really anything recommend simply want run stuff locally use run lab rather trying use docker argument lot interactive useful feedback easier identify nice work,issue,positive,positive,positive,positive,positive,positive
668149981,"@AshleyRudland  Hey, great question actually! So there are a few things going on here:

1. I needed to be able to add both spectral norm and batch norm for each layer.  FastAI's DynamicUnet doesn't support this, and it's insufficient to omit one or the other.  Believe me, I've tried.
2. I needed to be able to customize the width (number of channels per layer) with a multiplier (via nf_factor). Widening the models proved to make a big difference.
3. DynamicUnetDeep is a slight customization based on what has already been spelled out in #1 and #2.  But DynamicUnetWide is significantly different in the architecture in that it cuts the number of layers in favor of having fewer but wider layers. This proved to make video more stable.

My original intent was to eventually do a pull request on these customizations for the core FastAI library,  but naturally I let that slide and forgot about it.

I will say this too as a big hint on stuff I haven't released (it's closed source): It's worthwhile to reexamine the Unet for memory efficiency and effectiveness. There's a lot of low hanging fruit there.",hey great question actually going able add spectral norm batch norm layer support insufficient omit one believe tried able width number per layer multiplier via proved make big difference slight based already significantly different architecture number favor proved make video stable original intent eventually pull request core library naturally let slide forgot say big hint stuff closed source memory efficiency effectiveness lot low hanging fruit,issue,positive,positive,positive,positive,positive,positive
666794105,"I wouldn't recommend trying to do more than a length of a minute or two of video on Colabs. They have severe limitations. Beyond that, you'll be much better off doing this on a local install of DeOldify.

I've increased retries to make sure all fragments are downloaded.  It's a possibility that you're running into incomplete videos being stitched together after fragments are skipped due to download failure. (I've been able to reproduce that locally).",would recommend trying length minute two video severe beyond much better local install make sure possibility running incomplete together due failure able reproduce locally,issue,negative,positive,positive,positive,positive,positive
666749998,@haoyuying I simply can't reproduce your issue.  I'd try deleting the video model at /ssd3/haoyuying/DeOldify1/data/models/ and trying again.,simply ca reproduce issue try video model trying,issue,negative,neutral,neutral,neutral,neutral,neutral
666442737,"A good idea, but from what I can make out, to be able to create your own training data you need to train using millions of images. The image bank Jason has used is absolutely massive.  (many tens of GB)

Speaking with others in the community using a limited number of images is also not the answer as the program with 'overtrain'. 

Not sure however how many would be 'reasonable'. 

If I wanted to stylise akin to Van Gogh for example I am limited to the number of paintings available in this style and colour scheme. 

However I am keen to learn if the pretrain data can be used as a starting point. Can you add existing colour images to a training bank thereby fixing the occasional problem? I've tried using the standard stable pretrained file and the results are very good, although it is very shy around the colour red. I have a person in the photo with a red handkerchief standing next to a blue wall. The wall turns out dark grey with a hint of blue, yet I have a colour original of this photo and know that it is a navy blue. 

The red handkerchief is a standard polka-dot silk handkerchief so should be a strong dark red. 

And as for tweed! I've got no chance there...",good idea make able create training data need train million image bank used absolutely massive many speaking community limited number also answer program sure however many would akin van example limited number available style colour scheme however keen learn pretrain data used starting point add colour training bank thereby fixing occasional problem tried standard stable file good although shy around colour red person photo red handkerchief standing next blue wall wall turn dark grey hint blue yet colour original photo know navy blue red handkerchief standard silk handkerchief strong dark red tweed got chance,issue,positive,positive,positive,positive,positive,positive
665378899,"For the first time I use ""python app-vedio.py"", it download the checkpoint from 'https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0',and has these error:


$ python app-video.py
/ssd3/haoyuying/anaconda2/envs/GPU-Paddle-3/lib/python3.7/site-packages/fastai/data_block.py:442: UserWarning: Your training set is empty. If this is by design, pass ignore_empty=True to remove this warning.
warn(""Your training set is empty. If this is by design, pass ignore_empty=True to remove this warning."")
/ssd3/haoyuying/anaconda2/envs/GPU-Paddle-3/lib/python3.7/site-packages/fastai/data_block.py:445: UserWarning: Your validation set is empty. If this is by design, use split_none()
or pass ignore_empty=True when labelling to remove this warning.
or pass ignore_empty=True when labelling to remove this warning."""""")
Traceback (most recent call last):
File ""app-video.py"", line 83, in 
video_colorizer = get_video_colorizer()
File ""/ssd3/haoyuying/DeOldify1/deoldify/visualize.py"", line 357, in get_video_colorizer
return get_stable_video_colorizer(render_factor=render_factor)
File ""/ssd3/haoyuying/DeOldify1/deoldify/visualize.py"", line 378, in get_stable_video_colorizer
learn = gen_inference_wide(root_folder=root_folder, weights_name=weights_name)
File ""/ssd3/haoyuying/DeOldify1/deoldify/generators.py"", line 19, in gen_inference_wide
learn.load(weights_name)
File ""/ssd3/haoyuying/anaconda2/envs/GPU-Paddle-3/lib/python3.7/site-packages/fastai/basic_train.py"", line 269, in load
get_model(self.model).load_state_dict(model_state, strict=strict)
File ""/ssd3/haoyuying/anaconda2/envs/GPU-Paddle-3/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 719, in load_state_dict
self.class.name, ""\n\t"".join(error_msgs)))
RuntimeError: Error(s) in loading state_dict for DynamicUnetWide:
Missing key(s) in state_dict: ""layers.3.0.0.weight"", ""layers.3.1.0.weight"", ""layers.4.shuf.conv.0.weight"", ""layers.4.conv.0.weight"", ""layers.5.shuf.conv.0.weight"", ""layers.5.conv.0.weight"", ""layers.5.conv.3.query.weight"", ""layers.5.conv.3.key.weight"", ""layers.5.conv.3.value.weight"", ""layers.6.shuf.conv.0.weight"", ""layers.6.conv.0.weight"", ""layers.7.shuf.conv.0.weight"", ""layers.7.conv.0.weight"", ""layers.10.layers.0.0.weight"", ""layers.10.layers.1.0.weight"", ""layers.11.0.weight"".
Unexpected key(s) in state_dict: ""layers.3.0.0.weight_v"", ""layers.3.1.0.weight_v"", ""layers.4.shuf.conv.0.weight_v"", ""layers.4.conv.0.weight_v"", ""layers.5.shuf.conv.0.weight_v"", ""layers.5.conv.0.weight_v"", ""layers.5.conv.3.query.weight_v"", ""layers.5.conv.3.key.weight_v"", ""layers.5.conv.3.value.weight_v"", ""layers.6.shuf.conv.0.weight_v"", ""layers.6.conv.0.weight_v"", ""layers.7.shuf.conv.0.weight_v"", ""layers.7.conv.0.weight_v"", ""layers.10.layers.0.0.weight_v"", ""layers.10.layers.1.0.weight_v"", ""layers.11.0.weight_v"".

so ,I change the key name in the checkpoint, and it has the above problem(details in first issue). I show the model and the checkpoint, they do not match. For example,
layers.3.0.0.weight_orig torch.Size([4096, 2048, 3, 3])
layers.3.0.0.weight torch.Size([4096, 2048, 3, 3])
layers.3.0.0.weight_u torch.Size([4096]) for model,

layers.3.0.0.weight_orig torch.Size([4096, 2048, 3, 3])
layers.3.0.0.weight_u torch.Size([4096])
layers.3.0.0.weight_v torch.Size([18432]) for checkpoint.",first time use python error python training set empty design pas remove warning warn training set empty design pas remove warning validation set empty design use pas remove warning pas remove warning recent call last file line file line return file line learn file line file line load file line error loading missing key weight weight weight weight weight weight weight weight weight weight weight weight weight unexpected key change key name problem first issue show model match example weight model,issue,negative,positive,neutral,neutral,positive,positive
665361018,@haoyuying I'm not able to reproduce your issue.  I think it might be possible that the model download is corrupted and should be tried again. I'm not sure what else it could be if not that..,able reproduce issue think might possible model corrupted tried sure else could,issue,negative,positive,positive,positive,positive,positive
664549801,"> Actually, I didnot see a explanation of the time difference problem convincingly in your previous answer, is that related to some deep mechanism of the nvidia GPU system while we know nothing about it?

Simply put- my code behind the scenes was changing it to CPU.  That's why it was slower.",actually see explanation time difference problem convincingly previous answer related deep mechanism system know nothing simply code behind,issue,negative,negative,neutral,neutral,negative,negative
664145923,"@jantic At the code level, I totally understand what you talked about, simply, the two code parts should play exactly the same function.
 ```
import os
os.environ['CUDA_VISIBLE_DEVICES']='0'
```

```
#NOTE:  This must be the first call in order to work properly!
from nets.colorization.deoldify import device
from nets.colorization.deoldify.device_id import DeviceId
#choices:  CPU, GPU0...GPU7
device.set(device=DeviceId.GPU0)
```
After I read the codes of this repo, I do agree with you that the two codes were supposed to play the same role and indeed they did same in result while differs in inference time! Actually, I didnot see a explanation of the time difference problem convincingly in your previous answer, is that related to some deep mechanism of the nvidia GPU system while we know nothing about it? 
confused......",code level totally understand simply two code play exactly function import o note must first call order work properly import device import read agree two supposed play role indeed result inference time actually see explanation time difference problem convincingly previous answer related deep mechanism system know nothing confused,issue,positive,negative,neutral,neutral,negative,negative
663751589,"@kaoruse Yep...the answer is that this works fine on Linux and Windows isn't supported. You may be able to add a folder called ""dummy"" to the main folder and get it to work but you're bound to run into other issues.",yep answer work fine may able add folder dummy main folder get work bound run,issue,negative,positive,positive,positive,positive,positive
663747108,"@hiterjoshua I was able to replicate what you're seeing there, but I wouldn't call it an issue really. The whole idea is that the convention is to use the device functionality to use the GPUs. It coordinates the rest of the code in an easy to manage way....provided you stick to the convention.  I was almost tempted to accommodate this scenario by changing the source of the behavior your seeing- this code is called when you import from deoldify.visualize:

```
class _Device:
    def __init__(self):    
        self.set(DeviceId.CPU) 
```

I was about to change this to take into account the user deciding to use this instead:

```
import os
os.environ['CUDA_VISIBLE_DEVICES']='0'
```
Meaning that __init__ would have to do additional logic based on if this 'CUDA_VISIBLE_DEVICES' variable was already set. But then I realized a few things:

1. It will over complicate the code, meaning a greater chance for bugs.  In contrast, what you're seeing now is simply fallback behavior that results from not sticking with the convention- it goes to the CPU.
2. I'd have to take into account the possibility that the user does something this: `os.environ['CUDA_VISIBLE_DEVICES']='0,2,3'`. The problem with trying to accommodate that is two fold:  One, parallel and distributed training/inference aren't supported in the code explicitly so this would be misleading.  Two, the device ids for this reason are oriented to only having one device selectable at a time so doing something like just defaulting to the first device in the list silently would just be misleading. 
3. I'm a big stickler for being disciplined about sticking to convention for understandability reasons, generally. I don't see a point in adding another rabbit hole of ""many ways to do one thing""- that just means adding more maintenance and potential bugs to the plate.

Hope that all makes sense!
",able replicate seeing would call issue really whole idea convention use device functionality use rest code easy manage way provided stick convention almost accommodate scenario source behavior code import class self change take account user use instead import o meaning would additional logic based variable already set complicate code meaning greater chance contrast seeing simply fallback behavior sticking go take account possibility user something problem trying accommodate two fold one parallel distributed code explicitly would misleading two device reason one device selectable time something like first device list silently would misleading big stickler sticking convention understandability generally see point another rabbit hole many way one thing maintenance potential plate hope sense,issue,positive,positive,positive,positive,positive,positive
663407727,"@jantic Thanks for your kind reply, perhaps my explanation is not clear and what happened to me is slightly different from your reply above.

To make it clear, the code I use:

```
import os
os.environ['CUDA_VISIBLE_DEVICES']='0'


from deoldify.visualize import *
plt.style.use('dark_background')
torch.backends.cudnn.benchmark=True

# Weights are implicitly read from ./models/ folder
weights_name_nopth = 'imagenet-models/StableModel_imagenet_gen_0'
weights_name = os.path.join(os.path.dirname(os.path.abspath(__file__)), weights_name_nopth)
result_dir = 'output'

colorizer = ModelImageVisualizer(weights_name, result_dir)

##3NOTE:  Max is 45 with 11GB video cards. 35 is a good default
render_factor=25
##3NOTE:  Make source_url None to just read from file at ./video/source/[file_name] directly without modification

source_path = 'test-rgb'

result_path = None
for image_path in os.listdir(lab_path):
    image_path = lab_path + '/' + image_path

    import time
    t0 = time.time()
    result_path = colorizer.save_transformed_image(path=image_path, render_factor=render_factor, compare=True)

    t1 = time.time()
    print("" Timer: %.4f sec ."" % (t1 - t0))

```
The inference time I got is:

![image](https://user-images.githubusercontent.com/29330657/88369778-52d18100-cdc3-11ea-8544-114610ecaca8.png)

when I add the sentences below, the time differs much and the new inference time I got is:

```
#NOTE:  This must be the first call in order to work properly!
from nets.colorization.deoldify import device
from nets.colorization.deoldify.device_id import DeviceId
#choices:  CPU, GPU0...GPU7
device.set(device=DeviceId.GPU0)
```

![image](https://user-images.githubusercontent.com/29330657/88369797-5cf37f80-cdc3-11ea-864e-d5c52c5d5b68.png)

As you can see, `os.environ['CUDA_VISIBLE_DEVICES']='0'` is always used, while time is much different, so I wandered if you  noticed this phenomenon and could you give me a hand on this strange issue.

Again, sorry for my unclear explanation!







",thanks kind reply perhaps explanation clear slightly different reply make clear code use import o import implicitly read folder note video good default note make none read file directly without modification none import time print timer sec inference time got image add time much new inference time got note must first call order work properly import device import image see always used time much different phenomenon could give hand strange issue sorry unclear explanation,issue,positive,positive,positive,positive,positive,positive
663402001,@mbittraining have you tried running this in the cloud? Google Colab will let you run Python code on a GPU-enabled machine for free.,tried running cloud let run python code machine free,issue,positive,positive,positive,positive,positive,positive
661288595,"@hiterjoshua  I'm not clear on what your confusion is on exactly.  That code your citing above makes the difference between running inference on GPU and running it on CPU (which is default if those lines are omitted). So that would explain why it's running faster when you add the lines.

This device code was added back in March:  [code](https://github.com/jantic/DeOldify/blob/master/deoldify/_device.py)

The reason why I have it defaulting to CPU if the GPU is not specified is because that's the common denominator on all machines.  It'll at least run, albeit very slowly.

Basically what I was trying to achieve here was to make a reliable way for people to switch between different GPUs or CPU with a single line of code.  Previously the code that needed to be changed was a bit scattered and counterintuitive.  For example, behind the scenes you specific CPU like this (and this is what you used to have to do manually):

`os.environ['CUDA_VISIBLE_DEVICES']=''`

I'd rather have it be more explicit so now users can just do this at the top:

`device.set(device=DeviceId.CPU)`

I hope that makes sense.",clear confusion exactly code difference running inference running default would explain running faster add device code added back march code reason common denominator least run albeit slowly basically trying achieve make reliable way people switch different single line code previously code bit scattered example behind specific like used manually rather explicit top hope sense,issue,positive,negative,neutral,neutral,negative,negative
660923842,"@jantic could you please share me with your strategy here? cause I find the old test file committed last year does not have the codes mentioned above. and it did decrease the inference time a lot.

 And also, any advice from any person is highly appreciated and welcomed!!! ",could please share strategy cause find old test file last year decrease inference time lot also advice person highly,issue,positive,positive,neutral,neutral,positive,positive
660436590,"> @leeqiaogithub I'll just be brutally honest- I've had a lot of issues with W&B myself with reliability so I've avoided it personally. It doesn't actually give me anything that Tensorboard does already other than cloud storage and a somewhat nice web interface.
> 
> So I'll keep this open but it'll be pretty low priority.

ok，thank you very much.",brutally lot reliability personally actually give anything already cloud storage somewhat nice web interface keep open pretty low priority much,issue,negative,positive,neutral,neutral,positive,positive
660371768,"@leeqiaogithub I'll just be brutally honest- I've had a lot of issues with W&B myself with reliability so I've avoided it personally.  It doesn't actually give me anything that Tensorboard does already other than cloud storage and a somewhat nice web interface.

So I'll keep this open but it'll be pretty low priority. ",brutally lot reliability personally actually give anything already cloud storage somewhat nice web interface keep open pretty low priority,issue,negative,negative,neutral,neutral,negative,negative
657120065,"I don't want to come across as rude, but no, this isn't something I'm
willing to do. This is being run on Windows which we don't support and
already stated that, and I just don't do free personal tech support like
that.

On Sat, Jul 11, 2020 at 12:43 PM mbittraining <notifications@github.com>
wrote:

> Hi Jason. I really appreciate the help. Can you please set up a Teams call
> so that we can fix this together?
>
> work account: matthew.Bennett@platformhg.com
> personal: mattybennett@outlook.com
>
> Best regards,
>
> Matthew
> *Matthew Bennett*
> *Freelance Trainer and IT Consultant*
> *Mbe: **07871 480008*
>
> <https://www.facebook.com/mattyjbennett>
> <https://www.linkedin.com/in/matthew-bennett-a0384a11/>
> <https://www.youtube.com/channel/UCwi6J8wbTkNQ4-wJpPS70Ug>
> *[image: Microsoft Partner - Pinpoint page]*
> * <http://www.fsb.org.uk/>*
>
>
> On Thu, 9 Jul 2020 at 22:27, Jason Antic <notifications@github.com> wrote:
>
> > @mbittraining <https://github.com/mbittraining> There's two things going
> > on here. First that's clearly a Windows file path. Windows isn't
> supported
> > by us and you'll likely run into issues that we won't address here.
> >
> > Second- that warning doesn't matter in terms of inference (just
> colorizing
> > images). It'll work regardless (given you don't have other issues running
> > it on Windows).
> >
> > I'll commit code to silence the warning shortly just so it doesn't cause
> > further confusion though.
> >
> > —
> > You are receiving this because you were mentioned.
> > Reply to this email directly, view it on GitHub
> > <https://github.com/jantic/DeOldify/issues/255#issuecomment-656359617>,
> > or unsubscribe
> > <
> https://github.com/notifications/unsubscribe-auth/AF4V7MU5G22HTBL4QZU5UDLR2YY3DANCNFSM4OV6DN7Q
> >
> > .
> >
>
> —
> You are receiving this because you modified the open/close state.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/255#issuecomment-657118616>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AABL4L77GVO5LTFPRK2TXCLR3C6HTANCNFSM4OV6DN7Q>
> .
>
",want come across rude something willing run support already stated free personal tech support like sat wrote hi really appreciate help please set call fix together work account personal best trainer consultant image partner pinpoint page antic wrote two going first clearly file path u likely run wo address warning matter inference work regardless given running commit code silence warning shortly cause confusion though reply directly view state reply directly view,issue,positive,positive,positive,positive,positive,positive
657118616,"Hi Jason. I really appreciate the help. Can you please set up a Teams call
so that we can fix this together?

work account: matthew.Bennett@platformhg.com
personal:  mattybennett@outlook.com

Best regards,

Matthew
*Matthew Bennett*
*Freelance Trainer and IT Consultant*
*Mbe: **07871 480008*

<https://www.facebook.com/mattyjbennett>
<https://www.linkedin.com/in/matthew-bennett-a0384a11/>
<https://www.youtube.com/channel/UCwi6J8wbTkNQ4-wJpPS70Ug>
*[image: Microsoft Partner - Pinpoint page]*
* <http://www.fsb.org.uk/>*


On Thu, 9 Jul 2020 at 22:27, Jason Antic <notifications@github.com> wrote:

> @mbittraining <https://github.com/mbittraining> There's two things going
> on here. First that's clearly a Windows file path. Windows isn't supported
> by us and you'll likely run into issues that we won't address here.
>
> Second- that warning doesn't matter in terms of inference (just colorizing
> images). It'll work regardless (given you don't have other issues running
> it on Windows).
>
> I'll commit code to silence the warning shortly just so it doesn't cause
> further confusion though.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/255#issuecomment-656359617>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF4V7MU5G22HTBL4QZU5UDLR2YY3DANCNFSM4OV6DN7Q>
> .
>
",hi really appreciate help please set call fix together work account personal best trainer consultant image partner pinpoint page antic wrote two going first clearly file path u likely run wo address warning matter inference work regardless given running commit code silence warning shortly cause confusion though reply directly view,issue,positive,positive,positive,positive,positive,positive
656572426,"@leeqiaogithub it's filtering the folders with the classes variable.  So what the critic winds up getting is two classes to classify, as it's pretrained as a binary classifier.  It's a bit hacky now that I look at it again after almost two years.

 data_crit = get_crit_data([name_gen, 'test'], bs=bs, sz=sz).",filtering class variable critic getting two class binary classifier bit hacky look almost two,issue,negative,neutral,neutral,neutral,neutral,neutral
656433545,"'''
path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')
def get_crit_data(classes, bs, sz, pct=1.0):
    src = ImageList.from_folder(path, include=classes, recurse=True)...
'''
but according to the source code of ImageList.from_folder(), it seems all images in CLS-LOS(including train/val/test) is used.",path path class path according source code used,issue,negative,neutral,neutral,neutral,neutral,neutral
656370567,"@HpZhao607 Sorry my first response was incomplete.  

You need to add the models directory per the readme.  There's instructions there to download the models from dropbox and add to this folder.

THEN if you're still running into issues do this:

Open a terminal in PycharmProjects and add read/write permissions.

chmod -R 666 DeOldify

This will recursively add read/write permissions and you should be good to go. I'm a bit lazy so I'll just give you something pretty permissive here but if that worries you can adjust the ""666"" part accordingly.",sorry first response incomplete need add directory per add folder still running open terminal add add good go bit lazy give something pretty permissive adjust part accordingly,issue,positive,positive,neutral,neutral,positive,positive
656366245,"Yeah our stance is: Quick and easy:  Do Colab (urls only).  More involved but industrial strength:  Use Ubuntu install, where you can do longer videos and local videos, and not run into trouble with YouTube not accepting something, etc.",yeah stance quick easy involved industrial strength use install longer local run trouble something,issue,positive,positive,positive,positive,positive,positive
656365327,"@leeqiaogithub You only need a set of ""real"" images and generated images to train the critic.  You can go about this many ways but the way I went about it was to take advantage of the test set in imagenet that I otherwise wasn't using.  It was the right size for my purposes and I wanted something to train on that was separate from the GAN training portion for better generalization.",need set real train critic go many way way went take advantage test set otherwise right size something train separate gan training portion better generalization,issue,negative,positive,positive,positive,positive,positive
656364346,"@leeqiaogithub They're trained at different sizes: Once at 128px, and potentially multiple times at 192px.  So that's the first thing to be clear on.  The second thing is you can go through the cycle starting at the heading ""Repeatable GAN Cycle"" more than once- each time you should be incrementing old_checkpoint_num by 1 manually. Upon repetition, the Critic is finetuned further on distinguishing generated images from real images without the use of GAN training simply because it allows for the critic to get really good while avoiding the collateral damage of training as a GAN.  That's the whole point of NoGAN- minimize the GAN part as much as possible.",trained different size potentially multiple time first thing clear second thing go cycle starting heading repeatable gan cycle time manually upon repetition critic distinguishing real without use gan training simply critic get really good collateral damage training gan whole point minimize gan part much possible,issue,negative,positive,positive,positive,positive,positive
656359617,"@mbittraining There's two things going on here.  First that's clearly a Windows file path.  Windows isn't supported by us and you'll likely run into issues that we won't address here.

Second- that warning doesn't matter in terms of inference (just colorizing images).  It'll work regardless (given you don't have other issues running it on Windows).

I'll commit code to silence the warning shortly just so it doesn't cause further confusion though.",two going first clearly file path u likely run wo address warning matter inference work regardless given running commit code silence warning shortly cause confusion though,issue,negative,positive,neutral,neutral,positive,positive
655970527,I want to know why we do not use images in train and val folder.Is it because of the large amount of data too large?,want know use train large amount data large,issue,negative,positive,positive,positive,positive,positive
653801827,After editing Favourite movie bits not accepted youtube . So my favourite movie bits colorize my own Ubuntu system. Without youtube or any other. ,movie accepted movie colorize system without,issue,negative,neutral,neutral,neutral,neutral,neutral
653560817,Thank you for your reply! I believe you're correct. The input image is indeed first downsampled by a factor of 16 using resnet34 . Then it seems that the activations of the modules responsible for the downsampling are stored a hook. Then the code use F.interpolates to upsample the activations according to the shapes of the stored activations reversely. ,thank reply believe correct input image indeed first factor responsible hook code use according reversely,issue,positive,positive,positive,positive,positive,positive
653326991,"In my opinion, the image was downsampled 16x during the encoder part, and then upsampled to the input size, so the render_base was set to 16. and any resized image which is a multiple of 16 would be OK for this network.",opinion image part input size set image multiple would network,issue,negative,neutral,neutral,neutral,neutral,neutral
653214311,One simple solution is to upload your video on Youtube and then take the url. It should be quick if your video is not several hours long.,one simple solution video take quick video several long,issue,negative,positive,neutral,neutral,positive,positive
649710945,"I am having the same issue, started happening a few days ago, after last 3 weeks had done a lot of videos, more then 50 and no one have this problem, now all videos when completed ok have this length problem and but is more comum now a error and not complete, he show a error of limit of buffer length. Another issue I notice start happening, is...before last week, you dont need reset the virtual machine to do another video, now, every time you need to do this, otherwise, he dont take the new video....I think he not deleting old data, and start this issue this week too. last 3 weeks before this, how I saied, converted more of 50 videos and none of this 2 problems. Can be google colab, but I see, but I think is some of the changes you have done last 5 days or something like this. Here is the buffer error

[youtube] NwSwMm-Tbxg: Downloading webpage
[youtube] NwSwMm-Tbxg: Downloading MPD manifest
[dashsegments] Total fragments: 226
[download] Destination: video/source/video.f135.mp4
[download] 100% of 73.34MiB in 00:52
[dashsegments] Total fragments: 122
[download] Destination: video/source/video.mp4.f140
[download] 100% of 18.54MiB in 00:12
[ffmpeg] Merging formats into ""video/source/video.mp4""
Deleting original file video/source/video.f135.mp4 (pass -k to keep)
Deleting original file video/source/video.mp4.f140 (pass -k to keep)

91.70% [33012/36000 1:33:59<08:30]

Buffered data was truncated after reaching the output size limit.",issue happening day ago last done lot one problem length problem error complete show error limit buffer length another issue notice start happening last week dont need reset virtual machine another video every time need otherwise dont take new video think old data start issue week last converted none see think done last day something like buffer error manifest total destination total destination original file pas keep original file pas keep data truncated reaching output size limit,issue,negative,positive,neutral,neutral,positive,positive
648699559,"@jantic yes I did. I'm using the colab now, and @kirushyk is helping me set up a remote server. We can close this ticket, thanks",yes helping set remote server close ticket thanks,issue,positive,positive,neutral,neutral,positive,positive
647874956,"Thanks @Tortoise17 for all the help.

@andycloke did you install nvidia-docke per the readme?  https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0)#installing-version-20",thanks tortoise help install per,issue,positive,positive,positive,positive,positive,positive
646621910,"```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Wed_Oct_23_19:24:38_PDT_2019
Cuda compilation tools, release 10.2, V10.2.89
```",compiler driver copyright corporation built compilation release,issue,negative,neutral,neutral,neutral,neutral,neutral
646617347,"nvcc --version

can you confirm cuda version? 10.2 ? if not. than mismatch. 
mismatch and dropdown to 9.2 and see if that is really picking your environments. ",version confirm version mismatch mismatch see really,issue,negative,positive,positive,positive,positive,positive
646616258,Still not working with pytorch 1.0.1 and condo 10.2. Must be missing something else.,still working must missing something else,issue,negative,negative,negative,negative,negative,negative
646609335,Updated to cuda 10.2 and still no luck. Will try pytorch now. Thanks,still luck try thanks,issue,positive,positive,positive,positive,positive,positive
646602474,"I think it assumes you already have pytorch and updates it only. But see 
https://pytorch.org/

maybe it is already expecting your cuda 10.2?

update cuda and run. It should work. ",think already see maybe already update run work,issue,negative,neutral,neutral,neutral,neutral,neutral
646597361,"I got `No matching distribution found for tensorflow-gpu==1.15`, so installed 1.14 instead.

Do I actually need to install `pytorch` myself, won't docker handle that? https://github.com/jantic/DeOldify/blob/master/Dockerfile-api#L17
",got matching distribution found instead actually need install wo docker handle,issue,negative,neutral,neutral,neutral,neutral,neutral
646596372,"But, here could be that pytorch is mismatching. sorry. I am jumping back and forth in different tools. 

see which pytorch version you have. // and which cuda is compatible with?? or if you tell me torch version, I can tell you. 

You can check this with 
pip list 
or 
pip3 list",could sorry back forth different see version compatible tell torch version tell check pip list pip list,issue,negative,negative,negative,negative,negative,negative
646595501,"1.15 should work
pip3 install tensorflow-gpu==1.14
pip3 install tensorflow-gpu==1.15

Try 1.15 first. 
yes, if the versions or any other thing which is mismatching, than, maybe cuda 10.2 or 10.0 is the solution for definitely. Because this is the cuda mismatch. ",work pip install pip install try first yes thing maybe solution definitely mismatch,issue,positive,positive,positive,positive,positive,positive
646594134,"Thanks, installing TF now. I'll try cuda too if this does not fix it.

Is this correct? `pip3 install tensorflow-gpu==1.14`  ?",thanks try fix correct pip install,issue,negative,positive,positive,positive,positive,positive
646592266,Could be either you raise your cuda version to 10.2 could help. ,could either raise version could help,issue,negative,neutral,neutral,neutral,neutral,neutral
646589074,"python3 -c 'import tensorflow as tf; print(tf.__version__)'

this commands shows your version. Because above 1.15 TF is not compatible with cuda 10.1 ",python print version compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
646588512,"Thanks for the quick response! 

I didn't install TF as I assume docker handled it but guessing that's wrong. Will install it now.",thanks quick response install assume docker handled guessing wrong install,issue,negative,positive,neutral,neutral,positive,positive
646588274,"First see TF gpu is available if YES, which version ",first see available yes version,issue,negative,positive,positive,positive,positive,positive
646587480,could be wrong tensorflow version. !! see your TF version?,could wrong version see version,issue,negative,negative,negative,negative,negative,negative
644460441,"@jon0x0  So the models directory behavior is a fastai thing, and what they do is default to the base directory being wherever the training/val images folder is.  By default that's defined in the notebooks like this:

path = Path('data/imagenet/ILSVRC/Data/CLS-LOC')

So in there (that's relative to where you're running the notebook). That is to say, it's going to try to read/write to a models folder here:

./data/imagenet/ILSVRC/Data/CLS-LOC/models

So if you don't have the Imagenet training data downloaded and put in the proper place there, I'd first do that.  

Then you'd want to make sure there isn't a permissions issue (often the case with Linux).  I don't get overly paranoid on my computer as it's not really exposed so I just go for the 777 option on the directory.

[How to make a directory permanently writable?](https://askubuntu.com/questions/265381/how-to-make-a-directory-permanently-writable)

That really should be what you need to look at to solve your problem as far as I can tell.
",directory behavior thing default base directory wherever folder default defined like path path relative running notebook say going try folder training data put proper place first want make sure issue often case get overly paranoid computer really exposed go option directory make directory permanently writable really need look solve problem far tell,issue,negative,positive,neutral,neutral,positive,positive
643904248,"So this 'can't write to models' issue is a problem, trying to set up DeOldify on a new Linux installation and is what I am getting also. It's not clear to me which 'models' dir this refers to, and I have tried making them all writable to no avail.  Can you please help with a more indepth example. Thank you
",write issue problem trying set new installation getting also clear tried making writable avail please help example thank,issue,positive,positive,positive,positive,positive,positive
641153422,"I have had this issue too and understand why it happens but in this instance I would suggest two things that (in my opinion) can slightly improve the result:

1) Adjust the levels in the source image first
2) Experiment with a range of different 'render factor' settings 

![download (3)](https://user-images.githubusercontent.com/2345732/84129821-5e96fd00-aa3a-11ea-96e2-fa00682bb510.png)

By adjusting the levels first and picking a lower 'render factor' the result is still objectively coloured 'wrong' but I feel like it moves a bit more in the right direction and has a slightly more lifelike feel. ",issue understand instance would suggest two opinion slightly improve result adjust source image first experiment range different factor first lower factor result still objectively feel like bit right direction slightly lifelike feel,issue,negative,positive,positive,positive,positive,positive
641112328,"Oh, my bad, I forgot to set the bs to smaller. Problem solved.",oh bad forgot set smaller problem,issue,negative,negative,negative,negative,negative,negative
640272794,"Yeah...no... That's not how to persuade me to do things.  

Here, read a book instead:  [How to Win Friends and Influence People](https://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034)",yeah persuade read book instead win influence people,issue,positive,positive,positive,positive,positive,positive
636703352,"It could be at least Rick Astley video, disappointing.",could least rick video disappointing,issue,negative,negative,negative,negative,negative,negative
636218009,"If you have the prerequisite background in deep learning (fastai would be ideal), then what's in the [readme](https://github.com/jantic/DeOldify/blob/master/README.md) along with the Jupyter notebooks should suffice.  That being said, training NoGAN is particularly challenging even with a solid background (it's tricky to know when to stop, namely).

If you don't have that background I can't really help you and you won't get too far until something goes wrong. It's not so much of a ""follow this recipe"" sort of thing unfortunately.",prerequisite background deep learning would ideal along suffice said training particularly even solid background tricky know stop namely background ca really help wo get far something go wrong much follow recipe sort thing unfortunately,issue,negative,positive,neutral,neutral,positive,positive
634647727,"I'm sorry, I'm just in the learning stage, for my very interesting things, I have some uncertain operation, to bring you trouble please forgive me! ! !&nbsp;




------------------ 原始邮件 ------------------
发件人: &nbsp;""Jason Antic""<notifications@github.com&gt;;
发送时间: &nbsp;2020年5月27日(星期三)中午1:50
收件人: &nbsp;""jantic/DeOldify""<DeOldify@noreply.github.com&gt;;
抄送: &nbsp;""Subscribed""<subscribed@noreply.github.com&gt;;
主题: &nbsp;Re: [jantic/DeOldify] Pic tool (#241)





 
@nijinjose Can you add a description? I really don't know what you mean here....
  
— 
You are receiving this because you are subscribed to this thread. 
Reply to this email directly, view it on GitHub , or unsubscribe .",sorry learning stage interesting uncertain operation bring trouble please forgive antic pic tool add description really know mean thread reply directly view,issue,negative,negative,neutral,neutral,negative,negative
634443367,@nijinjose Can you add a description? I really don't know what you mean here....,add description really know mean,issue,negative,negative,neutral,neutral,negative,negative
633248249,"I I added the code to solve the problem  in app.py
![image](https://user-images.githubusercontent.com/5779513/82758031-a6047300-9e16-11ea-800e-6a96c0e8add3.png)
",added code solve problem image,issue,negative,neutral,neutral,neutral,neutral,neutral
633244899,"but I found  in  jupyter lab  GPU is work

![image](https://user-images.githubusercontent.com/5779513/82757459-32ad3200-9e13-11ea-87b8-4213737bfe7a.png)
![image](https://user-images.githubusercontent.com/5779513/82757482-4d7fa680-9e13-11ea-987f-3d6cc2beac3d.png)

why  python3 app.py service cannot use GPU?
",found lab work image image python service use,issue,negative,neutral,neutral,neutral,neutral,neutral
633218643,"![image](https://user-images.githubusercontent.com/5779513/82753239-c3c1e000-9df6-11ea-9fed-c984e90874bd.png)
Yes, I don't understand why I didn't use GPU. Please help me",image yes understand use please help,issue,positive,neutral,neutral,neutral,neutral,neutral
632903563,"Indeed, I find this phenomena as well. A typical example is Audrey Hepburn's cream color dress in Roman Holiday:

|  Original B&W | Colorized |
|---------------|-----------|
|![hepburn_rh](https://user-images.githubusercontent.com/3814842/82707970-740bd900-9c7d-11ea-892d-1383f834d0c2.jpg) | ![hepburn_rh_hd](https://user-images.githubusercontent.com/3814842/82707961-6eae8e80-9c7d-11ea-9ba9-2d3eac211893.jpg) |

The ground truth is:
![il_570xN 1121898953_8uyn](https://user-images.githubusercontent.com/3814842/82707981-7bcb7d80-9c7d-11ea-85a1-5e9f82550248.jpg)

Just bring it up for further optimization.

I can not compliment your work any more, this is a sensational application that will reprint the historical footages of the world! ",indeed find phenomenon well typical example cream color dress holiday original ground truth bring optimization compliment work sensational application reprint historical world,issue,positive,positive,positive,positive,positive,positive
632539054,"@jantic Thank you for the response! The AI is clearly getting clever, learning how to cheat the test.

I wish I'd have the knowledge and skills to help. Best of luck!",thank response ai clearly getting clever learning cheat test wish knowledge help best luck,issue,positive,positive,positive,positive,positive,positive
632369199,"Did you do the conda install and run app.py on that conda environment? And do you have a fairly recent NVidia GPU? The code will automatically revert to using the CPU if the prerequisites (like CUDA) aren't detected.

Also note that only Linux is supported.",install run environment fairly recent code automatically revert like also note,issue,negative,neutral,neutral,neutral,neutral,neutral
632367571,"If you want just simply DataParallel, Generator pretraining should be a simple conversion (haven't tried it lately, so be forewarned). For ColorizeTrainingArtistic that would be adding this code after getting the model

`model=torch.nn.DataParallel(model)`

In [here](https://github.com/jantic/DeOldify/blob/c6f28b9557cba1d38b580ae2bbfef2845103a7b3/deoldify/generators.py#L110)

But for the GAN part- that's just not easy and I haven't bothered trying to coerce it into working. [This ](https://towardsdatascience.com/how-to-train-a-gan-on-128-gpus-using-pytorch-9a5b27a52c73)looks like a good reference.

Now why haven't I bothered?  I think that's worth talking about.  First, if I'm going to do multi-gpu training, I do [distributed dataparallel training](https://docs.fast.ai/distributed.html) with SyncBatchNorm replacing the batchnorm layers. And I do this with my current work that isn't released to the public because I can get bigger batch sizes at bigger image sizes, but also training it in such a way that it's numerically the same as if they were on a single big gpu.  The key here in my mind is getting batch norm statistics combined- much better results that way.

But I can tell you this much- getting this to work for a generator is one thing, but a GAN is again more difficult. I won't be putting that work out as open source because it's a lot of extra work and only a few people will benefit from it.  Not to mention, it's a bit of a competitive advantage for us.",want simply generator pretraining simple conversion tried lately would code getting model model gan easy trying coerce working like good reference think worth talking first going training distributed training current work public get bigger batch size bigger image size also training way numerically single big key mind getting batch norm statistic much better way tell getting work generator one thing gan difficult wo work open source lot extra work people benefit mention bit competitive advantage u,issue,positive,positive,neutral,neutral,positive,positive
632361305,"@karrirasinmaki Oh yeah, this set of color issues is the bane of my existence.  It's slowly getting better with the newer models, but it's a difficult issue. The issue with blue/purple bias really boils down to this: Things like clothing and cars can be pretty much any color, so the model takes a bet on blue/purple because on average they're not bad bets.  It's basically the model cheating the exam. 

I'll keep this open just to let people know it's known, but I can tell you this much:  It'll probably take quite a long time to completely get rid of this problem.",oh yeah set color bane existence slowly getting better difficult issue issue bias really like clothing pretty much color model bet average bad basically model cheating exam keep open let people know known tell much probably take quite long time completely get rid problem,issue,negative,negative,neutral,neutral,negative,negative
632357606,That's really quite odd.... first time I've heard of this sort of thing being reported in a long time.  Can you provide the YouTube video link to reproduce this?,really quite odd first time sort thing long time provide video link reproduce,issue,negative,positive,neutral,neutral,positive,positive
631941681,"I wasn't sure if this was the right place to bring up these issues, but I thought might be useful.",sure right place bring thought might useful,issue,positive,positive,positive,positive,positive,positive
631511388,"I totally understand and I agree, @jantic. Thanks for taking the time and explain it!",totally understand agree thanks taking time explain,issue,positive,positive,neutral,neutral,positive,positive
630401307,"@nahuelhds Hey! It's possible, for sure, but it's not my call.  I don't have access to the code on that site- it has my name on it but I didn't have anything to do with it other than that they're using DeOldify. I did get asked by them if I thought anything should change about the UI and initially I was going to suggest render_factor, but two things:  

1. It would add an undesired level of complexity for the target audience (it's supposed to be super simple compared to the Colabs).
2. It would add a bunch of extra model hits on the server that weren't happening before per user, greatly adding to the expense of running it (it's free).

Since then I've though of the Colabs as the way to achieve the ""power user"" mode, and now you can add MyHeritage In Color to that because they actually did add render_factor to their UI.  They did it in a good way- they hid it as an advanced setting.

So in summary- I don't think I'll be advocating for this change at DeepAI.",hey possible sure call access code name anything get thought anything change initially going suggest two would add undesired level complexity target audience supposed super simple would add bunch extra model server happening per user greatly expense running free since though way achieve power user mode add color actually add good advanced setting think change,issue,positive,positive,positive,positive,positive,positive
629733196,"Final video generator model is [here.](https://www.dropbox.com/s/336vn9y4qwyg9yz/ColorizeVideo_gen.pth?dl=0)

The weights for the video generator model just as it was finished pretraining and just before doing the GAN training phase are [here.](https://www.dropbox.com/s/avzixh1ujf86e8x/ColorizeVideo_PretrainOnly_gen.pth?dl=0)

You can't use the critic or crit models for video generation.  That model just ""criticizes"" the video generator during training.  
",final video generator model video generator model finished pretraining gan training phase ca use critic video generation model video generator training,issue,negative,neutral,neutral,neutral,neutral,neutral
629732610,"There's trouble shooting text in the Colab that directs you where to look. The only thing that has changed since that was written is that the ""Files"" text became a folder icon.  Looks like this (highlighted in orange there).  Note that I haven't ran this Colab but that's where you'd look for the video once you're done running.  You might  have to hit refresh there too.

![image](https://user-images.githubusercontent.com/179759/82134196-16255000-97aa-11ea-94b3-c3b00d451cdd.png)

Instructions I'm referring to:

![image](https://user-images.githubusercontent.com/179759/82134189-086fca80-97aa-11ea-8eb4-03898e8fd9c1.png)
",trouble shooting text look thing since written text folder icon like orange note ran look video done running might hit refresh image image,issue,negative,negative,negative,negative,negative,negative
629685846,It seems like the final videomodel_gen_0.pth is the final ColorizeVideo_gen.pth . Am i right?,like final final right,issue,negative,positive,neutral,neutral,positive,positive
629666164,"Yes that was the link. Thank you very much. Greatly appreciated.
Have a nice day.",yes link thank much greatly nice day,issue,positive,positive,positive,positive,positive,positive
628307291,"Well unfortunately Windows isn't officially supported by DeOldify, in order to avoid supporting numerous extra issues unique issues that pop up there because Windows just doesn't get as good support as Linux with the libraries.

That being said, I personally won't do a detailed instruction guide because that'll inevitably make for a lot more reported issues that I just don't want to deal with. But I can tell you I regularly run inference doing the same install on Windows 10 as I've done on Linux, as described in the readme.  I believe you that you're having problems but actually digging into what you did differently just isn't something I'm willing to spend time on.  

It's disappointing for people to hear this for sure, but this is me sharing research code so it's not really tailored for non programmers.  It's not meant to be for everyday users really.  That's what other services (like deepai) and MyHeritage are for.

If you really want to press ahead- the path of least resistance is to do a dual boot of Ubuntu 18.04 and follow the installation instructions from there.  That might sound off putting but it's where you'll have much more likelihood of success.",well unfortunately officially order avoid supporting numerous extra unique pop get good support said personally wo detailed instruction guide inevitably make lot want deal tell regularly run inference install done believe actually digging differently something willing spend time disappointing people hear sure research code really non meant everyday really like really want press path least resistance dual boot follow installation might sound much likelihood success,issue,positive,positive,positive,positive,positive,positive
628305001,"The one I got was from [here at Kaggle](https://www.kaggle.com/c/imagenet-object-localization-challenge)

You have to ""join"" the competition but it doesn't obligate you do anything.",one got join competition obligate anything,issue,negative,neutral,neutral,neutral,neutral,neutral
628014829,Thanks so much @jantic - that's really really helpful!,thanks much really really helpful,issue,positive,positive,positive,positive,positive,positive
627571156,"Honestly i did your first recommendation (download and rename) but i got error ""StopIteration"" at the next step. After i run the command you added (also re-downloading and renaming the file) it created bandw folder and all its contents, everything goes fine.

Thank you very much for your precious help!
",honestly first recommendation rename got error next step run command added also file folder content everything go fine thank much precious help,issue,positive,positive,positive,positive,positive,positive
627550233,"Well keep in mind you're largely on your own here once you start trying to do stuff that it's outside the scope of the project.  And in this case it's running the code as a .py file in command line, and on Windows.

But it seems like the issue here might simply be that you have gpu memory being taken up by the notebook that you previously ran still, and that would be resolvable by shutting down the notebook. You can verify gpu memory by looking at Windows task manager's ""Dedicated GPU Memory"" for the GPU. It should be at or near 0 before you start running the code.",well keep mind largely start trying stuff outside scope project case running code file command line like issue might simply memory taken notebook previously ran still would resolvable shutting notebook verify memory looking task manager memory near start running code,issue,positive,positive,neutral,neutral,positive,positive
627548066,"So basically this is possibly in the cards, but it'd be a paid product and it'll take quite a while to get there at this point.  There's still a lot of research to be done. That is to say- when the time comes the ""donation"" will be simply to pay for the app!

I'm a big believer in taking time to do things right and seeing things through, but unfortunately for everybody else involved that means things go slooooow and seem to take forever. So hopefully you guys don't lose interest before then :P",basically possibly product take quite get point still lot research done time come donation simply pay big believer taking time right seeing unfortunately everybody else involved go seem take forever hopefully lose interest,issue,negative,negative,neutral,neutral,negative,negative
627062216,"Thank you very much, looking forward to your latest results",thank much looking forward latest,issue,negative,positive,positive,positive,positive,positive
626976904,"This notebook by default assumes that you had done training using ColorizeTrainingStable, and then renamed the resuling model to VideoModel_gen_0.pth.  If you're going directly to video training, you'll need to download the [pretrained stable model](https://www.dropbox.com/s/mdnuo1563bb8nh4/ColorizeStable_PretrainOnly_gen.pth?dl=0) and rename it and put it in the specified path.  That path, 'data\bandw\models\VideoModel_gen_0.pth,  is relative to the root DeOldify folder that contains the notebooks. And yes you'll want to extract  ILSVRC2017_CLS-LOC.tar.gz and use that to train (which I further elaborate on [here](https://github.com/jantic/DeOldify/issues/228)).  

Let me save you time- training simply won't work on Windows. I mean...it'll run, but super super slowly.  Windows is not supported.",notebook default done training model going directly video training need stable model rename put path path relative root folder yes want extract use train elaborate let save training simply wo work mean run super super slowly,issue,positive,positive,neutral,neutral,positive,positive
626973745,"Thanks.  Yeah unfortunately for now I don't have a solution for you.  Even in my latest models I still run into this.  I'll keep this issue open to document it for now (will rename it to be more specific).  

The best workaround I can give you (and it's not much of one) is to render with higher render_factors.  ",thanks yeah unfortunately solution even latest still run keep issue open document rename specific best give much one render higher,issue,positive,positive,positive,positive,positive,positive
626969993,"It turns out I omitted the step from ColorizeTrainingStable that generates the black and white training images from ImageNet:

![image](https://user-images.githubusercontent.com/179759/81612442-abcb7480-9391-11ea-9aa6-fba05a10e4bf.png)

I think the intention I had was that you would run the ColorizeTrainingStable.ipynb notebook first, as it says this on top of the video training notebook:

> It's assumed that there's a pretrained generator from the ColorizeTrainingStable notebook available at the specified path.

So if you'd had run ColorizeTrainingStable first you would already have the b&w images generated.  If not, that's probably your problem here. Anyway, I'll go ahead and add that step too ColorizeVideoTraining notebook to allow for the possibility that you'd want to jump straight to video training with the pretrained stable model I've uploaded already. This will be committed shortly.

Here's what example Imagenet data paths should look like, starting at the root DeOldify project folder. You may or may  not have the exact images I show here but the point is to just illustrate structure.  

training image x:  /data/imagenet/ILSVRC/Data/CLS-LOC/bandw/train/n01440764/n01440764_18.JPEG
training image y:  /data/imagenet/ILSVRC/Data/CLS-LOC/train/n01440764/n01440764_18.JPEG

validation image x:  /data/imagenet/ILSVRC/Data/CLS-LOC/bandw/val/n01440764/n01440764_18.JPEG
validation image y:  /data/imagenet/ILSVRC/Data/CLS-LOC/val/n01440764/n01440764_18.JPEG",turn step black white training image think intention would run notebook first top video training notebook assumed generator notebook available path run first would already probably problem anyway go ahead add step notebook allow possibility want jump straight video training stable model already shortly example data look like starting root project folder may may exact show point illustrate structure training image training image validation image validation image,issue,positive,positive,positive,positive,positive,positive
626957947,"This seems a bit one sided.  Just think about it- why exactly do I owe this to you...?

I'm just sharing research.  You didn't pay for it, nor am I getting paid to play customer service.  If you're not willing to read and are that ready to waste others' time, I'd consider that quite rude.",bit one sided think exactly owe research pay getting play customer service willing read ready waste time consider quite rude,issue,negative,positive,neutral,neutral,positive,positive
626955243,Is a waste of time for everyone using your project designing this so it only works for you and on top of it being very rude about it. ,waste time everyone project designing work top rude,issue,negative,neutral,neutral,neutral,neutral,neutral
626954247,"That's totally intentional.  The function of that is to serve as a preview of the frames at different render_factors. You're running out of memory with it because it goes up to a render_factor that your card can't handle.  That's a 4GB card.  That's not a lot to work with.

Please read the instructions next time before reporting a bug. It's a big waste of time not to for everybody involved. There's a reason I wrote this in the notebook:

> If a video you downloaded doesn't play, it's probably because the cell didn't complete processing and the video is in a half-finished state. If you get a 'CUDA out of memory' error, you probably have the render_factor too high. **The max is 44 on 11GB video cards.**

And this:

> If a video takes a long time to render and you're wondering how well the frames will actually be colorized, you can preview how well the frames will be rendered at each render_factor by using the code at the bottom. Just stop the video rendering by hitting the stop button on the cell, then run that bottom cell under ""See how well render_factor values perform on a frame here"". It's not perfect and you may still need to experiment a bit especially when it comes to figuring out how to reduce frame inconsistency. But it'll go a long way in narrowing down what actually works.

And even this title above the render_factor preview code you're citing:

> See how well render_factor values perform on a frame here

Closing.  ",totally intentional function serve preview different running memory go card ca handle card lot work please read next time bug big waste time everybody involved reason wrote notebook video play probably cell complete video state get memory error probably high video video long time render wondering well actually preview well code bottom stop video rendering stop button cell run bottom cell see well perform frame perfect may still need experiment bit especially come reduce frame inconsistency go long way actually work even title preview code see well perform frame,issue,positive,positive,neutral,neutral,positive,positive
626544296,Can you try uploading your image there again?  It's not showing up correctly.,try image showing correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
626031492,@yunnian Awesome.  Glad to help and thanks for the followup.,awesome glad help thanks,issue,positive,positive,positive,positive,positive,positive
625720573,"Thank you very much for solving my problem. It's true. I built a docker on the VPN machine and exported it for network reasons, Then loaded it on the GPU machine. But because the GPU machine can't climb over the wall, it can't run. Now I put this file in the docker models directory and it run  successfully!  ",thank much problem true built docker machine network loaded machine machine ca climb wall ca run put file docker directory run successfully,issue,positive,positive,positive,positive,positive,positive
625389090,"Hi again Jantic and thank you for your response.
I was just wondering about where I can download the imagenet dataset file.
Its worth noting I did ultimatelly convert to Linux as Windows wasnt the best OS for this.
I did download it a while ago but have been having some issues which resulted me in starting over, and forgot where I downloaded it.",hi thank response wondering file worth convert wasnt best o ago starting forgot,issue,negative,positive,positive,positive,positive,positive
624893793,"It looks like the artistic model isn't being downloaded successfully from Dropbox  [(link)](https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0) ).  That would indicate I think a network issue on your end.  It'd be worth trying to download from that link I just provided and see what happens.  

Like try this in the terminal:

`wget https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl`

And see how big the file winds up being. If it's 0 kb then you definitely know it's not downloading correctly of course, and it's probably a local network issue because it works fine elsewhere.  It could have also been a temporary issue and it might ""resolve itself"" if you try again now.",like artistic model successfully link would indicate think network issue end worth trying link provided see like try terminal see big file definitely know correctly course probably local network issue work fine elsewhere could also temporary issue might resolve try,issue,positive,positive,positive,positive,positive,positive
624545650,"> If your trying just to get it working here is it
> https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb
> 
> So here is a tip just abuse https://myheritage.co.il/incolor/ function
> make sure to upload a picture to there and then let it ask you to sign in and sign in from the incolor page eitherwise its hard to sign in

I want to use docker to run stably. I have my own GPU machine，Want it  to run stably as a service.",trying get working tip abuse function make sure picture let ask sign sign page hard sign want use docker run stably run stably service,issue,negative,positive,positive,positive,positive,positive
624360658,"@madhavasai  I'm unclear on what you're trying to accomplish here.  The models with the suffix ""_crit"" are strictly pretrained ""critic"" models used in training and they do not produce video output. Rather, they're used in training to ""criticize"" the images generated by the ""generator"" in training and improve it.  So you can't use them to generate video. 

ColorizeVideo_gen.pth is the only practically usable set of model weights for video.  ColorizeVideo_PretrainOnly_gen.pth are just the model weights snapshot just before GAN training was initiated.  Video can technically be produced by it but it'll look awful (very dull).

If you really want to use those weights, just search and replace the code ""ColorizeVideo_gen"" with ""ColorizeVideo_PretrainOnly_gen"", provided you have the source code running on a Linux box. If you're running on a Colab only you can simply swap out the Dropbox link download for the one provided in the readme for the PretrainOnly one.

But again, I don't recommend it. The results will simply be more disappointing.

> I am facing one more issue is I observed after converting to colourization videos black/blue shades are there on the body parts (faces, legs, hands). Please could you provide any solution for this issue.

As far as the issue you cite there- the only potential solution available to you is to play around with the render_factor value and see what renders best.  There's functionality in the notebook and Colab to preview a specific frame that you specify for a range of render_factors- generally a single image is a good indicator of overall video performance.  

Failing that, basically you're at the mercy of the fact that this is an older model with limitations! ",unclear trying accomplish suffix strictly critic used training produce video output rather used training criticize generator training improve ca use generate video practically usable set model video model snapshot gan training video technically produced look awful dull really want use search replace code provided source code running box running simply swap link one provided one recommend simply disappointing facing one issue converting body please could provide solution issue far issue cite potential solution available play around value see best functionality notebook preview specific frame specify range generally single image good indicator overall video performance failing basically mercy fact older model,issue,positive,positive,neutral,neutral,positive,positive
623766216,"@q5sys - I've actually even gone a step further and done transfer learning going from colorization to superresolution and defade models.  I've used both the pretrained generator and critic in this manner- it works! So in my mind it follows that you should be able to do this quite naturally. 

But as far as the data is concerned- I can tell you this much: It seems I got a big boost in ability to deal with buildings generally with incorporating a subset of the open images dataset (didn't use this for this open source version). So that may be another source to consider in addition to what you have, depending on just how much data you have.",actually even gone step done transfer learning going colorization used generator critic work mind able quite naturally far data tell much got big boost ability deal generally subset open use open source version may another source consider addition depending much data,issue,negative,positive,positive,positive,positive,positive
620282168,"Hi @srinivasbheesetty !  Great work on that channel.

So what you describe there is probably the direction we're going with video if we tackle video again (a big question currently). But that won't be for a while and it would be a paid product for sure (will require lots of work).  

The next best thing in the meantime is to basically achieve the same thing with some existing tools.  Garrett Gilchrist has a great workflow that he describes it in [this YouTube series.](https://www.youtube.com/playlist?list=PLj35L0AEP6qk467T8YDDiewGTcmsMg4vc).",hi great work channel describe probably direction going video tackle video big question currently wo would product sure require lot work next best thing basically achieve thing great series,issue,positive,positive,positive,positive,positive,positive
620277416,"@AshleyRudland Thanks!

So on production inference:  That's a really tricky question.  Basically I looked into it a while ago and your options get really limited (and expensive) when doing big models like this. Last time I looked doing [elastic inference on Amazon's sagemaker](https://docs.aws.amazon.com/sagemaker/latest/dg/ei.html) looked most appealing terms of cost and scaling, but getting PyTorch models ported to it looked like a huge challenge. But just 5 days ago, this came out:  [Introducing TorchServe: a PyTorch model serving framework](https://aws.amazon.com/about-aws/whats-new/2020/04/introducing-torchserve/) .  I haven't dug deep but that might be the answer to the difficulties I saw previously.

That all being said, what we wound up concluding is that we thought the problem was just to damn hard, risky, and expensive for the two of us bootstrapping a startup to take on.  So we opted to not do it, and instead focused on developing locally executed models on iOS (CoreML) and perhaps desktop.  Note that this required a lot of additional work on optimizing the models and coming up with pretty radically new designs. The thinking behind that is expanded upon in [this Tweet thread](https://twitter.com/citnaj/status/1248057682319134720)

What we wound up making a business out of is just focusing on the core model technology and outsourcing the difficulty of figuring out the production deployment to our licensee (a much bigger company that's well equipped for it).

So really if you want to ask about deployment of the open source model on a server, I don't really have satisfactory answers. The Dockerfile-api was created by @jqueguiner and they deployed that at his company, so he may be better able to help you out on questions regarding that.

In terms of running it on CPU mode- Did you see that I recently added [a toggle for CPU mode?](https://github.com/jantic/DeOldify/commit/e55c1abc86475a9a840ae2868a473e6e9779a393)  Do you have the most recent DeOldify? That being said- it's very slow to run it on CPU even with a 16 core ThreadRipper. Not sure that would be acceptable.",thanks production inference really tricky question basically ago get really limited expensive big like last time elastic inference appealing cost scaling getting ported like huge challenge day ago came model serving framework dug deep might answer saw previously said wound concluding thought problem damn hard risky expensive two u take instead locally executed perhaps note lot additional work coming pretty radically new thinking behind expanded upon tweet thread wound making business core model technology difficulty production deployment licensee much bigger company well really want ask deployment open source model server really satisfactory company may better able help regarding running see recently added toggle mode recent slow run even core sure would acceptable,issue,positive,positive,neutral,neutral,positive,positive
619403351,"It was Linux. I managed to run it natively without docker. As long as I know probably caffe2 is not compiled for my CPU in docker

On April 25, 2020 6:16:40 AM GMT+03:00, Jason Antic <notifications@github.com> wrote:
>I haven't been able to reproduce this issue, and the error codes aren't
>helping to narrow down the issue unfortunately. Are there any more logs
>to share, for example?  And this is Linux?
>
>-- 
>You are receiving this because you authored the thread.
>Reply to this email directly or view it on GitHub:
>https://github.com/jantic/DeOldify/issues/216#issuecomment-619311442

-- 
Sent from my Android device with K-9 Mail. Please excuse my brevity.",run natively without docker long know probably docker antic wrote able reproduce issue error helping narrow issue unfortunately share example thread reply directly view sent android device mail please excuse brevity,issue,negative,negative,neutral,neutral,negative,negative
619311442,"I haven't been able to reproduce this issue, and the error codes aren't helping to narrow down the issue unfortunately. Are there any more logs to share, for example?  And this is Linux?",able reproduce issue error helping narrow issue unfortunately share example,issue,negative,negative,neutral,neutral,negative,negative
619286500,"So I do like this idea in some sense but the thing is, it takes up a lot of extra space in GitHub with the images, and they have a strict limit on commits for that before you have to use GitLFS. Which costs an arm and a leg when you have a popular repo.  I learned that the hard way.  

So I'm not sure this is doable. If you mean just the training notebooks and not the Colabs then that would be much more doable I think. 

But also- there is the [W&B report page](https://app.wandb.ai/borisd13/DeOldify/reports/DeOldify--Vmlldzo0NDU3OA) and corresponding [training notebook]j(https://github.com/jantic/DeOldify/blob/master/ColorizeTrainingWandb.ipynb)- did you see these? 

I'll keep this issue open for now as I may be trying a new training run soon on better hardware, so that the notebook outputs can be generated (assuming that's what you're really looking for).


",like idea sense thing lot extra space strict limit use arm leg popular learned hard way sure doable mean training would much doable think report page corresponding training notebook see keep issue open may trying new training run soon better hardware notebook assuming really looking,issue,positive,positive,positive,positive,positive,positive
619283294,"@kirkgutierrdd I suppose you'll be our wealthy patron and pay for mine and Dana's bills then, so that we can give almost two years of work away for free and take care of some hefty hardware expenses? We'll take it! If that's not what you're suggesting, then I'm afraid we'll have to charge money for some of the work we do. And we're not going to apologize for that.",suppose wealthy patron pay mine give almost two work away free take care hefty hardware take suggesting afraid charge money work going apologize,issue,positive,positive,positive,positive,positive,positive
617492761,I've thought about this further.  This is definitely way beyond the scope of what I think is reasonable for this project. It may be done as a commercial effort but anybody that wants to tackle this in their own fork is more than welcome to! ,thought definitely way beyond scope think reasonable project may done commercial effort anybody tackle fork welcome,issue,positive,positive,positive,positive,positive,positive
617489198,"Documentation on this stuff is much more helpful now so I got to track down how to handle the memory issue finally.  Fixed in [this commit ](https://github.com/jantic/DeOldify/commit/696001361ae72bbce9cb50c4e9b60296e92695b7).

You'll now see this message when the render_factor is set too high:

> Warning: render_factor was set too high, and out of memory error resulted. Returning original image.

You can then simply set the render_factor lower and try again without starting over.  
",documentation stuff much helpful got track handle memory issue finally fixed commit see message set high warning set high memory error original image simply set lower try without starting,issue,negative,positive,positive,positive,positive,positive
617479376,"@kirkgutierrdd Yeah that's a great point.  The landscape, and the direction I want to take this project, has evolved since this issue was opened.  I'm going to close it for now as I don't see the sense in trying to do something that's already being done well elsewhere. ",yeah great point landscape direction want take project since issue going close see sense trying something already done well elsewhere,issue,positive,positive,positive,positive,positive,positive
617379772,"@kirkgutierrdd If running locally give it a try to [nvidia-smi](https://manpages.ubuntu.com/manpages/precise/en/man1/alt-nvidia-current-smi.1.html) cmd line to free up memory.

Otherwise, there's a need to look at what is leaking memory in DeOldify or maybe even underling libs.",running locally give try line free memory otherwise need look memory maybe even underling,issue,positive,positive,positive,positive,positive,positive
616845860,"Unfortunately Windows is not supported.  Only Linux. But luckily you can simply use the Colab here and not install anything locally:

https://colab.research.google.com/github/jantic/DeOldify/blob/master/VideoColorizerColab.ipynb",unfortunately luckily simply use install anything locally,issue,negative,neutral,neutral,neutral,neutral,neutral
616845230,"I'll illustrate what we're dealing with here, with something that's pretty middle of the road in terms of source material quality. 

Here's an example of a render using the current high quality jpeg frames:

![00001_old](https://user-images.githubusercontent.com/179759/79805031-1f8dda80-831a-11ea-963b-71efa02f63bc.jpg)

And here's that same frame using png:

![00001](https://user-images.githubusercontent.com/179759/79805049-27e61580-831a-11ea-87f6-91c8bab5eb1b.png)

I do see artifacts that are in the jpeg that aren't present in png, and that's chiefly noticeable around the watermark. That's to be expected. But also the difference in the size of these files is 10x! This becomes a concern precisely when using Colabs because this adds up quickly on VMs that have very limited resources.

Basically my position on what I'm providing with the open source DeOldify is this:  You have something to experiment with and have fun with, but it's not aiming to be production quality stuff.  It's free, and it's  aiming to be low maintenance.  

Especially when the trade off in resources required is steep and the quality gain is fairly minimal- I'll opt for the low resource option.  If you want to fork it and create something that does lossless conversions- great, go ahead!  But I don't think it's worth the tradeoff here.

One more thing to note:  Haloing actually comes from the model, if you're talking about the thick outlines that form around objects sometimes.",illustrate dealing something pretty middle road source material quality example render current high quality frame see present chiefly noticeable around watermark also difference size becomes concern precisely quickly limited basically position providing open source something experiment fun aiming production quality stuff free aiming low maintenance especially trade steep quality gain fairly opt low resource option want fork create something lossless great go ahead think worth one thing note actually come model talking thick form around sometimes,issue,positive,positive,positive,positive,positive,positive
616212246,"@CNugteren The only other thing I can tell you that might be helpful is that I just went from ONNX directly to CoreML and it worked. That was the only motivating factor to using it- as stepping stone towards CoreML.  I didn't use any runtime like that. Perhaps that wasn't the most methodical thing to do but hey...it worked. 

Question:
> And if you or someone else has a few minutes, could you confirm that my code (see above for the exact steps I took) also fails the assert-all-equal for you? 

Already Answered:
>  I won't be digging into this any further. ",thing tell might helpful went directly worked factor stepping stone towards use like perhaps methodical thing hey worked question someone else could confirm code see exact took also already wo digging,issue,positive,positive,positive,positive,positive,positive
616201903,"OK, I understand, hopefully someone in the community can help :-) Perhaps you could still share with us with what ONNX inference engine(s) you have made it work, e.g. did you also try `onnxruntime`? And if you or someone else has a few minutes, could you confirm that my code (see above for the exact steps I took) also fails the assert-all-equal for you? That might hint at whether this is a system/setup/version issue or rather an issue with my/your code.",understand hopefully someone community help perhaps could still share u inference engine made work also try someone else could confirm code see exact took also might hint whether issue rather issue code,issue,positive,positive,positive,positive,positive,positive
616189982,@CNugteren  You're on your own on this unfortunately.  It's outside the scope of the project and I won't be digging into this any further. It's something we have working on our end for our commercial efforts but don't plan on making open source.,unfortunately outside scope project wo digging something working end commercial plan making open source,issue,negative,neutral,neutral,neutral,neutral,neutral
616092824,"Thanks, but that doesn't help. I also got notified by someone else that they experience the same issue. Shall I open a new issue since it is not really the same as the original question here? Could be a bug in ONNX or in the runtime of course. For completeness, these are the steps I take on a Linux machine with Python 3.7:
```
git clone https://github.com/jantic/DeOldify.git
cd DeOldify
pip install onnx onnxruntime
pip install -r requirements.txt
mkdir -p models
cd models && wget https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=1 && cd ..
python export_to_onnx.py  # the file attached in one of my earlier comments
```

Some things I've tried also today:
* Run with ` do_constant_folding=False`, no change, just warning messages appearing.
* Run on a different machine, this time with a CUDA GPU available, no change.
* Run with an old commit, e01dd6b4cdc11f512e319e090604b81d4860a948 (2-okt-2019), still failing.
* Try to run with TF-ONNX as inference engine, but that resulted in a lot of error messages from that runtime.",thanks help also got notified someone else experience issue shall open new issue since really original question could bug course completeness take machine python git clone pip install pip install python file attached one tried also today run change warning run different machine time available change run old commit still failing try run inference engine lot error,issue,negative,positive,positive,positive,positive,positive
614361142,"> @guillefunes - using Colab or Notebook? Then jqueguiner 's instructions above apply just the same either way.
> 
> Otherwise, please clarify your situation- what you're trying to run and how.

Sorry, my bad. Already worked. Thanks
",notebook apply either way otherwise please clarify trying run sorry bad already worked thanks,issue,negative,negative,negative,negative,negative,negative
614313307,@CNugteren I did a double check and there is one thing sticking out that's different that you're doing from what I'm doing in my Onnx export.  And that is you're setting  do_constant_folding=True .  I set it to False. That looks like a potentially suspect operation.,double check one thing sticking different export setting set false like potentially suspect operation,issue,negative,negative,negative,negative,negative,negative
614310335,"@guillefunes - using Colab or Notebook?  Then jqueguiner 's instructions above apply just the same either way.

Otherwise, please clarify your situation- what you're trying to run and how.",notebook apply either way otherwise please clarify trying run,issue,negative,neutral,neutral,neutral,neutral,neutral
614292838,"Hello I am also trying to use DeOldify but for a video, and I am getting the same error:
5 if source_url is not None and source_url !='':
----> 6     video_path = colorizer.colorize_from_url(source_url, 'video.mp4', render_factor, watermarked=watermarked)
      7     show_video_in_notebook(video_path)
      8 else:

NameError: name 'colorizer' is not defined",hello also trying use video getting error none else name defined,issue,negative,neutral,neutral,neutral,neutral,neutral
614201160,@CNugteren  ok I don't think I can help you then.  This appears to be an issue to debug on your end. I can just tell you I've exported this model via ONNX before and it works.,think help issue end tell model via work,issue,negative,neutral,neutral,neutral,neutral,neutral
614180883,"Thank you for your elaborate response, but I don't think that helps me. I did originally run an image through squaring/unsquaring, with (de)normalisation and the other steps you mentioned, then tried to export the model itself to ONNX, failed, tried to reduce the problem to just the model itself. That's why my code is simplified, to pinpoint the problem.

If you take a look at the code I included in my previous message, you'll see that I run the model twice with some random squared input: once through Torch, and once with ONNX. The results are not the same at all afterwards. Everything you say shouldn't matter for this test, at least as in both cases everything is the same. I've also ran the exported with `verbose=True` and inspected some parts of the network, and they did make sense to me (e.g. it ends with a softmax and then a re-scaling into the -3/+3 range).",thank elaborate response think originally run image de tried export model tried reduce problem model code simplified pinpoint problem take look code included previous message see run model twice random squared input torch afterwards everything say matter test least everything also ran network make sense range,issue,negative,negative,neutral,neutral,negative,negative
613762890,"There's a few things to point out here.

First, you'll need to do normalization/denormalization somewhere.  My guess is that this is what you are missing.  That is to say- you can't just plug in an image to the model and get the expected result out- there's preprocessing and postprocessing to do.

To make that part easy for you, here's the code I've used to bake that in to the Onnx model itself:

```python
norm, denorm = normalize_funcs(*imagenet_stats)

class ImageScaleInput(nn.Module):
    def __init__(self): 
        super().__init__()
        self.norm = norm

    def forward(self, x): 
        out = (x.div(255.0)).type(torch.float32)
        out,_ = self.norm((out,out), do_x=True)
        return out

class ImageScaleOutput(nn.Module):
    def __init__(self): 
        super().__init__()
        self.denorm = denorm

    def forward(self, x):
        out = self.denorm(x, do_x=True)
        out = out.float().clamp(min=0,max=1) 
        out = self.denorm(out, do_x=False)
        out = (out.mul(255.0)).type(torch.float32)
        return out
```
You then sandwich your model with these, where initial_pytorch_model is the PyTorch model you have currently in your example:

```python
final_torch_model = nn.Sequential(ImageScaleInput(), torch_model, ImageScaleOutput())
```
Note that in hindsight I'd probably like to always just have normalization/denormalization built into the models like this, because it's very nice to work with compared to the way it is currently in the code.

Finally, note that there's significant steps done in filters.py via ColorizerFilter, which wraps the model, that you'll want to replicate somehow. Namely two big things are happening:

1. Before being put in the model, images are shrunken to a square, with dimensions in pixels being a multiple of 16 times the ""render_factor"" supplied. This can be thought of as another normalization step in a sense, in that it's making the images more like training. 
2. After going through the model, the model rendered image's ""chroma"" component is mapped back to the luminescence portion of the original HD image. The end result is a full resolution color rendering that most viewers can't tell was rendered at a much lower resolution.  This is ""chroma-subsampling"" and it's a key optimization.  You can't get HD images otherwise.",point first need somewhere guess missing ca plug image model get result make part easy code used bake model python norm class self super norm forward self return class self super forward self return sandwich model model currently example python note hindsight probably like always built like nice work way currently code finally note significant done via model want replicate somehow namely two big happening put model shrunken square multiple time thought another normalization step sense making like training going model model image chroma component back luminescence portion original image end result full resolution color rendering ca tell much lower resolution key optimization ca get otherwise,issue,positive,positive,positive,positive,positive,positive
613326464,"I'm also trying to export this model to ONNX, just to run it later on the same machine with [onnxruntime](https://github.com/Microsoft/onnxruntime), inspired by this [PyTorch superresolution tutorial](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html). Although I could export the model, I couldn't get the same results out: results are completely different.

Here's the most important part of my little script, [full 60-line code with failing test attached](https://github.com/jantic/DeOldify/files/4474350/export_to_onnx.zip):
```python
torch_model = deoldify.generators.gen_inference_wide(
    root_folder=Path(""./""), weights_name=""ColorizeStable_gen""
).model
torch_model.eval()

example_input_t = torch.rand(1, 3, 96, 96)
torch_output_t = torch_model(example_input_t)

remove_all_spectral_norm(torch_model)    # function posted above by 'jantic'
torch.onnx.export(torch_model, example_input_t, ""deoldify.onnx"",
                  export_params=True, opset_version=9,
                  do_constant_folding=True, input_names=[""input""],
                  output_names=[""output""])

onnx_session = onnxruntime.InferenceSession(""deoldify.onnx"")
example_input = example_input_t.detach().cpu().numpy()
onnx_output = onnx_session.run(None, {""input"": example_input})[0]
```
I then compare `torch_output_t` to `onnx_output`.

Any hints where the problem might be? Is it in the size-based conditional interpolation? I guess that would only yield issues if I try different input sizes, but I'm running both the original model and the ONNX model with the same input. I'm assuming here I don't have to retrain the network if I remove the spectral norm as suggested above for inference?",also trying export model run later machine inspired tutorial although could export model could get completely different important part little script full code failing test attached python function posted input output none input compare problem might conditional interpolation guess would yield try different input size running original model model input assuming retrain network remove spectral norm inference,issue,positive,positive,positive,positive,positive,positive
613136676,"This is a bit of a vague question.  I assume you mean to port the model to run natively on Android much like I've done for CoreML on iOS?  If so- I don't have anything for you- haven't done it myself! But I can tell you this much- you can't just port the model as is.  It takes a lot of optimization work and that amounted to about 6 months of research on my end.  And I haven't released that work nor will I for some time because it's something we are probably commercializing.

If you're talking about strictly using DeOldify on an existing app on Android-sure.  The very best version of DeOldify is on the [MyHeritage app](https://play.google.com/store/apps/details?id=air.com.myheritage.mobile&hl=en_US) .  Note that the colorization functionality requires a MyHeritage Complete subscription but they are offering unlimited colorization for free until April 22nd.

Then there's the less expensive option that runs a modified version of the open source DeOldify, [Colorize Images](https://play.google.com/store/apps/details?id=ml.colorize.app&hl=en_US).  ",bit vague question assume mean port model run natively android much like done anything done tell ca port model lot optimization work research end work time something probably talking strictly best version note colorization functionality complete subscription offering unlimited colorization free le expensive option version open source colorize,issue,positive,positive,neutral,neutral,positive,positive
612251123,ok. i try to install linux on my system next week when i come home. ,try install system next week come home,issue,negative,neutral,neutral,neutral,neutral,neutral
612250532,"First question is- is this Linux?  If not- only Linux is supported.

If it is Linux, then it appears what you need to do is install Anaconda.  Instructions [here](https://docs.anaconda.com/anaconda/install/linux/)",first question need install anaconda,issue,negative,positive,positive,positive,positive,positive
611688615,"@Hapsidra  So it looks like the conversion scripts basically have the same problems that I had 6 months ago when I had to modify them to work. 

Unfortunately the work I did is for our commercial efforts here at DeOldify and it is a competitive advantage.  So we're not going to release it anytime soon.  The other key thing that I haven't mentioned yet but perhaps it'll save you a lot of time/headache:  You can't run the open source DeOldify on mobile, practically speaking. You have to do a lot of optimizations on the model first.  So when I say I got DeOldify running on iPhone 6s, this entailed a lot of model rework to get it efficient enough. We're talking pretty dramatic levels of optimization here.

What I can tell you about making the conversion scripts work is that it takes a lot of debugging and trial/error, and recognizing that the scripts are making all sorts of wrong assumptions about what models consist of.",like conversion basically ago modify work unfortunately work commercial competitive advantage going release soon key thing yet perhaps save lot ca run open source mobile practically speaking lot model first say got running lot model rework get efficient enough talking pretty dramatic optimization tell making conversion work lot making wrong consist,issue,positive,negative,negative,negative,negative,negative
611413931,@jantic can you share your CoreML model or your solution? I got a strange error while trying to convert. [Here's](https://colab.research.google.com/drive/12f9aBzJQqGEr53gVdpFJvuAkWXpJmSzS) my code and [Onnx model](https://drive.google.com/file/d/1EAfmV_xCGriGo4Ogvks-_ZHhvl09qLF2/view?usp=sharing),share model solution got strange error trying convert code model,issue,negative,negative,neutral,neutral,negative,negative
610649163,"@Hapsidra  I've converted Onnx to CoreML. It required customization of the Onnx to CoreML script from Apple and a bit of trial and error (this was back in Fall 2019- things may have changed since then).  The customization was required because basically the original conversion script code had a lot of bugs, basically.",converted script apple bit trial error back fall may since basically original conversion script code lot basically,issue,negative,positive,positive,positive,positive,positive
608973323,"> Thanks for the fast response @jantic. Yes, I am planning to convert to CoreML. I will try and give feedback.

Hi, have you converted onnx to CoreML?",thanks fast response yes convert try give feedback hi converted,issue,positive,positive,positive,positive,positive,positive
608703518,"If you're intending to use the curl api via Docker then the answer is simple:  You can't directly use images on your machine.  The documentation is already ""proper"" in that regard-  It's not designed to do that. Reason:  It keeps things simple for 99% of users/use cases, and is low maintenance. 

The way around that is to upload the file to a hosting service first, such as Imgur, then use the resulting direct url link to that file.

Now if you really want to go further and do local files then the Jupyter notebooks have examples of that. But those aren't implemented as Docker images, and require more effort/know-how to install (there's instructions for that, too).  

**In short, I'm not here to support everybody's needs/wants. I'm here to put out my research and make it so that others can play around with it and make something out of it if they have the technical know-how and persistence to do it.  That's it.**",intending use curl via docker answer simple ca directly use machine documentation already proper designed reason simple low maintenance way around file hosting service first use resulting direct link file really want go local docker require install short support everybody put research make play around make something technical persistence,issue,positive,positive,neutral,neutral,positive,positive
606883178,Thanks for posting this.  I'll close this issue and am not going to take further action on it- Windows is not supported and won't be in the foreseable future.  But those who want to try getting it to work on Windows anyway may certainly find this useful.,thanks posting close issue going take action wo future want try getting work anyway may certainly find useful,issue,positive,positive,positive,positive,positive,positive
606277984,"I'm going to have to decline hunting this down.  I don't want to be mean, but you're doing custom work then asking me to debug it. That's not what I'm here for. ",going decline hunting want mean custom work,issue,negative,negative,negative,negative,negative,negative
605694537,"I ran the notebook to try training the model with imagenet images. Put 20k images under the path. The images were found after this `il = ImageList.from_folder(path_hr)`. But when I ran `data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)`, I got the exact same error as [#35](https://github.com/jantic/DeOldify/issues/35). I tried to create the folders manually with no luck.",ran notebook try training model put path found ran got exact error tried create manually luck,issue,negative,positive,positive,positive,positive,positive
605375789,"I've made the final changes needed and incorporated the work you guys did into it. Thanks!

Usage is described in the notebooks:

```python
#NOTE:  This must be the first call in order to work properly!
from deoldify import device
from deoldify.device_id import DeviceId
#choices:  CPU, GPU0...GPU7
device.set(device=DeviceId.GPU0)
```",made final incorporated work thanks usage python note must first call order work properly import device import,issue,negative,positive,positive,positive,positive,positive
605375646,"I've made the necessary final changes in a separate push, incorporating the changes here.  Thanks guys!",made necessary final separate push thanks,issue,negative,positive,neutral,neutral,positive,positive
605328192,"It sounds like you're not running the Jupyter notebook itself and are trying to do this in a custom way.... Is that accurate?  I would recommend sticking with the notebooks. I won't be supporting anything that diverges from what's in the GitHub repo.

",like running notebook trying custom way accurate would recommend sticking wo supporting anything,issue,positive,positive,positive,positive,positive,positive
605323063,"Thank you!  Note that I've made some bug fixes and cleaned up the api a bit.  Namely- I'm opting to hiding the ""stats"" parameter from users and making it so that if you want a model that uses these different stats, just make another model creation function in visualize.py. The user shouldn't need to care about this.",thank note made bug bit parameter making want model different make another model creation function user need care,issue,positive,neutral,neutral,neutral,neutral,neutral
600432917,"> If you're asking for this functionality in a Colab or easy to use UI: This is well beyond what we're willing to support for free. It introduces way too much complication and potential for confusion relative to the number of users who actually want it.
> 
> If you're asking for a Jupyter notebook: This is one of those things that I do actually use internally for myself in our own DeOldify business efforts. So I have batch processing notebooks but I haven't open sourced them because they're not coded to be robust enough to be more general use. It's just enough for us to use for our own purposes. And going beyond that is just something we're not willing to put time into yet with all the other things we have going on right now.
> 
> If somebody else wants to take a stab at it- great!

what is ui sir? ",functionality easy use well beyond willing support free way much complication potential confusion relative number actually want notebook one actually use internally business batch open robust enough general use enough u use going beyond something willing put time yet going right somebody else take stab great sir,issue,positive,positive,positive,positive,positive,positive
599637924,Hi Gant! We're honored by this.  We've decided to put it on our website instead of the readme (deoldify.ai). So I'll close this and you should see it there in a few days.,hi gant decided put instead close see day,issue,negative,neutral,neutral,neutral,neutral,neutral
598910074,"If you're asking for this functionality in a Colab or easy to use UI:  This is well beyond what we're willing to support for free.  It introduces way too much complication and potential for confusion relative to the number of users who actually want it.

If you're asking for a Jupyter notebook: This is one of those things that I do actually use internally for myself in our own DeOldify business efforts.  So I have batch processing notebooks but I haven't open sourced them because they're not coded to be robust enough to be more general use.  It's just enough for us to use for our own purposes. And going beyond that is just something we're not willing to put time into yet with all the other things we have going on right now. 

If somebody else wants to take a stab at it- great!",functionality easy use well beyond willing support free way much complication potential confusion relative number actually want notebook one actually use internally business batch open robust enough general use enough u use going beyond something willing put time yet going right somebody else take stab great,issue,positive,positive,positive,positive,positive,positive
597966453,It might be possible but we're not going to add anything in addition to what's already there.  If all else fails it should be in that path you mentioned there (video/result/video.mp4 ).  The Troubleshooting section details how to navigate to this folder if you don't see the video show up on the UI with a media player.  ,might possible going add anything addition already else path section navigate folder see video show medium player,issue,negative,neutral,neutral,neutral,neutral,neutral
597243653,"It looks like you're trying to install on Windows.  Windows is not supported for a number of reasons so I'll be closing this issue and directing you to go with a Linux install.
 
Reason- it's impractical for us to chase down all the issues that are associated with the lack of support in the key libraries themselves. Ubuntu 18.04 is the way to go- makes life way simpler.
",like trying install number issue go install impractical u chase associated lack support key way life way simpler,issue,negative,neutral,neutral,neutral,neutral,neutral
596696965,"Going to close this as work, was done on this already.",going close work done already,issue,negative,neutral,neutral,neutral,neutral,neutral
596695472,"Upon further consideration- this is adding a bit too much complication and potential unreliability for something that won't be used by the vast majority of users. Detecting reliably whether or not something is already colorized isn't entirely trivial. This is one of those things that I think is better left up to manual intervention.,",upon bit much complication potential unreliability something wo used vast majority reliably whether something already entirely trivial one think better left manual,issue,negative,positive,positive,positive,positive,positive
596694418,Going to close this.  This seems to be a dead issue.,going close dead issue,issue,negative,negative,negative,negative,negative,negative
596693653,"Upon further consideration, I think this is beyond the scope of what we want to do for DeOldify.  There are other offerings out there that already do this quite well.",upon consideration think beyond scope want already quite well,issue,negative,neutral,neutral,neutral,neutral,neutral
596692828,"Upon further consideration, this is a bit too much complication being introduced for a relatively small gain.",upon consideration bit much complication relatively small gain,issue,positive,negative,neutral,neutral,negative,negative
596692208,Going to close this.  I don't think the extra complication is worth it.,going close think extra complication worth,issue,negative,positive,positive,positive,positive,positive
596691165,"Higher quality images help a lot, in general.  You have to be careful though- a lot of what is done artificially to increase the quality of an image- noise reduction for example- often helps but sometimes winds up throwing off the model and making things worse.  Why?  Because a lot of what the model bases its colorization on is textures and minute patterns, and this upscaling/denoising processes often introduce subtle differences that aren't quite natural that the model just isn't use to seeing.

Similarly, increasing render_factor, as a rule of thumb, will get the details better (like determining what is skin), but that rule of thumb is highly dependent on input quality.  If the input quality is low, then you're actually better off with low render_factor.

Basically your best bet is to use the rule of thumb that yes- higher quality inputs and higher render_factors are going to do better in general, but ultimately you have to experiment to get best results.  Luckily there's the handy feature in the notebooks and Colabs to do just that- under the title ""See how well render_factor values perform on a frame here"" at the botom.  The idea there is that you can choose a frame and see which render_factor will work best for that frame. Generally- if it works best for that frame, it's probably a good bet for the rest of the scene/video.",higher quality help lot general careful lot done artificially increase quality noise reduction often sometimes throwing model making worse lot model base colorization minute often introduce subtle quite natural model use seeing similarly increasing rule thumb get better like skin rule thumb highly dependent input quality input quality low actually better low basically best bet use rule thumb higher quality higher going better general ultimately experiment get best luckily handy feature title see well perform frame idea choose frame see work best frame work best frame probably good bet rest,issue,positive,positive,positive,positive,positive,positive
596128519,"Ok, thanks. If you say it's flickering alot, I dont't want this anymore.",thanks say flickering want,issue,negative,positive,positive,positive,positive,positive
596117899,@MrBoombastic I'm not going to support this as it simply will not work well. I've tried. It'll flicker a lot because it's not designed for video. I don't want to encourage others to do it either and invite a flood of support requests so I'm not going to put instructions here to do it.  It can be done if you're really determined but that'll require digging into the code and that'll be up to you.,going support simply work well tried flicker lot designed video want encourage either invite flood support going put done really determined require digging code,issue,positive,positive,neutral,neutral,positive,positive
595552371,"@oyvigri  Thanks for this very detailed writeup and research.  I've been on vacation so I just got around to looking at this now.  

This will be a great resource for those power users who really really want to run DeOldify on Windows.  That being said, I don't plan on adding changes to support Windows, because that means we'll be promising support on that platform and opening a can of worms with maintenance on a platform that is generally just not well supported in the deep learning community.  It's a practical decision more than anything.  

Honestly, I secretly prefer Windows myself (heresy!). But not enough to open the flood gates on additional issues and corner cases to hunt down.",thanks detailed research vacation got around looking great resource power really really want run said plan support promising support platform opening maintenance platform generally well deep learning community practical decision anything honestly secretly prefer heresy enough open flood additional corner hunt,issue,positive,positive,positive,positive,positive,positive
592474553,"Ok so this is how I got it running on Windows 10 Step by step: 
Should note that I didnt fully got it working because Im not running CUDA graphics card, although I got the application up and running and am currently seeing if theres an option to run on CPU instead.

1. 
Download Anaconda from here: https://www.anaconda.com/distribution/#download-section. I got it running with Anaconda version (4.7.12) with python version 3.7 installer.
During the installation process, you will be presented with two checkboxes. Choose the ""recomended one"" (the bottom one).

2.
Download WGet from here: http://gnuwin32.sourceforge.net/packages/wget.htm
(I went for the `Complete package, except sources` one or with md5Sum `b4679ac6f7757b35435ec711c6c8d912`)

3.
Add PATH Variables to Windows (If you are unsure what Im referring to, just google how to add system path variables to windows)
This step is ofc dependant on where you installed your software. In my case it looked like this:
```
C:\Users\XXX\AppData\Local\Continuum\anaconda3
C:\Users\XXX\AppData\Local\Continuum\anaconda3\Scripts
C:\Users\XXX\AppData\Local\Continuum\anaconda3\Library\bin
C:\Program Files (x86)\GnuWin32\bin
```
4. open either CMD / powershell / git bash, in a folder where you would like to download deOldify and write the following:
`git clone https://github.com/jantic/DeOldify.git DeOldify`
(If you are unsure what GIT is, download git bash from here: https://git-scm.com/downloads).

5.
now write `cd DeOldify` in cmd.

6.
Open environment.yml file inside deOldify folder.
change its content to the following:
```
name: deoldify
channels:
- fastai
- conda-forge
- defaults
- pytorch
dependencies:
- python>=3.7.3
- fastai=1.0.51
- pytorch=1.0.1
- ffmpeg=4.1.1
- tensorboardX=1.6
- youtube-dl>=2019.4.17
- jupyterlab
- pillow=6.2.1
- pip:
  - ffmpeg-python==0.1.17
  - opencv-python>=3.3.0.10
  - wandb
```

7.
Go back to cmd / gitBash or whatever, and write the following:
`conda env create -f environment.yml` (this takes a long time), and although after a while it looks like nothing is happening, wait some more. it should say when its finished.

8.
when previous step is done, write the following in cmd:
`conda activate deoldify`
If this step provides with an error like Git isnt inited or similar, open separate cmd window and open in same folder (deOldify folder).
Depending on which editor you open, write the following:
`conda init cmd` (if you opened cmd) `conda init bash` for Git bash, `conda init powershell` etc.
restart all cmds.

9.
open new cmd and write `jupyter lab`
this step should take a while and when done should open a webApp running on port localhost port 8888.

10.
Open separate cmd and write the following:
`wget https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0 -O ./models/ColorizeStable_gen.pth --no-check-certificate`
(This will only work given we provided the `C:\Program Files (x86)\GnuWin32\bin` to windows PATH)
 (Should note that I created the folder `models` inside the deOldify folder beforehand and unchecked the attribute read-only, although unsure if you need to do that yourself)

11.
Enter localhost on port 8888 (it should open automatically) and open eg: `imagecolorizer.ipynb`
and go through the steps.

For me this is where it ended because Im not running CUDA although if you have nvidia card this shouldnt be an issue.",got running step step note didnt fully got working running graphic card although got application running currently seeing there option run instead anaconda got running anaconda version python version installer installation process two choose one bottom one went complete package except one add path unsure add system path step case like open either git bash folder would like write following git clone unsure git git bash write open file inside folder change content following name python pip go back whatever write following create long time although like nothing happening wait say finished previous step done write following activate step error like git similar open separate window open folder folder depending editor open write following bash git bash restart open new write lab step take done open running port port open separate write following work given provided path note folder inside folder beforehand unchecked attribute although unsure need enter port open automatically open go ended running although card shouldnt issue,issue,positive,positive,neutral,neutral,positive,positive
592107496,"seeing this post is marked as ""closed"", of which I assume noone in this thread will recieve any further notifications, Im trying this -> @ping @petem266 ",seeing post marked closed assume thread trying ping,issue,negative,neutral,neutral,neutral,neutral,neutral
592074347,"My god Im glad I stumbled upon this issue. Note to self: Check the ""Issues"" tab ALOT sooner.
Thank you for creating this issue and for continous feedback @petem266.

Im currently trying to setup this on my Windows 10 machine but have come across some of your mentioned errors. You have currently helped me to get past my previous errors atleast.

Could you take me through the process step by step in order to make this work ? (I understand if its a hassle, so no worries if you dont), but atleast please let me know if you wont so Im not wasting my life re-checking this thread every hour from now on.

Ive tried so much now and Im just completelly lost. Think I have 5 different python versions installed and anaconda aswell as miniconda installed.

Uninstalled Everything and started from scratch...
-----------------------------------------------------

Where Im currently at:

installed Anaconda3-2019.10-Windows-x86_64
setup the following ENV variables

C:\\users\xxx\Anaconda3
C:\\users\xxx\Anaconda3\Scripts
C:\\users\xxx\Anaconda3\Library\bin

Copied
libcrypto-1_1-x64.*
libssl-1_1-x64.*
from C:\\users\xxx\Anaconda3\Library\bin to C:\\users\xxx\Anaconda3\DLLs

(in order to get rid of the following error
`Error Caused by SSLError(""Can\'t connect to HTTPS URL because the SSL module is not available.` )

changed the environment.yml to the following, atleast thats what I figured from your previous comment.
```
name: deoldify
channels:
- fastai
- conda-forge
- defaults
- pytorch
dependencies:
- python>=3.7.3
- pytorch=1.0.1
- ffmpeg=4.1.1
- tensorboardX=1.6
- youtube-dl>=2019.4.17
- jupyterlab
- pillow=6.2.1
- pip:
  - ffmpeg-python==0.1.17
  - opencv-python>=3.3.0.10
  - wandb
  - fastai=1.0.51
```

After runing `conda env create -f environment.yml` Im now stuck with ""endless"" `InvalidArchiveError`
Example: 
```
InvalidArchiveError(""Error with archive C:\\Users\\XXX\\Anaconda3\\pkgs\\prometheus_client-0.7.1-py_0.tar.bz2.  You probably need to delete and re-download or re-create this file.  Message from libarchive was:\n\nFailed to open 'C:\\Users\\XXX\\Anaconda3\\pkgs\\prometheus_client-0.7.1-py_0.tar.bz2'"")
```

I should also mention if I eventually reach the following steps
```
source activate deoldify
jupyter lab
```
I understand that ""source"" is a unix specific command (have also no idea what jupyter is).
What are the Windows equivalent commands ?

Any help would be GREATLY!!!! appreciated.",god glad upon issue note self check tab sooner thank issue feedback currently trying setup machine come across currently get past previous could take process step step order make work understand hassle dont please let know wont wasting life thread every hour tried much lost think different python anaconda aswell uninstalled everything scratch currently setup following copied order get rid following error error connect module following thats figured previous comment name python pip create stuck endless example error archive probably need delete file message open also mention eventually reach following source activate lab understand source specific command also idea equivalent help would greatly,issue,negative,positive,neutral,neutral,positive,positive
590220596,@jantic Yes! I updated the version of Nvidia driver. The model is fantastic! Cheers!,yes version driver model fantastic,issue,positive,positive,positive,positive,positive,positive
589931936,"Having tested things on an Ubuntu, I am starting to believe that Conda seems to be the issue.

I went back to Windows, downgraded my conda (to 4.6.14). This caused the environment creation to crash at FASTAI / PYTORCH. I edited the environment.yml and moved FASTAI under PIP just to see what happens. Well, this caused the environment to be created without a single error message.  The environment was also activated by conda without any issues. 
However, FASTAI was not installed as it was ""missing"" when running the jupyter notebook. 

Trying to install Fastai 1.0.51 into the environment however produced a ""failed to build bottleneck"" error and an error about Microsoft Visual C++ 14 missing. So, some progress was made, but new errors have occurred. 

CORRECTION. Went back to the original ym.file just using a dowgraded CONDA.
It produced an error when creating the environment.
ResolvePackageNotFound:
  - fastai=1.0.51 -> torchvision

Added PYTORCH into channels in the environment.yml file and now the environment was created successfully.  

EDIT #3. After the previous environment creation, I installed the weights into the model directory and ran a test video. Everything runs smoothly in jupyter under Windows 10.

Edit #4. Tested this on my second Win10 laptop. This time I used the conda that came with the Anaconda package (4.7.12), no downgrades or updates to conda were done, however I again edited the environment.yml and added the channel name pytorch. Again, everything works just fine and the environment is created correctly. 
",tested starting believe issue went back environment creation crash pip see well environment without single error message environment also without however missing running notebook trying install environment however produced build bottleneck error error visual missing progress made new correction went back original produced error environment added file environment successfully edit previous environment creation model directory ran test video everything smoothly edit tested second win time used came anaconda package done however added channel name everything work fine environment correctly,issue,positive,positive,positive,positive,positive,positive
589588540,@petem266 take the [official example](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#create-env-file-manually) from Anaconda docs and try to reproduce the bug.,take official example anaconda try reproduce bug,issue,negative,neutral,neutral,neutral,neutral,neutral
589587392,"Based on the traceback it seems to me that the bug is in Anaconda itself, maybe you can try to use another Anaconda version or file an issue in their project.",based bug anaconda maybe try use another anaconda version file issue project,issue,negative,neutral,neutral,neutral,neutral,neutral
589435094,"Hi @petem266 ! So unfortunately Windows support is one of those things we've drawn a line on and have the stance that it's beyond the scope of what we're willing to support for this project.  There's many reasons for that, but one big one is that generally the deep learning ecosystem simply isn't tailored around Windows.  Windows is basically neglected. So for us to chase issues stemming from that fact just seems to be not worth it.  Granted, this alienates a lot of users and that's unfortunate, but it boils down to a simple question of time management for us.  And sanity.

That all being said, I did make an honest effort to look at the error text you posted there.  It's certainly not obvious what Anaconda is complaining about there, I can tell you that much. 

If you can do this, your best bet by far would be to install a dual boot of Ubuntu 18.04 on your machine and run DeOldify from that.  If I remember correctly, when you install Ubuntu it'll give you the option to make it a dual boot with Windows if you already have Windows on the machine.  But I'd double check on that before proceeding.",hi unfortunately support one drawn line stance beyond scope willing support project many one big one generally deep learning ecosystem simply around basically u chase stemming fact worth lot unfortunate simple question time management u sanity said make honest effort look error text posted certainly obvious anaconda tell much best bet far would install dual boot machine run remember correctly install give option make dual boot already machine double check proceeding,issue,positive,positive,positive,positive,positive,positive
589221277,"@alexandrevicenzi  sorry, I thought it's enough to attach screenshot
@jantic thanks!",sorry thought enough attach thanks,issue,negative,negative,neutral,neutral,negative,negative
589198504,@n10ty  I've fixed the issues.  Basically I made a series of bad decisions yesterday.  Sorry about that!,fixed basically made series bad yesterday sorry,issue,negative,negative,negative,negative,negative,negative
589155199,Could you be a little more descriptive? Just saying that it does not work is not that helpful.,could little descriptive saying work helpful,issue,negative,negative,negative,negative,negative,negative
586915562,"@shadowzoom You can run the jupyter notebook in Colab, it does not require any knowledge in Python and is cross-platform. There's even an [Android version](https://play.google.com/store/apps/details?id=ml.colorize.app&pcampaignid=MKT-Other-global-all-co-prtnr-py-PartBadge-Mar2515-1) app based on DeOldify.

If you are willing to send a donation I would happily implement a windows, linux, and os x desktop app.",run notebook require knowledge python even android version based willing send donation would happily implement o,issue,positive,positive,positive,positive,positive,positive
586696885,Donation will be raised by 500-100% because not every human know what is hack is python etc,donation raised every human know hack python,issue,negative,neutral,neutral,neutral,neutral,neutral
586547653,"> OK guys, thanks. I'll close this for now then.

It works perfectly fine now, as @asears mentioned it seems to be pytorch-colab issue. Thank you @jantic  for creating DeOldify and responding to issue quickly :) ",thanks close work perfectly fine issue thank issue quickly,issue,positive,positive,positive,positive,positive,positive
585972340,"Your best bet is going to be the [readme](https://github.com/jantic/DeOldify/blob/master/README.md), and then just following along with the [ColorizeTrainingArtistic](https://github.com/jantic/DeOldify/blob/master/ColorizeTrainingArtistic.ipynb) notebook (there's some documentation in there). I'm not sure what your background is but you should have a pretty solid background in deep learning already (FastAI courses, for example). The training of this particular stuff isn't easy unfortunately, even for people with the background.

To be clear on why we're not supporting a ColorizeTrainingArtistic Colab version:  
1. Indeed, you have to employ clever workarounds and/or buy Colab pro.  Very few people are going to want to do this.
2. More importantly, we can't be everything for everyone here.  We're making deliberate choices about how to spend our time and what we'll support (hence not supporting Windows). It's about sane time and resource management.  ",best bet going following along notebook documentation sure background pretty solid background deep learning already example training particular stuff easy unfortunately even people background clear supporting version indeed employ clever buy pro people going want importantly ca everything everyone making deliberate spend time support hence supporting sane time resource management,issue,positive,positive,positive,positive,positive,positive
585969140,Looks like the certificate is good again. I just tried running the Colab and it worked fine.  I also tried downloading that model from the url separately and that worked fine. I'm going to close.,like certificate good tried running worked fine also tried model separately worked fine going close,issue,positive,positive,positive,positive,positive,positive
585541164,"Hi, Jason Antic. I am sorry for bothering you. 
I have had the low running hours issues, but I can solve it by export the model and the trained weight to the google drive once the training part1 is down, and I can train it in part2 the next day, and I have been training the lesson7 notebook, with a CPU only laptop, for almost every day in this way.  In addition, you can upgrade to Colab pro is just 9.99 dollars can have 30 more GB ram and 24 hours run times. 


If you can, could you tell me what tech did you use in your deoldify project and the paper I have to read that I can learn by myself and build another deoldify? There are some pictures that I want to color them.
Thank You",hi antic sorry low running solve export model trained weight drive training part train part next day training lesson notebook almost every day way addition upgrade pro ram run time could tell tech use project paper read learn build another want color thank,issue,negative,negative,negative,negative,negative,negative
585493395,"Seems like it was a transitory Pytorch or Colab issue.  

I was getting this yesterday evening in VideoColorizerColab notebook.  I couldn't find where it implicitly downloads the cached Pytorch files, and even the versions I had locally weren't picked up.

Yup
https://github.com/pytorch/pytorch/issues/33234 ",like transitory issue getting yesterday evening notebook could find implicitly even locally picked,issue,negative,neutral,neutral,neutral,neutral,neutral
585424619,"Unfortunately this isn't something that we'll support here, for a number of reasons.  Big one is that basically Google won't let you run it long enough to train (it takes a few days- it times out at 12 hours). So it's basically just not doable.",unfortunately something support number big one basically wo let run long enough train time basically doable,issue,negative,negative,negative,negative,negative,negative
585423249,Are you guys still getting this? I just tried it specifically on Imgur images and it's working currently.  There's a distinct possibility that Imgur was having trouble at the time but I am curious if you have a specific URL you tried.,still getting tried specifically working currently distinct possibility trouble time curious specific tried,issue,negative,negative,neutral,neutral,negative,negative
572678659,Neither of those should matter- you should just be able to get the latest versions and they'll work.  I've been doing that myself with internal development.,neither able get latest work internal development,issue,negative,positive,positive,positive,positive,positive
570950646,"Yeah there's a pretty easy answer to this one- The lower quality the photo is, the harder time you're going to have getting a decent result. Makes sense, right?  The main things to be done is to reduce noise, sharpen the image, and improve the contrast.  Easier said than done and unfortunately not all situations can be rectified like this. ",yeah pretty easy answer lower quality photo harder time going getting decent result sense right main done reduce noise sharpen image improve contrast easier said done unfortunately rectified like,issue,positive,positive,positive,positive,positive,positive
570948532,"@jantic , I don't want to disturb you so much, but any other hint beside the aspect ratio?

I squared the following picture:
![WhatsApp Image 2020-01-05 at 13 04 53 (1)](https://user-images.githubusercontent.com/5733246/71786178-7e3b7c80-2fe7-11ea-8035-25f530cbcc6f.jpeg)


But still, the pictures are not being correctly colored. I think it is due to its low quality. Here's what I get:

![Screen Shot 2020-01-05 at 19 11 15](https://user-images.githubusercontent.com/5733246/71786109-cc9c4b80-2fe6-11ea-8d42-67b6227dd7b7.png)


![Screen Shot 2020-01-05 at 19 11 22](https://user-images.githubusercontent.com/5733246/71786112-d160ff80-2fe6-11ea-97e4-52c7c238c8a8.png)

Overall, I am getting very good results! Here are some pictures that it colored quite well:

![Screen Shot 2020-01-05 at 19 12 47](https://user-images.githubusercontent.com/5733246/71786131-1be27c00-2fe7-11ea-8947-0fc037c88612.png)


Thank you again! 

",want disturb much hint beside aspect ratio squared following picture image still correctly colored think due low quality get screen shot screen shot overall getting good colored quite well screen shot thank,issue,positive,positive,positive,positive,positive,positive
570820851,"Just to confirm that it's working like a charm now, don't even need to add the VRAM purge function anymore too...
Thanks a lot Jason, I feel ashamed that I can't donate anything :( 

",confirm working like charm even need add purge function thanks lot feel ashamed ca donate anything,issue,positive,positive,positive,positive,positive,positive
570766447,Hey guys- sorry- it's fixed now.  I got lazy with the last pull request and neglected to test stuff like I usually do. ,hey fixed got lazy last pull request test stuff like usually,issue,negative,negative,neutral,neutral,negative,negative
570764019,"right before you pip install the requirements.txt, change the last line in requirements.txt from ""pillow==6.2.2""
to just
""pillow""
after that it worked okay for me",right pip install change last line pillow worked,issue,negative,positive,positive,positive,positive,positive
570122320,"So what you have here is a pretty exceptional aspect ratio, where the photo is much taller than it is wide. This is actually contributing most to what you're seeing- the model simply wasn't trained to deal with images that have these sorts of dimensions.  In fact all images are reduced to a smaller square before they're fed through the model.  So what happens in this case is that this image gets super distorted in that process.  My guess is that the one thing it still recognizes in the image is the plants and so it just decides to make everything green.

SO the answer here to get a decent result is to crop the image. If you do that you get a much better result:

Current open source model, using render_factor at 22:  
![cropped2](https://user-images.githubusercontent.com/179759/71652985-c8b6b380-2cde-11ea-89a1-0b74e8f7b77f.png)

Bonus:  New model I'm working on (unreleased)
![cropped3](https://user-images.githubusercontent.com/179759/71652998-dec47400-2cde-11ea-8680-5de37df73e1f.png)

You'd get an even better result if the image wasn't so grainy- not sure if you can do anything about that.
 ",pretty exceptional aspect ratio photo much taller wide actually model simply trained deal fact reduced smaller square fed model case image super distorted process guess one thing still image make everything green answer get decent result crop image get much better result current open source model bonus new model working unreleased get even better result image sure anything,issue,positive,positive,positive,positive,positive,positive
568385028,"@flip111 I think I might have misread what you asked.  Yeah go ahead and just try it on the cpu.  It could probably work. 

As far as when this pull request will be merged in- I'm not working on this anytime soon. I can't speak for anybody else.  I'm too heavily involved in other stuff right now.",flip think might misread yeah go ahead try could probably work far pull request working soon ca speak anybody else heavily involved stuff right,issue,negative,positive,neutral,neutral,positive,positive
568303461,"I don't understand from the previous messages if there was still anyone who wants to fix the patch.

@jantic i don't understand your comment about having just one GPU, isn't that the normal situation for most people? My GPU only has 3GB of memory though, so i like to try it with the CPU",understand previous still anyone fix patch understand comment one normal situation people memory though like try,issue,negative,negative,neutral,neutral,negative,negative
563184112,"Thank you very much for all you have done. Due to the limited experimental conditions, I can only do this. Because it is remote, there is no way to change windows to Linux.

 I also hope to follow the method you provided as much as possible, but I'm sorry that I didn't do it due to objective conditions, and I hope you can forgive me.

Thank you and your team at one time.",thank much done due limited experimental remote way change also hope follow method provided much possible sorry due objective hope forgive thank team one time,issue,positive,negative,neutral,neutral,negative,negative
563076739,"I really don't think you should alter the sz parameters like that. If I remember correctly, 32 is too small and will cause an error when you try running it through the model because it can't reduce the image through all the layers (the image size gets halved many times in this process).

But besides that, what you're really asking about here is debugging a problem on your local for Windows that's again beyond the scope of what we're willing to deal with here. There's a number of reasons you could be running into that issue where it's not finding the val folder (missing? permissions not sufficient? etc). 

Anyway, it's one thing to be asked to deal with a bug in a typical install of DeOldify but that's not what you're asking for here.  You're basically asking for tech support, for free, on something you decided to try that we're not recommending. That's not what we're here for. 

So I'm going to leave it at that and hopefully you'll be able to resolve this.",really think alter like remember correctly small cause error try running model ca reduce image image size halved many time process besides really problem local beyond scope willing deal number could running issue finding folder missing sufficient anyway one thing deal bug typical install basically tech support free something decided try going leave hopefully able resolve,issue,positive,positive,positive,positive,positive,positive
562638029,"If you have 16GB GPU ram the existing settings should be fine.  This was all designed around 11GB.  bs is batch size (lowering it will lower your memory requirements).  sz is image size in pixels (they're squared).

But otherwise I think you're on your own on this one.  @alexandrevicenzi is right.",ram fine designed around batch size lowering lower memory image size squared otherwise think one right,issue,negative,positive,positive,positive,positive,positive
562632291,"But in theory, it may not be a system problem. I want to ask which parameters involve memory usage, so that I can adjust myself until I can run successfully",theory may system problem want ask involve memory usage adjust run successfully,issue,negative,positive,positive,positive,positive,positive
562587311,"DeOldify is only tested, developed and supported on Linux. On Windows, you may see weird errors and we can't help getting it up on Windows.",tested may see weird ca help getting,issue,negative,negative,negative,negative,negative,negative
562584177,"Maybe I've solved the problem, I used other methods, I don't know if it's right.

![image](https://user-images.githubusercontent.com/45026028/70328023-0f95c280-1873-11ea-9b6a-daee0537959a.png)

But I have a new problem.My computer is windows10 and GPU is 2080ti(11G), have 16G RAM. I put train data 200 and val data 50.

 When I run to the `learn_gen.fit_one_cycle`, it told me ` CUDA out of memory`.

![image](https://user-images.githubusercontent.com/45026028/70328580-5a640a00-1874-11ea-8a6f-68dbb07a0638.png)

Can I solve this problem by adjusting the parameters.

And I don't know the meaning of `bs=88` `sz=64`, can I adjust them?",maybe problem used know right image new computer ti ram put train data data run told memory image solve problem know meaning adjust,issue,negative,positive,positive,positive,positive,positive
561929273,"Now I know why I was get this error, but when I run this line

![](https://user-images.githubusercontent.com/179759/69890334-f0fb5e80-12a9-11ea-939f-2efd800f9d45.png)

I get nothing, my floder not create the ""data/imagenet/ILSVRC/Data/CLS-LOC/bandw"", I just get the photo number and seconds in the jupyter notebook

![](https://user-images.githubusercontent.com/45026028/70196786-c8a9af00-1744-11ea-85ba-26f3756b36f5.png)

This means that the image was read but not converted to black and white, I can't get black and white, so what should I do. I really don't know where have problem.
",know get error run line get nothing create get photo number notebook image read converted black white ca get black white really know problem,issue,negative,negative,neutral,neutral,negative,negative
559881719,Also I wouldn't change the keep_pct value.  That just determines what percentage of the data is pulled from the folders to train with that I just described.  1.0 would mean all the data (100%).  0.5 would mean 50%. ,also would change value percentage data train would mean data would mean,issue,negative,negative,negative,negative,negative,negative
559881554,"Yeah that'd be an issue you'd run into if the data loader fails to find the expected training data in the format expected.  Here's what that should look like:

There should be a data folder in the root DeOldify folder (where the notebooks are).  Inside that would be the imagenet data.  I used exactly what I got from downloading imagenet so the training/validation sets are in here:  data/imagenet/ILSVRC/Data/CLS-LOC

In that folder you'd have these two folders:

![image](https://user-images.githubusercontent.com/179759/69890177-74688000-12a9-11ea-8c7f-f6684992dd79.png)

train folder looks like this- each of the folders contains images belonging to the folder's class:

![image](https://user-images.githubusercontent.com/179759/69890255-af6ab380-12a9-11ea-87d0-2f3337003091.png)

val folder looks the same(except not nearly as much data of course).

Finally make sure that this runs in that ColorizeTrainingStable.ipython notebook:

![image](https://user-images.githubusercontent.com/179759/69890334-f0fb5e80-12a9-11ea-939f-2efd800f9d45.png)

It'll make the corresponding black and white images from those folders I just described and drop them in a newly created data/imagenet/ILSVRC/Data/CLS-LOC/bandw folder.   If you don't do that you'll see the error you're describing in this issue.
",yeah issue run data loader find training data format look like data folder root folder inside would data used exactly got folder two image train folder like belonging folder class image folder except nearly much data course finally make sure notebook image make corresponding black white drop newly folder see error issue,issue,positive,positive,positive,positive,positive,positive
559880388,"So...Windows 10 really isn't supported explicitly for DeOldify so any bugs you run into are beyond the scope of what we're willing to hunt down and deal with at this stage (it's just a matter of resources/practicality really).

Second- 2 GB GPU just isn't going to work for DeOldify. That's too little and that's why you'll be getting a CUDA out of memory error.

What I'd recommend, if your interest is simply colorizing images, is to use the Colab, which takes care of all these issues for you! 

https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb",really explicitly run beyond scope willing hunt deal stage matter really going work little getting memory error recommend interest simply use care,issue,positive,positive,neutral,neutral,positive,positive
559650187,"I try to run ""ColorizeTrainingStable.ipython"", but when I run to `data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)`, I was told ""IndexError: index 0 is out of bounds for axis 0 with size 0"", so I change keep_pct=0.9, is it ok?",try run run told index axis size change,issue,negative,neutral,neutral,neutral,neutral,neutral
558905096,"These are great points, for sure, and I've had other photographers point out this same thing. I've considered it quite a bit.  This would be an interesting project as a fork of DeOldify (go for it!) but I wouldn't want to design DeOldify around it.  Why?  Because it makes the usage a lot more complicated- most people simply aren't going to know which model to use because either they won't know the specifics of the photo (how it was taken), or they'll simply find the extra step annoying (I would, honestly). My ideal with DeOldify is to keep it super simple:  B&W photo comes in and color photo comes out.  No manual intervention (including getting rid of render_factor once and for all).  What you're describing strikes me more as a ""prosumer"" type of thing.  I'm sure there's people who want this so I fully encourage you to try it. But it is beyond the scope of DeOldify I think.",great sure point thing considered quite bit would interesting project fork go would want design around usage lot people simply going know model use either wo know photo taken simply find extra step annoying would honestly ideal keep super simple photo come color photo come manual intervention getting rid type thing sure people want fully encourage try beyond scope think,issue,positive,positive,positive,positive,positive,positive
558903438,"> I like to try this. Shall i clone it and run with this patch? Or better to wait? Would be nice if the patch includes some documentation what in the configuration needs to change to run in CPU mode.

I mean you can certainly try.  It could work just fine for you if you have just one GPU.  It'll be a bit more of a wait until the patch gets fixed.",like try shall clone run patch better wait would nice patch documentation configuration need change run mode mean certainly try could work fine one bit wait patch fixed,issue,positive,positive,positive,positive,positive,positive
557932052,I like to try this. Shall i clone it and run with this patch? Or better to wait? Would be nice if the patch includes some documentation what in the configuration needs to change to run in CPU mode.,like try shall clone run patch better wait would nice patch documentation configuration need change run mode,issue,positive,positive,positive,positive,positive,positive
557827107,"I dug in to this a bit more. Currently the function for creating training images uses the PIL convert function. 

```
def create_training_images(fn,i):
    dest = path_lr/fn.relative_to(path_hr)
    dest.parent.mkdir(parents=True, exist_ok=True)
    img = PIL.Image.open(fn).convert('LA').convert('RGB')
    img.save(dest)  
```

The PIL convert function converts RGB image to grayscale pixel by pixel using a so-called ""ITU-R 601-2 luma transform"" e.g. converting the RGB values into greyscale value with the formula 0.299*R+0.587*G+0.114*B. These weights have been designed to produce a nice looking grayscale conversion in standard use.

However, in black and white photography, the weights are probably different to begin with because of how the film chemistries. Adding to this, the most drastic difference to the weights comes when the photographer uses color filters in front of the camera. With a red filter, you would do the transform to grayscale  using only the red channel value, omitting the green and blue altogether, as these colors would not have gone past the red filter onto the film. Similarly with a yellow filter,  you would take the red and green channels: 0.5*R+0.5*G and omit the blue. And so on for the different filters.  So in order to test this idea, a more versatile convert function is needed.",dug bit currently function training convert function convert function image transform converting value formula designed produce nice looking conversion standard use however black white photography probably different begin film drastic difference come photographer color front camera red filter would transform red channel value green blue altogether color would gone past red filter onto film similarly yellow filter would take red green omit blue different order test idea versatile convert function,issue,positive,negative,neutral,neutral,negative,negative
550086787,"Fair warning:  I'm going to be blunt.

I stand in firm agreement with @alexandrevicenzi here. Let me be brutally honest:  The code in the pull request just didn't work for us. We kept giving feedback on this and were taking extra time to walk you through this, even though it was something that we could have just took over and done ourselves. Alexandre bent over backwards to accommodate this.  Believe it or not I'm not paying him.  He's awesome and quite frankly he should be paid.

But what wound up ultimately happening here is that Alexandre did what was practical because despite the feedback, the resulting changes weren't sufficient.  From my perspective some of the feedback was simply ignored. We only have so much time in a day.  I know he's working at a full time job and has a lot going on.  Me- I'm working on commercializing DeOldify.  So we simply can't afford to spend a lot of time on a back and forth that just seems counter-productive.

Remember too- this is all getting broadcast in peoples' inboxes.  It doesn't look great from my perspective, I can tell you that much. It's a bit whiny and petty, honestly.  I'd just take the feedback and try to correct course with that for next time.

",fair warning going blunt stand firm agreement let brutally honest code pull request work u kept giving feedback taking extra time walk even though something could took done bent backwards accommodate believe paying awesome quite frankly wound ultimately happening practical despite feedback resulting sufficient perspective feedback simply much time day know working full time job lot going working simply ca afford spend lot time back forth remember getting broadcast look great perspective tell much bit whiny petty honestly take feedback try correct course next time,issue,positive,positive,positive,positive,positive,positive
549827050,"@alejandrods I believe you don't understand. What part of co-author you don't understand?

You made some contributions, I made as well, we are both owners of this patch/commit, does it matter who opens the pull request? Your name will be in the contributor's list of this project, your name will be in the commit, what else do you want?

I believe that if you can't do what we were asking, why can't we do it by ourselves?
I gave proper credit to you, so stop with this attitude that I don't understand about open source.

Quite frankly, we could have closed your PR without even considering merging it and we can still give up on this CPU idea as it can be a little bit misleading about expectations of what kind of hardware you need to run DeOldify.",believe understand part understand made made well matter pull request name contributor list project name commit else want believe ca ca gave proper credit stop attitude understand open source quite frankly could closed without even considering still give idea little bit misleading kind hardware need run,issue,positive,positive,neutral,neutral,positive,positive
549791684,"I think you completly misunderstand the meaning of Open Source and how to contribute. You are not the best example of reviewer because you delete the pull request in order to do it by yourself, and @jantic, as owner, would have to check that. ",think misunderstand meaning open source contribute best example reviewer delete pull request order owner would check,issue,negative,positive,positive,positive,positive,positive
549509948,Awesome! Will post it in a duplicate thread to help others. It fixed both Spectral norm and self attention errors,awesome post duplicate thread help fixed spectral norm self attention,issue,positive,positive,positive,positive,positive,positive
548792211,"@flip111 you need to wait until is merged, probably in the upcoming days.",flip need wait probably upcoming day,issue,negative,neutral,neutral,neutral,neutral,neutral
548791797,"That's great, thank you. I would like to use the cpu option then, where can
I find the patch?

On Fri, Nov 1, 2019, 14:16 Alexandre Vicenzi <notifications@github.com>
wrote:

> @flip111 <https://github.com/flip111> no, you need a more capable GPU to
> use it.
>
> If you want only to colorize we have a patch to use CPU only, but you will
> need at least 8GB of ram on your computer.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/19?email_source=notifications&email_token=AARD7AAJNW7CH2W4PB3HPKLQRQTZDA5CNFSM4GB6WAP2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEC24M7I#issuecomment-548783741>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AARD7AGZ2IFAWUSAAVHPUV3QRQTZDANCNFSM4GB6WAPQ>
> .
>
",great thank would like use option find patch wrote flip need capable use want colorize patch use need least ram computer reply directly view,issue,positive,positive,positive,positive,positive,positive
548783741,"@flip111 no, you need a more capable GPU to use it.

If you want only to colorize we have a patch to use CPU only, but you will need at least 8GB of ram on your computer.",flip need capable use want colorize patch use need least ram computer,issue,negative,negative,neutral,neutral,negative,negative
548779023,Are there any parameters that can be tweak to reduce memory usage? Unfortunately my gpu only has 3 GB of memory,tweak reduce memory usage unfortunately memory,issue,negative,negative,negative,negative,negative,negative
548139163,"@alejandrods I closed because if you look into this PR there are just 2 lines from your previous PR, everything else is new. It was just easier to start from scratch.

Your work is not lost, what was actually meaningful is here.

About the ownership, you are included in the co-author list, you can check [here](https://github.com/jantic/DeOldify/pull/173/commits/7af71bc5e549fa8d8b659cee2f6c9fa0b694c003).",closed look previous everything else new easier start scratch work lost actually meaningful ownership included list check,issue,negative,positive,neutral,neutral,positive,positive
548116230,"I don't understand why you have closed my pull request. Why don't modify my pull request? If you are going to make a new one, my work is lost. I don't understand why are you proceeding on this way. Furthermore, if you make a new pull request, you are the owner of that work, when I have done it.",understand closed pull request modify pull request going make new one work lost understand proceeding way furthermore make new pull request owner work done,issue,negative,positive,neutral,neutral,positive,positive
548114607,"I don't understand why are you going to close the pull request? Why don't modify my pull request? If you are going to make a new one, my work is lost. I don't understand why are you proceeding on this way. Furthermore, if you make a new pull request, you are the owner of that work, when I have done it.",understand going close pull request modify pull request going make new one work lost understand proceeding way furthermore make new pull request owner work done,issue,negative,positive,positive,positive,positive,positive
548084793,@alejandrods I'm going to close this and send a new PR.,going close send new,issue,negative,positive,positive,positive,positive,positive
547966532,"@alexandrevicenzi Can you make this change, please? Actually, I'm not sure how to do it. When it's ready I would change the other changes requested.

",make change please actually sure ready would change,issue,positive,positive,positive,positive,positive,positive
547864299,"- I have updated my branch with the last changes from @alexandrevicenzi, I have discarded the changes in `loss.py`. 
- I have built a function for setting cpu/gpu in a folder called `utils`. 
- I have refactored the implicit switch and it's explicit now (based on the cpu/gpu function)

I will refactor the code to pass Travis",branch last built function setting folder implicit switch explicit based function code pas travis,issue,negative,neutral,neutral,neutral,neutral,neutral
547175386,"@jantic based on you comment on #168 about changs on `loss.py` I tracked down where `FeatureLoss` was being used for inference and it was because `gen_loss` had it as a default value, in this case, being loaded even if not in use.

This shouldn't cause any harm to training notebooks.",based comment tracked used inference default value case loaded even use cause harm training,issue,negative,neutral,neutral,neutral,neutral,neutral
546940365,"@alexandrevicenzi got it, thank you. As soon as the cpu PR is merged, I will prepare a new specific Dockerfile.",got thank soon prepare new specific,issue,negative,positive,neutral,neutral,positive,positive
546907803,"@alejandrods @loretoparisi this so-called app.py is not even part of DeOldify and is scheduled to be removed from this repository, in this case, don't touch it because we will happily throw all your changes away in the future.

@loretoparisi if you want a CPU Docker image just create your own and use DeOldify as a dependency/submodule on your project.

",even part removed repository case touch happily throw away future want docker image create use project,issue,positive,positive,positive,positive,positive,positive
546849938,"@alejandrods thank you, yes it would be great. Looking at the `Dockerfile` and `Dockerfile-api` are both: `FROM nvcr.io/nvidia/pytorch:19.04-py3` that is devised for gpu: `https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/rel_19-09.html#rel_19-09`.

In fact running an image, like the notebook without `nvidia-docker` you will get an assertion error:

```
AssertionError: 
Found no NVIDIA driver on your system. Please check that you
have an NVIDIA GPU and installed a driver from
http://www.nvidia.com/Download/index.aspx
Exception ignored in: <bound method FeatureLoss.__del__ of FeatureLoss()>
Traceback (most recent call last):
  File ""/data/deoldify/loss.py"", line 43, in __del__
    self.hooks.remove()
  File ""/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 537, in __getattr__
    type(self).__name__, name))
AttributeError: 'FeatureLoss' object has no attribute 'hooks'
```",thank yes would great looking fact running image like notebook without get assertion error found driver system please check driver exception bound method recent call last file line file line type self name object attribute,issue,positive,positive,positive,positive,positive,positive
546731333,"@loretoparisi No, I didn't update it. I had updated app.py and app-video.py but @alexandrevicenzi told me that this isn't unnecessary. Tell me if you need it, I can update them :) ",update told unnecessary tell need update,issue,negative,negative,negative,negative,negative,negative
546603089,@alejandrods amazing PR!!! Did you update the Dockerfile as well to run in CPU?,amazing update well run,issue,positive,positive,positive,positive,positive,positive
546255696,@alejandrods be patience. I'm waiting @jantic review as well.,patience waiting review well,issue,negative,neutral,neutral,neutral,neutral,neutral
545835922,"Squash and rebase please, it's hard to review this way, too many unrelated changes are appearing in git diff.

You can also give maintainers permissions to edit your PR, in this case we can fix any conflicts.",squash rebase please hard review way many unrelated git also give edit case fix,issue,negative,positive,positive,positive,positive,positive
545789330,"@alejandrods What we have been trying to say is, if the user has no GPU the notebook should FAIL as it's now, unless the user configures notebooks to use CPU.

We don't want the notebook to assume what is going to be used (CPU or GPU), we want the user to choose it, manually.

Also, please, squash and rebase your patch, there are unrelated changes on it from some bad merge.",trying say user notebook fail unless user use want notebook assume going used want user choose manually also please squash rebase patch unrelated bad merge,issue,negative,negative,negative,negative,negative,negative
545614925,"@alexandrevicenzi Using Stable Model (on the same laptop, MacBook Pro):

- Import `get_image_colorizer' takes 12 s (average).
![image](https://user-images.githubusercontent.com/32141606/67429757-1d53f900-f5e1-11e9-9260-f6780217f45c.png)

- Inference takes 25 s (average)
![image](https://user-images.githubusercontent.com/32141606/67429772-23e27080-f5e1-11e9-9a0f-4d29f7173101.png)

On Video:
- Inference takes 4 minutes (average)
![image](https://user-images.githubusercontent.com/32141606/67429792-2cd34200-f5e1-11e9-866b-e2dbdfac83e8.png)


On the other hand, CPU is not chosen by default. We checked if CUDA exists, if we don't find CUDA, we use CPU. But if we detect GPU, we use GPU. So, GPU is by default.

```
if torch.cuda.is_available():
    os.environ['CUDA_VISIBLE_DEVICES']='0'
    torch.backends.cudnn.benchmark=True
```

Thank you! Let me know with your comments! :) ",stable model pro import average image inference average image video inference average image hand chosen default checked find use detect use default thank let know,issue,positive,negative,negative,negative,negative,negative
545484893,"@jantic there's a need to enable Travis CI on this repo for these checks to work, you as the owner should enable it.

Also, you as the main contributor to this repo will be required to follow this, sorry for that, but you can easily [enable Black on your editor](https://black.readthedocs.io/en/stable/editor_integration.html).

If you don't want to enable on your editor you can enable a git hook with:

```sh
pip install -r requirements-dev.txt
pre-commit install
```

> The git hook is not required if Black is enabled on code editor.

As DeOldify is getting more external contributions I believe that these checks ensure at least code quality, we still need to find a way on how to test PR in an automated way.",need enable travis work owner enable also main contributor follow sorry easily enable black editor want enable editor enable git hook sh pip install install git hook black code editor getting external believe ensure least code quality still need find way test way,issue,positive,negative,neutral,neutral,negative,negative
545392291,"@alejandrods what we are trying to say is that CPU should not be chosen by default, it must be explicitly asked by the user to be used.

About your model, try to run on Stable model and check if you achieve the same performance, and also do a try on Video.",trying say chosen default must explicitly user used model try run stable model check achieve performance also try video,issue,negative,neutral,neutral,neutral,neutral,neutral
545347958,"Hi, thanks for your comments. :) 

First of all, I think that we don't have to suppose that every person has a GPU or every person want to run the code on GPU. Because, maybe, many people want to run the code in a CPU Instance on AWS or something like that (since it's cheaper) or they just don't have GPU. We should include both options. 

I don't understand the point: ""I may have a dual GPU system (true for anyone with CPU integrated graphics)."", basically for 2 reasons:
   - If you have Dual GPU system, you have to configure your code to run it in parallel, on both GPU, it's not by default. (Maybe this can be a point to improve the code).
   - CUDA, actually, doesn't support the integrated graphics of CPU, like Intel Iris Plus Graphics 640 or something like that.... Well, you can use integrated graphics with CUDA but you have to use other tools (OpenCL, ...) but they doesn't work well. Finally, to use GPU on an effective way you have to have a GPU with a computation power greater than 3. https://developer.nvidia.com/cuda-gpus

@jantic, you are right, `loss.py` is not used on inference but the class is initialized when we call `visualize.py`. Actually, if we don't insert the cpu funcionality in `loss.py` we get the next error:

```
---------------------------------------------------------------------------
AssertionError                            Traceback (most recent call last)
<ipython-input-2-e79761dd73f8> in <module>
----> 1 from deoldify.visualize import *
      2 plt.style.use('dark_background')

~/Documents/My Projects/DeOldify/deoldify/visualize.py in <module>
      5 from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas
      6 from .filters import IFilter, MasterFilter, ColorizerFilter
----> 7 from .generators import gen_inference_deep, gen_inference_wide
      8 from tensorboardX import SummaryWriter
      9 from scipy import misc

~/Documents/My Projects/DeOldify/deoldify/generators.py in <module>
     14       return learn
     15 
---> 16 def gen_learner_wide(data:ImageDataBunch, gen_loss=FeatureLoss(), arch=models.resnet101, nf_factor:int=2)->Learner:
     17     return unet_learner_wide(data, arch=arch, wd=1e-3, blur=True, norm_type=NormType.Spectral,
     18                         self_attention=True, y_range=(-3.,3.), loss_func=gen_loss, nf_factor=nf_factor)

~/Documents/My Projects/DeOldify/deoldify/loss.py in __init__(self, layer_wgts)
     11 
     12         # if torch.cuda.is_available():
---> 13         self.m_feat = models.vgg16_bn(True).features.cuda().eval()
     14         # else:
     15         #     self.m_feat = models.vgg16_bn(True).features

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/nn/modules/module.py in cuda(self, device)
    303             Module: self
    304         """"""
--> 305         return self._apply(lambda t: t.cuda(device))
    306 
    307     def cpu(self):

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    200     def _apply(self, fn):
    201         for module in self.children():
--> 202             module._apply(fn)
    203 
    204         def compute_should_use_set_data(tensor, tensor_applied):

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/nn/modules/module.py in _apply(self, fn)
    222                 # `with torch.no_grad():`
    223                 with torch.no_grad():
--> 224                     param_applied = fn(param)
    225                 should_use_set_data = compute_should_use_set_data(param, param_applied)
    226                 if should_use_set_data:

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/nn/modules/module.py in <lambda>(t)
    303             Module: self
    304         """"""
--> 305         return self._apply(lambda t: t.cuda(device))
    306 
    307     def cpu(self):

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/cuda/__init__.py in _lazy_init()
    190             raise RuntimeError(
    191                 ""Cannot re-initialize CUDA in forked subprocess. "" + msg)
--> 192         _check_driver()
    193         torch._C._cuda_init()
    194         _cudart = _load_cudart()

~/Documents/env/deoldify_cpu/lib/python3.7/site-packages/torch/cuda/__init__.py in _check_driver()
     93 def _check_driver():
     94     if not hasattr(torch._C, '_cuda_isDriverSufficient'):
---> 95         raise AssertionError(""Torch not compiled with CUDA enabled"")
     96     if not torch._C._cuda_isDriverSufficient():
     97         if torch._C._cuda_getDriverVersion() == 0:

AssertionError: Torch not compiled with CUDA enabled
```

I have the next MacbookPro: 

![image](https://user-images.githubusercontent.com/32141606/67369593-97e92e00-f579-11e9-908f-fdcaa684b5a5.png)

So I don't have a huge gamer computer, I have run the CPU inference on this computer and I got a good performance. I have done some benchmark on `ImageColorizer.ipynb`:

- Import `get_image_colorizer' takes 5.2 s (average)
![image](https://user-images.githubusercontent.com/32141606/67376763-f667d980-f584-11e9-9470-bb484e8fdcf4.png)

- Inference takes 27 s (average)
![image](https://user-images.githubusercontent.com/32141606/67376912-2fa04980-f585-11e9-8d16-7b4410624c50.png)

I have used these models:
https://www.dropbox.com/s/zkehq1uwahhbc2o/ColorizeArtistic_gen.pth?dl=0

I will reverse the changes I made in app.py and app-video.py

Thank you guys! :) 👍 
 ",hi thanks first think suppose every person every person want run code maybe many people want run code instance something like since include understand point may dual system true anyone graphic basically dual system configure code run parallel default maybe point improve code actually support graphic like iris plus graphic something like well use graphic use work well finally use effective way computation power greater right used inference class call actually insert get next error recent call last module import module import import import import import module return learn data learner return data self true else true self device module self return lambda device self self self module tensor self param param lambda module self return lambda device self raise forked raise torch torch next image huge computer run inference computer got good performance done import average image inference average image used reverse made thank,issue,positive,positive,positive,positive,positive,positive
545333251,"@jantic, @alejandrods may I ask what model you tried to run? Perhaps that's what made all the difference.

In my case I tried to run Stable model.",may ask model tried run perhaps made difference case tried run stable model,issue,negative,neutral,neutral,neutral,neutral,neutral
545194149,"As @jantic pointed out, `app.py` and `app-video.py` might not even be changed, as these files are not into core functionality (or even part) of DeOldify and not used by any of the notebooks.

The people using these files have no interest in running on CPU I believe.

I also agree with @jantic, we can merge this, but only if the user can explicitly ask for CPU, and should also be included in the README what kind of CPU/Memory requirements are needed as many won't be able to run on their ""Walmart"" laptop/pc.",pointed might even core functionality even part used people interest running believe also agree merge user explicitly ask also included kind many wo able run,issue,positive,positive,positive,positive,positive,positive
545187914,This looks great!  I went through every single line of code that was changed just to make sure it was just formatting.  Thanks and good call on that. ,great went every single line code make sure thanks good call,issue,positive,positive,positive,positive,positive,positive
545172105,"So... I tried running this on my own computer that has a Threadripper (16 core/32 thread) and it ran CPU inference quite fast actually (2-3 seconds).  I was quite pleasantly surprised actually as I swear it used to take a lot longer. So if you have a beastly machine like mine then having CPU as an option is great as you can get past the GPU memory limitations (I've done this before for this purpose).

But that's a Threadripper.  That's an extreme case. @alexandrevicenzi has reported he's getting much worse performance on a more normal setup and I believe him.

Relatedly, he has made some excellent arguments as to why we should stick with being on GPU by default (even if there isn't any detected by PyTorch).  I totally agree that GPU should be considered selected by default until CPU is explicitly selected by the user. It's indeed convenient in a lot of ways to be implicit but at the same time it allows for the potential for needless confusion. I'd have it fail loudly if the default assumption of GPU being available isn't true yet GPU is selected.

I reviewed the changes and one thing we'll need changed is that loss.py shouldn't be altered like that for the cpu functionality, as loss functions are only used for training and training is only done on the gpu practically speaking. They're not actually used for inference.  My solution if somehow you're finding these getting passed around for inference would be to just pass another loss function like L1 that doesn't use GPU.  That being said I'm already seeing that being done in the code that instantiates inference instances so I'm not sure what ever motivated this change in the first place....

I'd also be reluctant to have this altered line included in app.py:
`port = 9000 `

That used to be 5000 and this code is being used in production by at least one group that I know of.

With those changes made I'll be happy with this personally.  Thanks!",tried running computer thread ran inference quite fast actually quite pleasantly actually swear used take lot longer beastly machine like mine option great get past memory done purpose extreme case getting much worse performance normal setup believe made excellent stick default even totally agree considered selected default explicitly selected user indeed convenient lot way implicit time potential needle confusion fail loudly default assumption available true yet selected one thing need like functionality loss used training training done practically speaking actually used inference solution somehow finding getting around inference would pas another loss function like use said already seeing done code inference sure ever change first place also reluctant line included port used code used production least one group know made happy personally thanks,issue,positive,positive,positive,positive,positive,positive
545150205,Yeah I'm going to sound a bit mean here. But I'm  just going to lay down the law here and say we're not here to debug your own projects where you're doing something that we didn't code for. This is going to be a pretty simple issue to address if you just take the time to look at what the code's doing.  Not our job.,yeah going sound bit mean going lay law say something code going pretty simple issue address take time look code job,issue,positive,positive,neutral,neutral,positive,positive
545085829,"There's clearly a reason to use GPU, sorry if I went into the hype train about running DeOldify on CPU, but for me, this patch is a no go.

I applied your patch locally and tried to run as promised, but I was expecting something to actually work.

I don't have an outrageous PC, just an i7 8gen with 16 GB of ram, but this thing can't run on CPU.

First, it froze for about 10 minutes trying to load the model, CPU 100%, all 16gb gone and fan spinning like hell. Well, it managed to load the model :tada:, then I tried to actually run a colorization, again, the i7 started to cry, for a lot more time than loading the model, I actually had to hard reset my PC after 15 min of a crazy spinning fan because it was frozen and nothing was happening.

If you come to me and says that it runs on a Xeon E5 with 128 GB of ram I believe you. Would I want to advertise that DeOldify runs on a CPU like that? Not really, I would rather ask people to buy a GTX1080, is just cheaper and performs better probably.

We need to keep in mind that most of the people who actually run DeOldify or they either have a GPU or they will be happy running on Colab for free, not everyone has a crazy gamer setup.

Even though I really want to see DeOldify running on CPU and embedded devices as Coral/Jetson Nano, I believe we are a bit far from that yet.

Unless you point me out something that I did wrong when running your patch, the performance is not even close to reasonable.

What I want to spend time on is to get DeOldify running (not training) on less powerful GPUs, like 1050TI, which is a very common budget GPU for many (including me).",clearly reason use sorry went train running patch go applied patch locally tried run something actually work outrageous gen ram thing ca run first froze trying load model gone fan spinning like hell well load model tried actually run colorization cry lot time loading model actually hard reset min crazy spinning fan frozen nothing happening come ram believe would want advertise like really would rather ask people buy better probably need keep mind people actually run either happy running free everyone crazy setup even though really want see running believe bit far yet unless point something wrong running patch performance even close reasonable want spend time get running training le powerful like ti common budget many,issue,positive,negative,neutral,neutral,negative,negative
545067073,"One thing popped in my mind and I would like to hear @jantic opinions.

This change is introducing a behavior change because now is implicit what is being used.
I personally don't like this implicit approach over an explicit one.

Let's take this scenario:

I may have a dual GPU system (true for anyone with CPU integrated graphics).
If for some reason I misconfigure something in my setup, DeOldify will just assume no GPU is available and use CPU, which might not be desired.

Whoever is running DeOldify might not even notice that and think that is using GPU when a CPU is actually being used.

Things will get a little more complicated when people start to raise issues saying that the model has some misbehavior or similar because they were using CPU instead of GPU.

For this case, I would include some parameter somewhere, we only use CPU if the user really wants to and set this parameter.",one thing mind would like hear change behavior change implicit used personally like implicit approach explicit one let take scenario may dual system true anyone graphic reason something setup assume available use might desired whoever running might even notice think actually used get little complicated people start raise saying model misbehavior similar instead case would include parameter somewhere use user really set parameter,issue,positive,positive,neutral,neutral,positive,positive
544954320,"Overall LGTM, but I'll do a test run later to see if actually works as expected.",overall test run later see actually work,issue,negative,neutral,neutral,neutral,neutral,neutral
544953177,"Training notebooks are not built for Colab, but for your local computer.

Perhaps you need to do some tinkering with paths to get it working.",training built local computer perhaps need get working,issue,negative,neutral,neutral,neutral,neutral,neutral
544404264,"@jantic This patch has only code format now, nothing should break.",patch code format nothing break,issue,negative,neutral,neutral,neutral,neutral,neutral
544403445,You also need to rebase you patch and fix all conflicts.,also need rebase patch fix,issue,negative,neutral,neutral,neutral,neutral,neutral
544288093,I have make my pull request some minutes ago. I'm waiting for review,make pull request ago waiting review,issue,negative,neutral,neutral,neutral,neutral,neutral
544137611,"@jantic I just reverted all import changes, I'll look into that later, for now I will only format the code.",import look later format code,issue,negative,neutral,neutral,neutral,neutral,neutral
541847329,"I think this is a reasonable argument against my proposal. It should be made clear though for users of the exposed API (like [VideoColorizer#colorize_from_file_name](https://github.com/jantic/DeOldify/blob/e01dd6b4cdc11f512e319e090604b81d4860a948/deoldify/visualize.py#L176)), e.g. by including the term ""mp4"" in the method name and/or adding instructions to the wiki.",think reasonable argument proposal made clear though exposed like term method name,issue,negative,positive,positive,positive,positive,positive
541004002,"Thanks for the fast response @jantic. Yes, I am planning to convert to CoreML. I will try and give feedback.",thanks fast response yes convert try give feedback,issue,positive,positive,positive,positive,positive,positive
540882832,"You're in luck.  I've already set up an Onnx export for my unreleased work.  I won't release the entire export but I'll tell you what you need to do.

1.  Remove Spectral Norm after loading the model.  This is an unsupported operator in ONNX, and is probably why you're seeing the error. Here's a code snipped for you- just pass in the model:

```
def remove_all_spectral_norm(item):
    if isinstance(item, nn.Module):
        try:
            nn.utils.remove_spectral_norm(item)
        except Exception:
            pass
        
        for child in item.children():  
            remove_all_spectral_norm(child)

    if isinstance(item, nn.ModuleList):
        for module in item:
            remove_all_spectral_norm(module)

    if isinstance(item, nn.Sequential):
        modules = item.children()
        for module in modules:
            remove_all_spectral_norm(module)
```

Full list of supported Onnx operators are here:
https://github.com/onnx/onnx/blob/master/docs/Operators.md

2. If that still doesn't work, you may need the latest PyTorch (not sure on this). They've made quite a few updates to Onnx export suport lately. This will cause problems with the DeOldify however (not 100% compatible) so you may need to make a separate Conda environment to make this convenient and not interfere with your working DeOldify for training/etc.

3.  Keep in mind that if you're looking to export this Onnx model to something else like CoreML, you're bound to run into problems there as well.  I'd only recommend trying to target iOS 13 in that case.  Tensorflow? That's going to be tough too from the looks of it (haven't dug too deep but I know there's unnecessary difficulties).",luck already set export unreleased work wo release entire export tell need remove spectral norm loading model unsupported operator probably seeing error code pas model item item try item except exception pas child child item module item module item module module full list still work may need latest sure made quite export lately cause however compatible may need make separate environment make convenient interfere working keep mind looking export model something else like bound run well recommend trying target case going tough dug deep know unnecessary,issue,positive,positive,neutral,neutral,positive,positive
540707534,"> @alejandrods any updates on this?

Yeah, I would try to upload the code the next week. ",yeah would try code next week,issue,negative,neutral,neutral,neutral,neutral,neutral
540529540,"@jantic I started a [wiki page](https://github.com/jantic/DeOldify/wiki) with common questions, I'll try to add more topics and if you have anything feel free to add as well.",page common try add anything feel free add well,issue,positive,positive,neutral,neutral,positive,positive
540523199,"@hfarazi, @jantic  I created a [wiki page](https://github.com/jantic/DeOldify/wiki/Retrain-from-scratch) with this information for future reference.

Anyone can add more details there if needed.",page information future reference anyone add,issue,negative,neutral,neutral,neutral,neutral,neutral
540477966,"On a side note, MP4 is a standard pretty much everywhere, and MP4 is just a container, you can change change a video from MKV to MP4 without re-encoding as long they use the same coded.

H264 and H265, or any other coded is something completely different and should not be misunderstood.

For local files I would suggest to just run ffmpeg by yourself before inputting into the notebook.

We could definitely have these ffmpeg commands in a Wiki.",side note standard pretty much everywhere container change change video without long use something completely different misunderstood local would suggest run notebook could definitely,issue,negative,positive,neutral,neutral,positive,positive
540319147,"This wasn't just a lazy decision actually.  It was quite deliberate. First, the notebooks are meant to be used for videos that have urls (convenient and the most common use case so far). So the video generation just picks .mp4 to keep things simple.  The ingested videos are thus 100% consistent in terms of how they're encoded, etc.  This is actually really important in minimizing maintenance and keeping things working 100%. And that's basically the big thing I'm aiming for- low maintenance, low bugs, low fuss.  

So I'd say any effort to work on local files more robustly will want to keep this in mind.  The dual edge sword aspect of adding more features to anything is that you also add more bugs (that's inevitable, I'd argue).  ",lazy decision actually quite deliberate first meant used convenient common use case far video generation keep simple thus consistent actually really important maintenance keeping working basically big thing aiming low maintenance low low fuss say effort work local robustly want keep mind dual edge sword aspect anything also add inevitable argue,issue,negative,positive,neutral,neutral,positive,positive
540314472,"Oh an you can retrain from scratch by using the training notebooks (ColorizeTrainingArtistic.ipynb, for example). You'd need to download whatever datasets you want to use (I'd recommend big like imagenet) and extracting it to the directory the notebook is looking at. The path in the notebook by default is data/imagenet/ILSVRC/Data/CLS-LOC (relative to where you're running the jupyter instance from which is assumed to be the same folder as your notebook).  That folder should contain a train and val folder, and inside those contains a folder for each class of objects, and inside each of those are the original color photos themselves.  Once you have those, then just run the notebook and follow the instructions.",oh retrain scratch training example need whatever want use recommend big like directory notebook looking path notebook default relative running instance assumed folder notebook folder contain train folder inside folder class inside original color run notebook follow,issue,positive,positive,positive,positive,positive,positive
540313127,I do have an idea but i can't give it away yet.  It's a trade secret for now.  Sorry!,idea ca give away yet trade secret sorry,issue,negative,negative,negative,negative,negative,negative
540312611,"@shadowzoom New version will release when it's complete :) Commercial first before open source. It'll probably be Google Colab again, yes.",new version release complete commercial first open source probably yes,issue,negative,positive,neutral,neutral,positive,positive
540188591,@alexandrevicenzi  I mean this version https://www.youtube.com/watch?v=u9Z3lCmIOlg author sayed that this version still not released yet,mean version author version still yet,issue,negative,negative,negative,negative,negative,negative
540170434,"@shadowzoom improving the quality of images is not what this project aims for, this project is to colorize old images, AKA as black and white images.

Perhaps what you look for is something like [https://letsenhance.io/](https://letsenhance.io/).",improving quality project project colorize old aka black white perhaps look something like,issue,positive,negative,neutral,neutral,negative,negative
540169318,"@shadowzoom what you mean by a new version?

You can find the latest version on Colab and GitHub itself.",mean new version find latest version,issue,negative,positive,positive,positive,positive,positive
537833544,I've already said no dude.  I'm not sure what you think changed between now and then....,already said dude sure think,issue,negative,positive,positive,positive,positive,positive
537831699,"Yes, you are right, but sorry for interruption again. Please, can you help me how to place my data set in your algorithm? another question is video trainer use image datasets?",yes right sorry interruption please help place data set algorithm another question video trainer use image,issue,positive,negative,negative,negative,negative,negative
537753686,"One more thing to note is that ColorizeTrainingVideo.ipynb needs a reference added for this in lieu of the import changes:

from deoldify.augs import *",one thing note need reference added lieu import import,issue,negative,neutral,neutral,neutral,neutral,neutral
537111098,"@jantic can you please test this before merging? It should not break anything, but just to be 100% sure.",please test break anything sure,issue,positive,positive,positive,positive,positive,positive
536909197,"@jantic I want to remove all dev related info from readme, but I'm unsure where you want to be. We could move to GH Wiki or just to a new markdown file in the repo.

The readme would still point out where to find the dev setup information.",want remove dev related unsure want could move new markdown file would still point find dev setup information,issue,negative,positive,neutral,neutral,positive,positive
536809005,"@hammadalibutt  Don't take this the wrong way but I can tell you right now that this won't be the end of the list of questions you'll wind up running into if you're asking me this.  I'm not saying this to be mean but to be practical:  Basic skills (debugging, reading code) are in order first before you can tackle running this training.  I just don't have the time to walk you through all this.",take wrong way tell right wo end list wind running saying mean practical basic reading code order first tackle running training time walk,issue,negative,negative,neutral,neutral,negative,negative
536544702,"I am newbie. please mention how to place data in which format or folder. Please can you explain training process in detail ... or add new readme file or manual. because  i have got an error on this line 
learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))


---------------------------------------------------------------------------
StopIteration                             Traceback (most recent call last)
<ipython-input-22-bbab8ac1f10a> in <module>
----> 1 learn_gen.fit_one_cycle(1, pct_start=pct_start, max_lr=slice(5e-8,5e-5))

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/train.py in fit_one_cycle(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)
     20     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,
     21                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))
---> 22     learn.fit(cyc_len, max_lr, wd=wd, callbacks=callbacks)
     23 
     24 def lr_find(learn:Learner, start_lr:Floats=1e-7, end_lr:Floats=10, num_it:int=100, stop_div:bool=True, wd:float=None):

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in fit(self, epochs, lr, wd, callbacks)
    192         if not getattr(self, 'opt', False): self.create_opt(lr, wd)
    193         else: self.opt.lr,self.opt.wd = lr,wd
--> 194         callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)
    195         if defaults.extra_callbacks is not None: callbacks += defaults.extra_callbacks
    196         fit(epochs, self, metrics=self.metrics, callbacks=self.callbacks+callbacks)

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_train.py in <listcomp>(.0)
    192         if not getattr(self, 'opt', False): self.create_opt(lr, wd)
    193         else: self.opt.lr,self.opt.wd = lr,wd
--> 194         callbacks = [cb(self) for cb in self.callback_fns] + listify(callbacks)
    195         if defaults.extra_callbacks is not None: callbacks += defaults.extra_callbacks
    196         fit(epochs, self, metrics=self.metrics, callbacks=self.callbacks+callbacks)

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/callbacks/tensorboard.py in __init__(self, learn, base_dir, name, loss_iters, hist_iters, stats_iters, visual_iters)
    176                  visual_iters:int=100):
    177         super().__init__(learn=learn, base_dir=base_dir, name=name, loss_iters=loss_iters, hist_iters=hist_iters, 
--> 178                          stats_iters=stats_iters)
    179         self.visual_iters = visual_iters
    180         self.img_gen_vis = ImageTBWriter()

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/callbacks/tensorboard.py in __init__(self, learn, base_dir, name, loss_iters, hist_iters, stats_iters)
     36         self.data = None
     37         self.metrics_root = '/metrics/'
---> 38         self._update_batches_if_needed()
     39 
     40     def _get_new_batch(self, ds_type:DatasetType)->Collection[Tensor]:

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/callbacks/tensorboard.py in _update_batches_if_needed(self)
     48         if not update_batches: return
     49         self.data = self.learn.data
---> 50         self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)
     51         self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)
     52 

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/callbacks/tensorboard.py in _get_new_batch(self, ds_type)
     40     def _get_new_batch(self, ds_type:DatasetType)->Collection[Tensor]:
     41         ""Retrieves new batch of DatasetType, and detaches it.""
---> 42         return self.learn.data.one_batch(ds_type=ds_type, detach=True, denorm=False, cpu=False)
     43 
     44     def _update_batches_if_needed(self)->None:

~/anaconda3/envs/deoldify/lib/python3.7/site-packages/fastai/basic_data.py in one_batch(self, ds_type, detach, denorm, cpu)
    166         w = self.num_workers
    167         self.num_workers = 0
--> 168         try:     x,y = next(iter(dl))
    169         finally: self.num_workers = w
    170         if detach: x,y = to_detach(x,cpu=cpu),to_detach(y,cpu=cpu)

StopIteration: ",please mention place data format folder please explain training process detail add new file manual got error line recent call last module learn learn learn learner fit self self false else self none fit self self false else self none fit self self learn name super self learn name none self collection tensor self return self self collection tensor new batch return self none self detach try next iter finally detach,issue,positive,positive,neutral,neutral,positive,positive
535732426,"This was touched on in this issue here:  https://github.com/jantic/DeOldify/issues/121 .  The short of it is:  It's a fastai bug.   From the issue there's this comment (Onnx is spelled incorrectly...doh!)

> The Onyx error you're getting there is because of a change in fastai's Tensorboard callback plugin that was added to generate a of the model. It's annoying but not going to cause actual breaking issues. I just commented out that stuff myself in the fastai core code.",touched issue short bug issue comment incorrectly onyx error getting change added generate model annoying going cause actual breaking stuff core code,issue,negative,negative,negative,negative,negative,negative
534466728,"Very good.  Thanks for clearing that up.

Edit: no pun intended ",good thanks clearing edit pun intended,issue,positive,positive,positive,positive,positive,positive
533905964,It's an interesting paper and results but it's not really in the spirit of what this project is trying to achieve.  DeOldify is strictly attempting to restore/enhance photographs to make them look like they were taken more recently.  This dehazing paper is actually modifying the appearance of the subject matter itself (altering reality).  ,interesting paper really spirit project trying achieve strictly make look like taken recently paper actually appearance subject matter reality,issue,positive,positive,positive,positive,positive,positive
532916460,"One more thing- I think by default this should be disabled, unless this functionality proves to be highly reliable.",one think default disabled unless functionality highly reliable,issue,negative,negative,neutral,neutral,negative,negative
532916283,"This sounds cool.  It should definitely be kept as an option because it sounds like it could be error prone.  Regardless, knocking out 1/3 to 1/2 frames would make quite a bit of difference due to how long these videos take to process (on the order of many minutes to hours). So yeah, if you want to create a pull request that would be great!",cool definitely kept option like could error prone regardless knocking would make quite bit difference due long take process order many yeah want create pull request would great,issue,positive,positive,positive,positive,positive,positive
532913492,"I can't really answer most of these questions unless I really dig.  This particular code actually comes straight from the fastai library with only minor modification (mine isn't using weight norm though now I look at it the comments still remain that say it does).  So I guess the weight_norm isn't super crucial because I certainly don't see artifacts from using batch norm.  But beyond that I can't tell you what exactly motivated the decisions on this.  For me it ""just works"" (sorry if that's not a great answer but it's honest!).",ca really answer unless really dig particular code actually come straight library minor modification mine weight norm though look still remain say guess super crucial certainly see batch norm beyond ca tell exactly work sorry great answer honest,issue,positive,positive,positive,positive,positive,positive
532828027,"> @jqueguiner I believe that API code does not belongs to DeOldify, it should be a different project, but for now is not possible because DeOldify is not a Python package, I still need to finish this.

ok got it I'll let you create a PR to remove the API from the main project.",believe code different project possible python package still need finish got let create remove main project,issue,negative,positive,neutral,neutral,positive,positive
532112290,"@jqueguiner I believe that API code does not belongs to DeOldify, it should be a different project, but for now is not possible because DeOldify is not a Python package, I still need to finish this.",believe code different project possible python package still need finish,issue,negative,neutral,neutral,neutral,neutral,neutral
526077926,@jantic sounds good. I just wonder if would be useful to create a dropbox folder with all these images.,good wonder would useful create folder,issue,positive,positive,positive,positive,positive,positive
525982470,"I've addressed #2 and updated the readme to reflect the current situation with Git LFS.  I decided to just leave the notebooks as is however (explained in the readme).  Here's what it says:

> The images in this folder have been removed because they were using Git LFS and that costs a lot of money when GitHub actually charges for bandwidth on a popular open source project (they had a billing bug for while that was recently fixed). The notebooks that use them (the image test ones) still point to images in that directory that I (Jason) have personally and I'd like to keep it that way because, after all, I'm by far the primary and most active developer. But they won't work for you. Still, those notebooks are a convenient template for making your own tests if you're so inclined.

This seems like the most practical approach IMHO.  ",reflect current situation git decided leave however folder removed git lot money actually popular open source project billing bug recently fixed use image test still point directory personally like keep way far primary active developer wo work still convenient template making like practical approach,issue,positive,positive,positive,positive,positive,positive
525981131,"Ok I've addressed 2 and 3.  It's actually PyTorch 1.0.1 that we need (I've tested), and the TorchVision requirement is taken care of automatically by the install of PyTorch it appears (it installs 0.2.1).  FastAI's install should have taken care of the PyTorch requirement correctly but it just grabs the latest version.  So now it's patched over in our own install.

Something to point out- FastAI 1.0.51 is also specifically needed.  If you go higher  images will be rendered with weird grid artifacts.  Oye.  

In summary- for all those reading: Please please please just use the conda install.  It'll save you a lot of headache.",actually need tested requirement taken care automatically install install taken care requirement correctly latest version install something point also specifically go higher weird grid reading please please please use install save lot headache,issue,positive,positive,neutral,neutral,positive,positive
525864234,"by mistake I pushed the button ""close and comment"" :-)",mistake button close comment,issue,negative,neutral,neutral,neutral,neutral,neutral
525863401,"I have good news. I reinstalled the DeOldify env and reinstalled all the necessary packeges. Now I have almost the same quality and speed obtained in Linux. But the GPU memory usage in Windows is lower respect to Linux I was able to encode a short HD video (1920x1080) with a render_factor of 55 consuming about 5GB of memory, while in Linux I can use a max render_factor of 38, then I get out of memory. Moreover I cannot see any ""visible"" difference between RF 38 and 55. The situation change if I use ImageColorizer, in this case I can use max render_factor=44. 

These are the images obtained with ImageColorizer (artistic=False).

WINDOWS RF=21
![century20th_w10_RF=21](https://user-images.githubusercontent.com/5787502/63881812-eb535b80-c9d0-11e9-8198-82bbc3a16c2c.jpg)

WINDOWS RF=38
![century20th_w10_RF=38](https://user-images.githubusercontent.com/5787502/63881823-f0b0a600-c9d0-11e9-9c73-42d1ea2a0764.jpg)

WINDOWS RF=44
![century20th_w10_RF=44](https://user-images.githubusercontent.com/5787502/63881832-f4dcc380-c9d0-11e9-9816-5f38b22dcbef.jpg)

This is my env in Windows 10:
[conda-list.txt](https://github.com/jantic/DeOldify/files/3552269/conda-list.txt)


",good news necessary almost quality speed memory usage lower respect able encode short video consuming memory use get memory moreover see visible difference situation change use case use,issue,positive,positive,positive,positive,positive,positive
525329158,Now that sounds rather strange....It looks to me like the code running on Windows there is simply using the default render_factor (there's one).  Are the calls 100% identical in Linux vs Windows?  I suspect there's a subtle difference...,rather strange like code running simply default one identical suspect subtle difference,issue,negative,negative,negative,negative,negative,negative
525156832,"In effect doing more testing I discovered that in Windows the render_factor is not properly managed. The software will produce the same image quality even using a render_factor=256. Not only but the final image in windows is worse, as it is possible to see looking to the images bellow:

WINDOWS 10
https://i.ibb.co/PrDxw7r/windows-96.jpg
LINUX CR=25
https://i.ibb.co/svdgj5j/linux-cr-25-96.jpg
LINUX CR=38
https://i.ibb.co/vP4Fk0X/linux-cr-38-96.jpg",effect testing discovered properly produce image quality even final image worse possible see looking bellow,issue,negative,negative,neutral,neutral,negative,negative
525101875,I've run into this too.  Haven't quite figured it out yet.  This will take time but thanks for reporting it.,run quite figured yet take time thanks,issue,negative,positive,positive,positive,positive,positive
525101470,"1.  There have been issues reported in the past on Windows and I haven't explicitly added support because it's just more to handle and the deep learning community doesn't really pay much attention to it.  So that's a hard sell unless those issues are proven to not longer be issues (search ""Windows"" in Issues).  Basically extra support == extra burden and complication and I tend to say no to that unless there's a compelling case for it.
2.  That sounds great.
3.  That sounds great.

I'll get around to doing these last two in a bit.",past explicitly added support handle deep learning community really pay much attention hard sell unless proven longer search basically extra support extra burden complication tend say unless compelling case great great get around last two bit,issue,positive,positive,positive,positive,positive,positive
524629564,"I own a GTX 1070 with 8GB and I had the same problem. To understand the problem I installed in a dedicated partition Ubuntu 19.04, and I was able to run the notebooks only using the Docker : max memory usage 7.7GB. Looking to Anaconda repo, I discovered the were available older version of pytorch, so I switched back to Windows 10 and using Anaconda I installed these old versions with the command:
       `conda install pytorch=1.1.0 torchvision=0.3.0 cudatoolkit=10.0 -c pytorch`
With these versions I was able to run the notebooks in Windows 10. 
**Now the strange thing**:  In Ubunto the max memory usage (render_factor=25) was about 7.7GB, in Windows 10 only 4GB. The amount of RAM necessary to run the scripts in Windows is much lower respect to Linux (Ubuntu 19.04). I suspect that the render_factor is not properly managed in Windows .      
",problem understand problem partition able run docker memory usage looking anaconda discovered available older version switched back anaconda old command install able run strange thing memory usage amount ram necessary run much lower respect suspect properly,issue,negative,positive,positive,positive,positive,positive
524093482,"What we're lacking primarily I think is code documentation (usage/api).  That would of course be tailored to devs.  Table of contents in README would be a great.  GH Wiki on how to contribute to the project and other such misc items would be great.

README can be the gateway to point users of various backgrounds to where they need to go (again, table of contents sounds like a great way to do this).  But it should be tailored first and foremost to guide non-developers (to Colabs, what the project is about complete with pictures/videos, etc).   That's generally what I'm getting at.",primarily think code documentation would course table content would great contribute project would great gateway point various need go table content like great way first foremost guide project complete generally getting,issue,positive,positive,positive,positive,positive,positive
524087476,"@jantic I understand why the current README is this way, but I'm not sure if I understand what should we do :thinking: 

What should be in Sphinx? and what should be in README?

Based on your ""THAT"" I believe that README is for general users (explaining the why and how), Sphinx only for devs. As we will release on PyPI, I do believe that makes sense to use Sphinx to explain the code and how to use it to build other applications, but I don't think that Sphinx would be good for how to contribute to this project, for this, I believe that GH Wiki would be better.

Maybe, one thing that could help, is a Table of Contents on the README, similar to [pytorch](https://github.com/pytorch/pytorch). Way easier to navigate.",understand current way sure understand thinking sphinx based believe general explaining sphinx release believe sense use sphinx explain code use build think sphinx would good contribute project believe would better maybe one thing could help table content similar way easier navigate,issue,positive,positive,positive,positive,positive,positive
524082027,Agreed on both.  Though I haven't removed the twitter icon from git lfs yet.  A bit silly I know.,agreed though removed twitter icon git yet bit silly know,issue,negative,negative,negative,negative,negative,negative
524081654,"So both @dana-kelley  and I agree that we like the readme as is in a lot of ways (perhaps with some cleanup but not a total overhaul).

1. This project isn't strictly for developers.  There's a lot of general user interest.  The readme is more tailored towards that.
2. The readme is full of graphics and video razzle-dazzle because that works to attract attention (I attribute that to why it got picked up by Boing Boing and went viral in the first place).
3. The readme is tailored to answer the kinds of questions that we will inevitably get because many users are simply not going to go out of their way to find the information elsewhere.  In fact, it's hard enough to ask them to RTFM to begin with and many don't even do that.

That being said, I do think it's a great idea to get Sphinx based documentation in place.  THAT will be tailored towards developers for sure.  ",agree like lot way perhaps cleanup total overhaul project strictly lot general user interest towards full graphic video work attract attention attribute got picked went viral first place answer inevitably get many simply going go way find information elsewhere fact hard enough ask begin many even said think great idea get sphinx based documentation place towards sure,issue,positive,positive,positive,positive,positive,positive
523798533,"@jantic well, the images are still on git history... I have it...

on 1, good idea to loop over files
on 2, we need to remove git lfs from the readme, because it's not available anymore.",well still git history good idea loop need remove git available,issue,positive,positive,positive,positive,positive,positive
523681571,"On 1- I liked having the file names there as an example and because well...selfishly, I still have access to the images.  Keep in mind too- the images do much better with specific render factors and that's why there's value in the hardcoding as opposed to just looping through the images blindly in a directory.

That being said, in practice for 99% of people, having a simple loop through a list of files read dynamically from the folder will probably be much more beneficial so let's do that.  Bonus is that it'll be much less code.

On 2- If it's being removed from the notebooks I don't see any reason to have it clarified in the readme personally.",file example well selfishly still access keep mind much better specific render value opposed looping blindly directory said practice people simple loop list read dynamically folder probably much beneficial let bonus much le code removed see reason personally,issue,positive,negative,neutral,neutral,negative,negative
523641043,"@jantic I understand.

A couple of things should be done:

1. Remove hardcoded file names from notebook if there's no way to get the exact same files
2. Include a note in the readme about this behavior

I have a plan to rework the readme in some sort do documentation and I plan to do this soon.",understand couple done remove file notebook way get exact include note behavior plan rework sort documentation plan soon,issue,negative,positive,positive,positive,positive,positive
523637500,I guess we could just put comment in the ReadMe indicating this.  Yeah sorry...I just put a stop gap on that then went back to work on whatever I was fixated on at the time (probably staring at Tensorboard).,guess could put comment yeah sorry put stop gap went back work whatever time probably staring,issue,negative,negative,negative,negative,negative,negative
523635987,"@jantic I'll send in batches to be easier to review. Also, please test these changes because all I get is ""CUDA out of memory"".",send easier review also please test get memory,issue,positive,neutral,neutral,neutral,neutral,neutral
523633883,"I removed them because a few months ago I started racking up a big bill when GitHub finally fixed a bug in their system where they weren't charging for git lfs data usage (which happened to be introduced back in October, right before I introduced my project!).  So when that happened, I was like ""oh shit"" and just yanked the files out- I can't afford the data for the amount of traffic this repo gets.  I have them in dropbox still, but I wasn't planning on sharing that as a public link. They were there purely as a convenience. ",removed ago racking big bill finally fixed bug system charging git data usage back right project like oh ca afford data amount traffic still public link purely convenience,issue,negative,positive,neutral,neutral,positive,positive
523229679,"No it's great!  I just didn't put the review comment in the right place.  Did it as a normal comment instead.  ""This is great. Tested and reviewed. It's a go. Thanks!""",great put review comment right place normal comment instead great tested go thanks,issue,positive,positive,positive,positive,positive,positive
523226460,@jantic I'll try to do some cleanup or organize these deps in the future.,try cleanup organize future,issue,negative,neutral,neutral,neutral,neutral,neutral
523224077,Sorry.... Didn't go through proper review process on this one.  Doh!,sorry go proper review process one,issue,negative,negative,negative,negative,negative,negative
523221916,This is great.  Tested and reviewed.  It's a go.  Thanks!,great tested go thanks,issue,positive,positive,positive,positive,positive,positive
523166405,"@jantic Let me know your thoughts on this.

I do use this a lot, it's helpful for those who send PRs because they usually don't know who to request for review.",let know use lot helpful send usually know request review,issue,negative,negative,negative,negative,negative,negative
523073170,@alexandrevicenzi Great idea on the master protection.  I've put that in place.  Collaborator invite is sent!,great idea master protection put place collaborator invite sent,issue,positive,positive,positive,positive,positive,positive
522927603,"@jantic about the library I can do that, I have some projects on PyPI and I do similar work on my job as well.

Collaborator access would be great, I would never merge to master directly but you can force master protection on GitHub, so no one will merge to master, unless they are admins.

You may ask why I'm doing this, the main reason now is because I don't have a suitable graphics card to train the network and play with, but I really like this project.",library similar work job well collaborator access would great would never merge master directly force master protection one merge master unless may ask main reason suitable graphic card train network play really like project,issue,positive,positive,positive,positive,positive,positive
522770465,Good call on the misleading text. Thanks!,good call misleading text thanks,issue,negative,positive,positive,positive,positive,positive
522766359,@alexandrevicenzi @jqueguiner I'm totally in support of this.  I personally can't devote much if anytime to this sort of overhaul at this point so this would be something I'd have to trust you with.  It'd make sense to grant collaborator status if you're doing something of this magnitude.  Just let me know if you want that.  I'll still want to do code review and etc but really- I'm prepared to give you the keys and let you do what you think is right.,totally support personally ca devote much sort overhaul point would something trust make sense grant collaborator status something magnitude let know want still want code review prepared give let think right,issue,positive,positive,positive,positive,positive,positive
522715381,Personally I really love the idea but it’s up to Jason to decide !,personally really love idea decide,issue,positive,positive,positive,positive,positive,positive
522706825,"@jantic @jqueguiner what if we get rid of everything that is not DeOldify (code + notebooks) itself from this repo?

The idea would be making DeOldify a library, pip installable. For example `pip install DeOldify`. Everything else, API, apps, go to a spare repo, maybe DeOldifyApp, or any other name.

I believe that this would make more sense as @jantic is not interested in this part.

Also, thinking this way, what if someone wants an Android app, would it fit this repo? Of course not. So, the API might not be in this repo as well.

Making DeOldify a true python lib open new possibilities for a lot of extensions, like APIs, command-line tool, maybe even a desktop app.


",get rid everything code idea would making library pip example pip install everything else go spare maybe name believe would make sense interested part also thinking way someone android would fit course might well making true python open new lot like tool maybe even,issue,positive,positive,positive,positive,positive,positive
522685069,Thé split of API is due to overload of gpu. When having the 2 APIs in the same place the load into GPU being done at the main is allocating a lot of gpu memory which can result in lack of memory of you are ruining this on a small gpu.,split due overload place load done main lot memory result lack memory small,issue,negative,negative,neutral,neutral,negative,negative
522426796,"So that approach sounds great- it sounds a bit labor intensive but if you can do it it would certainly be ideal.  I'd focus on there is diversity of frames (so many different videos with a subset of frames sampled from each).  This is totally spitballing, but I'd think that you'd roughly want to target ImageNet numbers of frames ideally (roughly 1 million):

(1,000,000 frames)/(24 fps)/(3600 seconds in an hour) = about 12 hours of video.  I guess that's how you arrived at that?  If so we're arriving at the same answer then.  It seems to me though you'd really want like a few hundred videos to sample from and that might be the much more important part.  Otherwise you'll be training with too much of overly similar data.",approach bit labor intensive would certainly ideal focus diversity many different subset totally think roughly want target ideally roughly million hour video guess answer though really want like hundred sample might much important part otherwise training much overly similar data,issue,positive,positive,positive,positive,positive,positive
522424101,"- **PEP8/PyLint issues:**  Totally go for it.  These primarily stem from a combination of trying to stick with FastAI conventions (which don't conform 100%) and me being relatively new to Python and/or being in a hurry.  None of it's justified, basically- even FastAI conventions.  I've changed my mind on that.  No need for that anymore.
- **Enabling Black or Pylint + TravisCI**:   That sounds awesome.  If you're up for that, please do!
- **Splitting API not being a good idea**:  That one I'm just not sure about what should be done yet.  I'm definitely not a fan of duplicated code for the most part so that should be addressed for sure.  But I'm not sure what exactly motivated the splitting of the API (you'll want to ask the original author for that one).  Honestly I don't use any of that code and don't pay much attention to it- that's being developed for use over at OVH.

I'm a fan of your work so far so please, go ahead and do your thing!",totally go primarily stem combination trying stick conform relatively new python hurry none even mind need black awesome please splitting good idea one sure done yet definitely fan code part sure sure exactly splitting want ask original author one honestly use code pay much attention use fan work far please go ahead thing,issue,positive,positive,positive,positive,positive,positive
522422091,Great catch- I'm definitely a fan of fail early and loud.  Thanks!,great definitely fan fail early loud thanks,issue,positive,positive,positive,positive,positive,positive
522417056,"@jantic  Many thanks for the detailed answer, and also thanks for the linking of the detailed article. In this case, the ideal approach would be to create test footage, transfer it to VHS and then digitize it again. Thus you get the ideal artefacts, and you don't have to simulate everything first.

But what I'm still wondering is the amount of test data. Would 4 hours of material be enough, or are 12 hours necessary for this? What is your recommendation if you want to use the existing models for this. ",many thanks detailed answer also thanks linking detailed article case ideal approach would create test footage transfer digitize thus get ideal simulate everything first still wondering amount test data would material enough necessary recommendation want use,issue,positive,positive,positive,positive,positive,positive
522225893,"@jantic I understand your point of view. I'll keep my custom Colab just for me in this case, as I want to try both models to see which one gives me better results.",understand point view keep custom case want try see one better,issue,negative,positive,positive,positive,positive,positive
522206895,"@alexandrevicenzi Thanks for the offer!  So believe it or not, the way it is now was a deliberate choice.  It might seem broken to do it this way but the motivation was two-fold:  Avoid having any conditional workflow possibilities in the Colab notebook (to avoid confusion and false reports of bugs), and to not have to download any more data than what was already needed.   If we were to offer the stable model in the Colabs, we'd probably opt to make it a separate notebook.  But then that'd probably also add confusion.",thanks offer believe way deliberate choice might seem broken way motivation avoid conditional notebook avoid confusion false data already offer stable model probably opt make separate notebook probably also add confusion,issue,negative,negative,negative,negative,negative,negative
522206604,"In principle, this can be done with the existing DeOldify models retrained on a different dataset tailored for this problem.  Training X images would be your bad VHS images, and Training Y images would be the ideal versions of those same images (just unaltered normal photographs, probably).  Probably the best way to obtain the bad VHS images (Training X) would be to simulate them starting from good normal images (ImageNet etc) and ""crappifying"" them as described here:  https://www.fast.ai/2019/05/03/decrappify/ .  

The whole business of simulating bad VHS images however isn't trivial, of course.  Perhaps there are filters out there that do that already?  Don't know.  That'd combined with some additional augmentations like blur/noise/warps/contrast/brightness (just like in DeOldify) should all help in training for this task. 

To be clear- I've done super resolution and deblur tasks with the DeOldify model so that's where this confidence comes from.",principle done different problem training would bad training would ideal unaltered normal probably probably best way obtain bad training would simulate starting good normal whole business bad however trivial course perhaps already know combined additional like like help training task done super resolution model confidence come,issue,positive,positive,positive,positive,positive,positive
522017398,"@jantic I got both models on Colab, are you interested in a patch? It's still simple but download extra data.",got interested patch still simple extra data,issue,negative,positive,neutral,neutral,positive,positive
517386087,"Yes, this would happen if you used a link that's not actually directly linking to an image.  For example, if you use an imgur link, you have to be careful.  

This won't work:  https://imgur.com/WOiOMCD

This will:  https://i.imgur.com/WOiOMCD.png

That might seem strange but the former is the link to the page as a whole, and the latter is the actual direct link to the image.  When you go to that link in the first url, you can get the actual image link (second url) by right clicking on the image itself and selecting ""Copy Image Address"" or whatever the equivalent option is that you have.",yes would happen used link actually directly linking image example use link careful wo work might seem strange former link page whole latter actual direct link image go link first get actual image link second right image copy image address whatever equivalent option,issue,negative,positive,neutral,neutral,positive,positive
516535917,Ok since we have a thread for that memory issue I'm going to close this.,since thread memory issue going close,issue,negative,neutral,neutral,neutral,neutral,neutral
516373252,"I unticked read only on the 'models' folder. Seems to have fixed it. Now I get 

RuntimeError: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 6.00 GiB total capacity; 1.09 GiB already allocated; 196.68 MiB free; 27.63 MiB cached)

Using windows 10 and I think I saw another thread for this so will look up now. ",read folder fixed get memory tried allocate mib gib total capacity gib already mib free mib think saw another thread look,issue,positive,positive,positive,positive,positive,positive
514423071,I hit a similar issue when running through some steps of the training. The fix for me was to set num_workers between 0 - 2 and a 96Gb swap space (on NVME). Otherwise the 32Gb of memory I had in my machine would be consumed very quickly.,hit similar issue running training fix set swap space otherwise memory machine would quickly,issue,negative,positive,positive,positive,positive,positive
512625531,"Thx for the update!

On Thu, Jul 18, 2019 at 12:03 AM Jason Antic <notifications@github.com>
wrote:

> Closed #116 <https://github.com/jantic/DeOldify/issues/116>.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/116?email_source=notifications&email_token=AFTOANUUAN4DESCCAHXXXN3P747F7A5CNFSM4HQZ4VNKYY3PNVWWK3TUL52HS4DFWZEXG43VMVCXMZLOORHG65DJMZUWGYLUNFXW5KTDN5WW2ZLOORPWSZGOSRVCNYA#event-2489984736>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFTOANRNQB3YEESI4CDAWK3P747F7ANCNFSM4HQZ4VNA>
> .
>
",update antic wrote closed thread reply directly view mute thread,issue,negative,neutral,neutral,neutral,neutral,neutral
512621629,"@xuancao19 You'll have to unsubscribe to notifications on this GitHub page then.

Look to the right on the comment thread page:  
![image](https://user-images.githubusercontent.com/179759/61421296-56cae900-a8bb-11e9-94fe-fa2bf3540eee.png)
",page look right comment thread page image,issue,negative,positive,positive,positive,positive,positive
512619240,"I don't want to receive your mail again.------------------ 原始邮件 ------------------
发件人: ""Jason Antic""<notifications@github.com>
发送时间: 2019年7月18日(星期四) 凌晨2:58
收件人: ""jantic/DeOldify""<DeOldify@noreply.github.com>;
抄送: ""Subscribed""<subscribed@noreply.github.com>;
主题: Re: [jantic/DeOldify] What card is best for training? (#131)



Closed #131.
 
&mdash;
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub, or mute the thread.",want receive mail antic card best training closed thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
512524924,"In this case you need to make sure to have as much memory as possible.  Otherwise you simply can't train your models effectively (insufficient batch size).  So optimize for that rather than sheer performance.  Second, I've been using 16 bit training on experimental stuff recently but I haven't tackled the issues when running on GANs yet (lots of conversion bugs when switching between 16 bit and 32 bit in fastai unfortunately).  So that's not really an option (to cut down on memory requirements) unless you're prepared to debug and fix the fastai source code.  Finally, I would stick with Nvidia- way better support in the deep learning ecosystem.

That being said, it looks like the good old Nvidia 1080TI is still your best bet (an 11 GB card).  It looks like you may be able to snag it for less than $1000 (on Amazon).  If you can get the 2080 RTX TI and it fits your budget, go for it.  Prices fluctuate so keep that in mind.  

Here's a guide I've found quite helpful:  https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/

",case need make sure much memory possible otherwise simply ca train effectively insufficient batch size optimize rather sheer performance second bit training experimental stuff recently tackled running yet lot conversion switching bit bit unfortunately really option cut memory unless prepared fix source code finally would stick way better support deep learning ecosystem said like good old ti still best bet card like may able snag le get ti budget go fluctuate keep mind guide found quite helpful,issue,positive,positive,positive,positive,positive,positive
512345964,Sorry for the long wait.  So yes- it seems like setting detach=False is beneficial.  It isn't hugely beneficial which might be surprising but regardless it makes sense to do it and it works. I've committed this change.  Thanks! ,sorry long wait like setting beneficial hugely beneficial might surprising regardless sense work change thanks,issue,positive,positive,positive,positive,positive,positive
512343091,This is a fastai thing.  Going to close this and refer you to that.  https://docs.fast.ai/basic_train.html#Discriminative-layer-training,thing going close refer,issue,negative,neutral,neutral,neutral,neutral,neutral
512328820,"This looks like an awesome change.  I don't have time to test it yet but what I'm going to do is merge it then change the notebook to another one that explicitly states it's using this feature (and retain the previous version of ColorizeTrainingStable).  I'll call the new one **ColorizeTrainingStableLargerBatch**.  Reasons being because of the dependency on IBM stuff and the lack of resources on my part to fully verify the training of this new version.  But having this available as an additional option is awesome. I'm really interested in stretching my own limited resources further for sure. 

Thanks!",like awesome change time test yet going merge change notebook another one explicitly feature retain previous version call new one dependency stuff lack part fully verify training new version available additional option awesome really interested limited sure thanks,issue,positive,positive,positive,positive,positive,positive
506549911,"Oh man you're looking at old code (archived elsewhere- see https://github.com/dana-kelley/DeOldify).  That's not even done anymore in the current code.  

That being said, gen_freeze_tos[-1] actually means freezing just the encoder in this case (the resnet backbone).  Then yes, gen_freeze_to[0] is unfreezing everything.  Confusing I know.  It's an old fastai convention.",oh man looking old code see even done current code said actually freezing case backbone yes unfreezing everything know old convention,issue,negative,positive,neutral,neutral,positive,positive
506546770,"gen_freeze_tos=[-1]
that means you only train the  last layer in all layers in genrator 
 gen_freeze_tos=[0]
that means you train all layers in genrator 
that is right or not?
thanks 
",train last layer train right thanks,issue,negative,positive,positive,positive,positive,positive
506527534,"Yes- both freezing initially and then unfreezing later.  This follows practice advocated by fast.ai.  Freezing of the resnet portion is implicit in the creation of the DynamicUnet, so I don't have the code explicitly freezing it currently.  This would probably make the code clearer though.",freezing initially unfreezing later practice freezing portion implicit creation code explicitly freezing currently would probably make code clearer though,issue,negative,neutral,neutral,neutral,neutral,neutral
504224014,"Hi, thank for the reply!

The problem may be caused by ""Dataloader"" in Pytorch. ""fastai"" probably uses it for loading data. etc.

The discussion thread for this is:

https://discuss.pytorch.org/t/cpu-ram-usage-increasing-for-every-epoch/24475/15

Based on my observation, every time when you run ""fit_one_cycle"" or even ""fit"" in an epoch, the main memory consumption is increased until all the memory is allocated. ""gc_collect()"" does not help either.

Since you have a big database, so you can just call ""fit_one_cycle"" or ""fit"" once, so it is likely to be fine. For those who may utilize small database, there may be a need to run the code with multiple epochs . Each epoch adds memory. 

Thanks a lot!
 ",hi thank reply problem may probably loading data discussion thread based observation every time run even fit epoch main memory consumption memory help either since big call fit likely fine may utilize small may need run code multiple epoch memory thanks lot,issue,positive,positive,positive,positive,positive,positive
502840152,So question is- how much RAM do you actually have running this?  I have 128GB which may be why I've never noticed this....,question much ram actually running may never,issue,negative,positive,neutral,neutral,positive,positive
501923516,"So just to be clear- I just wanted to establish a baseline before we go into specifics here, because often there's some ""small detail""  that is actually the key issue, like running on Windows instead of Linux.  That being said, the error message itself is stating that there's a permissions issue with the folder. Regardless of the operating system, you'll have to figure out a way to grant permissions to the the folder if they're not there already for the application.  This is where you'll have to exercise some Google-Fu because this isn't a DeOldify problem but rather an environment specific problem.",establish go often small detail actually key issue like running instead said error message issue folder regardless operating system figure way grant folder already application exercise problem rather environment specific problem,issue,negative,negative,neutral,neutral,negative,negative
501490154,"Unfortunately the conda install can’t be used as I’m running this on a IBM Power9 server with 4 x V100 32GPU’s (ppc64le) and the conda packages have been built for x86 :(

The end result is having to build packages like spaCy etc manually to satisfy the requirements.

It’s bizarre in the fact it will sometimes train the 64px part and then fail at the 128px and then other times fail at the 64px portion.

I will continue digging into things on my end, but if you had any further suggestions that would be great. ",unfortunately install used running power server built end result build like spacy manually satisfy bizarre fact sometimes train part fail time fail portion continue digging end would great,issue,negative,negative,neutral,neutral,negative,negative
501483433,Nvidia driver- yes.  CUDA stuff should come along with the fastai portion of the install.,yes stuff come along portion install,issue,negative,neutral,neutral,neutral,neutral,neutral
501158240,"before I excute 'conda env create -f environment.yml' ,do i need to Install driver and cuda？",create need install driver,issue,negative,neutral,neutral,neutral,neutral,neutral
501111013,I would just use the conda install.  The environment.yml has all the requirements in there (pip install really isn't recommended).  I don't want to encourage anything else.,would use install pip install really want encourage anything else,issue,positive,positive,positive,positive,positive,positive
501100600,"ubuntu18.04 + cuda10.0 + python3.6. I don't use 'conda env create -f environment.yml' ,but use 'pip install -r requirment.txt '. I find this code using torch and what's the version of the torch?",python use create use install find code torch version torch,issue,negative,neutral,neutral,neutral,neutral,neutral
501059344,"Sorry, I could not locate the function. Is it in ""ColorizeTrainingVideo.ipynb""?",sorry could locate function,issue,negative,negative,negative,negative,negative,negative
501055509,"This is happening in the training Jupyter notebooks (conversion from color to black/white) if that's what you're asking:  

```
def create_training_images(fn,i):
    dest = path_lr/fn.relative_to(path_hr)
    dest.parent.mkdir(parents=True, exist_ok=True)
    img = PIL.Image.open(fn).convert('LA').convert('RGB')
    img.save(dest)  
```",happening training conversion color,issue,negative,neutral,neutral,neutral,neutral,neutral
501054841,"Thanks, just wondering where the ""RGB -> YUV -> YYY"" happening in preparing the training dataset? perhaps you have consolidated it into ""Noise Augmented Images"" phase? 
",thanks wondering happening training perhaps consolidated noise augmented phase,issue,negative,positive,positive,positive,positive,positive
501051700,"> Also, where is the ""RGB -> YUV -> YYY"" happening?

There's a function called post_process in the ColorizeFilter class in filters.py that does what you seem to be asking about.

> I did have a test in which ""accuracy_thresh_expand"" value goes up to certain value quickly and stays unchanged afterwards.

As far as accuracy_thresh_expand goes- I think it's very possible that it's simply not getting any more accurate and the number of images correctly classified remains unchanged.  Especially with a set that small (5000) and the fact that the value does go up for a while.  It's the simplest explanation, and it can be validated fairly easily by some manual interventions on the data.

> Not sure about this but I feel that for relatively small dataset, ""Generator"" can boost the accuracy really fast after few epochs.

Is this a theory of yours or is this actually happening in practice?  Sounds like the former given your results.  I've only been training on a much larger dataset (1-2 million images).  I'd suggest if you're trying to working with a small dataset that you start with the ""pretrain only"" weights listed in in the readme and finetune from there.

",also happening function class seem test value go certain value quickly stay unchanged afterwards far think possible simply getting accurate number correctly classified remains unchanged especially set small fact value go explanation fairly easily manual data sure feel relatively small generator boost accuracy really fast theory actually happening practice like former given training much million suggest trying working small start pretrain listed,issue,positive,positive,positive,positive,positive,positive
501047171,"It is almost 5000 images. 

I did have a test in which ""accuracy_thresh_expand"" value goes up to certain value quickly and stays unchanged afterwards. 

Not sure about this but I feel that for relatively small dataset, ""Generator"" can boost the accuracy really fast after few epochs. It is also interesting that the temporary colorized images in ""VideoModel_image_gen"" folder looks pretty good, but it comes to using the same generator model to colorize the entire clip, the output is still pretty much black and white. 

Also, where is the ""RGB -> YUV -> YYY"" happening? the old version used to have a ""transform.py"", is it replaced by some call in ""fastai"" in the new version?

Thanks a lot!",almost test value go certain value quickly stay unchanged afterwards sure feel relatively small generator boost accuracy really fast also interesting temporary folder pretty good come generator model colorize entire clip output still pretty much black white also happening old version used call new version thanks lot,issue,positive,positive,positive,positive,positive,positive
501038307,Well I guess then the question is- just how small is this dataset for testing?  ,well guess question small testing,issue,negative,negative,negative,negative,negative,negative
501034434,"Thanks for the quick reply!

For the test here, for example, in the first epoch - ""Pretrain Citiic"" phase:

-------------------------------------------------------------------------------------------
epoch | train_loss | valid_loss | accuracy_thresh_expand | time
-- | -- | -- | -- | --
0 | 0.316569 | 0.287891 | 0.933468 | 02:21
1 | 0.238969 | 0.245925 | 0.933468 | 02:22
2 | 0.213034 | 0.246162 | 0.933468 | 02:22
3 | 0.213239 | 0.243953 | 0.933468 | 02:23

-------------------------------------------------------------------------------------------

The dataset for testing is pretty small compared to IMAGENET. 

Both loss values are going down in general, but the ""accuracy_thresh_expand"" stay unchanged. 

Thanks a lot!

",thanks quick reply test example first epoch pretrain phase epoch time testing pretty small loss going general stay unchanged thanks lot,issue,positive,positive,positive,positive,positive,positive
500915342,So first question I have to ask- what is the operating system you're running?,first question operating system running,issue,negative,positive,positive,positive,positive,positive
500633945,"Sorry to just get to this now.  The first thing I'd suggest is use the conda install instead of trying to install the latest versions of fastai, tensorboardx, etc.  The conda install specifies specific versions and for good reason:  It's not tested to work with newer packages.  I know for a fact that things keep getting changed in fastai that have the potential to cause issues from time to time (they have for me in the past).  

The Onyx error you're getting there is because of a change in fastai's Tensorboard callback plugin that was added to generate a of the model.  It's annoying but not going to cause actual breaking issues.  I just commented out that stuff myself in the fastai core code.

Finally, have you made any other changes?  Like the loss function, for example?",sorry get first thing suggest use install instead trying install latest install specific good reason tested work know fact keep getting potential cause time time past onyx error getting change added generate model annoying going cause actual breaking stuff core code finally made like loss function example,issue,negative,negative,neutral,neutral,negative,negative
500627903,"accuracy_thresh_expand should definitely change.  This is what it looks like on my current run, for example:
![image](https://user-images.githubusercontent.com/179759/59232933-4a0bf480-8b9b-11e9-9ae3-cbbbc5fe60ce.png)
",definitely change like current run example image,issue,positive,neutral,neutral,neutral,neutral,neutral
500614684,The issue seems to be gone when I restructured the training set. Thanks!,issue gone training set thanks,issue,negative,positive,positive,positive,positive,positive
499399261,"Yes, I have followed step by step, I meet a problem of `authentication` at `docker build -t deoldify_jupyter -f Dockerfile .` even I have login in docker.",yes step step meet problem authentication docker build even login docker,issue,negative,neutral,neutral,neutral,neutral,neutral
499288655,Interpolation in general.  In fact I think it may be more effective to do it pre-colorization.,interpolation general fact think may effective,issue,negative,positive,positive,positive,positive,positive
498947432,"So you're thinking about frame interpolation for colorization, like colorizing a perfect frames and interpolating colorization in between?",thinking frame interpolation colorization like perfect colorization,issue,positive,positive,positive,positive,positive,positive
497638172,"Thank you very much! Looking forward to hear from you the findings!

On Thu, May 30, 2019 at 1:32 PM Jason Antic <notifications@github.com>
wrote:

> It'll take a while to get definitive results (a few days I think). But do
> know I'm taking a look. Thanks again.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/jantic/DeOldify/issues/116?email_source=notifications&email_token=AFTOANWTM7RNSECZBOGN263PX5RG7A5CNFSM4HQZ4VNKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODWRM25Y#issuecomment-497208695>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFTOANWVCNB236CTXJSQTLDPX5RG7ANCNFSM4HQZ4VNA>
> .
>
",thank much looking forward hear may antic wrote take get definitive day think know taking look thanks thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
497585732,"Your best bet on this one is stackoverflow.  This is almost certainly an issue specific to your local so I'm going to close this issue and hopefully you'll be set on the right foot.  I'd start here:  https://stackoverflow.com/questions/50305725/condahttperror-http-000-connection-failed-for-url-https-repo-continuum-io-pk
",best bet one almost certainly issue specific local going close issue hopefully set right foot start,issue,positive,positive,positive,positive,positive,positive
497208695,It'll take a while to get definitive results (a few days I think).  But do know I'm taking a look. Thanks again.,take get definitive day think know taking look thanks,issue,negative,positive,positive,positive,positive,positive
497178014,I think you'll need to be familiar with Pytorch and FastAI first.  Start there!  There's no shortcuts in this case.,think need familiar first start case,issue,negative,positive,positive,positive,positive,positive
497176128,"Interesting finding!  Yeah that was surprising to me too.... This actually came from fastai's DynamicUnet.  I just modified that a bit.  It still does this same thing.

The good news is that having this line:

> layers = [encoder, batchnorm_2d(ni), nn.ReLU(), middle_conv]

Means that there's still some learning going on with the backbone because backpropogation will go back through these layers unimpeded.  I can verify that weights are changing on Tensorboard for the resnet backbone as I train a model now.

To complicate matters:  The detach is apparently not happening in the original implementation of Unet:  https://github.com/fastai/fastai/blob/master/old/fastai/models/unet.py

So yeah I'm guessing it's not intended but I'm not 100% sure, and I'm not sure just how big of a difference it makes.... I'll try a run that doesn't detach now and see what happens....",interesting finding yeah surprising actually came bit still thing good news line ni still learning going backbone go back unimpeded verify backbone train model complicate detach apparently happening original implementation yeah guessing intended sure sure big difference try run detach see,issue,positive,positive,positive,positive,positive,positive
496035925,Thanks a lot @jqueguiner it worked. New to Colab...,thanks lot worked new,issue,negative,positive,positive,positive,positive,positive
496025023,"Try to rerun from scratch the notebook starting from line 1 :

https://colab.research.google.com/github/jantic/DeOldify/blob/master/ImageColorizerColab.ipynb#scrollTo=00_GcC_trpdE&line=2&uniqifier=1
",try rerun scratch notebook starting line,issue,negative,neutral,neutral,neutral,neutral,neutral
495943581,In fact I took and documentary from WWII and half of the images were colorized. A good use case is history documentaries when you have archive videos mixed with historian interviews ! It’s just crazy every day I think deoldify is changing the way we consume media,fact took documentary half good use case history archive mixed historian crazy every day think way consume medium,issue,negative,negative,neutral,neutral,negative,negative
495943486,In my experience Pytorch manage easily the cpu mode by checking the gpu device. If it would have been tensorflow it would have been a bit lore complicated. I’ll look into it in 2 weeks ;-),experience manage easily mode device would would bit lore complicated look,issue,negative,negative,neutral,neutral,negative,negative
495938963,Oh man...I can't believe how wrong the previous implementation was.  What an embarrassment lol.  I tested your fix.  It's great.  Thanks for figuring out that this was happening and submitting this.  Many will be happy!,oh man ca believe wrong previous implementation embarrassment tested fix great thanks happening many happy,issue,positive,positive,positive,positive,positive,positive
495933679,"There is no need to calculate the frame rate. The way to go is just copy it from the source video. For instance, my video the frame rate is '30000/1001', but if you divide and round it will be 30 and this will slightly desynchronise video with the audio. 
",need calculate frame rate way go copy source video instance video frame rate divide round slightly video audio,issue,negative,negative,negative,negative,negative,negative
495478291,"Good points. 
Yeah - you're right about the GPU being the big bottleneck but yes I agree there is room for some parallelization overall.",good yeah right big bottleneck yes agree room parallelization overall,issue,positive,positive,positive,positive,positive,positive
495476982,"Well, the gpu will only accept one image at a time (it's not taking in a batch- they're generally too big for that), and there's only one gpu being shared by all the threads here, so that's the big bottleneck here ultimately.  

That being said, perhaps multiple threads could be used to just preprocess images (loading, etc) to the point where they're ready for the model and then put in a thread safe queue.  The gpu could constantly consume from this queue and spit out results into another queue, which a separate set of threads could batch process post-processing of images coming out of the model.    Adjust design according to what's actually bottlenecking of course (I don't know)- you'll want to use a profiler for that.  In the current state (not speaking of your code) the gpu isn't constantly busy so there's definitely room for improvement.

In this way all three steps could be constantly working and maximizing output.  Now whether all this extra complication is worth it- depends on speed up.  I wouldn't call 30% worth it.  

As far as the progress bar goes- I'd have to bet it's just not threadsafe.  Most things aren't by default, and I've certainly noticed issues with the progress bar myself when parallelizing stuff.",well accept one image time taking generally big one big bottleneck ultimately said perhaps multiple could used loading point ready model put thread safe queue could constantly consume queue spit another queue separate set could batch process coming model adjust design according actually course know want use profiler current state speaking code constantly busy definitely room improvement way three could constantly working output whether extra complication worth speed would call worth far progress bar bet default certainly progress bar stuff,issue,positive,positive,positive,positive,positive,positive
495439353,"Hi,

In ""unet.py"" and ""class UnetBlockWide(nn.Module)"" at line#107

**def forward(self, up_in:Tensor) -> Tensor:**
         s = self.**hook**.stored
         up_out = self.shuf(up_in)
         ssh = s.shape[-2:]
         if ssh != up_out.shape[-2:]:
             up_out = F.interpolate(up_out, s.shape[-2:], mode='nearest')  
        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))
        return self.conv(cat_x)

Also, what is the best practice to set up multiple epochs for 'Repeatable GAN Cycle""?

Thanks!
",hi class line forward self tensor tensor self hook return also best practice set multiple gan cycle thanks,issue,positive,positive,positive,positive,positive,positive
495438507,That does seem to be a common use case from what I've come across.  I'll keep this open for now as it seems like a legit good feature.,seem common use case come across keep open like legit good feature,issue,positive,positive,positive,positive,positive,positive
495438113,"@jqueguiner if you want to try doing that, go for it!  CPU support in general isn't something I'm prioritizing myself just because I'm not making DeOldify for everybody and everything.  CPU support/Windows/etc....I'm fond of the word ""No"".  Now if the submitter of this issue wants to send in a pull request- great!  

I know that having the habit of saying ""No"" makes me harder to get along with but...it really is one the best sanity saving tricks I know of.

So feel free to just say ""No"" and I'll close this accordingly.",want try go support general something making everybody everything word submitter issue send pull great know habit saying harder get along really one best sanity saving know feel free say close accordingly,issue,positive,positive,positive,positive,positive,positive
495437529,"Sorry, where are you seeing hook? Hook is called all over the place.  Hooks == Callbacks.",sorry seeing hook hook place,issue,negative,negative,negative,negative,negative,negative
495340800,"Hi,

I compared the original notebook and the converted python code. The code is the same. 

To provide some details:

----------------------------------------------------------------------------------------------------------------------

DeOldify/fasterai/unet.py:123: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  print ( s.size(), up_in.size(), up_out.size() )
torch.Size([1, 1024, 12, 12]) torch.Size([1, 2048, 6, 6]) torch.Size([1, 512, 12, 12])
torch.Size([1, 512, 24, 24]) torch.Size([1, 512, 12, 12]) torch.Size([1, 512, 24, 24])
torch.Size([8, 256, 48, 48]) torch.Size([1, 512, 24, 24]) torch.Size([1, 512, 48, 48])
Error occurs, No graph saved
torch.Size([1, 1024, 12, 12]) torch.Size([1, 2048, 6, 6]) torch.Size([1, 512, 12, 12])
torch.Size([1, 512, 24, 24]) torch.Size([1, 512, 12, 12]) torch.Size([1, 512, 24, 24])
torch.Size([1, 256, 48, 48]) torch.Size([1, 512, 24, 24]) torch.Size([1, 512, 48, 48])
torch.Size([1, 64, 96, 96]) torch.Size([1, 512, 48, 48]) torch.Size([1, 256, 96, 96])
**torch.Size([1, 1024, 12, 12]) torch.Size([8, 2048, 6, 6]) torch.Size([8, 512, 12, 12])**

----------------------------------------------------------------------------------------------------------------------

This is inside ""class UnetBlockWide(nn.Module)"".

What does ""hook"" do exactly? registering previous graph information? Since the graph was not saved, does it affect ""hook"" then? 

The error does **not** happen to ""crit_data"":

learn_critic = colorize_crit_learner(data=data_crit, nf=256).load(crit_old_checkpoint_name, with_opt=False)


Thanks,
",hi original notebook converted python code code provide converting tensor python integer might cause trace incorrect ca record data flow python value constant future trace might generalize print error graph saved inside class hook exactly previous graph information since graph saved affect hook error happen thanks,issue,positive,positive,neutral,neutral,positive,positive
495331197,For now I didn’t adapt it to Non GPU mode but I’ll try soon however it will be veeeerrrrrryyyyy slllloooow,adapt non mode try soon however,issue,negative,neutral,neutral,neutral,neutral,neutral
495048733,Something's amiss here.... You must have changed something else in the process of moving the code to Python.  I've -never- used a batch size of 1.,something amiss must something else process moving code python used batch size,issue,negative,neutral,neutral,neutral,neutral,neutral
495048089,"Hi,

I had to change it to “1”; otherwise, the the size of two tensors in dimension m#1 would not match. :)

Thanks! 
",hi change otherwise size two dimension would match thanks,issue,negative,positive,positive,positive,positive,positive
495046624,"Did you change bs to 1?  For a number of reasons that would be problematic, even if you didn't run into this bug.  ",change number would problematic even run bug,issue,negative,neutral,neutral,neutral,neutral,neutral
495044808,"Hi,

Thanks for the quick feedback!

I did not change anything except ""bs"" value. For curiosity, I converted the notebook of ""ColorizeTrainingVideo"" to python code, but that should not affect anything.

",hi thanks quick feedback change anything except value curiosity converted notebook python code affect anything,issue,positive,positive,positive,positive,positive,positive
495043657,"You actually have to set batch size when you retrieve your data (which would be the second cell under the 64px heading):
`data_gen = get_data(bs=bs, sz=sz, keep_pct=keep_pct)`

Or the cell under 128px like this:
`learn_gen.data = get_data(sz=sz, bs=bs, keep_pct=keep_pct)`

I'm assuming you're looking at one of the *Training.ipynb notebooks here.

That all being said...I suspect something else changed that's really the issue here.  Did you happen to change image size?  You can't go lower than 64px with the Unet....
",actually set batch size retrieve data would second cell heading cell like assuming looking one said suspect something else really issue happen change image size ca go lower,issue,negative,positive,neutral,neutral,positive,positive
494976058,I'm adding this to bookmarks for options in denoising when I get to that point.  Hopefully soon.  Thanks for bringing that to my attention!,get point hopefully soon thanks attention,issue,positive,positive,positive,positive,positive,positive
494708032,"Yep also saw the bad quality, fixing it",yep also saw bad quality fixing,issue,negative,negative,negative,negative,negative,negative
494700538,"Bonus: You can add a thread parameter to leverage multithreading which leads to a much faster operation. Example:

`final.write_videofile(out, codec='libx264', threads=64, audio_codec='aac', temp_audiofile='temp-audio.m4a', remove_temp=True)`",bonus add thread parameter leverage much faster operation example,issue,positive,positive,positive,positive,positive,positive
494700322,"Sweet! One more thing I should mention. I realized that writing the Video to mpeg4 leads to bad quality video (for some reason). If you change the codec to libx264, the quality is much better!

`final.write_videofile(out, codec='libx264', audio_codec='aac', temp_audiofile='temp-audio.m4a', remove_temp=True)`

",sweet one thing mention writing video bad quality video reason change quality much better,issue,negative,positive,neutral,neutral,positive,positive
493246866,Ohhh...I just realized something.  Your weights file that you're referencing there are from the old version of DeOldify.  Everything has been upgraded.  You'll want to get the latest version!!,something file old version everything want get latest version,issue,negative,positive,positive,positive,positive,positive
493245774,"**Edit- See other reply for update** There really isn't enough information to go off of here to really help you.  Mainly, I can't reproduce the results for any version of the image I found online (big or small).  So a direct url to the source would help in that. Here's what I get for 41 using https://www.ceskaordinace.cz/obrazek-text-6742-310-min.jpg as the source:

![41](https://user-images.githubusercontent.com/179759/57889689-5003f500-77ea-11e9-836e-47778aaf5fd4.png)

And with a more carefully selected render_factor of 14, derived from using the code under ""See how well render_factor values perform on the image here"" in the Colab:

![14](https://user-images.githubusercontent.com/179759/57889693-54301280-77ea-11e9-8201-540cb75a15ed.png)

That being said, I can tell you a few things:
1.  jpeg artifacts have a negative impact on performance for sure.  Looks like I'm seeing that here.
2.  render_factor is important at this stage of research (still in development!).  At this point it's a matter of trial and error (with that tool I described above).  Should it be this way?  No.  But I don't have easy answers on how to do away with that yet.  It probably has a lot to do with training on small square image sizes and not generalizing well in the size dimensions, but the hard part is figuring out how to adequately solve that.",see reply update really enough information go really help mainly ca reproduce version image found big small direct source would help get source carefully selected derived code see well perform image said tell negative impact performance sure like seeing important stage research still development point matter trial error tool way easy away yet probably lot training small square image size well size hard part adequately solve,issue,positive,positive,neutral,neutral,positive,positive
492950406,I'll do another PR for the README.md merge conflict message doesn't make any sens,another merge conflict message make,issue,negative,neutral,neutral,neutral,neutral,neutral
492303132,"There's sufficient documentation for my liking now.  I still am a firm believer in ""self-documenting code"" and don't think every line of code should have comment but should be clear enough to stand on its own in understandability.  If I'm failing on that...then I'll fix the code!",sufficient documentation liking still firm believer code think every line code comment clear enough stand understandability failing fix code,issue,negative,negative,neutral,neutral,negative,negative
492301584,"You have a few options.  If you want to start completely from scratch, then you should train ColorizeTrainingStable.ipynb.  This is stated at the top of ColorizeTrainingVideo.ipynb:

> It's assumed that there's a pretrained generator from the ColorizeTrainingStable notebook available at the specified path.

But this isn't the only way to get a pretrained generator.  You can also download its weights from the readme, in the section that looks like this:

> Pretrain Only Generator Weights
> artistic stable video
> 
> Pretrain Only Critic Weights
> artistic stable video

Download the stable ones if you want to start the ColorizeTrainingVideo.ipynb exactly like you would training ColorizeTrainingStable.ipynb on your own first.  Or you download the video weights to get straight to GAN training and bypass pre-training on noise augmentation.",want start completely scratch train stated top assumed generator notebook available path way get generator also section like pretrain generator artistic stable video pretrain critic artistic stable video stable want start exactly like would training first video get straight gan training bypass noise augmentation,issue,positive,positive,positive,positive,positive,positive
491893834,Oh good catch.  Thank you!,oh good catch thank,issue,positive,positive,positive,positive,positive,positive
491892047,Oh that's a funny mistake.  Thanks for catching it!,oh funny mistake thanks catching,issue,positive,positive,positive,positive,positive,positive
491657762,"Product? - I want to use it as a tool for a start and second, I dunno what you mean by customer, but I wouldve actually paid for this - if that's the irony?",product want use tool start second mean customer actually irony,issue,negative,negative,negative,negative,negative,negative
491657313,It's not a product dude.  And you're not a customer.  End of story.,product dude customer end story,issue,negative,neutral,neutral,neutral,neutral,neutral
491653605,"Honestly, really disappointed by the put down on this - I don't want in any way to learn how to code just so I can colourise a video 'quickly.' Also, no offence - but I'm stressed after hours of attempting this, so I'm not exactly gonna pleased...
",honestly really disappointed put want way learn code video also exactly gon na,issue,negative,negative,negative,negative,negative,negative
491652839,"There's an error in the line above.  Here's the fixed version:

!wget wget https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0 -O ./models/ColorizeStable_gen.pth

",error line fixed version,issue,negative,positive,neutral,neutral,positive,positive
491652495,"Colabs aren't really for people who aren't comfortable with code.  Sorry but it's not made for you. 

Also..... I'm not exactly the type to respond favorably to being talked to like this. Not sure if you needed that reminder but it appears that's the case.",really people comfortable code sorry made also exactly type respond favorably like sure reminder case,issue,positive,positive,positive,positive,positive,positive
491651945,"And non of the listed instructions make sense to me - not sure if that's because I have no coding experience... and clearly, the damb thing works - because I've seen the results and I wouldn't be asking.",non listed make sense sure experience clearly thing work seen would,issue,positive,positive,positive,positive,positive,positive
491651548,This isn't supported guys.  I'm keeping the notebooks as simple as possible to make it so that there's only one way of doing things.  If you want to do rendering with the stable model I'd just suggest installing locally (there's a notebook specifically setup already for stable model colorization).,keeping simple possible make one way want rendering stable model suggest locally notebook specifically setup already stable model colorization,issue,positive,neutral,neutral,neutral,neutral,neutral
491651126,I was also getting the same error but that was after setting 'artistic=False' on the Image Colab notebook. I even made sure I downloaded 'ColorizeStable_gen.pth',also getting error setting image notebook even made sure,issue,negative,positive,positive,positive,positive,positive
491650494,"@gaiar how did you fix this issue? I'm getting an error that says:

FileNotFoundError: [Errno 2] No such file or directory: 'models/ColorizeStable_gen.pth'

after I set 'artistic=False'",fix issue getting error file directory set,issue,negative,neutral,neutral,neutral,neutral,neutral
491649151,"Right, the problem was gone when I update my NVIDIA driver.",right problem gone update driver,issue,negative,positive,positive,positive,positive,positive
491521363,"It looks like you'll have to try to install a later NVidia driver then per the error:

> The NVIDIA driver on your system is too old (found version 9000).
> Please update your GPU driver by downloading and installing a new
> version from the URL: http://www.nvidia.com/Download/index.aspx

I'm using the 410 drivers (open source)",like try install later driver per error driver system old found version please update driver new version open source,issue,negative,positive,neutral,neutral,positive,positive
491520921,"This is something I have no idea how to gracefully integrate into the existing functionality yet. It certainly would be great but the ""how"" is the hard part.",something idea gracefully integrate functionality yet certainly would great hard part,issue,positive,positive,positive,positive,positive,positive
491520699,It's simply easier work with 3 channels in the Unets when the intention is to re-use the Unets in the long term for other image manipulations.,simply easier work intention long term image,issue,negative,negative,neutral,neutral,negative,negative
491222367,"This is what you need:

```
from moviepy.editor import *

# get the original video
videoclip = VideoFileClip('original.mp4')

# extract the audio from video
audioclip = videoclip.audio

# DeOldify video as usual and get the colorized output
video = VideoFileClip('colorized_output.mp4')

# set audio
final = video.set_audio(audioclip)

# save with required settings (mostly to make Quicktime happy and recognize the audio): https://github.com/Zulko/moviepy/issues/820
final.write_videofile(""final_output.mp4"", codec='mpeg4', audio_codec='aac', temp_audiofile='temp-audio.m4a', remove_temp=True)
```
You can look at the options for saving here: https://zulko.github.io/moviepy/ref/VideoClip/VideoClip.html

Quicktime is really annoying and won't recognize the audio so you have to set those codecs for it.

Love the project. Great job! :)

",need import get original video extract audio video video usual get output video set audio final save mostly make happy recognize audio look saving really annoying wo recognize audio set love project great job,issue,positive,positive,positive,positive,positive,positive
491166152,"Yes， I use commands you provided on ""easy install"".
```
git clone https://github.com/jantic/DeOldify.git DeOldify
cd DeOldify
conda env create -f environment.yml
source activate deoldify
jupyter lab
```",use provided easy install git clone create source activate lab,issue,positive,positive,positive,positive,positive,positive
490922950,"So first question is:  Did you use the conda install for DeOldify?  If not, what did you do?",first question use install,issue,negative,positive,positive,positive,positive,positive
490578150,There isn't actually much to be gained here both in terms of performance and in terms of how many people will benefit from this.  Originally I wanted to do it but then later decided that focusing on one card support would be the most effective use of effort.  Reason- having multiple simultaneous experiments turns out to be much more important for productivity.  So if anybody wants to implement this and submit a pull request- great!  But I'm going to close this for now for the reasons above.,actually much performance many people benefit originally later decided one card support would effective use effort multiple simultaneous turn much important productivity anybody implement submit pull great going close,issue,positive,positive,positive,positive,positive,positive
490576212,It's always 3 channels even for gray scale input.  Nothing special beyond that.,always even gray scale input nothing special beyond,issue,negative,positive,positive,positive,positive,positive
490345802,I'll keep this issue open for now but Windows is basically just not a priority right now.  This was an issue reported with the previous version of DeOldify so it's familiar.  ,keep issue open basically priority right issue previous version familiar,issue,negative,positive,positive,positive,positive,positive
490323865,"It's easy for ffmpeg to remux the audio track back from the original one.
Such as
> ffmpeg -i original.video -vn -acodec copy temp.aac
> ffmpeg -i results.video -i temp.aac -codec copy final.video",easy audio track back original one copy copy,issue,positive,positive,positive,positive,positive,positive
489393133,"@RihamHazem Sorry, just saw this now.  I'm saying that GANTrainerSchedule was just burdensome and unnecessary code that got in the way overall, and that it was better to just implement the steps it executed as individual cells in Jupyter. i.e. it's just a refactoring change I'm talking about.  Though, separately, training actually did change quite dramatically in the latest DeOldify.  But these are two separate things.",sorry saw saying burdensome unnecessary code got way overall better implement executed individual change talking though separately training actually change quite dramatically latest two separate,issue,negative,positive,neutral,neutral,positive,positive
489392894,"This is one of those things I'll have to draw a line on with my time and decline to address this or similar questions.  This isn't something you need specifically me or @dana-kelley to answer as this isn't really specific to DeOldify, and it's not something I'm looking to support in the code anymore.  This is a general programming question.  We'd love to help but it really boils down to time management for us.",one draw line time decline address similar something need specifically answer really specific something looking support code general question love help really time management u,issue,positive,positive,positive,positive,positive,positive
489306446,I've used the wrong link. Correct link is `https://www.dropbox.com/s/mwjep3vyqk5mkjc/ColorizeStable_gen.pth?dl=0`,used wrong link correct link,issue,negative,negative,negative,negative,negative,negative
486453736,Okay I've understood the functionality of GANTrainerSchedule but you're saying that you found out that this approach in training doesn't really help in improving the results or I misunderstand you 😅 sorry for asking so much questions. ,understood functionality saying found approach training really help improving misunderstand sorry much,issue,negative,negative,neutral,neutral,negative,negative
486378341,"Oh and the best values for the hyper-parameters, as far as I can tell, are the ones chosen already in the notebooks.  Those came out of a lot of trial and error.",oh best far tell chosen already came lot trial error,issue,negative,positive,positive,positive,positive,positive
486378085,"Thanks!

Unfortunately it's not tuning any hyperparameters for you.  In fact that's precisely the part of this work that's such a pain- there's really a narrow set that actually works with learning rates etc.  There's new work I've been doing that will be revealed next week that alleviates all this thankfully.  

So all the GANTrainSchedule really does for you is it allows you to pass an ordered list of configurations for training.  You're configuring things like batch size, learning rate, etc.  The main motivation of this was to cut down on code, but I've abandoned this approach in the latest iteration.  It doesn't really seem to actually help much in practice (code should evolve like that!)",thanks unfortunately tuning fact precisely part work really narrow set actually work learning new work revealed next week thankfully really pas ordered list training like batch size learning rate main motivation cut code abandoned approach latest iteration really seem actually help much practice code evolve like,issue,positive,positive,positive,positive,positive,positive
486375242,"Thanks a lot for this rapid response and the clear explanation, it really helped me a lot in understanding the architecture of the network. 
I've another question concerning the GANTrainSchedule. Does its purpose mainly is to tune the model's hyper-parameters by trying different values for them? and if so is it available to know what's the best value for those hyper-parameters?

Thanks for sharing such awesome work.",thanks lot rapid response clear explanation really lot understanding architecture network another question concerning purpose mainly tune model trying different available know best value thanks awesome work,issue,positive,positive,positive,positive,positive,positive
485384183,"> We're going to be doing a new release in a little over a week, and there's going to be a brand new Colab there that definitely works. I'll keep this open for now but the gist is I have too much going on right now to address this on the soon to be outdated version of DeOldify.

Thanks so much for the quick reply @jantic ! Glad to hear that. I managed to get it running again with the torch version 1.0.0 from
 `http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl`
and setting the accelerator to cu100",going new release little week going brand new definitely work keep open gist much going right address soon outdated version thanks much quick reply glad hear get running torch version setting accelerator,issue,positive,positive,positive,positive,positive,positive
485348784,"I guess I should clarify:  Not standard U-Net as in the original implementation but standard U-Net as in the basic thrust of what a U-Net does, if that makes sense.  The configuration of number of filters and other such details are different, for sure, but that shouldn't distract from the big picture.",guess clarify standard original implementation standard basic thrust sense configuration number different sure distract big picture,issue,negative,positive,positive,positive,positive,positive
485294037,"1)  The learning rates are constant once set so no they won' change.
2)  I'm not sure sure what you mean by graph.  
3)  gen_freeze_tos is determining whether or not the rensnet34 backbone is trainable.  When set to -1, then that means you're freezing the backbone.  Otherwise, all of the unet is trainable.  ",learning constant set sure sure mean whether backbone trainable set freezing backbone otherwise trainable,issue,negative,positive,positive,positive,positive,positive
485293493,"Well...first of all, based on the way you're posing the question ""how you combined the resnet34 with U-Net"", that's indicating to me that there might be a misunderstanding of what U-Net itself is.  U-Net is half image recognition model (such as resnet), acting as an ""encoder"".  In the case of DeOldify, that resnet34 is already pretrained for free, meaning we get to focus mostly on training the other half- the decoder.  The decoder just takes what the feature detecting encoder ""encoded"", and generates an image from that by making decision on the features indicated by the encoder.  A big part of the magic of U-Net and how it achieves particularly high quality images is the fact that that information loss is minimized by extra ""cross-connections"" from encoder layers to the equivalent decoder layers.  

i.e it's just this diagram, except replace the left half with a resnet.  

https://pics.spark-in.me/upload/98bb30833b833a1ef9199feca3b9ed5c.png

Basically the Generator is a standard U-Net (as described above) plus spectral normalization added to the layers of decoder end.  

",well first based way posing question combined might misunderstanding half image recognition model acting case already free meaning get focus mostly training feature image making decision big part magic particularly high quality fact information loss extra equivalent diagram except replace left half basically generator standard plus spectral normalization added end,issue,negative,positive,positive,positive,positive,positive
485287095,"We're going to be doing a new release in a little over a week, and there's going to be a brand new Colab there that definitely works.  I'll keep this open for now but the gist is I have too much going on right now to address this on the soon to be outdated version of DeOldify.",going new release little week going brand new definitely work keep open gist much going right address soon outdated version,issue,negative,positive,neutral,neutral,positive,positive
484844809,"Can you just explain the architecture used in the generator? and how you combined the resnet34 with U-Net as I didn't manage to understand it from the code.

Thanks in advance.",explain architecture used generator combined manage understand code thanks advance,issue,negative,positive,positive,positive,positive,positive
484746902,"Hi,

Three quick questions:

1). Will the ""c_lrs"" and ""g_lrs"" change inside each schedule? It seems like it is adjusted based on the index of ""szs"" array and ""lrs_unfreeze_factor"".
2). Since the schedule is extendable, the graph is shared as schedule switches from ""size=64"" to ""size=96"",  from ""size=160"" to ""size=192"", and so forth. Is it correct?
3). Would you mind explaining what the role of ""gen_freeze_tos"" is?

Thanks a lot!
",hi three quick change inside schedule like based index array since schedule graph schedule forth correct would mind explaining role thanks lot,issue,positive,positive,positive,positive,positive,positive
482870566,"Now you are making me very curious :-) I will be following your project.
",making curious following project,issue,negative,negative,neutral,neutral,negative,negative
482868019,"@django554  Oops...Yeah so time frames changed a bit.  Sorry about that.  There's actually a set date that we're working with (and we definitely can't kick the can on) to release this stuff once and for all.  That'll be end of April.  Since the Jan 20 comment of mine, things have improved rather drastically.  ",yeah time bit sorry actually set date working definitely ca kick release stuff end since comment mine rather drastically,issue,positive,negative,negative,negative,negative,negative
482863284,Hey @jantic - are you able to post the 'great video stuff' you have mentioned? Thanks!,hey able post video stuff thanks,issue,negative,positive,positive,positive,positive,positive
478832487,"The Singaporean government basically did that actually.  I asked directly about this- they said they used the same DeOldify model, but just trained on more people oriented data.  https://blog.data.gov.sg/bringing-black-and-white-photos-to-life-using-colourise-sg-435ae5cc5036

That being said, I can tell you what else does a great job of getting (much) better results on people- pretrained resnet101 backbone instead of resnet34.  This is actually what I'm using in my next release coming up soon.  You might just want to wait for that.",government basically actually directly said used model trained people data said tell else great job getting much better backbone instead actually next release coming soon might want wait,issue,positive,positive,positive,positive,positive,positive
478288873,"@ChakriMuthyala It turns out that very soon there will be an update to DeOldify that should help you out quite a bit.  The short of it is that you will be able to take the network having been pertained on Imagenet for colorization (which has over a million images), and then fine tune it with the anime images.  I haven't tried this specifically yet but I'm almost certain it should work great.  The fine tuning would first consist of pertaining with non-gan training generator only, then pertaining the critic to do binary classification on generated vs real images.  Finally you do the (no)GAN training to top it off, which will be described in detail in the readme.",turn soon update help quite bit short able take network colorization million fine tune anime tried specifically yet almost certain work great fine tuning would first consist pertaining training generator pertaining critic binary classification real finally gan training top detail,issue,positive,positive,positive,positive,positive,positive
478284421,"So two things to look at here:

First, that's a really small number of images.  The save_iters parameter of the GANTrainer constructor is set to 1000.  Meaning- every 1000 batch iterations it's going to save the model (counted per GANTrainer instance).  Be careful about that number there- it's not images, it's batches of images, so chances are you're not even hitting that number of batches in this case unless you're running many training epochs.  So you can lower this parameter easily but do keep in mind that saves are expensive in this version of DeOldify (new one is coming out soon).

If that's not the issue the only other thing I can think of is that you may just not be looking in the right place for the weights.  The place to look is wherever they're set to in this line in the notebook:

gpath = IMAGENET.parent/(proj_id + '_gen_128.h5')
dpath = IMAGENET.parent/(proj_id + '_critic_128.h5')
",two look first really small number parameter constructor set every batch going save model per instance careful number even number case unless running many training lower parameter easily keep mind expensive version new one coming soon issue thing think may looking right place place look wherever set line notebook,issue,positive,positive,neutral,neutral,positive,positive
477621117,"@xyy-Iv How many training images you've taken, I'm taking ~3k images of anime, is it enough?
@jantic Please suggest!",many training taken taking anime enough please suggest,issue,negative,positive,positive,positive,positive,positive
476036095,"Thanks! There's actually a bunch of merge conflicts that are from me being dumb, so I think the easiest thing for me to do is to is to grab the _noisify function and the noisify transformation wrapper and put it in an augs.py file that I'm creating.  The option to disable pretrained models though- I don't get that.  Any reason?  I'm omitting that part in the meantime.",thanks actually bunch merge dumb think easiest thing grab function transformation wrapper put file option disable get reason part,issue,negative,negative,neutral,neutral,negative,negative
476031068,This is actually already on a TODO list of things to explore that we're maintaining separately.  It's indeed a really promising idea.  I'll keep this open for now- thanks!,actually already list explore separately indeed really promising idea keep open thanks,issue,positive,positive,neutral,neutral,positive,positive
476030732,colorize.ml actually isn't my service and there's others so I don't want to play favorites (I really don't have one!).  So I won't be merging this.  Thanks though.,actually service want play really one wo thanks though,issue,positive,positive,positive,positive,positive,positive
475321661,Based on the short snippet of the stack trace provided that would be the model weights being loaded not matching what the loading model expects.  I'm not going to tackle this yet though because this stuff isn't being published for public consumption yet but rather for cooperating with other developers (it's still in development as a separate branch).  I believe it'll be up in a few weeks.  So I just have to ask for patience on this one. ,based short snippet stack trace provided would model loaded matching loading model going tackle yet though stuff public consumption yet rather still development separate branch believe ask patience one,issue,negative,neutral,neutral,neutral,neutral,neutral
474617512,"This will be revisted when we explicitly add support for Windows (a distant future thing for now).  Windows support currently isn't a priority, especially since Colab notebooks exists. Closing.",explicitly add support distant future thing support currently priority especially since,issue,positive,negative,neutral,neutral,negative,negative
474614218,"This is a nice to have in the way beyond future, to be honest.  Putting in a todo list we're maintaining separately.",nice way beyond future honest list separately,issue,positive,positive,positive,positive,positive,positive
474608445,Part of the upgrade.  Closing time is happy time.,part upgrade time happy time,issue,positive,positive,positive,positive,positive,positive
474608139,I've tried this and it doesn't make much of a difference.  Closing time is happy time!,tried make much difference time happy time,issue,positive,positive,positive,positive,positive,positive
474554057,This is no longer an issue with the upcoming release and it's still documented in the readme.  So going to close this.,longer issue upcoming release still going close,issue,negative,neutral,neutral,neutral,neutral,neutral
474553570,I'm going to close this.  It's not actually worth it if you have a gpu (almost everybody does these days) and there's a Colab notebook that provides the hardware if you don't have it.  ,going close actually worth almost everybody day notebook hardware,issue,negative,positive,positive,positive,positive,positive
474552962,This is on the todo list I have.  Not an issue per se so I'll close this to clean up stuff.,list issue per se close clean stuff,issue,negative,positive,positive,positive,positive,positive
474550636,This isn't an officially released/merged branch yet so I'm going to close this issue.  Rest assured I'll make sure fastai works before the merge :),officially branch yet going close issue rest assured make sure work merge,issue,positive,positive,positive,positive,positive,positive
474150076,"I don't have full context to be sure if I'm correct, but it looks most likely that you're not preprocessing the images in the same way as the open_image function does, which is used in visualize.py.  I'll paste that here for convenience.  So tl;dr- I'd use the open_image function if you aren't already to get the images.

```
def open_image(fn):
    """""" Opens an image using OpenCV given the file path.
    Arguments:
        fn: the file path of the image
    Returns:
        The image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0
    """"""
    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR
    if not os.path.exists(fn) and not str(fn).startswith(""http""):
        raise OSError('No such file or directory: {}'.format(fn))
    elif os.path.isdir(fn) and not str(fn).startswith(""http""):
        raise OSError('Is a directory: {}'.format(fn))
    else:
        #res = np.array(Image.open(fn), dtype=np.float32)/255
        #if len(res.shape)==2: res = np.repeat(res[...,None],3,2)
        #return res
        try:
            if str(fn).startswith(""http""):
                req = urllib.urlopen(str(fn))
                image = np.asarray(bytearray(req.read()), dtype=""uint8"")
                im = cv2.imdecode(image, flags).astype(np.float32)/255
            else:
                im = cv2.imread(str(fn), flags).astype(np.float32)/255
            if im is None: raise OSError(f'File not recognized by opencv: {fn}')
            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        except Exception as e:
            raise OSError('Error handling image at: {}'.format(fn)) from e
```",full context sure correct likely way function used paste convenience use function already get image given file path file path image image format array range raise file directory raise directory else none return try image image else none raise return except exception raise handling image,issue,negative,positive,positive,positive,positive,positive
472157870,"Ok great this helps.  The Pytorch version is not the expected version.  It should be 0.4.1.  I omitted that from requirements.txt and I don't remember why, but really I'd just install requirements using Anaconda.

```
 git clone https://github.com/jantic/DeOldify.git DeOldify
 cd DeOldify
 conda env create -f environment.yml
```

Then you'd start running with:

```
 source activate deoldify
 jupyter lab
```

I'll bet once you install this way you'll have it up and running properly.",great version version remember really install anaconda git clone create start running source activate lab bet install way running properly,issue,positive,positive,positive,positive,positive,positive
471872477,"I create a VirtualEnv for the project , and install dependency with ""pip install -r requirements.txt "" in the environment .

Pytorch version in pip list as following :
torch              1.0.1.post2 
torchtext          0.2.3       
torchvision        0.2.2.post3

Some simple test in pytorch:
![image](https://user-images.githubusercontent.com/31433358/54178786-e574c700-44d1-11e9-9f3f-b4212eee8e12.png)

",create project install dependency pip install environment version pip list following torch post post simple test image,issue,negative,neutral,neutral,neutral,neutral,neutral
471803927,"That's not normal.  First thing I'd do is get nvidia-smi and check if your gpu is actually being hit (I doubt it).  It should take about 1 second per image if the gpu is being hit.  

If it's not being hit- if I remember correctly cuda device settings will be silently ignored and the rendering will go do cpu if gpu is not available (if for example you installed cpu pytorch or something like that).  So that brings me to the question- how did you install everything?",normal first thing get check actually hit doubt take second per image hit remember correctly device silently rendering go available example something like install everything,issue,negative,positive,positive,positive,positive,positive
468984568,Yeah you can't (easily) get the critic caught up to work in the way you want to here.  That'll change with the next update to DeOldify but unfortunately basically the saved generator for now is -just- good for visualization.,yeah ca easily get critic caught work way want change next update unfortunately basically saved generator good visualization,issue,positive,positive,positive,positive,positive,positive
468504470,"Hi,

To resume training based on your pre-trained model, I can just load up the pre-trained ""gen_192"" model in the ""ColorizeTraining.py"", is it correct? 

Without the pre-trained ""_critic_192"" model, it may be hard to reach the same level of result, as the discriminator that starts from nothing needs to catch up with the generator. 

Thanks!",hi resume training based model load model correct without model may hard reach level result discriminator nothing need catch generator thanks,issue,positive,negative,neutral,neutral,negative,negative
468010863,I don't think I'm ready to commit updates to the notebooks yet.  But it's a simple fix- just make sure to change references from fasterai.tensorboard to faster.tensorboard.  Then you may have to get the latest source code from fasta.ai's github repo rather than do the conda install.,think ready commit yet simple make sure change may get latest source code rather install,issue,positive,positive,positive,positive,positive,positive
467979083,"All previous errors has been resolved. 
Now I got a new one. 
```
ModuleNotFoundError: No module named 'fasterai.tensorboard'
```

Here is environment.yml
```
name: coolio
dependencies:
- ffmpeg >= 4.0 
- pip:
  - tensorflow-gpu
  - ipython
  - ipython_genutils
  - opencv-python
  - fastai>=1.0.42
  - tensorboardX>=1.4
  - youtube-dl
  - ffmpeg-python
```",previous resolved got new one module name pip,issue,negative,negative,neutral,neutral,negative,negative
467704367,You should be able to comment out _call_train_loop_hooks() if you're not concerned about Tensorboard functionality.  ,able comment concerned functionality,issue,negative,positive,positive,positive,positive,positive
467699573,"Thanks!

Can I comment out ""_call_train_loop_hooks()"" for now? will it affect training like forward propagation, etc.? 
",thanks comment affect training like forward propagation,issue,positive,positive,positive,positive,positive,positive
467698254,Ok that's helpful information.  I've never done that many epochs with the training.  I'll keep this open for now but this may very well be a non-issue with FastAI v1 upgrade.,helpful information never done many training keep open may well upgrade,issue,positive,positive,positive,positive,positive,positive
467694500,"Hi,

Thanks for the quick response! I actually changed the data directory. 

The training went on nicely until epoch#33 where the error occurred. If I changed the number of epochs from ""50"" to ""100"", the training would stop at epoch#10. That is why I said there may be something dealing with the batch number and number of epochs. I could be wrong though. 

Thanks!
",hi thanks quick response actually data directory training went nicely epoch error number training would stop epoch said may something dealing batch number number could wrong though thanks,issue,negative,positive,positive,positive,positive,positive
467693469,"This might help actually, though it's a bit hard to read.  It looks like you may not have images in the expected directory for imagenet.  Double check where it's looking for that.  Starting from the folder where you're executing from, this would be data/imagenet/ILSVRC/Data/CLS-LOC/train.  i.e. this would be relative to the Jupyter notebooks. ",might help actually though bit hard read like may directory double check looking starting folder would would relative,issue,positive,negative,neutral,neutral,negative,negative
467686773,"Not really. Just run multiple epochs.

Here is the error. I feel like there is something about the data-iterator. What does ""_call_train_loop_hooks()"" do exactly? 

-----------------------------------------------------------------------------------------
Traceback (most recent call last):
  line 242, in _call_train_loop_hooks
    hook_result = hook(gresult, cresult)
  line 70, in train_loop_hook
    tbwriter=self.tbwriter)
  line 138, in output_image_gen_visuals
      self._output_visuals(ds=md.val_ds, model=model, iter_count=iter_count, tbwriter=tbwriter, validation=True)
  line 147, in _output_visuals
      image_sets = ModelImageSet.get_list_from_model(ds=ds, model=model, idxs=idxs)
  line 35, in get_list_from_model
      x,y=ds[idx]
  line 168, in __getitem__
      return self.get1item(idx)
  line 161, in get1item
      x,y = self.get_x(idx),self.get_y(idx)
  line 13, in get_x
      x = super().get_x(i)
  line 245, in get_x
      def get_x(self, i): return open_image(os.path.join(self.path, self.fnames[i]))
  IndexError: index 0 is out of bounds for axis 0 with size 0
-----------------------------------------------------------------------------------------

Thanks!
",really run multiple error feel like something exactly recent call last line hook line line line line line return line line super line self return index axis size thanks,issue,positive,positive,positive,positive,positive,positive
467683579,"Also, are you trying to do anything custom?",also trying anything custom,issue,negative,neutral,neutral,neutral,neutral,neutral
467683484,"I need more information (like a stack trace perhaps).  This error is new to me.  Multiple epochs are nothing new in training, so I don't think it's quite that per se that is the triggering issue.",need information like stack trace perhaps error new multiple nothing new training think quite per se issue,issue,negative,positive,neutral,neutral,positive,positive
467653982,This won't affect training.  This is strictly tensorboard functionality I built in to generate a graph of the model.  I found the graph of the model to be pretty useless so I'm just getting rid of it all together with the FastAI v1 upgrade.,wo affect training strictly functionality built generate graph model found graph model pretty useless getting rid together upgrade,issue,negative,negative,negative,negative,negative,negative
467653246,It'll be available when I merge the FastAI v1 upgrade for DeOldify.  Which hopefully will be soon (within a month?).  But I'm working on some ambitious changes so this date isn't too firm.,available merge upgrade hopefully soon within month working ambitious date firm,issue,positive,positive,positive,positive,positive,positive
464301353,I'll keep this open but I won't be worrying about it until I'm about ready to release- other fish to fry currently.  ,keep open wo worrying ready fish fry currently,issue,negative,positive,neutral,neutral,positive,positive
464301128,"If I try to run this command
```
conda install -c fastai fastai=1.0.42
```
Getting an error too
```
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - fastai=1.0.42
  - pytorch[version='>=1.0.0']

Current channels:

  - https://conda.anaconda.org/fastai/linux-64
  - https://conda.anaconda.org/fastai/noarch
  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/free/linux-64
  - https://repo.anaconda.com/pkgs/free/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch
  - https://repo.anaconda.com/pkgs/pro/linux-64
  - https://repo.anaconda.com/pkgs/pro/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

```",try run command install getting error environment following available current current search alternate may provide package looking navigate,issue,negative,positive,neutral,neutral,positive,positive
464299692,"Shouldn't **environment.yml**  looke like this?

```
name: deoldify
dependencies:
- pip:
  - fastai>=1.0.42
  - tensorboardX>=1.4
```

Anyway, if I make it like this, I get an error.

```
Fetching package metadata ...........
Solving package specifications: An unexpected error has occurred.
Please consider posting the following information to the
conda GitHub issue tracker at:

    https://github.com/conda/conda/issues



Current conda install:

               platform : linux-64
          conda version : 4.3.30
       conda is private : False
      conda-env version : 4.3.30
    conda-build version : 3.0.27
         python version : 3.6.3.final.0
       requests version : 2.18.4
       root environment : /root/anaconda3  (writable)
    default environment : /root/anaconda3
       envs directories : /root/anaconda3/envs
                          /root/.conda/envs
          package cache : /root/anaconda3/pkgs
                          /root/.conda/pkgs
           channel URLs : https://repo.continuum.io/pkgs/main/linux-64
                          https://repo.continuum.io/pkgs/main/noarch
                          https://repo.continuum.io/pkgs/free/linux-64
                          https://repo.continuum.io/pkgs/free/noarch
                          https://repo.continuum.io/pkgs/r/linux-64
                          https://repo.continuum.io/pkgs/r/noarch
                          https://repo.continuum.io/pkgs/pro/linux-64
                          https://repo.continuum.io/pkgs/pro/noarch
            config file : None
             netrc file : None
           offline mode : False
             user-agent : conda/4.3.30 requests/2.18.4 CPython/3.6.3 Linux/3.10.0-693.2.2.el7.x86_64 debian/stretch/sid glibc/2.23    
                UID:GID : 0:0

`$ /root/anaconda3/bin/conda-env create -f environment.yml`




    Traceback (most recent call last):
      File ""/root/anaconda3/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler
        return_value = func(*args, **kwargs)
      File ""/root/anaconda3/lib/python3.6/site-packages/conda_env/cli/main_create.py"", line 108, in execute
        installer.install(prefix, pkg_specs, args, env)
      File ""/root/anaconda3/lib/python3.6/site-packages/conda_env/installers/pip.py"", line 8, in install
        pip_cmd = pip_args(prefix) + ['install', ] + specs
    TypeError: unsupported operand type(s) for +: 'NoneType' and 'list'
```",like name pip anyway make like get error fetching package package unexpected error please consider posting following information issue tracker current install platform version private false version version python version final version root environment writable default environment package cache channel file none file none mode false gid create recent call last file line file line execute prefix file line install prefix spec unsupported operand type,issue,negative,negative,neutral,neutral,negative,negative
456076068,"Actually very probably it was my fault. I forgot to add parameter. 
 --runtime=nvidia",actually probably fault forgot add parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
455965444,"So first- that's awesome! Surprisingly good quality in certain spots.   Somebody else is actually doing great video stuff as well and I'll be posting his Colab notebook for it in about two-three weeks.  After I get a new model up which should largely address rendering inconsistency issues that happen to be magnified by video (the flashing colors, for example).

As far as the memory issue is concerned:  Try using this method instead:

vis.get_transformed_image_as_pil('./income/black_images/output_' + name + '.png', render_factor=42)

You'll have to save the pil image returned yourself but it won't be producing a pyplot image which is what is causing your problem here apparently.
",awesome surprisingly good quality certain somebody else actually great video stuff well posting notebook get new model largely address rendering inconsistency happen video flashing color example far memory issue concerned try method instead name save image returned wo image causing problem apparently,issue,positive,positive,positive,positive,positive,positive
455907725,"@jantic 
Here is the result of my first test. 

Original 
https://drive.google.com/file/d/14hZVjM6QMoJkhAEEOiPDZXqRFON2tpvg/view?usp=sharing
Result
https://drive.google.com/file/d/1UuavfNgOY0HiSOpqCH_172Qmp5EwPT0r/view?usp=sharing

Render factor = 22

This is super intensive task. I converted video frames into folder with images using ffmpeg, processed every single image, converted those images back to video and added audio from original video. 

It took 50 minutes on 1080ti to process 3320 frames which is about 2 minutes and 18 seconds long video. And also I have a memory leak, so  I needed to restart script manually sometimes. 

So the main function within recursive function I use is

```
      vis.plot_transformed_image('./income/black_images/output_' + name + '.png', render_factor=42)

```
After about 20 images I get a warning 
```
More than 20 figures have been opened. Figures created through the pyplot interface
```

I tried to clear memory with
```
torch.cuda.empty_cache()
    gc.collect()
```
But it doesn't really help. Still not sure if this leak because of pyplot. I was using server with RAM over 32 GB. Like in 10-15 minutes there was 6Gb memory leak and counting. And process was slowing down. After restart it worked pretty fast once again, but no longer than 10-15 min.",result first test original result render factor super intensive task converted video folder every single image converted back video added audio original video took ti process long video also memory leak restart script manually sometimes main function within recursive function use name get warning interface tried clear memory really help still sure leak server ram like memory leak counting process restart worked pretty fast longer min,issue,positive,positive,positive,positive,positive,positive
455907323,"@jantic @impredicative 
I launched it inside of a docker container. And even created image from it.  The base images was nvidia/cuda.
But there are 2 issues.
1. I was able to launch it only on version 7.5 of cuda. All other version provided the same error 
```
CUDA driver version is insufficient for CUDA runtime version
```
I thought it's related to host OS cuda version, but looks like not. 
2. I was able to commit container and created an image from it. But after launching new container from it I got this error once again.
```
CUDA driver version is insufficient for CUDA runtime version
```
Which is super strange, taking into account that it's exactly the same code with the same versions. ",inside docker container even image base able launch version version provided error driver version insufficient version thought related host o version like able commit container image new container got error driver version insufficient version super strange taking account exactly code,issue,negative,positive,positive,positive,positive,positive
452995175,"oh thank you for your time for replying my queries my friend,
i am making a youtube series on deep-learning, so i show people how to setup awesome apps like yours on their local machines, not the technical aspects but a how to video, with all the credits given to the app maker.
that was the reason i was trying to set it up on my windows machine .",oh thank time friend making series show people setup awesome like local technical video given maker reason trying set machine,issue,positive,positive,positive,positive,positive,positive
452438304,"@Baukebrenninkmeijer Just thought of something: The '<' character it's complaining about there happens to be the opening tag of an html tag.  Here's my guess:  Perhaps the "".h5"" file you downloaded isn't in fact .h5, but the html page it's hosted on instead?  This has happened to me before in using curl.  Easy way to verify:  Try opening the .h5 file using a text editor.  Also- the file should be very big (hundreds of MB if I remember correctly).",thought something character opening tag tag guess perhaps file fact page instead curl easy way verify try opening file text editor file big remember correctly,issue,negative,positive,positive,positive,positive,positive
452418975,"Ah alright, understandable. 
To add some more info; I tried recreating the environment and this did not fix the issue. ",ah alright understandable add tried environment fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
452415044,Keeping open in case anybody else reports it.,keeping open case anybody else,issue,negative,neutral,neutral,neutral,neutral,neutral
452414062,"I'm not dismissing the likelihood that it's a legit issue.  It's more of a practical consideration at this point: I don't have a reliable way to reproduce this yet, and this is the first time I've heard of this (there's a lot of users!).  So...I have to keep myself sane, basically, and wait for that, as opposed to try to reproduce the environment your working on based on what you're telling me, hunting down an issue that hasn't been reported by anybody else.  That's basically it- prioritization.",likelihood legit issue practical consideration point reliable way reproduce yet first time lot keep sane basically wait opposed try reproduce environment working based telling hunting issue anybody else basically,issue,negative,positive,positive,positive,positive,positive
452406215,"I understand, but conda (and pickle for that matter) has essentially no difference between OSs, so even though it's a different OS, the issue is still relevant. ",understand pickle matter essentially difference even though different o issue still relevant,issue,negative,positive,positive,positive,positive,positive
452403635,Yeah Windows 10 isn't officially supported here yet in my effort so I haven't tested/verified that anything works there.  I know others have gotten it to run but have run into issues.,yeah officially yet effort anything work know gotten run run,issue,negative,neutral,neutral,neutral,neutral,neutral
452391748,"Yeah, I just used the environment.yml, not much else to it, right? I'll try with a new environment. 
I'll fiddle a bit more, but thought maybe it was a known issue. I'm using Windows 10 with this. Can't rule out that has something to do with it, but that would surprise me. ",yeah used much else right try new environment fiddle bit thought maybe known issue ca rule something would surprise,issue,positive,positive,positive,positive,positive,positive
452379041,"Ok so next question- did you use the conda install described in the readme?  That's your best bet to avoid issues.

Unfortunately this seems like it's specific to your environment so figuring it out from my end is tough.  So I'd focus on getting synched up with a matching environment first.

That also includes OS- I'm assuming you're using Linux?",next use install best bet avoid unfortunately like specific environment end tough focus getting matching environment first also assuming,issue,negative,positive,neutral,neutral,positive,positive
452297181,"I've downloaded them twice, and another time with the second link lower in the readme. Doesn't seem to fix the problem.",twice another time second link lower seem fix problem,issue,negative,neutral,neutral,neutral,neutral,neutral
452142654,@Baukebrenninkmeijer Simple suggestion:  Just download the weights again.  The file appears to be corrupted.  I've seen this happen before believe it or not and that was the solution.,simple suggestion file corrupted seen happen believe solution,issue,negative,neutral,neutral,neutral,neutral,neutral
452085067,"@grinvaldsjanis Well...that's the ultimate goal of this project.  I want to make a one stop shop for fixing old photos (de-fading, color correction/colorizing, super-res, etc).  I just haven't gotten to that point yet.  I know other ai implementations have been attempted but I don't know of any that are easily accessible.",well ultimate goal project want make one stop shop fixing old color gotten point yet know ai know easily accessible,issue,negative,positive,positive,positive,positive,positive
452069732,"Oh! Thank You!

Do You know any AI implementations for old image improvements? I think that this overPRed ""lets enhance"" is not real AI but just filters, as I could not get any significant results better than just filtering in gimp.",oh thank know ai old image think enhance real ai could get significant better filtering gimp,issue,positive,positive,positive,positive,positive,positive
452034294,"@grinvaldsjanis I do have plans to make this an app eventually.  But I wouldn't expect that for a while yet.  Luckily, there's a website that actually renders using DeOldify that's super easy to use (I didn't make it):

https://colorize.cc/

The server is using a GPU with less memory than ideal so they're not as good as what you can get using a full 11GB+ GPU.  But they're still quite good.",make eventually would expect yet luckily actually super easy use make server le memory ideal good get full still quite good,issue,positive,positive,positive,positive,positive,positive
451849574,"Hello! I tried this stuff in google code/server... amazing, although sometimes it throws out absolutely colourized nonsense. But that's ok.
Tried this photo with Chopin https://twitter.com/StaffGrand/status/1082178202376851462
Is there a plan to make it as an installable app for windows with some user-friendly UI. It took me some time as for artist to figure out what to do with this code, and I am not sure what to do with downloadable stuff at all. Should be some exact instructions on how to get it to work.
Thank You for Your work and the opportunity to try it out!",hello tried stuff amazing although sometimes absolutely nonsense tried photo chopin plan make took time artist figure code sure stuff exact get work thank work opportunity try,issue,positive,positive,positive,positive,positive,positive
451610137,"So a few things:  First, Windows just isn't officially supported yet by me.  I have other higher priority items before I try to seriously knock that out.  That being said, there's a thread on this memory issue that appears to be Windows specific, here:

https://github.com/jantic/DeOldify/issues/49

Another thing that jumped out to me looking at what you posted above is that your Pytorch version is 0.3.1, which is below minimum required (0.4.1).  That's really indicating to me that the conda install wasn't used- I'd go right to the instructions on that in the readme and use that to install prereqs instead- much easier!

**But even easier than what I just said**:  Just use the Colab notebook- it comes with a free 11GB GPU and none of the install hassles!  https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb",first officially yet higher priority try seriously knock said thread memory issue specific another thing looking posted version minimum really install go right use install much easier even easier said use come free none install,issue,positive,positive,positive,positive,positive,positive
451609162,"@haoopan  This isn't -officially- supported yet (I need to prioritize items) but I think I can point you the way to making progress.

First, if you're asking for this because of a lack of suitable GPU on your end, I'd suggest using the Colab notebook instead (super easy to use, and comes with a free 11 GB GPU!):

https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb

Otherwise, here's a thread that should at least help you get started:

https://github.com/jantic/DeOldify/issues/40",yet need think point way making progress first lack suitable end suggest notebook instead super easy use come free otherwise thread least help get,issue,positive,positive,positive,positive,positive,positive
451608720,"> I think you may like to use this for squarifying: https://github.com/Vooban/Smoothly-Blend-Image-Patches

Hey- just got back from vacation and finally got to sit down and look at this.  This looks like it has potential to be useful!  I'll keep this issue open for now and it might take a while for me to get to it.  But- thank you.",think may like use got back vacation finally got sit look like potential useful keep issue open might take get thank,issue,positive,positive,neutral,neutral,positive,positive
451607358,"> @jantic how about adding [pixiedust](https://github.com/pixiedust/pixiedust) ui support to the colab notebook

Colab has it's own set of UI widgets that look quite nice that I'll want to try first.  I couldn't tell you what is better yet (this is the first I've heard of pixiedust). That'll have to come a bit later though (if I wind up doing it at all..time..)- working furiously on fast.ai v1/Pytorch v1 upgrade first!",support notebook set look quite nice want try first could tell better yet first come bit later though wind time working furiously upgrade first,issue,positive,positive,positive,positive,positive,positive
451438341,@haoopan  don't think of it because it's need cuda acceleration to process pytorch task .. ,think need acceleration process task,issue,negative,neutral,neutral,neutral,neutral,neutral
447527154,Good to know.  I'm doing a big overhaul of the code right now.  This will be part of that.,good know big overhaul code right part,issue,negative,positive,positive,positive,positive,positive
447518055,"I've switched to Linux (Ubuntu 18.04 for now) and can confirm I got the out of memory error too -- in my case, when I try render factors above about 30, it barfs. Moreover, it doesn't seem to be ""self-cleaning"" -- once it hits the CUDA out of memory error, it stays there. HOWEVER the code above (the sample clean_mem() function I wrote above) does work. 

With minimal modification to the Jupyter color visualization notebook, I run something like this:

```
for i in range(10):
    clean_mem()
    vis.plot_transformed_image(""/home/steve/build/DeOldify/input_images/""+str(i+1)+"".jpg"", render_factor=30)
```

(obviously I have a folder called input_images, and have plunked a bunch of B&W photos in there starting with 1.jpg, 2.jpg... 10.jpg)
",switched confirm got memory error case try render moreover seem memory error stay however code sample function wrote work minimal modification color visualization notebook run something like range obviously folder bunch starting,issue,negative,negative,neutral,neutral,negative,negative
446910166,"You might have to do this 

`conda remove pytorch `

then 

`conda install -c pytorch=0.4.1 pytorch-cpu=0.4.1` 

I -think- the second command will work but haven't tried it.

I say that because it's complaining about CUDA, yet CUDA is only relevant to GPUs.  And it shouldn't even get to this point unless something's screwed up with the pytorch-cpu install.",might remove install second command work tried say yet relevant even get point unless something screwed install,issue,negative,positive,positive,positive,positive,positive
446816186,"I have a problem fou you. I follow your guidance. Everything went successful until this step:
![image](https://user-images.githubusercontent.com/38806779/49910801-0cb1fd00-febf-11e8-9791-744ef42bedcd.png)
![image](https://user-images.githubusercontent.com/38806779/49910806-120f4780-febf-11e8-988c-692e5294c79a.png)
Hope get your help, thanks!!",problem fou follow guidance everything went successful step image image hope get help thanks,issue,positive,positive,positive,positive,positive,positive
446485708,Good find good person!,good find good person,issue,positive,positive,positive,positive,positive,positive
445701996,"Looks like the only thing hindering this was a bug in Pytorch itself with spectral norm, that should be fixed for 1.0.  Will investigated soon.",like thing bug spectral norm fixed soon,issue,negative,positive,neutral,neutral,positive,positive
445701755,I'm a big fan of automated testing and in fact have made significant tooling myself at work for it....But it's not needed here imho.,big fan testing fact made significant tooling work,issue,negative,positive,positive,positive,positive,positive
445700778,Don't think I'll need this.  It's a very small project.  One guy (me).  I'm going to close for now.  Sorry!,think need small project one guy going close sorry,issue,negative,negative,negative,negative,negative,negative
445670871,"So I think you're having problems that beget problems that beget problems here :)  First- that connection error you're seeing is probably just local to you and your computer trying to get that Pytorch package.  My advice on that would be to simply try again- after you update to the latest version of DeOldify.  I just added code to make sure that Pytorch 0.4.1 specifically is pulled instead of the new 1.0 version (haven't tested it yet but it was a very small change).

The syntax error that's being reported is most likely a symptom of not completing the conda install and having a python version older than when f strings were introduced (3.6).  

If your interest is just to colorize images and not train, I'd suggest using the Colab notebook instead (so much easier!):

https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb

They provide an 11 GB gpu..for free!  And zero hassle for you.  Do note I just realized the images being referenced on there for examples are broken (wtf).",think beget beget connection error seeing probably local computer trying get package advice would simply try update latest version added code make sure specifically instead new version tested yet small change syntax error likely symptom install python version older interest colorize train suggest notebook instead much easier provide free zero hassle note broken,issue,negative,positive,positive,positive,positive,positive
444773497,"The x_tfms are dynamic, so it shouldn't have anything to do with the tmp folder.  If you can replicate it as a legit issue I'll certainly try to hunt it down.",dynamic anything folder replicate legit issue certainly try hunt,issue,positive,positive,positive,positive,positive,positive
444773157,"Yeah, I found that the tmp image files should be generated first and then go to the training process. Otherwise, the input would become colorful.

Don't know what the problem is.",yeah found image first go training process otherwise input would become colorful know problem,issue,negative,positive,positive,positive,positive,positive
444772764,That looks like it's working then.... in that second image you posted here.,like working second image posted,issue,negative,neutral,neutral,neutral,neutral,neutral
444772054,"Yeah, of course. 
I thought it would be some display errors, but in fact, it is not -- The train_gen_images trained so well that it must be some weird changes in input images.
Also refresh the webpage didn't get any better.

And maybe there is something wrong when the tmp images are first generated. After the first time it would be normal black and white images. I don't know where the bug is.

This is the second time I trained:

![image](https://user-images.githubusercontent.com/44690327/49567803-c235fb00-f969-11e8-9698-138594c3e795.png)
",yeah course thought would display fact trained well must weird input also refresh get better maybe something wrong first first time would normal black white know bug second time trained image,issue,negative,negative,neutral,neutral,negative,negative
444770719,I've actually run into issues with Tensorboard doing weird caching of images in the browser.  Have you tried moving the train_gen_images scrubber tool (the orange thing)?,actually run weird browser tried moving scrubber tool orange thing,issue,negative,negative,negative,negative,negative,negative
444202975,"Yeah the smaller, more further away, and cluttered objects are in a picture, the harder time model will have generally speaking.  Much like it'd be hard for a person to manually colorize such a scene, I think.  

I'm working on improving the model, I assure you!",yeah smaller away picture harder time model generally speaking much like hard person manually colorize scene think working improving model assure,issue,positive,negative,neutral,neutral,negative,negative
444186343,"Agree. I'm going to close this ""issue"" since it's really not a known work item. ",agree going close issue since really known work item,issue,negative,positive,positive,positive,positive,positive
444170706,"Not much improvement after increasing render factor
![bad1](https://user-images.githubusercontent.com/5250490/49458256-6ebb9400-f827-11e8-865b-a4f6d5145491.png)
",much improvement increasing render factor bad,issue,negative,negative,negative,negative,negative,negative
443989821,It does work! Thank you for your answer. I will think about what you said in practice.,work thank answer think said practice,issue,negative,neutral,neutral,neutral,neutral,neutral
443987975,"Yeah, they're actually using the same exact model.  The only difference is the augmentation used to alter the input photos for training.  For colorizer you'll see this in the notebook:

x_tfms = [BlackAndWhiteTransform()]

Which changes the input photos from color to black and white (target photos remain the same- color).

And DeFade uses photos as input altered by this augmentation:

x_tfms = [RandomLighting(0.5, 0.5)]

Originally I was just going to have DeFade do color photo defade (that is, have it process after the black and white to color transform is done by the colorizer). But the more I thought about it, I started thinking that it might be more useful to defade black and white photos first before trying to colorize them.  So I introduced this transform which turns both the input and target photos into black and white for training in DeFade:

extra_aug_tfms = [BlackAndWhiteTransform(tfm_y=TfmType.PIXEL)]

Does that help?",yeah actually exact model difference augmentation used alter input training see notebook input color black white target remain color input augmentation originally going color photo process black white color transform done thought thinking might useful black white first trying colorize transform turn input target black white training help,issue,positive,positive,neutral,neutral,positive,positive
443984126,"Thanks for your help！ @jantic ，I am a little confuse about the different of ""ColorizeTraning"" and ""Defadetraning"".  I have checked the code. They look the same. Do they have different scenarios? ",thanks little confuse different checked code look different,issue,negative,positive,neutral,neutral,positive,positive
443981414,"That specific set of weights for colorizer aren't available, but this older proven set is:  https://www.dropbox.com/s/7r2wu0af6okv280/colorize_gen_192.h5 .  Note the name difference.  The defader weights aren't yet available because I haven't zeroed in on a model I'm happy with yet.",specific set available older proven set note name difference yet available model happy yet,issue,positive,positive,positive,positive,positive,positive
443940421,"Great input. So on your suggestions there- I do think there's potential for improvement in having more clever usage of the model. Unfortunately (yet fortunately!), the model learns all this stuff for me and is pretty much a black box- as opposed to being a deterministic set of lines of code.  So there's a few things that follow as a result:

1.  It has to be a matter of generalized training/model tweaks to get these things improved.
2.  The model deals with whole images. Tweaks involving segmentation would be pretty much impossible because I don't have access to the fine details that would ensure that the whole image that emerges wouldn't look segmented as a result (and I can't say ""if face...do this...else..do that"").

That being said...I think what you're talking about here could be addressed by better training and better models.  I'm already seeing improvements; with a better model that affect these things.",great input think potential improvement clever usage model unfortunately yet fortunately model stuff pretty much black opposed deterministic set code follow result matter generalized get model whole segmentation would pretty much impossible access fine would ensure whole image would look segmented result ca say face else said think talking could better training better already seeing better model affect,issue,positive,positive,positive,positive,positive,positive
443902363,Wow great detective work Steve!  Thank you.  I'll keep this open to remind myself to put in a fix for this- didn't realize Windows machines would have this sort of issue.,wow great detective work thank keep open remind put fix realize would sort issue,issue,positive,positive,positive,positive,positive,positive
443823555,"Yes, I can confirm that calling a clean_mem function prior to each visualization call makes the sample notebook reliably work on Win10 GTX980. For others, on my specific machine, I had to keep the render_factor at around 30 or below. But there is now no longer any need to restart the kernel to get through the entire notebook. 

For those, like me, rather new to python, the steps you want to do are:

1) Add this to top of file (because you'll need the general python garbage collector): 

`import gc `

2) Add a new function definition toward the top of the notebook: 

```
def clean_mem():
    torch.cuda.empty_cache()
    n = 2**14
    a_2GB = np.ones((n, n))  # RAM: +2GB
    del a_2GB  # RAM: -2GB
    gc.collect()
```

3) call clean_mem() before (or after) rendering each image.

I understand from Jason and others that this memory management isn't explicitly required on non-Windows platforms. 
",yes confirm calling function prior visualization call sample notebook reliably work win specific machine keep around longer need restart kernel get entire notebook like rather new python want add top file need general python garbage collector import add new function definition toward top notebook ram ram call rendering image understand memory management explicitly,issue,positive,positive,positive,positive,positive,positive
443816345,"I was able to get it to successively render by calling torch.cuda.empty_cache() and gc.collect() after each colorize call. 

For instance:

`torch.cuda.empty_cache()`
`n = 2**14
`a_2GB = np.ones((n, n))  # RAM: +2GB
`del a_2GB  # RAM: -2GB`

 // be sure to import gc in declarations and then 
`gc.collect()`

Still trying to isolate which of these are necessary and which are simply decorative. Will update when I know more. I'm still coming up to speed on python and pytorch, so bear with the hacky code above... will incorporate into a new function in a more elegant solution.

On my machine, I also had to keep the render_factor below about 30 -- and note that it's hard-coded in the Jupyter notebook sample in several calls to a number larger than 30.",able get successively render calling colorize call instance ram ram sure import still trying isolate necessary simply decorative update know still coming speed python bear hacky code incorporate new function elegant solution machine also keep note notebook sample several number,issue,positive,positive,positive,positive,positive,positive
443800659,"An update: 
* My card is actually a 980 Ti WDDM
* Win10 config.

Note that in the ""Color Visualization"" notebook at present writing, in some cases, the render_factor is hard-coded and in some cases it uses the default. I was just mindlessly pressing the run button on the cells -- e.g. some lines are written thus:

`vis.plot_transformed_image(""test_images/DayAtSeaBelgium.jpg"", render_factor=41)`

Be sure to use a lower render_factor if you get out of memory.

But I can still confirm with the nvidia-smi.exe tool (installed by default in c:\program files\nvidia corporation\nvsmi -- it's a command-line tool)... that memory does not appear to be released on Windows, and that once the video memory is used up, you'll get out-of-memory errors. 

I haven't yet tried out the pytorch link above to try to clear memory.",update card actually ti win note color visualization notebook present writing default mindlessly pressing run button written thus sure use lower get memory still confirm tool default tool memory appear video memory used get yet tried link try clear memory,issue,positive,positive,positive,positive,positive,positive
443792579,"Just jumping in here to confirm adkulas's report above also on Win10 with an NVIDIA GTX970 graphics card. Works for 2-3 images at render factor 15 before reporting CUDA error: out of memory. Trying with render factor 42 immediately throws out of memory error.

Perhaps this is helpful? I haven't yet tried it out: https://discuss.pytorch.org/t/how-to-clear-some-gpu-memory/1945

(By the way, amazing work Jason!) ",confirm report also win graphic card work render factor error memory trying render factor immediately memory error perhaps helpful yet tried way amazing work,issue,positive,positive,positive,positive,positive,positive
443468519,"Oh, by the way, I've got a killer YOLO demo that uses my GUI!

https://github.com/MikeTheWatchGuy/PySimpleGUI/tree/master/YoloObjectDetection

You'll like it.

You can download the training file and it'll just work 👍 
",oh way got killer like training file work,issue,negative,neutral,neutral,neutral,neutral,neutral
443467460,"Having pretrained discriminator weights available is the intention, for sure.  I screwed up on my initial commit of this project and only had good generator weights to upload (I was in a hurry).  Ever since, I've been working on improving the model but I won't put up anything until it's solid.  

I'm definitely working on this, and now I have some really really good help on this (will elaborate more later).  But yeah...definitely on the agenda.  Sorry about the wait!  It takes a long ass time to train on my end as well, even with 4 1080TIs....",discriminator available intention sure screwed initial commit project good generator hurry ever since working improving model wo put anything solid definitely working really really good help elaborate later yeah definitely agenda sorry wait long as time train end well even,issue,positive,positive,positive,positive,positive,positive
443463081,"The Colab notebooks have the ability to add a GUI interface- that was what I was thinking of going for fairly soonish (there's already a Colab notebook for this project). I'd say the Colab route may be best in the short term just because it solves the problem of deployment for most people in a beautiful way:  No need for installing Linux; No need for your own pricey GPU; and all the install steps are done for you and fully reproducible without question.  

In the longer term of course I'd like to not make this just a Python thing, and that's where a real application UI would come into play.  I'll keep  your offer in mind!  For now I'd actually hold off on creating an interface other than Colab just because the project is so unstable right now (it adds another component to maintain, needing tested whenever things change).  There's going to be a lot of changes in the next few weeks/months.",ability add thinking going fairly soonish already notebook project say route may best short term problem deployment people beautiful way need need install done fully reproducible without question longer term course like make python thing real application would come play keep offer mind actually hold interface project unstable right another component maintain needing tested whenever change going lot next,issue,positive,positive,positive,positive,positive,positive
443460301,"Have you thought about putting a GUI onto the front end of this?

I find the machine learning stuff is generally not very approachable due to all the command line.  Your notebook made that not an issue for this project.

Wondering if there is room for something. If so, I would be happy to help.

I have a tkinter and Qt wrapper package called PySimpleGUI that makes adding a GUI downright trivial.  I've put GUIs onto the front of OpenCV applications and chatterbot.  This one seems like it would be mostly selecting options, paths, and then folders or individual photos.
",thought onto front end find machine learning stuff generally approachable due command line notebook made issue project wondering room something would happy help wrapper package downright trivial put onto front one like would mostly individual,issue,positive,positive,positive,positive,positive,positive
443445302,"My pleasure! 

Hmm..an empty slot to put the weights would be a great idea.... Keeps it unambiguous.  The folder it grabs from should be relative to where the notebook is that you're running.
",pleasure empty slot put would great idea unambiguous folder relative notebook running,issue,positive,positive,positive,positive,positive,positive
443441824,There's still some key components I used from Fast.AI that I really didn't want to rewrite and were really helpful.  That and I believe in the future of Fast.AI- they didn't have explicit GAN support at the time when I wrote this but...stay tuned on that :),still key used really want rewrite really helpful believe future explicit gan support time wrote stay tuned,issue,positive,positive,neutral,neutral,positive,positive
443412401,"This is a great suggestion.  Only reason why I'm not going to prioritize this right away is because this is a fairly small code base (I'm not changing Fast.ai fork that's currently included, so speaking strictly about the code specific to this project).  So manual testing currently is doing the job adequately.

And believe it or not I'm a big unit testing/automation advocate at my day job (set up the framework for a lot of it). But I try to keep decisions practical.

So I'll keep this open because yes, this is definitely desired.",great suggestion reason going right away fairly small code base fork currently included speaking strictly code specific project manual testing currently job adequately believe big unit advocate day job set framework lot try keep practical keep open yes definitely desired,issue,positive,positive,neutral,neutral,positive,positive
442765830,"Not AFAIK unless I monkeyed with it when trying to solve the previous issue
with images downloaded via git cmdline. I'll revert the file and see if
that fixes it.
",unless trying solve previous issue via git revert file see,issue,negative,negative,negative,negative,negative,negative
442732709,That's quite odd...You haven't altered that visualize.py file locally at all besides the re-cast you mentioned..?,quite odd file locally besides,issue,negative,negative,neutral,neutral,negative,negative
442668576,"There's nvidia-smi to monitor memory usage.  render_factor 42 won't work for sure on the 970.  

Given that graphics card, I think your best bet is to use the Colab notebook for now- 
https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb

Unfortunately the next two weeks or so are going to be super busy for me and I'll probably not be able to do much until then.
",monitor memory usage wo work sure given graphic card think best bet use notebook unfortunately next two going super busy probably able much,issue,positive,positive,positive,positive,positive,positive
442653397,"I tried different render factors, 42 will give me the error on the first picture but if I use 15 I can do about two or three sample pictures before I get the error.

Do you know of a way to monitor the memory usage of the graphics card? I can try to get some more concrete metrics.",tried different render give error first picture use two three sample get error know way monitor memory usage graphic card try get concrete metric,issue,negative,positive,neutral,neutral,positive,positive
442609974,What's the render factor you're trying to use?  I haven't run into a memory leak yet myself (I don't think at least).  I've tried looking for them too.,render factor trying use run memory leak yet think least tried looking,issue,negative,negative,negative,negative,negative,negative
442547440,Because that output on the bottom came from a newer model that I didn't commit yet.  Sorry for the confusion!  The newer model just isn't ready to be deployed yet. ,output bottom came model commit yet sorry confusion model ready yet,issue,negative,negative,negative,negative,negative,negative
442332660,Yeah let's keep it open exactly for that purpose- I need to either make it explicitly part of the install process described in text or something. I'll think on that.,yeah let keep open exactly need either make explicitly part install process text something think,issue,negative,positive,positive,positive,positive,positive
442303641,"Now why didn't I think of that. 1852GatekeepersWindsor.jpg is a glorious 131 bytes in size as are all the images in the test_images directory. 

I'm using Ubuntu xenial for this so it's an older version of git.  I fixed the issue by updating git and installing git-lfs according to https://github.com/git-lfs/git-lfs/wiki/Installation and then re-fetched the single image.  
```
git fetch
git checkout origin/master test_images/1852GatekeepersWindsor.jpg
```

and it's now 227kb and the image transform is called. Thanks for the pointer!  I'll leave this open in case you want to use it as a prompt to update the README.",think glorious size directory xenial older version git fixed issue git according single image git fetch git image transform thanks pointer leave open case want use prompt update,issue,positive,positive,neutral,neutral,positive,positive
442279354,"Yeah actually I do have an initial idea here (haven't dug deep to test the theory myself yet).  If it's getting past the os.path.exists part in open_image then what you're probably dealing with is a corrupted file (says it's a jpg but isn't- it's probably a very small file).  And there's something I haven't explicitly covered in the installation process yet which is that this particular repo uses git lfs (https://git-lfs.github.com/) for the images.  Now in my case I use the GitHub desktop client for Linux, which automatically prompts you to install Git LFS if you don't have it yet.  But if your'e just using the plain old git command line client, it probably didn't say anything about it and it probably just downloaded the images incorrectly as a result.  

So that's where I'd start.

If it's not that, I'd still wager the files are corrupted and would simply need to be downloaded again.",yeah actually initial idea dug deep test theory yet getting past part probably dealing corrupted file probably small file something explicitly covered installation process yet particular git case use client automatically install git yet plain old git command line client probably say anything probably incorrectly result start still wager corrupted would simply need,issue,negative,negative,neutral,neutral,negative,negative
442178011,"Hi again!

Yes, I followed your installation guide and chose ""CPU"" whenever possible :wink: That said, I am launching the application with `conda activate fastai-cpu && jupyter notebook`. The render speed is not _that_ regarding the possible outcome. Yesterday, I went over to creating colorizations for 8 different render factors per image to spot the right one (which is by far not always the highest possible render factor!), and it takes me about 1-2 minutes per batch (of 8). The bottleneck of choosing higher render factors was rather my 16GB of RAM than the Coffee Lake CPU. Code see Appendinx.

Colab/Jupyter and all this was new to me, so I felt more comfortable having all set up locally at first. 

That's a good point with mathplotlib, I always went straight to the result_images directory to see the images and didn't mind the plotted figure :) 

---
Appendix: Code to create multiple versions of the input image. I needed to patch the underlying method to create different output images, though.

```python
import os
for subdir, dirs, files in os.walk('input_images'):
    for file in files:
        print(file)
        for render_factor in [6, 12, 18, 24, 30, 36, 42, 48]:
            new_path = os.path.join(subdir, file)
            vis.plot_transformed_image(new_path, render_factor=render_factor)
```",hi yes installation guide chose whenever possible wink said application activate notebook render speed regarding possible outcome yesterday went different render per image spot right one far always highest possible render factor per batch bottleneck choosing higher render rather ram coffee lake code see new felt comfortable set locally first good point always went straight directory see mind plotted figure appendix code create multiple input image patch underlying method create different output though python import o file print file file,issue,positive,positive,positive,positive,positive,positive
441932322,"> Would it really cool if you made a bot that colorized photos on Redit HistoryPorn of TheWayWeWere

Mayhaps!  That's actually where I've been getting most of my photos....  ",would really cool made bot actually getting,issue,negative,positive,positive,positive,positive,positive
441912708,"Oh sweet! I didnt realize I could run it online! Thanks you so much for making this @jantic. I just colorized a few old photos of my grandfather. Would it **really** cool if you made a bot that colorized photos on Redit HistoryPorn of TheWayWeWere

![before-after 2](https://user-images.githubusercontent.com/1794527/49059383-7cd13b00-f1bd-11e8-8a5c-85eb5b374c2b.png)
![before-after](https://user-images.githubusercontent.com/1794527/49059384-7cd13b00-f1bd-11e8-9571-57eea55b2f76.png)
![before-after 3](https://user-images.githubusercontent.com/1794527/49059385-7cd13b00-f1bd-11e8-924b-5e9d57239df5.png)


",oh sweet didnt realize could run thanks much making old grandfather would really cool made bot,issue,positive,positive,positive,positive,positive,positive
441867339,"Update on the plt.close(fig) thing- It turns out that if you close the figure, it makes the image disappear in Juptyer (doh!). Currently at least- I'm sure there's a way around it.  I do wonder if it's worth trying to address it though.  I suppose if somebody is doing a huge batch then yeah...problem.   But then really should they be plotting to matplotlib to begin with...(and therefore the plot_transformed_image function)?  Sounds like it should just be going straight to file, because it'd also be problematic to load up Jupyter with tons of images (I've tried...it doesn't like it).

I'm going to let this go for now unless I hear of concrete problems with this that convince me it needs to be changed.  #TradeOffs and whatnot.",update fig turn close figure image disappear currently sure way around wonder worth trying address though suppose somebody huge batch yeah problem really plotting begin therefore function like going straight file also problematic load tried like going let go unless hear concrete convince need whatnot,issue,positive,positive,positive,positive,positive,positive
441856277,"Also- ultimately if you're just interested in rendering with pretrained weights, I strongly recommend to just use the Colab notebook.  They give you a free 11GB (!!!) gpu and it's as hassle free as you can get right now.  Though I do understand it's nice to just have it on your own machine and do whatever you want with it. ",ultimately interested rendering strongly recommend use notebook give free hassle free get right though understand nice machine whatever want,issue,positive,positive,positive,positive,positive,positive
441855934,"Thanks!  This will be a good reference before I try to programmatically support this stuff.

The plt.close(fig) thing in particular- great catch. I'll look at fixing that today.

Did you use the pytorch-cpu install for this?  That was the only way I got it working at a reasonable speed (it's a PITA to have a separate install just for cpu though).  For rendering mind you (don't try to train with CPU!)  And by reasonable I mean slow as fuck but not infinitely slow (like maybe a minute or two for some renders on a 16 core threadripper lol).

`conda install -c pytorch pytorch-cpu `

",thanks good reference try programmatically support stuff fig thing great catch look fixing today use install way got working reasonable speed pita separate install though rendering mind try train reasonable mean slow infinitely slow like maybe minute two core install,issue,positive,positive,positive,positive,positive,positive
441833253,"Some hints for those trying to execute your (awesome :heart_eyes: ) piece of software without a Cuda-supporting GPU:

In ColorizeVisualization.ipynb
- Remove the line `torch.cuda.set_device(0)`, since you don't use cuda
- Remove the line `""IMAGENET = Path('data/imagenet/ILSVRC/Data/CLS-LOC/train')\n"",`
- Modify the following to `""colorizer_path = 'colorize_gen_192.h5'\n"",`

In DeFadeVisualization.ipynb
- Replace all `.cuda(gpu)` calls with `.cpu()`

Bonus, if you want to transform an entire input folder (e.g. called ""input_images""):
```python
    ""import os\n"",
    ""for subdir, dirs, files in os.walk('input_images'):\n"",
    ""    for file in files:\n"",
    ""        vis.plot_transformed_image(os.path.join(subdir, file))""
```
For this, you might also want to patch fastai, since it does keep the figures in memory which will eventually cause you to run out of memory.

In fasterai/visualize.py:37
- Add `plt.close(fig)`

Cheers, Ichaelus",trying execute awesome piece without remove line since use remove line path modify following replace bonus want transform entire input folder python import file file might also want patch since keep memory eventually cause run memory add fig,issue,positive,positive,positive,positive,positive,positive
441491164,"From Twitter thread earlier:   Unfortunately MacOS support hasn't been on my radar yet- most of the tools and the community in deep learning is centered around Linux.  So I haven't tried (can't 100% rule it out!).  It looks like a difficult road though.  On the Pytorch site, for example, I see this:

# MacOS Binaries dont support CUDA, install from source if CUDA is needed

I think that's the source of your issue you submitted to Github.  That being said, not sure if you saw that there's a Colab notebook.  If all you want to do is colorize and not train, then I'd say that's the way to go- way easier!

https://colab.research.google.com/github/jantic/DeOldify/blob/master/DeOldify_colab.ipynb

I'm going to close this because MacOS simply isn't advertised to be supported for development, and I need to pick my problems wisely.  ",twitter thread unfortunately support radar community deep learning centered around tried ca rule like difficult road though site example see dont support install source think source issue said sure saw notebook want colorize train say way way easier going close simply development need pick wisely,issue,positive,positive,neutral,neutral,positive,positive
441490845,"> Also, I used `brew cask install miniconda` to get conda up and running. I'm assuming not everyone who wants to run this program has conda installed.

So here's the thing on that- this is  pretty much a one-man show (with some great contributions from others).  And I'm a big believer in being very selective about the problems I'm willing to tackle (because every problem you tackle means something else gets put back in line).

I'm not (yet) in the business of making this project work for -everybody- and everybody's tastes/situation.  I went with Conda with quite frankly, it's simply the best installation choice (compared to PIP), and I'm choosing to go with a single and opinionated installation process to simplify things in terms of support.  Not only because it's easier to maintain one thing instead of two, but I know that there will be more problems with trying to support other options.

Now that being said- this does seem on the surface to contradict my stance that I want to make this project useful/usable on the broad scale.  But to clarify that- I ultimately intend on making this a simple web app/phone app that requires zero technical know-how (but won't be free).  The Colab notebook will still be there though, and that doesn't require much know-how to operate either.  ",also used brew cask install get running assuming everyone run program thing pretty much show great big believer selective willing tackle every problem tackle something else put back line yet business making project work everybody went quite frankly simply best installation choice pip choosing go single opinionated installation process simplify support easier maintain one thing instead two know trying support seem surface contradict stance want make project broad scale clarify ultimately intend making simple web zero technical wo free notebook still though require much operate either,issue,positive,positive,positive,positive,positive,positive
441490064,"Decided to go with stretch augmentation instead- achieves the same end more or less, and easier to implement.  Closing.",decided go stretch augmentation end le easier implement,issue,negative,neutral,neutral,neutral,neutral,neutral
441489983,Forgot to close this- merged this in a few days ago.,forgot close day ago,issue,negative,neutral,neutral,neutral,neutral,neutral
441481723,"Didn't work 😭 

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-2-b6b2691acef0> in <module>
     20 plt.style.use('dark_background')
     21 torch.backends.cudnn.benchmark=True
---> 22 torch.cuda.set_device(0)

/usr/local/miniconda3/envs/deoldify/lib/python3.6/site-packages/torch/cuda/__init__.py in set_device(device)
    260     """"""
    261     if device >= 0:
--> 262         torch._C._cuda_setDevice(device)
    263 
    264 

AttributeError: module 'torch._C' has no attribute '_cuda_setDevice'
```

![image](https://user-images.githubusercontent.com/1794527/48985947-a5283f00-f0c2-11e8-8d1f-c60f2c0b04ca.png)

Would be sweet if this were an executable program...",work recent call last module device device device module attribute image would sweet executable program,issue,negative,positive,positive,positive,positive,positive
441480419,"Also, I used `brew cask install miniconda` to get conda up and running. I'm assuming not everyone who wants to run this program has conda installed.",also used brew cask install get running assuming everyone run program,issue,negative,neutral,neutral,neutral,neutral,neutral
441480384,"I'm on MacOS... https://github.com/fastai/fastai/issues/84

Commenting out `#- cuda90` fixed the issue... Will keep you posted on whether the program is able to run.

",fixed issue keep posted whether program able run,issue,negative,positive,positive,positive,positive,positive
441470754,"@MayeulC Dude, you made such a huge impact on this project already!  That was the single most impactful improvement I've been able to make on the rendering.  So thank you thank you thank you. ",dude made huge impact project already single improvement able make rendering thank thank thank,issue,positive,positive,positive,positive,positive,positive
441465977,"All right, your explanation makes sense. I was expecting the dimensionality gains to propagate down the layers, but it is true that it might only provide tangible gains in the first (and last) channels. And the fact that you are using a pre-trained network also makes sense!

I am afraid I can't really provide much more interesting input for your project, I wish you the best of luck with it, and I look forward to its next iterations!",right explanation sense dimensionality gain propagate true might provide tangible gain first last fact network also sense afraid ca really provide much interesting input project wish best luck look forward next,issue,positive,positive,positive,positive,positive,positive
441341169,Great catch.  I've found it in a few more places where it needed to be updated and made those corrections as well.  Thank you!,great catch found made well thank,issue,positive,positive,positive,positive,positive,positive
441090240,"@MayeulC So yes I did consider reducing the dimensionality of the input like you suggested here.  But  here's the thing- as far as I can tell it wouldn't actually make a huge inpact on model size/efficiency.  Reason being:  The gray scale input currently comes in 3 channels in an input layer, but then is immediately expanded into much higher dimensional data as the model processes it.  3 to 64 to 256 to 512, etc.  At that point I'd expect that the model would be effectively consolidating the redundancies in the channels (when I reduce the dimensions I get reduced performance).  So really, as soon as they're processed, the fact that they were 3 channels or one quickly becomes almost irrelevant- the information that's relevant, regardless of redundancy is already extracted.

So in other words- I'd expect it to make almost no difference to reduce the input channels here.  Now to complicate matters, I'm also using a pretrained network that expects 3 channels already.  The pretrained network (Resnet34) has these hard-earned weights (coefficients) that took a lot of time to train on somebody else's machine.  So I think that's worth keeping as well- there's not going to be a 1 channel version of this pretrained.

I might be wrong somehow here.  Please correct me if that's the case!",yes consider reducing dimensionality input like far tell would actually make huge model reason gray scale input currently come input layer immediately expanded much higher dimensional data model point expect model would effectively reduce get reduced performance really soon fact one quickly becomes almost information relevant regardless redundancy already extracted expect make almost difference reduce input complicate also network already network took lot time train somebody else machine think worth keeping going channel version might wrong somehow please correct case,issue,positive,positive,positive,positive,positive,positive
441018269,"On a related note, it might be interesting to have this for #18, in order to keep colors consistent from one frame to another.
The previous frame could be compared with the next, and if reasonably similar (might be worth a look at what video codecs are doing), provide colour from the previous frame as a seed/hint value for the next frame.
I mentioned modern video codecs as they should be able to relatively efficiently map objects displacement from one frame to the other, so this could be used to move hints to the right place, from one frame to another. But as a first approach, hinting could be used only for static parts of the images.

And of course, this might not be needed at all! (though I doubt it)",related note might interesting order keep color consistent one frame another previous frame could next reasonably similar might worth look video provide colour previous frame value next frame modern video able relatively efficiently map displacement one frame could used move right place one frame another first approach could used static course might though doubt,issue,positive,positive,positive,positive,positive,positive
441014703,"Hey, I just read your answer on HN. I'm glad to have been helpful, and that you were able to take advantage of this. Also, thank you for making this an issue, it makes further discussion easier.

For future reference, here is some material that was part of the original comment thread:

- [example Octave/Matlab code for the proposed improvement](https://gist.github.com/MayeulC/626bafbaf925fb3a3c80fdba76b7e8be)
- [imgur gallery displaying the improvement brought by the above script](https://imgur.com/a/n2sBYCi)

---------
I had a look at https://github.com/jantic/DeOldify/commit/dabb3a00edb7300a0f71cf97df6c2a9bda184799 but couldn't determine if you reduced the dimensionality of the input/output data? Unfortunately, I am not familiar enough with the code to tell this or contribute in a meaningful way.

I touched on this idea [here](https://news.ycombinator.com/item?id=18381759) and [there](https://news.ycombinator.com/item?id=18509240), but the basic idea is that you should be able to reduce the size of the input data a lot (factor 3) by feeding your network the luma channel instead of RGB. And have it only output the chroma channels. You can probably leave most hyperparameters untouched, although you might be able to reduce its size further (I am by no mean an authority on this, so take this with a grain of salt). This could provide quite sizeable performance improvements; mostly for training, but also at runtime.",hey read answer glad helpful able take advantage also thank making issue discussion easier future reference material part original comment thread example code improvement gallery improvement brought script look could determine reduced dimensionality data unfortunately familiar enough code tell contribute meaningful way touched idea basic idea able reduce size input data lot factor feeding network channel instead output chroma probably leave untouched although might able reduce size mean authority take grain salt could provide quite sizeable performance mostly training also,issue,positive,positive,positive,positive,positive,positive
440068884,"Update:  Turns out that Fast.ai 1.0 is still quite unstable.  The conda install, for example, is clearly from at least a month ago and is missing quite a few key things from the current source.  There's still a lot of refactoring going on in the code base there.  

Thus, I'm going to hold off on this upgrade indefinitely.  I may wind up not doing it altogether because it may just not be worth the hassle based on what I saw.  That isn't to say there isn't benefits (augmentations and transformations should be way faster, for example), but it's looking like for this particular project the benefits may not justify how much would have to be rewritten to get it to work. 

Keep in mind that this doesn't preclude the project from being upgrade to Pytorch 1.0. Now I'll wait until that's in stable release mode before I get the project on using that.  

What I've done instead is made the install of this project, via Conda, super easy (see readme for instructions).  I achieved this partially by just forking Fast.ai 0.7 and making it a part of the project.  This will also shield the project from whatever the fate of Fast.ai 0.7 is and we simply won't have to worry about it.",update turn still quite unstable install example clearly least month ago missing quite key current source still lot going code base thus going hold upgrade indefinitely may wind altogether may worth hassle based saw say way faster example looking like particular project may justify much would get work keep mind preclude project upgrade wait stable release mode get project done instead made install project via super easy see partially making part project also shield project whatever fate simply wo worry,issue,positive,positive,neutral,neutral,positive,positive
439249155,"For inference, I optimized the hell out of memory usage via implementing this jira:  https://github.com/jantic/DeOldify/issues/17 .  I'm going to close this because that was my primary concern- making it usable for end users.  If your training...it'll still take the same amount of memory.  I'll still try to attack that but I think that's going to be a tougher nut to crack.",inference hell memory usage via going close primary making usable end training still take amount memory still try attack think going nut crack,issue,negative,positive,positive,positive,positive,positive
438298529,I'm sure it's feasible but I haven't figured out a way yet to specify/override colors.  The ambition is to have the user guide colors as an option but that's going to take some time to figure out.,sure feasible figured way yet color ambition user guide color option going take time figure,issue,negative,positive,positive,positive,positive,positive
438226626,"Thank you for your reply.I will test it with sz parameter and Im looking forword to the best version.
In addition, is there any way to set output colors if I know the colors of input picture?Just like the first picture, can I set the output colors like green、yellow except blue?If not , is it feasible to set output colors?",thank test parameter looking best version addition way set output color know color input picture like first picture set output color like except blue feasible set output color,issue,positive,positive,positive,positive,positive,positive
438154039,I should add- your best bet to getting a better picture is to play around with the sz parameter. Can't go much above 500 if you have a 11GB GPU though.,best bet getting better picture play around parameter ca go much though,issue,positive,positive,positive,positive,positive,positive
438152041,"This is a known problem without a known solution- the model tends to pick blue.  I'm actually testing out a new feature right now that seems to make colors better.  But ""better"" doesn't mean it'll necessarily pick green in the first case, for example.  Just means it picks something not blue and something interesting- the color is an ""unconstrained problem"". It could be anything.

The zombie hands are definitely a problem too.  The thing I'm working on now seems to help but I don't foresee this going away anytime soon.

I'll keep this issue open just to formally document it.",known problem without known model pick blue actually testing new feature right make color better better mean necessarily pick green first case example something blue something color unconstrained problem could anything zombie definitely problem thing working help foresee going away soon keep issue open formally document,issue,negative,positive,neutral,neutral,positive,positive
437748559,"Phew.... So there was a lingering issue with Pillow versioning there.  I hacked away at it until it finally worked.  Was really quite a confusing issue honestly.  Anyway- it works, I've confirmed!  Really like this- this makes this notebook even more useful and user friendly which I love.  Thank you so much @mariabg  and @mc-robinson!",phew issue pillow hacked away finally worked really quite issue honestly work confirmed really like notebook even useful user friendly love thank much,issue,positive,positive,positive,positive,positive,positive
437737675,Going to merge -then- test my attempted PIL fixes....Will just be easier that way!  This PIL stuff is a pain.,going merge test easier way stuff pain,issue,negative,neutral,neutral,neutral,neutral,neutral
437707840,"Update! I fixed the ""Open in Colab"" thingy",update fixed open thingy,issue,negative,positive,neutral,neutral,positive,positive
437707372,"You are right, I will restart the kernel and push again. Also, I realised that the [""Open in Colab""](https://github.com/mariabg/DeOldify/blob/colab-working-with-Drive/DeOldify_colab.ipynb) button from Github still links to your notebook, how did you set that?

I didn't know I could pydrive to get/save images, I'll try to implement it that way and upload the new version. Do you think that they can be used for the same on this case?

I asked the other day to @jantic for this ""results_dir"" and he implemented right away, I also think it will help users a lot! ",right restart kernel push also open button still link notebook set know could try implement way new version think used case day right away also think help lot,issue,negative,positive,positive,positive,positive,positive
437706323,"Thanks! I'll definitely want us to restart the kernel and rerun all the way through before we pull in order to get rid of messages like this:
""
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(""/content/drive"", force_remount=True).
mkdir: cannot create directory ‘/content/drive/My Drive/deOldifyImages’: File exists
mkdir: cannot create directory ‘/content/drive/My Drive/deOldifyImages/results’: File exists
""

I also think we should consider the consistency of syntax. For example, I am using `from pydrive.drive import GoogleDrive` while you are using `from google.colab import drive` to accomplish similar things. We should really decide on one in order to make it most simple for the user. 

Great catch with using `results_dir` in the `ModelImageVisualizer`. That will help users a lot. Hope to look over this all soon and get things up to date. (Also your picture at the end turned out very well).
",thanks definitely want u restart kernel rerun way pull order get rid like drive already mounted attempt forcibly remount call create directory file create directory file also think consider consistency syntax example import import drive accomplish similar really decide one order make simple user great catch help lot hope look soon get date also picture end turned well,issue,positive,positive,positive,positive,positive,positive
437655604,"Upon further consideration, this was a silly thought.  Nobody wants gigs of data by default in their repo.  Closing.",upon consideration silly thought nobody data default,issue,negative,negative,negative,negative,negative,negative
437568183,"Ok man, you'll now get a message that says something like this the one below.  So basically it looks like you need to make sure you have imagenet in the right location.

ValueError: No image files were found in specified image directory. Path provided was: data/imagenet/ILSVRC/Data/CLS-LOC/trai",man get message something like one basically like need make sure right location image found image directory path provided,issue,positive,positive,positive,positive,positive,positive
437567657,OK this one looks like it's pretty easy to address. I was able to reproduce by making the IMAGENET path passed to the scheds in ColorizeTraining invalid.  It's crashing because the array of files it's trying to index into is empty.  I'll fix the code to make a clear message to the user that no files were found. This should go up shortly.  Thanks!,one like pretty easy address able reproduce making path invalid array trying index empty fix code make clear message user found go shortly thanks,issue,positive,positive,positive,positive,positive,positive
437566625,"It looks much nicer, great!
Good luck for your project :+1:",much great good luck project,issue,positive,positive,positive,positive,positive,positive
436883896,"Phew....yeah I've had this thought spring into mind.  I'll chew on it for a while because it's not immediately obvious to me how to get this to work well with what I currently have.  But if it did, it'd be a really sweet combination!",phew yeah thought spring mind chew immediately obvious get work well currently really sweet combination,issue,positive,positive,positive,positive,positive,positive
436843822,Added the requirements.txt.  Tried to be diligent about if it actually works but it seems doubtful it'll get you there 100%.  I think the ultimate answer is just going to be to rely on the conda install of Fast.AI V1 (working on upgrading to that soon).,added tried diligent actually work doubtful get think ultimate answer going rely install working soon,issue,negative,negative,negative,negative,negative,negative
436799781,"I'm a bit confused what you're seeing actually.  If you look at the images, it says this on GitHub

Stored with Git LFS

I think we're good to go...right...?",bit confused seeing actually look git think good go right,issue,negative,positive,positive,positive,positive,positive
436744985,Man I'm a terrible speller apparently.  Thank you for this!,man terrible speller apparently thank,issue,negative,negative,negative,negative,negative,negative
436744151,"Soo...I tried this before as well, namely because i wanted to see if I could run bigger images (more memory with CPU of course).  

The way to really force CPU is putting this up top in the notebook before the pytorch related imports:

import os
os.environ['CUDA_VISIBLE_DEVICES'] = ''

When I tried that myself, it seemed to have done the trick, but then the processing was super slow and I just abandoned the effort in the meantime.  The reason why I abandoned it was because I looked up the CPU performance issue and the man himself (Soumith Chintala) said this:

> @zhangchenkai you will see great improvement if you build from source from these instructions: https://github.com/pytorch/pytorch#from-source
> 
> The instructions will build and link with mkl-dnn and that will show you remarkable improvements to what you see.

https://github.com/pytorch/pytorch/issues/9873

I think that's probably the issue I saw.  I didn't go ahead and build from source because it wasn't pressing and I'm a bit lazy.  You could try it...I just don't know for certain if that's actually the issue.

Please, if you're so inclined and do try it out, please let me know!  We can put it in the readme then because that'll probably be helpful to a lot of people.",tried well namely see could run bigger memory course way really force top notebook related import o tried done trick super slow abandoned effort reason abandoned performance issue man said see great improvement build source build link show remarkable see think probably issue saw go ahead build source pressing bit lazy could try know certain actually issue please try please let know put probably helpful lot people,issue,positive,positive,positive,positive,positive,positive
436662596,Wow yeah not sure how I wound up doing that.  Just a dumb mistake it appears. I'll remove it this afternoon.,wow yeah sure wound dumb mistake remove afternoon,issue,negative,positive,neutral,neutral,positive,positive
436653621,"Definitely did the raw mirror steps verbatim.  I must be misunderstanding something here. 

I'll come back to this later.  I don't think it's a pressing need given the images are pretty static at this point.",definitely raw mirror verbatim must misunderstanding something come back later think pressing need given pretty static point,issue,negative,positive,positive,positive,positive,positive
436530636,"@jantic doesn't look that images were migrated to lfs. I can still see it in the main repo.
Are you sure you did the bfg part on raw mirror?",look still see main sure part raw mirror,issue,negative,positive,positive,positive,positive,positive
436462330,"Ok after a minor hiccup I think I got this in place now.  I didn't get the history down to the size you reported but it's definitely smaller.  I'll close for now but if I screwed up please let me know!  I'll add weights to this (making a separate issue for that) once I see that the dust has settled on this.

I'm -so- glad you found this and found this early!",minor hiccup think got place get history size definitely smaller close screwed please let know add making separate issue see dust settled glad found found early,issue,positive,positive,positive,positive,positive,positive
436420708,"Wow man that's fantastic.  Yeah, this is new to me so yeah- wasn't sure what all was required to get this going.  And I was distracted with day job lol.  Please do add to the README- that would be so appreciated.  I'll merge it in when I get the steps above done.  I'll try working on this today.  ",wow man fantastic yeah new sure get going distracted day job please add would merge get done try working today,issue,positive,positive,positive,positive,positive,positive
436415169,"Unfortunately it doesn't seem it would be possible to perform history clean up via pull request :)

But here's a step-by-step on what is required:

- download [bfg](https://rtyley.github.io/bfg-repo-cleaner/):
```bash
wget -O bfg.jar ""https://search.maven.org/classic/remote_content?g=com.madgag&a=bfg&v=LATEST""
```
- mirror the repo as bare git:
```bash
git clone --mirror git@github.com:jantic/DeOldify deoldify_bare
```
- go to deoldify_bare and cleanup image history:
```bash
cd deoldify_bare
java -jar /path/to/bfg.jar --no-blob-protection --delete-files ""*.jpg""
java -jar /path/to/bfg.jar --no-blob-protection --delete-files ""*.jpeg""
java -jar /path/to/bfg.jar --no-blob-protection --delete-files ""*.[Pp][Nn][Gg]""
git reflog expire --expire=now --all 
git gc --prune=now --aggressive
git push
```
That would cleanup history and reduce repo size to around 77M (already 3 times smaller).

By that point there will be only one huge file left in history -- ColorizeVisualization.ipynb. Not sure on why it got that big in history (though it's still pretty small). Didn't trace back what was going on with it.

You also can completely remove bare repo here. Clone a new one normal repo (or pull the changes to existing clone, but be careful with history changes, so you won't endup pushing old history again).

In normal repo we need to bring back the images, install lfs and push the images there.

```bash
git lfs install
git lfs track ""*images/*""
cp -r /path/to/test_images/ /path/to/result_images/ /repo/
# as we already set lfs track on images it should be automatically treated as lfs
git add test_images result_images
git commit -m ""comment"" test_images result_images
git push origin master
```

Meanwhile I can write a mini-howto and add it to README on how to checkout and work with LFS. That can be done via pull request :)

PS. [Here](https://github.com/avaika/DeOldify) is cleaned repo. Though I couldn't add git-lfs images due to [bug](https://github.com/git-lfs/git-lfs/issues/1906) which doesn't allow adding new lfs objects to public forks.",unfortunately seem would possible perform history clean via pull request bash mirror bare git bash git clone mirror git go cleanup image history bash git reflog expire git aggressive git push would cleanup history reduce size around already time smaller point one huge file left history sure got big history though still pretty small trace back going also completely remove bare clone new one normal pull clone careful history wo pushing old history normal need bring back install push bash git install git track already set track automatically git add git commit comment git push origin master meanwhile write add work done via pull request though could add due bug allow new public,issue,positive,positive,neutral,neutral,positive,positive
436309936,"> > I'm not in a hurry to do it because the images are currently taking ~110MB
> 
> After commit [71c7729](https://github.com/jantic/DeOldify/commit/71c772922bba175e53e4ebe9fe783f8311cea37a) it takes 200Mb in history :)
> .git is already 289Mb at my computer even after `git gc`.
> 
> Moreover, if you decide to rewrite the history (to reduce it) it's better to do it as soon as possible, cause time goes by and it will be more harder to rewrite the history (especially in case more people will be interested in the project).

Craaaaaap...  Good point.  Sorry, I fucked this one up then.  If you want to go ahead and do it please do.  I'll gladly take the pull request.  Otherwise I'll try to get to it within a few days.",hurry currently taking commit history already computer even git moreover decide rewrite history reduce better soon possible cause time go harder rewrite history especially case people interested project good point sorry one want go ahead please gladly take pull request otherwise try get within day,issue,positive,positive,positive,positive,positive,positive
436307716,">  I'm not in a hurry to do it because the images are currently taking ~110MB

After commit https://github.com/jantic/DeOldify/commit/71c772922bba175e53e4ebe9fe783f8311cea37a it takes 200Mb in history :)
.git is already 289Mb at my computer even after `git gc`.

Moreover, if you decide to rewrite the history (to reduce it) it's better to do it as soon as possible, cause time goes by and it will be more harder to rewrite the history (especially in case more people will be interested in the project).
",hurry currently taking commit history already computer even git moreover decide rewrite history reduce better soon possible cause time go harder rewrite history especially case people interested project,issue,positive,positive,positive,positive,positive,positive
436280310,"> Hi!
> 
> Have you updated dependencies requirements in the README ?

They're indeed in the readme and updated, but not terribly user friendly.  I put this project up just a few days ago not thinking I'd have -this- much attention so I wasn't too worried about user friendliness for thousands of people.  Obviously the situation has changed (LOL).

I think i'm going to prioritize this soon:  https://github.com/jantic/DeOldify/issues/22 which covers requirements.txt.  I was originally relying on users to just install fast.ai which basically takes care of all the dependencies and then just plop this code on top of it.  But yeah, now that there's thousands of eyeballs on this, that just doesn't work anymore.",hi indeed terribly user friendly put project day ago thinking much attention worried user friendliness people obviously situation think going soon originally install basically care plop code top yeah work,issue,positive,positive,neutral,neutral,positive,positive
436278026,"> Images consume a lot of space in this repo.
> 
> Please consider an option to move it outside e.g. to [git-lfs](https://git-lfs.github.com/).
> 
> Though in order to reduce current repo size [history cleanup](https://help.github.com/articles/removing-sensitive-data-from-a-repository/) will also be required.

Good suggestion.  I'm not in a hurry to do it because the images are currently taking  ~110MB which while a lot bigger than code, it isn't horrible imho.  I put it up that way consciously because it was quick, easy, and convenient.  But I do have ambitions of adding more images so yeah...need to tame it.",consume lot space please consider option move outside though order reduce current size history cleanup also good suggestion hurry currently taking lot bigger code horrible put way consciously quick easy convenient yeah need tame,issue,positive,positive,neutral,neutral,positive,positive
436276578,"> Please also include detailed step-by-step guide on how to apply the project on new images with ipython / etc. Not every potential user is familiar with that and it might be quite tricky to figure it out without a hint.

I'll keep that in mind.  For now there's jupyter notebooks that already run with examples that I've pointed the way to in the readme.  I have to prioritize efforts here so for now this will be on the back burner.  ",please also include detailed guide apply project new every potential user familiar might quite tricky figure without hint keep mind already run pointed way back burner,issue,negative,positive,positive,positive,positive,positive
436247032,Please also include detailed step-by-step guide on how to apply the project on new images with ipython / etc. Not every potential user is familiar with that and it might be quite tricky to figure it out without a hint.,please also include detailed guide apply project new every potential user familiar might quite tricky figure without hint,issue,negative,positive,positive,positive,positive,positive
436218103,Please also add requirements.txt to the project.,please also add project,issue,negative,neutral,neutral,neutral,neutral,neutral
436114100,I'll keep this open with the intention of getting that done.  I agree- it should be done!  It'll be slightly lower on my priority list (no guarantees on timeline) but it'll get done.,keep open intention getting done done slightly lower priority list get done,issue,negative,negative,neutral,neutral,negative,negative
435947911,Perhaps the model thought it was some kind of mushroom growing on the tree?,perhaps model thought kind mushroom growing tree,issue,positive,positive,positive,positive,positive,positive
435943334,LOL don't feel bad.  It's a giant ass README.  And it does stick out like a sore...hand.  Not entirely sure if she really did just have a cartoonishly large/red hand there now that I look at the source image....,feel bad giant as stick like sore hand entirely sure really hand look source image,issue,positive,positive,neutral,neutral,positive,positive
435937476,"Yes, https://news.ycombinator.com/item?id=18363870

Also, search the README for 'red hand' and you'll see that they point it out right above the picture.",yes also search hand see point right picture,issue,positive,positive,positive,positive,positive,positive
435769108,"Yes that's what I meant when I said ""that is, the loss that encourages the generator to replicate the black and white input image (the target is the color version of the same image).""

It's just Imagenet photos being converted to gray scale, then the neural net's job is to convert it back to color. But I say it's complicated because that's not the complete picture (see above).",yes meant said loss generator replicate black white input image target color version image converted gray scale neural net job convert back color say complicated complete picture see,issue,negative,negative,neutral,neutral,negative,negative
435764425,"Taking classification problem as an example,given an image of cat,its groundtruth or label is cat.In your work,what I say groundtruth is the color version of the same image given an input black image.So my question is that whether both the black images and their correspondingly colorful version exist in your dataset.

I'm sorry for confusing you for so long.",taking classification problem example given image cat label work say color version image given input black question whether black correspondingly colorful version exist sorry long,issue,negative,negative,negative,negative,negative,negative
435734706,"So what it does is strictly supervised learning when it comes to training the generator on perceptual loss- that is, the loss that encourages the generator to replicate the black and white input image (the target is the color version of the same image).  Now the reason why I say I'm fuzzy on definitions here is because the critic portion is looking at real and fake versions and being asked to assign a score to each for ""realism"", but it's not actually being told that the fake image and real image should match per se.  So all the critic can tell the generator then is that ""that's not realistic"" and by how much according to a numerical score.  But the end result is that taken together this combination drives the generator to create vividly colorful transformations of the input black & white images.

So mostly yes on ""ground truth is used"" but it's complicated I guess.... I might just be confused myself on definitions though honestly...",strictly learning come training generator perceptual loss generator replicate black white input image target color version image reason say fuzzy critic portion looking real fake assign score realism actually told fake image real image match per se critic tell generator realistic much according numerical score end result taken together combination generator create vividly colorful input black white mostly yes ground truth used complicated guess might confused though honestly,issue,negative,negative,neutral,neutral,negative,negative
435732638,"Oh,I'm sorry.Regarding as being unsupervised,I want to know whether groundtruth is used during training.",oh unsupervised want know whether used training,issue,negative,neutral,neutral,neutral,neutral,neutral
435709327,Can't accept this one...That wouldn't make the formatting correct as far as I can tell.  Thanks but I'll have to close this without merging.,ca accept one would make correct far tell thanks close without,issue,positive,positive,positive,positive,positive,positive
435709118,"I actually tried the original wgan, wgan-gp and even tried the consistency penalty from ""improving the improved training of Wasserstein gans"".    And I mean...really really tried.  Desperately.  For 6 weeks.  Because I kept getting ""almost good"" results that kept diverging after initially looking promising.  It drove me nuts, because I kept thinking ""this should work!!!""  I clearly am missing something on that.  Anyway- I just casually plugged in the Self-Attention GAN stuff after all that frustration, and it just worked.  The first time.  It was amazing.

As far as it being ""unsupervised""- I guess technically you'd call it both unsupervised and supervised?  Not really sure how to define it honestly.  Probably sounds stupid but...yeah....",actually tried original even tried consistency penalty improving training mean really really tried desperately kept getting almost good kept diverging initially looking promising drove kept thinking work clearly missing something casually plugged gan stuff frustration worked first time amazing far unsupervised guess technically call unsupervised really sure define honestly probably stupid yeah,issue,positive,positive,neutral,neutral,positive,positive
435590428,"Down towards the bottom of the readme there's this section:

**For those wanting to start transforming their own images right away**: To start right away with your own images without training the model yourself (understandable)...well, you'll need me to upload pre-trained weights first. I'm working on that now. Once those are available, you'll be able to refer to them in the visualization notebooks. I'd use ColorizationVisualization.ipynb. Basically you'd replace

colorizer_path = IMAGENET.parent/('bwc_rc_gen_192.h5')

With the weight file I upload for the generator (colorizer).

Then you'd just drop whatever images in the /test_images/ folder you want to run this against and you can visualize the results inside the notebook with lines like this:

vis.plot_transformed_image(""test_images/derp.jpg"", netG, md.val_ds, tfms=x_tfms, sz=500)

I'd keep the size around 500px, give or take, given you're running this on a gpu with plenty of memory (11 GB GeForce 1080Ti, for example). If you have less than that, you'll have to go smaller or try running it on CPU. I actually tried the latter but for some reason it was -really- absurdly slow and I didn't take the time to investigate why that was other than to find out that the Pytorch people were recommending building from source to get a big performance boost. Yeah...I didn't want to bother at that point.",towards bottom section wanting start transforming right away start right away without training model understandable well need first working available able refer visualization use basically replace weight file generator drop whatever folder want run visualize inside notebook like keep size around give take given running plenty memory ti example le go smaller try running actually tried latter reason absurdly slow take time investigate find people building source get big performance boost yeah want bother point,issue,positive,positive,positive,positive,positive,positive
435509065,"I've added the ""For those wanting to start transforming their own images right away"" section.  Hopefully this clarifies things!  tl;dr:  I didn't upload weights yet but they're coming soon.  Until you have weights you'd have to train it yourself.  Sorry!  I wanted to get this out as soon as possible because I was just that excited...and I didn't expect - this- much interest.",added wanting start transforming right away section hopefully yet coming soon train sorry get soon possible excited expect much interest,issue,positive,positive,neutral,neutral,positive,positive
