id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
1301914865,"I have the same question. The basic PPO (tutorial_PPO) can only arrive the goal when there are no obstacles. Moreover, why is variable ""logstd"" in line 91 of tutorial_PPO always zero when running?",question basic arrive goal moreover variable line always zero running,issue,negative,neutral,neutral,neutral,neutral,neutral
1179612130,i don't know if you are able to figure it out.. I'd be glad to have the solution.,know able figure glad solution,issue,positive,positive,positive,positive,positive,positive
1057028715,i am also facing same problem... and downloading vgg19 from the link and adding in directory won't help in kaggle. as it downloads the vgg19 model in output folder and it can't write or download it in input folder...,also facing problem link directory wo help model output folder ca write input folder,issue,negative,neutral,neutral,neutral,neutral,neutral
1057022272,"Hello @jaried... i resolved this issue by downgrading the **h5py** version to 2.10.0
pip install h5py==2.10.0

hope this helps...",hello resolved issue version pip install hope,issue,positive,neutral,neutral,neutral,neutral,neutral
1041039623,"原来的代码是tensorlayer提供的，问题也是tensorlayer产生的。
我在上面的回答中，也提到，2617行我已经改好了。
新的问题是2618行。

我是继续在这个issue提问还是新开一个issue？

The original code is provided by tensorlayer, and the problem is also generated by tensorlayer.
In my answer above, I also mentioned that I have changed line 2617.
The new problem is line 2618.

Do I continue to ask questions in this issue or open a new one?",original code provided problem also answer also line new problem line continue ask issue open new one,issue,negative,positive,positive,positive,positive,positive
1041036766,Line 2617 of the code also needs to be modified in the same way. Now this is a problem you may need to check if the parameters are imported correctly.This error occurs when the parameter shapes are inconsistent.,line code also need way problem may need check error parameter inconsistent,issue,negative,neutral,neutral,neutral,neutral,neutral
1041006463,"tensorlayer\files\utils.py, line 2617, the same question, I modified。

```
Traceback (most recent call last):
  File ""D:/Tony/Documents/yunpan/invest/2022/quant/gym/study PG/PG/pg.py"", line 219, in <module>
    agent.load()
  File ""D:/Tony/Documents/yunpan/invest/2022/quant/gym/study PG/PG/pg.py"", line 161, in load
    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'pg_policy.hdf5'), self.model)
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\files\utils.py"", line 2730, in load_hdf5_to_weights_in_order
    _load_weights_from_hdf5_group_in_order(f, network.all_layers)
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\files\utils.py"", line 2617, in _load_weights_from_hdf5_group_in_order
    weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\files\utils.py"", line 2617, in <listcomp>
    weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]
AttributeError: 'str' object has no attribute 'decode'
```
深圳？
Keep running, new questions:
```
Traceback (most recent call last):
  File ""D:/Tony/Documents/yunpan/invest/2022/quant/gym/study PG/PG/pg.py"", line 219, in <module>
    agent.load()
  File ""D:/Tony/Documents/yunpan/invest/2022/quant/gym/study PG/PG/pg.py"", line 161, in load
    tl.files.load_hdf5_to_weights_in_order(os.path.join(path, 'pg_policy.hdf5'), self.model)
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\files\utils.py"", line 2729, in load_hdf5_to_weights_in_order
    _load_weights_from_hdf5_group_in_order(f, network.all_layers)
  File ""D:\Anaconda3\envs\gym_env\lib\site-packages\tensorlayer\files\utils.py"", line 2618, in _load_weights_from_hdf5_group_in_order
    assign_tf_variable(layer.all_weights[iid], np.asarray(g[w_name]))
IndexError: list index out of range
```

",line question recent call last file line module file line load path file line file line file line object attribute keep running new recent call last file line module file line load path file line file line list index range,issue,negative,positive,neutral,neutral,positive,positive
1039775822,"@jaried This problem has been solved in version 2.2.5, please update to version 2.2.5",problem version please update version,issue,negative,neutral,neutral,neutral,neutral,neutral
955197433,"tensorlayer2.2.3 is no longer the latest version, please install it from the source code.
pip3 install git+https://github.com/tensorlayer/tensorlayer.git
",longer latest version please install source code pip install,issue,negative,positive,positive,positive,positive,positive
955184125,Install the latest version of tensorlayer from the source code and this problem will be solved.,install latest version source code problem,issue,negative,positive,positive,positive,positive,positive
914792720,Sorry!  It is too late to reply you. I will modify them and update. @DLPerf ,sorry late reply modify update,issue,negative,negative,negative,negative,negative,negative
914790153,"Thanks！I will modify them.
```python
def read_and_decode(filename):
    # generate a queue with a given file name
    raw_dataset = tf.data.TFRecordDataset([filename]).shuffle(1000).batch(4)
    features = {}
    for serialized_example in raw_dataset:
        features['label'] = tf.io.FixedLenFeature([], tf.int64)
        features['img_raw'] = tf.io.FixedLenFeature([], tf.string)
        features = tf.io.parse_example(serialized_example, features)
        # You can do more image distortion here for training data
        img_batch = tf.io.decode_raw(features['img_raw'], tf.uint8)
        img_batch = tf.reshape(img_batch, [4, 224, 224, 3])
        # img = tf.cast(img, tf.float32) * (1. / 255) - 0.5
        label_batch = tf.cast(features['label'], tf.int32)
        yield img_batch, label_batch
```
@DLPerf ",modify python generate queue given file name image distortion training data yield,issue,negative,neutral,neutral,neutral,neutral,neutral
912516814,"@hanjr92 Hi, bro, could you consider the issue?",hi could consider issue,issue,negative,neutral,neutral,neutral,neutral,neutral
908957429,"Hello,
How long do you need to confirm this problem? @zsdonghao 
Thank you~",hello long need confirm problem thank,issue,negative,negative,neutral,neutral,negative,negative
889624451,"Okay, thanks for the clarification. I was confused by the naming then, I understand that this form doesn't necessarily refer to the jaccard index.

Edit: I see that your second formulation is also the one that is used in the V-Net paper.",thanks clarification confused naming understand form necessarily refer index edit see second formulation also one used paper,issue,negative,negative,neutral,neutral,negative,negative
889580305,"The equation of Dice coefficient is `2∣X⋂Y∣/(∣X∣+∣Y∣)`.The general denominator is `(∣X∣+∣Y∣)`, or it can be` (∣X∣^2+∣Y∣^2)`. Here jaccard represents the way `(∣X∣^2+∣Y∣^2)`.",equation dice coefficient general denominator way,issue,negative,positive,neutral,neutral,positive,positive
880432128,"> It seems to require you to set reuse=tf.AUTO_REUSE

when i set reuse=tf.AUTO_REUSE and run 'print(model.evaluate(input_fn=lambda: eval_input_fn(test, test_y)))', the dense layers have a new weight actually.The trained weight isn't shared.",require set set run test dense new weight trained weight,issue,negative,positive,positive,positive,positive,positive
797202015,"the problenr  can be solved by the following methods
1.SRGAN source code for the static model, there are compatibility problems, can be modified to dynamic model
1.Version TensorFlow:2.0;Version tensorlayer:2.23
code:
import tensorflow as tf 
import tensorlayer as tl 
from tensorlayer.layers import (Input, Conv2d, BatchNorm2d, Elementwise, SubpixelConv2d, Flatten, Dense, inputs) 
from tensorlayer.models import Model 
from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2 
class MyG(Model): 
    def __init__(self): 
        super().__init__() 
        w_init = tf.random_normal_initializer(stddev=0.02) 
        g_init = tf.random_normal_initializer(1., 0.02) 
        self.conv1=Conv2d(in_channels=3,n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', W_init=w_init) 
        self.block0=self.get_block() 
        self.block1=self.get_block() 
        self.block2=self.get_block() 
        self.block3=self.get_block() 
        self.block4=self.get_block() 
        self.block5=self.get_block() 
        self.block6=self.get_block() 
        self.block7=self.get_block() 
        self.block8=self.get_block() 
        self.block9=self.get_block() 
        self.block10=self.get_block() 
        self.block11=self.get_block() 
        self.block12=self.get_block() 
        self.block13=self.get_block() 
        self.block14=self.get_block() 
        self.block15=self.get_block() 
        self.conv2=Conv2d(in_channels=64,n_filter=64, filter_size=(3, 3), strides=(1, 1), padding='SAME', W_init=w_init, b_init=None) 
        self.bn2=BatchNorm2d(num_features=64,gamma_init=g_init) 
        layer_list=[] 
        layer_list.append(Conv2d(in_channels=64,n_filter=256, filter_size=(3, 3), strides=(1, 1), padding='SAME', W_init=w_init)) 
        layer_list.append(SubpixelConv2d(in_channels=256,scale=2, n_out_channels=None, act=tf.nn.relu)) 
        layer_list.append(Conv2d(in_channels=64,n_filter=256, filter_size=(3, 3), strides=(1, 1), act=None, padding='SAME', W_init=w_init)) 
        layer_list.append(SubpixelConv2
        layer_list.append(SubpixelConv2d(in_channels=256,scale=2, n_out_channels=None, act=tf.nn.relu)) 
        layer_list.append(Conv2d(in_channels=64,n_filter=3, filter_size=(3, 3), strides=(1, 1), act=tf.nn.tanh, padding='SAME', W_init=w_init)) 
        self.blockend=tl.layers.LayerList(layer_list) 
    def get_block(self): 
        w_init = tf.random_normal_initializer(stddev=0.02) 
        g_init = tf.random_normal_initializer(1., 0.02) 
        layer_list=[] 
        layer_list.append(Conv2d(in_channels=64,n_filter=64, filter_size=(3, 3), strides=(1, 1), padding='SAME', W_init=w_init, b_init=None)) 
        layer_list.append(BatchNorm2d(num_features=64,act=tf.nn.relu, gamma_init=g_init)) 
        layer_list.append(Conv2d(in_channels=64,n_filter=64, filter_size=(3, 3), strides=(1, 1), padding='SAME', W_init=w_init, b_init=None)) 
        layer_list.append(BatchNorm2d(num_features=64,gamma_init=g_init)) 
        block=tl.layers.LayerList(layer_list) 
        return block 
    def forward(self,x): 
        x=self.conv1.forward(x) 
        x=self.block0.forward(x)+x 
        x=self.block1.forward(x)+x 
        x=self.block2.forward(x)+x 
        x=self.block3.forward(x)+x 
        x=self.block4.forward(x)+x 
        x=self.block5.forward(x)+x 
        x=self.block6.forward(x)+x 
        x=self.block7.forward(x)+x 
        x=self.block8.forward(x)+x 
        x=self.block9.forward(x)+x 
        x=self.block10.forward(x)+x 
        x=self.block11.forward(x)+x 
        x=self.block12.forward(x)+x 
        x=self.block13.forward(x)+x 
        x=self.block14.forward(x)+x 
        x=self.block15.forward(x)+x 
        x=self.conv2.forward(x) 
        x=self.bn2.forward(x) 
        x=self.blockend.forward(x) 
        return x 
        @tf.function(experimental_relax_shapes=True) 
    def infer(self,x): 
        return self.forward(x)  
myg=MyG() 
myg.eval() 
input_signature=tf.TensorSpec([1,128,128,3]) 
concrete_function=myg.infer.get_concrete_function(x=input_signature) 
frozen_graph=convert_variables_to_constants_v2(concrete_function) 
frozen_graph_def=frozen_graph.graph.as_graph_def() 
tf.io.write_graph(graph_or_graph_def=frozen_graph_def,logdir=""./"",name=f""srgan.pb"",\ as_text=False)
x=self.blockend.forward(x) 

",following source code static model compatibility dynamic model version code import import import input flatten dense import model import class model self super self return block forward self return infer self return,issue,positive,positive,positive,positive,positive,positive
782602952,"@Laicheng0830, if you want more security fixes and patches like this in the future, you can let security researchers know that they can win bounties protecting your repository by copying this small code snippet into your README.md: 

```[![huntr](https://cdn.huntr.dev/huntr_security_badge.svg)](https://huntr.dev)```

👇 👇 👇 

[![huntr](https://cdn.huntr.dev/huntr_security_badge.svg)](https://huntr.dev)",want security like future let security know win protecting repository small code snippet,issue,positive,positive,positive,positive,positive,positive
781839351,"@Laicheng0830 
I have created a fix with huntr. Please find the fix here (https://github.com/418sec/tensorlayer/pull/1). ",fix please find fix,issue,negative,neutral,neutral,neutral,neutral,neutral
781141212," @d3m0n-r00t This is a potential security hole, you can fix it with Pull requests.",potential security hole fix pull,issue,negative,neutral,neutral,neutral,neutral,neutral
776774295,"@zsdonghao @Laicheng0830 Did you have any chance to look at it? 
If it is a valid vulnerability in the context of tensorlayer we (at [Snyk](https://snyk.io) would like to add it to our vulnerability db ",chance look valid vulnerability context would like add vulnerability,issue,negative,neutral,neutral,neutral,neutral,neutral
755055742,The implementation of `PrioritizedReplayBuffer` stores prioritized weights in variable `transitions`. See a comparison between [PrioritizedReplayBuffer.sample](https://github.com/tensorlayer/tensorlayer/blob/93905f057e74d87be7c7d29de40ef917b758db8e/examples/reinforcement_learning/tutorial_prioritized_replay.py#L357) and  [ReplayBuffer.sample](https://github.com/tensorlayer/tensorlayer/blob/93905f057e74d87be7c7d29de40ef917b758db8e/examples/reinforcement_learning/tutorial_prioritized_replay.py#L297).,implementation variable see comparison,issue,negative,neutral,neutral,neutral,neutral,neutral
755053253,"Hi, 

You can refer to [RLzoo](https://github.com/tensorlayer/RLzoo) to check the PG working for both discrete and continuous cases. The key point is to make action distribution from the policy a Gaussian for continuous case to replace the categorical distribution in discrete case, and derive the differentiable log-probability with it in the loss function.
Additionally, if you simply want continuous PG-based algorithms, you can also check or use more advanced ones like PPO.

Best,
Zihan",hi refer check working discrete continuous key point make action distribution policy continuous case replace categorical distribution discrete case derive differentiable loss function additionally simply want continuous also check use advanced like best,issue,positive,positive,positive,positive,positive,positive
755049553,"Hi,
PPO generally does not require the length of trajectory to be fixed/same, for update in either a batch manner or based on samples a single episode/trajectory. As PPO has on-policy update process, the update may not take a batch (usually applied in off-policy algorithms like DDPG, SAC) but a single episodic trajectory, as shown in our [tutorial](https://github.com/tensorlayer/tensorlayer/blob/93905f057e74d87be7c7d29de40ef917b758db8e/examples/reinforcement_learning/tutorial_PPO.py#L155). So the length of  trajectory is never required to be fixed, the trajectory will finish once it's *done*. Note that for other implementations in which PPO is updated in a batch manner, a fixed length is also not required, a batch filled with samples from different trajectories can be created.

Best,
Zihan",hi generally require length trajectory update either batch manner based single update process update may take batch usually applied like sac single episodic trajectory shown tutorial length trajectory never fixed trajectory finish done note batch manner fixed length also batch filled different best,issue,positive,positive,positive,positive,positive,positive
755043185,Sorry for the late reply. What you mentioned might be caused by some numerical issues in tf.minimum if I understood correctly. Could you please print out an example case and paste it here? I'm a bit confused by your description since you mentioned both large negative value (-1e10) and small positive value (1e-10). A case showing how it causes  a large loss value would be great.,sorry late reply might numerical understood correctly could please print example case paste bit confused description since large negative value small positive value case showing large loss value would great,issue,positive,negative,neutral,neutral,negative,negative
755038973,"Hi, 
Please check [here](https://github.com/tensorlayer/tensorlayer/blob/master/examples/reinforcement_learning/tutorial_prioritized_replay.py). The prioritized experience replay is demonstrated in an individual script.",hi please check experience replay individual script,issue,negative,neutral,neutral,neutral,neutral,neutral
755019308,@timgates42 you may need to fix the travis before merging,may need fix travis,issue,negative,neutral,neutral,neutral,neutral,neutral
753639054,"@MoscowskyAnton I'm very happy my solution helps you with your problem, and also thanks for your unique contribution!",happy solution problem also thanks unique contribution,issue,positive,positive,positive,positive,positive,positive
753630542,"Hi @suntaochun!
Super thanks for your solution, it helps me too.
Just to clarify:
Need to change tensorlayers core.py file, which is (for me)
`/home/user/.local/lib/python3.8/site-packages/tensorlayer/models/`
213 line: change on `if tf_ops.is_dense_tensor_like(check_argu):`
223 line: change on `if not tf_ops.is_dense_tensor_like(check_argu[idx]):`
It allowed me to use tensorlayer 2.2.3 with tensorflow-gpu 2.4.0 and cuda 11.0. Tested on RL examples.
",hi super thanks solution clarify need change file line change line change use tested,issue,positive,positive,positive,positive,positive,positive
750737144,"vgg19 link :https://drive.google.com/file/d/1pZ0v-sLj-glfSx3Cssk_aBFRI8mF0hiq/view?usp=sharing
This is a problem with Git downloading large files.
@LILIXIYA ",link problem git large,issue,negative,positive,positive,positive,positive,positive
750735452,"@priyagupta18 Have you already solved the issue?
@Laicheng0830 I have the same HTTP issue with @priyagupta18 even in the U.S. I don't think it's because of the GFW.",already issue issue even think,issue,negative,neutral,neutral,neutral,neutral,neutral
714155984,"it will only update `train_params`, but will take all weights into account when calculating the loss.",update take account calculating loss,issue,negative,neutral,neutral,neutral,neutral,neutral
713804610,"@zsdonghao 
hello
will
train_op = tf.train.AdamOptimizer(0.001).minimize(cost, var_list= train_params) 
freeze the layer weights? or only will not take them into account when calculating the loss?
I want to fix some layers weights",hello cost freeze layer take account calculating loss want fix,issue,negative,neutral,neutral,neutral,neutral,neutral
706501460,"Thanks! there is a problem with this code, we will fix it immediately.

",thanks problem code fix immediately,issue,negative,positive,positive,positive,positive,positive
699182871,"@Laicheng0830 I downloaded the model but running `model = np.load('vgg19.npy', encoding='latin1',allow_pickle=True)` gives the error 'Failed to interpret file 'vgg19.npy' as a pickle'. How to resolve ?",model running model error interpret file pickle resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
678054875,"You can download vgg19 model from：https://drive.google.com/file/d/1pZ0v-sLj-glfSx3Cssk_aBFRI8mF0hiq/view?usp=sharing
Place the file in the directory /models.
",model place file directory,issue,negative,neutral,neutral,neutral,neutral,neutral
669639651,"' object has no attribute '_info'. check your inputs, the input needs to be Layer;
For Your Information:
```    import tensorlayer as tl
    from tensorlayer.layers import Input, Flatten, Dense
    ni = Input([32, 40, 40, 1])
    nn = Flatten()(ni)
    nn = Dense(n_units=20, act=tf.nn.tanh)(nn)
    stn = SpatialTransformer2dAffine(out_size=(20, 20), in_channels=20)
    nn = stn((nn, ni))
    print(nn)`",object attribute check input need layer information import import input flatten dense ni input flatten ni dense ni print,issue,negative,neutral,neutral,neutral,neutral,neutral
650742898,"Hi @Laicheng0830 . Thanks for your help. I have already tried your solution (upgrading Keras to v2.4.3) but failed. 
However, I find what might be the crux of this problem. I track the problem and find that the basic for Model to generically be used in Network is that the `core.py` introduces Layer class, and in the 213 line of that file the Model class tries to identify whether the Layer is the instance of a tensor of Tensorflow using `tf_ops._TensorLike` or `tf_ops._is_dense_tensor_like` where `tf_ops` is `tensorflow.python.ops`.  
However the `tensorflow.python.ops`, or `tf_ops` doesn't include _TensorLike attribute in version 2.20 (thanks to Kite's AI search I search for all the possible method to identify the type of tensor), so I modify the code in `core.py` such that:

`if  tf_ops.is_dense_tensor_like(check_argu):
       pass`

instead of the original one. And the code works. 
I don't know whether this is the root of compatibility for this problem or just a bug. However I think maybe for user of v2.2 this is a universal problem because of the change in its core ops APIs. And I may be happy if it helps and you helped me with this issue.

Appendix:
![image](https://user-images.githubusercontent.com/31800139/85947008-ce820e80-b97a-11ea-8ab5-ba7020ebe55f.png)
It works. 


",hi thanks help already tried solution however find might crux problem track problem find basic model generically used network layer class line file model class identify whether layer instance tensor however include attribute version thanks kite ai search search possible method identify type tensor modify code pas instead original one code work know whether root compatibility problem bug however think maybe user universal problem change core may happy issue appendix image work,issue,positive,positive,positive,positive,positive,positive
650701063,Hi. Is there any future updated version to solve this problem of compatibility?  Or is there any possibility to use TensorFlow.Keras instead of Keras or vice versa to modify the code to use in TensorFlow v2.2 (because I realy love the convenience of using TensorLayer)? Big THANKS!,hi future version solve problem compatibility possibility use instead vice modify code use love convenience big thanks,issue,positive,positive,positive,positive,positive,positive
650687390,"    import tensorflow as tf
    from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2
    from tensorlayer.layers import Dense, Dropout
    from tensorlayer.models import Model
    
    ## define the network
    class CustomModel(Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.dropout1 = Dropout(keep=0.8)  # (self.innet)
        self.dense1 = Dense(n_units=800, act=tf.nn.relu, in_channels=784)
        self.dense2 = Dense(n_units=800, act=tf.nn.relu, in_channels=800)
        self.dense3 = Dense(n_units=10, act=tf.nn.relu, in_channels=800)
        
    @tf.function
    def forward(self, x, foo=None):
        z = self.dropout1(x)
        z = self.dense1(z)
        z = self.dense2(z)
        out = self.dense3(z)
        if foo is not None:
            out = tf.nn.relu(out)
        return out
        
    if __name__ == '__main__':
        save_model = CustomModel()
        save_model.eval()
        inHeight = 800
        inWidth = 784
    
        input_sigbature = tf.TensorSpec(shape=(None, inHeight, inWidth))
        concrete_function = save_model.forward.get_concrete_function(x=input_sigbature)
    
        # This one is a no @tf.funcion
        # forward_function = tf.function(lambda x:save_model.forward(x))
        # concrete_function = forward_function.get_concrete_function(x = input_sigbature)
    
        frozen_graph = convert_variables_to_constants_v2(concrete_function)
        frozen_graph_def = frozen_graph.graph.as_graph_def()
        tf.io.write_graph(graph_or_graph_def=frozen_graph_def,
                          logdir=""./frozen_models"",
                          name=""frozen_graph.pb"",
                          as_text=False
                          )",import import import dense dropout import model define network class model self super self dropout dense dense dense forward self foo none return none one lambda,issue,positive,positive,positive,positive,positive,positive
643936648,"I've requested a rebuild on travis-ci (https://travis-ci.org/github/tensorlayer/tensorlayer/builds/696471153), hope it would return status soon.",rebuild hope would return status soon,issue,negative,neutral,neutral,neutral,neutral,neutral
642589060,"Not sure what has happened. It seems CI's report didn't return back, though it should have passed.",sure report return back though,issue,negative,positive,positive,positive,positive,positive
630080350,"We wanted to compute mean and variance.
norm_axes = range(begin_norm_axis, len(inputs_shape)-1)
mean, var = tf.nn.moments(inputs, norm_axes, keepdims=True)
for so-called ""global normalization"", used with convolutional filters with shape [batch, height, width, depth], pass norm_axes=[0, 1, 2]
",compute mean variance range mean global normalization used convolutional shape batch height width depth pas,issue,negative,negative,negative,negative,negative,negative
629951446,"> You may need to set the parameters begin_norm_axis=-1
> nn = Conv2d(64, (3, 3), (1, 1), padding='SAME', W_init=w_init, b_init=None)(n)
> nn = LayerNorm(begin_norm_axis=-1, act=tf.nn.relu)(nn)

Thanks laicheng.
May I know what is begin_norm_axis this setting mean?
Infact, the input batch of image is [8, 48, 48, 3] (the batch_size =8 ).",may need set thanks may know setting mean input batch image,issue,negative,negative,neutral,neutral,negative,negative
625660002,"Travis-CI is complaining about RNN test failure, after I modified changelog?...
```
=========================== short test summary info ============================
FAILED tests/layers/test_layers_recurrent.py::Layer_RNN_Test::test_basic_simplernn_dropout_1
============ 1 failed, 229 passed, 10 warnings in 347.14s (0:05:47) ============
```",test failure short test summary,issue,negative,negative,negative,negative,negative,negative
625600780,please update the changelog and make the travis pass,please update make travis pas,issue,negative,neutral,neutral,neutral,neutral,neutral
623983750,"@joybhallaa hi, sure, see you on slack.",hi sure see slack,issue,negative,positive,positive,positive,positive,positive
623929178,"@zsdonghao can you point me to some resources for HoloGAN and AdaIN? 
Also, can I message you guys on slack?",point also message slack,issue,negative,neutral,neutral,neutral,neutral,neutral
622258773,"oh, great! welcome to join us. let us know if you need any discussions.


@luomai @lgarithm @warshallrho @Laicheng0830 ",oh great welcome join u let u know need,issue,positive,positive,positive,positive,positive,positive
602686797,"Thanks! I will have a look at it now!
________________________________
From: Hao <notifications@github.com>
Sent: Monday, 23 March 2020 9:17 AM
To: tensorlayer/tensorlayer <tensorlayer@noreply.github.com>
Cc: Kong, Yingxiao <yingxiao.kong@vanderbilt.edu>; Author <author@noreply.github.com>
Subject: Re: [tensorlayer/tensorlayer] Is PPO still applicable if the length of each trajectory is different? (#1074)


Hi, I highly recommend TL2.0, so you can use the new RL code

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorlayer%2Ftensorlayer%2Fissues%2F1074%23issuecomment-602666816&data=02%7C01%7Cyingxiao.kong%40vanderbilt.edu%7C66d212053db4435b00ea08d7cf3d50aa%7Cba5a7f39e3be4ab3b45067fa80faecad%7C0%7C0%7C637205734559740779&sdata=7gqoxlR7mfKJrdvqNCV%2F8pKEikNQ7rBuO8kmAaU2XXA%3D&reserved=0>, or unsubscribe<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAIMGH4WHOF6LAOB7NCA7XEDRI54Q5ANCNFSM4LREWUZQ&data=02%7C01%7Cyingxiao.kong%40vanderbilt.edu%7C66d212053db4435b00ea08d7cf3d50aa%7Cba5a7f39e3be4ab3b45067fa80faecad%7C0%7C0%7C637205734559750777&sdata=9Y0TP6SPcpqseNYmWudDA3jrdpfcptAKdG7yQWBjsUk%3D&reserved=0>.
",thanks look hao sent march author author subject still applicable length trajectory different hi highly recommend use new code thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
602666816,"Hi, I highly recommend TL2.0, so you can use the new RL code ",hi highly recommend use new code,issue,negative,positive,positive,positive,positive,positive
602665689,some example demo on the doc could be great,example doc could great,issue,positive,positive,positive,positive,positive,positive
598812807,"> Why the negative value causes failure in actor loss?
> You can also refer to OpenAI baselines [here](https://github.com/openai/baselines/tree/master/baselines/ppo1), which has similar process as our repo.

I drawed the loss polt and reward plot, when there is a very small negative value, such as 1e-10, the loss will be extremly larger than normally, and the reward stoped increase.
I just tried lower learning rate, and there was no such 1e-10 value came out.
I wonder if it's the same that use my code above, since it's more robust.",negative value failure actor loss also refer similar process loss polt reward plot small negative value loss normally reward increase tried lower learning rate value came wonder use code since robust,issue,negative,negative,negative,negative,negative,negative
598764036,"Why the negative value causes failure in actor loss?
You can also refer to OpenAI baselines [here](https://github.com/openai/baselines/tree/master/baselines/ppo1), which has similar process as our repo.",negative value failure actor loss also refer similar process,issue,negative,negative,negative,negative,negative,negative
598761590,"The 'v_' is target value, which is not supposed to be optimized. That's also why it's called the target, like the label in supervised learning.",target value supposed also target like label learning,issue,positive,neutral,neutral,neutral,neutral,neutral
592325269,"Hey  @pusabre 
Can you share the details on how you changed the code from tf.contrib.layers.layer_norm() to be compatible for TF2.0.0",hey share code compatible,issue,negative,neutral,neutral,neutral,neutral,neutral
582905086,"> you can also print the configuration via:
> 
> ```
> print(model)
> ```

But it can not show the feature map's width and hight, the visualization like keras summary is really convenient.",also print configuration via print model show feature map width hight visualization like summary really convenient,issue,negative,positive,positive,positive,positive,positive
582243542,"you can also print the configuration via:

```
print(model)
```",also print configuration via print model,issue,negative,neutral,neutral,neutral,neutral,neutral
581768519,"> 感谢您的建议，我将尝试在新版本中添加此功能。

the function like Keras summary() is good enough, I hope the visualization can show each layer's input and output tensor's shape",function like summary good enough hope visualization show layer input output tensor shape,issue,positive,positive,positive,positive,positive,positive
579138467,"@Laicheng0830 Hi, just checking on the update for this Feature Request. Is it still on the roadmap?",hi update feature request still,issue,negative,neutral,neutral,neutral,neutral,neutral
575985292,Thank you for the feature request. We will implement the Mish activation function.,thank feature request implement activation function,issue,negative,neutral,neutral,neutral,neutral,neutral
573850997,@zsdonghao Ready to merge. I can make a 2.2.1 release after this PR.,ready merge make release,issue,negative,positive,positive,positive,positive,positive
572887817,"```python
import time
import numpy as np
import tensorflow as tf

import tensorlayer as tl
from tensorlayer.layers import Dense, Dropout, Input
from tensorlayer.models import Model

tl.logging.set_verbosity(tl.logging.DEBUG)

X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1, 784))


def hidden_model(inputs_shape):
    ni = Input(inputs_shape)
    nn = Dropout(keep=0.8)(ni)
    nn = Dense(n_units=800, act=tf.nn.relu)(nn)
    nn = Dropout(keep=0.8)(nn)
    nn = Dense(n_units=800, act=tf.nn.relu)(nn)

    return Model(inputs=ni, outputs=nn, name=""mlp_hidden"")


def get_model(inputs_shape, hmodel):
    hidden = hmodel.as_layer()
    ni = Input(inputs_shape)
    nn = hidden(ni)
    nn = Dropout(keep=0.8)(nn)
    nn = Dense(n_units=10, act=tf.nn.relu)(nn)

    return Model(inputs=ni, outputs=nn, name=""mlp"")


MLP_hidden = hidden_model([None, 784])
MLP = get_model([None, 784], MLP_hidden)

n_epoch = 20
batch_size = 500
print_freq = 5
train_weights = MLP.trainable_weights
optimizer = tf.optimizers.Adam(lr=0.0001)

for epoch in range(n_epoch):
    start_time = time.time()
    for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=True):
        MLP.train()
        with tf.GradientTape() as tape:
            _logits = MLP(X_batch)
            _loss = tl.cost.cross_entropy(_logits, y_batch, name='train_loss')
        grad = tape.gradient(_loss, train_weights)
        optimizer.apply_gradients(zip(grad, train_weights))

    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:
        MLP.eval()
        print(""Epoch {} of {} took {}"".format(epoch + 1, n_epoch, time.time() - start_time))
        train_loss, train_acc, n_iter = 0, 0, 0
        for X_batch, y_batch in tl.iterate.minibatches(X_train, y_train, batch_size, shuffle=False):

            _logits = MLP(X_batch)
            train_loss += tl.cost.cross_entropy(_logits, y_batch, name='eval_loss')
            train_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))
            n_iter += 1
        print(""   train loss: {}"".format(train_loss / n_iter))
        print(""   train acc:  {}"".format(train_acc / n_iter))
        val_loss, val_acc, n_iter = 0, 0, 0
        for X_batch, y_batch in tl.iterate.minibatches(X_val, y_val, batch_size, shuffle=False):
            _logits = MLP(X_batch)
            val_loss += tl.cost.cross_entropy(_logits, y_batch, name='eval_loss')
            val_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))
            n_iter += 1
        print(""   val loss: {}"".format(val_loss / n_iter))
        print(""   val acc:  {}"".format(val_acc / n_iter))

MLP.eval()
test_loss, test_acc, n_iter = 0, 0, 0
for X_batch, y_batch in tl.iterate.minibatches(X_test, y_test, batch_size, shuffle=False):
    _logits = MLP(X_batch)
    test_loss += tl.cost.cross_entropy(_logits, y_batch, name='test_loss')
    test_acc += np.mean(np.equal(np.argmax(_logits, 1), y_batch))
    n_iter += 1
print(""   test loss: {}"".format(test_loss / n_iter))
print(""   test acc:  {}"".format(test_acc / n_iter))
```

It is almost the same as tutorial_mnist_mlp_static_2.py except n_epoch is 20. 

Here is the console output:

```bash
[TL] Load or Download MNIST > data/mnist
[TL] data/mnist/train-images-idx3-ubyte.gz
[TL] data/mnist/t10k-images-idx3-ubyte.gz
[TL] Input  _inputlayer_1: [None, 784]
2020-01-10 13:56:35.326152: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-01-10 13:56:35.334688: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3100010000 Hz
2020-01-10 13:56:35.337981: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d85660 executing computations on platform Host. Devices:
2020-01-10 13:56:35.338013: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version
[TL] Dropout dropout_1: keep: 0.800000 
[TL] Dense  dense_1: 800 relu
[TL] Dropout dropout_2: keep: 0.800000 
[TL] Dense  dense_2: 800 relu
[TL] ModelLayer modellayer_1 from Model: mlp_hidden
[TL] Input  _inputlayer_2: [None, 784]
[TL] Dropout dropout_3: keep: 0.800000 
[TL] Dense  dense_3: 10 relu
Epoch 1 of 20 took 1.862147569656372
   train loss: 2.316465139389038
   train acc:  0.1945600000000001
   val loss: 2.327671527862549
   val acc:  0.19480000000000003
Epoch 5 of 20 took 1.8243627548217773
   train loss: 2.039196491241455
   train acc:  0.2961999999999999
   val loss: 2.033693790435791
   val acc:  0.2982
Epoch 10 of 20 took 1.867743968963623
   train loss: 1.8629565238952637
   train acc:  0.3595800000000001
   val loss: 1.8483893871307373
   val acc:  0.36410000000000003
Epoch 15 of 20 took 1.9024789333343506
   train loss: 1.4744011163711548
   train acc:  0.4884799999999998
   val loss: 1.437483787536621
   val acc:  0.5021000000000001
Epoch 20 of 20 took 1.834416389465332
   train loss: 1.3005656003952026
   train acc:  0.5417399999999999
   val loss: 1.267709493637085
   val acc:  0.5511
   test loss: 1.2561590671539307
   test acc:  0.5582
```",python import time import import import import dense dropout input import model ni input dropout ni dense dropout dense return model hidden ni input hidden ni dropout dense return model none none epoch range tape grad zip grad epoch epoch print epoch took epoch print train loss print train print loss print print test loss print test almost except console output bash load input none binary use frequency service platform host device host default version dropout keep dense dropout keep dense model input none dropout keep dense epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss test loss test,issue,negative,negative,negative,negative,negative,negative
570486949,"I changed the n_epoch of tutorial_mnist_mlp_static_2.py to 20 and the test accuracy is ~0.97, no repetition of your results.

can you give some reproducible code?

",test accuracy repetition give reproducible code,issue,negative,neutral,neutral,neutral,neutral,neutral
567368566,"Thanks for your suggestion, I will try to add this feature in the new version. ",thanks suggestion try add feature new version,issue,negative,positive,positive,positive,positive,positive
563994970,"Hello, to support multiple backends, it is better to implement the low-level APIs, such as `tl.nn.relu`, `tl.nn.dense` (we can follow the tf's standard), so that we can simply replace all `tf` to `tl` in the layer APIs?

It would be great to show your design docs in the group chat before coding ...",hello support multiple better implement follow standard simply replace layer would great show design group chat,issue,positive,positive,positive,positive,positive,positive
544594345,"> 
> 
> I approved the commit, you can merge now.
> pls make sure the training is success.

Yes, the training process are successfully executed as follows:   

Epoch 1 of 50000 took 35.78486728668213
   train loss: 1.774621844291687
   train acc:  0.35478340792838875
   val loss: 1.5231056213378906
   val acc:  0.4544106012658228
Epoch 5 of 50000 took 21.534682750701904
   train loss: 1.2511804103851318
   train acc:  0.5558184143222507
   val loss: 1.2007725238800049
   val acc:  0.5768393987341772
Epoch 10 of 50000 took 21.450966358184814
   train loss: 1.0803945064544678
   train acc:  0.6229619565217391
   val loss: 1.039731502532959
   val acc:  0.6417128164556962
Epoch 15 of 50000 took 21.823768854141235
   train loss: 0.9821569919586182
   train acc:  0.6564817774936061
   val loss: 0.9525172710418701
   val acc:  0.6738528481012658
Epoch 20 of 50000 took 22.02230429649353
   train loss: 0.9058035016059875
   train acc:  0.6823449488491049
   val loss: 0.9106171727180481
   val acc:  0.6838409810126582
Epoch 25 of 50000 took 21.58187484741211
   train loss: 0.8422892689704895
   train acc:  0.7055187020460358
   val loss: 0.8647022247314453
   val acc:  0.6975870253164557
Epoch 30 of 50000 took 22.386353015899658
   train loss: 0.7991064786911011
   train acc:  0.7203164961636829
   val loss: 0.8315731287002563
   val acc:  0.7063884493670886
Epoch 35 of 50000 took 21.895057678222656
   train loss: 0.754762589931488
   train acc:  0.733899456521739
   val loss: 0.798484206199646
   val acc:  0.7184533227848101",commit merge make sure training success yes training process successfully executed epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss epoch took train loss train loss,issue,negative,positive,positive,positive,positive,positive
544510328,"I approved the commit, you can merge now.
pls make sure the training is success.",commit merge make sure training success,issue,positive,positive,positive,positive,positive,positive
542535807,"@JunbinWang  As I said, SAC supported multiple discrete already, you just need a small modification.

For users do not have any RL background, please refer to our RL zoo project here: https://github.com/tensorlayer/RLzoo",said sac multiple discrete already need small modification background please refer zoo project,issue,negative,negative,negative,negative,negative,negative
542528277,@zsdonghao is there any plans to incorporate support for multiple discrete action in SAC ?,incorporate support multiple discrete action sac,issue,negative,positive,neutral,neutral,positive,positive
542494783,"we don't have the example for SAC with discrete space, but you can just simply modify it from the continuous space 

https://github.com/tensorlayer/tensorlayer/blob/master/examples/reinforcement_learning/tutorial_SAC.py

check this may help:https://github.com/tensorlayer/tensorlayer/blob/master/examples/reinforcement_learning/tutorial_PG.py",example sac discrete space simply modify continuous space check may help,issue,negative,neutral,neutral,neutral,neutral,neutral
537992366,Changing APIs of a large repo like TL which has lots of users should always have  your greatest discreetness. We should be considerable as this change will potentially affect lots of other users once they upgrade.,large like lot always discreetness considerable change potentially affect lot upgrade,issue,negative,positive,positive,positive,positive,positive
537912236,"@luomai If we have to keep everything stable .. why should we release TensorLayer 2.0 .. we should open a new repo to create a new library then. 

We can modify Seq2seq, so that if someone use it in old way, we raise an error.",keep everything stable release open new create new library modify someone use old way raise error,issue,negative,positive,neutral,neutral,positive,positive
537887556,"@ChrisWu1997 @zsdonghao 

After releasing the API, a good practice is to keep it stable unless there is a bug. This is typical in maintaining a project used by many people.

For supporting a new use case, the best way is to add a new layer. This is a win-win solution as it satisfies transformer users and does not affect existing users. I remember that we have a feed forward layer for transformers. We can follow the same way. 

If you feel the current seq2seq API has a fundamental flaw, we can consider to mark it deprecated and encourage users to use the transformer-favourited seq2seq.",good practice keep stable unless bug typical project used many people supporting new use case best way add new layer solution transformer affect remember feed forward layer follow way feel current fundamental flaw consider mark encourage use,issue,positive,positive,positive,positive,positive,positive
537853406,I agree with @zsdonghao about the customized embedding things. Pytorch's newly updated [Transformer](https://pytorch.org/docs/stable/nn.html#transformer) does not include the embedding step and this feature really helps me a lot in my recent work. I believe there are many scenarios where people use `Seq2seq`/`Transformer` for customized tasks other than language modeling.,agree newly transformer include step feature really lot recent work believe many people use transformer language modeling,issue,negative,positive,positive,positive,positive,positive
537775724,I would suggesting writing a new `Seq2Seq` model which supports customised encoding/embedding of data for the Encoder and Decoder in `Seq2Seq`.,would suggesting writing new model data,issue,negative,positive,positive,positive,positive,positive
531463016,"Hi the RST format is not correct in many function, please check~",hi format correct many function please,issue,negative,positive,positive,positive,positive,positive
531414621,"> Hi, could you provide an example code in the examples folder? and update changelog.md ? thanks

 done",hi could provide example code folder update thanks done,issue,negative,positive,positive,positive,positive,positive
531182314,"> @ArnoldLIULJ any update?

was on vocation and would be working on a simplified tutorial today",update vocation would working simplified tutorial today,issue,negative,neutral,neutral,neutral,neutral,neutral
529341521,@zsdonghao add free gpu section by a link,add free section link,issue,positive,positive,positive,positive,positive,positive
529217074,"@JingqingZ & @se7enXF, I would suggest you made installation on a local machine more clearer than the current documentation indicates, I think its rather confusing. You have a good framework with a good potential. Tensorlayer is good if well maintained. I don't think the problem was my environment. Like I said, other py modules work in the same without any flaws.
What I did to resolve this issue was to go on the cloud and installed tensor-layer. 
I am still testing your framework and will definitely post any issue that needs attention. In the nearest future, I would love to be part of your development team.",would suggest made installation local machine clearer current documentation think rather good framework good potential good well think problem environment like said work without resolve issue go cloud still testing framework definitely post issue need attention nearest future would love part development team,issue,positive,positive,positive,positive,positive,positive
527149247,"Hi, could you provide an example code in the examples folder? and update changelog.md ? thanks",hi could provide example code folder update thanks,issue,negative,positive,positive,positive,positive,positive
527084109,"Add documentation
Add attention-weights visualisation and pass unit-testing
READY TO MERGE",add documentation add pas ready merge,issue,negative,positive,positive,positive,positive,positive
526905619,"Add attention visualisation util
Add attention visualisation test",add attention add attention test,issue,negative,neutral,neutral,neutral,neutral,neutral
526745557,"I'm fixing this. Temporally, you can just use `BatchNorm` instead of `BatchNorm1d` since `BatchNorm ` works for all when using static model.",fixing temporally use instead since work static model,issue,negative,positive,positive,positive,positive,positive
518710813,"@Officium I am using a recent miniconda version on Windows, where conda install will fail once the requests are uninstalled. Even after using pip to install the request, the use of conda install will cause a RemoveError. I can solve this problem, but I want to say that tensorlayer should not specify a specific version of requests, at least should provide a warning to avoid problems.
Saying a digression. Why can't tensorlayer.rotation set the center of rotation?",recent version install fail uninstalled even pip install request use install cause solve problem want say specify specific version least provide warning avoid saying digression ca set center rotation,issue,negative,negative,negative,negative,negative,negative
518571126,"@Sakura-Luna 
Here are my reproduction steps:
1. remove conda by `rm -rdf ~/miniconda3`
2. install the newest miniconda by `sh Miniconda3-latest-MacOSX-x86_64.sh` without auto init
3. activate the base environment by `. ~/miniconda3/bin/activate`
4. update requests by `pip install requests --upgrade && conda update requests`. The version of requests installed is 2.22.0
5. install tensorlayer by `pip install tensorlayer`. The requests==2.21.0 is installed. The tail of output log is 
```
Installing collected packages: numpy, scipy, joblib, scikit-learn, h5py, pillow, imageio, cycler, kiwisolver, pyparsing, python-dateutil, matplotlib, decorator, networkx, PyWavelets, scikit-image, wrapt, cloudpickle, python-utils, progressbar2, requests, tensorlayer
  Found existing installation: requests 2.22.0
    Uninstalling requests-2.22.0:
      Successfully uninstalled requests-2.22.0
Successfully installed PyWavelets-1.0.3 cloudpickle-1.2.1 cycler-0.10.0 decorator-4.4.0 h5py-2.9.0 imageio-2.5.0 joblib-0.13.2 kiwisolver-1.1.0 matplotlib-3.1.1 networkx-2.3 numpy-1.16.4 pillow-6.1.0 progressbar2-3.39.3 pyparsing-2.4.2 python-dateutil-2.8.0 python-utils-2.3.0 requests-2.21.0 scikit-image-0.15.0 scikit-learn-0.21.0 scipy-1.2.1 tensorlayer-2.1.0 wrapt-1.11.1
```
6. Both `conda install numpy` and `pip install requests --upgrade` are work in a new shell session.

As you mentioned, conda goes wrong because requests is uninstalled and all internet needed conda operation do not work, is it right? 

The solution above is for reinstalling the requests. Another without requests solution for reinstalling requests is to download whl file from the website and then execute `pip install ****.whl`.

",reproduction remove install sh without auto activate base environment update pip install upgrade update version install pip install tail output log collected pillow cycler decorator found installation successfully uninstalled successfully install pip install upgrade work new shell session go wrong uninstalled operation work right solution another without solution file execute pip install,issue,positive,positive,positive,positive,positive,positive
518479487,"@Officium @ChrisWu1997 I don't think you understand what I mean. I am using miniconda. If you want to reproduce this problem, you must install tensorlayer in the base environment. Before that, you need to make sure that the requests version of conda is higher than the version required by tensorlayer. This is the reason why pip uninstall the requests.",think understand mean want reproduce problem must install base environment need make sure version higher version reason pip,issue,negative,negative,neutral,neutral,negative,negative
518247908,"Hi, @Sakura-Luna 

I'm using anaconda and there should be no problem to install tensorlayer using pip under anaconda a environment. Try:
```bash
conda create -n tl2 python==3.6
conda activate tl2
pip install tensorflow==2.0.0-beta
pip install tensorlayer
```
My anaconda version is `4.5.4`.",hi anaconda problem install pip anaconda environment try bash create activate pip install pip install anaconda version,issue,negative,neutral,neutral,neutral,neutral,neutral
518145529,"I can not reproduce with the last version of miniconda. 
The following code may reinstall requests.
```python
from pip._internal import main
main(['install', 'requests'])
```",reproduce last version following code may reinstall python import main main,issue,negative,positive,neutral,neutral,positive,positive
517975506,"Now I can't install any package using conda, because the request will be installed when using conda installation. If there is no request, it will go wrong. If you use pip install requests, the installation will fail. My conda environment is abolished.


RemoveError: 'requests' is a dependency of conda and cannot be removed from conda's operating environment.",ca install package request installation request go wrong use pip install installation fail environment dependency removed operating environment,issue,negative,negative,negative,negative,negative,negative
517696471,@zsdonghao  This PR is ready to merge. Please have a check.,ready merge please check,issue,positive,positive,positive,positive,positive,positive
515988544,"@tobimichigan 
---
Because you install tensorlayer in python3.7, but install keras and tensorflow in conda python3.6. From your upper information:   
```python  
# In your system python3.7
Requirement already satisfied: tensorlayer in ./.local/lib/python3.7/site-packages (2.1.0)   

# In your conda python3.6
conda list: 
python                    3.6.8                h0371630_0    anaconda  
tensorflow-gpu            1.5.0                         0    anaconda  
keras-applications        1.0.8                      py_0    anaconda 
...   
```  
It`s your fault to install tensorlayer in the wrong place.  
To solve your problem, activate the conda environment and then use pip to install tensorlayer. ",install install python upper information python system python requirement already satisfied python list python anaconda anaconda anaconda fault install wrong place solve problem activate environment use pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
515909048,"@JingqingZ  & @se7enXF , for the record, I use the following on my system running, flawlessly:
keras gpu [all the modules for scientific computation]
tensorflow_gpu[with all the features for computation]
pytorch_gpu[with all the features for computation]

My question is why should Tensorlayer be finding it difficult to be found in an environment that other python modules find so easily to run?",record use following system running flawlessly scientific computation computation computation question finding difficult found environment python find easily run,issue,negative,positive,positive,positive,positive,positive
515880528,"@tobimichigan 
---
## Different python environment error
Obviously, it is caused by different python environment. If you use conda make sure that you are always in conda environment. Check your command line if you are using correct environment when you use pip or python. ",different python environment error obviously different python environment use make sure always environment check command line correct environment use pip python,issue,negative,positive,positive,positive,positive,positive
515870100,"@rinabell 
### About vgg16 and vgg19
The pretrained weights of VGG16 and VGG19 are in different encodings which causes the different loading code in TL source code.   
```python
    if layer_type == 'vgg16':
        npz = np.load(os.path.join('models', model_saved_name[layer_type]))
        ...
    elif layer_type == 'vgg19':
        # npz = np.load(os.path.join('models', model_saved_name[layer_type]), encoding='latin1').item()
        npz = np.load(os.path.join('models', model_saved_name[layer_type]), encoding='latin1', allow_pickle=True).item()
        ...
```  
Now I think you know why VGG16 works well.  If you have downloaded the pretrained weights of VGG19 but failed to loading, please change the TL code tensorlayer/models/vgg.py in python site-packages as shown upper.   
### About downloading error  
If you are in China might cause some TIMEOUT error. I suggest you to try more because I did it and I am in China.  
To load the local vgg*.npz, you can find out by reading the source code and here is my submission:  
*  The pertrained *.npz will be downloaded into $PROJECT_DIR/models. 
*  Make a directory in your project root directory and download the pretrained *.npz into it. (Make sure that your project root is python execute root)   
*  The TL code will load *.npz from $PROJECT_DIR/models   
",different different loading code source code python think know work well loading please change code python shown upper error china might cause error suggest try china load local find reading source code submission make directory project root directory make sure project root python execute root code load,issue,negative,positive,neutral,neutral,positive,positive
515396796,"谢谢，我了解一下


 
---原始邮件---
发件人: ""Hao""<notifications@github.com>
发送时间: 2019年7月26日(星期五) 晚上6:02
收件人: ""tensorlayer/tensorlayer""<tensorlayer@noreply.github.com>;
抄送: ""YonghuiXu""<2259949930@qq.com>;""Author""<author@noreply.github.com>;
主题: Re: [tensorlayer/tensorlayer] 请问能不能实现一机多GPU数据或者模型并行处理？ (#1031)



Horvord supports tensorlayer
 
—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub, or mute the thread.",hao author author thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
515281034,"same problem. and my version is the latest 2.1.1.
but what puzzles me is when I ran codes in tensorlayer/examples/pretrained_cnn/, tutorial_models_vgg16.py worked prefect and tutorial_models_vgg19.py did't.
I guess it is the problem of tensorlayer/tensorlayer/models/vgg.py, cauz http://www.cs.toronto.edu/~frossard/post/vgg16/ can be acceessed and link to vgg19 cannot.
maybe change the link to a more stable one will help.
also hope codes in tensorlayer/examples/ could show how to load local vgg*.npz.
thanks a lot.",problem version latest ran worked prefect guess problem link maybe change link stable one help also hope could show load local thanks lot,issue,positive,positive,positive,positive,positive,positive
513480812,"@Rafagd The problem is fixed by PR #1025. Try install the latest version of TensorLayer:
```
pip3 install https://github.com/tensorlayer/tensorlayer/archive/master.zip
```",problem fixed try install latest version pip install,issue,negative,positive,positive,positive,positive,positive
513478750,Thanks for the correction! I was in a bit of a hurry when I was writing that minimal example.,thanks correction bit hurry writing minimal example,issue,negative,positive,neutral,neutral,positive,positive
513306521,I can't see any problem to remove the @private_method of SubpixelConv1d. ,ca see problem remove,issue,negative,neutral,neutral,neutral,neutral,neutral
513279691,"Hi, @Rafagd, thanks for questions!

I found this might be caused by the incompatibility of `@tf.function` and `@private_method`. The function `tl.utils.fit` internally calls the function `tl.utils._train_step` which is by default decorated with `tf.function` for speedup. I tried two things:

- Remove the decorator `@tf.function` of `tl.utils._train_step` and then every thing works well.
- Remove the decorator `@private_method ` of the `_PS` function of `tl.layers.SubpixelConv1d` and then everything works well too.

The reason why `@tf.function` conflicts with `@private_method` is still unclear to me. Currently, I think the simplest way to solve this issue is to remove the `@private_method` decorator of `tl.layers.SubpixelConv1d `. @zsdonghao @JingqingZ  Is it OK to do this? What's the initial reason to decorate it as private_method?

BTW, the above script is not runnable, try this one:
```python
#!/usr/bin/python3

import tensorflow as tf
import tensorlayer as tl
import numpy as np

inputs = tl.layers.Input((1, 2, 2))
prev   = tl.layers.SubpixelConv1d(2, in_channels=2)(inputs)
model = tl.models.Model(inputs, prev)

train_batch = np.array([1, 2, 3, 4]).reshape((1,2,2)).astype(np.float)
train_batch_y = train_batch.reshape((1, 4, 1))
valid_batch = train_batch
valid_batch_y = train_batch_y

tl.utils.fit(model,
    train_op=tf.optimizers.Adam(learning_rate=0.0001),
    cost=tf.losses.MeanSquaredError(),
    X_train=train_batch, y_train=train_batch_y,
    batch_size=len(train_batch), n_epoch=20, X_val=valid_batch, y_val=valid_batch_y, eval_train=True,
)
```",hi thanks found might incompatibility function internally function default decorated tried two remove decorator every thing work well remove decorator function everything work well reason still unclear currently think way solve issue remove decorator initial reason decorate script runnable try one python import import import model model,issue,positive,positive,neutral,neutral,positive,positive
513127650,"*change to sequence_length
*check length of the argument sequence_length",change check length argument,issue,negative,neutral,neutral,neutral,neutral,neutral
513102559,"do you mean, we can input a batch of text without using padding?  or we provide a sequence length into the RNN?",mean input batch text without padding provide sequence length,issue,negative,negative,negative,negative,negative,negative
513000129,"TODO: example needed for Dynamic RNN in the documentation, changelog, unittest coverage ",example dynamic documentation coverage,issue,negative,neutral,neutral,neutral,neutral,neutral
512680937,"I use pip installing the TL2 and the version is the latest 2.1.0.  
I edit the TF2 source code in python with 'allow_pickle=True' and it works well.  
Hope you can add some more useful model in TL, such as ResNet, YOLO. TL is very useful and thank you.
",use pip version latest edit source code python work well hope add useful model useful thank,issue,positive,positive,positive,positive,positive,positive
512308766,This issue is mentioned by #1019 and has been fixed in PR[#1021](https://github.com/tensorlayer/tensorlayer/pull/1021). Please pull the latest TensorLayer code and modify your code accordingly. Thanks!,issue fixed please pull latest code modify code accordingly thanks,issue,positive,positive,positive,positive,positive,positive
511929651,"Hi @ChrisWu1997 
Thank you for your answer. I was trying to migrate one application which uses Bert model and this is some internal logic in that code. As it seems, it might be faster to rewrite it from scratch with Bert implementation in TF 2.0 since there is a lot of code which can not be easily migrated without deep knowledge of internal logic.
Best,
Ljubisa. ",hi thank answer trying migrate one application model internal logic code might faster rewrite scratch implementation since lot code easily without deep knowledge internal logic best,issue,positive,positive,positive,positive,positive,positive
511906604,"The reason behind this error is clear: the `build` function of `Layer` has no return value but you call `forward` upon that, which results in accessing an attribute of `None` object.  

The `build` / `forward` function is not supposed to be called explicitly by user. If you just want to use `LayerNorm` as one intermediate layer of your model, use it in a normal way as in the doc  [here](https://tensorlayer.readthedocs.io/en/latest/user/get_start_model.html#define-a-model).",reason behind error clear build function layer return value call forward upon attribute none object build forward function supposed explicitly user want use one intermediate layer model use normal way doc,issue,negative,negative,neutral,neutral,negative,negative
511854173,Thanks for your suggestion! I will deal with this issue soon.,thanks suggestion deal issue soon,issue,negative,positive,positive,positive,positive,positive
511197527,"Hi, it seems tensorlayer is not in your conda environment.

To add a path to the PATH, you need to find `~/.bashrc` and add the path `/home/hgh/.local/bin` into it.

Please check https://stackoverflow.com/questions/14637979/how-to-permanently-set-path-on-linux-unix ",hi environment add path path need find add path please check,issue,negative,neutral,neutral,neutral,neutral,neutral
511160481,"Jin, your right, It was the ""low speed of the Internet"". I got it sorted after switching to a higher Internet speed.

Besides, I had to switch the version of tensorgflow-gpu to specifically 1.5.0 for tensorflow to run.
Thanks Jin.",right low speed got sorted switching higher speed besides switch version specifically run thanks,issue,negative,positive,positive,positive,positive,positive
509029924,"Hi, this problem can be caused by the low speed of the internet. 

1. Please check if the pip3 is in its latest version.
2. Please check this link: https://askubuntu.com/questions/905196/pip-fails-with-readtimeouterror

Using a higher speed internet can be a better solution.

Hope this may help you.",hi problem low speed please check pip latest version please check link higher speed better solution hope may help,issue,positive,positive,positive,positive,positive,positive
508986497,Need to rewrite after TL supports layer inside layer feature,need rewrite layer inside layer feature,issue,negative,neutral,neutral,neutral,neutral,neutral
508779660,"Hi, the docs format is not correct, and I suggest you to add more explanation and examples ~~",hi format correct suggest add explanation,issue,negative,neutral,neutral,neutral,neutral,neutral
508707929,@JingqingZ Thank you for your reply!  Your solution works well. ,thank reply solution work well,issue,positive,neutral,neutral,neutral,neutral,neutral
508695666,"Hi, thanks for questions!

Unfortunately, `Layer`-wise operations have not been supported in a customised `Layer` yet, which means you couldn't use `Layer` objects like `Conv2d`, `BatchNorm` etc in the `forward()` of a customised `Layer`. More specifically,

```python
# If you define a ResidualBlock class as Layer object
class ResidualBlock(Layer):
	def __init__(self, n_filter=64, filter_size=(3, 3), strides=(1, 1), name=None):
	    # your code

	def build(self, inputs_shape):
	    pass

	def __repr__(self):
	    # your code
        
        # Unfortunately, Layer-wise operations can not be used in the forward() of a Layer object. 
        # Only Tensor-wise operations are allowed now.
        def forward(self, inputs):
            nn = Conv2d(self.n_filter, self.filter_size, self.strides)(inputs)
            nn = BatchNorm(act=tf.nn.relu)(nn)
            nn = Conv2d(self.n_filter, self.filter_size, self.strides)(nn)
            nn = BatchNorm()(nn)
            nn = Elementwise(tf.add)([inputs, nn])
            return nn
```

The solution is you may define the `ResidualBlock` class as a customised `Model` object. For example:

```python
from tensorlayer.models import Model

# You may define a ResidualBlock class as Model object
class ResidualBlock(Model):
	def __init__(self, n_filter=64, filter_size=(3, 3), strides=(1, 1), name=None):
	    # your code
        
        # In the forward() of a Model object, you are free to use Layer (and Model) operations.
        def forward(self, inputs):
            nn = Conv2d(self.n_filter, self.filter_size, self.strides)(inputs)
            nn = BatchNorm(act=tf.nn.relu)(nn)
            nn = Conv2d(self.n_filter, self.filter_size, self.strides)(nn)
            nn = BatchNorm()(nn)
            nn = Elementwise(tf.add)([inputs, nn])
            return nn
```

And you may define a `ResNet` as another `Model` then:

```python
from tensorlayer.models import Model

# You may define a ResNet class as Model object
class ResNet(Model):
	def __init__(self, args, name=None):
            self.residual_block = ResidualBlock(...)
	    # your code
        
        # In the forward() of a Model object, you are free to use Layer (and Model) operations. 
        # You can use the ResidualBlock which you just defined here.
        def forward(self, inputs):
            # customise the ResNet
            for i in range(10):
                 nn = self.residual_block(nn)
            return nn
```

We do have the plan to unify the base `Layer` and `Model` recently. But at this stage, using `Model` to include `Layer`-wise operations is recommended.

Hope this may solve your problems.

Thanks!
",hi thanks unfortunately layer layer yet could use layer like forward layer specifically python define class layer object class layer self code build self pas self code unfortunately used forward layer object forward self return solution may define class model object example python import model may define class model object class model self code forward model object free use layer model forward self return may define another model python import model may define class model object class model self code forward model object free use layer model use defined forward self range return plan unify base layer model recently stage model include layer hope may solve thanks,issue,positive,negative,neutral,neutral,negative,negative
507012150,"the docs has error, run `yapf -i --recursive tensorlayer` should help",error run recursive help,issue,negative,neutral,neutral,neutral,neutral,neutral
504997032,"> Thanks! Please use 'yapf -i --style=setup.cfg tensorlayer/layers/spatial_transformer.py' to fix the code style (shwon in https://travis-ci.org/tensorlayer/tensorlayer/jobs/549620322).

The travis-ci issue has been solved. Please approve this PR and I will merge this when all the tests pass. 
",thanks please use fix code style issue please approve merge pas,issue,positive,positive,positive,positive,positive,positive
504951087,Thanks! Please use 'yapf -i --style=setup.cfg tensorlayer/layers/spatial_transformer.py' to fix the code style (shwon in https://travis-ci.org/tensorlayer/tensorlayer/jobs/549620322).,thanks please use fix code style,issue,positive,positive,positive,positive,positive,positive
503412107,you can use `copy.copy` to make a new image before adding noise.,use make new image noise,issue,negative,positive,positive,positive,positive,positive
503411908,"Thanks, could you make a PR? I will accept it.",thanks could make accept,issue,positive,positive,positive,positive,positive,positive
500368356,"> I will update the chatbot repository with an attention version when this merge is done.
> Or you are suggesting add an example of chatbot in the \examples folder of tensorlayer?

A chatbot example in the chatbot repo would be fine.",update repository attention version merge done suggesting add example folder example would fine,issue,negative,positive,positive,positive,positive,positive
500368044,"I will update the chatbot repository with an attention version when this merge is done.
Or you are suggesting add an example of chatbot in the \examples folder of tensorlayer?",update repository attention version merge done suggesting add example folder,issue,negative,neutral,neutral,neutral,neutral,neutral
500363397,@zsdonghao Please check this branch and merge when you feel it is ready. And how to solve the Travis-CI problem?,please check branch merge feel ready solve problem,issue,negative,positive,positive,positive,positive,positive
500201342,"Change model name to Seq2seqLuongAttention
Modify changelog.md",change model name modify,issue,negative,neutral,neutral,neutral,neutral,neutral
500200124,"Hello, I am going to merge this PR, but 3 things need to be done:
- update `changelog.md`
- improve the docs
- implement the unitest",hello going merge need done update improve implement,issue,negative,neutral,neutral,neutral,neutral,neutral
500199953,"- `Seq2seqLuongAttention` is better for Python naming
- remember to change `changelog.md`",better python naming remember change,issue,negative,positive,positive,positive,positive,positive
500194159,"> I think `make.bat` should be deleted as well as it is already in the `docs/` folder.
> 
> Have you tested if the attention can learn the period of sin function?

Yes. It can learn the sequence.",think well already folder tested attention learn period sin function yes learn sequence,issue,negative,neutral,neutral,neutral,neutral,neutral
500061927,"I think `make.bat` should be deleted as well as it is already in the `docs/` folder.

Have you tested if the attention can learn the period of sin function?",think well already folder tested attention learn period sin function,issue,negative,neutral,neutral,neutral,neutral,neutral
499688756,"- Rewrite sin sequence with noise testing. 
- Remove conf.py and index.rst",rewrite sin sequence noise testing remove,issue,negative,neutral,neutral,neutral,neutral,neutral
499550019,"> * Rebase done
> * Add **init**  and **ALL** done
> * Modify docs
> * Unit-test based on sin-sequence with Gaussian distribution noise. However, performance not convincing. Need your double check.
> * Re-test by running the chatbot, generating better results compared to simple multi-stacked Seq2Seq

I will double check the unittest.

BTW: After this PR is merged, you may upload your code which uses Luong attention to the chat-bot repo as an example of using Luong attention.",rebase done add done modify based distribution noise however performance convincing need double check running generating better simple double check may code attention example attention,issue,positive,positive,positive,positive,positive,positive
499548197,"- Rebase done
- Add __init__  and __ALL__ done
- Modify docs
- Unit-test based on sin-sequence with Gaussian distribution noise. However, performance not convincing. Need your double check.
- Re-test by running the chatbot, generating better results compared to simple multi-stacked Seq2Seq",rebase done add done modify based distribution noise however performance convincing need double check running generating better simple,issue,positive,positive,positive,positive,positive,positive
495914498,"Let me know, when it is ready to be merged, but it seem that some conflicts must be solved before merging.",let know ready seem must,issue,negative,positive,positive,positive,positive,positive
493209919,"Hello,

The error seen in both:
  * **ci/cirlceci: test_sources_py2_cpu** and 
  * **ci/cirlceci: test_sources_py2_cpu**
    
is:

```
ERROR: No matching distribution found for scikit-learn==0.21.0 (from tensorlayer==2.0.0)
```

I do not believe my doc change resulted in this error.

MORE INFO: On the scikit web pageScikit-learn from 0.21 requires Python 3.5 or greater, but the tests that are failing are being run with Python 2.7 so perhaps a configuration file needs to be modified to use a different version of scikit-learn when using Python2.7.

I don't want to further edit anything in an attempt to address an ERROR, but am just reporting what I am seeing...


",hello error seen error matching distribution found believe doc change error web python greater failing run python perhaps configuration file need use different version python want edit anything attempt address error seeing,issue,negative,positive,positive,positive,positive,positive
493145558,Thank you very much for considering my pull request. I have updated the changelog.md with my GitHub ID and PR,thank much considering pull request id,issue,negative,positive,positive,positive,positive,positive
492583085,"Thanks a lot for the contribution, however, I am not able to merge this PR before you update the `changelog.md`. Could you put your Github and PR IDs in to the `changelog.md`? 

Many thanks.",thanks lot contribution however able merge update could put many thanks,issue,positive,positive,positive,positive,positive,positive
492202576,"Hi,
I've cleaned the code and changed the log.

Thanks",hi code log thanks,issue,negative,positive,positive,positive,positive,positive
492050662,"Hi,

To correct the Python format automatically, run:
`yapf -i xxx.py` or `yapf -i --recursive folder`

Also, please put your name and PR ID in `changelog.md`, it will be listed in the next release.",hi correct python format automatically run recursive folder also please put name id listed next release,issue,negative,neutral,neutral,neutral,neutral,neutral
491871405,"TF slim is removed by TF team, and we just release TL 2.0.0. 

enjoy coding",slim removed team release enjoy,issue,negative,positive,positive,positive,positive,positive
491870774,"feel free to reopen if you want to discuss.

we just release 2.0.0 version",feel free reopen want discus release version,issue,positive,positive,positive,positive,positive,positive
491869484,"feel free to reopen if you want to discuss.

we just release 2.0.0 version, matplotlib is removed",feel free reopen want discus release version removed,issue,positive,positive,positive,positive,positive,positive
491868812,feel free to reopen if you want to discuss,feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
491867504,"Hello, we have just release the 2.0 version, this branch is out-of-date with the base branch.
I suggest the best way is to reopen a PR, then it can be easily to be merged.",hello release version branch base branch suggest best way reopen easily,issue,positive,positive,positive,positive,positive,positive
491676911,Remember to update `changelog.md` and run `yapf -i xxx.py` to refactor the code.,remember update run code,issue,negative,neutral,neutral,neutral,neutral,neutral
491676723,"It is time to merge this PR.

Please run `yapf -i xxx.py` or `yapf -i --recursive xxx(folder)xxx` to refactor the code format.

Also, I suggest to put the keras cells into `tl.layers.` then we can reimplement the cell function in the future, without requiring users to change their code.",time merge please run recursive folder code format also suggest put cell function future without change code,issue,negative,neutral,neutral,neutral,neutral,neutral
491597075,"ok, will do so. thanks

Hao <notifications@github.com> 于 2019年5月12日周日 下午2:15写道：

> Hello, remember to put your name in changelog.md and use yapf -i xxxx.py
> to correct the code format.
>
> We just have a discussion for the augments of RNN, we would like to change
> the :
>
>    - return_last_state equals to True by default.
>    - return_2d and return_last_output to False by default.
>
> We are also thinking a better name for the augments ... feel free to
> discuss in the group chat.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorlayer/tensorlayer/pull/958#issuecomment-491595003>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFN2EY3SJ3V7DSYFT3JDPE3PVAJ53ANCNFSM4HLRZATA>
> .
>
",thanks hao hello remember put name use correct code format discussion would like change true default false default also thinking better name feel free discus group chat reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
491595003,"Hello, remember to put your name in `changelog.md` and use `yapf -i xxxx.py` to correct the code format.

We just have a discussion for the augments of RNN, we would like to change the :

- `return_last_state` equals to `True` by default.
- `return_2d` and `return_last_output` to `False` by default.

We are also thinking a better name for the augments ... feel free to discuss in the group chat.",hello remember put name use correct code format discussion would like change true default false default also thinking better name feel free discus group chat,issue,positive,positive,positive,positive,positive,positive
491593156,"yes. text tutorial runnable, rnn api changed as required. PTB tutorial need
review as its perplexity is too high

Jingqing Zhang <notifications@github.com> 于 2019年5月12日周日 下午1:48写道：

> @1FengL <https://github.com/1FengL> Is this PR ready to review?
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorlayer/tensorlayer/pull/958#issuecomment-491592759>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AFN2EY5KLWI3XV7QGPDFFGLPVAGZDANCNFSM4HLRZATA>
> .
>
",yes text tutorial runnable tutorial need review perplexity high ready review reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
491577301,"> @ChrisWu1997 Should I close this PR?
> Ruihai fixed the numpy bug already, but I have changed `tensorlayer/models/core.py` for the docs here.

All checks passed now. Should be closed and merged.",close fixed bug already closed,issue,negative,neutral,neutral,neutral,neutral,neutral
491493443,"@ChrisWu1997 Should I close this PR?
Ruihai fixed the numpy bug already, but I have changed `tensorlayer/models/core.py` for the docs here.",close fixed bug already,issue,negative,positive,neutral,neutral,positive,positive
491490302,"also, remember to correct the python format:

```bash
yapf -i xxx.py
# or for a folder
yapf -i --recursive folder
```",also remember correct python format bash folder recursive folder,issue,negative,neutral,neutral,neutral,neutral,neutral
475570154,do you mean TensorFlow 2.0.0alpha? we will release TensorLayer 2.0.0alpha at the beginning of next month.,mean alpha release alpha beginning next month,issue,negative,negative,negative,negative,negative,negative
473959650,"> For tl.prepro, could you also contribute your code to tensorlayer/tensorlayer? Many thanks

What do you mean?
",could also contribute code many thanks mean,issue,negative,positive,positive,positive,positive,positive
465112233,"Thanks for your reply, but this does not work. However, by changing `assign_params` as following works fine.
```
def assign_params(sess, params, network):
    for idx, param in enumerate(params['params']):
           net_g.all_params[idx].load(param, sess)
    return
```",thanks reply work however following work fine sess network param enumerate param sess return,issue,positive,positive,positive,positive,positive,positive
465031994,"Hi, could you have a try on the following code by adding `del weights` ? Many thanks.

```python
def load_and_assign_npz(sess=None, name=None, network=None):
    """"""Load model from npz and assign to a network.

    Parameters
    -------------
    sess : Session
        TensorFlow Session.
    name : str
        The name of the `.npz` file.
    network : :class:`Layer`
        The network to be assigned.

    Returns
    --------
    False or network
        Returns False, if the model is not exist.

    Examples
    --------
    - See ``tl.files.save_npz``

    """"""
    if network is None:
        raise ValueError(""network is None."")
    if sess is None:
        raise ValueError(""session is None."")
    if not os.path.exists(name):
        logging.error(""file {} doesn't exist."".format(name))
        return
    else:
        weights = load_npz(name=name)
        assign_weights(sess, weights, network)
        logging.info(""[*] Load {} SUCCESS!"".format(name))
        del weights   # <--- new
        return network
```",hi could try following code many thanks python load model assign network sess session session name name file network class layer network assigned false network false model exist see network none raise network none sess none raise session none name file exist name return else sess network load success name new return network,issue,positive,positive,neutral,neutral,positive,positive
464030711,"> Any update for this PR?

Yep. i had remove the extra `s` from `paddings`. ",update yep remove extra,issue,negative,neutral,neutral,neutral,neutral,neutral
459943509,"@zsdonghao yes,i know. But it's not satisfy the performance requirement. Also i'm very interested in quantisation.",yes know satisfy performance requirement also interested,issue,positive,positive,positive,positive,positive,positive
459643235,"I'm trying to implement a real time pose estimation(mulit-person) on mobile. For the mobile performence, I'm trying to quantize the model.",trying implement real time pose estimation mobile mobile trying quantize model,issue,negative,positive,positive,positive,positive,positive
457987879,"Hi, we are refactoring all codes in [tensorlayer2 branch: tl2-layer](https://github.com/zsdonghao/tensorlayer2/tree/tl2-layer), if you feel there are some bugs there, feel free to make commits ~",hi branch feel feel free make,issue,positive,positive,positive,positive,positive,positive
457130550,"Hi, Is this issue still open, I would like to work on this, I am new here.",hi issue still open would like work new,issue,negative,positive,neutral,neutral,positive,positive
454312384,"I would advise that you don't do anything until the API is fixed (RC released is out).
The API changes all the time at the moment",would advise anything fixed time moment,issue,negative,positive,neutral,neutral,positive,positive
454309666,"after running `tf_upgrade_v2` for layers, some `initialisations` need manual check:

```bash
Detected 109 errors that require attention
--------------------------------------------------------------------------------
tensorlayer/layers/recurrent.py:151:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:336:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:640:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/recurrent.py:642:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/recurrent.py:646:61: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:715:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:913:50: tf.zeros_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/recurrent.py:1051:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:1306:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/recurrent.py:1581:24: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/activation.py:48:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/activation.py:120:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/activation.py:200:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:226:19: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:228:23: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:230:23: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:284:18: tf.nn.embedding_lookup requires manual check: WARNING: validate_indices argument has been removed..
tensorlayer/layers/inputs.py:290:12: tf.nn.nce_loss requires manual check: WARNING: `partition_strategy` has been removed from `tf.nn.nce_loss`  The 'div' strategy is used by default..
tensorlayer/layers/inputs.py:340:19: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:367:18: tf.nn.embedding_lookup requires manual check: WARNING: validate_indices argument has been removed..
tensorlayer/layers/inputs.py:413:35: tf.random_uniform_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/inputs.py:446:26: tf.nn.embedding_lookup requires manual check: WARNING: validate_indices argument has been removed..
tensorlayer/layers/utils.py:45:57: tf.zeros_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/utils.py:46:55: tf.zeros_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/utils.py:50:40: tf.ones_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/utils.py:51:8: tf.zeros_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/normalization.py:170:22: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:171:23: tf.random_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:172:29: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:202:32: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:235:28: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:304:24: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:308:73: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:452:72: tf.ones_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:453:70: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:456:83: tf.ones_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:457:81: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:506:22: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:507:23: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:508:29: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:529:79: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/normalization.py:530:77: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/scale.py:40:69: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/spatial_transformer.py:154:19: tf.ones_like requires manual check: WARNING: tf.zeros_like and tf.ones_like no longer have the optimize argument in TF 2.0 or after (also, `tensor' argument is renamed to `input').
The calls have been converted to compat.v1 for safety (even though  they may already have been correct)..
tensorlayer/layers/dense/ternary_dense.py:52:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/ternary_dense.py:53:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense_bn.py:84:23: tf.ones_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense_bn.py:85:22: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense_bn.py:87:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense_bn.py:136:58: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense_bn.py:143:28: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/binary_dense.py:51:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/binary_dense.py:52:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/dorefa_dense.py:60:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/dorefa_dense.py:61:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense.py:57:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/quan_dense.py:58:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/dropconnect.py:65:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/dropconnect.py:66:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/dense/dropconnect.py:108:20: WARNING: tf.nn.dropout has changed the semantics of the second argument. Please check the applied transformation.
tensorlayer/layers/convolution/ternary_conv.py:81:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/ternary_conv.py:82:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/ternary_conv.py:147:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/separable_conv.py:69:19: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/separable_conv.py:176:19: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:59:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:60:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:189:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:190:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:233:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/expert_conv.py:292:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_conv.py:293:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/deformable_conv.py:77:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/deformable_conv.py:78:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_deconv.py:69:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_deconv.py:70:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_deconv.py:144:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_deconv.py:145:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_deconv.py:101:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_deconv.py:102:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_deconv.py:187:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/expert_deconv.py:188:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/atrous_conv.py:206:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/atrous_conv.py:207:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_conv.py:73:48: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_conv.py:74:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_conv.py:164:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/simplified_conv.py:165:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/dorefa_conv.py:73:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/dorefa_conv.py:74:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/dorefa_conv.py:140:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/binary_conv.py:79:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/binary_conv.py:80:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/binary_conv.py:147:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/quan_conv.py:85:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv.py:86:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv.py:152:18: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/quan_conv_bn.py:108:23: tf.ones_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv_bn.py:109:22: tf.zeros_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv_bn.py:113:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv_bn.py:151:19: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/quan_conv_bn.py:174:58: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv_bn.py:181:28: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/quan_conv_bn.py:210:24: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".
tensorlayer/layers/convolution/depthwise_conv.py:83:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/depthwise_conv.py:84:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/group_conv.py:55:19: tf.truncated_normal_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/group_conv.py:56:19: tf.constant_initializer requires manual check: WARNING: tf.initializers and tf.keras.initializers no longer have the dtype argument in the constructor or partition_info argument in the call method in TF 2.0 and after. The only API symbols are now tf.keras.initializers.* or tf.initializers.*.
The calls have been converted to compat.v1 for safety (even though they may already have been correct)..
tensorlayer/layers/convolution/group_conv.py:83:38: tf.nn.conv2d requires manual check: WARNING: use_cudnn_on_gpu argument has been removed and ""filter"" was renamed to ""filters"".

```",running need manual check bash require attention manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed manual check warning removed strategy used default manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer optimize argument also tensor argument input converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct warning semantics second argument please check applied transformation manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning longer argument constructor argument call method converted safety even though may already correct manual check warning argument removed filter,issue,negative,neutral,neutral,neutral,neutral,neutral
453495044,Nothing to do with TL. Please go on stack overflow.,nothing please go stack overflow,issue,negative,neutral,neutral,neutral,neutral,neutral
450908000,"it use horovod, you can setup via https://github.com/tensorlayer/tensorlayer/tree/master/scripts

hope it help",use setup via hope help,issue,positive,neutral,neutral,neutral,neutral,neutral
447625790,"@zsdonghao please don't merge without:
1. Updating the Changelog
2. Changing the dependencies to a range instead of a fixed pinned

Now we need a new PR to fix this one",please merge without range instead fixed pinned need new fix one,issue,negative,positive,positive,positive,positive,positive
446197137,"I will try to look into conda packaging, no promise on release date.
",try look promise release date,issue,negative,neutral,neutral,neutral,neutral,neutral
446196891,"Hi @20chase here is my  matplot version

matplotlib                2.2.3            py27hb69df0a_0  https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main

i use conda upgrade --all first and reinstall matplotlib. 

i use the faulthander and guess is binary compability issue
hope it works for you",hi chase version use upgrade first reinstall use guess binary issue hope work,issue,negative,positive,positive,positive,positive,positive
446189548,"@fangde Can you tell me the version of matplotlib you used? 

It would be appreciated if tensorlayer provide a conda package. In this case, the conflicts would be solved. ",tell version used would provide package case would,issue,negative,neutral,neutral,neutral,neutral,neutral
446092200,@20chase I fixed it via upgrade the matplot via conda,chase fixed via upgrade via,issue,negative,positive,neutral,neutral,positive,positive
446071080,"I have the same issue, have you figure out why?
downgrade tl is not good option to solve this",issue figure downgrade good option solve,issue,positive,positive,positive,positive,positive,positive
445162305,"Hi, it is not a bug. In doing so, up-sampling layer supports both dynamic or fixed input size.
I updated it here https://github.com/tensorlayer/tensorlayer/pull/901 for showing the actual size if the input has actual size.",hi bug layer dynamic fixed input size showing actual size input actual size,issue,negative,positive,neutral,neutral,positive,positive
442330961,@duohappy  Try to describe your problem. I hope I can help you to find out the reason.,try describe problem hope help find reason,issue,negative,neutral,neutral,neutral,neutral,neutral
442136454,"@zsdonghao I I just updated my tensorlayer to 1.11.1 and the problem still exists.
c.outputs.shape still is (?, ?, ?, 3)
My tensorflow version is 1.12.0.",problem still still version,issue,negative,neutral,neutral,neutral,neutral,neutral
442133770,"@One-sixth hi, which version of TL are you using? I remember this problem happen before, but @DEKHTIARJonathan has fixed this in the latest version.",hi version remember problem happen fixed latest version,issue,negative,positive,positive,positive,positive,positive
441363041,"> 
> 
> if you clone the repo to your machine, remember to copy the tensorlayer folder out.
thanks!!!",clone machine remember copy folder thanks,issue,negative,positive,positive,positive,positive,positive
441361480,"if you clone the repo to your machine, remember to copy the tensorlayer folder out.",clone machine remember copy folder,issue,negative,neutral,neutral,neutral,neutral,neutral
441360086,"I encounter the problem just now, because I am learning how to use tensorlayer to combine tfslim. 😞 .",encounter problem learning use combine,issue,negative,neutral,neutral,neutral,neutral,neutral
441356443,"it will be helpful to format your code. By the way, the py filename is ""tensorlayer.py""?",helpful format code way,issue,negative,neutral,neutral,neutral,neutral,neutral
439604610,"Hi @jackyko1991. Thanks for your bug report.

Unfortunately, this code miss some part (e.g. VNet) and has a lot of parts non necessary.

Can you please provide a **minimal and complete** code with the expected result, that we can try to understand the issue

Here is an example of an issue with a reproducible example: https://github.com/tensorlayer/tensorlayer/issues/686

Thanks",hi thanks bug report unfortunately code miss part lot non necessary please provide minimal complete code result try understand issue example issue reproducible example thanks,issue,positive,negative,neutral,neutral,negative,negative
436979609,"``class SplitLayer(Layer):
    def __init__(
        self,
        layer,
        name ='split_layer',
    ): ``

        super(SplitLayer, self).__init__(prev_layer=layer, name=name)
        self.inputs = layer.outputs
        split0, split1 = tf.split(self.inputs, num_or_size_splits=2, axis=3)
        self.outputs = [split0, split1]
        self._add_layers(self.outputs)``

I write a split layer class and rewrite the shufflenet code as followed, is that correct?
`z = tl.layers.StackLayer([x, y], axis=3, name='stack_layer')  # shape [batch_size, height, width, 2, depth]`
 `z = tl.layers.TransposeLayer(z, perm=[0, 1, 2, 4, 3], name='transpose_layer')`
 `z = tl.layers.ReshapeLayer(z, [batch_size, height, width, 2*depth], name='reshape_layer')`
 `tempt = SplitLayer(z)`
` return z, tempt[0], tempt[1]`",class layer self layer name super self split split split split write split layer class rewrite code correct shape height width depth height width depth tempt return tempt tempt,issue,positive,positive,positive,positive,positive,positive
432962974,"you need to replace `layer.outputs = layer.outputs * TCB_avg_Rescale.outputs` by using `ElementwiseLayer` with `tf.add`, otherwise, you can't store the whole network using npz.
Alternatively, you can use `save_ckpt` instead.",need replace otherwise ca store whole network alternatively use instead,issue,negative,positive,positive,positive,positive,positive
432962392,"```python
def TCB_avg(layer, ratio):
	with tf.name_scope(""TCB_avg"") :
		TCB_temporal = int(layer.outputs.get_shape()[2])
		TCB_spatio = int(layer.outputs.get_shape()[3])
		out_channel = int(layer.outputs.get_shape()[-1])
		TCB_avg_Pool = tl.layers.PoolLayer(layer,
			                               ksize = [1,TCB_temporal,TCB_spatio,TCB_spatio,1],
			                               strides = [1,TCB_temporal,TCB_spatio,TCB_spatio,1],
			                               padding = 'VALID',
			                               pool = tf.nn.avg_pool3d,
			                               name = 'TCB_avg_Pool_layer')
		TCB_avg_Flatten = tl.layers.FlattenLayer(TCB_avg_Pool,
			                                     name = 'TCB_avg_Flatten_layer')
		TCB_avg_F_Dense = tl.layers.DenseLayer(TCB_avg_Flatten,
			                                   n_units = out_channel//ratio,
			                                   act = tf.nn.relu,
			                                   name = 'TCB_avg_First_Dense_layer')
		TCB_avg_S_Dense = tl.layers.DenseLayer(TCB_avg_F_Dense,
			                                   n_units = out_channel,
			                                   act = tf.nn.sigmoid,
			                                   name = 'TCB_avg_Second_Dense_layer')
		TCB_avg_Rescale = tl.layers.ReshapeLayer(TCB_avg_S_Dense,
			                                     shape = [-1,1,1,1,out_channel],
			                                     name = 'TCB_avg_Rescale_layer')
		layer.outputs = layer.outputs * TCB_avg_Rescale.outputs
		return layer

def TCB_max(layer, ratio):
	with tf.name_scope(""TCB_max"") :
		TCB_temporal = int(layer.outputs.get_shape()[2])
		TCB_spatio = int(layer.outputs.get_shape()[3])
		out_channel = int(layer.outputs.get_shape()[-1])
		TCB_max_Pool = tl.layers.PoolLayer(layer,
			                               ksize = [1,TCB_temporal,TCB_spatio,TCB_spatio,1],
			                               strides = [1,TCB_temporal,TCB_spatio,TCB_spatio,1],
			                               padding = 'VALID',
			                               pool = tf.nn.max_pool3d,
			                               name = 'TCB_max_Pool_layer')
		TCB_max_Flatten = tl.layers.FlattenLayer(TCB_max_Pool,
			                                     name = 'TCB_max_Flatten_layer')
		TCB_max_F_Dense = tl.layers.DenseLayer(TCB_max_Flatten,
			                                   n_units = out_channel//ratio,
			                                   act = tf.nn.relu,
			                                   name = 'TCB_max_First_Dense_layer')
		TCB_max_S_Dense = tl.layers.DenseLayer(TCB_max_F_Dense,
			                                   n_units = out_channel,
			                                   act = tf.nn.sigmoid,
			                                   name = 'TCB_max_Second_Dense_layer')
		TCB_max_Rescale = tl.layers.ReshapeLayer(TCB_max_S_Dense,
			                                     shape = [-1,1,1,1,out_channel],
			                                     name = 'TCB_max_Rescale_layer')
		layer.outputs = layer.outputs * TCB_max_Rescale.outputs
		return layer
DenseNet3D:
with tf.variable_scope('block_1'):
	block_1 = dense_block(initial_pool, is_training, 2)
with tf.variable_scope('transition_1'):
	tran_1 = transition_layer(block_1, is_training)
        TCB_avg_tran_1 = TCB_avg(tran_1, 4)
with tf.variable_scope('block_2'):
	block_2 = dense_block(TCB_avg_tran_1, is_training, 3)
with tf.variable_scope('transition_2'):
        tran_2 = transition_layer(block_2, is_training)
       TCB_avg_tran_2 = TCB_avg(tran_2, 4)
```
these are some code, the DenseNet3D above is ok, but when I replace TCB_avg with TCB_max, something wrong like what i asked, and after the block is same. thank you.",python layer ratio layer padding pool name name act name act name shape name return layer layer ratio layer padding pool name name act name act name shape name return layer code replace something wrong like block thank,issue,negative,negative,negative,negative,negative,negative
431647833,"@DEKHTIARJonathan could you test this PR to see if it is as fast as the original implementation for data_format == 'channels_last'?

I think reimplement `tf.nn.batch_normalization` to make it support `data_format == 'channels_first'` is still required, not just for the https://github.com/tensorlayer/openpose-plus project, but for all projects who uses `data_format == 'channels_first'` in BN. 

In fact this should be better fixed in TF not in TL, but it would take a much longer cycle to make the change in TF.",could test see fast original implementation think make support still project fact better fixed would take much longer cycle make change,issue,positive,positive,positive,positive,positive,positive
431571136,"I use both. But the issues is with your python install .

You use Python 3.3.x, not compatible with TF and TL.

Please update to python 3.6.x

_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_",use python install use python compatible please update python galaxy,issue,negative,neutral,neutral,neutral,neutral,neutral
429659161,@Windaway Is this PR ready to merge?  We are thinking to make a 1.11 release and would like to contains this new layer.,ready merge thinking make release would like new layer,issue,positive,positive,positive,positive,positive,positive
429602931,"@zsdonghao @lgarithm if you want to want to merge this PR, LGTM.
Thanks to @luomai, the merge commit it already done in TL 2.0

Before merging please fix these issues: https://app.codacy.com/app/zsdonghao/tensorlayer/pullRequest?prid=2273850",want want merge thanks merge commit already done please fix,issue,positive,positive,positive,positive,positive,positive
429546481,Thanks. I will try to use them when them release.,thanks try use release,issue,negative,positive,positive,positive,positive,positive
429536642,"@luomai @DEKHTIARJonathan I think the cache is working now.
`pip install` is using cache and the time it takes reduced to 45s from 90s.",think cache working pip install cache time reduced,issue,negative,neutral,neutral,neutral,neutral,neutral
429536205,"The test has passed magically without any modification.
@DEKHTIARJonathan @luomai can I merge this PR immediately now?",test magically without modification merge immediately,issue,negative,positive,positive,positive,positive,positive
429534948,"Basically I'd like to cache the `.whl` files downloaded by pip.
I need to make sure they are stored in `$HOME/.cache/pip`.",basically like cache pip need make sure,issue,positive,positive,positive,positive,positive,positive
429534844,"Not yet, maybe some additional steps are required:
https://docs.travis-ci.com/user/caching/#before_cache-phase
Or maybe this needs to be on master to make it work for later CIs.",yet maybe additional maybe need master make work later,issue,negative,neutral,neutral,neutral,neutral,neutral
429534691,Does it actually works? I tried to do it but never succeeded 😅,actually work tried never,issue,negative,neutral,neutral,neutral,neutral,neutral
429306732,"@lgarithm given your knowledge about affine transformation, could you have a look about the affine transformation API and tutorial written by @zsdonghao  to see if that makes sense to you? ",given knowledge affine transformation could look affine transformation tutorial written see sense,issue,negative,neutral,neutral,neutral,neutral,neutral
429302723,"@DEKHTIARJonathan I didn't see that many places this PR would conflict with the layer refactoring in TL 2.0. In case there is any, I will resolve that in my TL 2.0 branch. I vote for merging this PR and make a release. ",see many would conflict layer case resolve branch vote make release,issue,negative,positive,positive,positive,positive,positive
429300616,"@zsdonghao I'm sorry, but this PR conflicts with TL 2.0 and we said no more new features until TL 2.0 is out. If we merge this PR, this means again more work for TL 2.0 and no one is helping for it.

If you want to merge this one, let's finish TL 2.0 and then you can merge it. I know it's not a fun a work, but it's necessary to commit a little work to make things go further.

If this PR is necessary for openpose, please build the library wheel and add it to the repository. 

_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_",sorry said new merge work one helping want merge one let finish merge know fun work necessary commit little work make go necessary please build library wheel add repository galaxy,issue,positive,negative,neutral,neutral,negative,negative
428998966,"I vote to merge this one, it wouldn't affect layers and it is being use in openpose-plus project",vote merge one would affect use project,issue,negative,neutral,neutral,neutral,neutral,neutral
428979264,"Can we make a release to contain this API and a bug fix? I agree that prepro.py shall be refactored into multiple files later. But can we do that after TL 2.0. This PR is required by TL users due to the rising popularity of openpose-plus. Also, affine transformation has shown great speed up for image augmentation. Thoughts? @zsdonghao @DEKHTIARJonathan @lgarithm ",make release contain bug fix agree shall multiple later due rising popularity also affine transformation shown great speed image augmentation,issue,positive,positive,positive,positive,positive,positive
428972384,"I tried to run openpose-plus project, but it seen that I need this branch and #849 , it is hard to manage like that. 

It is better to merge these two branches asap.",tried run project seen need branch hard manage like better merge two,issue,positive,positive,positive,positive,positive,positive
427676173,"Hey @One-sixth , we recently also discover the same speed up using CV2 in #857 . We are working on integrating this into our tl.prepro API. In fact, in our [openpose-plus](https://github.com/tensorlayer/openpose-plus) project where data pre-processing is pretty heavy, using affine transformation + cv2 can offer 78x speed up. ",hey recently also discover speed working fact project data pretty heavy affine transformation offer speed,issue,negative,positive,neutral,neutral,positive,positive
427663320,Great work. Hope to see it soon.,great work hope see soon,issue,positive,positive,positive,positive,positive,positive
427638238,"Hi, I also work on something like this in #857
but didn’t finish the coordinate part. Probably we can discuss in that PR and you can commit into it.

I provided cv2 wrapper and allow users to combine all affine matrices, then only one transformation is enough.

```python
    # 1. get all affine transform matrices
    M_rotate = tl.prepro.affine_rotation_matrix(rg=20, is_random=False)
    M_flip = tl.prepro.affine_horizontal_flip_matrix(is_random=False)
    M_shift = tl.prepro.affine_shift_matrix(wrg=0.2, hrg=0.2, h=h, w=w, is_random=False)
    M_shear = tl.prepro.affine_shear_matrix(intensity=0.2, is_random=False)
    M_zoom = tl.prepro.affine_zoom_matrix(zoom_range=0.8)
    # 2. combine all affine transform matrices to one matrix, the rotation is the first transformation
    M_combined = M_rotate.dot(M_flip).dot(M_shear).dot(M_zoom).dot(M_shift)
    # 3. transfrom the matrix from Cartesian coordinate (the origin in the middle of image)
    # to Image coordinate (the origin on the top-left of image)
    transform_matrix = tl.prepro.transform_matrix_offset_center(M_combined, h, w)
    # 4. then we can transfrom the image once for all transformations
    result = tl.prepro.affine_transfrom_cv2(image, transform_matrix)
```",hi also work something like finish part probably discus commit provided wrapper allow combine affine matrix one transformation enough python get affine transform matrix combine affine transform matrix one matrix rotation first transformation matrix origin middle image image origin image image result image,issue,positive,positive,neutral,neutral,positive,positive
427635111,"@One-sixth interesting point. Could you open a PR implementing your proposal ?
I'll be glad to review it ;)
",interesting point could open proposal glad review,issue,positive,positive,positive,positive,positive,positive
425963229,Btw. Please reflect the changes that you made on Readme.md into Readme.rst (necessary for PyPI),please reflect made necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
425732820,"I vote **not to merge** this PR for the following reasons:

1. A merge commit is required for TL 2.0, `tl.prepro` has been modified. It's already complex enough like this. If you need these features now, you still can provide a python wheel file to install a temporary version of TL.

2. I just noticed that the file `tensorlayer/prepro.py` is around 4000 lines. And this is not acceptable.
This files need to be refactored into a module and numerous sub-files.

So to conclude a lot of work need to be done on this topic, work which is not a top priority at the moment.",vote merge following merge commit already complex enough like need still provide python wheel file install temporary version file around acceptable need module numerous conclude lot work need done topic work top priority moment,issue,positive,positive,neutral,neutral,positive,positive
425724042,Not only the Layer module has been modified. If it can be merged without problem. I'll accept the merge :smile:,layer module without problem accept merge smile,issue,negative,positive,positive,positive,positive,positive
425714304,"**Related Issue**: `NCHW/NHWC`, `channels_first/last` (Issue #561, PR #640)
We need to refer to comments on the posts.",related issue issue need refer,issue,negative,neutral,neutral,neutral,neutral,neutral
425712837,"@DEKHTIARJonathan this PR would not affect the layer APIs, and we really need it for the openpose project.",would affect layer really need project,issue,negative,positive,positive,positive,positive,positive
425712718,"@DEKHTIARJonathan Hey John, this PR would not touch any layer. It only affect tl.prepro.",hey would touch layer affect,issue,negative,neutral,neutral,neutral,neutral,neutral
425711487,"@lgarithm your PR is bugged ... I don't know the issue ... Travis can't manage to build ... This doesn't make any sense as you haven't modify any code ...

Link: https://travis-ci.org/tensorlayer/tensorlayer/jobs/435173003#L755

Not the first time I have this issue: https://github.com/travis-ci/travis-ci/issues/8982",know issue travis ca manage build make sense modify code link first time issue,issue,negative,positive,positive,positive,positive,positive
425711009,"@zsdonghao @2wins All right. Let's do this. I close this PR, we will follow this trend in PR #755 ;)",right let close follow trend,issue,negative,positive,positive,positive,positive,positive
425708989,"
I close this issue, feel free to reopen it if you want to discuss.",close issue feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
425708968,"fixed. 
I close this issue, feel free to reopen it if you want to discuss.",fixed close issue feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
425708938,"The `reuse=True` means we use the same network, this is how TF static graph reuses a network.

I close this issue, feel free to reopen it if you want to discuss.",use network static graph network close issue feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
425708839,"hi, I close this issue, feel free to reopen it if you want to discuss.",hi close issue feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
425708785,"hi, people use dataset API and TL together now.
I close this issue, feel free to reopen it if you want to discuss.",hi people use together close issue feel free reopen want discus,issue,positive,positive,positive,positive,positive,positive
425708661,"It seen TF will use `channel_last/first` to replace all `NCHW/NHCW`,  we can follow them. ",seen use replace follow,issue,negative,neutral,neutral,neutral,neutral,neutral
425681412,"@luomai please write the docs, and also the dataset prefetch order of existing examples.

- [x] `docs/modules/prepro.rst`
- [x] `examples/data_process/README.md`
- [x] `examples/basic_tutorial/tutorial_cifar10_datasetapi.py`
- [x] check https://github.com/tensorlayer/tensorlayer/pull/855/files",please write also order check,issue,negative,neutral,neutral,neutral,neutral,neutral
425640070,"> @DEKHTIARJonathan Actually, if we want to provide two kinds of `data_format`s, then we allow users use consistent arguments e.g. only `channel_last/first` instead of `NCHW/NHCW`, or both of them. At that time, string mapping techniques are needed. (Some convs in TF take `channel_last/first` while others do `NCHW/NHCW`)
> If we only support the argument `channel_last` (`NHWC`), we don't need such processing because a user don't put an argument into it and we just handle `data_format` internally.



> @DEKHTIARJonathan As seen in the @zsdonghao 's last comment, we can handle this issue by removing `data_format` and just using the argument set to `channels_last` internally.

@DEKHTIARJonathan, @zsdonghao As I said before, many conv layers are inconsistent in terms of argument `data_format`. Some of them take `NCHW/NHWC` and others do `channels_first/channels_last`. Meanwhile, some do not have `data_format` and process features (tensors) in a `NHWC/channels_last` way internally. We have to discuss it.",actually want provide two allow use consistent instead time string take support argument need user put argument handle internally seen last comment handle issue removing argument set internally said many inconsistent argument take meanwhile process way internally discus,issue,negative,positive,positive,positive,positive,positive
425639312,"Can we close this @lgarithm, this PR is not active for a while and it does not seem to be a short term objective ;)

Furthermore, the workload to merge with master and TL 2.0 is going to be quite high... I don't believe it is worth the effort at the moment.",close active seem short term objective furthermore merge master going quite high believe worth effort moment,issue,positive,positive,neutral,neutral,positive,positive
425638980,"@zsdonghao can you have a look ? If you confirm the bug, we can open PR and merge before TL 2.0. 
This is not a problem for me ;)",look confirm bug open merge problem,issue,negative,neutral,neutral,neutral,neutral,neutral
425638901,"I don't understand the problem. Try recreating the virtualenv.
The issue is clearly on your side, you are the only one having this issue.

Please open a new issue if you have any clues that the problem comes from TensorLayer. For now, I highly doubt about it.",understand problem try issue clearly side one issue please open new issue problem come highly doubt,issue,negative,positive,neutral,neutral,positive,positive
424403073,"@2wins in the latest version, it seen that conv2d and many other layers already support `channel_first`, also the batch norm layer",latest version seen many already support also batch norm layer,issue,negative,positive,positive,positive,positive,positive
424328363,@wagamamaz We need to discuss `data_format`. Many layers we have do not support it normally as can be seen in #640.,need discus many support normally seen,issue,negative,positive,positive,positive,positive,positive
424324358,"@DEKHTIARJonathan I think this one can be merged first, we need to support these kinds of layers, and I have no idea when the 2.0 can be finished..  😭",think one first need support idea finished,issue,negative,positive,positive,positive,positive,positive
424323931,"@2wins I see, could you raise an exception if the input doesn't fix the shape? 
Also, could we support `channel_first` using `data_format`? Many thanks.",see could raise exception input fix shape also could support many thanks,issue,positive,positive,positive,positive,positive,positive
424323261,"> @2wins Thanks.
> 
> Could you also change these two files:
> 
> * https://github.com/tensorlayer/tensorlayer/blob/master/docs/modules/layers.rst
> * https://github.com/tensorlayer/tensorlayer/blob/master/CHANGELOG.md

@zsdonghao OK. I'll reflect it to them.

> It seen this layer only support 2D images?
> 
> ```
> mean, var = tf.nn.moments(x, [1, 2, 4], keep_dims=True)
> ```

@wagamamaz Yes, I only consider 2D input case like `InstanceNormLayer`.",thanks could also change two reflect seen layer support mean yes consider input case like,issue,positive,negative,neutral,neutral,negative,negative
424320448,"It seen this layer only support 2D images?

```
mean, var = tf.nn.moments(x, [1, 2, 4], keep_dims=True)
```",seen layer support mean,issue,negative,negative,negative,negative,negative,negative
424319763,"@2wins Thanks. 

Could you also change these two files:

- https://github.com/tensorlayer/tensorlayer/blob/master/docs/modules/layers.rst

- https://github.com/tensorlayer/tensorlayer/blob/master/CHANGELOG.md",thanks could also change two,issue,negative,positive,positive,positive,positive,positive
424313272,"Thanks, as @zsdonghao changed `sign` to `Sign, I close this issue now. 
Feel free to reopen if you have any other problems.",thanks sign sign close issue feel free reopen,issue,positive,positive,positive,positive,positive,positive
424056030,"Even we have fixed the support of `tf.nn.bias_add` in tensorRT, this PR is still required to make `BatchNormLayer` work with tensorRT, because currently it doesn't support `channels_first`.",even fixed support still make work currently support,issue,positive,positive,neutral,neutral,positive,positive
424055098,"I just had a long conversation with @luomai, and I've learnt that the most concern of changing the API is about backward compatibility.

My favourite change in TL 2.0 would be moving the `net` argument to the `__call__` method of a layer instance, that means 

```
y = Layer(net, otherParams)
```

would change to 

```
y = Layer(otherParams)(net)
```

This would make it easier to reuse layers with the same `otherParams` (like padding, strides).
To see how it could simplify the code, just compare  https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/models/vgg16.py and https://github.com/tensorlayer/tensorlayer/issues/770#issuecomment-416543581


I think this should be the **only user visible change**.

But I don't know how would existing users react to this change if it has to happen and would like to hear about some opinion.


I think other changes like model API should be additional features and it should be technically possible to keep the original API unchanged.",long conversation learnt concern backward compatibility change would moving net argument method layer instance layer net would change layer net would make easier reuse like padding see could simplify code compare think user visible change know would react change happen would like hear opinion think like model additional technically possible keep original unchanged,issue,positive,positive,neutral,neutral,positive,positive
424049571,"@DEKHTIARJonathan 
This bug will only be trigged when you are converting the model to tensorRT and `b` is not a constant in `tf.nn.bias_add`. The fix here will be just enough for current use case.

And this is just a temporary solution. The long term solution would be fixing `convert_bias_add` in tensorRT. It should be easy but the code is not open sourced. ",bug converting model constant fix enough current use case temporary solution long term solution would fixing easy code open,issue,positive,positive,neutral,neutral,positive,positive
423976756,"We have a problem. We use tf.nn.bias_add everywhere. If this is problem, this problem is everywhere 

It really seems weird that one of the core functionalities of TF have such an issue

_Sent from my Galaxy S9+ using [FastHub](https://play.google.com/store/apps/details?id=com.fastaccess.github)_",problem use everywhere problem problem everywhere really weird one core issue galaxy,issue,negative,negative,negative,negative,negative,negative
423975957,@DEKHTIARJonathan We will merge this PR into your model-api PR. No worry. It shall be simple.,merge worry shall simple,issue,negative,neutral,neutral,neutral,neutral,neutral
423974404,"@DEKHTIARJonathan We are adding minimal but necessary features that on-going TL-related projects are required, and also bug fix. They are mostly regarding to the distributed training and the openpose-plus project. The openpose-plus project is to make TL fully completely compatible with TensorRT 4.0.",minimal necessary also bug fix mostly regarding distributed training project project make fully completely compatible,issue,negative,positive,positive,positive,positive,positive
423972413,"As TensorFlow 2.0 is coming later the end of this year and comes with the new eager execution mode, this PR shall be merged after TF 2.0 and must be compatible with TF 2.0. We also need to check how does the eager mode affect the existing API and check its performance overheads.

A bunch of old TF APIs would be removed, and we need to check how does it affect us.

Also, according to a recent discussion with other TL contributors @zsdonghao @wagamamaz @nebulaV @Windaway @lgarithm @fangde , a API design doc is required to create consensus to the proposed API change. ",coming later end year come new eager execution mode shall must compatible also need check eager mode affect check performance bunch old would removed need check affect u also according recent discussion design doc create consensus change,issue,positive,positive,neutral,neutral,positive,positive
423971445,"Wait before merging please. We are changing all layers for TL 2.0...
I prefer finishing this PR first. It's already complex, we don't need an increase in complexity ",wait please prefer finishing first already complex need increase complexity,issue,positive,negative,neutral,neutral,negative,negative
423970110,"@DEKHTIARJonathan 
the detail is here: https://github.com/tensorlayer/openpose-plus/issues/75

TL;DR;
The support of tf.nn.bias_add in tensorRT is incomplete, it only works when `b` is a constant.
And in BN, b is computed from rolling mean and rolling variant, which isn't a constant. ",detail support incomplete work constant rolling mean rolling variant constant,issue,negative,negative,negative,negative,negative,negative
423048254,"@jianlong-yuan
Im really sorry, I have no time to investigate the issue as @zsdonghao...

If you ever find the issue, feel free to open a PR, we will happily merge your work 😊",really sorry time investigate issue ever find issue feel free open happily merge work,issue,positive,positive,positive,positive,positive,positive
423046728,"@DEKHTIARJonathan Hi, i just write a simple code to get this problems, so can you help me? ",hi write simple code get help,issue,negative,neutral,neutral,neutral,neutral,neutral
423046544,"

```python
import tensorflow as tf
import tensorlayer as tl
import time
import os
os.environ['CUDA_VISIBLE_DEVICES']='1'
slim=tf.contrib.slim
```

    /users2/ml/jlong.yuan/config/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters


    WARNING:tensorflow:From /users2/ml/jlong.yuan/config/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use the retry module or similar alternatives.



```python
a = tf.ones([1,10,10,1])
with tf.name_scope('standard'):
    b = slim.conv2d(a, num_outputs=2, kernel_size=[3,3], stride=1, scope='conv/test', weights_initializer=tf.ones_initializer(), activation_fn=None)

with tf.name_scope('temsorlayer'):
    c = tl.layers.InputLayer(a, name='input')
    offset1 = tl.layers.Conv2d(c, 18, (3, 3), (1, 1), act=tf.nn.relu, padding='SAME', name='offset1')
    c = tl.layers.DeformableConv2d(c, offset1, n_filter=2, filter_size=(3,3), W_init=tf.ones_initializer(), b_init=None)
```

    [TL] InputLayer  input: (1, 10, 10, 1)
    [TL] Conv2d offset1: n_filter: 18 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu
    [TL] DeformableConv2d deformable_conv_2d: n_filter: 2, filter_size: (3, 3) act: No Activation



```python
op1=tf.train.MomentumOptimizer(0.1, momentum=0.9)
op2=tf.train.MomentumOptimizer(0.1, momentum=0.9)
```


```python
loss1= b - tf.zeros([1,10,10,2])
loss2= c.outputs - tf.to_float(tf.zeros([1,10,10,2]))

```


```python
for i in tf.trainable_variables():
    print(i)
```

    <tf.Variable 'conv/test/weights:0' shape=(3, 3, 1, 2) dtype=float32_ref>
    <tf.Variable 'conv/test/biases:0' shape=(2,) dtype=float32_ref>
    <tf.Variable 'offset1/kernel:0' shape=(3, 3, 1, 18) dtype=float32_ref>
    <tf.Variable 'offset1/bias:0' shape=(18,) dtype=float32_ref>
    <tf.Variable 'deformable_conv_2d/W_deformableconv2d:0' shape=(1, 1, 9, 1, 2) dtype=float32_ref>



```python
grad1 = tf.gradients(loss1, tf.trainable_variables()[0])
grad2 = tf.gradients(loss2, tf.trainable_variables()[0:])
```


```python
train_op1 =op1.apply_gradients(zip(grad1, [tf.trainable_variables()[0]]))
train_op2 =op2.apply_gradients(zip(grad2, tf.trainable_variables()[0:]))
```


```python
sess=tf.Session()
```


```python
sess.run(tf.global_variables_initializer())
```


```python
s=time.time()

sess.run([train_op1])

e=time.time()
```


```python
print(e-s)
```

    0.11040878295898438



```python
s=time.time()

sess.run([train_op2])

e=time.time()
```


```python
print(e-s)
```

    1.5291271209716797

",python import import import time import o conversion second argument float future float import warning retry removed future version use retry module similar python offset offset input offset pad act act activation python python python print python grad loss grad loss python zip grad zip grad python python python python print python python print,issue,negative,neutral,neutral,neutral,neutral,neutral
422885432,"@zsdonghao @luomai @lgarithm for information.
Matplotlib drop support for Python 2. Tensorflow will soon do the same ;)",information drop support python soon,issue,negative,neutral,neutral,neutral,neutral,neutral
421966348,"Hi, I don't have time to work on this issue at the moment, any contributions could be welcome.",hi time work issue moment could welcome,issue,negative,positive,positive,positive,positive,positive
421498428,Better to put this in a very early position.,better put early position,issue,negative,positive,positive,positive,positive,positive
420273034,try to set padding to `valid` then,try set padding valid,issue,negative,neutral,neutral,neutral,neutral,neutral
419998740,"the stride value will control the output size.

e.g. if your `strides=(2, 2)`, your input is `100x100,3`, the output will become `200x200x3`",stride value control output size input output become,issue,negative,neutral,neutral,neutral,neutral,neutral
419556958,"btw, tensorflow doesn't support NCHW on CPU machine, there will a runtime error for CPU users
```
UnimplementedError (see above for traceback): Generic conv implementation only supports NHWC tensor format for now.
```",support machine error see generic implementation tensor format,issue,negative,neutral,neutral,neutral,neutral,neutral
418814672,"https://tensorlayer.readthedocs.io/en/stable/modules/distributed.html

We now have a simple way to support distributed training.

Feel free to reopen this issue if you want to discuss.",simple way support distributed training feel free reopen issue want discus,issue,positive,positive,positive,positive,positive,positive
418310101,"![image](https://user-images.githubusercontent.com/10923599/45024562-9d17b080-b039-11e8-930b-7ca41aeeb8b0.png)

I'm gonna have a look",image gon na look,issue,negative,neutral,neutral,neutral,neutral,neutral
418309444,@DEKHTIARJonathan I have no idea why the style test will fail ..,idea style test fail,issue,negative,negative,negative,negative,negative,negative
418303325,@zsdonghao I have edited your post. I will merge this PR once we have a unittest to highlight the issue.,post merge highlight issue,issue,negative,neutral,neutral,neutral,neutral,neutral
418302422,"Hi @ndiy , it seems that your update will break the test codes in tests folder

https://travis-ci.org/tensorlayer/tensorlayer/builds/424147149?utm_source=github_status&utm_medium=notification

please check.",hi update break test folder please check,issue,negative,neutral,neutral,neutral,neutral,neutral
417974430,"I see.The latest version contain the line of code in super class ""Layer"" ,altogether,but the previous version did not.Thank you .",latest version contain line code super class layer altogether previous version,issue,positive,positive,positive,positive,positive,positive
417944916,"Can u check with the latest version of TensorLayer?
It's version 1.9.1",check latest version version,issue,negative,positive,positive,positive,positive,positive
417915863,"cannot import the cifar10
![image](https://user-images.githubusercontent.com/29772895/44954304-ac301e80-aed2-11e8-9927-8e8847e53c2e.png)


2018.9.3
I have found the reason.",import image found reason,issue,negative,neutral,neutral,neutral,neutral,neutral
417844606,"As it is `rc` version, it would not be shown as stable version on online docs, we should set readthedocs to show latest version, right?",version would shown stable version set show latest version right,issue,negative,positive,positive,positive,positive,positive
417510875,Have you tested on a clean machine to see if this has resolved the issue?,tested clean machine see resolved issue,issue,positive,positive,positive,positive,positive,positive
416547499,"@DEKHTIARJonathan @lgarithm  could we support these?

1) advanced decorator, supports the existing way to define layer.

```python
net = DenseLayer(net, 100)
```

```python
model.add(DenseLayer(100))
```

https://github.com/ildoonet/tf-pose-estimation/blob/master/tf_pose/network_base.py#L170
https://github.com/ildoonet/tf-pose-estimation/blob/master/tf_pose/network_mobilenet.py#L23

2) auto naming, adds name automatically if not using reuse.

```python
with tl.layers.auto_name():
       model.add(DenseLayer(100))
       model.add(DenseLayer(100))
```


3) skip, supports resnet with model

```python
model.add(Conv2d(name=“conv1”))
model.add(Conv2d(name=“conv2”))
model.add(Conv2d(name=“conv3”))
model(""conv1"", ""conv3”).add(Concat())
```",could support advanced decorator way define layer python net net python auto naming name automatically reuse python skip model python model,issue,positive,positive,positive,positive,positive,positive
416545698,"Let's see if we can do something like this:

class VGG16Base(object):
    """"""The VGG16 model.""""""

    @staticmethod
    def vgg16_simple_api(net_in, end_with):
        with tf.name_scope('preprocess'):
            # Notice that we include a preprocessing layer that takes the RGB image
            # with pixels values in the range of 0-255 and subtracts the mean image
            # values (calculated over the entire ImageNet training set).

            # Rescale the input tensor with pixels values in the range of 0-255
            net_in.outputs = net_in.outputs * 255.0

            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')
            net_in.outputs = net_in.outputs - mean

        maxpool = MaxPool2d(filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool')
        conv1 = Conv2d(n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv1')
        conv2 = Conv2d(n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv2')
        conv3 = Conv2d(n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3')
        conv4_5 = Conv2d(n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4')

        layers = [
            # conv_1
            (""conv_1"", [conv1, conv1, maxpool]),
            # conv_2
           (""conv_2"", [ conv2, conv2, maxpool]),
            # conv_3
            (""conv_3"", [conv3, conv3, conv3, maxpool]),
            # conv_4
            (""conv_4"", [conv4_5, conv4_5, conv4_5, maxpool]),
            # conv_5
            (""conv_5"", [conv4_5, conv4_5, conv4_5, maxpool]),
            FlattenLayer(name='flatten'),
            DenseLayer(n_units=4096, act=tf.nn.relu, name='fc1_relu'),
            DenseLayer(n_units=4096, act=tf.nn.relu, name='fc2_relu'),
            DenseLayer(n_units=1000, name='fc3_relu'),
        ]

        net = net_in
        for l in layers:
            net = l(net)
            # if end_with in net.name:
            if net.name.endswith(end_with):
                return net

        raise Exception(""unknown layer name (end_with): {}"".format(end_with))",let see something like class object model notice include layer image range mean image calculated entire training set input tensor range mean mean net net net return net raise exception unknown layer name,issue,negative,negative,negative,negative,negative,negative
416543581,"@DEKHTIARJonathan one thing that I would like to do in the new API is

```python
class VGG16Base(object):
    """"""The VGG16 model.""""""

    @staticmethod
    def vgg16_simple_api(net_in, end_with):
        with tf.name_scope('preprocess'):
            # Notice that we include a preprocessing layer that takes the RGB image
            # with pixels values in the range of 0-255 and subtracts the mean image
            # values (calculated over the entire ImageNet training set).

            # Rescale the input tensor with pixels values in the range of 0-255
            net_in.outputs = net_in.outputs * 255.0

            mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 1, 3], name='img_mean')
            net_in.outputs = net_in.outputs - mean

        maxpool = MaxPool2d(filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool')
        conv1 = Conv2d(n_filter=64, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv1')
        conv2 = Conv2d(n_filter=128, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv2')
        conv3 = Conv2d(n_filter=256, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv3')
        conv4_5 = Conv2d(n_filter=512, filter_size=(3, 3), strides=(1, 1), act=tf.nn.relu, padding='SAME', name='conv4')

        layers = [
            # conv1
            conv1, conv1, maxpool,
            # conv2
            conv2, conv2, maxpool,
            # conv3
            conv3, conv3, conv3, maxpool,
            # conv4
            conv4_5, conv4_5, conv4_5, maxpool,
            # conv5
            conv4_5, conv4_5, conv4_5, maxpool,
            FlattenLayer(name='flatten'),
            DenseLayer(n_units=4096, act=tf.nn.relu, name='fc1_relu'),
            DenseLayer(n_units=4096, act=tf.nn.relu, name='fc2_relu'),
            DenseLayer(n_units=1000, name='fc3_relu'),
        ]

        net = net_in
        for l in layers:
            net = l(net)
            # if end_with in net.name:
            if net.name.endswith(end_with):
                return net

        raise Exception(""unknown layer name (end_with): {}"".format(end_with))
```

It could simplify the current implementation VGG a lot.",one thing would like new python class object model notice include layer image range mean image calculated entire training set input tensor range mean mean net net net return net raise exception unknown layer name could simplify current implementation lot,issue,negative,negative,neutral,neutral,negative,negative
415970939,Do I need a revert commit for this one?,need revert commit one,issue,negative,neutral,neutral,neutral,neutral,neutral
415970852,"@lgarithm please do not merge automatically these ... 
We need to update 2 things first:
- update Changelog.md
- unpin the dependency",please merge automatically need update first update unpin dependency,issue,negative,positive,positive,positive,positive,positive
415954315,"The link of binary connect network is:[https://arxiv.org/pdf/1511.00363.pdf](url)
In this paper, author only use binary weight in forward and backward propagation but when it comes to update author update the gradient to original weights.

_**def binarize(x):
    g = tf.get_default_graph()
    with ops.name_scope(""Binarized"") as name:
        with g.gradient_override_map({""Sign"": ""Identity""}):
            x=tf.clip_by_value(x,-1,1)
            return tf.sign(x)**_

It is my implement, so is ""TL_Sign_QuantizeGrad"" same as  ""Identity""?
@zsdonghao 
",link binary connect network paper author use binary weight forward backward propagation come update author update gradient original name sign identity return implement identity,issue,negative,positive,positive,positive,positive,positive
415899860,"As discuss @DEKHTIARJonathan @luomai , in some case: 

1) sub-network : `is_train=False`, and the other layers : `is_train=True`

```python
cnn = tl.models.MobileNetV1(x, end_with='depth13', is_train=False)
net = tl.layers.BatchNormLayer(cnn, is_train=True, name=‘bn1’)
net = tl.layers.Conv2d(net, 32, (3, 3), name=‘cnn’)

sess = tf.InteractiveSession()
tl.layers.initialize_global_variables(sess)

cnn.restore_params(sess)
```

manual compile mode may become difficult. need to think about it.

2) reuse same layer in a model, compile model can't support it. TF vars and Ops are only created at compilation... the reuse here is not useful.

```python
with tf.variable_scope('test'):
    model.add(tl.layers.DenseLayer(n_units=50, act=tf.nn.relu, name=""seq_layer_9""))

with tf.variable_scope('test', reuse=True):
    model.add(tl.layers.DenseLayer(n_units=50, act=tf.nn.relu, name=""seq_layer_9""))
```


",discus case python net net net sess sess sess manual compile mode may become difficult need think reuse layer model compile model ca support compilation reuse useful python,issue,negative,negative,neutral,neutral,negative,negative
415887901,"Only DownSampling2dLayer is concerned.

It must be a layer not oftenly used. It's broken for a long time 😅",concerned must layer used broken long time,issue,negative,negative,negative,negative,negative,negative
415887536,"@DEKHTIARJonathan will this bug make other codes fail to run? 
If it is very important, could we make a PR for it?",bug make fail run important could make,issue,negative,negative,neutral,neutral,negative,negative
415834458,"@wagamamaz Actually you can have the tensor output immediately, if you build your model like that without using Squential:

```python
n = InputLayer()(plh)
n = DenseLayer(100)(n)
n.all_xxx
n.outputs
n = DenseLayer(100)(n)
# no compile is required.
```

But with Sequential, you need to compile in the end.
We can actually make a small tool to help people automatically change existing code to the new way.

@DEKHTIARJonathan Ask we discuss, you want to add `local_params`, `local_drop` in a layer.
I am thinking, should we keep the name the same?

Someone suggests `all_layers` --> `all_outputs` .
If we change it, we can still allow users to use `all_layers`, but we can give a warning.

```python
@property
def all_layers():
     logging.warning(""all_layers --> all_outputs"")
     return self.all_outputs
```
",actually tensor output immediately build model like without python compile sequential need compile end actually make small tool help people automatically change code new way ask discus want add layer thinking keep name someone change still allow use give warning python property return,issue,positive,negative,neutral,neutral,negative,negative
415791878,"@lgarithm  hello, I don't know why the ""layers/cost.py"" doesn't pass the format check, can you help me have a look? ( I just copy and paste it from the master branch )",hello know pas format check help look copy paste master branch,issue,negative,neutral,neutral,neutral,neutral,neutral
415791648,"@DEKHTIARJonathan thanks, just want to check whether you have some suggestions.",thanks want check whether,issue,negative,positive,positive,positive,positive,positive
415771468,"@zsdonghao what am I supposed to review ? There is nothing big that was changed right ?
If so, you can merge, I have approved ;)",supposed review nothing big right merge,issue,negative,positive,positive,positive,positive,positive
415724458,"@zsdonghao Why is this example inside main TL repository ?
It is **huge**, I think it should be like SRGAN you did in its own repository

Btw. I was thinking what do you think about converting the existing examples in Jupyter Notebooks ?
They are a lot easier to read and they are designed to be read effectively.",example inside main repository huge think like repository thinking think converting lot easier read designed read effectively,issue,positive,positive,positive,positive,positive,positive
415721096,"I meet the same problem, so how did you rewrite it in vim in detail, thanks a lot 啊",meet problem rewrite vim detail thanks lot,issue,negative,positive,positive,positive,positive,positive
415700728,"This is due to quantize op

https://github.com/tensorlayer/tensorlayer/blob/69f3f3693b0d7e35f1910fbea3c1ac9c97583cc2/tensorlayer/layers/utils.py#L345

```python
def quantize(x):
    # ref: https://github.com/AngusG/tensorflow-xnor-bnn/blob/master/models/binary_net.py#L70
    #  https://github.com/itayhubara/BinaryNet.tf/blob/master/nnUtils.py
    with tf.get_default_graph().gradient_override_map({""Sign"": ""TL_Sign_QuantizeGrad""}):
        return tf.sign(x)
```",due quantize python quantize ref sign return,issue,negative,negative,negative,negative,negative,negative
415659135,"Sorry for that. @lgarithm. Following is the test code. 
```python
import tensorlayer as tl
import tensorflow as tf
from tensorlayer.layers import *
import numpy as np
t_image = tf.placeholder(tf.float32, [None,128,128,1])
h_image = tf.placeholder(tf.float32, [None,128,128,1])
net = InputLayer(t_image)
net = BinaryConv2d(net)
sess = tf.InteractiveSession()
cost = tl.cost.mean_squared_error(net.outputs, h_image)
train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost, var_list=net.all_params)
tl.layers.initialize_global_variables(sess)

for i in range(1):
    tr = np.random.randn(32,128,128,1)
    hr = np.random.randn(32,128,128,1)
    cost = sess.run([train_op, net.outputs], feed_dict = {t_image:tr, h_image:hr})
```
These codes will throw error like:

> NotImplementedError                       Traceback (most recent call last)
> <ipython-input-1-69743098cf3e> in <module>()
>       9 sess = tf.InteractiveSession()
>      10 cost = tl.cost.mean_squared_error(net.outputs, h_image)
> ---> 11 train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(cost, var_list=net.all_params)
>      12 tl.layers.initialize_global_variables(sess)
>      13 
> 
> ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
>     408 
>     409     return self.apply_gradients(grads_and_vars, global_step=global_step,
> --> 410                                 name=name)
>     411 
>     412   def compute_gradients(self, loss, var_list=None,
> 
> ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
>     603           scope_name = var.op.name
>     604         with ops.name_scope(""update_"" + scope_name), ops.colocate_with(var):
> --> 605           update_ops.append(processor.update_op(self, grad))
>     606       if global_step is None:
>     607         apply_updates = self._finish(update_ops, name)
> 
> ~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in update_op(self, optimizer, g)
>     187 
>     188   def update_op(self, optimizer, g):
> --> 189     raise NotImplementedError(""Trying to update a Tensor "", self._v)
>     190 
>     191 
> 
> NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'binary_cnn2d/Sign:0' shape=(3, 3, 1, 32) dtype=float32>)",sorry following test code python import import import import none none net net net sess cost cost sess range cost throw error like recent call last module sess cost cost sess minimize self loss name return self loss self name self grad none name self self raise trying update tensor update tensor,issue,negative,negative,neutral,neutral,negative,negative
415657940,"For the case 1), could you give the complete example code?
Some variables are not defined in the above sample code.",case could give complete example code defined sample code,issue,negative,positive,neutral,neutral,positive,positive
415634889,"(1) Binary
Some code like this:
```python
import tensorlayer as tl
import tensorflow as tf
from tensorlayer.layers import *

t_image = tf.placeholder(tf.float32, [None,128,128,1])
h_image = tf.placeholder(tf.float32, [None,128,128,1])
net = InputLayer(t_image)
net = BinaryConv2d(net)
sess = tf.InteractiveSession()
train_op = tf.train.AdamOptimizer(learning_rate=LR).minimize(cost, var_list=model.all_params)
tl.layers.initialize_global_variables(sess)
cost = tl.cost.mean_squared_error(net.outputs, h_image)
for i in range(Max_iter):
    tr, hr = dataloader.get_next_train_batch()
    cost = sess.run([train_op, net.outputs], feed_dict = {t_image:tr, h_image:hr})
```
This will give you some error like: tensor can not be gradient.
(2) for the second issue, I try to add code at line 148 in deformable_conv.py. And it works:
```python
        self._add_layers(self.outputs)
        new_variables = get_collection_trainable(offset_layer.name)
        self._add_params(new_variables)
        
        if b_init:
            self._add_params([W, b])
        else:
            self._add_params(W)
```",binary code like python import import import none none net net net sess cost sess cost range cost give error like tensor gradient second issue try add code line work python else,issue,negative,neutral,neutral,neutral,neutral,neutral
415573447,"@ArturoDeza I didn't encounter any version dependent problem. When you make sure you can 'import tensorlayer as tl, tensorflow as tl'  correctly in your python3 environment, then I think it would be enough to run my code.
If you encounter any problems later, you can pose here, maybe I can help : ) ",encounter version dependent problem make sure correctly python environment think would enough run code encounter later pose maybe help,issue,negative,positive,positive,positive,positive,positive
415571533,"Hi @JunbinWang , thanks! Sure I was referring to the versions of CUDA and CUDNN used with tensorflow and tensorlayer (as well as their versions), apologies for not being clear on the request. I know that tensorflow sometimes does not have backwards compatibility or the name of functions change after each version so I wanted to be aware of any possible bugs I might encounter that are version dependent.",hi thanks sure used well clear request know sometimes backwards compatibility name change version aware possible might encounter version dependent,issue,positive,positive,positive,positive,positive,positive
415566231,"Well,  as long as you install Tensorflow and Tenlayer successfully, you can run the code with Python3.
For Tensor-GPU version, of course you need CUDA, CUDNN.",well long install successfully run code python version course need,issue,positive,positive,positive,positive,positive,positive
415494938,@lgarithm yes it is. we want to update some result images.,yes want update result,issue,negative,neutral,neutral,neutral,neutral,neutral
415485802,"Is this an update of #796?
Could you make it branched from master?

I don't have access to push to master branch..
I have the format problem again, do you know how to fix it? 
Thanks !",update could make branched master access push master branch format problem know fix thanks,issue,negative,positive,positive,positive,positive,positive
415406154,"@wagamamaz Thanks a lot. I will try to reimplement this and I come back to you ;)

If you are right and it can't be done with the new paradigm, I think we should abort or completely rethink the change ;)

I may need some time, some layers are not compliant with the new API yet ;)",thanks lot try come back right ca done new paradigm think abort completely rethink change may need time compliant new yet,issue,negative,positive,positive,positive,positive,positive
415404238,"```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-

import tensorflow as tf
import tensorlayer as tl

tf.logging.set_verbosity(tf.logging.DEBUG)
tl.logging.set_verbosity(tl.logging.DEBUG)

# 1) simple dynamic rnn
input_seqs = tf.placeholder(dtype=tf.int64, shape=[None, None], name=""input"")
is_train = True
ne = tl.layers.EmbeddingInputlayer(inputs=input_seqs, vocabulary_size=100, embedding_size=100, name='embedding')
# inside DynamicRNNLayer, batch_size need to be computed automatically from previous outputs.
# there are also many cases that require tensor from previous layers as input of next layer
fixed_batch_size = ne.outputs.get_shape().with_rank_at_least(1)[0]
if fixed_batch_size.value:
    batch_size = fixed_batch_size.value
else:
    from tensorflow.python.ops import array_ops
    batch_size = array_ops.shape(ne.outputs)[0]
print(batch_size)
n = tl.layers.DynamicRNNLayer(
    ne,
    cell_fn=tf.contrib.rnn.BasicLSTMCell,
    n_hidden=100,
    dropout=(0.7 if is_train else None),
    sequence_length=tl.layers.retrieve_seq_length_op2(input_seqs),  # previous tensor is required
    return_last=False,
    return_seq_2d=True,
    name='dynamicrnn'
)
n = tl.layers.DenseLayer(n, n_units=100, name=""output"")
print(input_seqs, ne.outputs)

# 2) intermedia tensor (IMPORTANT)
x = tf.placeholder(tf.float32, [None, 100, 100, 3], name='x')

nin = tl.layers.InputLayer(x, name='in')
n1 = tl.layers.Conv2d(nin, 32, (3, 3), (1, 1), act=tf.nn.relu, name='c1')
n1 = tl.layers.MaxPool2d(n1, (3, 3), (2, 2), 'SAME', name='pad1')
n1 = tl.layers.Conv2d(n1, 32, (3, 3), (1, 1), act=tf.nn.relu, name='c2')
print(n1.all_layers)  # intermedia layers should have this before compiling
print(n1.all_params)  # not only for debugging, but for building complex network
print(n1.outputs)     # I can give more examples later

n2 = tl.layers.Conv2d(nin, 32, (3, 3), (1, 1), act=tf.nn.relu, name='c1')
n2 = tl.layers.MaxPool2d(n2, (3, 3), (2, 2), 'SAME', name='pad1')
n2 = tl.layers.Conv2d(n2, 32, (3, 3), (1, 1), act=tf.nn.relu, name='c2')
print(n1.all_layers)
print(n1.all_params)
print(n1.outputs)

n = tl.layers.ElementwiseLayer([n1, n2], tf.add, name='add')
n.n1_output = n1.outputs   # advanced usage of TL which is useful for complex network
n.n2_output = n2.outputs

# 3) tl.models
x2 = tf.placeholder(tf.float32, [None, 224, 224, 3])
# get VGG without the last layer
vgg = tl.models.VGG16(x2, end_with='fc2_relu')
# add one more layer
n3 = tl.layers.DenseLayer(vgg, 100, name='out')

# initialize all parameters
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# download and restore vgg paramters online
vgg.restore_params(sess)    # vgg need all_params to restore params
train_params = tl.layers.get_variables_with_name('out')
print(train_params)

```

I believe there should be some ways to solve this by following Keras. But if the code is quite different from existing usage, why don't just build another library?
",python python import import simple dynamic none none input true ne inside need automatically previous also many require tensor previous input next layer else import print ne else none previous tensor output print tensor important none print print building complex network print give later print print print advanced usage useful complex network none get without last layer add one layer initialize sess restore sess need restore print believe way solve following code quite different usage build another library,issue,positive,positive,neutral,neutral,positive,positive
415395404,"@wagamamaz alright, let me have a think to this. I prefer thinking first about the problem you highlight instead of saying something wrong ;)

Just to be sure, the new API was designed to be more intuitive, but also to improve the *heavy work happening behind the hood*, current version of TL is quite incredibly inefficient with a lot of redundancy and quite a lot of complexity in the internals.

Can you give me a quick example of the code above with the current version of TL ? That way I have something to work with ;)

Do you mean something like this (a dummy AE):

```python
def get_ae_model(plh, is_train, reuse):  # is_train not useful here, no dropout or BN

    with tf.variable_scope(""my_scope"", reuse=reuse):

        net_encoder = tl.layers.InputLayer(plh)
        net_encoder = tl.layers.DenseLayer(net_encoder, n_units=50)
        net_encoder = tl.layers.DenseLayer(net_encoder, n_units=10)

        net_decoder =  tl.layers.DenseLayer(net_encoder, n_units=50)
        net_decoder =  tl.layers.DenseLayer(net_decoder, n_units=100)

    return net_encoder, net_decoder

plh = tf.placeholder(tf.float16, (None, 100))

encoder, decoder = get_ae_model(plh, is_train=True, reuse=False)
```

Could this kind of workaround solve the issue:

```python
class Encoder_Network(tl.networks.CustomModel):

    def model(self):
        input_layer = tl.layers.InputLayer(name='input_layer')
        net_encoder = tl.layers.DenseLayer(n_units=50)(input_layer)
        net_encoder = tl.layers.DenseLayer(n_units=10)(net_encoder)

        return input_layer, net_encoder 

class Decoder_Network(tl.networks.CustomModel):

    def model(self):
        input_layer = tl.layers.InputLayer(name='input_layer')
        net_decoder = tl.layers.DenseLayer(n_units=50)(input_layer)
        net_decoder = tl.layers.DenseLayer(n_units=100)(net_decoder)

        return input_layer, net_decoder 

model_encoder = Encoder_Network(name=""my_encoder"")
model_decoder = Decoder_Network(name=""my_decoder"")

plh = tf.placeholder(tf.float16, (None, 100))

encoder = model_encoder.compile(plh, reuse=False, is_train=True)  # type == CompiledNetwork

# two possibilities here, it doesn't change anything ;)
decoder = model_decoder.compile(encoder, reuse=False, is_train=True)  # type == CompiledNetwork
decoder = model_decoder.compile(encoder.outputs, reuse=False, is_train=True)  # type == CompiledNetwork

### And then you can do

encoder.save()
encoder.restore()
encoder.all_layers
encoder.all_params

decoder.save()
decoder.restore()
decoder.all_layers
decoder.all_params
```

You can even do this:

```python
class AE_Network(tl.networks.CustomModel):

    def model(self):
        input_layer= tl.layers.InputLayer(name='input_layer')

        net_encoder = tl.layers.DenseLayer(n_units=50)(input_layer)
        net_encoder = tl.layers.DenseLayer(n_units=10)(net_encoder)

        net_decoder = tl.layers.DenseLayer(n_units=50)(net_encoder)
        net_decoder = tl.layers.DenseLayer(n_units=100)(net_decoder)

        return input_layer, net_decoder 

net_AE = AE_Network(name=""my_autoencoder"")

plh = tf.placeholder(tf.float16, (None, 100))

model_ae = net_AE.compile(plh, reuse=False, is_train=True)

### And then you can do

model_ae.save()
model_ae.restore()
model_ae.all_layers
model_ae.all_params
```

Could you give me a precise example, where this could not work ? I must be missing something ;)

----------

Concerning **dynamic RNNs**, to be very honest, I very rarely use RNNs. Could you provide an example, I can't design one myself with current API.",alright let think prefer thinking first problem highlight instead saying something wrong sure new designed intuitive also improve heavy work happening behind hood current version quite incredibly inefficient lot redundancy quite lot complexity internals give quick example code current version way something work mean something like dummy ae python reuse useful dropout return none could kind solve issue python class model self return class model self return none type two change anything type type even python class model self return none could give precise example could work must missing something concerning dynamic honest rarely use could provide example ca design one current,issue,positive,positive,positive,positive,positive,positive
415393088,"@DEKHTIARJonathan Thank you for the quick reply, let me point out a case that I think compile method can't handle: To build complex network, all intermedia layers should have `all_xxx` and `outputs` before compiling.

Take the example I and @lgarithm described above, the tensors of a layer (`all_xxx` and `outputs`) are usually required to build complex network. One example is the dynamic RNN that @zsdonghao described, you will find many problems when building RNN network, and that is why it is hard to use dynamic RNN with Keras. (we can find a lot examples like that) Another disadvantage I can come out with now is that `tl.models` need `all_params` tensors.

**I think this problem need to be solved before we move forward, otherwise, it is just waste of time..**

```python
n1 = Layer1()(x)
n2 = Model1()(n1)  <== 2) tl.models
n3 = Layer3(n1.outputs.shape())(n2)  <== 1) need previous tensor, while with compile, n1 doesn't have `outputs`.
n4 = Layer4()(n3)

n4.compile()

n2.all_params  <== 2) tl.models needs `all_params` to restore the parameters. How `n4.compile()` gives the `all_params` to `n2` ?
n2.restore(sess) <== 2)  restore parameters
n4.all_params
```

Using compile to build simple CNN network is fine, but when you try to build complex network, you will always find problems. This is very important for academic users like me. **Layer as network is a good abstraction.**
Besides, the code you show above is not simple than the existing way. I am not saying I don't want to change my codebase, instead, I can't see the reason to change the codebase ..
Therefore, if the existing usage move to TL 1.x, I believe no one will use it in my lab, all people are forced to update their codes. But to be honest, I personally will move to Tflearn as it is similar with TL. Alternatively, copy existing TL to build another library...

",thank quick reply let point case think compile method ca handle build complex network take example layer usually build complex network one example dynamic find many building network hard use dynamic find lot like another disadvantage come need think problem need move forward otherwise waste time python layer model layer need previous tensor compile layer need restore sess restore compile build simple network fine try build complex network always find important academic like layer network good abstraction besides code show simple way saying want change instead ca see reason change therefore usage move believe one use lab people forced update honest personally move similar alternatively copy build another library,issue,positive,positive,neutral,neutral,positive,positive
415374228,"@wagamamaz I thin your remarks comes from a misunderstanding. We are perfectly aware of the case you would like to use, and it works perfectly with the new API.

I said we would like to have a more Keras-like API because it is more intuitive and a cleaner approach. I never said it has to be identical and work the same way.

The few times I try to use Keras to see how it works, I also struggled on similar situations as you pointed out. The plan is **by no mean** to complexify or prevent users to do things that they currently do easily.

Unfortunately, I can't share with you some code. Because, it's still not perfectly working. However, as I said in our previous meeting.

If any important feature (e.g non sequential model) can't be made to work easily and smoothly, **We will cancel this update and abort the update**. This would be **an absolute no-go**. I 100% agree with you.

-------------

I would also like to highlight something, we used the term `Network.compile()`, maybe this was unfortunate and shall be renamed (any proposition ?). TL compile and Keras compile are by no mean identical. 

Keras compile takes a loss and an optimizer. In the other hand, at the current stage, TL compile is only a function that will generate the corresponding TF Ops/Tensors/Vars of each Layer.

In the new API, Network and Layers shall be understood as ""blueprints"" of networks and layers. The `.compile()` method create the corresponding TF Ops/Tensors/Vars for each Layer/Network.

I hope to made it clearer. @wagamamaz I would like to involve you in the test of the new API. Would you be free for some discussions and maybe some tests ?

TL 2.0 is still under heavy discussion/development, and i would love to have someone like you to have an honest opinion. I can show you the differences and the new TL API and you'll be free to try it ;)
If something important is not working, I can try to see how to solve it thanks to your help ;)

## Two Quick Examples in actual code

### Sequential Network

 ```python
model = tl.networks.Sequential(name=""My_Sequential_1D_Network"")

model.add(tl.layers.DenseLayer(n_units=10, act=tf.nn.relu, name=""seq_layer_1""))
model.add(tl.layers.DenseLayer(n_units=20, act=None, name=""seq_layer_2""))
model.add(tl.layers.PReluLayer(channel_shared=True, name=""prelu_layer_2""))

model.add(tl.layers.DenseLayer(n_units=30, act=None, name=""seq_layer_3""))
model.add(tl.layers.PReluLayer(channel_shared=False, name=""prelu_layer_3""))

plh = tf.placeholder(tf.float16, (100, 32))

train_model = model.compile(plh, reuse=False, is_train=True)
test_model = model.compile(plh, reuse=True, is_train=False)

print(type(train_model))  # tl.models.CompiledNetwork
print(type(test_model))  # tl.models.CompiledNetwork

# What can you do with them ?

# 1. Get Layers by name

## Compiled_Layer are generated by a factory and are immutable.
print(type(train_model[""seq_layer_3""]))  # tl.models.Compiled_DenseLayer
print(type(train_model[""prelu_layer_3""]))  # tl.models.Compiled_PReluLayer

# 2. You still can do all the cool stuff you are used to doing

## Compiled_Layer are generated by a factory and are immutable.
print(train_model[""seq_layer_3""].outputs)  #output of this specific layer => tf.Tensor
print(train_model[""seq_layer_3""]._local_weights)  # weights Tensors of this specific layer
print(train_model[""seq_layer_3""])  # returns the string describing the object.
print(train_model[""seq_layer_3""].__dict__)  # all the hyperparameters of the layer

print(train_model.inputs)  # input of the model => tf.placeholder
print(train_model.outputs)  # output of the model => tf.Tensor
print(train_model.all_params)  # all the parameters of the model
print(train_model.all_layers)  # all the Compiled_Layers of the model
print(train_model.all_drop)  # all the dropout placeholders of the model
```

### Custom Non-Sequential Networks

```python
def fire_module(inputs, squeeze_depth, expand_depth, name):
    """"""Fire module: squeeze input filters, then apply spatial convolutions.""""""

    with tf.variable_scope(name, ""fire"", [inputs]):
        squeezed = tl.layers.Conv2d(
            n_filter=squeeze_depth,
            filter_size=(1, 1),
            strides=(1, 1),
            padding='SAME',
            act=tf.nn.relu,
            name='squeeze'
        )(inputs)

        e1x1 = tl.layers.Conv2d(
            n_filter=expand_depth,
            filter_size=(1, 1),
            strides=(1, 1),
            padding='SAME',
            act=tf.nn.relu,
            name='e1x1'
        )(squeezed)

        e3x3 = tl.layers.Conv2d(
            n_filter=expand_depth,
            filter_size=(3, 3),
            strides=(1, 1),
            padding='SAME',
            act=tf.nn.relu,
            name='e3x3'
        )(squeezed)

        return tl.layers.ConcatLayer(concat_dim=3, name='concat')([e1x1, e3x3])
        
class MyCustomNetwork(tl.networks.CustomModel):

    def model(self):
        input_layer = tl.layers.InputLayer(name='input_layer')

        net = fire_module(input_layer, 32, 24, ""fire_module_1"")
        net = fire_module(net, 32, 24, ""fire_module_2"")

        return input_layer, net

model = MyCustomNetwork(name=""my_custom_network"")

plh = tf.placeholder(tf.float16, (100, 16, 16, 3))

train_model = model.compile(plh, reuse=False, is_train=True)
test_model = model.compile(plh, reuse=True, is_train=False)

print(type(train_model))  # tl.models.CompiledNetwork
print(type(test_model))  # tl.models.CompiledNetwork
```

And of course all the same stuff as above for the Sequential Network. Do you really think the API is changing so much that it becomes unfixable ?

Maybe I am missing some edge cases, and I'll be glad to see some examples and I'll work to make them possible ;)

## Loss in confidence

@luomai and I actually talked about this, namely after what happened to Caffe2. However, this a completely different situation: the change won't be huge and complex to do. 

Moreover, I would like to point out, as you probably know, Tensorflow is going for the 2nd radical change (1st being TF1.0 now heading for TF 2.0). People are still using it. 

Sometimes we have to break backward compatibility to be able to improve something. There might be only a very small amount of project that never did a radical change of design in the course of their existance. Not even in Deep Learning... Take JQuery in the WebDesign domain, Every new big versions, so many functionalities are broken that many plugins or libraries needs to be rewritten. Take Django 2.0 released a few months ago, same story. It might be true for you that you will loose confidence in TL, but honestly did you ever have a seen any active project which doesn't deprecate a ton of features over the years ?

*Even some of the most important projects of computer science, like Python 2 and Python 3 completely broke everything, however you are still using Python I believe...*

Supporting the new & old way to create TL Layers won't be working for long. The approach is completely different and it will require a ton of work to keep mainting both, time that we don't have.

I understand that you don't want to update your codebase, that's why we plan to keep fixing bugs on TL 1.x.",thin come misunderstanding perfectly aware case would like use work perfectly new said would like intuitive cleaner approach never said identical work way time try use see work also similar pointed plan mean complexify prevent currently easily unfortunately ca share code still perfectly working however said previous meeting important feature non sequential model ca made work easily smoothly cancel update abort update would absolute agree would also like highlight something used term maybe unfortunate shall proposition compile compile mean identical compile loss hand current stage compile function generate corresponding layer new network shall understood method create corresponding hope made clearer would like involve test new would free maybe still heavy would love someone like honest opinion show new free try something important working try see solve thanks help two quick actual code sequential network python model print type print type get name factory immutable print type print type still cool stuff used factory immutable print output specific layer print specific layer print string object print layer print input model print output model print model print model print dropout model custom python name fire module squeeze input apply spatial name fire ex ex return ex ex class model self net net net return net model print type print type course stuff sequential network really think much becomes unfixable maybe missing edge glad see work make possible loss confidence actually namely however completely different situation change wo huge complex moreover would like point probably know going radical change st heading people still sometimes break backward compatibility able improve something might small amount project never radical change design course even deep learning take domain every new big many broken many need take ago story might true loose confidence honestly ever seen active project deprecate ton even important computer science like python python completely broke everything however still python believe supporting new old way create wo working long approach completely different require ton work keep time understand want update plan keep fixing,issue,positive,positive,positive,positive,positive,positive
415370243,"Hello, I am a big fan of TensorLayer who used Keras and Tflearn. Please consider my following comments.

# Why no Keras

Making TensorLayer more like Keras is not a good decision. Actually, the practice shows that compiling the network after declaration is an disadvantage, it will have problems for building complex models especially dynamic models which need the tensor from previous layers to define the next layers.. the code will become dirty.. Many people would not use that way to build network unless the network is very simple... That is the reason I moved from Keras to TensorLayer and Tflearn.

Besides, other disadvantage I can come out with now is that for `tl.models` which has `restore` function. It needs the `all_params` from intermedia layer. In other word, not only the latest layer need `all_xxx`, but also all layers in the network should have `all_xxx`.

```python
n1 = Layer1()(x)
n2 = Model1()(n1)
n3 = Layer3(n1.outputs.shape())(n2)
n4 = Layer4()(n3)

n4.compile()

n2.all_params  <--- tl.models need all_params list to restore the parameters.
n2.restore(sess)
n4.all_params
```

I guess this is the reason that Keras' models do not support `end_with` like TensorLayer.


# TensorLayer abstraction

Layer as network is the key abstraction of TensorLayer, and make sense to me and my colleagues.
Some Keras fans may say the TensorLayer's abstraction level is not high enough, but TensorLayer's abstraction successfully help us to build any complex networks we designed, while, Keras cannot. 
This is why I think TensorLayer's abstraction is actually better than Keras.

For bugs, @DEKHTIARJonathan mentions above, I think it is just an engineering work, we should not consider changing TensorLayer usage because of it. 

# Conclusion

To summarise, if the release of network API will deprecates the existing TensorLayer's usage, I believe it is a bad decision. For existing users, if they need to rewrite the code like Keras, why they don't rewrite it in Keras or Tflearn? (they will loss confidence of TensorLayer as TensorLayer become more like Keras). For new users, if the abstraction is similar with Keras, why they don't choice Keras? (as they don't need many advanced layers at the beginning stage.)

Overall, I and my colleagues believe that the existing TensorLayer abstraction is better than other libraries. If the network API must be released, it is fine for me if the existing TensorLayer usage is maintained, otherwise, it is very wired. No one will use `tl.depercated.layers` and I probably will move to Tflearn instead.. honest.

# TensorLayer 2.0

For next version, I suggest the community can put more effect on the following few functions:
- `tl.models`: It is one of our big advantage over Keras and other TensorFlow-based libraries.
- More key applications: I see there are two PRs about openpose and style transfer, I believe they will help TensorLayer to get more attention. Providing code consistent applications can have big impact.., TensorFlow's model repo is too messy.
- Distributed training: TensorPack focuses on big data is a trend. Even though, TensorLayer start to support this, it is not good enough, at least, we should provide working example to train ImageNet.
The `tl.prepro` is for numpy-array preprocessing (`threading_data`), while TensorFlow dataset API and `tl.image` have better performance, we should let user know how to choice the best way, and update the example code.


Best wishes,",hello big fan used please consider following making like good decision actually practice network declaration disadvantage building complex especially dynamic need tensor previous define next code become dirty many people would use way build network unless network simple reason besides disadvantage come restore function need layer word latest layer need also network python layer model layer layer need list restore sess guess reason support like abstraction layer network key abstraction make sense may say abstraction level high enough abstraction successfully help u build complex designed think abstraction actually better think engineering work consider usage conclusion release network usage believe bad decision need rewrite code like rewrite loss confidence become like new abstraction similar choice need many advanced beginning stage overall believe abstraction better network must fine usage otherwise wired one use probably move instead honest next version suggest community put effect following one big advantage key see two style transfer believe help get attention providing code consistent big impact model messy distributed training big data trend even though start support good enough least provide working example train better performance let user know choice best way update example code best,issue,positive,positive,positive,positive,positive,positive
415347023,"Basically, TL 2.0 API has been motivated by a few points:

### Being more like Keras
New Deep Learning tend to favor Keras by a large marging. Why ? It's because Keras has been designed with an intuitive API, very easy to manipulate and to use.

I won't go into much details, but I think we can agree that Keras API is a piece of good design.

### Why using TL and not Keras then ?

By trying to support different backends and to be (excessively) simple, Keras is unfortunately very slow (compared to other solutions) and incredibly difficult to use when you want to realize something not supported out of the box.

How many times we see people on stackoverflow struggling to do simple stuff with TF not managing to make it work in Keras.

### Get some inspiration from PyTorch

I'm in no ways an experienced PyTorch user, however this library also has an amazing API. We also try to get inspiration from it. Feel free to give us suggestion about stuff you like in PyTorch.

### TL needs a more systematic and logical approach

I will take a simple example.

```
net_in = tl.layers.InputLayer(...)
net_1 = tl.layers.Conv2d(net_in, ...)
net_2 = tl.layers.Conv2d(net_1, ...)

print(net_2.all_params) # W and b for both Conv2d Layers => 4 params
print(net_2.all_layers)  # net_in, net_1 and net_2
print(isinstance(net_2, tl.layers.Layer))  # True
```
So we have Layer objects which behave like Network objects. Am I supposed to understand Layer objects as Network objects ? Why when you save a Layer, you actually save a Network ?

It's not consistent, not logical, not intuitive. We are used to this mindset, but it doesn't mean it's good.

This fact also bring a ton of redundancy in the Layers (each layer contains an exact copy of the information contained in the all the previous ones). Not memory efficient.

This redundancy brings up a ton of bugs, I can't even count how many of these bugs we solved over the last 6 months with @zsdonghao (I prefer not knowing ^^).

More clarity, more consistency will bring less complexity, less bugs.

## Conclusion

To summarise, what we try to achieve with TL 2.0 is simply to make TensorLayer an easier library to use on a day to day basis. 

Also to reduce the library maintainance workload, we are less than 8 people working actively on this. We are not supported by any large company (GAFAM/BATX). We do this on our free time, we need to make sure to keep it as efficient as possible.

It's true, TL 2.0 will break code compatibility. However, you can solve it in two ways:
- The code update will be very fast even for large projects (<30min-1h)
- TL 1.x will get bug fixes and will still be available if you have some code in production and have no time to update your code base. Please be sure that you pinned your dependencies and you'll be fine ;)

TL 2.0 will be a **huge** step forward to a more professional and efficient library. We are really excited about it and hope you will share our excitement ;)

In any case, if you feel like some features should be added in TensorLayer, or you always wanted that TL could do something. Please feel free to share sudo-code, we'll try our best to fit it inside the new API if it fits our roadmap ;)",basically like new deep learning tend favor large designed intuitive easy manipulate use wo go much think agree piece good design trying support different excessively simple unfortunately slow incredibly difficult use want realize something box many time see people struggling simple stuff make work get inspiration way experienced user however library also amazing also try get inspiration feel free give u suggestion stuff like need systematic logical approach take simple example print print print true layer behave like network supposed understand layer network save layer actually save network consistent logical intuitive used mean good fact also bring ton redundancy layer exact copy information previous memory efficient redundancy ton ca even count many last prefer knowing clarity consistency bring le complexity le conclusion try achieve simply make easier library use day day basis also reduce library le people working actively large company free time need make sure keep efficient possible true break code compatibility however solve two way code update fast even large get bug still available code production time update code base please sure pinned fine huge step forward professional efficient library really excited hope share excitement case feel like added always could something please feel free share try best fit inside new,issue,positive,positive,positive,positive,positive,positive
415341833,"Wow... This seems like a very huge bug.

Could you please give us a short snippet to test both cases. We will release a bug fix as soon as possible.

Thanks a lot for this bug report. Really appreciated ;)",wow like huge bug could please give u short snippet test release bug fix soon possible thanks lot bug report really,issue,positive,positive,positive,positive,positive,positive
414926990,"The new layer design will please the NLP researchers, and does not make much difference for CNN user?",new layer design please make much difference user,issue,negative,positive,positive,positive,positive,positive
414925450,"Sorry, I am very busy with a competition recently, and will fix it in future.
@DEKHTIARJonathan ",sorry busy competition recently fix future,issue,negative,negative,neutral,neutral,negative,negative
414909421,"@zsdonghao The new API is still capable of building such network, you just need to use it in the following way:

```python
n1 = Layer1()(x)
n2 = Layer2()(n1)
n3 = Layer3(n1.outputs.shape())(n2)
n4 = Layer4()(n3)
```

Basically, the new API isn't less expressive than the old one.
Every expression in the old API can be converted into an equivalent one in the new API, by just moving the first argument `net` to the end: `ALayer(net, ...)` -> `ALayer(...)(net)`.
So we won't drop anything when upgrading to the new API.



",new still capable building network need use following way python layer layer layer layer basically new le expressive old one every expression old converted equivalent one new moving first argument net end net net wo drop anything new,issue,negative,positive,positive,positive,positive,positive
414824690,"Could anyone explain in more detail what advantages of new Network API over the existing one? It would be better to consider thoroughly whether these advantages are really critical, before choosing to break the compatibility. Thanks.",could anyone explain detail new network one would better consider thoroughly whether really critical choosing break compatibility thanks,issue,negative,positive,positive,positive,positive,positive
414665224,"@lgarithm in this example, `sequence_length` is not get from from previous layer, but from the input tensor. This is just an example, there would be some cases that the next layer need information from previous layers which can't be get before compiling.

for example, the tensor information can be from any previous layers:

```python
n1 = Layer(x)
n2 = Layer(n1)
n3 = Layer(n2, n1.outputs.shape())
n4 = Layer(n3)
```

I think this is a main drawback of using compiling.",example get previous layer input tensor example would next layer need information previous ca get example tensor information previous python layer layer layer layer think main drawback,issue,negative,negative,neutral,neutral,negative,negative
414654746,"You don’t really need `sequence_length` when you constructing the Layer.
in the new API, it would be simply moved to the `compile` method, like the following:

```python

def transform(net, layers):
    """"""Apply a sequence of layers to a net to form new net.""""""
    for layer in layers:
        net = layer(net)
    return net


def rnn_unit(net):
    """"""A combination of several layers.""""""
    embed_layer = EmbeddingInputlayer(
        vocabulary_size=1000,
        embedding_size=200,
        name='ebb')

    d_rnn_layer = DynamicRNNLayer(cell_fn=LSTMCell,
                                  n_hidden=200,
                                  dropout=(keep_prob if is_train else None),
                                  # sequence_length is moved to compile
                                  # sequence_length=tl.layers.retrieve_seq_length_op2(x),
                                  return_last=True,
                                  name='ann')

    return transform(net, [embed_layer, d_rnn_layer])

# where DynamicRNNLayer is defined as the folowing

class DynamicRNNLayer(Layer):
    def compile(prev_layer):
        sequence_length = tl.layers.retrieve_seq_length_op2(prev_layer)
        ….
```

Or, you can make `sequence_length` a lambda to the constructor of DynamicRNNLayer, 
and call it when you have the `net` variable, which is also in the compile method.",really need layer new would simply compile method like following python transform net apply sequence net form new net layer net layer net return net net combination several else none compile return transform net defined class layer compile make lambda constructor call net variable also compile method,issue,negative,positive,neutral,neutral,positive,positive
414643468,"@DEKHTIARJonathan @lgarithm @lgarithm and I just discuss the network API.

For the compatibility, @DEKHTIARJonathan suggest to put existing layers in to `tl.depercated.layers`, then all new codes should implement in new way.

However, I am worry about the network API, and suggest to support two ways at the same time.

In my opinion, even though some people feel the existing layers don't have good network abstraction, the existing way can build any networks. While, the compile way doesn't have big advantage over the existing way, and it may not able to handle some complex situation. 

Let me discuss with more users, I will put more feedback later.

# Existing and Keras ways

- Existing way compiles the network in real-time.

```python
net = InputLayer(net)
net = DenseLayer(net, 100)
```

- Keras way compiles the network in the end.

```python
net = InputLayer()(net)
net = DenseLayer(100)(net)
net.compile()
```

In my opinion, the main differences between our way and keras way is they compile the network in the end but we simply compile network in real-time, and we mix up layer and network together.
The other parts are the same.

While, to build complex network, in some cases, the next layer will need the information (like shape) of the input or outputs of previous layers. But the input and `net.outputs` is unknown before we compile it. Take dynamic RNN layer below for example, we need the input placeholder/tensor to compute the `sequence_length`. Therefore, if we compile the network in the end, we can't give the `sequence_length` to `DynamicRNNLayer` .

```python
net = EmbeddingInputlayer(
                     inputs = x,
                     vocabulary_size = 1000,
                     embedding_size = 200,
                     name = 'ebb')
net = DynamicRNNLayer(net,
                     cell_fn = LSTMCell,
                     n_hidden = 200,
                     dropout = (keep_prob if is_train else None),
                     sequence_length = tl.layers.retrieve_seq_length_op2(x),
                     return_last = True,
                     name = 'ann')
```


# Problems need to be solved:

- [ ] Next layer need tensor from previous layer like `sequence_length` of `DynamicRNNLayer`.
- [ ] If a network have two branch of outputs:
    - [x] how to combine them?
            answer: use `ConcatLayer`
    - [ ] apart from using `ConcatLayer` in the end, any other ways? returns two network via compile?
    - [ ] if compile returns two network, how to store model to one file?
- [ ] It would be great to support both existing way and new way.
- [ ] If we only support the new way.
    - [ ] how to prevent losing existing users?
    - [ ] how to ",discus network compatibility suggest put new implement new way however worry network suggest support two way time opinion even though people feel good network abstraction way build compile way big advantage way may able handle complex situation let discus put feedback later way way network python net net net net way network end python net net net net opinion main way way compile network end simply compile network mix layer network together build complex network next layer need information like shape input previous input unknown compile take dynamic layer example need input compute therefore compile network end ca give python net name net net dropout else none true name need next layer need tensor previous layer like network two branch combine answer use apart end way two network via compile compile two network store model one file would great support way new way support new way prevent losing,issue,positive,positive,neutral,neutral,positive,positive
414564550,"i just figure it out , please check here, this is my learning blog, but chapter3_3.5_2~9渐变, i cannot figure it out stil, could you help me? many thanks：
https://blog.csdn.net/weixin_42025210/article/details/81906595
@ousay ",figure please check learning figure could help many,issue,positive,positive,positive,positive,positive,positive
414530128,"@zouqy 
sorry  i  did not mark it，so  i cannot remember it clearly. maybe you can  check you tensorlayer version,lower version sometimes better for some issues.",sorry mark remember clearly maybe check version lower version sometimes better,issue,negative,positive,neutral,neutral,positive,positive
414121877,"# Context

After some discussion with @lgarithm, we came up with a conceptual design that would highly clarify the TL architecture and make future development a lot more easy and integrable.

@lgarithm: Please edit my post if you want to add new stuff or correct some stuff I m ight have got wrong.

# Conceptual Design

Layer and Network Classes will be **converted** to factory-like objects. They contain critical information to generate Layers and Network, however, they do not contain anymore any information about the generated Layer. 

## Layer API
Layer Class becomes a factory encapsulating the required information to generate a TF Ops, TF Tensors and TF Vars. Layer object do not contain anymore any information about the network or TF objects.

### Deprecation 

Which means that these informations will be removed:
```python
Layer.outputs
Layer.inputs
Layer.all_params
Layer.all_layers
Layer.all_graph
and so on ...
```

### Factory API

In order to insure a stable and consistent API, connecting Layer will now happen using the functional API. This manner to call Layers allows us to use Layer as factories without having to give all the parameters again.

```python
tl.layers.Conv2D(prev_layer, ...)  # Deprecated
tl.layers.Conv2D(...)(prev_layer)  # Using the Layer.__call__() method, factory way.
```

### Layer Output

Now a TL Layer returns a TF Tensor, not a TL Layer anymore.

## Network API
Network Classes (CustomNetwork and Sequential) also become factories encapsulating the required information to generate the Model using TL Layers.

### Deprecation 

Which means that these informations will be removed:
```python
Network.outputs
Network.inputs
Network.all_params
Network.all_layers
Network.all_graph
and so on ...
```

### Factory API

The model generation will be launched using the `.compile()` method.

Network.compile(input_placeholder, is_train=True, reuse=True)

### Network Output

The model output a new class `CompiledNetwork`, this model will have all the old attributes and methods that makes TL Layers so easy and pleasant to use.

",context discussion came conceptual design would highly clarify architecture make future development lot easy integrable please edit post want add new stuff correct stuff got wrong conceptual design layer network class converted contain critical information generate network however contain information layer layer layer class becomes factory information generate layer object contain information network deprecation removed python factory order insure stable consistent layer happen functional manner call u use layer without give python method factory way layer output layer tensor layer network network class sequential also become information generate model deprecation removed python factory model generation method network output model output new class model old easy pleasant use,issue,positive,positive,positive,positive,positive,positive
413407240,"@DEKHTIARJonathan Since we've fixed RTD, this PR is not urgent ATM.
But I still think we should remove depencendy on matplotlib as part of TL2.0.
I think TL should focus on **layers,** **models** and **networks,** visualization is not a key feature of TL.
",since fixed urgent still think remove part think focus visualization key feature,issue,negative,positive,neutral,neutral,positive,positive
413405892,"I like the new layer API, in particular for it implements the idea of **layer is a (unary, in most case) function of tensors**.

With such signature of layer API implemented, we can stack layers in an easier way:

```python
layers = [
    tl.layers.DenseLayer(n_units=20, act=None, name=""seq_layer_2""),
    tl.layers.PReluLayer(channel_shared=True, name=""prelu_layer_2""),
    tl.layers.DenseLayer(n_units=50, act=None, name=""seq_layer_3""),
    tl.layers.PRelu6Layer(channel_shared=False, name=""prelu6_layer_3"")
]

net_out = transform(layers, net_in)
```

where

```python
def transform(layers, net):
    y = net
    for l in layers:
        y = l(y)
    return y
```

We can even introduce a combinator for layers:

```python
# stack :: [Layer] -> Layer
def stack(layers):
    return StackedLayers(layers)
```

which allows the following:
```python
vgg_unit = stack(...)
net_out = vgg_unit(net_in)
```
",like new layer particular idea layer unary case function signature layer stack easier way python transform python transform net net return even introduce combinator python stack layer layer stack return following python stack,issue,positive,positive,neutral,neutral,positive,positive
413298556,"Before merging the followings need to be done;
- [ ] update the Changelog 
- [ ] Travis needs to pass
- [ ] implement unittests for your new layer",need done update travis need pas implement new layer,issue,negative,positive,positive,positive,positive,positive
412997314,"@Windaway this PR has to many files changed..and has conflicts.
It seen it is based on old TL version.
Could you git clone the latest version of tensorlayer and PR again?",many seen based old version could git clone latest version,issue,negative,positive,positive,positive,positive,positive
412943695,@lgarithm is it really necessary now to remove all these functions ? Now that we don't need any dependency for the documentation ?,really necessary remove need dependency documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
412578322,"Sorry, I didn't catch what @zsdonghao said here. He said this has been implemented in Tensorlayer but I couldn't find it in the document. Could either of you provide a link please? Much appreciated. ",sorry catch said said could find document could either provide link please much,issue,negative,negative,negative,negative,negative,negative
412500941,"Please open a PR, we will be more than happy to merge your work ;)",please open happy merge work,issue,positive,positive,positive,positive,positive,positive
410454832,"@luomai sometimes it may fail if pip install failed (due to network error), you can just click retry on travis-ci.",sometimes may fail pip install due network error click retry,issue,negative,negative,negative,negative,negative,negative
410442474,@lgarithm CI becomes quite unstable these days.,becomes quite unstable day,issue,negative,neutral,neutral,neutral,neutral,neutral
410233888,"@DEKHTIARJonathan i think these APIs can be removed except the TSNE one.

",think removed except one,issue,negative,neutral,neutral,neutral,neutral,neutral
410018781,"I am more concerned about the *general idea* than the technical details. 
For ResNet, I am working on this and for your second point, we can always create an ""OutputLayer"" that does just this ^^",concerned general idea technical working second point always create,issue,negative,positive,neutral,neutral,positive,positive
410005073,"### TBD
Feel free to add more questions, and add answers below them.

#### Network API
- [ ] How to build ResNet with Network API?
     - Using `network[""layer_name""]`, example:
- [ ] What if the outputs of different branches have different shape? Will StackLayer stills work?
- [ ] Could we code like that?: `model.add(DenseLayer(20, None, name=""seq_layer_2""))`
- [ ] The following PyTorch-like layer access can be implemented by `def __getitem__(self, key):` of core layer. Is there any other advantages of using Network class?

```python
# PyTorch-Like Layer Access

layer_1 = model[""seq_layer_1""]
```

#### Database for life-cycle management
Here https://github.com/tensorlayer/tensorlayer/pull/751
- [ ] Any comments about this design?

#### Graph API design
A list of dictionary with the same order of layer construction that contains all augments
- [ ] Any comments about this design?

#### Distributed training
It is available in master branch, try `tl.distributed.Trainer`.
- [ ] Any comments about this design?",feel free add add network build network network example different different shape work could code like none following layer access self key core layer network class python layer access model management design graph design list dictionary order layer construction design distributed training available master branch try design,issue,positive,positive,positive,positive,positive,positive
409884918,Why did u numerous functionalities and not set the dependency as optional. I don't understand  ,numerous set dependency optional understand,issue,negative,neutral,neutral,neutral,neutral,neutral
409884468,"@zsdonghao Sure.
I know this change will remove some important function, so please review carefully.
",sure know change remove important function please review carefully,issue,positive,positive,positive,positive,positive,positive
409653630,"@boldjoel Thank you for your great contribution, there are some extra works need to be done before we can merge this PR.
- [ ] APIs for Evaluation
- [ ] Provides pre-trained model here: https://github.com/tensorlayer/pretrained-models/tree/master/models
- [ ] Provides AP results of your pre-trained model
- [ ] Moves (numpy-based, non-TF) data augmentation APIs into `tl.prepro`.
- [ ] Moves COCO dataset download and preprocess code into `tl.files` that allow users to automatically download and prepare the dataset.
- [ ] Readme:
    - [ ] introduction of this project
    - [ ] how to use your pre-trained model
    - [ ] how to download dataset
    - [ ] how to train and evaluate model
    - [ ] how to train model on customised dataset
    - [ ] how to customise data augmentation

To speed up the inferencing:
- [ ] For pose-processing, uses the C implementation of OpenPose
- [ ] Smaller model
- [ ] TensorRT float16 (half-float) inferencing

For TensorLayer Project Quality:
- [ ] Update Changelog.md
- [ ] Add unittests",thank great contribution extra work need done merge evaluation model model data augmentation coco code allow automatically prepare introduction project use model train evaluate model train model data augmentation speed implementation smaller model float project quality update add,issue,positive,positive,positive,positive,positive,positive
409592202,@lgarithm @DEKHTIARJonathan could you have a look on this PR? Hao would like to merge it today if everything looks fine with you.,could look hao would like merge today everything fine,issue,positive,positive,positive,positive,positive,positive
409589542,"I implemented a graph API here https://github.com/tensorlayer/tensorlayer/pull/751

then we can save the architecture and parameters together ",graph save architecture together,issue,negative,neutral,neutral,neutral,neutral,neutral
409467886,"@DEKHTIARJonathan I have changed the code, added an option argument 'is_dynamic_batch_size'",code added option argument,issue,negative,neutral,neutral,neutral,neutral,neutral
409181880,"Returning a different batch size may lead to error when users set a fixed batch size in the input placeholder (many people do that),
soI think we should add an argument (e.g. `is_dynamic_batch_size` etc) in iterate API, and set it to `False` by default.
",different batch size may lead error set fixed batch size input many people think add argument iterate set false default,issue,negative,positive,neutral,neutral,positive,positive
409170119,"I have tested this PR it works. However, I would like to have your opinion before merging @zsdonghao  @luomai @lgarithm 

Let's consider this simple script:

```python
import numpy as np
import tensorlayer as tl

data = np.random.random((1050, 100))
y = np.random.random((1050, ))

i = 0
total_data = 0

for batch in tl.iterate.minibatches(data, y, batch_size=100, shuffle=True):
    print(""Batch ID: %d - Batch Size: %d"" % (i, batch[0].shape[0]))
    i += 1

    total_data += batch[0].shape[0]

print(""Total Data: %d"" % total_data)
```

## Output with current TensorLayer

```
Batch ID: 0 - Batch Size: 100
Batch ID: 1 - Batch Size: 100
Batch ID: 2 - Batch Size: 100
Batch ID: 3 - Batch Size: 100
Batch ID: 4 - Batch Size: 100
Batch ID: 5 - Batch Size: 100
Batch ID: 6 - Batch Size: 100
Batch ID: 7 - Batch Size: 100
Batch ID: 8 - Batch Size: 100
Batch ID: 9 - Batch Size: 100
Total Data: 1000
```

## Output with this PR work

```
Batch ID: 0 - Batch Size: 100
Batch ID: 1 - Batch Size: 100
Batch ID: 2 - Batch Size: 100
Batch ID: 3 - Batch Size: 100
Batch ID: 4 - Batch Size: 100
Batch ID: 5 - Batch Size: 100
Batch ID: 6 - Batch Size: 100
Batch ID: 7 - Batch Size: 100
Batch ID: 8 - Batch Size: 100
Batch ID: 9 - Batch Size: 100
Batch ID: 10 - Batch Size: 50
Total Data: 1050
```

## My question

Usually people don't really care if they loose a small number of samples (dataset is very large) and the dataset should be shuffle at the beginning of each epoch.
Is it actually a good thing to enforce a smaller batch at the end (potentially of size 1) if the number of samples is not a multiple of batch_size ?

I believe, and might be wrong, that the version currently in Tensorlayer is more robust than the version proposed in this PR and more standard practice in Deep Learning. But I genuinely have doubts ...

I'm actually puzzled with this situation, what you think is best ?
",tested work however would like opinion let consider simple script python import import data batch data print batch id batch size batch batch print total data output current batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size total data output work batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size batch id batch size total data question usually people really care loose small number large shuffle beginning epoch actually good thing enforce smaller batch end potentially size number multiple believe might wrong version currently robust version standard practice deep learning genuinely actually puzzled situation think best,issue,positive,positive,neutral,neutral,positive,positive
409000474,"Hi @prabhjot-singh-gogana, Have you solved your conversion problem yet? I faced the similar issue about converting numpy .npz format model to .caffemodel. Can I discuss this problem with you? Many thanks for that!",hi conversion problem yet faced similar issue converting format model discus problem many thanks,issue,negative,positive,positive,positive,positive,positive
408870960,"Thanks @thangvubk, as soon as Travis finish building I'll merge your contribution.
Do not hesitate to send further PRs ;) We appreciate good contribs",thanks soon travis finish building merge contribution hesitate send appreciate good,issue,positive,positive,positive,positive,positive,positive
408840668,"Hello, you would also need to update the CHANGELOG.",hello would also need update,issue,negative,neutral,neutral,neutral,neutral,neutral
408829780,Could you make a PR to modify this? Thanks! Your contribution is highly appreciated. ,could make modify thanks contribution highly,issue,negative,positive,positive,positive,positive,positive
407816437,This PR LGTM. I am fine with putting the entry point in the documentation.,fine entry point documentation,issue,negative,positive,positive,positive,positive,positive
407731781,@lgarithm This PR also fails because of the import error. Merge with the master does not resolve the issue.,also import error merge master resolve issue,issue,negative,neutral,neutral,neutral,neutral,neutral
407069243,"To be honest, I really don't see the benefits ... 

#### about `pip3`

If you have to use `pip3` or `python3` it means that you don't use a `virtualenv` as recommended in [Contributing.md](https://github.com/tensorlayer/tensorlayer/blob/master/CONTRIBUTING.md#build-from-sources).

#### concerning doing it in Travis

We never had any bug happening in Travis and not in a local machine. I would agree with you if we had such an issue (it would be problematic not being able to reproduce a bug). However, I'm not aware of any issue of this kind. Adding layers of complexity, if not necessary, only tend to make the project more difficult for newcomers. Having to install docker (which is **highly unstable** on windows) in order to add a simple function seem very overkill to me.

Btw. if you are concerned on running a different env than Docker, you can create a Dockerfile based on the image used by Travis: https://hub.docker.com/search/?isAutomated=0&isOfficial=0&page=1&pullCount=0&q=travis&starCount=0",honest really see pip use pip python use concerning travis never bug happening travis local machine would agree issue would problematic able reproduce bug however aware issue kind complexity necessary tend make project difficult install docker highly unstable order add simple function seem concerned running different docker create based image used travis,issue,negative,positive,positive,positive,positive,positive
407066403,"@lgarithm can you have a final look and do the merging. It looks good to me.

@zsdonghao @OwenLiuzZ @fangde we will do the TensorDB integration later. ",final look good integration later,issue,negative,positive,positive,positive,positive,positive
407061763,"@DEKHTIARJonathan I didn't mean to leave TravisCI.
The way I'd like to do it would be roughly like:

```yaml
services:
- docker

# TODO: add compatibility matrix
before_install:
- docker build --rm -t tensorlayer:snapshot -f docker/Dockerfile.test .

script:
- docker run --rm -it tensorlayer:snapshot pytest
```

* enable [docker service](https://docs.travis-ci.com/user/docker/) in travis.yml 
* build a temporary docker image on travis CI
* run tests inside the temporary image

Then you will be able to reproduce what exactly happened on travis-ci by typing the following commands:

```bash
docker build --rm -t tensorlayer:snapshot -f docker/Dockerfile.test .
docker run --rm -it tensorlayer:snapshot pytest
```

I think this way is more hermetic, and portable. (This is exactly what docker tries to solve.)
Because the installation of python can be different on our dev machines,
For me, the command I type is `pip3` rather that `pip`, 
and maybe for other people, root is required to run pip, etc.




",mean leave way like would roughly like docker add compatibility matrix docker build snapshot script docker run snapshot enable docker service build temporary docker image travis run inside temporary image able reproduce exactly following bash docker build snapshot docker run snapshot think way hermetic portable exactly docker solve installation python different dev command type pip rather pip maybe people root run pip,issue,positive,positive,neutral,neutral,positive,positive
407053085,"@lgarithm for information Travis runs a Docker image (and CircleCI too). It won't change anything to run the tests in a Docker container embedded inside a Docker container.

Requirements files are splitted to allow a finer control on dependencies.

As a TL developer, the **only way** you should install tensorlayer for development are the following commands:
```shell
# for a machine **without** an NVIDIA GPU
pip install -e .[all_cpu_dev] --upgrade

# for a machine **with** an NVIDIA GPU
pip install -e .[all_gpu_dev] --upgrade
```

See [Contributing.md](https://github.com/tensorlayer/tensorlayer/blob/master/CONTRIBUTING.md#build-from-sources)

If you follow this pattern, I don't see what are the benefits of doing this. The Docker containers are not build with the test dependencies ... And are not suited for test today. CircleCI is really difficult to use and totally unintuitive. Leaving TravisCI (which is an OpenSource standard) to run the tests on CircleCI which is currently building the docker images is for me a bad idea.

------------

If you want to build a DockerImage running the tests for yourself on your local machine, it should be very easy to do. But to be honest, I don't see any advantage doing it... A *virtualenv* + the install command cited above does perfectly the job",information travis docker image wo change anything run docker container inside docker container allow finer control developer way install development following shell machine without pip install upgrade machine pip install upgrade see follow pattern see docker build test test today really difficult use totally unintuitive leaving standard run currently building docker bad idea want build running local machine easy honest see advantage install command perfectly job,issue,positive,positive,neutral,neutral,positive,positive
407046882,"Yet another reason to run tests in docker in that, you can easily run the tests locally.
Currently we have a complex combination of requirement files, 
when the core team do large scale changes, it's very hard to run a full test locally.

@luomai any thoughts about this?",yet another reason run docker easily run locally currently complex combination requirement core team large scale hard run full test locally,issue,negative,positive,neutral,neutral,positive,positive
406010179,"@mutewall cool, this PR is already approved, but you still need to:

1) make travis pass and remove 
2) `WIP` from the title 
3) modify the changelog.md

before you can click the `merge bottom`.",cool already still need make travis pas remove title modify click merge bottom,issue,negative,positive,positive,positive,positive,positive
405963698,"@DEKHTIARJonathan 
[arch.py.zip](https://github.com/tensorlayer/tensorlayer/files/2206276/arch.py.zip)
No problem, here is not 1 layer, but simple Unet **with the only difference** - 1 layer in the end of encoder are conv and deform conv respectively (conv5 layer). Just one layer!
I got such results with that code:
`
Sunet time is 0.1247713565826416 seconds <-- plain convolution layers
`
`
Dunet time is 2.2409575939178467 seconds <-- one deform convolution layer in the middle
`
It's about 20 times slower. 
Original paper states that there should be not very significant slowdown. Definitely not in 20 times. 

It's very critical, but for now I can't understand why it is so slow.",problem layer simple difference layer end deform respectively layer one layer got code time plain convolution time one deform convolution layer middle time original paper significant slowdown definitely time critical ca understand slow,issue,negative,positive,neutral,neutral,positive,positive
405934505,@zsdonghao  I have renamed the file and added the example links. Please check.,file added example link please check,issue,negative,neutral,neutral,neutral,neutral,neutral
405898092,"Thanks for your suggestion, unfortunately we are quite busy right now. The easiest way to have it available as soon as possible would be to implement it yourself and open a PR.

We would be very happy to receive a PR from you implementing it ;)",thanks suggestion unfortunately quite busy right easiest way available soon possible would implement open would happy receive,issue,positive,positive,positive,positive,positive,positive
405630455,"@mutewall thanks, this PR is almost done. two more things need to be do before you merge it.

1) for consistent file name, tutorial name:  cifar -->  cifar10    
2) to allow more people read your code, add example links to the example page: 
https://github.com/tensorlayer/tensorlayer/blob/master/docs/user/example.rst
https://github.com/tensorlayer/awesome-tensorlayer",thanks almost done two need merge consistent file name tutorial name allow people read code add example link example page,issue,positive,positive,positive,positive,positive,positive
405604748,@DEKHTIARJonathan I have added the layers in the tests folder. Please check.,added folder please check,issue,negative,neutral,neutral,neutral,neutral,neutral
405560481,"@undeadblow I reopen the issue. Please provide a simple working example to show your point.

Just a 1 layer network and make it comparable with a non deformable conv. I can't test your assumption with your code.

@zsdonghao and me, we'll have a look at this as soon as you provide some working example showing your point. ",reopen issue please provide simple working example show point layer network make comparable non deformable ca test assumption code look soon provide working example showing point,issue,negative,neutral,neutral,neutral,neutral,neutral
405554615,"@DEKHTIARJonathan 
I confirm that Deformable conv, for example:

`
offset1 = Conv2d(conv5, 18, (3, 3), (1, 1), act=None, padding='SAME'')
`
`
conv5 = DeformableConv2d(conv5, offset1, int(512 * filter_size_scale), (3, 3), act=tf.nn.leaky_relu, W_init=w_init, b_init=None'
`

is about 3+ times slower than simple conv:

`conv5 = Conv2d(pool4, 512 * filter_size_scale, (3, 3), act=tf.nn.leaky_relu, W_init=w_init)`",confirm deformable example offset offset time simple pool,issue,negative,neutral,neutral,neutral,neutral,neutral
405009288,"It seems like the library matplotlib is importing python-tk right?

I need to bring a few fixes to the Docker file 👍. I'm gonna open a PR this weekend.

How do you explain that python-tk is missing on Docker and not in native python when you run it on your machine? ",like library right need bring docker file gon na open weekend explain missing docker native python run machine,issue,negative,positive,neutral,neutral,positive,positive
405009032,"I had to manu do sudo install python-tk
shall we add this in the docker file?
",install shall add docker file,issue,negative,neutral,neutral,neutral,neutral,neutral
405007868,"@DEKHTIARJonathan  any suggestion to fix that?
the error occurs every time import tl on dockers",suggestion fix error every time import,issue,negative,neutral,neutral,neutral,neutral,neutral
403865846,"Perfect answer, I will close this question tomorrow, thank you! Changyu Liu 邮箱：shiyipaisizuo@gmail.com 签名由 网易邮箱大师 定制 On 07/10/2018 23:09, Hao wrote: hi, yes, y_op = tf.argmax(tf.nn.softmax(y), 1) is useless in this code, but it is useful when users want to know the way to obtain the output. That is why I put it here. — You are receiving this because you authored the thread. Reply to this email directly, view it on GitHub, or mute the thread.",perfect answer close question tomorrow thank hao wrote hi yes useless code useful want know way obtain output put thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
403857724,"hi, yes, `y_op = tf.argmax(tf.nn.softmax(y), 1)` is useless in this code, but it is useful when users want to know the way to obtain the output. That is why I put it here.",hi yes useless code useful want know way obtain output put,issue,negative,negative,neutral,neutral,negative,negative
403844063,"@mutewall Look at this PR: #737, @zsdonghao added directly the reference inside the docstring ;)
Just add the title, authors and link to arxiv paper. It will be perfect :)

And btw. we have the practice to implement small unittests before merging, just to insure everything run fine.

Do you think you could have a look to the tests and add the new layers you created in this PR in the tests ? It should be very quick ;)",look added directly reference inside add title link paper perfect practice implement small insure everything run fine think could look add new quick,issue,positive,positive,positive,positive,positive,positive
403826750,"@DEKHTIARJonathan hi, switchable normalisation does not require separating training and testing.",hi require separating training testing,issue,negative,neutral,neutral,neutral,neutral,neutral
403796627,"@zsdonghao thanks for your answer ;) Last questions for you, BN implements the `is_train` bool. Do you need a similar behaviour that only trains the mean & var during training ?",thanks answer last bool need similar behaviour mean training,issue,negative,negative,neutral,neutral,negative,negative
403792883,@zsdonghao I applied small changes to insure we have the correct tensors in the params attribute ;),applied small insure correct attribute,issue,negative,negative,negative,negative,negative,negative
403786060,"@DEKHTIARJonathan I have put the link in the ""tutorial_quanconv_cifar.py"". If it is not obvious enough, where should I add the reference?
And here is the link: https://arxiv.org/abs/1712.05877
",put link obvious enough add reference link,issue,negative,neutral,neutral,neutral,neutral,neutral
403759459,"@DEKHTIARJonathan not yet, just simply multiple 255 inside the model.",yet simply multiple inside model,issue,negative,neutral,neutral,neutral,neutral,neutral
403758336,@DEKHTIARJonathan This is a script I add *temporarily* for helping some students to setup the environment to test this PR. It would not be merged into the master. ,script add temporarily helping setup environment test would master,issue,negative,neutral,neutral,neutral,neutral,neutral
403757210,"Very nice job @mutewall, do you thin you can add the references (research papers) related to your implementation if there is any inside the docstring ?

I would love to read more details about this ;)

Cheers",nice job thin add research related implementation inside would love read,issue,positive,positive,positive,positive,positive,positive
403753912,"I'm not sure that a script to install CUDA and CuDNN should be added in the repo (2bc6094), the versions are updated very oftenly, the installation process changes if depending on the OS you use.

I would think a CUDA_CuDNN_Install.md is much more appropriate ;)

If we add a script to do this, we will constently have issues telling us, it's not working anymore or outdated.",sure script install added installation process depending o use would think much appropriate add script telling u working outdated,issue,negative,positive,positive,positive,positive,positive
403456388,"@tayloreisman16  sorry, I just see this .. I just change the code as you suggest: https://github.com/tensorlayer/tensorlayer/pull/734/files",sorry see change code suggest,issue,negative,negative,negative,negative,negative,negative
403254957,"similar issue with python-3
Traceback (most recent call last):
  File ""/usr/lib/python3.5/tkinter/__init__.py"", line 36, in <module>
    import _tkinter
ImportError: No module named '_tkinter'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorlayer/__init__.py"", line 34, in <module>
    from . import files
  File ""/usr/local/lib/python3.5/dist-packages/tensorlayer/files/__init__.py"", line 12, in <module>
    from .dataset_loaders.celebA_dataset import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorlayer/files/dataset_loaders/__init__.py"", line 4, in <module>
    from .celebA_dataset import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorlayer/files/dataset_loaders/celebA_dataset.py"", line 10, in <module>
    from tensorlayer.files.utils import download_file_from_google_drive
  File ""/usr/local/lib/python3.5/dist-packages/tensorlayer/files/utils.py"", line 31, in <module>
    import matplotlib.pyplot as plt
  File ""/usr/local/lib/python3.5/dist-packages/matplotlib/pyplot.py"", line 115, in <module>
    _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup()
  File ""/usr/local/lib/python3.5/dist-packages/matplotlib/backends/__init__.py"", line 62, in pylab_setup
    [backend_name], 0)
  File ""/usr/local/lib/python3.5/dist-packages/matplotlib/backends/backend_tkagg.py"", line 4, in <module>
    from . import tkagg  # Paint image to Tk photo blitter extension.
  File ""/usr/local/lib/python3.5/dist-packages/matplotlib/backends/tkagg.py"", line 5, in <module>
    from six.moves import tkinter as Tk
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 92, in __get__
    result = self._resolve()
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 115, in _resolve
    return _import_module(self.mod)
  File ""/usr/local/lib/python3.5/dist-packages/six.py"", line 82, in _import_module
    __import__(name)
  File ""/usr/lib/python3.5/tkinter/__init__.py"", line 38, in <module>
    raise ImportError(str(msg) + ', please install the python3-tk package')
ImportError: No module named '_tkinter', please install the python3-tk package
",similar issue recent call last file line module import module handling exception another exception recent call last file line module file line module import file line module import file line module import file line module import file line module import file line module file line file line module import paint image photo blitter extension file line module import file line result file line return file line name file line module raise please install package module please install package,issue,positive,neutral,neutral,neutral,neutral,neutral
403211704,@rachitagrawal if you wanna help to implement this functionnality. You are more than welcome in the team ;),wan na help implement welcome team,issue,positive,positive,positive,positive,positive,positive
403037507,"@rachitagrawal hi, this ONNX function still under development.
before that, you can use TF's saver such as `.pb`.",hi function still development use saver,issue,negative,neutral,neutral,neutral,neutral,neutral
402986236,"@lgarithm fully agree with slack, we can create a channel specific for build infos ;)
",fully agree slack create channel specific build,issue,positive,neutral,neutral,neutral,neutral,neutral
402972188,"@zsdonghao Thanks Hao. Is there a pre-released version that I can use to save my tensorlayer based model into ONNX format?
",thanks hao version use save based model format,issue,positive,positive,positive,positive,positive,positive
402941822,The issue #469 is closed but looks like saving the model structure option is still not available. Can someone please confirm?,issue closed like saving model structure option still available someone please confirm,issue,positive,positive,positive,positive,positive,positive
402926420,"Currently it only supports sending directly message to me from slackbot.
Ideally it should be send to a channel.",currently sending directly message ideally send channel,issue,negative,positive,positive,positive,positive,positive
402815422,"@lgarithm I'm not sure having Firebug to create an issue for every build to be a good idea... 
We are spamming everyone following the project ...",sure firebug create issue every build good idea everyone following project,issue,positive,positive,positive,positive,positive,positive
402754856,"@DEKHTIARJonathan great, thank!

I'm using this PR just for triggering travis CI, I reduced the CI matrix for quicker run in it.
",great thank travis reduced matrix run,issue,positive,positive,positive,positive,positive,positive
402753693,"@lgarithm you dont need to precise **dont merge**. If ""WIP"" is in the title, merging is blocked ;)

I have set the branch **debug-ci** to build on RTD: https://readthedocs.org/projects/tensorlayer/builds/7442938/

You can check than the branch is building fine before merging ;)",dont need precise dont merge title blocked set branch build check branch building fine,issue,negative,positive,positive,positive,positive,positive
402743485,"Yes, this is an attempt to resolve that, hope it would work.",yes attempt resolve hope would work,issue,positive,neutral,neutral,neutral,neutral,neutral
402554303,"Maybe you should have a look to basic tensorflow tutorial.
Tensorlayer is nothing else than additional functionalities to TF.

Short answer : yes it possible, look on Google on how to do it with TF",maybe look basic tutorial nothing else additional short answer yes possible look,issue,negative,neutral,neutral,neutral,neutral,neutral
402049973,"@zsdonghao I don't think we should approve a PR when it's not ready to merge, we could indivertibly click on merge ... That would be bad ... Travis build is not passing yet",think approve ready merge could indivertibly click merge would bad travis build passing yet,issue,negative,negative,negative,negative,negative,negative
401818355,"@DEKHTIARJonathan but we could merge this one first, it would not effect the previous codes.",could merge one first would effect previous,issue,negative,positive,neutral,neutral,positive,positive
401810794,Why using regex which is a pain and not simply the `glob` package already present in python that does this job very nicely ?,pain simply package already present python job nicely,issue,negative,positive,positive,positive,positive,positive
401604002,Thanks @DEKHTIARJonathan ...  I will close and let this be handled under the general effort to drop Python 2 support.,thanks close let handled general effort drop python support,issue,positive,positive,positive,positive,positive,positive
401603644,"@cclauss thanks for your contribution. Much appreciated.

However, you added an import in an area which exclusively run for Python 2.

Up to me, we should not merge this PR as-is.

Two solutions :
1. Close this PR without merging and let the Python 2 & 3 separated behavior as it exists today.
2. Remove the conditions that separate python 2 and python 3 and merge them into a single line.
=> PR should be updated.

The current situation does not require any additional import, and TF will be droping Python 2 support very soon. So I'm not sure it is a very good decision to go for more ""six"" library in the project than necessary ;)

I let you decide @zsdonghao ",thanks contribution much however added import area exclusively run python merge two close without let python behavior today remove separate python python merge single line current situation require additional import python support soon sure good decision go six library project necessary let decide,issue,positive,positive,positive,positive,positive,positive
401316837,"@DEKHTIARJonathan ok, your idea is great. I will implement it from your advice :)",idea great implement advice,issue,positive,positive,positive,positive,positive,positive
401097085,"We have moved from 22 failures reported above down to 16 failures currently...

flake8 testing of https://github.com/tensorlayer/tensorlayer on Python 3.6.3

$ __flake8 . --count --select=E901,E999,F821,F822,F823 --show-source --statistics__
```
./tensorlayer/nlp.py:283:13: F821 undefined name 'tl'
            tl.logging.fatal(""Vocab file %s not found."" % vocab_file)
            ^
./tensorlayer/nlp.py:284:9: F821 undefined name 'tl'
        tl.logging.info(""Initializing vocabulary from file: %s"" % vocab_file)
        ^
./tensorlayer/nlp.py:1102:9: F821 undefined name 'tl'
        tl.logging.info(""Unable to fetch multi-bleu.perl script, using local."")
        ^
./tensorlayer/nlp.py:1130:17: F821 undefined name 'tl'
                tl.logging.warning(""multi-bleu.perl script returned non-zero exit code"")
                ^
./tensorlayer/nlp.py:1131:17: F821 undefined name 'tl'
                tl.logging.warning(error.output)
                ^
./tensorlayer/files/dataset_loaders/imdb_dataset.py:59:13: F821 undefined name 'gzip'
        f = gzip.open(os.path.join(path, filename), 'rb')
            ^
./tensorlayer/files/dataset_loaders/imdb_dataset.py:63:17: F821 undefined name 'cPickle'
    X, labels = cPickle.load(f)
                ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:61:18: F821 undefined name 'nlp'
    word_to_id = nlp.build_vocab(nlp.read_words(train_path))
                 ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:61:34: F821 undefined name 'nlp'
    word_to_id = nlp.build_vocab(nlp.read_words(train_path))
                                 ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:63:18: F821 undefined name 'nlp'
    train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)
                 ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:63:40: F821 undefined name 'nlp'
    train_data = nlp.words_to_word_ids(nlp.read_words(train_path), word_to_id)
                                       ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:64:18: F821 undefined name 'nlp'
    valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)
                 ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:64:40: F821 undefined name 'nlp'
    valid_data = nlp.words_to_word_ids(nlp.read_words(valid_path), word_to_id)
                                       ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:65:17: F821 undefined name 'nlp'
    test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)
                ^
./tensorlayer/files/dataset_loaders/ptb_dataset.py:65:39: F821 undefined name 'nlp'
    test_data = nlp.words_to_word_ids(nlp.read_words(test_path), word_to_id)
                                      ^
./tests/test_yapf_format.py:20:20: F821 undefined name 'unicode'
            return unicode(f.read(), 'utf-8')
                   ^
16    F821 undefined name 'gzip'
16
```",currently flake testing python count undefined name file found undefined name vocabulary file undefined name unable fetch script local undefined name script returned exit code undefined name undefined name path undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name undefined name return undefined name,issue,negative,negative,negative,negative,negative,negative
401074111,"Now it points to 

* https://app.codacy.com/app/zsdonghao/tensorlayer (I'm now sure if this is due to the original project URL of tensorlayer)

@zsdonghao could you check you codacy account, If you have such an app, please remove it.",sure due original project could check account please remove,issue,positive,positive,positive,positive,positive,positive
400840000,We still need to write down the Document and fix some todo before merging.,still need write document fix,issue,negative,neutral,neutral,neutral,neutral,neutral
400808977,"@DEKHTIARJonathan I submit a PR, it is true that tl.prepro does not import logging",submit true import logging,issue,negative,positive,positive,positive,positive,positive
400808708,"Hum could you provide the code that produce this error?

Minimal example will be fine. ",hum could provide code produce error minimal example fine,issue,negative,positive,positive,positive,positive,positive
400805875,"I think this API can be merged, and mark as alpha version",think mark alpha version,issue,negative,neutral,neutral,neutral,neutral,neutral
400632498,"Yeap, LazyImport is bypassed for documentation building ;)
I believed it was a good idea, maybe it is not necessary and we can remove this feature.",documentation building good idea maybe necessary remove feature,issue,negative,positive,positive,positive,positive,positive
400632256,"Personally, I would agree with the idea of replacing old VGG network using range [0, 255] completely.
Releasing a v2 is confusing, if we have done something bad, it is better to remove it ;)

However, you can still implement a _sanity check_, in case people use data within [0, 255] you can raise a warning. @zsdonghao ",personally would agree idea old network range completely done something bad better remove however still implement case people use data within raise warning,issue,negative,positive,neutral,neutral,positive,positive
400447523,"The documentation might not be perfectly accurate.
It's me who implemented this the idea is to create a tensor with a shape similar to another one set to any given value.

To be very honest with you, the function will keep supporting string I have designed it to support every kind of dtype (at least I hope so) in a more efficient manner than the APIs existing in TF.

@Novog maybe you could submit a PR correcting the documentation. It's always easier to correct something with an external point view.
I would be very happy to review this work if you ever have time 😊",documentation might perfectly accurate idea create tensor shape similar another one set given value honest function keep supporting string designed support every kind least hope efficient manner maybe could submit correcting documentation always easier correct something external point view would happy review work ever time,issue,positive,positive,positive,positive,positive,positive
400444619,"I think that there may be some minor issues with the comments. First, the documentation for `alphas_like` does not list `str` in the list of allowed types of the `alpha_value` parameter. However, in my testing, it does seem to work for building string tensors with a default value. (A good thing; that is what I wanted it for!) Could this function fail with strings in some cases, or could the documentation be updated to include `str` in the list of allowed types?

Second, the documentation for `alphas` describes a nonexistent `dtype` paramenter instead of the `alpha_value` paramenter that the function actually uses.",think may minor first documentation list list parameter however testing seem work building string default value good thing could function fail could documentation include list second documentation nonexistent instead function actually,issue,negative,positive,neutral,neutral,positive,positive
400319013,"`LazyImport` doesn't work for document. 
When we run doc test on travis-ci, or build doc on RTD, horovod has to be imported, and its dependency openmpi has to be installed.",work document run doc test build doc dependency,issue,negative,neutral,neutral,neutral,neutral,neutral
400290620,"@DEKHTIARJonathan @zsdonghao  Yes, currently, most of the VGG models are using input nets with pixels values in the range of [0-1]  to avoid the network crashing. However，we just modified the input image value range rather than the input nets. So, @zsdonghao advised me to release VGG16V2 and VGG19V2 to get rid of this problem",yes currently input range avoid network input image value range rather input advised release get rid problem,issue,negative,neutral,neutral,neutral,neutral,neutral
400083915,"@DEKHTIARJonathan I discussed with @OwenLiuzZ , for this parameters, we can multiple 255 inside the network. Besides, we can also release VGG16V2 and VGG19V2 that use tf-slim's parameters. ",multiple inside network besides also release use,issue,negative,neutral,neutral,neutral,neutral,neutral
400025194,"In principle, I agree with the direction of this PR.
Be careful, as the network is changed, it needs to be retrained and the current weights won't be working

@zsdonghao ",principle agree direction careful network need current wo working,issue,negative,negative,neutral,neutral,negative,negative
399739474,"In case you need, you can use lazy loading .
I have implemented it, it was only load a package when you use it for the first time",case need use lazy loading load package use first time,issue,negative,neutral,neutral,neutral,neutral,neutral
399739446,"In short term, I have to solve the problem of running TL program in classical environment anyway, because Read The Doc is also classical.",short term solve problem running program classical environment anyway read doc also classical,issue,negative,neutral,neutral,neutral,neutral,neutral
399739302,"@DEKHTIARJonathan I've added you as the admin of https://app.codacy.com/organization/tensorlayer.
Could you delete https://app.codacy.com/app/DEKHTIARJonathan/tensorlayer?

I can't tell which one is which from the webhook URLs. ",added could delete ca tell one,issue,negative,neutral,neutral,neutral,neutral,neutral
399739198,"For the problem of installing openmpi, I have some solutions in mind for it.

But I feel like running test in docker would be an elegant way to go in our long term plan.
",problem mind feel like running test docker would elegant way go long term plan,issue,negative,positive,positive,positive,positive,positive
399738941,"Wow... 
That will require quite a lot of work...

You sure there is no way to use classical Travis? Maybe you should ask the support how we could do this ",wow require quite lot work sure way use classical travis maybe ask support could,issue,positive,positive,positive,positive,positive,positive
399345857,"Then the checkpoint folder should be small, the size of our model is about 

```
 (28*28 * 800 + 800*800 + 800 * 10) * 4 / 1024 / 1024 = 4  # MB
```

So what's inside `events.out.tfevents.*`?",folder small size model inside,issue,negative,negative,negative,negative,negative,negative
399344870,"The dataset is not checkpointed. Only parameters are checkpointed.

> On 22 Jun 2018, at 08:10, LG <notifications@github.com> wrote:
> 
> I can understand that dataset may take extra space, but I don't expect it to be that large.
> The original size of mnist data is only 53MB (unzipped).
> 
> Another question is, why data should be in checkpoint?
> Shouldn't checkpoint only contain parameters?
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub <https://github.com/tensorlayer/tensorlayer/pull/700#issuecomment-399344524>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ABFXR-kdllM6YmhE2hGJFTBCQrwNASYXks5t_JiDgaJpZM4UqV_J>.
> 

",wrote understand may take extra space expect large original size data another question data contain reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
399344524,"I can understand that dataset may take extra space, but I don't expect it to be that large.
The original size of mnist data is only 53MB (unzipped).

Another question is, why data should be in checkpoint?
Shouldn't checkpoint only contain parameters?",understand may take extra space expect large original size data another question data contain,issue,negative,positive,positive,positive,positive,positive
399340239,"Are you sure you replicate the same behaviours as DataSet? DataSet is shuffling, batching and repeating the data. These operations can take extra space.",sure replicate shuffling data take extra space,issue,negative,positive,positive,positive,positive,positive
399147110,"Hi, @OwenLiuzZ I've tested this PR on a clean VM, it works!
Please resolve conflicts and then it will be ready to merge.",hi tested clean work please resolve ready merge,issue,positive,positive,positive,positive,positive,positive
399130087,"It looks like the size of `events.out.tfevents.*` being too large is caused by dataset.
When training without dataset, it is just around 5MB.

@luomai @zsdonghao I think we need to have a good understanding of how dataset and MonitoredTrainingSession works before use them.
",like size large training without around think need good understanding work use,issue,positive,positive,positive,positive,positive,positive
398452229,"The program creates a large (666MB) file `events.out.tfevents.*` on start, we need to understand what it is for.",program large file start need understand,issue,negative,positive,positive,positive,positive,positive
397892504,the point of an RC release is to check that everything works as expected without any bug. if we just add a bug fix. we dont need an rc release in my opinion,point release check everything work without bug add bug fix dont need release opinion,issue,negative,neutral,neutral,neutral,neutral,neutral
397872356,"@DEKHTIARJonathan I tested it on mac, it's working. Thanks for the fix!",tested mac working thanks fix,issue,negative,positive,positive,positive,positive,positive
397872185,"@zsdonghao @lgarithm @luomai  do you think we should release version ""1.9.1"" due to this bug ?
If this makes the library impossible to use on MacOS, maybe it's a good idea to release a bug fix ...

Not sure about this ;)",think release version due bug library impossible use maybe good idea release bug fix sure,issue,positive,positive,positive,positive,positive,positive
397872031,"@lgarithm I tried to use the sys package (already imported) instead of `package`, could you check it's still working ;)

Thanks, for me it's fine we can merge ;)",tried use package already instead package could check still working thanks fine merge,issue,positive,positive,positive,positive,positive,positive
397864193,"Do you think you can add some code to only run this change under Mac OS?

I would prefer not having a bug fix which affects everyone if not necessary 😉",think add code run change mac o would prefer bug fix everyone necessary,issue,negative,neutral,neutral,neutral,neutral,neutral
397814592,"This is the API I would imagine.

```python
x, y = dataset

net = model(x)

loss = tl.cost….(net.outputs, y)

Horovod = tl.distributed.Horovod()
Horovod.init()
n_gpus =  Horovod.hvd_size

lr_init = 0.0001 * n_gpus
global_step = tf.contrib.framework.get_or_create_global_step()
#with tf.variable_scope('learning_rate'):
#      lr_v = tf.Variable(lr_init, trainable=False)
lr_v = tf.train.exponential_decay(    # decay learning rate
    learning_rate= lr_init,
    global_step=global_step,
    decay_steps=100,
    decay_rate=0.5,
    staircase=False,
    name=None
)
train_opt = tf.train.AdamOptimizer(learning_rate=lr_v,  loss)  # we should support Adam

trainer = Horovod.get_trainer(train_opt, [x, y])
# some cases, we will have several trainers with different losses
trainer2 = Horovod.get_trainer(train_opt2, [x, y])

#for i in range(n_epoch):
     # i may update learning rate here
      # sess.run(tf.assign(lr_v, lr * 0.5))
     # update 1 iternation	

while not trainer.mon_sess.should_stop():
    start_time = time.time()
    # run 1 iteration 
    err = trainer.update()
    # check info
    print(“global steps:%d    learning rate:%f   loss:%f   took:%f” % (sess.run(global_step), sess.run(lr_v), err, time.time()-start_time)
    # save model every 10000 steps
    if trainer.is_master and (sess.run(global_step) % 10000 == 0):
          tl.files.save_npz(net, ‘mode.npz’)
```",would imagine python net model loss decay learning rate loss support trainer several different trainer range may update learning rate update run iteration err check print global learning rate loss took err save model every net,issue,negative,neutral,neutral,neutral,neutral,neutral
397811228,"The API is not flexible enough, there are many cases need to be considered.

DCGAN : X only, no Y
            2 losses
GAN-CLS: X1, X2 --> Y
            3 losses ",flexible enough many need considered,issue,negative,positive,positive,positive,positive,positive
397608364,"@zsdonghao for vgg16 we used the pre-trained weights provided by <http://www.cs.toronto.edu/~frossard/post/vgg16/>. 
But there is no corresponding weights  for vgg19.
Where can we find a publicly available pre-trained vgg19 model for TF/TL?",used provided corresponding find publicly available model,issue,negative,positive,positive,positive,positive,positive
397330632,"You can use ‘git lfs’ for uploading large file.

________________________________
From: Sichao Liu <notifications@github.com>
Sent: Thursday, June 14, 2018 5:10:38 PM
To: tensorlayer/tensorlayer
Cc: LG; Mention
Subject: Re: [tensorlayer/tensorlayer] [WIP] VGG19 Implementation with example (#698)


@OwenLiuzZ commented on this pull request.

________________________________

In tensorlayer/models/vgg19.py<https://github.com/tensorlayer/tensorlayer/pull/698#discussion_r195351190>:

> +            lambda net: MaxPool2d(net, filter_size=(2, 2), strides=(2, 2), padding='SAME', name='pool5'),
+            lambda net: FlattenLayer(net, name='flatten'),
+            lambda net: DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc1_relu'),
+            lambda net: DenseLayer(net, n_units=4096, act=tf.nn.relu, name='fc2_relu'),
+            lambda net: DenseLayer(net, n_units=1000, act=tf.identity, name='fc3_relu'),
+        ]
+        net = net_in
+        for l in layers:
+            net = l(net)
+            # if end_with in net.name:
+            if net.name.endswith(end_with):
+                return net
+
+        raise Exception(""unknown layer name (end_with): {}"".format(end_with))
+
+    def restore_params(self, sess):


Yes, it is about 570MB for this npy file. The public repo maximum limit for the file is 100 MB. However, I do not know what the limit is in the private repo.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/tensorlayer/tensorlayer/pull/698#discussion_r195351190>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AB8ydOuPFO-sYon3M0JU9R09BJHzIG9yks5t8iiOgaJpZM4Um-Q1>.
",use git large file sent june mention subject implementation example pull request lambda net net lambda net net lambda net net lambda net net lambda net net net net net return net raise exception unknown layer name self sess yes file public maximum limit file however know limit private reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
397305356,"Why you opened a second PR ? Please merge all the changes into the other PR.

@OwenLiuzZ @zsdonghao  I am closing this issue.",second please merge issue,issue,negative,neutral,neutral,neutral,neutral,neutral
397101236,"A better coding style can be found here https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/models/squeezenetv1.py, instead of using `self.all_layers = net.xxxx`, we set the model as `Layer` class.
Therefore, for other models, you can connect tfslim into the layer class.",better style found instead set model layer class therefore connect layer class,issue,negative,positive,positive,positive,positive,positive
397099862,"Hi, please also update the docs https://github.com/tensorlayer/tensorlayer/blob/master/docs/modules/models.rst

and the changelog https://github.com/tensorlayer/tensorlayer/blob/master/CHANGELOG.md",hi please also update,issue,negative,neutral,neutral,neutral,neutral,neutral
397096461,"@OwenLiuzZ Thank you so much for your PR.

Please update the changelog with your contribution and add your name to the contributor list.
**Please follow the formating**

I let @zsdonghao reviewing your work ;)",thank much please update contribution add name contributor list please follow let work,issue,positive,positive,positive,positive,positive,positive
396949470,"Logging need to set a verbosity level, normally TL set this value by default on ""INFO"" level 

add these two lines right in your code after importing TF and TL

```python
tf.logging.set_verbosity(tf.logging.DEBUG)
tl.logging.set_verbosity(tl.logging.DEBUG)
```",logging need set verbosity level normally set value default level add two right code python,issue,negative,positive,positive,positive,positive,positive
396899347,"The original author of A3C update his code to TF1.8

https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/commit/682e89bd7f00b7c2b3598a7bb5975598def9e4cf#diff-2202f4dab91d61358d9ac9058a499562",original author update code,issue,negative,positive,positive,positive,positive,positive
396555318,"I observed (using a few different model) that if I introduce mechanism like semaphores in order to synchronize the update of weights between threads and common network, the convergence improve. Should be an issue of multithreading? ",different model introduce mechanism like order synchronize update common network convergence improve issue,issue,positive,negative,negative,negative,negative,negative
396230377,"They are not really Shape operation like Flatten or Reshape but more like Layer Operations...

I will remove Keras Layer as soon as the deprecation date is reached. I don't remember exactly when, I'm in the train. ",really shape operation like flatten reshape like layer remove layer soon deprecation date remember exactly train,issue,positive,positive,positive,positive,positive,positive
396229118,"I think the KerasLayer can be removed now, as we use LambdaLayer in the tutorial of Keras, they are the same.

For Merge, Extend Layer, Stack layers, we can put all of them into `Shape layer`?",think removed use tutorial merge extend layer stack put shape layer,issue,negative,neutral,neutral,neutral,neutral,neutral
396065130,"We are investigating it. The error may come from TF not TL.

If you have any idea, we would be glad to receive any PR",investigating error may come idea would glad receive,issue,negative,positive,positive,positive,positive,positive
396039101,"Hi, just wondering if there has been any development with this issue?  I've reverted back to TF 1.0 with no TL for my research now (i.e. the original authors A3C implementation), but of course would much rather be using up to date versions of TF and TL!",hi wondering development issue back research original implementation course would much rather date,issue,negative,positive,positive,positive,positive,positive
395974103,I mean we can set it to none  but provide an argument to do it no? ,mean set none provide argument,issue,negative,negative,negative,negative,negative,negative
395973740,"I usually add regularisation ""visually"" instead of automatically use it, for example in `Word2vectEmbeddingLayer`, it provides a loss that can be used in users scripts.",usually add visually instead automatically use example loss used,issue,negative,negative,negative,negative,negative,negative
395083958,"@DEKHTIARJonathan As seen in the @zsdonghao 's last comment, we can handle this issue by removing `data_format` and just using the argument set to `channels_last` internally.  ",seen last comment handle issue removing argument set internally,issue,negative,neutral,neutral,neutral,neutral,neutral
395009256,"@luomai I have modified the badges URLs with a custom shields.io mirror hosted on AWS by @fangde.

Could you please modify directely the latest version of `Readme.md` and reflect the changes on `Readme.rst` ?",custom mirror could please modify latest version reflect,issue,negative,positive,positive,positive,positive,positive
395008793,"Hey @fangde ,

I've reading your PR with much interests, I have a few remarks:
1. You need to merge your PR with Master ;) The tl.files API have been remastered last week.
2. Could you update `Changelog.md` ?
3. Could you implement and add unittests regarding your new features ? You said you tested it, could you add simple code to tests the features with unittests ? That helps a lot to insure robustness and stability ;)

If you need help, please reach out, I'll be glad to help you",hey reading much need merge master last week could update could implement add regarding new said tested could add simple code lot insure robustness stability need help please reach glad help,issue,positive,positive,positive,positive,positive,positive
394932833,"Dear all:
I've update and test 6 functions
on my production environment
i
the load/save npz,
load/save_any_npy
and 
load/save_dict
",dear update test production environment,issue,negative,neutral,neutral,neutral,neutral,neutral
394615460,"You have to install the newly released RC release to be able to run new features in the Layer API.

You can run either:

```shell
pip install --upgrade --pre tensorlayer  # install the latest pre-release from PyPI
pip install tensorlayer==1.8.6rc2   # install the version 1.8.6rc2
pip install -e .[]  # install the version checked out in your local repository
```
` or `` once you are in the TL repository with the master branch checked out

New behaviors have been introduced recently, they cannot be run in 1.8.5

The error raised is definetely coming from an old release, the line:
```python
 ""tensorlayer/layers/core.py"", line 963, in __init__
  logging.info(""DenseLayer  %s: %d %s"" % (name, n_units, act.__name__))
```

This line does not exist anymore in 1.8.6rc2

Thanks anyway for taking the time to open an issue @jeffscott, I really appreciate your detailled report.
Very easy to identify the issue ;)

If this solves the situation, please close this issue and why not joining the slack !
Welcome on board",install newly release able run new layer run either shell pip install upgrade install latest pip install install version pip install install version checked local repository repository master branch checked new recently run error raised coming old release line python line name line exist thanks anyway taking time open issue really appreciate report easy identify issue situation please close issue joining slack welcome board,issue,positive,positive,positive,positive,positive,positive
394346832,"@zsdonghao fine with me. I renamed everything ;)
@2wins as soon as you push the Changelog for `AtrousDeConv2dLayer` I merge this PR",fine everything soon push merge,issue,negative,positive,positive,positive,positive,positive
394341338,"Hi , as we use `DeConv` for transpose convolutional layer, should we name it as `AtrousDeConv2dLayer ` ?",hi use transpose convolutional layer name,issue,negative,neutral,neutral,neutral,neutral,neutral
394338579,@2wins damned I just noticed you didn't updated the Changelog.md. I let you the pleasure to add your name and your changes and I merge ;),damned let pleasure add name merge,issue,negative,neutral,neutral,neutral,neutral,neutral
394333654,"@DEKHTIARJonathan Many thanks! Owing to your support, I've achieved it.",many thanks owing support,issue,positive,positive,positive,positive,positive,positive
394333258,"@2wins i finished to merge with master ;) I'll merge your PR as soon as CI Tests are finished
Thanks a lot for your work !",finished merge master merge soon finished thanks lot work,issue,negative,positive,positive,positive,positive,positive
394281979,@luomai please reflect all your changes in Readme.rst. This file is also very important as it is displayed on PyPI,please reflect file also important displayed,issue,negative,positive,positive,positive,positive,positive
394178131,"Damned Python 2.7 is broken ... For today it's enough, I'll fix it another day...",damned python broken today enough fix another day,issue,negative,negative,negative,negative,negative,negative
394157169,"Nope but I'm pretty sure, you will need to apply YAPF corrections ;)
I haven't done it",nope pretty sure need apply done,issue,negative,positive,positive,positive,positive,positive
394156124,"@2wins should be OK now, maybe you'll need to apply YAPF, i have applied the change using the browser",maybe need apply applied change browser,issue,negative,neutral,neutral,neutral,neutral,neutral
394154412,"@DEKHTIARJonathan I've merged the latest changes to mine. Please compare my `convolution.py` with the latest one. In addition, I added you as a collaborator in my fork.",latest mine please compare latest one addition added collaborator fork,issue,negative,positive,positive,positive,positive,positive
394153529,"@2wins we have bring a lot of changes yesterday to the layers API, could you please apply the same practices ? Or give me access to your fork and I will apply the changes directly.",bring lot yesterday could please apply give access fork apply directly,issue,negative,positive,neutral,neutral,positive,positive
394142894,"Okay, I will make a PR for the Chinese repo to implement the git submodule in a few days.",make implement git day,issue,negative,neutral,neutral,neutral,neutral,neutral
394141932,"@DEKHTIARJonathan Because `rate` can be adjusted differently for the same `output_shape`, it is not redundant information.",rate differently redundant information,issue,negative,negative,negative,negative,negative,negative
394122496,"Yeah it's my PR on lazy loading , I'll have a look tomorrow
",yeah lazy loading look tomorrow,issue,negative,negative,negative,negative,negative,negative
394027193,"@zsdonghao The idea was to remove redundant code that was everywhere.

Maybe it is not the best idea, I can change it, if its a problem. The idea is to design one systematic manner to handle dictionary arguments  ",idea remove redundant code everywhere maybe best idea change problem idea design one systematic manner handle dictionary,issue,negative,positive,positive,positive,positive,positive
393990019,We can easily try and see what changed. Who designed this tutorial ?,easily try see designed tutorial,issue,negative,positive,positive,positive,positive,positive
393947464,What is the last working version tested on?,last working version tested,issue,negative,neutral,neutral,neutral,neutral,neutral
393907491,@luomai and myself agreed to merge and fix later the 3 remaining tests,agreed merge fix later,issue,negative,neutral,neutral,neutral,neutral,neutral
393897091,"Thanks for the pointer. I am aware of git submodule in fact. That could resolve the synchronisation issue. The reason that we did not move into the direction is because we were also aware of another option of using the [single project with multiple translations](http://docs.readthedocs.io/en/latest/localization.html#project-with-multiple-translations) supported by ReadTheDocs. However, the latter option would take extra reworks in the master repo.

Given the time limit, shall we first try use the git submodule, suggested by @DEKHTIARJonathan ?Later,  we can try to look at if we can enable the multiple translation support in ReadTheDocs for TL.

@zsdonghao @lgarithm Thoughts?",thanks pointer aware git fact could resolve issue reason move direction also aware another option single project multiple however latter option would take extra master given time limit shall first try use git later try look enable multiple translation support,issue,positive,positive,neutral,neutral,positive,positive
393882286,"Have a look to git submodule, that's **exactly** what you are looking for",look git exactly looking,issue,negative,positive,positive,positive,positive,positive
393879997,"If you look into the current Chinese document [repo](https://github.com/tensorlayer/tensorlayer-chinese/tree/master/tensorlayer), it has a full replication of the TensorLayer source code. Maintaining the source code replication is difficult. My proposal is about making the Chinese documentation refer to the main TL repository directly. @zsdonghao and I were looking into it before, but we cannot find a good way of doing that through ReadTheDoc.",look current document full replication source code source code replication difficult proposal making documentation refer main repository directly looking find good way,issue,negative,positive,positive,positive,positive,positive
393854737,"We have to be careful I think with the Chinese / English integration. Having both is without a doubt a good opportunity, however if we want to have international adoption  and not focus on China, we need to keep the focus primarily on English and keep English as the main official language.

I would consider Chinese documentation to be important to maintain, however not a primary focus.

You should have a look to `git submodule`. It is a nice way to synchronize to repositories. In my opinion it would be a very good way to keep the synchronisation through time with minimal effort. 

Some documentation on the submodules: https://chrisjean.com/git-submodules-adding-using-removing-and-updating/",careful think integration without doubt good opportunity however want international adoption focus china need keep focus primarily keep main official language would consider documentation important maintain however primary focus look git nice way synchronize opinion would good way keep time minimal effort documentation,issue,negative,positive,positive,positive,positive,positive
393831654,"@lgarithm @luomai I will close this issue because Docker is implemented.
I will shortly update my blog with an article on this topic. It just need minor adaptation: https://www.born2data.com",close issue docker shortly update article topic need minor adaptation,issue,negative,negative,neutral,neutral,negative,negative
393830558,"@zsdonghao @lgarithm @luomai  I can reproduce the error. Could one of you have a look why it happens ?
It would be nice to fix it before releasing `1.8.6rc0`

Thanks @dengyueyun666 for this bug report, we are investigating this ;) If you think you can fix, you are more than welcome to submit a PR",reproduce error could one look would nice fix thanks bug report investigating think fix welcome submit,issue,positive,positive,positive,positive,positive,positive
393824471,According to @zsdonghao we also need to provide a small article to show how to run a simple mnist example in a docker.,according also need provide small article show run simple example docker,issue,negative,negative,negative,negative,negative,negative
393809605,"```python
import tensorflow as tf
import tensorlayer as tl
from tensorlayer.layers import *

config = tf.ConfigProto(log_device_placement=True, allow_soft_placement=True)
config.gpu_options.allow_growth = True
sess = tf.Session(config=config)

train_x = tf.random_uniform(shape=(10, 32, 32, 3))
train_y = tf.random_uniform(shape=(10, 32, 32, 1))

ConvLSTM_last_state = None

net_in = InputLayer(train_x, name='inputs')

net = Conv2d(net_in, 16, (3, 3), (1, 1), act=tf.nn.relu, padding='SAME', name='conv1')
net = ReshapeLayer(net, shape=(10, 1, 32, 32, 16))
net = ConvLSTMLayer(net, cell_shape=(32, 32), feature_map=16, filter_size=(3, 3), cell_fn=BasicConvLSTMCell,
                    n_steps=1, initial_state=ConvLSTM_last_state, return_last=False, return_seq_2d=False)

ConvLSTM_last_state = net.final_state


net = ReshapeLayer(net, shape=(10, 1, 32, 32, 16))
net = ConvLSTMLayer(net, cell_shape=(32, 32), feature_map=16, filter_size=(3, 3), cell_fn=BasicConvLSTMCell,
                    n_steps=1, initial_state=ConvLSTM_last_state, return_last=False, return_seq_2d=False)

net = ReshapeLayer(net, shape=(10, 32, 32, -1))
net = Conv2d(net_in, 1, (1, 1), (1, 1), act=tf.nn.relu, padding='SAME', name='output')

output = net.outputs

loss = tl.cost.mean_squared_error(output, train_y, is_mean=True)
train_params = net.all_params
train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss, var_list=train_params)

tl.layers.initialize_global_variables(sess)

for epoch in range(100):
    l = sess.run(loss)
    print(l)

sess.close()

```",python import import import true sess none net net net net net net net net net net net net output loss output loss sess epoch range loss print,issue,negative,positive,neutral,neutral,positive,positive
393781154,"Please fix  your code and keep an appropriate formating.
It is impossible to run your code.

- imports are missing
- vars are missing
- `conv2d(...)` this is not code
- etc.

If you want some help, you need to make sure that we can easily reproduce your problem

Make something **simple and minimal** that I can execute with `python main.py`",please fix code keep appropriate impossible run code missing missing code want help need make sure easily reproduce problem make something simple minimal execute python,issue,negative,positive,neutral,neutral,positive,positive
393773172,Can you give a simple snippet to reproduce? ,give simple snippet reproduce,issue,negative,neutral,neutral,neutral,neutral,neutral
393455373,"@luomai @zsdonghao do you think we should use `AtrousConv2dTransLayer` or `AtrousConv2dTransposeLayer`?

In my opinion I would be in favor of the second one which is less ambiguous, and its not really a big deal if the name is a little longer.

What do you think about it guys? ",think use opinion would favor second one le ambiguous really big deal name little longer think,issue,negative,negative,neutral,neutral,negative,negative
393091747,"sorry for the delay

I think the code should be:

`
W = (tf.get_variable(name='W', initializer=W_init) if W_init is not None and not callable(W_init) else tf.get_variable(name='W', shape=SHAPE, initializer=W_init))
`

if `W_init is not None and not callable(W_init ) `, then the shape argument can be ignored.",sorry delay think code none callable else none callable shape argument,issue,negative,negative,negative,negative,negative,negative
393063195,"@zsdonghao we all agree to merge, I let you do it ;)

The following needs to be done:
- Merge this PR
- Delete the repository: https://github.com/tensorlayer/home",agree merge let following need done merge delete repository,issue,negative,neutral,neutral,neutral,neutral,neutral
392984343,"previously it works very well, that should be a problem, and I guess the problem is related to the sampling part.",previously work well problem guess problem related sampling part,issue,negative,negative,neutral,neutral,negative,negative
392887580,"I tested on tf 1.6.0, I have a low convergence, but acceptable. If I overtraining (i.e. > 40000 iterations) it looks after convergence, diverge again",tested low convergence acceptable convergence diverge,issue,negative,neutral,neutral,neutral,neutral,neutral
392848413,"All reviews have been reset @zsdonghao @lgarithm @luomai.

Please have a look to the changes, if you agree please mark it as **accepted**. Once all of us, have marked it *accepted*, @zsdonghao  you can merge ;)",reset please look agree please mark accepted u marked accepted merge,issue,positive,positive,neutral,neutral,positive,positive
392752153,"@zsdonghao there was an error with Unicode and Python3, I just fixed it and now it's working.

Only 4 tutorials to fix and 2 to more to test.",error python fixed working fix test,issue,negative,positive,neutral,neutral,positive,positive
392729646,"for `tutorial_generate_text.py`, I try in on my mac, it works well:

```
Start learning a model to generate text
Epoch: 1/100 Learning rate: 1.00000000
0.002 perplexity: 8721.404 speed: 2486 wps
0.100 perplexity: 1586.901 speed: 3974 wps
0.199 perplexity: 1021.575 speed: 4101 wps
0.297 perplexity: 766.037 speed: 4146 wps
0.395 perplexity: 625.603 speed: 4162 wps
0.493 perplexity: 533.806 speed: 4164 wps
0.592 perplexity: 481.060 speed: 4146 wps
0.690 perplexity: 439.476 speed: 4109 wps
0.788 perplexity: 404.770 speed: 4090 wps
0.886 perplexity: 375.800 speed: 4086 wps
0.985 perplexity: 352.464 speed: 4080 wps
Epoch: 1/100 Train Perplexity: 350.236
1 : it is a long time , and i would like that . we can not do that . we can not do that . we can not do that . we can not
3 : it is a much , but the fact is , we will make a new new jobs . we need a deal to be a long time , and i want to the
5 : it is a much for the new york . the way , the fact , i want a great time , and i had it to do , the way , and i
10 : it is a long , a lot of a lot to the oil , but a good , the world would not be the “ . ” would like the obama , and

...
```",try mac work well start learning model generate text epoch learning rate perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed perplexity speed epoch train perplexity long time would like much fact make new new need deal long time want much new york way fact want great time way long lot lot oil good world would would like,issue,positive,positive,positive,positive,positive,positive
392728919,"for `tutorial_tf_dataset_voc.py` is seem there is a problem on the VOC website...

http://host.robots.ox.ac.uk/pascal/VOC/voc2007/

http://host.robots.ox.ac.uk/pascal/VOC/voc2012/

We can try later?",seem problem try later,issue,negative,neutral,neutral,neutral,neutral,neutral
392702973,"Is it related to the PR #655 you opened ? If yes, please close this issue, that way we keep the discussion in only one place.
",related yes please close issue way keep discussion one place,issue,positive,neutral,neutral,neutral,neutral,neutral
392701200,"If @luomai decides it should be merged in TL, please do the following:
- Bring back the modification of #654 in this PR
- implement some unittest to test the features you have implemented
- update the changelog respecting the formating
- fix Travis-CI build

BTW. You need to adapt your changes to the new `tl.files` API, it should be easy, the code base hasn't changed much.

Thanks a lot for the work, it's really cool 👍 ",please following bring back modification implement test update respecting fix build need adapt new easy code base much thanks lot work really cool,issue,positive,positive,neutral,neutral,positive,positive
392700332,"Thanks a lot for the work. I let @luomai decide what happens next with this feature.

Why doing 2 PRs for only one feature ? Could you merge all your changes in #655 ?
I close this PR in favor of #655.",thanks lot work let decide next feature one feature could merge close favor,issue,positive,positive,neutral,neutral,positive,positive
392647484,"Hi Fangde, thanks for the PR. I prefer using a separated function to implement the load of remote file objects, given the much clearer semantics to the users. ",hi thanks prefer function implement load remote file given much clearer semantics,issue,negative,positive,positive,positive,positive,positive
392641229,"the np.savez accept both file object and file name, maybe we can update the docs to highlight this
while it sees loading is notworkinging ",accept file object file name maybe update highlight loading,issue,positive,neutral,neutral,neutral,neutral,neutral
392596141,"@EnricoBeltramo sorry for the late reply, actually, I tested it today, but I found that both TL code and the original implementation cannot converge ... 😢 (under the lastest TL and TF 1.7)",sorry late reply actually tested today found code original implementation converge,issue,negative,negative,negative,negative,negative,negative
392572720,"> @zsdonghao who can be relevant for RL? I can't be of any help on this topic

I supposed that if a porting on tensorlayer of a working example don't work, may be there is some issue related to library. If is not considered relevant, should be closed.",relevant ca help topic supposed working example work may issue related library considered relevant closed,issue,negative,positive,positive,positive,positive,positive
392559022,"@zsdonghao I have cleaned and reorganized the Readme can you have a look ?
Can you please create the file: examples/Readme.md and add the information you think are relevant to it ;)

",look please create file add information think relevant,issue,positive,positive,positive,positive,positive,positive
392518377,"@DEKHTIARJonathan Actually, if we want to provide two kinds of `data_format`s, then we allow users use consistent arguments e.g. only `channel_last/first` instead of `NCHW/NHCW`, or both of them. At that time, string mapping techniques are needed. (Some convs in TF take `channel_last/first` while others do `NCHW/NHCW`)
If we only support the argument `channel_last` (`NHWC`), we don't need such processing because a user don't put an argument into it and we just handle `data_format` internally.",actually want provide two allow use consistent instead time string take support argument need user put argument handle internally,issue,negative,positive,neutral,neutral,positive,positive
392515947,@DEKHTIARJonathan @zsdonghao  Do you have any python techniques for mapping a word (string) into another one? I think it's a top priority to support consistent `data_format`s.,python word string another one think top priority support consistent,issue,positive,positive,positive,positive,positive,positive
392339980,"@DEKHTIARJonathan Yes, I think I need some help. As I mentioned before, the current `data_format`s in TensorFlow are very inconsistent, e.g. some operations uses `channels_first/last` while others `NCHW (NHWC)` . Moreover, the convolution operations (or layers) seem to be affected by weight variables' `shape`. I think we'd better support only `channels_last`, `NHWC` (default `data_format`) for the time being.
What do you think of it?",yes think need help current inconsistent moreover convolution seem affected weight shape think better support default time think,issue,positive,positive,positive,positive,positive,positive
392331438,"Do not merge now
I need to unpin the change",merge need unpin change,issue,negative,neutral,neutral,neutral,neutral,neutral
392322761,@2wins How is this PR going so far ? U need help ?,going far need help,issue,negative,positive,neutral,neutral,positive,positive
392322733,The CircleCI builds were not activated for forked PR this has been fixed and this PR will be build when a new commit will be pushed ;),forked fixed build new commit,issue,negative,positive,positive,positive,positive,positive
392294366,"@lgarithm and @zsdonghao I have solved the synchronisation issue between Travis and CircleCI with Approval and Orchestration: https://circleci.com/docs/2.0/workflows/#holding-a-workflow-for-a-manual-approval

**A few screenshots to illustrate:**

![image](https://user-images.githubusercontent.com/10923599/40580985-fa49c23e-614c-11e8-8799-72e525492376.png)

![image](https://user-images.githubusercontent.com/10923599/40580988-11f01e56-614d-11e8-817b-dde7f5dabb9e.png)

**Once approved, the jobs starts running:**

![image](https://user-images.githubusercontent.com/10923599/40580990-238cc3c6-614d-11e8-932b-891565d61385.png)
",issue travis approval orchestration illustrate image image running image,issue,negative,neutral,neutral,neutral,neutral,neutral
392288310,I tried the original implementation of the author (https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/experiments/Solve_BipedalWalker/A3C.py) and it looks to work. May be the issue is the porting on tensorlayer,tried original implementation author work may issue,issue,negative,positive,positive,positive,positive,positive
392218281,@zsdonghao who can be relevant for RL? I can't be of any help on this topic ,relevant ca help topic,issue,negative,positive,positive,positive,positive,positive
391362846,"@dengyueyun666 We would much appreciate a PR on this topic. If you need any help, please contact us over Skype",would much appreciate topic need help please contact u,issue,positive,positive,positive,positive,positive,positive
391313453,"If you are able to provide a simple example without any business logic @ywangeq just a few layers and dummy random input, I'll be glad to give it a look @ywangeq ",able provide simple example without business logic dummy random input glad give look,issue,negative,positive,positive,positive,positive,positive
391272504,"Im sorry, we can't fix an issue if you are not able to provide a simple dummy example.

The issue may be in the TF lib, now t necesseraly in TL. 

I'm closing this issue. ",sorry ca fix issue able provide simple dummy example issue may issue,issue,negative,neutral,neutral,neutral,neutral,neutral
391239510,"I don't find a good implement based on tensorflow. And I check the speed in the MXnet(the offical version). It is similar to the speed which is told in the paper. But when i use the deformable conv in the tensorlayer with my current work. My speed decrease from 11ms to 1s. Now i cannot give my code which related some works. 
I am quite sorry. If i found some good implement, i will remind you!
Thank for you work",find good implement based check speed version similar speed told paper use deformable current work speed decrease give code related work quite sorry found good implement remind thank work,issue,positive,positive,positive,positive,positive,positive
391101522,@youkaichao Let me rewrite my sentence.. How could the tensorlayer user pass the SHAPE arg into the TensorLayer API?,let rewrite sentence could user pas shape,issue,negative,neutral,neutral,neutral,neutral,neutral
391026607,"@zsdonghao I can't get it. what do you mean by "" I am wondering how could we pass the SHAPE arg into the API"" ? which API, tensorlayer API or tensorflow API? ""we"" means tensorlayer contributor or tensorlayer user? ",ca get mean wondering could pas shape contributor user,issue,negative,negative,negative,negative,negative,negative
391007651,"@lgarithm SiChao Liu is working on that, but the progress is very slow. Douglas and I are worry about that.",working progress slow worry,issue,negative,negative,negative,negative,negative,negative
391007334,I just resolved the conflicts and corrected the formating with YAPF. I think we should really focus on it ☺️,resolved corrected think really focus,issue,negative,positive,positive,positive,positive,positive
390923082,"@zsdonghao @lgarithm @luomai 

As you can see, we have broken support for any TF version older than 1.6.0. So TL users **must use** a TF version which is less than 2 months old.

Is it something that we want to fix ? If not, I'll remove the build support for any version older than 1.6.0.
If we want to fix this issue, where do you want to stop backward support ?

When TF versions older than 1.6.0 are removed, we obtain the following:

![image](https://user-images.githubusercontent.com/10923599/40356112-7144ebe0-5db8-11e8-90c5-7c692724416e.png)
",see broken support version older must use version le old something want fix remove build support version older want fix issue want stop backward support older removed obtain following image,issue,positive,positive,neutral,neutral,positive,positive
390843059,"Action items: 

* [X] convert the dockerhub account of tensorlayer to an org
* [ ] setup auto build

",action convert account setup auto build,issue,negative,positive,neutral,neutral,positive,positive
390842905,We still need to setup auto build for docker image.,still need setup auto build docker image,issue,negative,neutral,neutral,neutral,neutral,neutral
390776705,No one seems to be using GitKraken platform. I close the issue in favor of Github Projects,one platform close issue favor,issue,negative,neutral,neutral,neutral,neutral,neutral
390775518,"Your script does not import `tensorlayer` library. Github Issues are meants to report **issues** or **bugs** with the library. If you need TL/TF related help, I would advise you to go to StackOverflow: https://stackoverflow.com/",script import library report library need related help would advise go,issue,negative,neutral,neutral,neutral,neutral,neutral
390620559,"@dengyueyun666 Hi, I suggest to use English here, as many contributors are not Chinese, they can help you as well.",hi suggest use many help well,issue,positive,positive,positive,positive,positive,positive
390620339,"@youkaichao @DEKHTIARJonathan Just saw this message, @youkaichao I simplify your code as follow, however, I am wondering how could we pass the SHAPE arg into the API?

```python
W = (tf.get_variable(name='W', shape=(n_in, n_units)) if W_init is not None and not callable(W_init ) else tf.get_variable(name='W', shape=SHAPE))
```

",saw message simplify code follow however wondering could pas shape python none callable else,issue,negative,neutral,neutral,neutral,neutral,neutral
390492201,"Can you give some reproducible code?

It is hardly possible to do anything with your statement. ",give reproducible code hardly possible anything statement,issue,negative,neutral,neutral,neutral,neutral,neutral
390407973,We do our best regarding the slack account. Do you have any alternative email ?,best regarding slack account alternative,issue,positive,positive,positive,positive,positive,positive
390407951,"@DEKHTIARJonathan Of course! please help me a lot. Plus, please let me join the Slack :cry:.",course please help lot plus please let join slack cry,issue,positive,neutral,neutral,neutral,neutral,neutral
390407830,"May you add me as a contributor to your fork ?
I would like to be able to help you directly. You started some quite heavy changes, you may want some help ;)

I will re-origanize the tests for you, I think some refactoring may help ;)",may add contributor fork would like able help directly quite heavy may want help think may help,issue,positive,positive,positive,positive,positive,positive
390407682,"@DEKHTIARJonathan I agree with you. After no error on graph definition, sanity check is essential. I missed the important point. Thanks.",agree error graph definition sanity check essential important point thanks,issue,positive,positive,positive,positive,positive,positive
390406798,"@2wins I'm not sure to understand your point ;)

The objective of having the ""same"" file, modified accordingly to the position of the channel layer is not really about testing the output shape. The output shape can be seen as a sanity check ;)

No the rpincipal objective of the convolution test file is to be sure that no error is generated during the graph definition.

Thus, up to me, it is perfectly relevant to have the exact same file modified to use a the channel first data format. 

Could you please explain a bit more why you think it's not appropriate, I may have missed smthg.

Thanks buddy ",sure understand point objective file accordingly position channel layer really testing output shape output shape seen sanity check objective convolution test file sure error graph definition thus perfectly relevant exact file use channel first data format could please explain bit think appropriate may thanks buddy,issue,positive,positive,positive,positive,positive,positive
390405860,"@DEKHTIARJonathan It was not appropriate to duplicate `test_layers_convolution.py` directly because here `data_format` problem does not require output for testing (does not include any `assertion` requiring output). Anyway, as you said, I think dividing the file into two files are better.
Meanwhile, I found that `Conv*dLayer` using `tf.nn.conv*d` had problems. I think `tf.nn.conv*d` must not be affected by `shape` parameter but different `data_format` has an impact on `shape`.",appropriate duplicate directly problem require output testing include assertion output anyway said think dividing file two better meanwhile found think must affected shape parameter different impact shape,issue,negative,positive,positive,positive,positive,positive
390404197,"Shall we open a PR related to this topic ? If no, please close the issue @zsdonghao ",shall open related topic please close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
390403850,"@2wins It is a good thing that you thought about creating a unittest file.
I was wondering, don't you think that the most efficient and simplest way is to duplicate the file `test_layers_convolution.py` and create:
- `test_layers_convolution_channel_first.py`
- `test_layers_convolution_channel_last.py`

The exact same code just switching the position of the channnel dimension and adapting the tests.",good thing thought file wondering think efficient way duplicate file create exact code switching position dimension,issue,positive,positive,positive,positive,positive,positive
390255124,"<!--
  1 Error: Please include a CHANGELOG ent...
  0 Warnings
  1 Message: Note, we hard-wrap at 80 chars...
  0 Markdowns
-->
<table>
  <thead>
    <tr>
      <th width=""50""></th>
      <th width=""100%"" data-danger-table=""true"" data-kind=""Error"">
          1 Error
      </th>
     </tr>
  </thead>
  <tbody>
    <tr>
      <td>:no_entry_sign:</td>
      <td data-sticky=""false"">Please include a CHANGELOG entry. <br />
You can find it at <a href=""https://github.com/tensorlayer/tensorlayer/blob/master/CHANGELOG.md"">CHANGELOG.md</a>.</td>
    </tr>
  </tbody>
</table>
<table>
  <thead>
    <tr>
      <th width=""50""></th>
      <th width=""100%"" data-danger-table=""true"" data-kind=""Message"">
          1 Message
      </th>
     </tr>
  </thead>
  <tbody>
    <tr>
      <td>:book:</td>
      <td data-sticky=""false"">Note, we hard-wrap at 80 chars and use 2 spaces after the last line.</td>
    </tr>
  </tbody>
</table>

<p align=""right"" data-meta=""generated_by_danger"">
  Generated by :no_entry_sign: <a href=""http://danger.systems/"">Danger</a>
</p>
",error please include message note table th th true error error false please include entry find table th th true message message book false note use last right,issue,negative,positive,neutral,neutral,positive,positive
390222423,@zsdonghao some tutorials are still failing. I would prefer fixing everything before merging,still failing would prefer fixing everything,issue,negative,neutral,neutral,neutral,neutral,neutral
390205484,"I don't think that the flags API will change once more. They have mapped to a separate and robust Google API: Abseil_PY.
I have modified it now it should stay good for a moment. However, if TF people start talking about changing it once more, we should implement a similar thing more `stable` over time in my opinion. For now, it doesn't seem to me to be an emergency.",think change separate robust stay good moment however people start talking implement similar thing stable time opinion seem emergency,issue,positive,positive,positive,positive,positive,positive
390203837,@DEKHTIARJonathan I update the vgg16 tutorial with auto model download~,update tutorial auto model,issue,negative,neutral,neutral,neutral,neutral,neutral
390049367,"Thank you for your help! I know using name scope to add regularization, but I wish developers can add regularization in Conv2 Layer. ",thank help know name scope add regularization wish add regularization layer,issue,positive,neutral,neutral,neutral,neutral,neutral
390010822,"hi, this is a way to add regularisation using name scope.

```python
l2 = 0
for w in tl.layers.get_variables_with_name('W_conv2d', train_only=True, printable=False):
    l2 += tf.contrib.layers.l2_regularizer(1e-4)(w)
cost = tl.cost.cross_entropy(y, y_) + l2
```",hi way add name scope python cost,issue,negative,neutral,neutral,neutral,neutral,neutral
389972075,"@SPY-Ming Hi, the latest version supports list as the input, so it can handle your situation.",hi latest version list input handle situation,issue,negative,positive,positive,positive,positive,positive
389928930,Shall it be merged in the main repository @zsdonghao or please close the issue.,shall main repository please close issue,issue,negative,positive,positive,positive,positive,positive
389928299,"Was it submited as a PR ? If yes, please close the issue",yes please close issue,issue,positive,neutral,neutral,neutral,neutral,neutral
389924085,"@zsdonghao @lgarithm I don't have time to investigate why some models are not working.
I didn't try the distributed ones, I don't have the setup for it.

Could you please **checkout** the branch *tutorial_fix* and try to fix them ;)

Please tell me when some of them are fixed. That way I can flag them as good. Thanks",time investigate working try distributed setup could please branch try fix please tell fixed way flag good thanks,issue,positive,positive,positive,positive,positive,positive
389904329,"@2wins Absolutely ! Open a PR with [WIP] in the title.
Please start by implementing the unittest ;) It's always a good thing to start with the problem.

Try to follow the same pattern and rules I have given you in your first PR. In case u need help, just ping me",absolutely open title please start always good thing start problem try follow pattern given first case need help ping,issue,positive,positive,positive,positive,positive,positive
389878963,"@DEKHTIARJonathan Do you know that our many other layers _e.g._ `DeConv2dLayer`, `Conv3dLayer` do NOT support `data_format` even though their corresponding TensorFlow layers do. We have to make them consistent.",know many support even though corresponding make consistent,issue,negative,positive,positive,positive,positive,positive
389877621,"@zsdonghao Tutorial VGG16 is not working, can't load the checkpoint.
I have cleaned it a little but still not working, you can checkout the branch of this PR if you want ;)",tutorial working ca load little still working branch want,issue,negative,negative,negative,negative,negative,negative
389876827,"@DEKHTIARJonathan Can I solve this problem? I have already had a solution for `tl.layers.Conv2d`.
```python
# before
strides = (1, strides[0], strides[1], 1)

# after
if data_format in [None, 'NHWC', 'channels_last']:
    strides = (1, strides[0], strides[1], 1)
elif data_format in ['NCHW', 'channels_first']:
    strides = (1, 1, strides[0], strides[1])
else:
    raise ValueError(""'data_format' should be among 'NHWC', 'channels_last', 'NCHW', 'channels_first'."")
```",solve problem already solution python none else raise among,issue,negative,neutral,neutral,neutral,neutral,neutral
389866570,"@zsdonghao Nope, I have just applied your workaround.
And I also modified a few errors I have noticed in the tutorials. I will check that all of them run fine.",nope applied also check run fine,issue,negative,positive,positive,positive,positive,positive
389864625,"@DEKHTIARJonathan Thanks for your reply. 

 I implemented it in the [tutorial_inceptionV3_tfslim.py](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_inceptionV3_tfslim.py) .Here is my code to get the output of the layer called ""InceptionV3/InceptionV3/Conv2d_4a_3x3/Relu"".

```python
......
print(""Model Restored"")

y = network.outputs
network.print_layers()

# get_layers_with_name, return a tensor list, so get the first element
infolayer=tl.layers.get_layers_with_name(network,name='InceptionV3/InceptionV3/Conv2d4a3x3/Relu',printable=True)[0]
probs = tf.nn.softmax(y)

# test data in github: https://github.com/zsdonghao/tensorlayer/tree/master/example/data
img1 = load_image(""data/puzzle.jpeg"")
img1 = img1.reshape((1, 299, 299, 3))
prob = sess.run(probs, feed_dict={x: img1})  # the 1st time need time to compile
start_time = time.time()
prob = sess.run(probs, feed_dict={x: img1})

# get the tensor output by sess.run and fill the placeholder
print(sess.run(infolayer, feed_dict={x: img1}))

print(""End time : %.5ss"" % (time.time() - start_time))
print_prob(prob0)  # Note : as it have 1001 outputs, the 1st output is nothing

```

Could you help me to check it whether there is a easier way to implement it.
Thank you!",thanks reply code get output layer python print model return tensor list get first element network test data prob st time need time compile prob get tensor output fill print print end time prob note st output nothing could help check whether easier way implement thank,issue,positive,positive,positive,positive,positive,positive
389860433,"@DEKHTIARJonathan Okay, I set up a minimal example

```python
import tensorflow as tf
import tensorlayer as tl

x = tf.placeholder(tf.float32, shape=[32, 28, 28, 1])

# [N H W C] -> [N C H W]
x = tf.transpose(x, perm=[0, 3, 1, 2])
print(""transposed shape: "", x.shape)

network = tl.layers.InputLayer(x)
network = tl.layers.Conv2dLayer(network, data_format=""NCHW"")
```

which will give you

> Traceback (most recent call last):
  File ""tutorial_mnist.py"", line 11, in <module>
    network = tl.layers.Conv2dLayer(network, data_format=""NCHW"")
  **File ""/usr/lib/python3.6/site-packages/tensorlayer/layers/convolution.py"", line 207, in __init__
    tf.nn.conv2d(self.inputs, W, strides=strides, padding=padding, use_cudnn_on_gpu=use_cudnn_on_gpu, data_format=data_format) + b)**
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py"", line 979, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 297, in add
    ""Add"", x=x, y=y, name=name)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3392, in create_op
    op_def=op_def)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1734, in __init__
    control_input_ops)
  File ""/usr/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1570, in _create_c_op
    raise ValueError(str(e))
ValueError: Dimensions must be equal, but are 28 and 100 for 'cnn_layer/add' (op: 'Add') with input shapes: [32,100,28,28], [100].

Same thing for `Conv2d`.",set minimal example python import import print shape network network network give recent call last file line module network network file line file line return file line add add file line file line file line file line raise must equal input thing,issue,negative,negative,neutral,neutral,negative,negative
389849806,@zsdonghao how comes ? I can't manage to make this work...,come ca manage make work,issue,negative,neutral,neutral,neutral,neutral,neutral
389847528,"@DEKHTIARJonathan @liudragonfly Hi, I just run the code, it works fine.
I think this error is related to this issue https://github.com/tensorlayer/tensorlayer/issues/160
",hi run code work fine think error related issue,issue,negative,positive,positive,positive,positive,positive
389830998,"@baiyancheng20 Hello, did you solve the problem, I want to extract feature of image from resnet, do you know the methods?",hello solve problem want extract feature image know,issue,negative,neutral,neutral,neutral,neutral,neutral
389830484,"If it is working fine, please close the issue ;)",working fine please close issue,issue,negative,positive,positive,positive,positive,positive
389829391,"I confirm this issue, I can reproduce @zsdonghao.

```python
in main_word2vec_basic()
    271             tl.files.save_npz(emb_net.all_params, name=model_file_name + '.npz', sess=sess)

ValueError: could not broadcast input array from shape (50000,128) into shape (50000)
```

I have opened a new branch to fix the tutorials: https://github.com/tensorlayer/tensorlayer/tree/tutorial_fix

Most of them use old TF API which is not working anymore (tf.flags namely).

Ping @liudragonfly @aMarry",confirm issue reproduce python could broadcast input array shape shape new branch fix use old working namely ping,issue,negative,positive,positive,positive,positive,positive
389829328,"@jorgemf Hello, I launched the program on the platform of ubuntu16.4, my tensorlayer version is 1.8.4. Thanks！ Maybe the problem was caused by the network settings, rather than the GRPC.",hello program platform version maybe problem network rather,issue,negative,neutral,neutral,neutral,neutral,neutral
389826444,@DEKHTIARJonathan I see. BTW could you explain when one can use the deocrator `deprecated_alias`?,see could explain one use,issue,negative,neutral,neutral,neutral,neutral,neutral
389824868,"@unclejimbo can you please provide a sample code that I can execute to reproduce the issue ?
Thanks for reporting the issue btw.

@2wins I prefer to first focus on implementing a unittest to reproduce the bug. Then we'll find out how to solve this bug. It doesn't seem `data_format` has been deprecated to me so we'll see how to solve it ;)

I have updated the roadmap accordingly and added this issue as a bug to fix",please provide sample code execute reproduce issue thanks issue prefer first focus reproduce bug find solve bug seem see solve accordingly added issue bug fix,issue,positive,positive,positive,positive,positive,positive
389820139,@DEKHTIARJonathan What do you think of this issue? Please check my previous comment.,think issue please check previous comment,issue,negative,negative,negative,negative,negative,negative
389809614,"@DEKHTIARJonathan If you look at `tl.layers.Conv2D` and `tl.layers.Conv2DLayer`, there are no arguments for regularizations of kernel and bias. I think either of them should provide those arguments.",look kernel bias think either provide,issue,negative,neutral,neutral,neutral,neutral,neutral
389808231,"@2wins Do yo think of anything specific missing ?
I will add it to the roadmap ;)",yo think anything specific missing add,issue,negative,negative,neutral,neutral,negative,negative
389805999,"@WZMIAOMIAO At present, you have to use `tf.layers.conv2d` if you want to use a regularizer found in [regularizer.py](https://github.com/tensorflow/tensorflow/blob/r1.8/tensorflow/contrib/layers/python/layers/regularizers.py).
@DEKHTIARJonathan I think we need to support regularization in some layers as well as `tl.layers.Conv2D`.",present use want use regularizer found think need support regularization well,issue,positive,neutral,neutral,neutral,neutral,neutral
389784011,"Do you want more advanced layer than `tf.nn.atrous_conv2d` ([Link](https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d)) which is a.k.a. dilated convolution?
If so, which function is needed more? or You mean a wrapper layer?

*Add: I found `tl.layers.AtrousConv2dLayer`exists already.",want advanced layer link dilated convolution function mean wrapper layer add found already,issue,negative,positive,neutral,neutral,positive,positive
389526050,"yes, but many codes using it already~ and if google update the API name, it can still works~",yes many update name still,issue,negative,positive,positive,positive,positive,positive
389520378,"The code for TF0.12 has been commented out, so now it's more or less useless right ?
I mean it is something that we could safely deprecate.",code le useless right mean something could safely deprecate,issue,negative,negative,neutral,neutral,negative,negative
389519574,Because we want to make TF0.12 and TF1.0+ use the same code.,want make use code,issue,negative,neutral,neutral,neutral,neutral,neutral
389505284,"@One-sixth I was first focusing on the ""cosmetic"" aspects.

Now I see two major issues before merging:

1. The layer you implemented is missing some unittest to make sure everything works as planned: [test_layers_merge.py](https://github.com/tensorlayer/tensorlayer/blob/master/tests/test_layers_merge.py)

2. You need to update the documentation to insert this layer: 
https://github.com/tensorlayer/tensorlayer/edit/master/docs/modules/layers.rst#L326",first cosmetic see two major layer missing make sure everything work need update documentation insert layer,issue,negative,positive,positive,positive,positive,positive
389489745,"@li-zemin both workers finished by themselves because it seems both are set as master. It is something weird with the environment variables (what bash are you using to launch the program?). What version of tensorlayer are you using? I am going to test it again.

Tensorflow is compiled with GRPC so I think this is not the cause.",finished set master something weird environment bash launch program version going test think cause,issue,negative,negative,negative,negative,negative,negative
389465810,"We implemented a helper command to launch distributed training, which is installed with tensorlayer.
I've tested it several months ago, and it used to work.

Could you try this and see what's in the output?
`tl train example/tutorial_imagenet_inceptionV3_distributed.py`",helper command launch distributed training tested several ago used work could try see output train,issue,negative,neutral,neutral,neutral,neutral,neutral
389452586,Maybe we can make sure that the libraries `grpcio` and `grpc-tools` are installed when calling `tl.distributed`. Would this help and solve the issue ?,maybe make sure calling would help solve issue,issue,positive,positive,positive,positive,positive,positive
389415300,"@jorgemf Thanks for your recommendation. I solved this problem by installing grpcio and grpc-tools with pip, and tested GRPC before running this example. 
More information in:
http://www.grpc.io/docs/quickstart/python.html#download-the-example

Besides, I forbide the ipv6 service in order to gaurantee that the grpc service runs in ipv4 correctly. Meanwhile, the impact of firewall should be excluded.

And, after running the demo, I found that the two workers create respective session, and the training process was finished by each of themselves. There is no ""division of work"" in the process between the two workers.

I am not sure whether I understand it correctly. And how I change this demo to solve the problem that single gpu server can't run a big model with large number of parameters.
Thanks!",thanks recommendation problem pip tested running example information besides service order service correctly meanwhile impact running found two create respective session training process finished division work process two sure whether understand correctly change solve problem single server ca run big model large number thanks,issue,positive,positive,positive,positive,positive,positive
389196912,"@DEKHTIARJonathan FYI,
- `np.concatenate`: Join a sequence of arrays along an existing axis.
- `np.hstack`: Stack arrays in sequence horizontally (column wise, `axis=1`)
- `np.vstack`: Stack arrays in sequence vertically (row wise, `axis=0`)

`np.hstack` and `np.vstack` are implemented using `np.concatenate`.",join sequence along axis stack sequence horizontally column wise stack sequence vertically row wise,issue,positive,positive,positive,positive,positive,positive
389193095,"I don't use **numpy** much, but it looks OK from the [document](https://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html).
And it actually fixed the referred issue in comment.

LGTM

(Not that the layout of the **result** variable is changed.)


@zsdonghao I'll leave it to you to merge.
",use much document actually fixed issue comment layout result variable leave merge,issue,negative,positive,positive,positive,positive,positive
389184612,"@zsdonghao @lgarithm can you please take 2 minutes to have a look.
I don't really why we had to change from np.hstack, so please verify that everything is correct ;)

To me the general structure of the PR is okay, I am unsure about np.concatenate vs np.hstack vs np.vstack. I let you have a look.

**If it is good, you can remove [WIP] in the PR title, and you will be able to merge the PR**",please take look really change please verify everything correct general structure unsure let look good remove title able merge,issue,positive,positive,positive,positive,positive,positive
389181815,@DEKHTIARJonathan I tried it first but `Mark as resolved` button was inactive even if there was no conflict. Now I've done it. Many Thanks.,tried first mark resolved button inactive even conflict done many thanks,issue,negative,positive,positive,positive,positive,positive
389181130,"@2wins you have to use the resolve conflicts function 

![image](https://user-images.githubusercontent.com/10923599/40062008-79c01636-585a-11e8-9a46-41aa65ba5d4b.png)

Please copy+paste exactly the code I gave you by replacing everything ;)",use resolve function image please exactly code gave everything,issue,positive,positive,positive,positive,positive,positive
389180215,"Contribution Guidelines: https://github.com/tensorlayer/tensorlayer/blob/master/CONTRIBUTING.md
Instruction to install the project: https://github.com/tensorlayer/tensorlayer/blob/master/README.md

Beginner Guide: https://akrabat.com/the-beginners-guide-to-contributing-to-a-github-project/
",contribution instruction install project beginner guide,issue,negative,neutral,neutral,neutral,neutral,neutral
389179884,"Ok, just leave it as it is for now.

________________________________
From: Jonathan DEKHTIAR <notifications@github.com>
Sent: Tuesday, May 15, 2018 10:03:39 PM
To: tensorlayer/tensorlayer
Cc: LG; Mention
Subject: Re: [tensorlayer/tensorlayer] PyUP and Stale Bot Configuration Updated (#577)


@lgarithm<https://github.com/lgarithm> if you want to ;) To me it doesn't change much. I let you open a PR if you want, I will approve it.

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/tensorlayer/tensorlayer/pull/577#issuecomment-389178631>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AB8ydHGbtt5Bpf-TQLpusCSbkbU1inQRks5tyuA7gaJpZM4T_j9v>.
",leave sent may mention subject stale bot configuration want change much let open want approve reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
389178999,"@2wins Please resolve the conflicts by copy-pasting the following, I can not do it on your fork.
```
# Changelog
All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/)
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

<!--

============== Guiding Principles ==============

* Changelogs are for humans, not machines.
* There should be an entry for every single version.
* The same types of changes should be grouped.
* Versions and sections should be linkable.
* The latest version comes first.
* The release date of each version is displayed.
* Mention whether you follow Semantic Versioning.

============== Types of changes (keep the order) ==============

* `Added` for new features.
* `Changed` for changes in existing functionality.
* `Deprecated` for soon-to-be removed features.
* `Removed` for now removed features.
* `Fixed` for any bug fixes.
* `Security` in case of vulnerabilities.
* `Dependencies Update` in case of vulnerabilities.
* `Contributors` to thank the contributors that worked on this PR.

============== How To Update The Changelog for a New Release ==============

** Always Keep The Unreleased On Top **

To release a new version, please update the changelog as followed:
1. Rename the `Unreleased` Section to the Section Number
2. Recreate an `Unreleased` Section on top
3. Update the links at the very bottom

======================= START: TEMPLATE TO KEEP IN CASE OF NEED ===================

** DO NOT MODIFY THIS SECTION ! **

## [Unreleased]

### Added

### Changed

### Deprecated

### Removed

### Fixed

### Security

### Dependencies Update

### Contributors

** DO NOT MODIFY THIS SECTION ! **

======================= END: TEMPLATE TO KEEP IN CASE OF NEED ===================

-->

<!-- YOU CAN EDIT FROM HERE -->

## [Unreleased]

### Added
- Tutorials:
  - `tutorial_tfslim` has been introduced to show how to use `SlimNetsLayer` (by @2wins in #560).
- Test:
  - `Layer_DeformableConvolution_Test` added to reproduce issue #572 with deformable convolution (by @DEKHTIARJonathan in #573)  
  - `test_utils_predict.py` added to reproduce and fix issue #288 (by @2wins in #566)
- CI Tool:
  - Danger CI has been added to enforce the update of the changelog (by @lgarithm and @DEKHTIARJonathan in #563)
  - https://github.com/apps/stale/ added to clean stale issues (by @DEKHTIARJonathan in #573)

### Changed
- Tensorflow CPU & GPU dependencies moved to separated requirement files in order to allow PyUP.io to parse them (by @DEKHTIARJonathan in #573)

### Deprecated

### Removed

### Fixed
- Issue #498 - Deprecation Warning Fix in `tl.layers.RNNLayer` with `inspect` (by @DEKHTIARJonathan in #574)
- Issue #498 - Deprecation Warning Fix in `tl.files` with truth value of an empty array is ambiguous (by @DEKHTIARJonathan in #575)
- Issue #572 with deformable convolution fixed (by @DEKHTIARJonathan in #573)
- Issue #565 related to `tl.utils.predict` fixed - `np.hstack` problem in which the results for multiple batches are stacked along `dim=1` (by @2wins in #566)

### Security

### Dependencies Update

### Contributors
@lgarithm @DEKHTIARJonathan @2wins


## [1.8.5] - 2018-05-09

### Added
- Github Templates added (by @DEKHTIARJonathan)
  - New issues Template
  - New PR Template
- Travis Deploy Automation on new Tag (by @DEKHTIARJonathan)
  - Deploy to PyPI and create a new version.
  - Deploy to Github Releases and upload the wheel files
- PyUP.io has been added to ensure we are compatible with the latest libraries (by @DEKHTIARJonathan)
- `deconv2d` now handling dilation_rate (by @zsdonghao)
- Documentation unittest added (by @DEKHTIARJonathan)
- `test_layers_core` has been added to ensure that `LayersConfig` is abstract.

### Changed
- All Tests Refactored - Now using unittests and runned with PyTest (by @DEKHTIARJonathan)
- Documentation updated (by @zsdonghao)
- Package Setup Refactored (by @DEKHTIARJonathan)
- Dataset Downlaod now using library progressbar2 (by @DEKHTIARJonathan)
- `deconv2d` function transformed into Class (by @zsdonghao)
- `conv1d` function transformed into Class (by @zsdonghao)
- super resolution functions transformed into Class (by @zsdonghao)
- YAPF coding style improved and enforced (by @DEKHTIARJonathan)

### Fixed
- Backward Compatibility Restored with deprecation warnings (by @DEKHTIARJonathan)
- Tensorflow Deprecation Fix (Issue #498):
  - AverageEmbeddingInputlayer (by @zsdonghao)
  - load_mpii_pose_dataset (by @zsdonghao)
- maxPool2D initializer issue #551 (by @zsdonghao)
- `LayersConfig` class has been enforced as abstract
- Pooling Layer Issue #557 fixed (by @zsdonghao)

### Dependencies Update
- scipy>=1.0,<1.1 => scipy>=1.1,<1.2

### Contributors
@zsdonghao @luomai @DEKHTIARJonathan

[Unreleased]: https://github.com/tensorlayer/tensorlayer/compare/1.8.5...master
[1.8.5]: https://github.com/tensorlayer/tensorlayer/compare/1.8.4...1.8.5

```",please resolve following fork notable project file format based keep project semantic entry every single version grouped linkable latest version come first release date version displayed mention whether follow semantic keep order added new functionality removed removed removed fixed bug security case update case thank worked update new release always keep unreleased top release new version please update rename unreleased section section number recreate unreleased section top update link bottom start template keep case need modify section unreleased added removed fixed security update modify section end template keep case need edit unreleased added show use test added reproduce issue deformable convolution added reproduce fix issue tool danger added enforce update added clean stale requirement order allow parse removed fixed issue deprecation warning fix inspect issue deprecation warning fix truth value empty array ambiguous issue deformable convolution fixed issue related fixed problem multiple along security update added added new template new template travis deploy new tag deploy create new version deploy wheel added ensure compatible latest handling documentation added added ensure abstract documentation package setup library function class function class super resolution class style enforced fixed backward compatibility deprecation deprecation fix issue issue class enforced abstract layer issue fixed update unreleased,issue,positive,positive,positive,positive,positive,positive
389178631,"@lgarithm if you want to ;) To me it doesn't change much. I let you open a PR if you want, I will approve it.",want change much let open want approve,issue,negative,positive,neutral,neutral,positive,positive
389176267,@zsdonghao I'm new to github. Can you help modify the code and add this new feature?,new help modify code add new feature,issue,negative,positive,positive,positive,positive,positive
389175039,"Should we remove the message from danger?
`Note, we hard-wrap at 80 chars and use 2 spaces after the last line.`

I think it's just an example of how to use danger message.
",remove message danger note use last think example use danger message,issue,negative,neutral,neutral,neutral,neutral,neutral
389159334,"@One-sixth this is a very good feature indeed !

I think you can make a new push request and put your code as example ~

BTW, please make sure the previous codes can still run well.

Best wishes.

",good feature indeed think make new push request put code example please make sure previous still run well best,issue,positive,positive,positive,positive,positive,positive
389156964,"@li-zemin the PS seems ok, so there is an issue in the communication between the workers and the PS. It could be the port is closed or something like that. I also see the first worker prints the parameters of the network and if you see the code it shouldn't do it. Have you tried to run everything in the same machine without using a GPU? It will help to know whether is a network issue or something else",issue communication could port closed something like also see first worker network see code tried run everything machine without help know whether network issue something else,issue,negative,positive,neutral,neutral,positive,positive
389156070,@DEKHTIARJonathan i suggest to close it and reopen a new one later~,suggest close reopen new one,issue,negative,positive,positive,positive,positive,positive
389118422,"You can use ""AssertRaises"" in the unittest to check that some case are raising an Exception ;)
Maybe raising a `ValueError` or something similar is the best thing to do.",use check case raising exception maybe raising something similar best thing,issue,positive,positive,positive,positive,positive,positive
389117425,"It was partly correct.

Once #574 and #575 will be merged, there will remain no deprecation warning to the best of my knowledge.

@zsdonghao shall we keep this issue in case new deprecation warnings appear ? Or I close it and we will reopen a new one when necessary ?",partly correct remain deprecation warning best knowledge shall keep issue case new deprecation appear close reopen new one necessary,issue,negative,positive,positive,positive,positive,positive
389117381,@DEKHTIARJonathan You're right. I completely have the same opinion about that. I think the explicit exception is the best solution here. Thanks.,right completely opinion think explicit exception best solution thanks,issue,positive,positive,positive,positive,positive,positive
389113286,"@2wins I believe that everyone can make mistakes beginners like experts. Maybe we can raise an explicit Exception ""Batch_Size Mismatched bla bla bla"" to make it easier to debug. 

However, in my opinion, we should **not** in any case torture the library to make it user-error proof.",believe everyone make like maybe raise explicit exception make easier however opinion case torture library make proof,issue,negative,neutral,neutral,neutral,neutral,neutral
389095841,"@DEKHTIARJonathan I agree with you. However, some beginners could encounter this problem as seen in issue #288, and might think it as tensorlayer's problem. What do you think of it?",agree however could encounter problem seen issue might think problem think,issue,negative,neutral,neutral,neutral,neutral,neutral
389088830,"I'm not sure to understand. Why do we want to fix this issue ?

It is not a correct practice to input non-matching data size into a placeholders. If the PLH size varies, you have to use `None` as a batch_size dimension.

Is there any usecase where the user do not control the placeholder and need to be able custom batch size ?

I believe it would be of very bad practice to allow people to input any batch_size in any placeholder.",sure understand want fix issue correct practice input data size size use none dimension user control need able custom batch size believe would bad practice allow people input,issue,negative,positive,positive,positive,positive,positive
389082559,"@jorgemf Thank you for your reply, I described my problem in detail, and format it in markdown.
Thank you for your help!",thank reply problem detail format markdown thank help,issue,positive,neutral,neutral,neutral,neutral,neutral
389082336,"Alright, I managed to reproduce the problem and I have added a unittest relative to this issue: https://github.com/tensorlayer/tensorlayer/pull/573

This seems like a simple fix, I will try to merge today. You will be able to install TL from master.",alright reproduce problem added relative issue like simple fix try merge today able install master,issue,negative,positive,positive,positive,positive,positive
389079718,"As @Liang-yc said, it works well after removing [Line 971](https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L971)",said work well removing line,issue,negative,neutral,neutral,neutral,neutral,neutral
389064239,"@DEKHTIARJonathan That's the problem of `tl.utils.predict`. Of course, we cannot feed data into `tf.placeholder` with non-matched size. I think there are two options for `tl.utils.predict`.

1. not allow the function to take data with non-matched size.
2. temporally using dummy data to fit input size.",problem course feed data size think two allow function take data size temporally dummy data fit input size,issue,negative,positive,positive,positive,positive,positive
389062027,"Are you sure to understand how TF placeholders works?
To my best knowledge, it is simply impossible to input [7, 5, 5, 3] into a plh of shape [8, 5, 5, 3]",sure understand work best knowledge simply impossible input shape,issue,positive,positive,positive,positive,positive,positive
388885363,@DEKHTIARJonathan @zsdonghao Please check the Travis-CI error and give me a advice.,please check error give advice,issue,negative,neutral,neutral,neutral,neutral,neutral
388883693,"@li-zemin The log you provided doesn't say anything. How do you run the programs? I need to know the command line used to start the training. PS usually doesn't output very much to the console, so not seeing anything doesn't mean they are not working",log provided say anything run need know command line used start training usually output much console seeing anything mean working,issue,negative,negative,negative,negative,negative,negative
388863897,"Thank you very much, can you do me a favor? @zsdonghao Thanks!",thank much favor thanks,issue,positive,positive,positive,positive,positive,positive
388863418,"I'm not against in any case, however I still struggle to see the added value.

If we decide to provide let's say a GoLang API for tensorlayer. Clearly you will not use anaconda. Same remark for Tensorflow-JS if we decide to provide tensorlayer in javascript.

So what kind of project would benefit of having Anaconda package that can not use at all PyPI. There must be something I don't understand.

Uo to me Anaconda and PyPI basically do the exact same stuff, however conda allows much more complex librairies to be installed. As TL is only developed on top of TF in pure Python... I don't see the benefits.

Even TF which is developed in C++ and heavily use CUDA and CUDNN code do not rely on Conda package manager.

I mean if you want to do it, I'm not against in any case. However, doing such a thing takes some time, I would like to be sure that you are not doing it for nothing ",case however still struggle see added value decide provide let say clearly use anaconda remark decide provide kind project would benefit anaconda package use must something understand anaconda basically exact stuff however much complex top pure python see even heavily use code rely package manager mean want case however thing time would like sure nothing,issue,positive,positive,positive,positive,positive,positive
388860518,"Could you please format correctly your message. It is unreadable.

Please have a look to how you should format code and logs in markdown.

BTW @zsdonghao I can't help on this. ",could please format correctly message unreadable please look format code markdown ca help,issue,positive,neutral,neutral,neutral,neutral,neutral
388822084,"Yup, the installation was very smooth using pip and it does show under my Conda packages now. 

`conda list`

![screen shot 2018-05-14 at 06 47 10](https://user-images.githubusercontent.com/7281944/40001261-b3efd7d8-5742-11e8-8e37-81d17ae63f65.png)

The reason I mentioned it is pip handles only Python based dependencies whereas conda handles non-python library dependencies as well. It can be essentially helpful for large relational databases. If in future, we plan to extend it to include other implementations it should be a requirement. 

Any dependency that doesn't have a setup.py file in its source file should be hard to integrate. 

Maybe I can start experimenting with it and once we have it, we can eventually decide whether to include it or not? No harm, right?",installation smooth pip show list screen shot reason pip python based whereas library well essentially helpful large relational future plan extend include requirement dependency file source file hard integrate maybe start eventually decide whether include harm right,issue,negative,positive,positive,positive,positive,positive
388818966,"I would also support Github Projects.
I need to use it a little to see if it would suit our needs.",would also support need use little see would suit need,issue,negative,negative,negative,negative,negative,negative
388817368,"@2wins what is your slack account, I add you first.",slack account add first,issue,negative,positive,positive,positive,positive,positive
388814672,"+1 for `github project`

We can use Automated kanban template that moves the cards automatically move between To do, In progress, and Done columns making it much easier to keep track of everything.

Here's how to configure it-

![screen shot 2018-05-14 at 06 20 47](https://user-images.githubusercontent.com/7281944/39999861-107988a4-573f-11e8-8ab9-ae4c81bc563f.png)

That said, if needed (won't be, yet) we can always transition to another one later. ",project use template automatically move progress done making much easier keep track everything configure screen shot said wo yet always transition another one later,issue,positive,positive,neutral,neutral,positive,positive
388803917,@zsdonghao It does not work as well. You'd better contact Slack.,work well better contact slack,issue,positive,positive,positive,positive,positive,positive
388799897,"@2wins @DEKHTIARJonathan the invitation link doesn't expire, can you try again ?",invitation link expire try,issue,negative,neutral,neutral,neutral,neutral,neutral
388774418,"I have no control over this.

@zsdonghao why is Slack only under invitation? Can't we open it fully? ",control slack invitation ca open fully,issue,negative,neutral,neutral,neutral,neutral,neutral
388773847,@DEKHTIARJonathan @zsdonghao Please solve the Slack problem. I'd like to ask you guys some questions in Slack.,please solve slack problem like ask slack,issue,negative,neutral,neutral,neutral,neutral,neutral
388771929,"It is perfectly installable using `pip` which is the most widespread and commonly used python package management system.

PIP is perfectly usable even if you use Anaconda Distributions (very good practice in my opinion). 

Maybe I'm missing, what would be the advantage(s) in your opinion ? In my opinion, conda packages are only interesting in case of very complex packages: `python-opencv` is a good example.

Any additional system or distribution network requires maintenance and will be only added if it represents a real added-value to the project.",perfectly pip widespread commonly used python package management system pip perfectly usable even use anaconda good practice opinion maybe missing would advantage opinion opinion interesting case complex good example additional system distribution network maintenance added real project,issue,positive,positive,positive,positive,positive,positive
388770828,"Okay now we can work on the core of the PR (preferably in this order):
1. You should fix the test file to use `unittest` and be coherent with the rest of the project (see [comment](https://github.com/tensorlayer/tensorlayer/pull/566#pullrequestreview-119734721))
2. We will see how we can fix `tl.utils.predict` in order to make sure that the test pass.

Btw. I try to coach you for this specific PR hoping that you will learn how to make more efficient and clearner contributions on later PRs.",work core preferably order fix test file use coherent rest project see comment see fix order make sure test pas try coach specific learn make efficient later,issue,positive,positive,positive,positive,positive,positive
388729575,"@DEKHTIARJonathan I tried joining Slack a few times, but the mail does not come.",tried joining slack time mail come,issue,negative,neutral,neutral,neutral,neutral,neutral
388729278,"@2wins That's not how it is supposed to work.
The objective of a test is to highlight the errors and bugs. If something is not working, you should let the tests as it is supposed to be. And fix the codebase.",supposed work objective test highlight something working let supposed fix,issue,negative,neutral,neutral,neutral,neutral,neutral
388728496,"@zsdonghao can you please have a look and merge if it looks good.
I'm not very good in RST, I don't want to make any mistake ;)",please look merge good good want make mistake,issue,positive,positive,positive,positive,positive,positive
388728073,@DEKHTIARJonathan `case 2` and `case 3` make an error. So I commented them out.,case case make error,issue,negative,neutral,neutral,neutral,neutral,neutral
388725556,@DEKHTIARJonathan It's my mistake. Please check and merge it.,mistake please check merge,issue,negative,neutral,neutral,neutral,neutral,neutral
388719286,"@DEKHTIARJonathan OK, I'll check it.
For Slack, can you invite me directly? The invitation mail is not sent to me.",check slack invite directly invitation mail sent,issue,negative,positive,neutral,neutral,positive,positive
388716288,"BTW. As you can see the test is still failing. So there is an error either in your modifications or your test 😉.
That's exactly why I asked you to implement a test 😊",see test still failing error either test exactly implement test,issue,negative,positive,positive,positive,positive,positive
388715826,There is a huge button in the readme. There is no way u can miss it. ,huge button way miss,issue,negative,positive,positive,positive,positive,positive
388664433,"@DEKHTIARJonathan I see. Sorry to bother you.
@zsdonghao BTW, how can I join the Slack? Every invitation has not been sent.",see sorry bother join slack every invitation sent,issue,negative,negative,negative,negative,negative,negative
388642150,"@2wins please add the test you have created in #567 in this PR.
We try to only merge working PRs.

Please also update the file Changelog.md with your modifications. From now on, we will enforce this policy.",please add test try merge working please also update file enforce policy,issue,positive,neutral,neutral,neutral,neutral,neutral
388630498,"@DEKHTIARJonathan As you know, the error shown below is related to [this](https://github.com/tensorflow/tensorflow/issues/18592). Therefore, I think there is no solvable problem in tensorlayer itself. What do you think about this?
> WARNING:tensorflow:From C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\contrib\learn\python\learn\datasets\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating: Use the retry module or similar alternatives.",know error shown related therefore think solvable problem think warning retry removed future version use retry module similar,issue,negative,neutral,neutral,neutral,neutral,neutral
388629528,Problem with API key I'll fix it tonight. I don't have my computer now,problem key fix tonight computer,issue,negative,neutral,neutral,neutral,neutral,neutral
388626766,@2wins nice! it would be great to add this test scripts into test folder ~,nice would great add test test folder,issue,positive,positive,positive,positive,positive,positive
388626483,"@DEKHTIARJonathan @zsdonghao Please see the below snippets.
```python
# case 1: No. of examples is not divisible by batch_size
# but the input placeholder's first dim is None.
x = tf.placeholder(tf.float32, [None, 5, 5, 3])
X = np.ones([127, 5, 5, 3])
net = tl.layers.InputLayer(x)
y = net.outputs
y_op = tf.nn.softmax(y)
result = tl.utils.predict(sess, net, X, x, y_op, batch_size=8)
```
```python
# case 2: No. of examples > batch_size & not divisible by batch_size
x = tf.placeholder(tf.float32, [8, 5, 5, 3])
X = np.ones([127, 5, 5, 3])
net = tl.layers.InputLayer(x)
y = net.outputs
y_op = tf.nn.softmax(y)
result = tl.utils.predict(sess, net, X, x, y_op, batch_size=8)
```
```python
# case 3: No. of examples < batch_size (actually same with the last mini-batch in case 2)
x = tf.placeholder(tf.float32, [8, 5, 5, 3])
X = np.ones([7, 5, 5, 3])
net = tl.layers.InputLayer(x)
y = net.outputs
y_op = tf.nn.softmax(y)
result = tl.utils.predict(sess, net, X, x, y_op, batch_size=8)
```",please see python case divisible input first dim none none net result sess net python case divisible net result sess net python case actually last case net result sess net,issue,negative,positive,neutral,neutral,positive,positive
388626313,"@DEKHTIARJonathan good idea.  @2wins could you help to put the test in this PR? 

Simply extend this one https://github.com/tensorlayer/tensorlayer/blob/master/tests/test_mnist_simple.py",good idea could help put test simply extend one,issue,positive,positive,positive,positive,positive,positive
388625600,"@2wins @zsdonghao why not trying to write a small unittests for all the cases you could think of?
If it is something tricky it would definitely be a good thing to have it tested. 

BTW. In case you need smthg more versatile, `np.stack` is more generic and work just fine.  ",trying write small could think something tricky would definitely good thing tested case need versatile generic work fine,issue,positive,positive,positive,positive,positive,positive
388624197,@zsdonghao Partially yes. Please check [my comment](https://github.com/tensorlayer/tensorlayer/issues/288#issuecomment-388623558) in #288.,partially yes please check comment,issue,positive,negative,neutral,neutral,negative,negative
388623975,"@2wins thanks, I have one question. If the number of example can not be divided by the batch size, it is still work fine?",thanks one question number example divided batch size still work fine,issue,positive,positive,positive,positive,positive,positive
388623558,"@luomai I think this issue hasn't solved yet. Now, `tl.utils.predict` works only if `x.shape[0]` is equal to `batch_size` or `x.shape[0]` is `None`. When `x.shape[0]` is not `None` and is just equal to `batch_size`, `X.shape[0]` that is not a multiple of `batch_size` raises an error.
If we provide the argument `batch_size` continuously, this function should be modified to account for the issue.",think issue yet work equal none none equal multiple error provide argument continuously function account issue,issue,negative,neutral,neutral,neutral,neutral,neutral
388617470,"@DEKHTIARJonathan @zsdonghao  When does one use `deprecated_alias`. Most convolutional layers in tensorlayer had the decorator `@deprecated_alias(layer='prev_layer', end_support_version=1.9)`. [Link](https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L66)
Can I modify the code to support `data_format=""NCHW""` or leave it?
Please comment it. Thanks.",one use convolutional decorator link modify code support leave please comment thanks,issue,positive,positive,positive,positive,positive,positive
388589874,"The advantage of putting it inside the repo is it ease the release process.

TF is merging an incredible amount of PR from numerous people. If they implemented such a practice it would become huge very fast. I think their Changelog is focusing on the major stuff. 

I think the release of 1.8.5 is a good example of why we need it.
Some changes are missing and some contributors are also missing. ",advantage inside ease release process incredible amount numerous people practice would become huge fast think major stuff think release good example need missing also missing,issue,positive,positive,positive,positive,positive,positive
388589795,"Apart from tensorflow saver, we are working on a graph API which can save model into caffe-like prototxt file https://github.com/tensorlayer/tensorlayer/pull/469",apart saver working graph save model file,issue,negative,neutral,neutral,neutral,neutral,neutral
388589758,"@lgarithm tensorflow also don't have **Changelog.md**, i think that info can put into release?",also think put release,issue,negative,neutral,neutral,neutral,neutral,neutral
388565023,"@DEKHTIARJonathan I think you can update this post.  `Conv1dLayer` has been solved by @zsdonghao. In addition, The warning of `inspect.getargspec()` in 6 - `RNNLayer` seem to be solved in tensorflow itself. [Link](https://github.com/tensorflow/tensorflow/pull/19199)",think update post addition warning seem link,issue,negative,neutral,neutral,neutral,neutral,neutral
388456086,"@zsdonghao @lgarithm we should try a merge. It seems that we need to have the Changelog inside the repository for it to work.

Is it fine if I merge and we'll see if it works ? We can still rollback if it doesn't. 

I have modified in the PR :
- Danger will only run with Python 3.6 on pull requests.
- I have added a template Changelog
- Added a few custom configs to danger i found in various places. 
 ",try merge need inside repository work fine merge see work still rollback danger run python pull added template added custom danger found various,issue,negative,positive,positive,positive,positive,positive
388436060,"@lgarithm I have added a template for the Changelog.md

I will update it with the relevant information for the 1.8.5 release",added template update relevant information release,issue,negative,positive,positive,positive,positive,positive
388405134,"@luomai the danger check provides a way of running additional checks within travis-ci. 
We were searching for such a solution (a way to add warning message in test, but not fail to test.) when fixing pylints.",danger check way running additional within searching solution way add warning message test fail test fixing,issue,negative,negative,negative,negative,negative,negative
388403666,"The remaining thing TODO is just introduce the CHANGELOG.md file, 
and rewrite Dangerfile appropriately.",thing introduce file rewrite appropriately,issue,negative,positive,positive,positive,positive,positive
388403200,"Looks like it's working.

And according to our settings, failed to pass danger check won't block merge.
<img width=""710"" alt=""screen shot 2018-05-11 at 11 43 27 pm"" src=""https://user-images.githubusercontent.com/2044532/39933354-518538c8-5575-11e8-889d-886a76205e5c.png"">

",like working according pas danger check wo block merge screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
388368378,"I have seen something called Danger doing exactly this : https://github.com/danger/danger

I don't know how much effort would it be to enforce this policy on the repository. ",seen something danger exactly know much effort would enforce policy repository,issue,negative,positive,positive,positive,positive,positive
388365858,"I like this idea. 
We need to find out if there are free automation bot could help it.",like idea need find free bot could help,issue,positive,positive,positive,positive,positive,positive
388293192,@2wins Awesome ! I merge immediately. Thanks a lot for your contribution.,awesome merge immediately thanks lot contribution,issue,positive,positive,positive,positive,positive,positive
387967026,"@2wins thanks for this PR.

Can you update your file to use the SlimNetsLayer instead of LambdaLayer. It will be perfect 👌.

Thanks again

Jonathan  ",thanks update file use instead perfect thanks,issue,positive,positive,positive,positive,positive,positive
387797427,"I vote `github project`, it can keep the management simple ~",vote project keep management simple,issue,negative,neutral,neutral,neutral,neutral,neutral
387793729,"@cdhcs1516 thank you for reporting this bug, it is due to a previous update.",thank bug due previous update,issue,negative,negative,negative,negative,negative,negative
387692929,"Tensorflow should be updated to latest version: 

- `pip install --upgrade tensorflow`
- `pip install --upgrade tensorflow-gpu`

How can you hope it is working a version of TF more than 1 year old and the latest version of TL?",latest version pip install upgrade pip install upgrade hope working version year old latest version,issue,negative,positive,positive,positive,positive,positive
386890811,"For adding a new example link, please update both github readme and docs (en, cn).

- [readme](https://github.com/tensorlayer/tensorlayer/blob/master/README.md)
- [docs-en](https://github.com/tensorlayer/tensorlayer/blob/master/docs/user/example.rst)
- [docs-cn](https://github.com/tensorlayer/tensorlayer-chinese/blob/master/docs/user/example.rst)",new example link please update en,issue,negative,positive,positive,positive,positive,positive
386890585,"@2wins Hi, the example list was updated like [this](https://github.com/tensorlayer/tensorlayer#basics) yesterday. Could you put your link based on the new update? Many thanks.",hi example list like yesterday could put link based new update many thanks,issue,positive,positive,positive,positive,positive,positive
386880821,"I tried a fix, I think to know where the error come from. I have merged the PR #550.
Can you update your library to the latest commit in master ? And try again 👍.

I hope this will solve the issue.",tried fix think know error come update library latest commit master try hope solve issue,issue,negative,positive,positive,positive,positive,positive
386871141,"Are you sure, you have the latest master version?

If you don't need to install from sources, you can use the PyPI package.

`pip install tensorlayer`

Can you give us the output of this command :

`pip freeze`",sure latest master version need install use package pip install give u output command pip freeze,issue,negative,positive,positive,positive,positive,positive
386824176,"Look at Travis or launch PyTest.
You will see the documentation is failing ",look travis launch see documentation failing,issue,negative,neutral,neutral,neutral,neutral,neutral
386790434,"@2wins thanks a lot for your PR. Nice work.

@zsdonghao do you know why codacy can't analyse this PR?
It also seems that Travis didn't launch a commit build. Just a PR build. 

Maybe because the commits are not from the root repository.  ",thanks lot nice work know ca analyse also travis launch commit build build maybe root repository,issue,positive,positive,positive,positive,positive,positive
386628469,"The log says
```
E               SphinxWarning: docs/user/example.rst:5:Duplicate explicit target name: ""here"".
```
you can preview `docs/user/example.rst` with **ReStructured Text Previewer** VSCode plugin:
(To install the plugin, just search **rst** in VScode Extensions Market.)
<img width=""1328"" alt=""screen shot 2018-05-04 at 10 56 13 pm 1"" src=""https://user-images.githubusercontent.com/2044532/39634856-710b88d2-4fee-11e8-911a-2dc51a7819aa.png"">
",log explicit target name preview text install search market screen shot,issue,negative,neutral,neutral,neutral,neutral,neutral
386571473,"The documentation test is failing. Can you have a look why?
My implementation of the test might be faulty. ",documentation test failing look implementation test might faulty,issue,negative,neutral,neutral,neutral,neutral,neutral
385976673,@Windaway please update to the latest TL version.. I just found that this problem already fixed.,please update latest version found problem already fixed,issue,negative,positive,positive,positive,positive,positive
385311272,"I don't really know what is not working here. Could someone have a look?
Thanks
I'm pretty sure that the commands I launched are fine, it must come from the RTD config. ",really know working could someone look thanks pretty sure fine must come,issue,positive,positive,positive,positive,positive,positive
385310613,Please accept changes and immediately merge 😉,please accept immediately merge,issue,positive,neutral,neutral,neutral,neutral,neutral
385279562,"@lgarithm I have looked quite extensively the documentation for read the docs. I think I may have a more elegant.
I will give it a shot ;) I assign you @lgarithm as reviewer.
",quite extensively documentation read think may elegant give shot assign reviewer,issue,negative,positive,positive,positive,positive,positive
385274208,"We are not the only ones with these issues:
https://github.com/rtfd/readthedocs.org/issues/3738
https://github.com/tensorflow/tensorflow/issues/17411
https://github.com/Bihaqo/t3f/pull/127

It seems to be linked with facts that the binaries are built with AVX instruction support. Some systems may lack this instruction",linked built instruction support may lack instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
385266114,"I currently created two files: requirements.txt and requirements-rtd.txt.
requirements-rtd.txt is for doc build only. 
(Can't think of other solutions ATM, use this work around temporarily)",currently two doc build ca think use work around temporarily,issue,negative,neutral,neutral,neutral,neutral,neutral
385265928,"There will be problems very soon with TF 1.5.0, many tf APIs are deprecated or not existing on this versions. 

We need to fix this very soon. And BTW. It must be working because the official TF document is also using Sphinx. ",soon many need fix soon must working official document also sphinx,issue,negative,positive,positive,positive,positive,positive
384678184,"@lgarithm is correct. Any helper script shall go into the ""tl cmd"" module. ""tl train"" is a simple example how to add a new helper script.",correct helper script shall go module train simple example add new helper script,issue,positive,positive,neutral,neutral,positive,positive
384653529,"I have created a batch version

```python
import os

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import tensorflow as tf

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}
tf.logging.set_verbosity(tf.logging.ERROR)

def rename(checkpoint_dir, replace_list, dry_run):
    
    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)
    
    renamed_vars = 0
    
    with tf.Session() as sess:
        for var_name, _ in tf.contrib.framework.list_variables(checkpoint_dir):
            
            is_renamed = False
            new_name = var_name
            
            for rename_var in replace_list:
                if rename_var[0] in var_name:
                    new_name = var_name.replace(rename_var[0], rename_var[1], 1)
                    is_renamed = True
                    renamed_vars += 1
            
            # Load the variable
            var = tf.contrib.framework.load_variable(checkpoint_dir, var_name)
            
            if is_renamed:
                if dry_run:
                    print('%s would be renamed to %s.' % (var_name, new_name))
                else:
                    print('Renaming %s to %s.' % (var_name, new_name))
                    # Rename the variable
            
            var = tf.Variable(var, name=new_name)

        if not dry_run and renamed_vars:
            # Save the variables
            print(""Saving Checkpoint (%s)..."" % checkpoint_dir)
            saver = tf.train.Saver()
            sess.run(tf.global_variables_initializer())
            saver.save(sess, checkpoint.model_checkpoint_path)
            print(""Checkpoint Saved!"")
            
        elif not renamed_vars:
            print(""Checkpoint (%s) is already in correct format - Not Modified"" % checkpoint_dir) 


if __name__ == '__main__':
    replace_list = [
        ('W_conv2d', 'kernel'),
        ('b_conv2d', 'bias'),
    ]
        
    checkpoint_dirs = [ckpt_dir for ckpt_dir in os.listdir() if os.path.isdir(ckpt_dir)]

    dry_run = False # set to True to run a simulation and do not modify your checkpoint file
    
    for ckpt_dir in checkpoint_dirs:
        rename(ckpt_dir, replace_list, dry_run)
```",batch version python import o import import rename sess false true load variable print would else print rename variable save print saving saver sess print saved print already correct format false set true run simulation modify file rename,issue,positive,negative,neutral,neutral,negative,negative
384652738,"I think the `tl` command is a good place to go.

Users can do 
```
tl convert xxx.ckpt
```",think command good place go convert,issue,negative,positive,positive,positive,positive,positive
384650981,"@lgarithm @luomai Hi, is there some standard to put this kind of API into the library?  e.g. `tl.helpers` , `tl.files`?",hi standard put kind library,issue,positive,positive,positive,positive,positive,positive
384650377,"It can be solved with the following script:

```python
import tensorflow as tf


def rename(checkpoint_dir, replace_list, dry_run):
    
    checkpoint = tf.train.get_checkpoint_state(checkpoint_dir)
    
    with tf.Session() as sess:
        for var_name, _ in tf.contrib.framework.list_variables(checkpoint_dir):
            
            is_renamed = False
            new_name = var_name
            
            for rename_var in replace_list:
                if rename_var[0] in var_name:
                    new_name = var_name.replace(rename_var[0], rename_var[1], 1)
                    is_renamed = True
            
            # Load the variable
            var = tf.contrib.framework.load_variable(checkpoint_dir, var_name)
            
            if is_renamed:
                if dry_run:
                    print('%s would be renamed to %s.' % (var_name, new_name))
                else:
                    print('Renaming %s to %s.' % (var_name, new_name))
                    # Rename the variable
            
            var = tf.Variable(var, name=new_name)

        if not dry_run:
            # Save the variables
            print(""Saving Checkpoint..."")
            saver = tf.train.Saver()
            sess.run(tf.global_variables_initializer())
            saver.save(sess, checkpoint.model_checkpoint_path)
            print(""Checkpoint Saved!"")


if __name__ == '__main__':
    replace_list = [
        ('W_conv2d', 'kernel'),
        ('b_conv2d', 'bias'),
    ]
        
    checkpoint_dir = ""the/folder/relative_path/where/your/ckptfile/is_saved""
    dry_run = False # set to True to run a simulation and do not modify your checkpoint file

    rename(checkpoint_dir, replace_list, dry_run)
```",following script python import rename sess false true load variable print would else print rename variable save print saving saver sess print saved false set true run simulation modify file rename,issue,positive,negative,neutral,neutral,negative,negative
384639121,"I just had a look. It seems that no.

Wait in TF they have:
* tf.nn.convolution
* tf.nn.conv2d
* tf.layers.conv2d (func)
* tf.layers.Conv2D (class)
* tf.contrib.layers.conv2d
* tf.keras.layers.Conv2D

Why using one, when you can have six of them !!!",look wait class one six,issue,negative,neutral,neutral,neutral,neutral,neutral
384631290,"@zsdonghao I was just wondering if it was a volontary move. Because it completely breaks the backward compatibbility of all convnets.

If you think this should not be solved, you can close the issue.",wondering move completely backward think close issue,issue,negative,positive,neutral,neutral,positive,positive
384629556,"it is caused by using `tf.layers` .. 

the easy way to do it is to save the network to TL `.npz`, and restore it in new version and save to TF `.ckpt` again ...",easy way save network restore new version save,issue,positive,positive,positive,positive,positive,positive
384348675,"for mac, to visualize dot file: `brew install graphviz` ",mac visualize dot file brew install,issue,negative,neutral,neutral,neutral,neutral,neutral
384214333,"Is it something that we should address ? It basically deprecated all the training I have ever did. And the work of everyone who used Convolutions...

I can relaunch my work. Maybe not everyone is willing to do so...",something address basically training ever work everyone used relaunch work maybe everyone willing,issue,negative,positive,positive,positive,positive,positive
384212255,"I just took a quick look, this is caused by
https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1547
it will be triggered when 
```
        if tf.__version__ > '1.5':
```
https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/convolution.py#L1541

",took quick look triggered,issue,negative,positive,positive,positive,positive,positive
384194855,"Can you implement new unittests for the layer `SeparableConv1d` ? Or at least modify an existing test by adding one `SeparableConv1d` layer inside the network ;)

Thanks",implement new layer least modify test one layer inside network thanks,issue,negative,positive,neutral,neutral,positive,positive
383871176,What would you need to release to this feature @lgarithm? ,would need release feature,issue,negative,neutral,neutral,neutral,neutral,neutral
383585709,I was going to add the **linked list** https://github.com/tensorlayer/tensorlayer/pull/469/files#diff-1a0995d02752855445936bb440663aceR401,going add linked list,issue,negative,neutral,neutral,neutral,neutral,neutral
383582776," You don't need to install abc @zsdonghao, it's a core module of python ",need install core module python,issue,negative,neutral,neutral,neutral,neutral,neutral
383498852,"I think we can used a linked list in Layer Base class. Each layer has a direct connection to the previous one (Layer) or ones (list of Layers: `ConcatLayer` & `MergeLayer`).
Usually a network does not have thousands of thousands of Layers. We should be fine about the time & space complexity.

The advantage is that we don't need to maintain a list or dictionnary of layers ;) (This behaviour can rapidly be unstrusful). And we can add some additional check, everytime a layer is accessed, we can check if the layer still exists in the TF Graph, else we raise a RuntimeException.

Both features (linked_list of Layers & checking if the layer still exists in the TF Graph) can easily be implemented in the Base Layer Class without modifying the whole library.",think used linked list layer base class layer direct connection previous one layer list usually network fine time space complexity advantage need maintain list behaviour rapidly add additional check layer check layer still graph else raise layer still graph easily base layer class without whole library,issue,positive,negative,negative,negative,negative,negative
383420393,"@zsdonghao abc is one of the core standard module from python 😁
No new dependency is required, its the standard way of doing abstract class and method in python ",one core standard module python new dependency standard way abstract class method python,issue,negative,positive,neutral,neutral,positive,positive
383418905,"Damned this doesn't work with Python 2.7...
I need to fix this ;)",damned work python need fix,issue,negative,neutral,neutral,neutral,neutral,neutral
383418119,"hi, where is the `abc` from? it is a new dependence? we may need to add it into `requirement.txt`",hi new dependence may need add,issue,negative,positive,positive,positive,positive,positive
383352808,"I just tested it for two iterations, it took too much time. 
Epoch 1 of 500 took 301.238368s
   train loss: 0.588028
   train acc: 0.878385
   val loss: 0.552224
   val acc: 0.891827
Epoch 2 of 500 took 302.669358s
   train loss: 0.284668
   train acc: 0.935697
   val loss: 0.257953
   val acc: 0.944311
",tested two took much time epoch took train loss train loss epoch took train loss train loss,issue,negative,positive,positive,positive,positive,positive
383310217,@DEKHTIARJonathan I think we can use TF name to check whether the layer is unique ? ~ Let me think~.,think use name check whether layer unique let,issue,negative,positive,positive,positive,positive,positive
383305653,"Alternatively, we can compute the **BFLOPs** of each layer, and the whole network, like darknet:
```
layer     filters    size              input                output
    0 conv     32  3 x 3 / 1   416 x 416 x   3   ->   416 x 416 x  32  0.299 BFLOPs
    1 max          2 x 2 / 2   416 x 416 x  32   ->   208 x 208 x  32
    2 conv     64  3 x 3 / 1   208 x 208 x  32   ->   208 x 208 x  64  1.595 BFLOPs
    3 max          2 x 2 / 2   208 x 208 x  64   ->   104 x 104 x  64
    4 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128  1.595 BFLOPs
    5 conv     64  1 x 1 / 1   104 x 104 x 128   ->   104 x 104 x  64  0.177 BFLOPs
    6 conv    128  3 x 3 / 1   104 x 104 x  64   ->   104 x 104 x 128  1.595 BFLOPs
    7 max          2 x 2 / 2   104 x 104 x 128   ->    52 x  52 x 128
    8 conv    256  3 x 3 / 1    52 x  52 x 128   ->    52 x  52 x 256  1.595 BFLOPs
    9 conv    128  1 x 1 / 1    52 x  52 x 256   ->    52 x  52 x 128  0.177 BFLOPs
...
```",alternatively compute layer whole network like layer size input output,issue,negative,positive,positive,positive,positive,positive
383304220,"You have two ways.

First, you create a list of layers and you loop through it.

However, names in TensorFlow shall be unique, so I would say that dictionary is a much more efficient approach.

One down side doing this, @zsdonghao we deprecated the function that managed tl layers name. Shall we bring it back to life? Because we need to make sure that the graph has not been reset since the layer has been created. 

If I have the time, I will have a look to Pytorch and look how they implemented this  ",two way first create list loop however shall unique would say dictionary much efficient approach one side function name shall bring back life need make sure graph reset since layer time look look,issue,positive,positive,positive,positive,positive,positive
383303566,Could easily be done within the base class Layer. ,could easily done within base class layer,issue,negative,negative,negative,negative,negative,negative
383303453,"@BaptisteAmato yes.. that is a problem .. 

What if we store all previous layer object here? https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/core.py#L408",yes problem store previous layer object,issue,negative,negative,negative,negative,negative,negative
383303373,"@zsdonghao I totally agree. However current code seems kind of weird to me: when using integer indexes, a network is returned like this:
```
net_new = Layer(prev_layer=None, name=self.name)
net_new.inputs = self.inputs
net_new.outputs = self.outputs[key]
net_new.all_layers = list(self.all_layers[:-1])
net_new.all_layers.append(net_new.outputs)
net_new.all_params = list(self.all_params)
net_new.all_drop = dict(self.all_drop)
return net_new
```
It seems that it returns the right outputs but the other parameters are from the entire network...",totally agree however current code kind weird integer network returned like layer key list list return right entire network,issue,positive,positive,neutral,neutral,positive,positive
383303309,"How would you implement this?
Before returning the Tensor you feed it into an InputLayer?
Because I'm not sure if there is anyway to find the previously created Layer object",would implement tensor feed sure anyway find previously layer object,issue,negative,positive,positive,positive,positive,positive
383303150,"@BaptisteAmato @DEKHTIARJonathan hi, for building a network, I think return a network could be better, otherwise, users have to feed the tensor into an `InputLayer` manually.

For example, the current layer support slicing as follow

```python
>>> x = tf.placeholder(""float32"", [None, 100])
>>> n = tl.layers.InputLayer(x, name='in')
>>> n = tl.layers.DenseLayer(n, 80, name='d1')
>>> n = tl.layers.DenseLayer(n, 80, name='d2')
>>> print(n)
... Last layer is: DenseLayer (d2) [None, 80]
```

```python
>>> n2 = n[:, :30]
>>> print(n2)
... Last layer is: Layer (d2) [None, 30]
```

The code is here:
https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers/core.py#L477",hi building network think return network could better otherwise feed tensor manually example current layer support slicing follow python float none print last layer none python print last layer layer none code,issue,positive,positive,positive,positive,positive,positive
383301542,@zsdonghao I let you answer where you would like this contribution to be driven. ,let answer would like contribution driven,issue,negative,neutral,neutral,neutral,neutral,neutral
383299131,"It seems that in the reference [tl.layers.get_layers_with_name](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#get-layers-with-name), a Tensor is returned and not a Layer. Should ""net['dense1']"" return a Tensor? In that case, there could not be an output attribute ""net['dense1'].outputs"".",reference tensor returned layer net return tensor case could output attribute net,issue,negative,neutral,neutral,neutral,neutral,neutral
383198563,This PR becomes too clumsy. I close it and re-open one,becomes clumsy close one,issue,negative,negative,negative,negative,negative,negative
383179930,"After having a graph, would we compute the total number of multiplication and add operation in a network? I found it is useful for network design.",graph would compute total number multiplication add operation network found useful network design,issue,negative,positive,positive,positive,positive,positive
383134125,"@wagamamaz  masked conv will accelerate the initialization, but masked conv need to be done by someone. pong友 这个主要是overlap那部分比keras强了一些，没法按照那个去做了。这样做应该更省内存？如果没有masked conv的话. Anyway, too lazy, can not code.",masked accelerate masked need done someone anyway lazy code,issue,negative,negative,negative,negative,negative,negative
383123315,"We use to have this problem on tensorflow 1.6, but fixed after switching to 1.5.",use problem fixed switching,issue,negative,positive,neutral,neutral,positive,positive
383110455,"@Windaway Thanks, it is great to have more layers. 

For the speed,  I have an idea, is it possible to use other method to initialize the model without using for loop?

By the way, you can change `Layer.__init__(self, name=name)` to `Layer.__init__(self, layer , name=name)` and remove:

```
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
```",thanks great speed idea possible use method initialize model without loop way change self self layer remove list list,issue,positive,positive,positive,positive,positive,positive
383104037,I'll check this issue. I'm totally new to this repo so it'll help me dive into the code.,check issue totally new help dive code,issue,negative,positive,positive,positive,positive,positive
383084575,"@wagamamaz  the performance problem of  this implement lies in the fact that many operations are established while the graph is initializing. Maybe eager mode will be better? The compute performance will not degrade much while training(compare to kears implement).
Also this implement cause the computing operations very falt(matrix), if we use winograd, the optimization will be very difficult(for who wants to implement this idea on cpu).
Last but not least, the implement on lower layer may not be a good idea? I may give a locally connected deconv layer implement someday for dcgan?",performance problem implement fact many established graph maybe eager mode better compute performance degrade much training compare implement also implement cause matrix use optimization difficult implement idea last least implement lower layer may good idea may give locally connected layer implement someday,issue,negative,positive,positive,positive,positive,positive
383067647,"@zsdonghao the problem is not about doing it manually. I would have done it. Yapf enforce this code style, if you change smthg the test for Yapf will fail. ",problem manually would done enforce code style change test fail,issue,negative,negative,negative,negative,negative,negative
383062371,"hi @DEKHTIARJonathan , there still many code can be shorten 😭, I will do it when I am free",hi still many code shorten free,issue,positive,positive,positive,positive,positive,positive
383061183,@zsdonghao I did my best to fine tune YAPF. What do you think about the formating ?,best fine tune think,issue,positive,positive,positive,positive,positive,positive
383053167,"Here is some code to help on the implementation, the ""Layer"" class needs to be made **subscriptable**.

Here is a simple example on how to do this:

```python
class SuperHero(object):
    def __init__(self):
        # My code ===> I have the power to decide who is the strongest :D 
        self.heros_strength = {
            ""SuperMan"": 666,
            ""Batman"": 650,
            ""Hulk"": 6e10,
            ""Prof. Xavier"": 10
        }

    def __getitem__(self, item):
        try:
            return self.heros_strength[item]
        except KeyError:
            raise ValueError(""The SuperHero `%s` is not cool enough to be known !"" % item)


myHero = SuperHero()
print(""Superman strength: %d"" % myHero[""SuperMan""])
print(""Hulk strength: %d"" % myHero[""Hulk""])

print(""SpiderMan strength: %d"" % myHero[""SpiderMan""])
```

@BaptisteAmato would you be interested in having a look at this ? It would a very good quickstart.
If you have any question, myself and @zsdonghao would be glad to help you if you decide to gve it a shot.",code help implementation layer class need made simple example python class superhero object self code power decide superman batman hulk self item try return item except raise superhero cool enough known item superhero print superman strength superman print hulk strength hulk print strength would interested look would good question would glad help decide shot,issue,positive,positive,positive,positive,positive,positive
383050020,"@DEKHTIARJonathan I prefer to use API 1. Thanks, I added `help wanted` label ~",prefer use thanks added help label,issue,positive,positive,positive,positive,positive,positive
383034045,"@zsdonghao which API would you prefer using ?

```python
import tensorflow as tf
import tensorlayer as tl

x = tf.placeholder(tf.float32, [None, 300])
net = InputLayer(x)
net = DenseLayer(net, name='dense1')
net = DenseLayer(net, name='dense2')

## API 1:
layer = net['dense1']

## API 2:
layer = net.tops['dense1']
```
",would prefer python import import none net net net net net layer net layer,issue,negative,neutral,neutral,neutral,neutral,neutral
382980851,"Very easy to implement, I have a few students who might want to work on this (@BaptisteAmato interested ?)

Would you might not doing it now and add contribution wanted to the title of the issue? ",easy implement might want work interested would might add contribution title issue,issue,positive,positive,positive,positive,positive,positive
382869252," @zsdonghao I can't manage to reproduce a fixed behavior with Conv1dLayer on the master branch. Are you sure ?

I edited my post for the AverageEmbeddingLayer and marked it as solved",ca manage reproduce fixed behavior master branch sure post marked,issue,negative,positive,positive,positive,positive,positive
382779262,"@zsdonghao I used yapf auto-formating tool to produce the changes.
I will try to clean the code ASAP ;)",used tool produce try clean code,issue,negative,positive,positive,positive,positive,positive
382768783,"@zsdonghao I'm afraid that most changes you pointed out are enforced by YAPF script ...
I'll try to find how to fix this ;)",afraid pointed enforced script try find fix,issue,negative,negative,negative,negative,negative,negative
382649035,"I would prefer using a unittest if it is possible. The best thing would have been to use a CI but rtfd do not provide one yet.

This could change in a near future:  https://github.com/rtfd/readthedocs.org/issues/1340",would prefer possible best thing would use provide one yet could change near future,issue,positive,positive,positive,positive,positive,positive
382396795,"@lgarithm hi, thank you for your great contribution. I just open a PR to make all pooling layer from function to class. This can help all layers to save it's own layer type, which is import for building the network from the graph file.~",hi thank great contribution open make layer function class help save layer type import building network graph,issue,positive,positive,positive,positive,positive,positive
382382563,"I originally think about running the following command in travis ci 
```
cd docs
pip install -r requirements.txt
make html
```
as a simple test. But didn't do it, because tests were small, and I didn't want to increase test time.
Now I think it's reasonable to add a test for docs, since user experience is becoming more and more important.

The above snippet looks good, and more formal, I'm OK with it, too.
",originally think running following command travis pip install make simple test small want increase test time think reasonable add test since user experience becoming important snippet good formal,issue,positive,positive,positive,positive,positive,positive
382086171,"oh, this one can be ignored .. I mixed some codes together, my mistake.",oh one mixed together mistake,issue,negative,neutral,neutral,neutral,neutral,neutral
382057516,"@zsdonghao I don't really understand what you tried to achieve here ...
Is it just some cosmetic changes ? Because nothing else is changed.",really understand tried achieve cosmetic nothing else,issue,negative,positive,positive,positive,positive,positive
382047112,"if batch size > 1 work, and you set batch size to 1, it should not have OOM problem.",batch size work set batch size problem,issue,negative,neutral,neutral,neutral,neutral,neutral
382046693,"@Windaway Hi, I tried your implementation but it is a little bit slow, do you have plan to improve it?",hi tried implementation little bit slow plan improve,issue,negative,negative,negative,negative,negative,negative
382045763,"@zsdonghao if you prefer I can vendor the package. However, this will prevent us from getting the package updates.

As @lgarithm pointed out it is a small lib which does one thing, and do it very well. It could also be used for tl.train API",prefer vendor package however prevent u getting package pointed small one thing well could also used,issue,negative,negative,negative,negative,negative,negative
382039291,"This library is small, shouldn’t matter.

________________________________
From: Hao <notifications@github.com>
Sent: Tuesday, April 17, 2018 11:35:29 PM
To: tensorlayer/tensorlayer
Cc: LG; Mention
Subject: Re: [tensorlayer/tensorlayer] ProgressBar added for dataset download. Now stops bombarding console (#502)


it that good to include a library for this task only? @lgarithm<https://github.com/lgarithm> what do you think?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/tensorlayer/tensorlayer/pull/502#issuecomment-382037460>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AB8ydImEFbY0GvpwIhTW0adPNOwDDTYkks5tpgvBgaJpZM4TYgMA>.
",library small matter hao sent mention subject added console good include library task think reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
382037460,it that good to include a library for this task only? @lgarithm what do you think?,good include library task think,issue,negative,positive,positive,positive,positive,positive
381973084,@zsdonghao if the work is finished you can merge in **master**,work finished merge master,issue,negative,neutral,neutral,neutral,neutral,neutral
381963104,"@zsdonghao thanks for your corrections, only one small issue left. Once it's corrected I'll merge the PR ;)",thanks one small issue left corrected merge,issue,negative,negative,neutral,neutral,negative,negative
381954482,"### tl.files.load_mpii_pose_dataset [SOLVED]

img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()

```
/Users/haodong/..../tensorlayer/files.py:1401: FutureWarning: elementwise != comparison failed and returning scalar instead; this will raise an error or perform elementwise comparison in the future.
  if annopoint != []:
```",comparison scalar instead raise error perform comparison future,issue,negative,neutral,neutral,neutral,neutral,neutral
381592250,"I run it on nvidia 1080ti with 11G memory. I change the code a little to realize another task. Although my input image size is 700*700, it works when I use 'BN' in the generator and will OOM using instancenorm. I set batchsize=1. ",run ti memory change code little realize another task although input image size work use generator set,issue,negative,negative,negative,negative,negative,negative
381100352,"hi, @DEKHTIARJonathan  we have a private channel in slack , and we just invite you",hi private channel slack invite,issue,negative,neutral,neutral,neutral,neutral,neutral
381095990,"@zsdonghao thanks a lot, I was already on slack. It is pretty inactive, is there any private channel where you discuss with the development team? ",thanks lot already slack pretty inactive private channel discus development team,issue,positive,positive,positive,positive,positive,positive
381094549,"@wagamamaz a friendly ping, in case you are also interested in joining the Slack channel.",friendly ping case also interested joining slack channel,issue,positive,positive,positive,positive,positive,positive
381077843,Cool! The solution looks pretty elegant! Nice effort. :),cool solution pretty elegant nice effort,issue,positive,positive,positive,positive,positive,positive
381001870,"Ha, I figure out my problem. I think my guess is partially right and it is not an issue related to TL: normally, Python stdout will only flush by the end of the program or something special event occurs like input (or stream buffer is full I guess). So, I run the srgan train, then it will take long time to finish, and then it creates an illusion that it doesn't work. So using: print('Hello', flush=True) or sys.stdout.flush() after print will make print statement instantly appears in the log file. 

So, the conclusion is, it has nothing to do with TL. I'll close it.",ha figure problem think guess partially right issue related normally python flush end program something special event like input stream buffer full guess run train take long time finish illusion work print print make print statement instantly log file conclusion nothing close,issue,positive,positive,positive,positive,positive,positive
380961191,"With the commit 8ea85df I have finished implementing this idea.

You can now run this code seemlessly:

```python
import tensorflow as tf
import tensorlayer as tl

x = tf.placeholder(""float32"", [None, 100])

input = tl.layers.InputLayer(x, name='in')

out1 = tl.layers.DenseLayer(input, 80, name='h1')
out2 = tl.layers.DenseLayer(prev_layer=input, n_units=80, name='h2')
out3 = tl.layers.DenseLayer(layer=input, n_units=80, name='h3')
```

Gives the following output:
```
[TL] InputLayer  in: (?, 100)
[TL] DenseLayer  h1: 80 identity
[TL] DenseLayer  h2: 80 identity
[TL] DeprecationWarning: DenseLayer.__init__(): `layer` argument is deprecated and will be removed in version 1.9, please change for `prev_layer.`
[TL] DenseLayer  h3: 80 identity
```

The new solution should address every situations ;)",commit finished idea run code python import import float none input input following output identity identity layer argument removed version please change identity new solution address every,issue,positive,positive,neutral,neutral,positive,positive
380924591,"I have implemented a solution based on this simple idea, I am currently updating all my modifications.

I implemented a custom decorator that handle renaming for both functions and methods.
This has the advantages of just providing an alias to arguments, not adding any argument and not changing the order.

```python
import sys
import functools
import warnings
import logging


def deprecated_alias(end_support_version, **aliases):
    def deco(f):

        @functools.wraps(f)
        def wrapper(*args, **kwargs):

            try:
                func_name = ""{}.{}"".format(args[0].__class__.__name__, f.__name__)
            except (NameError, IndexError):
                func_name = f.__name__

            rename_kwargs(
                kwargs,
                aliases,
                end_support_version,
                func_name
            )

            return f(*args, **kwargs)

        return wrapper

    return deco


def rename_kwargs(kwargs, aliases, end_support_version, func_name):

    for alias, new in aliases.items():

        if alias in kwargs:

            if new in kwargs:
                raise TypeError('{}() received both {} and {}'.format(func_name, alias, new))

            warnings.warn('{}() - {} is deprecated; use {}'.format(func_name, alias, new), DeprecationWarning)
            logging.warning(""DeprecationWarning: {}(): ""
                            ""`{}` argument is deprecated and will be removed in version {}, ""
                            ""please change for `{}.`"".format(func_name, alias, end_support_version, new))
            kwargs[new] = kwargs.pop(alias)
            

class MyClass(object):

    @deprecated_alias(object_id='id_object', end_support_version=1.6)
    def __init__(self, id_object):
        self.id = id_object
        
        print(""I am object: %d"" % self.id)
        sys.stdout.flush()

@deprecated_alias(name='username', end_support_version=1.6)        
def yolo(username):
    print(""YOLLLOOOOOOO %s"" % username)    
    sys.stdout.flush()

if __name__ == ""__main__"":

    object1 = MyClass(id_object=1234) ## Correct call with new argument name
    object2 = MyClass(object_id=1234) ## DeprecationWarning: call with old argument name
    
    print()
    
    yolo(username=""John"") ## Correct call with new argument name
    yolo(name=""John"")     ## DeprecationWarning: call with old argument name
```
Launch with the script: `python main.py`

**Console Output:**
```
I am object: 1234
WARNING:root:DeprecationWarning: MyClass.__init__(): `object_id` argument is deprecated and will be removed in version 1.6, please change for `id_object.`
I am object: 1234

YOLLLOOOOOOO John
WARNING:root:DeprecationWarning: yolo(): `name` argument is deprecated and will be removed in version 1.6, please change for `username.`
YOLLLOOOOOOO John
```

### What are the benefits of this PR

In addition to handling the deprecation warning in a proper manner (thanks to your feedback), I have implemented a decorator that will ease a lot future deprecations and can be fixed in a minute.

I hope this solution will satisfy your requirements. 

@zsdonghao && @luomai: Do you have any communication channel that I can join ? I would love to join the TL community and help you on the development.",solution based simple idea currently custom decorator handle providing alias argument order python import import import import logging wrapper try except return return wrapper return alias new alias new raise received alias new use alias new argument removed version please change alias new new alias class object self print object print object correct call new argument name object call old argument name print correct call new argument name call old argument name launch script python console output object warning root argument removed version please change object warning root name argument removed version please change addition handling deprecation warning proper manner thanks feedback decorator ease lot future fixed minute hope solution satisfy communication channel join would love join community help development,issue,positive,positive,positive,positive,positive,positive
380882412,"Hi @luomai thanks a lot for your feedback, I really appreciate it.

I completely miss this fact... I think I can fix this point. I try to submit a new commit tonight or tomorrow solving this. Would this be fine to you ?

It may not require a lot of work to make this PR compatible with function/class calls without named arguments.",hi thanks lot feedback really appreciate completely miss fact think fix point try submit new commit tonight tomorrow would fine may require lot work make compatible without,issue,positive,positive,positive,positive,positive,positive
380867356,"Hi @DEKHTIARJonathan we have extensively discussed the PR and tested it with the TL users at Imperial College London. We find that this PR could break some of their programs if they are using position-based argument to specify the input layer instead of the key-based. Given this, we considering merging your PR into a `back-support` branch and let the specific users to install it by themselves.

If this PR contains other commits for the deprecation issues 2,3,4 in #479, would you mind submitting them in separated PRs?

Again, we are all appreciated of your time in looking into this and coming up with this PR. We will take the reported issue as a lesson and would try best to avoid it in the future. Thanks for your contribution again! :)

cc: @zsdonghao @lgarithm ",hi extensively tested imperial college find could break argument specify input layer instead given considering branch let specific install deprecation would mind time looking coming take issue lesson would try best avoid future thanks contribution,issue,positive,positive,positive,positive,positive,positive
380866414,"But in srgan code, the print statement won't appear in log.txt file. That essentially what my problem is.",code print statement wo appear file essentially problem,issue,negative,neutral,neutral,neutral,neutral,neutral
380865253,"Come on ... I told you  3 times **This Is Not Related To TL**. I can't be more clear !

This is basic python. Please go to  https://stackoverflow.com/

As you don't want to listen, **I prove it and you close this issue. Please stop this!**

### Experimentation

**main.py:**
```python
import logging

logging.debug(""I am a debug print"")
logging.info(""I am an info print"")
logging.warning(""I am a warning print"")
logging.error(""I am an error print"")
logging.critical(""I am a critical print"")

print(""I am a classical print"")
```

Now you launch:

```shell
python main.py > log.txt
```

### Results

**Console:**
```
WARNING:root:I am a warning print
ERROR:root:I am an error print
CRITICAL:root:I am a critical print
```

**log.txt file:**
```
I am a classical print
```


------------------------

We explain something you don't listen (read). This is NOT TL related => https://stackoverflow.com/
",come told time related ca clear basic python please go want listen prove close issue please stop experimentation python import logging print print warning print error print critical print print classical print launch shell python console warning root warning print error root error print critical root critical print file classical print explain something listen read related,issue,negative,positive,neutral,neutral,positive,positive
380860338,"In srgan code, I have manually shut down logging library by logging.disable(logging.CRITICAL+1), and the only things appear in the terminal is from print statements. So I guess pure print and logging won't affect each other?

Also, just test the code:
`import logging
logging.disable(logging.CRITICAL+1)
print('hello')`

and I can redirect the print to a file using python test.py >temp.txt

So, disable logging shouldn't have side effect on print statement?",code manually shut logging library appear terminal print guess pure print logging wo affect also test code import logging print redirect print file python disable logging side effect print statement,issue,negative,positive,positive,positive,positive,positive
380844680,"No problem, if you need additional as I said please go to https://stackoverflow.com/. 
TL use logging library to display many informations which may be hidden due to a different logging levels.

The best would be to be able to create a simple scripts with minimal code and ask help about it on StackOverflow.

Thanks for your understanding",problem need additional said please go use logging library display many may hidden due different logging best would able create simple minimal code ask help thanks understanding,issue,positive,positive,positive,positive,positive,positive
380838768,"It's my bad not clarifying that I'm using Windows sorry (I thought cmd could imply that). But I don't mean accusing anyone, I just want to state my opinion about the potential problems and very likely I'm wrong. (And my first language is not English, so probably I don't use words properly, also I admit I'm a newbie in Python) 

Back to my question, the thing is that, when I write a simple print('Hello') test.py, I open my cmd, then say 'python test.py >temp.txt', I will get a temp.txt and observe 'Hello' is in the temp.txt. However, when I do the same thing using TL, more specifically, using srgan code (though I think srgan code has nothing to do with I/O stream stuff), the program will be seem running, but nothing appears in the temp.txt. 

Actually I have an idea of manually flushing streams after every print, and I'll test it tomorrow.",bad sorry thought could imply mean anyone want state opinion potential likely wrong first language probably use properly also admit python back question thing write simple print open say get observe however thing specifically code though think code nothing stream stuff program seem running nothing actually idea manually flushing every print test tomorrow,issue,negative,negative,negative,negative,negative,negative
380734598,"Let me guess, you are using Windows ? Did you realise that you accuse people and a library when you don't even take the time to read and understand what you are reading.  These commands only apply to Linux-based systems

Before saying that something doesn't work and accuse anyone, you should definetely ask yourself what you did wrong and what are the differences between what you did and what others did ...

Consequently, it is not TL related and thus this is not the place to answer your question. Please close this issue and go to https://stackoverflow.com/",let guess accuse people library even take time read understand reading apply saying something work accuse anyone ask wrong consequently related thus place answer question please close issue go,issue,negative,negative,negative,negative,negative,negative
380657999,"Actually I tried most of them, and don't work. I think the root cause is somewhere in the tensorlayer, kind of modifying the default behavior of stream stuff, but I can't figure out how to fix that.",actually tried work think root cause somewhere kind default behavior stream stuff ca figure fix,issue,positive,positive,positive,positive,positive,positive
380385280,I think where you can achieve better results easier is in post-processing. At least in tf-pose-estimation that's the current bottleneck. maybe implementing a c++/cython multiprocessing module for that can be enough,think achieve better easier least current bottleneck maybe module enough,issue,positive,positive,neutral,neutral,positive,positive
380374043,"You could at least take the time to say hello and give details...

Just launch this: `pip install tensorlayer`, you may need admin rights (sudo in linux)

Please close this issue",could least take time say hello give launch pip install may need please close issue,issue,negative,negative,negative,negative,negative,negative
380372791,"Try the following:

```shell
python main.py --mode=srgan &> temp.txt
```

One of these could work ;)


As highlighted here: https://askubuntu.com/questions/420981/how-do-i-save-terminal-output-to-a-file

              || visible in terminal ||   visible in file   || existing
      Syntax  ||  StdOut  |  StdErr  ||  StdOut  |  StdErr  ||   file   
    ==========++==========+==========++==========+==========++===========
        >     ||    no    |   yes    ||   yes    |    no    || overwrite
        >>    ||    no    |   yes    ||   yes    |    no    ||  append
              ||          |          ||          |          ||
       2>     ||   yes    |    no    ||    no    |   yes    || overwrite
       2>>    ||   yes    |    no    ||    no    |   yes    ||  append
              ||          |          ||          |          ||
       &>     ||    no    |    no    ||   yes    |   yes    || overwrite
       &>>    ||    no    |    no    ||   yes    |   yes    ||  append
              ||          |          ||          |          ||
     | tee    ||   yes    |   yes    ||   yes    |    no    || overwrite
     | tee -a ||   yes    |   yes    ||   yes    |    no    ||  append
              ||          |          ||          |          ||
     n.e. (*) ||   yes    |   yes    ||    no    |   yes    || overwrite
     n.e. (*) ||   yes    |   yes    ||    no    |   yes    ||  append
              ||          |          ||          |          ||
    |& tee    ||   yes    |   yes    ||   yes    |   yes    || overwrite
    |& tee -a ||   yes    |   yes    ||   yes    |   yes    ||  append",try following shell python one could work visible terminal visible file syntax file yes yes overwrite yes yes append yes yes overwrite yes yes append yes yes overwrite yes yes append tee yes yes yes overwrite tee yes yes yes append yes yes yes overwrite yes yes yes append tee yes yes yes yes overwrite tee yes yes yes yes append,issue,positive,neutral,neutral,neutral,neutral,neutral
380252329,"## Any ideas to make it run faster?

### Algorithm

|             	| OpenPose      	| Idea      	| 
|-------------	|---------------	|---------------	|
| image size 	| 368x654 i.e. 9:16 	| 244x244 ? 	| 
| CNN  	| VGG-19 	| mobilenetV2 or others, see https://github.com/tensorlayer/tensorlayer/issues/416 	| 
| CNN  | Residual Squeeze | [(1)](http://cs231n.stanford.edu/reports/2016/pdfs/410_Report.pdf),   [(2)](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301729)|

### Engineering
- Faster pre-processing : ❓
- Faster post-processing : 
  - @filipetrocadoferreira c++/cython multiprocessing module
  - pypy vs cython
- TensorRT",make run faster algorithm idea image size see residual squeeze engineering faster faster module,issue,negative,neutral,neutral,neutral,neutral,neutral
380207401,I release example code for using TensorFlow Dataset API to preprocess VOC data [here](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_tf_dataset_voc.py),release example code data,issue,negative,neutral,neutral,neutral,neutral,neutral
380204128,"I release the APIs to download or visualise MPII dataset in 1 line of code here: https://github.com/tensorlayer/tensorlayer/pull/482

```python
import pprint
import tensorlayer as tl

img_train_list, ann_train_list, img_test_list, ann_test_list = tl.files.load_mpii_pose_dataset()

print(img_train_list[0])
pprint.pprint(ann_train_list[0])

for i in range(100):  # show 100 images
    image = tl.vis.read_image(img_train_list[i])
    tl.vis.draw_mpii_people_to_image(image, ann_train_list[i], '_temp%d.png' % i)

```",release line code python import import print range show image image,issue,negative,neutral,neutral,neutral,neutral,neutral
380136252,"I would say it depends on your coding practice, I tend to precise all of the arguments. Much easier to read a while later and I am sure to be robust to any unwanted API change and detect it.

Information: I finally fixed my PR. Now **layer** and **prev_layer** are both working. **layer** argument send a deprecated warning",would say practice tend precise much easier read later sure robust unwanted change detect information finally fixed layer working layer argument send warning,issue,negative,positive,positive,positive,positive,positive
380108814,"I think the first argument of `Layer` should be passed as positional argument instead of keyword argument (we should fix our code to follow that convention too), in that case, the name of the first argument (`layer` or `prev_layer`) becomes an internal concept, and it shouldn't appear at the caller.
",think first argument layer positional argument instead argument fix code follow convention case name first argument layer becomes internal concept appear caller,issue,negative,positive,positive,positive,positive,positive
380097058,"@zsdonghao @luomai thanks a lot for your feedback, I am finishing this PR and fixing the errors with the tests. It will be easier to fix the library whatever decision you take afterward.

In anyway, I think it is of good practice to not ""just removed"" something, and first put a deprecation warning. My complaint wasn't about ""why did you do it"" but more about the way it is conducted. However, I must admit I don't really understand why it was changed and would be fine with a rollback on the name of this argument.",thanks lot feedback finishing fixing easier fix library whatever decision take afterward anyway think good practice removed something first put deprecation warning complaint way however must admit really understand would fine rollback name argument,issue,positive,positive,positive,positive,positive,positive
380093948,"@DEKHTIARJonathan Thanks for the PR. Really appreciate it.
I am discussing with @zsdonghao  and @lgarithm to figure out the best way to resolve the `layer` vs `prev_layer`. The other deprecation fixes look good to us. 

In the long term, providing these two key-based input parameters would confuse users. We prefer keeping either one of them. We are open for switching back to the `layer`. However, we need to discuss with other TL users to see if they have switched to `prev_layer` already, and they may complain why we change back.

This change was included TL 1.8 which is a maintenance release. TL 1.8 resolves many technical debts and we expected some of the API might breaks. We are very sorry about for your experience. At that time, we are also conservative about the release and 1.8 gone through a pre-release stage for public feedbacks.

Thanks again for your contribution.",thanks really appreciate figure best way resolve layer deprecation look good u long term providing two input would confuse prefer keeping either one open switching back layer however need discus see switched already may complain change back change included maintenance release many technical might sorry experience time also conservative release gone stage public thanks contribution,issue,positive,positive,positive,positive,positive,positive
380091925,"@DEKHTIARJonathan wait a moment, let me discuss with @lgarithm and @luomai first ..",wait moment let discus first,issue,negative,positive,positive,positive,positive,positive
380088409,"@zsdonghao : damned, I just spent two hours correcting this :(

I do my best to re-modify all of this and then update this PR",damned spent two correcting best update,issue,negative,positive,positive,positive,positive,positive
380083394,"hi, i prefer to change all `pre_layer` back to `layer`.",hi prefer change back layer,issue,negative,neutral,neutral,neutral,neutral,neutral
379543469,"hi, could you create a push request for it?",hi could create push request,issue,negative,neutral,neutral,neutral,neutral,neutral
379150114,"Hi, one of the simple way to customize you layer is to use `LambdaLayer`, alternatively, you can write a new layer like `PReluLayer: http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/layers/special_activation.html#PReluLayer",hi one simple way layer use alternatively write new layer like,issue,negative,positive,neutral,neutral,positive,positive
379138319,"you are right, it loads different network, if you want to load the network you just trained. 
you can add `tl.files.load_and_assign_npz` in the training code.",right different network want load network trained add training code,issue,negative,positive,positive,positive,positive,positive
378824256,"Thank you for reporting this bug, I will fixed it in this PR https://github.com/tensorlayer/tensorlayer/pull/467/commits/3825e6c8a6d8fff0a1bdee0bb6ca313696761a84 , so you can fix the bug by reinstalling TL in 10 mins.",thank bug fixed fix bug,issue,negative,positive,neutral,neutral,positive,positive
378116700,@lgarithm the latest commit changes the Conv2d from function to class.,latest commit function class,issue,negative,positive,positive,positive,positive,positive
377855932,"great

________________________________
From: Hao <notifications@github.com>
Sent: Monday, April 2, 2018 1:07:57 PM
To: tensorlayer/tensorlayer
Cc: LG; Mention
Subject: Re: [tensorlayer/tensorlayer] Models squeezenet (#461)


@lgarithm<https://github.com/lgarithm> this model can help to debug the TL graph

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/tensorlayer/tensorlayer/pull/461#issuecomment-377855826>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AB8ydBYei7_KRgPw3SRtdu09FZflWIxGks5tkbItgaJpZM4TDH-i>.
",great hao sent mention subject model help graph reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
377835620,"Dear all,

That is a really exciting discussion.
I've working on this for a while when I develop tensordb.

Keras has already implmented this for simple sequential network, they call it `Sequential`

My advise is that maybe we should follow caffe standard, 
overall, the data scheme should be three part:

- 1) the topology
- 2) node attribute
- 3) and the attribute them self.

Keep in mind we may need to support muli-input and multi-output, and weight sharing.
they are very valulabe for research purpose, which tl targets.

Another issue i want to point out is we need tl support  some iterators, for each layer, there is a interface to visit its input and outputs layer.

",dear really exciting discussion working develop already simple sequential network call sequential advise maybe follow standard overall data scheme three part topology node attribute attribute self keep mind may need support weight research purpose another issue want point need support layer interface visit input layer,issue,positive,positive,neutral,neutral,positive,positive
377834808,"We can save the model structure into TL graph, and the parameters into TL npz file.

- Variables only: TF .ckpt --> TL .npz   ([list or dict](http://tensorlayer.readthedocs.io/en/latest/modules/files.html#load-and-save-network))
- Structure only: TF .pb --> TL graph (something like .tlgraph ? )

The TensorLayer and TensorFlow version may need to stored into the graph file as well.",save model structure graph file list structure graph something like version may need graph file well,issue,positive,neutral,neutral,neutral,neutral,neutral
377796095,We might need to use [pickle](https://docs.python.org/3/library/pickle.html) if we want to restore the network from saved file.,might need use pickle want restore network saved file,issue,positive,neutral,neutral,neutral,neutral,neutral
377795572,what kind of data format will be used for TL graph?,kind data format used graph,issue,positive,positive,positive,positive,positive,positive
377795303,"https://github.com/tensorlayer/tensorlayer/issues/394

We can save the model structure into TL graph, and the parameters into TL npz file.

TF .ckpt --> TL .npz   
TF .pb --> TL graph

",save model structure graph file graph,issue,negative,neutral,neutral,neutral,neutral,neutral
377795131,"Hi, all `tl.files.load_xxx_dataset` APIs have an argument `path`, user can set they own path if they want 2 tensorlayer codes use the same data.

ref: http://tensorlayer.readthedocs.io/en/latest/modules/files.html#mnist",hi argument path user set path want use data ref,issue,negative,neutral,neutral,neutral,neutral,neutral
377793752,"We need all args of the class, otherwise, we can't fully store the behaviour of the layer, then we can't reconstruct the network from the graph.

For example:
- For `DenseLayer`, we need `act`, `n_units` and `b_init` (sometime it is `None`).
- For `Conv2d`, we need `act`, `n_filter`, `filter_size`, `strides`, `padding` and `b_init` (alternatively, we can get the `n_filter` and `filter_size` from the shape of `W_conv2d`, and get the `stride` by comparing the outputs shape the this layer and previous layer.)

ref: https://stackoverflow.com/questions/218616/getting-method-parameter-names-in-python

![49691522596388_ pic](https://user-images.githubusercontent.com/10713581/38174726-7ea4b88c-3604-11e8-8b97-deaa5343ca1c.jpg)
",need class otherwise ca fully store behaviour layer ca reconstruct network graph example need act sometime none need act padding alternatively get shape get stride shape layer previous layer ref pic,issue,negative,negative,negative,negative,negative,negative
376192536,"Hi, thanks for your notice.
This is a bug due to a recent update.. I am fixing it now, it will be done in few minutes.
Please use the master version~",hi thanks notice bug due recent update fixing done please use master,issue,positive,positive,neutral,neutral,positive,positive
376029951,another paper :  [Focal Loss for Dense Object Detection (RetinaNet)](https://arxiv.org/abs/1708.02002),another paper focal loss dense object detection,issue,negative,neutral,neutral,neutral,neutral,neutral
375997497,"### Other well-known models (TODO) 🌺

TensorLayer already have some implementations, just wrap them up!

- VGG19 : https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_vgg19.py
- SqueezeNet : https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_squeezenet.py
- MobileNet : https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mobilenet.py
- InceptionV3 using TF-Slim : https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_inceptionV3_tfslim.py
- Need to support ALL Keras and TF-Slim's pre-tained models.   
  - https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models
  - https://keras.io/applications/#applications

then provide examples in documentation along with the APIs, and `tutorial_models_xxx.py` in [here](https://github.com/tensorlayer/tensorlayer/tree/master/example)",already wrap need support provide documentation along,issue,negative,neutral,neutral,neutral,neutral,neutral
375980749,"# Usage examples for image classification models

## Classify ImageNet classes with VGG16
```python
x = tf.placeholder(tf.float32, [None, 224, 224, 3])

# get the whole model
vgg = tl.models.VGG16(x)

# restore pre-trained VGG parameters
vgg.restore_params(sess)

# use pre-trained VGG
softmax = tf.nn.softmax(vgg.outputs)
...
```

## Extract features with VGG16 and Train a classifier with 100 classes

```python
x = tf.placeholder(tf.float32, [None, 224, 224, 3])

# get VGG without the last layer
x = tf.placeholder(tf.float32, [None, 224, 224, 3])

# get VGG without the last layer
vgg = tl.models.VGG16(x, end_with='fc2_relu')

# add one more layer
net = tl.layers.DenseLayer(vgg, 100, name='out')

# initialize all parameters
sess = tf.InteractiveSession()
tl.layers.initialize_global_variables(sess)

# restore pre-trained VGG parameters
vgg.restore_params(sess)

# train your own classifier (only update the last layer)
train_params = tl.layers.get_variables_with_name('out')
…
```

## Reuse model

```python
x1 = tf.placeholder(tf.float32, [None, 224, 224, 3])

x2 = tf.placeholder(tf.float32, [None, 224, 224, 3])

# get VGG without the last layer
vgg1 = tl.models.VGG16(x1, end_with='fc2_relu')

# reuse parameters of vgg1, but use different input 
vgg2 = tl.models.VGG16(x2, end_with='fc2_relu', reuse=True)

# restore pre-trained VGG parameters (as they share parameters, don’t need to restore vgg2)
sess = tf.InteractiveSession()
vgg1.restore_params(sess)
```

ref: https://keras.io/applications/#applications",usage image classification class python none get whole model restore sess use extract train classifier class python none get without last layer none get without last layer add one layer net initialize sess sess restore sess train classifier update last layer reuse model python none none get without last layer reuse use different input restore share need restore sess sess ref,issue,positive,positive,neutral,neutral,positive,positive
375910718,"I think keeping the original code is better, we can have another .py file, because we have a link in the TL book, and some academic users may like to use the original code.",think keeping original code better another file link book academic may like use original code,issue,positive,positive,positive,positive,positive,positive
375443375,"### Release MobileNetV1 (**3 time faster than Keras**)
https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_mobilenet.py

- Note that, TensorLayer MobileNet is **3 time faster than Keras** (same weights and architecture).
- TensorLayer takes 0.001~0.002 second for one image on Titan XP.
- Keras takes 0.004~0.005 second for one image on Titan XP.

```python
import keras
keras_model = keras.applications.mobilenet.MobileNet(input_shape=None, alpha=1.0, depth_multiplier=1, dropout=1e-3, include_top=True, weights='imagenet', input_tensor=None, pooling=None, classes=1000)
prob = keras_model.predict(np.asarray([img]), batch_size=1)
```

### TODO MobileNetV2

- https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models
- https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/README.md",release time faster note time faster architecture second one image second one image python import prob,issue,negative,neutral,neutral,neutral,neutral,neutral
375372868,"@XJTUWYD Hi, i suggest you to add this comment into this issue https://github.com/tensorlayer/tensorlayer/issues/416, more people can see it.

and include your previous comment:

I add two compress strategies tenary weight network  and dorefa-net into tensorlayer,  did two experiments to compare the accuracy of different compress strategies based on mnist and cifar-10.
the result of the experiment is below:

|             	| BinaryNet      	| Tenary Weight      	| DoReFa-Net      	|
|-------------	|---------------	|---------------	|---------------	|
| MNIST 	| 98.86% 	| 99.27% 	| 98.89% 	|
| CIFAR10  	| 41.1% 	| 80.6% 	| 81.1% 	|",hi suggest add comment issue people see include previous comment add two compress weight network two compare accuracy different compress based result experiment weight,issue,negative,negative,neutral,neutral,negative,negative
375368724,"bnn is  a excellent work in the compression of neuron network but it can not get a satisfied accuracy on relative large datasets, in order to solve the problem, tenary weight networks and dorefa were put forward. I add 4 apis for tensorlayer, Tenary Denselayer, TenaryConv2d, DorefaDenselayer, and DorefaConv2d . I perform 6 experiment based on mnist and cifar10，the details are in thr titorials.
finally thank you very much for the help of HaoDong, LuoMai, and Igarithm.",excellent work compression neuron network get satisfied accuracy relative large order solve problem weight put forward add perform experiment based finally thank much help,issue,positive,positive,positive,positive,positive,positive
375353730,"## Release DoReFa-Net, BinaryNet, Ternary Weight Network 🚀

 - SqueezeNet (ImageNet). Classification task, see [tutorial_squeezenet.py](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_squeezenet.py)
 - BinaryNet. Model compression, see [mnist](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_binarynet_mnist_cnn.py) [cifar10](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_binarynet_cifar10_tfrecord.py).
 - Ternary Weight Network. Model compression, see [mnist](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_ternaryweight_mnist_cnn.py) [cifar10](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_tenaryweight_cifar10_tfrecord.py).
 - DoReFa-Net. Model compression, see [mnist](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_dorefanet_mnist_cnn.py) [cifar10](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_dorefanet_cifar10_tfrecord.py).
",release ternary weight network rocket classification task see model compression see ternary weight network model compression see model compression see,issue,negative,neutral,neutral,neutral,neutral,neutral
375307900,"Since our functions have long argument lists, and many of them have default values, I suggest that we always use keyword style argument passing (except for small helper functions that 1,2 or 3 arguments).",since long argument many default suggest always use style argument passing except small helper,issue,negative,positive,neutral,neutral,positive,positive
375257728,"Coding is pretty fast in China and people can use git to push/pull their code. We use it inside our company.

Sent from Mail<https://go.microsoft.com/fwlink/?LinkId=550986> for Windows 10

________________________________
发件人: Luo Mai <notifications@github.com>
发送时间: Thursday, March 22, 2018 5:59:47 PM
收件人: tensorlayer/tensorlayer
抄送: Junchen; Author
主题: Re: [tensorlayer/tensorlayer] 中国广东东莞这里无法下载多个数据集 (#420)


Does coding.net or gitee.com have stable and fast connection within China?

―
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorlayer%2Ftensorlayer%2Fissues%2F420%23issuecomment-375240950&data=02%7C01%7C%7Cdf437ca116134b28657108d58fdba5ac%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636573095887134916&sdata=xcuiX6Fd05KuxwqZPNHlKS30ch%2B%2BOUBSvukF45DmHvo%3D&reserved=0>, or mute the thread<https://nam04.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAI7VmX79Mj6HieUJana9-8GGe1bnnDogks5tg3YSgaJpZM4StHp2&data=02%7C01%7C%7Cdf437ca116134b28657108d58fdba5ac%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636573095887134916&sdata=UkVS%2B33ieV9jkoWqIvMCFoZ38FrsDuumsP16Q9lAnTQ%3D&reserved=0>.
",pretty fast china people use git code use inside company sent mail march author stable fast connection within china thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
375240950,Does coding.net or gitee.com have stable and fast connection within China?,stable fast connection within china,issue,negative,positive,positive,positive,positive,positive
375239108,"@zsdonghao as we are having a large number of examples, shall we actually start organizing the examples into different folders?",large number shall actually start different,issue,negative,positive,neutral,neutral,positive,positive
375208543,"I think `RefineDet` is also a good object detection model recently[[Paper](https://arxiv.org/pdf/1711.06897.pdf)][[Code](https://github.com/sfzhang15/RefineDet)]
",think also good object detection model recently paper,issue,negative,positive,positive,positive,positive,positive
375191216,"@luomai, I think we can create mirrors of TL repo in [coding.net](https://coding.net), [gitee.com](https://gitee.com), etc, and put some small datasets as project attachment.",think create put small project attachment,issue,positive,negative,negative,negative,negative,negative
375190383,"We are aware of these downloading issues in mainland China. However, we don't have extra budget to buy resource to host dataset mirrors in China, e.g., Ali Cloud. ",aware china however extra budget buy resource host china cloud,issue,negative,positive,positive,positive,positive,positive
374462168,"I modified the `tl.layers.DenseLayer`, and add `tf.device(/cpu:0)` directly on the `tf.get_variable`. The results seems correct.
This is the modified `tl.layers.DenseLayer`:
```Python
class DenseLayer(Layer):
    """"""
    The :class:`DenseLayer` class is a fully connected layer.

    Parameters
    ----------
    layer : a :class:`Layer` instance
        The `Layer` class feeding into this layer.
    n_units : int
        The number of units of the layer.
    act : activation function
        The function that is applied to the layer activations.
    W_init : weights initializer
        The initializer for initializing the weight matrix.
    b_init : biases initializer or None
        The initializer for initializing the bias vector. If None, skip biases.
    W_init_args : dictionary
        The arguments for the weights tf.get_variable.
    b_init_args : dictionary
        The arguments for the biases tf.get_variable.
    name : a string or None
        An optional name to attach to this layer.

    Examples
    --------
    >>> network = tl.layers.InputLayer(x, name='input_layer')
    >>> network = tl.layers.DenseLayer(
    ...                 network,
    ...                 n_units=800,
    ...                 act = tf.nn.relu,
    ...                 W_init=tf.truncated_normal_initializer(stddev=0.1),
    ...                 name ='relu_layer'
    ...                 )

    >>> Without TensorLayer, you can do as follow.
    >>> W = tf.Variable(
    ...     tf.random_uniform([n_in, n_units], -1.0, 1.0), name='W')
    >>> b = tf.Variable(tf.zeros(shape=[n_units]), name='b')
    >>> y = tf.nn.relu(tf.matmul(inputs, W) + b)

    Notes
    -----
    If the input to this layer has more than two axes, it need to flatten the
    input by using :class:`FlattenLayer` in this case.
    """"""

    def __init__(
            self,
            layer=None,
            n_units=100,
            act=tf.identity,
            W_init=tf.truncated_normal_initializer(stddev=0.1),
            b_init=tf.constant_initializer(value=0.0),
            W_init_args={},
            b_init_args={},
            name='dense_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        if self.inputs.get_shape().ndims != 2:
            raise Exception(""The input dimension must be rank 2, please reshape or flatten it"")

        n_in = int(self.inputs.get_shape()[-1])
        self.n_units = n_units
        print(""  [TL] DenseLayer  %s: %d %s"" % (self.name, self.n_units, act.__name__))
        with tf.variable_scope(name) as vs:
            with tf.device('/cpu:0'):
                W = tf.get_variable(name='W', shape=(n_in, n_units), initializer=W_init, dtype=D_TYPE, **W_init_args)
            if b_init is not None:
                try:
                    with tf.device('/cpu:0'):
                        b = tf.get_variable(name='b', shape=(n_units), initializer=b_init, dtype=D_TYPE, **b_init_args)
                except:  # If initializer is a constant, do not specify shape.
                    with tf.device('/cpu:0'):
                        b = tf.get_variable(name='b', initializer=b_init, dtype=D_TYPE, **b_init_args)
                self.outputs = act(tf.matmul(self.inputs, W) + b)
            else:
                self.outputs = act(tf.matmul(self.inputs, W))

        # Hint : list(), dict() is pass by value (shallow), without them, it is
        # pass by reference.
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend([self.outputs])
        if b_init is not None:
            self.all_params.extend([W, b])
        else:
            self.all_params.extend([W])
```
This is the modified code:
```Python
def inference():
        x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
        network = tl.layers.InputLayer(x, name='input')
        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')
        network = DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu1')
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
        network = DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu2')
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')
        network = DenseLayer(network, n_units=10, act=tf.identity, name='output')
        return network


if __name__ == '__main__':
    with tf.device('/gpu:0'):
        network = inference()
        network.print_layers()
        sess = tf.Session(config=tf.ConfigProto(
                allow_soft_placement=True,
                log_device_placement=True))
        tl.layers.initialize_global_variables(sess)
```
This is the placement result:
```
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
output/b: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
output/b/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
output/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
output/W: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
output/W/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
drop3/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
relu2/b/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
relu2/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
drop2/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
relu1/b/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
relu1/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W: (VariableV2): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/read: (Identity): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:CPU:0
init: (NoOp): /job:localhost/replica:0/task:0/device:CPU:0
drop1/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
drop1/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop1/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop1/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop1/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
relu1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
drop2/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop2/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop2/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop2/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop2/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
relu2/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
drop3/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop3/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop3/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop3/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop3/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
output/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
output/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
output/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:CPU:0
output/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:CPU:0
drop3/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder_2: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:CPU:0
drop2/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder_1: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:CPU:0
relu1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:CPU:0
drop1/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
x: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
```",add directly correct python class layer class class fully connected layer layer class layer instance layer class feeding layer number layer act activation function function applied layer weight matrix none bias vector none skip dictionary dictionary name string none optional name attach layer network network network act name without follow input layer two ax need flatten input class self self raise exception input dimension must rank please reshape flatten print name none try except constant specify shape act else act hint list pas value shallow without pas reference list list none else code python inference none network network network network network network network network network network network network network return network network inference sess sess placement result device device name bus id compute capability device name ti bus id compute capability identity assign identity add assign sub identity assign identity add assign sub identity assign identity add assign noop sub shape add add floor add shape add add floor add shape add add floor add identity,issue,positive,negative,negative,negative,negative,negative
374180396,"### Release SqueezeNet Example 
🚀 https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_squeezenet.py

### About MobileNet and ShuffleNet
**Q**: I found a problem of MobileNet and ShuffleNet: they use depthwise convolution, but TensorFlow's `tf.nn.depthwise_conv2d` and `tf.nn.separable_conv2d` kernel are very slow , so in practice, it seems that these methods could not speed up a lot?   https://github.com/tensorflow/tensorflow/issues/12940

**A**: TensorFlow 1.5 solved this problem, these operator run faster now.
",release example rocket found problem use depthwise convolution kernel slow practice could speed lot problem operator run faster,issue,negative,negative,negative,negative,negative,negative
374149736,"I write a simple code follow the [tutorial_cifar10_tfrecord.py](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_cifar10_tfrecord.py).
```Python
import tensorflow as tf
import tensorlayer as tl

with tf.device('/cpu:0'):
    def inference():
        x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
        network = tl.layers.InputLayer(x, name='input')
        network = tl.layers.DropoutLayer(network, keep=0.8, name='drop1')
        network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu1')
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2')
        network = tl.layers.DenseLayer(network, n_units=800, act=tf.nn.relu, name='relu2')
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop3')
        network = tl.layers.DenseLayer(network, n_units=10, act=tf.identity, name='output')
        return network


if __name__ == '__main__':
    with tf.device('/gpu:0'):
        network = inference()
        network.print_layers()
        sess = tf.Session(config=tf.ConfigProto(
                allow_soft_placement=True,
                log_device_placement=True))
        tl.layers.initialize_global_variables(sess)
```
The following is the code output:
```
Device mapping:
/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: TITAN Xp, pci bus id: 0000:02:00.0, compute capability: 6.1
/job:localhost/replica:0/task:0/device:GPU:1 -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1
output/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
output/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
output/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
output/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W: (VariableV2): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/read: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Assign: (Assign): /job:localhost/replica:0/task:0/device:GPU:0
init: (NoOp): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/device:GPU:0
drop1/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop1/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop1/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop1/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu1/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
relu1/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu1/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
drop2/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop2/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop2/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop2/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop2/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
relu2/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
relu2/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
relu2/Relu: (Relu): /job:localhost/replica:0/task:0/device:GPU:0
drop3/div: (RealDiv): /job:localhost/replica:0/task:0/device:GPU:0
drop3/Shape: (Shape): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop3/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
drop3/Floor: (Floor): /job:localhost/replica:0/task:0/device:GPU:0
drop3/mul: (Mul): /job:localhost/replica:0/task:0/device:GPU:0
output/MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0
output/add: (Add): /job:localhost/replica:0/task:0/device:GPU:0
output/Identity: (Identity): /job:localhost/replica:0/task:0/device:GPU:0
output/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
output/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop3/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder_2: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
relu2/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu2/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop2/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder_1: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
relu1/b/Initializer/Const: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/device:GPU:0
relu1/W/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/max: (Const): /job:localhost/replica:0/task:0/device:GPU:0
drop1/random_uniform/min: (Const): /job:localhost/replica:0/task:0/device:GPU:0
Placeholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
x: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0
```
The parameters and inference are all placed on `gpu`.
The code `with tf.device('/cpu:0'):` seems have no effect. ",write simple code follow python import import inference none network network network network network network network network network network network network network return network network inference sess sess following code output device device name bus id compute capability device name ti bus id compute capability identity assign identity add assign sub identity assign identity add assign sub identity assign identity add assign noop sub shape add add floor add shape add add floor add shape add add floor add identity inference code effect,issue,negative,neutral,neutral,neutral,neutral,neutral
373991556,"Thank u to give me a new view to slove this problem.But np.hstack can not do this work because:
ValueError: all the input arrays must have same number of dimensions
My input is x=[1,2,3,4,5,6,0,0,0,0] and seq = [6],that means seq input a list of int which is number of list x No zero int.
So do u know anthor function except ""for"" iter?Or how can I do in data process for blstm+mask",thank give new view work input must number input input list number list zero know function except iter data process,issue,negative,positive,positive,positive,positive,positive
373938383,"### Release 1.8.2

https://github.com/tensorlayer/tensorlayer/releases/tag/1.8.2

This is an experimental API package for building Binary Nets. We are using matrix multiplication rather than add-minus and bit-count operation at the moment. Therefore, these APIs would not speed up the inferencing, for production, you can train model via TensorLayer and deploy the model into other customized C/C++ implementation (We probably provide users an extra C/C++ binary net framework that can load model from TensorLayer).

### TODO
- For binary input, use XNOR, bit-count operation to replace matrix multiplication (i.e. dot production).
- For non binary input, use add and minus operation to replace matrix multiplication.",release experimental package building binary matrix multiplication rather operation moment therefore would speed production train model via deploy model implementation probably provide extra binary net framework load model binary input use operation replace matrix multiplication dot production non binary input use add minus operation replace matrix multiplication,issue,negative,neutral,neutral,neutral,neutral,neutral
373690547,"@wagamamaz Hello, thanks for merging this. If the PR contains many small commits like ""yapf"" ""format"", we usually use the ""Squash and Merge"" in order to the keep the master commit log simple.",hello thanks many small like format usually use squash merge order keep master commit log simple,issue,positive,positive,neutral,neutral,positive,positive
373626616,"@nebulaV The code on pypi is not the latest version.
https://pypi.python.org/pypi/tensorlayer/1.8.1
This version still has error",code latest version version still error,issue,negative,positive,positive,positive,positive,positive
373622798,"I use breakwall pptp, But I still can't download them. Is there another way? Maybe someone can make it available in the project?",use still ca another way maybe someone make available project,issue,negative,positive,positive,positive,positive,positive
373591963,"
________________________________________
From: Hao <notifications@github.com>
Sent: Friday, March 16, 2018 9:39:44 AM
To: tensorlayer/tensorlayer
Cc: Junchen; Author
Subject: Re: [tensorlayer/tensorlayer] 中国广东东莞这里无法下载多个数据集 (#420)

try vpn?

―
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Ftensorlayer%2Ftensorlayer%2Fissues%2F420%23issuecomment-373576482&data=02%7C01%7C%7C1b59feda6c844158c54f08d58adecc31%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636567611861900704&sdata=vC8ibL9hlwaBuaLrAdPDyh0RZuRS2FHabwmLJ1S5JhY%3D&reserved=0>, or mute the thread<https://nam02.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAI7VmS17jC3eS8OeYv1uXyG3DA6NM8M8ks5texfggaJpZM4StHp2&data=02%7C01%7C%7C1b59feda6c844158c54f08d58adecc31%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C636567611861900704&sdata=19jhNZqHnpFP6p%2FW%2FhUXUtCVQeGt%2FWKbBJu5lQ2uHF4%3D&reserved=0>.

",hao sent march author subject try thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
373586018,"Thanks @nebulaV , I check the latest code, the error has been fix.
",thanks check latest code error fix,issue,negative,positive,positive,positive,positive,positive
373420891,@wugh This bug has been fixed before. The code snippet you pasted here is the old version. Could you give a go using the latest master version?,bug fixed code snippet pasted old version could give go latest master version,issue,negative,positive,positive,positive,positive,positive
373374153,"My version is `tensorlayer (1.8.1)`, and I can found this error from latest code in the master brunch
the file is `recurrent.py`,  the line
```
rnn_creator = lambda: cell_fn(num_units=n_hidden, **cell_init_args)
```
make error

```
        # Creats the cell function
        # cell_instance_fn=lambda: cell_fn(num_units=n_hidden, **cell_init_args) # HanSheng
        # *** code make error ***
        rnn_creator = lambda: cell_fn(num_units=n_hidden, **cell_init_args)

        # Apply dropout
        if dropout:
            if isinstance(dropout, (tuple, list)):
                in_keep_prob = dropout[0]
                out_keep_prob = dropout[1]
            elif isinstance(dropout, float):
                in_keep_prob, out_keep_prob = dropout, dropout
            else:
                raise Exception(""Invalid dropout type (must be a 2-D tuple of "" ""float)"")
            try:  # TF1.0
                DropoutWrapper_fn = tf.contrib.rnn.DropoutWrapper
            except Exception:
                DropoutWrapper_fn = tf.nn.rnn_cell.DropoutWrapper

            # cell_instance_fn1=cell_instance_fn        # HanSheng
            # cell_instance_fn=DropoutWrapper_fn(
            #                     cell_instance_fn1(),
            #                     input_keep_prob=in_keep_prob,
            #                     output_keep_prob=out_keep_prob)
            cell_creator = lambda is_last=True: \
                    DropoutWrapper_fn(rnn_creator(),
                                      input_keep_prob=in_keep_prob,
                                      output_keep_prob=out_keep_prob if is_last else 1.0)
        else:
            cell_creator = rnn_creator
        self.cell = cell_creator()
        # Apply multiple layers
        if n_layer > 1:
            try:
                MultiRNNCell_fn = tf.contrib.rnn.MultiRNNCell
            except Exception:
                MultiRNNCell_fn = tf.nn.rnn_cell.MultiRNNCell

            # cell_instance_fn2=cell_instance_fn # HanSheng
            try:
                # cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)], state_is_tuple=True) # HanSheng
                self.cell = MultiRNNCell_fn([cell_creator(is_last=i == n_layer - 1) for i in range(n_layer)], state_is_tuple=True)
            except Exception:  # when GRU
                # cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)]) # HanSheng
                self.cell = MultiRNNCell_fn([cell_creator(is_last=i == n_layer - 1) for i in range(n_layer)])
```",version found error latest code master brunch file line lambda make error cell function code make error lambda apply dropout dropout dropout list dropout dropout dropout float dropout dropout else raise exception invalid dropout type must float try except exception lambda else else apply multiple try except exception try range range except exception range range,issue,negative,positive,positive,positive,positive,positive
373365160,"@wugh It works well on my side, what is your TensorFlow, TensorLayer version?",work well side version,issue,negative,neutral,neutral,neutral,neutral,neutral
373294303,"I could under somehow ...
Sorry for boring u.",could somehow sorry boring,issue,negative,negative,negative,negative,negative,negative
373056999,"addressed by [PR 408](https://github.com/tensorlayer/tensorlayer/pull/408)

@AutuanLiu thanks for your suggestion. Please close this issue if you think the PR has addressed your issue.",thanks suggestion please close issue think issue,issue,positive,positive,positive,positive,positive,positive
373019077,"Hello,

Please find  the example code below:

```python
network = tl.layers.ConvLSTMLayer(
      network, 
      cell_shape = (11, 11), 
      filter_size=(3, 3), 
      cell_fn = tl.layers.BasicConvLSTMCell, 
      n_steps = 12, 
      feature_map = 3,  
      name='convlstm')
```

I notice that TF has released a build-in ConvLSTM cell which I will incorporate into TL soon.",hello please find example code python network network notice cell incorporate soon,issue,negative,neutral,neutral,neutral,neutral,neutral
372969905,please run `yapf -i tensorlayer/_logging.py` to format the changed file.,please run format file,issue,negative,neutral,neutral,neutral,neutral,neutral
372919827,"I think using `__all__` is a better practice. @zsdonghao Could you start a PR to do this?

@lgarithm thought?",think better practice could start thought,issue,negative,positive,positive,positive,positive,positive
372893527,"The size of docker images are 1G and 3G, building them on each commit would case a lot of work load on travis-ci.",size docker building commit would case lot work load,issue,negative,neutral,neutral,neutral,neutral,neutral
372884511,@luomai @AutuanLiu I think add `__all__` in every py files is better than in `__init__.py`?,think add every better,issue,negative,positive,positive,positive,positive,positive
372273014,"‘’’
1 files need to be formatted, run the following commands to fix
yapf -i tensorlayer/files.py
‘’’",need run following fix,issue,negative,neutral,neutral,neutral,neutral,neutral
372204555,"People may create a new kind of mnist in the future.
If that happens, the bool flag would not be enough for extension.
I think url (for download) and name (for logging) should be generic enough for other mnist-like dataset, and pass them in could avoid using `if` `else` on the `flag` variable in the implementation of `load_mnist_dataset`.
",people may create new kind future bool flag would enough extension think name logging generic enough pas could avoid else flag variable implementation,issue,positive,positive,positive,positive,positive,positive
372200535,"Yes, I don't think we need yet another tutorial.

Since the fashion mnist is designed as a drop in replacement for mnist,
Can we slightly refactor the original `load_mnist_dataset` function to make it work for both datasets?

A possible solution would be 

1. change the original `load_mnist_dataset` to 
```python
def _load_mnist_dataset(shape=(-1, 784), name='mnist', path='data', url='...'):
      """"""A generic function to load mnist-like dataset.
....
      """"""
```

2. add two new functions:
```python
def load_mnist_dataset(shape=(-1, 784), ...):
      """"""Load the original mnist.""""""
      return _load_mnist_dataset(shape, name='mnist', ...)

def load_fashion_mnist_dataset(shape=(-1, 784), ...):
      """"""Load the fashion mnist.""""""
      return _load_mnist_dataset(shape, name='fashion_mnist', ...)

```",yes think need yet another tutorial since fashion designed drop replacement slightly original function make work possible solution would change original python generic function load add two new python load original return shape load fashion return shape,issue,positive,positive,positive,positive,positive,positive
372196692,"The size, shape, format, and training/val/test set partition of Fashion-MNIST is exactly the same as the original MNIST. One can use it to test your machine learning and deep learning algorithm performance directly, and only need to change is the data set you want input. So adding tutorial_fashion_mnist.py is not necessary. (personal opinion) 
@luomai ",size shape format set partition exactly original one use test machine learning deep learning algorithm performance directly need change data set want input necessary personal opinion,issue,negative,positive,neutral,neutral,positive,positive
372190343,"Hi @AutuanLiu thanks a lot for your contribution! nice work. Instead of adding a hint to the mnist_simple.py, I am wondering if it is better to create an example called tutorial_fashion_mnist.py? 

cc @zsdonghao @lgarithm ",hi thanks lot contribution nice work instead hint wondering better create example,issue,positive,positive,positive,positive,positive,positive
372115399,"@zsdonghao after scanning the code of tensorflow, i come up with this solution:

```
if W_init is not None and not callable(W_init ):
    #this means initializing with specified shape, so we should get weight without specify shape
else:
    # get weight with shape
```

I recommend that you update this into tensorlayer ^_^",scanning code come solution none callable shape get weight without specify shape else get weight shape recommend update,issue,positive,neutral,neutral,neutral,neutral,neutral
372115377,"Cool, a TL graph can also help user to export the model into other framework.
For example, we can train a binary-net with TensorLayer, then deploy it using crystalnet (leeloo).

To build the graph, every layer should contain the following information:

- What is the previous layer
- What is the next layer
- Layer type, hyper-parameters
- The weight/parameter(s) of the current layer

It is great to have the following functions:
- A connection with TensorBoard.
- A function to visualize the graph by image or prototxt-style file.
- A function to build a model with prototxt-style file.

BTW, TensorLayer doesn't support TensorFlow summary functions for TensorBoard at the moment, it is also a great idea to support it later.

Reference
- Caffe graph: https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt
- TensorBoard: https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard
",cool graph also help user export model framework example train deploy build graph every layer contain following information previous layer next layer layer type current layer great following connection function visualize graph image file function build model file support summary moment also great idea support later reference graph,issue,positive,positive,positive,positive,positive,positive
372079567,"i come out with 2 solutions.

1) find the TF initializer for your requirement.
2) customize a layer like that:

```python
class MyDenseLayer(Layer):
  def __init__(
      self,
      layer = None,
      n_units = 100,
      act = tf.nn.relu,
      name ='simple_dense',
  ):
      # check layer name (fixed)
      Layer.__init__(self, layer, name=name)

      # the input of this layer is the output of previous layer (fixed)
      self.inputs = layer.outputs

      # print out info (customized)
      print(""  MyDenseLayer %s: %d, %s"" % (self.name, n_units, act))

      # operation (customized)
      n_in = int(self.inputs._shape[-1])
      with tf.variable_scope(name) as vs:
          # create new parameters
          W = tf.get_variable(name='W', shape=(n_in, n_units))   <--- you may want to change this part
          b = tf.get_variable(name='b', shape=(n_units))
          # tensor operation
          self.outputs = act(tf.matmul(self.inputs, W) + b)

      # update layer (customized)
      self.all_layers.extend( [self.outputs] )
      self.all_params.extend( [W, b] )
```",come find layer like python class layer self layer none act name check layer name fixed self layer input layer output previous layer fixed print print act operation name create new may want change part tensor operation act update layer,issue,positive,positive,neutral,neutral,positive,positive
372079202,sometimes eye matrix is a good initialization which can speed up training,sometimes eye matrix good speed training,issue,negative,positive,positive,positive,positive,positive
372079123,"you will need an initializer for it them. btw, what situation make you to initialise an eye matrix?",need situation make eye matrix,issue,negative,neutral,neutral,neutral,neutral,neutral
372078944,@zsdonghao your answer only works when i want the matrix to be initialized by a single number. What if I want to initialize a matrix to be eye matrix?,answer work want matrix single number want initialize matrix eye matrix,issue,negative,negative,neutral,neutral,negative,negative
372074506,"@auroua please read the code carefully, `tutorial_cifar10_tfrecord.py` put the inference under `with tf.device('/gpu:0'):`",please read code carefully put inference,issue,negative,negative,neutral,neutral,negative,negative
372074229,"if my answer solve your question, please close this issue, many thanks.",answer solve question please close issue many thanks,issue,positive,positive,positive,positive,positive,positive
372065191,"Hi @vyokky , could you provide an example code to use `ConvLSTMLayer` ? 

Many thanks.",hi could provide example code use many thanks,issue,negative,positive,positive,positive,positive,positive
371851069,"I think the method in [tutorial_cifar10_tfrecord.py](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_cifar10_tfrecord.py) is not correct. The [multiple gpu](https://www.tensorflow.org/tutorials/deep_cnn) tutorial put the inference, loss  and the gradient  all on `gpu`, and the variable is on `cpu`. But the `tutorial_cifar10_tfrecord.py` put the inference procedure on `cpu`.",think method correct multiple tutorial put inference loss gradient variable put inference procedure,issue,negative,neutral,neutral,neutral,neutral,neutral
371806543,"there is a hint in the docs http://tensorlayer.readthedocs.io/en/latest/modules/iterate.html#non-time-series


If you have two inputs and one label and want to shuffle them together, e.g. X1 (1000, 100), X2 (1000, 80) and Y (1000, 1), you can stack them together (np.hstack((X1, X2))) into (1000, 180) and feed to inputs. After getting a batch, you can split it back into X1 and X2.
",hint two one label want shuffle together stack together feed getting batch split back,issue,negative,neutral,neutral,neutral,neutral,neutral
371702738,"I want to write blstm model with mask,and I saw API shows that BiDynamicRNNLayer can do this whit argument ""sequence_length"".But Iterate function ,for example ,tl.tierate.seq_minbatch can just feed input_x and input_y,
So how can I use iterate function to split ""seq_len"" into batch iterate?
PS:input example
x  = [2,5,13,5,6,4,3,6,0,0,0,0,0]
sel_len = [8]",want write model mask saw whit argument iterate function example feed use iterate function split batch iterate input example,issue,negative,neutral,neutral,neutral,neutral,neutral
371584845,"Sorry for the noise, I got confused about the axis definition of TF previously.",sorry noise got confused axis definition previously,issue,negative,negative,negative,negative,negative,negative
371584569,"@luomai  why the placeholder of `y_` is modified like that? any way to fixed all of them automatically?  

Why after changing this, it can't pass `yapf` ??",like way fixed automatically ca pas,issue,negative,positive,neutral,neutral,positive,positive
371217983,"@luomai this commit do 2 things:

- 1. Simplified MNIST example
- 2. Fixed bug of D_TYPE (after we splited the `layers.py` into many files, `tutorial_minst_float16.py` have bug, as the `D_TYPE` can't be changed in `convolution.py` and others)",commit simplified example fixed bug many bug ca,issue,negative,positive,positive,positive,positive,positive
370757121,"hi @hanm thanks a lot for your contribution.

The docs is automatically updated once the master is changed. We have setup a web hook for achieving this. 

The reason that the docs has not been updated is because the recent doc building attempt fails. Recently the readthedoc service is not very stable. We are going to manually rebuild it.",hi thanks lot contribution automatically master setup web hook reason recent doc building attempt recently service stable going manually rebuild,issue,positive,positive,neutral,neutral,positive,positive
370679908,"Great to hear that's on the roadmap! Starting from sqlite sounds a good idea as it's relatively simple and self contained from operation's perspective.

I am definitely interested on working in this area as well.",great hear starting good idea relatively simple self operation perspective definitely interested working area well,issue,positive,positive,positive,positive,positive,positive
370673044,"Yes, this is actually on our plan. 

The current db.py is going to under a big refactoring. That is why we have taken its document and code offline and only keep the copy in the repo for future reference.

The refactoring plan aims to achieve the following goals:
- Developing a general DB interface that contains model table schema and dataset table schema. The schema can be actually implemented by any SQL-compatible backend. We are thinking implementing **sqlite** as the first storage and queryable backend. As we use SQL database, then we can naturally use any Cloud provider to provide the database backend for us.
- We are also going to implement a **tl.models** module so that users can easily import established models such as resnet and inceptionv3. The imported model can be reused partially. The user can selectively restore part of this model based on layers. This is useful for transfer learning, for example.

We are going to have a Master student at Imperial College London to work on this project. However, we also highly welcome any developers to work with us on this direction, either by proposing new user scenarios or participating in the development.",yes actually plan current going big taken document code keep copy future reference plan achieve following general interface model table schema table schema schema actually thinking first storage use naturally use cloud provider provide u also going implement module easily import established model partially user selectively restore part model based useful transfer learning example going master student imperial college work project however also highly welcome work u direction either new user development,issue,positive,positive,positive,positive,positive,positive
370612241,"Thanks for merging this @luomai .

I see http://tensorlayer.readthedocs.io/en/latest/user/tutorial.html is not updated with this fix - just curious what should be done to get the doc updated?",thanks see fix curious done get doc,issue,positive,positive,neutral,neutral,positive,positive
370246539,"@zsdonghao The model API can allow model specific parameter as follows:

```python
net = tl.models.inceptionv3(**args)  # import inception v3 model with args
```

The API shall also allow ""partially reusing the model based on layers"" for transfer learning, for example.

```python
net = tl.models.inceptionv3(**args)[:-1]  # reuse model and remove last layer
net = tl.models.inceptionv3(**args)[`conv3_2`]  # reuse model until conv3_2 layer
```


",model allow model specific parameter python net import inception model shall also allow partially model based transfer learning example python net reuse model remove last layer net reuse model layer,issue,negative,negative,neutral,neutral,negative,negative
370233802,"I have implemented `Resent` in `tensorlayer` which can be created  by 
```Python
nets = get_resnet(x, 1000, 50, sess)
```
The code can load the `tf-slim` pre-trained `ckpt file`, but I found the inference results have some differences with the `tf-slim` model.
The code can be found [here](https://github.com/auroua/InsightFace_TF/blob/master/nets/resnet.py).",resent python sess code load file found inference model code found,issue,negative,neutral,neutral,neutral,neutral,neutral
370204352,"Shall we also provide the following syntax sugar if it is possible to do so in python?
```python
net = tl.models[""inceptionv3""][:-1]
net = tl.models[""inceptionv3""][""conv3_2""]
```
",shall also provide following syntax sugar possible python python net net,issue,negative,neutral,neutral,neutral,neutral,neutral
370189003,"@youkaichao TL will automatically compute the shape of the parameters, so you don't need to specify the shape.

Please, use `tf.constant_initializer(value=1.0)` instead of `tf.constant()`.

```python
x = tf.placeholder(""float32"", [None, 100])
n = tl.layers.InputLayer(x, name='in')
n = tl.layers.DenseLayer(n, 80, W_init=tf.constant_initializer(value=1.0), name='d1')
```",automatically compute shape need specify shape please use instead python float none,issue,negative,neutral,neutral,neutral,neutral,neutral
370175136,"With the master version of TL, given the following model:
```python
>>> x = tf.placeholder(""float32"", [None, 100])
>>> n = tl.layers.InputLayer(x, name='in')
>>> n = tl.layers.DenseLayer(n, 80, name='d1')
>>> n = tl.layers.DenseLayer(n, 80, name='d2')
>>> print(n)
... Last layer is: DenseLayer (d2) [None, 80]
```

The outputs can be sliced as follow:
```python
>>> n2 = n[:30]
>>> print(n2)
... Last layer is: Layer (d2) [None, 30]
```

The outputs of all layers can be iterated as follow:
```python
>>> for l in n:
>>>    print(l)
... Tensor(""d1/Identity:0"", shape=(?, 80), dtype=float32)
... Tensor(""d2/Identity:0"", shape=(?, 80), dtype=float32)
```",master version given following model python float none print last layer none sliced follow python print last layer layer none follow python print tensor tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
370172299,"@luomai @lgarithm 

I am thinking to release an API that help people import any type of model without knowing anything about slim or other resources.

Something like:

- Get the whole model:
```python
>>> net = tl.model.get_model(""inceptionv3"", last_layer=True)
>>. print(net.input_placeholder) # <-- where to feed the data in.
... (None, 299, 299, 3)
```

- Get just a part of the model:
```python
>>> net = tl.model.get_model(""inceptionv3"", layer_name=""conv3_2"")
```

Any idea about how to design the API ?",thinking release help people import type model without knowing anything slim something like get whole model python net print feed data none get part model python net idea design,issue,positive,positive,neutral,neutral,positive,positive
369901703,"Yes we do, https://join.slack.com/t/tensorlayer/shared_invite/enQtMjUyMjczMzU2Njg4LWI0MWU0MDFkOWY2YjQ4YjVhMzI5M2VlZmE4YTNhNGY1NjZhMzUwMmQ2MTc0YWRjMjQzMjdjMTg2MWQ2ZWJhYzc

However, we suggest to use issues to discuss bug

",yes however suggest use discus bug,issue,negative,neutral,neutral,neutral,neutral,neutral
369561110,@zsdonghao Please use squash merge,please use squash merge,issue,negative,neutral,neutral,neutral,neutral,neutral
369556427,"🆕 Hi, we support this with `tl.models` now , see [here](https://github.com/tensorlayer/pretrained-models/blob/master/README.md)

==================
Hi, TL provides' [SlimNetsLayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#connect-tf-slim) that can use all Google TF-Slim's pre-tained model [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models). This is an example for Inception V3 ([click](https://github.com/tensorlayer/tensorlayer/blob/master/example/tutorial_inceptionV3_tfslim.py))
",hi support see hi use model example inception click,issue,negative,neutral,neutral,neutral,neutral,neutral
369441604,"Final Version:

```python

class GroupConv2d(Layer):
    """"""The :class:`GroupConv2d` class is 2D grouped convolution, see `here <https://blog.yani.io/filter-group-tutorial/>`__.

    Parameters
    --------------
    layer : :class:`Layer`
        Previous layer.
    n_filter : int
        The number of filters.
    filter_size : int
        The filter size.
    stride : int
        The stride step.
    n_group : int
        The number of groups.
    act : activation function
        The activation function of this layer.
    padding : str
        The padding algorithm type: ""SAME"" or ""VALID"".
    W_init : initializer
        The initializer for the weight matrix.
    b_init : initializer or None
        The initializer for the bias vector. If None, skip biases.
    W_init_args : dictionary
        The arguments for the weight matrix initializer.
    b_init_args : dictionary
        The arguments for the bias vector initializer.
    name : str
        A unique layer name.
    """"""

    def __init__(
            self,
            layer=None,
            n_filter=32,
            filter_size=(3, 3),
            strides=(2, 2),
            n_group=2,
            act=tf.identity,
            padding='SAME',
            W_init=tf.truncated_normal_initializer(stddev=0.02),
            b_init=tf.constant_initializer(value=0.0),
            W_init_args=None,
            b_init_args=None,
            name='groupconv',
    ):  # Windaway
        if W_init_args is None:
            W_init_args = {}
        if b_init_args is None:
            b_init_args = {}

        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        groupConv = lambda i, k: tf.nn.conv2d(i, k, strides=[1, strides[0], strides[1], 1], padding=padding)
        channels = int(self.inputs.get_shape()[-1])
        with tf.variable_scope(name) as vs:
            We = tf.get_variable(
                name='weights',
                shape=[filter_size[0], filter_size[1], channels / n_group, n_filter],
                initializer=W_init,
                dtype=D_TYPE,
                trainable=True,
                **W_init_args)
            if b_init:
                bi = tf.get_variable(
                    name='biases', shape=[
                        n_filter,
                    ], initializer=b_init, dtype=D_TYPE, trainable=True, **b_init_args)
        if n_group == 1:
            conv = groupConv(self.inputs, We)
        else:
            inputGroups = tf.split(axis=3, num_or_size_splits=n_group, value=self.inputs)
            weightsGroups = tf.split(axis=3, num_or_size_splits=n_group, value=We)
            convGroups = [groupConv(i, k) for i, k in zip(inputGroups, weightsGroups)]
            conv = tf.concat(axis=3, values=convGroups)
        if b_init is not None:
            conv = tf.add(conv, bi, name='add')

        self.outputs = act(conv)
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend([self.outputs])
        if b_init is not None:
            self.all_params.extend([We, bi])
        else:
            self.all_params.extend([We])
```",final version python class layer class class grouped convolution see layer class layer previous layer number filter size stride stride step number act activation function activation function layer padding padding algorithm type valid weight matrix none bias vector none skip dictionary weight matrix dictionary bias vector name unique layer self none none self lambda name else zip none act list list none else,issue,negative,positive,neutral,neutral,positive,positive
368911520,"@Windaway hi, 我改了一下代码，并加了一些描述，你帮忙看看对不对？

```python
class GroupConv2d(Layer):
    """"""The :class:`GroupConv2d` class is 2D grouped convolution, see `here <https://blog.yani.io/filter-group-tutorial/>`__.

    Parameters
    --------------
    layer : :class:`Layer`
        Previous layer.
    n_filter : int
        The number of filters.
    filter_size : int
        The filter size.
    stride : int
        The stride step.
    n_group : int
        The number of groups.
    act : activation function
        The activation function of this layer.
    padding : str
        The padding algorithm type: ""SAME"" or ""VALID"".
    W_init : initializer
        The initializer for the weight matrix.
    b_init : initializer or None
        The initializer for the bias vector. If None, skip biases.
    W_init_args : dictionary
        The arguments for the weight matrix initializer.
    b_init_args : dictionary
        The arguments for the bias vector initializer.
    name : str
        A unique layer name.
    """"""
    def __init__(
        self,
        layer = None,
        n_filter =0,
        filter_size=(3, 3),
        strides=(2, 2),
        n_group=2,
        act = tf.identity,
        padding='SAME',
        W_init=tf.truncated_normal_initializer(stddev=0.02),
        b_init=tf.constant_initializer(value=0.0),
        W_init_args=None,
        b_init_args=None,
        name ='groupconv',
    ): # Windaway 
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        groupConv = lambda i, k: tf.nn.conv2d(i, k, strides=[1, strides[0], strides[1], 1], padding= padding)
        channels = int(self.inputs.get_shape()[-1])
        with tf.variable_scope(name) as vs:
            We = tf.get_variable(name='weights', shape=[filter_size[0], filter_size[1], channels / n_group, n_filter],  initializer=W_init, dtype=D_TYPE,  trainable=True, **W_init_args)
            if bi:
                bi = tf.get_variable(name='biases', shape=[n_filter, ],
                                      initializer=tf.constant_initializer(value=0.1),
                                      initializer=b_init, dtype=D_TYPE, trainable=True, **b_init_args )
        if groups == 1:
            conv = groupConv(self.inputs, We)
        else:
            inputGroups = tf.split(axis=3, num_or_size_splits=n_group, value=self.inputs)
            weightsGroups = tf.split(axis=3, num_or_size_splits=n_group, value=We)
            convGroups = [groupConv(i, k) for i, k in zip(inputGroups, weightsGroups)]
            conv = tf.concat(axis=3, values=convGroups)
        if b_init is not None:
            conv = tf.add(conv, bi, name='add')

        self.outputs =act(conv)
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
        if b_init is not None:
            self.all_params.extend( [We, bi] )
        else:
            self.all_params.extend( [We] )",hi python class layer class class grouped convolution see layer class layer previous layer number filter size stride stride step number act activation function activation function layer padding padding algorithm type valid weight matrix none bias vector none skip dictionary weight matrix dictionary bias vector name unique layer self layer none act name self lambda padding name else zip none list list none else,issue,negative,positive,positive,positive,positive,positive
368207558,Thanks for your code. Could you please submit your code as a PR to our project? ,thanks code could please submit code project,issue,positive,positive,positive,positive,positive,positive
368207266,"`def obj_box_crop(im, classes=None, coords=None, wrg=100, hrg=100, is_rescale=False, is_center=False, is_random=False, thresh_wh=0.02, thresh_wh2=12., thresh_wh3=0.1):
    """"""Randomly or centrally crop an image, and compute the new bounding box coordinates.
    Objects outside the cropped image will be removed.

    Parameters
    -----------
    im : numpy.array
        An image with dimension of [row, col, channel] (default).
    classes : list of int or None
        Class IDs.
    coords : list of list of 4 int/float or None
        Coordinates [[x, y, w, h], [x, y, w, h], ...]
    wrg hrg and is_random : args
        See ``tl.prepro.crop``.
    is_rescale : boolean
        Set to True, if the input coordinates are rescaled to [0, 1]. Default is False.
    is_center : boolean, default False
        Set to True, if the x and y of coordinates are the centroid (i.e. darknet format). Default is False.
    thresh_wh : float
        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.
    thresh_wh2 : float
        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.
    thresh_wh3 : float
        Threshold, remove the box if its ratio of (new bbox size)/(old bbox size) smaller than the threshold.
    Returns
    -------
    numpy.array
        A processed image
    list of int
        A list of classes
    list of list of 4 numbers
        A list of new bounding boxes.

    """"""
    if classes is None:
        classes = []
    if coords is None:
        coords = []

    h, w = im.shape[0], im.shape[1]
    assert (h > hrg) and (w > wrg), ""The size of cropping should smaller than the original image""
    if is_random:
        h_offset = int(np.random.uniform(0, h-hrg) -1)
        w_offset = int(np.random.uniform(0, w-wrg) -1)
        h_end = hrg + h_offset
        w_end = wrg + w_offset
        im_new = im[h_offset: h_end ,w_offset: w_end]
    else:   # central crop
        h_offset = int(np.floor((h - hrg)/2.))
        w_offset = int(np.floor((w - wrg)/2.))
        h_end = h_offset + hrg
        w_end = w_offset + wrg
        im_new = im[h_offset: h_end, w_offset: w_end]

    #              w
    #   _____________________________
    #   |  h/w offset               |
    #   |       -------             |
    # h |       |     |             |
    #   |       |     |             |
    #   |       -------             |
    #   |            h/w end        |
    #   |___________________________|

    def _get_coord(coord):
        """""" Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.
        """"""
        size_ratio_temp=1.
        if is_center:
            coord = obj_box_coord_centroid_to_upleft(coord)

        ##======= pixel unit format and upleft, w, h ==========##

        # x = np.clip( coord[0] - w_offset, 0, w_end - w_offset)
        # y = np.clip( coord[1] - h_offset, 0, h_end - h_offset)
        # w = np.clip( coord[2]           , 0, w_end - w_offset)
        # h = np.clip( coord[3]           , 0, h_end - h_offset)

        x = coord[0] - w_offset
        y = coord[1] - h_offset
        w = coord[2]
        h = coord[3]
        size_raw=w*h
        if x < 0:
            if x + w <= 0:
                return None,size_ratio_temp
            w = w + x
            x = 0
        elif x > im_new.shape[1]:   # object outside the cropped image
            return None,size_ratio_temp

        if y < 0:
            if y + h <= 0:
                return None,size_ratio_temp
            h = h + y
            y = 0
        elif y > im_new.shape[0]:   # object outside the cropped image
            return None,size_ratio_temp

        if (x is not None) and (x + w > im_new.shape[1]):   # box outside the cropped image
            w = im_new.shape[1] - x

        if (y is not None) and (y + h > im_new.shape[0]):   # box outside the cropped image
            h = im_new.shape[0] - y
        size_changed=w*h
        if (w / (h+1.) > thresh_wh2) or (h / (w+1.) > thresh_wh2):           # object shape strange: too narrow
            # print('xx', w, h)
            return None,size_ratio_temp

        if (w / (im_new.shape[1]*1.) < thresh_wh) or (h / (im_new.shape[0]*1.) < thresh_wh):    # object shape strange: too narrow
            # print('yy', w, im_new.shape[1], h, im_new.shape[0])
            return None,size_ratio_temp

        coord = [x, y, w, h]

        ## convert back if input format is center.
        if is_center:
            coord = obj_box_coord_upleft_to_centroid(coord)
        size_ratio_temp=(size_changed/size_raw)
        return coord,size_ratio_temp

    coords_new = list()
    classes_new = list()
    for i in range(len(coords)):
        coord = coords[i]
        assert len(coord) == 4, ""coordinate should be 4 values : [x, y, w, h]""
        if is_rescale:
            """""" for scaled coord, upscaled before process and scale back in the end. """"""
            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)
            coord,size_ratio = _get_coord(coord)
            if coord is not None and size_ratio>thresh_wh3:
                coord = obj_box_coord_rescale(coord, im_new.shape)
                coords_new.append(coord)
                classes_new.append(classes[i])
        else:
            coord,size_ratio = _get_coord(coord)
            if coord is not None and size_ratio>thresh_wh3:
                coords_new.append(coord)
                classes_new.append(classes[i])
    return im_new, classes_new, coords_new`


需求比较简单就是对每个bbox算了一下 zoom也类似。",randomly centrally crop image compute new bounding box outside image removed image dimension row col channel default class list none class list list none see set true input default false default false set true centroid format default false float threshold remove box ratio width height image size le threshold float threshold remove box ratio width height vice verse higher threshold float threshold remove box ratio new size old size smaller threshold image list list class list list list new bounding class none class none assert size smaller original image else central crop offset end input format make sure getting new image unit format return none object outside image return none return none object outside image return none none box outside image none box outside image object shape strange narrow print return none object shape strange narrow print return none convert back input format center return list list range assert scaled process scale back none class else none class return,issue,negative,positive,neutral,neutral,positive,positive
368041509,"@luomai running `make format` in this PR will change several files again, which might cause conflict.
We can do that in any next PR whose has merged this Makefile.",running make format change several might cause conflict next whose,issue,negative,neutral,neutral,neutral,neutral,neutral
368040692,@luomai disabled this lint to reduce travis-ci log.,disabled lint reduce log,issue,negative,negative,negative,negative,negative,negative
367710474,"@luomai @zsdonghao when you finish setup the webhook, please reply to this thread, to trigger a comment event.",finish setup please reply thread trigger comment event,issue,negative,neutral,neutral,neutral,neutral,neutral
367706972,"@luomai @zsdonghao could you help setup the webhook for codacy, instruction is [here](https://app.codacy.com/app/tensorlayer/tensorlayer/settings/integrations).",could help setup instruction,issue,negative,neutral,neutral,neutral,neutral,neutral
366955877,"你好，刚看到你的邮件里有这个代码：

```python
def obj_box_crop_limited(im, classes=[], coords=[], wrg=100, hrg=100,
    is_rescale=False, is_center=False, is_random=False,
    thresh_wh=0.02, thresh_wh2=12.,min_size_ratio=0.1):
    """"""Randomly or centrally crop an image, and compute the new bounding box coordinates.
    Objects outside the cropped image will be removed.

    Parameters
    -----------
    im : numpy array
        An image with dimension of [row, col, channel] (default).
    classes : list of class ID (int).
    coords : list of list for coordinates [[x, y, w, h], [x, y, w, h], ...]
    wrg, hrg, is_random : see ``tl.prepro.crop`` for details.
    is_rescale : boolean, default False
        Set to True, if the input coordinates are rescaled to [0, 1].
    is_center : boolean, default False
        Set to True, if the x and y of coordinates are the centroid. (i.e. darknet format)
    thresh_wh : float
        Threshold, remove the box if its ratio of width(height) to image size less than the threshold.
    thresh_wh2 : float
        Threshold, remove the box if its ratio of width to height or vice verse higher than the threshold.
    """"""
    h, w = im.shape[0], im.shape[1]
    assert (h > hrg) and (w > wrg), ""The size of cropping should smaller than the original image""
    if is_random:
        h_offset = int(np.random.uniform(0, h-hrg) -1)
        w_offset = int(np.random.uniform(0, w-wrg) -1)
        h_end = hrg + h_offset
        w_end = wrg + w_offset
        im_new = im[h_offset: h_end ,w_offset: w_end]
    else:   # central crop
        h_offset = int(np.floor((h - hrg)/2.))
        w_offset = int(np.floor((w - wrg)/2.))
        h_end = h_offset + hrg
        w_end = w_offset + wrg
        im_new = im[h_offset: h_end, w_offset: w_end]

    #              w
    #   _____________________________
    #   |  h/w offset               |
    #   |       -------             |
    # h |       |     |             |
    #   |       |     |             |
    #   |       -------             |
    #   |            h/w end        |
    #   |___________________________|

    def _get_coord(coord):
        """""" Input pixel-unit [x, y, w, h] format, then make sure [x, y] it is the up-left coordinates,
        before getting the new coordinates.
        Boxes outsides the cropped image will be removed.
        """"""
        size_ratio=1.
        if is_center:
            coord = obj_box_coord_centroid_to_upleft(coord)

        ##======= pixel unit format and upleft, w, h ==========##

        # x = np.clip( coord[0] - w_offset, 0, w_end - w_offset)
        # y = np.clip( coord[1] - h_offset, 0, h_end - h_offset)
        # w = np.clip( coord[2]           , 0, w_end - w_offset)
        # h = np.clip( coord[3]           , 0, h_end - h_offset)
        size_raw=w*h
        x = coord[0] - w_offset
        y = coord[1] - h_offset
        w = coord[2]
        h = coord[3]

        if x < 0:
            if x + w <= 0:
                return None
            w = w + x
            x = 0
        elif x > im_new.shape[1]:   # object outside the cropped image
            return None

        if y < 0:
            if y + h <= 0:
                return None
            h = h + y
            y = 0
        elif y > im_new.shape[0]:   # object outside the cropped image
            return None

        if (x is not None) and (x + w > im_new.shape[1]):   # box outside the cropped image
            w = im_new.shape[1] - x

        if (y is not None) and (y + h > im_new.shape[0]):   # box outside the cropped image
            h = im_new.shape[0] - y
        size_changed=w*h
        if (w / (h+1.) > thresh_wh2) or (h / (w+1.) > thresh_wh2):           # object shape strange: too narrow
            # print('xx', w, h)
            return None

        if (w / (im_new.shape[1]*1.) < thresh_wh) or (h / (im_new.shape[0]*1.) < thresh_wh):    # object shape strange: too narrow
            # print('yy', w, im_new.shape[1], h, im_new.shape[0])
            return None

        coord = [x, y, w, h]

        ## convert back if input format is center.
        if is_center:
            coord = obj_box_coord_upleft_to_centroid(coord)

        return coord,(size_changed/size_raw)

    coords_new = list()
    classes_new = list()
    for i in range(len(coords)):
        coord = coords[i]
        assert len(coord) == 4, ""coordinate should be 4 values : [x, y, w, h]""
        if is_rescale:
            """""" for scaled coord, upscaled before process and scale back in the end. """"""
            coord = obj_box_coord_scale_to_pixelunit(coord, im.shape)
            coord,size_ratio = _get_coord(coord)
            if coord is not None and size_ratio>min_size_ratio:
                coord = obj_box_coord_rescale(coord, im_new.shape)
                coords_new.append(coord)
                classes_new.append(classes[i])
        else:
            coord,size_ratio = _get_coord(coord)
            if coord is not None and size_ratio>min_size_ratio:
                coords_new.append(coord)
                classes_new.append(classes[i])
    return im_new, classes_new, coords_new
```

你能不能开一个branch，发一个push request，在`obj_box_crop`上加一个新参数叫 `thresh_wh3`？我和其它人review后，merge进去。",python randomly centrally crop image compute new bounding box outside image removed array image dimension row col channel default class list class id list list see default false set true input default false set true centroid format float threshold remove box ratio width height image size le threshold float threshold remove box ratio width height vice verse higher assert size smaller original image else central crop offset end input format make sure getting new image unit format return none object outside image return none return none object outside image return none none box outside image none box outside image object shape strange narrow print return none object shape strange narrow print return none convert back input format center return list list range assert scaled process scale back none class else none class return,issue,positive,positive,neutral,neutral,positive,positive
366727925,"Sorry I miss these functions in the code:

```python
def tf_flatten(a):
    """"""Flatten tensor""""""
    return tf.reshape(a, [-1])

def _get_vals_by_coords(inputs, coords, idx, out_shape):
    indices = tf.stack([
        idx, tf_flatten(coords[:, :, :, :, 0]), tf_flatten(coords[:, :, :, :, 1])
    ], axis=-1)
    vals = tf.gather_nd(inputs, indices)
    vals = tf.reshape(vals, out_shape)
    return vans
```",sorry miss code python flatten tensor return index index return,issue,negative,negative,negative,negative,negative,negative
366723730,"hi, it seems you miss some code in the pr.

```
NameError: name '_get_vals_by_coords' is not defined
```",hi miss code name defined,issue,negative,neutral,neutral,neutral,neutral,neutral
366706905,"```
************* Module tensorlayer.files
W:120,77: Unused argument 'second' (unused-argument)
************* Module tensorlayer.db
W:497,29: Unused argument 'logs' (unused-argument)
W:500,27: Unused argument 'logs' (unused-argument)
W:503,36: Unused argument 'logs' (unused-argument)
W:523,36: Unused argument 'logs' (unused-argument)
W:528,27: Unused argument 'batch' (unused-argument)
************* Module tensorlayer.prepro
W:228,65: Unused argument 'channel_index' (unused-argument)
W:270,71: Unused argument 'channel_index' (unused-argument)
W:1773,55: Unused argument 'positive_orientation' (unused-argument)
W:1773,32: Unused argument 'fully_connected' (unused-argument)
************* Module tensorlayer.cost
W:516,20: Unused argument 'name' (unused-argument)
W:479,26: Unused argument 'scope' (unused-argument)
W:529,26: Unused argument 'scope' (unused-argument)
W:579,35: Unused argument 'scope' (unused-argument)
W:630,33: Unused argument 'scope' (unused-argument)
W:680,33: Unused argument 'scope' (unused-argument)
************* Module tensorlayer.layers.recurrent
W:477,37: Unused argument 'dtype' (unused-argument)
W:1527,12: Unused argument 'initializer' (unused-argument)
```",module unused argument module unused argument unused argument unused argument unused argument unused argument module unused argument unused argument unused argument unused argument module unused argument unused argument unused argument unused argument unused argument unused argument module unused argument unused argument,issue,negative,neutral,neutral,neutral,neutral,neutral
366513473,let me know when this pr is ready to review.,let know ready review,issue,negative,positive,positive,positive,positive,positive
366504349,"autoflake can't fix wildcard import, need to find other ways.",ca fix import need find way,issue,negative,neutral,neutral,neutral,neutral,neutral
366461446,"no tool for auto fix D413 found, will use the following poor man's script

```
#!/usr/bin/env python

import re

def fix_D413(filename):
    code = open(filename).read()
    code = re.sub(r'([^:\n])\n(\s+)""""""', r'\1\n\n\2""""""', code)
    with open(filename, 'wb') as f:
        f.write(code)

def main(args):
    # fix_D413('tensorlayer/layers/core.py')
```",tool auto fix found use following poor man script python import code open code code open code main,issue,negative,negative,neutral,neutral,negative,negative
366445538,"we got 778 violations from 20 categories, as suggested by pydocstyle:
```
D413      274 Missing blank line after last section
D205      114 1 blank line required between summary line and description
D400       92 First line should end with a period
D107       61 Missing docstring in __init__
D401       54 First line should be in imperative mood
D210       47 No whitespaces allowed surrounding docstring text
D100       32 Missing docstring in public module
D202       24 No blank lines allowed after function docstring
D301       22 Use r"""""" if any backslashes in a docstring
D102       18 Missing docstring in public method
D207        9 Docstring is under-indented
D200        9 One-line docstring should fit on one line with quotes
D103        9 Missing docstring in public function
D403        4 First word of the first line should be properly capitalized
D412        2 No blank lines allowed between a section header and its content
D402        2 First line should not be the function's ""signature""
D105        2 Missing docstring in magic method
D208        1 Docstring is over-indented
D104        1 Missing docstring in public package
D101        1 Missing docstring in public class
tot      778 violations
```",got missing blank line last section blank line summary line description first line end period missing first line imperative mood surrounding text missing public module blank function use missing public method fit one line missing public function first word first line properly blank section header content first line function signature missing magic method missing public package missing public class tot,issue,negative,positive,neutral,neutral,positive,positive
366432735,"@ning180 any update? if not, we are going to close this issue.",update going close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
366422863,"Currently we are using
* yapf (format code)
* isort (sort imports)
* autoflake (remove unused imports)

and we should also use
* pylint
* pydocstyle
when most lints warnings are fixed",currently format code sort remove unused also use fixed,issue,negative,positive,neutral,neutral,positive,positive
366213843,"Hi, i close this issue now, feel free to reopen if you want discussion~

To implement different kinds of RNN, we can feed in any third party cell functions into `RNNLayer` and `DynamicRNNLayer`. Other normalisations like layer norm can be found [here]( http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#normalization-layer).",hi close issue feel free reopen want implement different feed third party cell like layer norm found,issue,positive,positive,positive,positive,positive,positive
366009331,"We first need to replace the ‘print’ to a logging function, which is now defined in _logging.py. We can later switch to tf.logging easily by just changing _logging.py.

________________________________
From: Hao <notifications@github.com>
Sent: Friday, February 16, 2018 1:52:46 AM
To: tensorlayer/tensorlayer
Cc: LG; Author
Subject: Re: [tensorlayer/tensorlayer] [WIP] replace all print to logging.info (#313)


we use the python's logging rather than TF's logging?

—
You are receiving this because you authored the thread.
Reply to this email directly, view it on GitHub<https://github.com/tensorlayer/tensorlayer/pull/313#issuecomment-366008180>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AB8ydCExx_OHtDPLnl7rGdk-jTdGVbjhks5tVG7tgaJpZM4SGcbO>.
",first need replace print logging function defined later switch easily hao sent author subject replace print use python logging rather logging thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
366008180,we use the python's logging rather than TF's logging?,use python logging rather logging,issue,negative,neutral,neutral,neutral,neutral,neutral
365274548,"Just to repeat what I proposed in #306 

The idea would be to make this output optional (default = True or False). I think there could be different ways to do this.

##### 1. Solution - Create a **verbose** parameter in the Layer API
Simple and backward compatible, a ""verbose"" parameter can be added to the Layer Class and influence the behavior of [print_params()](https://github.com/tensorlayer/tensorlayer/blob/master/tensorlayer/layers.py#L321) method.

##### 2. Use the Logging module from TF.

Why should we re-invent the wheel ? Everything is already implemented in Tensorflow.
We can use the logging level already existing in TF.

```python
tf.logging._level_names    ## outputs => {50: 'FATAL', 40: 'ERROR', 30: 'WARN', 20: 'INFO', 10: 'DEBUG'}
tf.logging.get_verbosity() ## outputs => 30 (default value)

tf.logging.set_verbosity(tf.logging.DEBUG)
tf.logging.get_verbosity() ## outputs => 10
```

We could for instance determine that for logging level <= 20 (INFO & DEBUG), we normally output the Tensorlayer informations, and we don't perform this action for any higher value.
",repeat idea would make output optional default true false think could different way solution create verbose parameter layer simple backward compatible verbose parameter added layer class influence behavior method use logging module wheel everything already use logging level already python default value could instance determine logging level normally output perform action higher value,issue,positive,positive,neutral,neutral,positive,positive
365272477,"Hi, @DEKHTIARJonathan thank you for your suggestion.

- @luomai @lgarithm and me just have a discussion,  we will change all `print` to `logging`, just like this issues https://github.com/tensorlayer/tensorlayer/issues/207 discuss.

- An alternative way to print parameters is``tl.layers.get_variables_with_name``, which allow users to print and get the specific parameters.",hi thank suggestion discussion change print logging like discus alternative way print allow print get specific,issue,positive,neutral,neutral,neutral,neutral,neutral
364920537,"I think this is because `global_step` was not specified on the optimizer, a possible solution would be like:
```
-    train_op = tf.train.AdamOptimizer(learning_rate=0.0001
-                        ).minimize(cost, var_list=train_params)
+    optimizer = tf.train.AdamOptimizer(learning_rate=0.0001)
+    train_op = optimizer.minimize(cost, var_list=train_params, global_step=tf.train.get_or_create_global_step())

```",think possible solution would like cost cost,issue,positive,neutral,neutral,neutral,neutral,neutral
364640146,"@jorgemf The main purpose of this PR is to help users to automatically setup the TF_CONFIG environment for parallel TF programs locally.  We have run the inception v3 example on a Azure GPU 4 card VM using asynchronous synchronization. We observe that the GPUs utilization is around 90%. Each worker is processing their data shard at almost equivalent rate. This means that for GPU-intensive model, we could expect performance speed up more or less, though we do admit that the fact that gRPC is not very efficient for big models. As for the model convergence time, we believe this would depends on the model, data shading, as well as the synchronization approach. We regards this as a separated issue which shall be explored by the users themselves.",main purpose help automatically setup environment parallel locally run inception example azure card asynchronous synchronization observe utilization around worker data shard almost equivalent rate model could expect performance speed le though admit fact efficient big model convergence time believe would model data shading well synchronization approach issue shall,issue,positive,positive,neutral,neutral,positive,positive
364351481,"I made a really stupid mistake. I feed wrong ROI between training image and ground truth image.

Everything is going well right now. Thanks for your help! ",made really stupid mistake feed wrong roi training image ground truth image everything going well right thanks help,issue,negative,negative,negative,negative,negative,negative
364316354,"Hi @zsdonghao , I just create a new [pull request](https://github.com/tensorlayer/tensorlayer/pull/302) to delete this two line. 

There also some confusion about the function params, why use the dropout as keep_prob, I think we should make the argument name more sensible. We can create another argument name `keep_prob` here and recommend user not to use the `dropout` params.",hi create new pull request delete two line also confusion function use dropout think make argument name sensible create another argument name recommend user use dropout,issue,negative,positive,positive,positive,positive,positive
364120496,"yes, you are right, this line is useless",yes right line useless,issue,negative,negative,negative,negative,negative,negative
363599262,"@xx4966946 Could you check whether your code update all parameters of the network?

You can feed all parameters with the name of ""VDSR"" to the optimiser as follow:

```python
train_params = tl.layers.get_variables_with_name('VDSR', train_only=True, printable=False)
train_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost, var_list=train_params)
```",could check whether code update network feed name follow python cost,issue,negative,neutral,neutral,neutral,neutral,neutral
363579363,"This model is a super resolution model. I am using it to do something like denoise image. The original VDSR model can make the input smooth and reduce noise, but when I try to use tensorlayer to reproduce, the result is super blur (even worse than the input noisy image), cannot figure out why (I set all paras same, also learning rate). 

",model super resolution model something like image original model make input smooth reduce noise try use reproduce result super blur even worse input noisy image figure set also learning rate,issue,positive,positive,positive,positive,positive,positive
363576400,"Hi, i just have a quick check, it seems your code is correct, what is the error?

this code may look simpler.
```python
def model(x, reuse):
    with tf.variable_scope(""VDSR"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        ni = InputLayer(x, name='inputs')
        n = Conv2d(ni, 64, (3, 3), act=tf.nn.relu, W_init=w_init, b_init=b_init, name='conv1_1')
        for i in range(18):
            n = Conv2d(n, 64, (3, 3), act=tf.nn.relu, W_init=w_init1, b_init=b_init, name='conv2_%s' % i)
        n = Conv2d(n, 1, (3, 3), act=None,  W_init=w_init1, b_init=b_init, name='conv3_1')
        n = ElementwiseLayer([n, ni], tf.add, 'add1')
    return n
```",hi quick check code correct error code may look simpler python model reuse reuse ni ni range ni return,issue,negative,positive,positive,positive,positive,positive
363544224,"@youkaichao hi, please check this commit https://github.com/tensorlayer/tensorlayer/commit/980e148d7541da9a3ef7b04a1d1e68a1df8caa05


",hi please check commit,issue,positive,neutral,neutral,neutral,neutral,neutral
363063599,"Mark as TODO, [tf.contrib.layers.conv2d_transpose](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d_transpose) don't support old version of TF. 
As suggest we can update `DeConv2d` by using [tf.contrib.layers.conv2d_transpose](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/conv2d_transpose), then when users are using new version of TF, TL can use it directly.",mark support old version suggest update new version use directly,issue,negative,positive,positive,positive,positive,positive
362952338,"@alokgrover88 try `tl.vis`, actually you can implement it yourself by using libraries like `matplotlib`",try actually implement like,issue,negative,neutral,neutral,neutral,neutral,neutral
362952239,"Set name automatically can help users to build layer quick in many case, but may easily led to fault reuse in some case. I think the library can provide a global setting like `tl.layers.auto_name(True)` to enable this kind of feature? 

Simply disable one `if` statement in `Layer` class and set a new name.",set name automatically help build layer quick many case may easily led fault reuse case think library provide global setting like true enable kind feature simply disable one statement layer class set new name,issue,positive,positive,positive,positive,positive,positive
362951709,"If I am correct, the author want everyone use the default shape format of TensorFlow, because it can run faster.",correct author want everyone use default shape format run faster,issue,negative,neutral,neutral,neutral,neutral,neutral
361705584,"hi @matthew-z , do you have any hint to implement dropout with [tf.contrib.rnn.stack_bidirectional_dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/stack_bidirectional_dynamic_rnn) ? ",hi hint implement dropout,issue,negative,neutral,neutral,neutral,neutral,neutral
361681300,Can any help me with the code? I want to crop out multiple detected objects in an image and store them as separate images?,help code want crop multiple image store separate,issue,negative,neutral,neutral,neutral,neutral,neutral
361585048,"@luomai I haven't done a lot of tests with more than one local GPU but in my current setup with 2 GPUs 1080ti and a small model (NasNet mobile), the improvement with 2 GPUs is only 10%. Instead of training the model in 10 days it takes 9. It means it spends the most time in the network communication rather than processing the data. That is why I think it is not a good practice and we should try to avoid it.

I am working on splitting the model as the estimators do and also integrating it with the datasets classes. I think we can provide a better API that uses the datasets, helps them with the distributed environment and also works with more than one GPU locally without doing anything else. Although we would need some way to disable the autoreplication of the model in all local GPUs for the people who already splits the model themselves.",done lot one local current setup ti small model mobile improvement instead training model day time network communication rather data think good practice try avoid working splitting model also class think provide better distributed environment also work one locally without anything else although would need way disable model local people already model,issue,positive,positive,positive,positive,positive,positive
361497952,"We run `pip install tensorflow` in `.travis.yml` without specific version, so it should be TF 1.5 now.",run pip install without specific version,issue,negative,neutral,neutral,neutral,neutral,neutral
361467955,"@jorgemf Also, for the cluster training mode, we would recommend users to use the Google Kubeflow or TensorPort. We have run some internal tests with the Kubeflow team. It turns out to be easy for TL users to use Kubeflow in public Clouds like GCE, Azure and AWS, or private Clouds that already have a docker-enabled environment. ",also cluster training mode would recommend use run internal team turn easy use public like azure private already environment,issue,positive,positive,positive,positive,positive,positive
361467753,"@jorgemf Hi Jorge, thanks a lot for your inspiring comments. :) 

We are aware of the estimator API as well as its mission for making distributed training transparent to users. However, a bunch of TL users would have to completely rewrite their code to fit the estimator model, if possible. Our current approach is to provide a helper script to help them use tl.distributed to modify their program and use the train.py to bootstrap the local training. We also recommend them to use estimator API to write new projects whenever possible. 

We are also aware of the gRPC communication overhead. TF has some proposals to resolve it using commodity hardware. However, we treat this as a separate issue with this PR. 

To sum up, the main purpose of this PR is to simply provide a helper script to start a TF_CONFIG environment locally so that existing TL programs (like the minst_distributed and inception_v3_distributed) may find easy to be setup locally. This PR does not aim to create another paradigm in parallel with the estimator.

What do you think?",hi thanks lot inspiring aware estimator well mission making distributed training transparent however bunch would completely rewrite code fit estimator model possible current approach provide helper script help use modify program use bootstrap local training also recommend use estimator write new whenever possible also aware communication overhead resolve commodity hardware however treat separate issue sum main purpose simply provide helper script start environment locally like may find easy setup locally aim create another paradigm parallel estimator think,issue,positive,positive,positive,positive,positive,positive
361398536,"How do I do it based on a specific class?

On Jan 29, 2018 4:46 PM, ""Hao"" <notifications@github.com> wrote:

> Hi, if you want to crop the sub-image out of the image, you can use the
> tl.prepro.crop. For visualization, you can use
> tl.vis.draw_boxes_and_labels_to_image.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/tensorlayer/tensorlayer/issues/293#issuecomment-361397491>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AiRed8kjfiXjFkuVUn6TUh1SCxtoDGkAks5tPjw5gaJpZM4RxYTZ>
> .
>
",based specific class hao wrote hi want crop image use visualization use thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
361397491,"Hi, if you want to crop the sub-image out of the image, you can use the `tl.prepro.crop`. For visualization, you can use `tl.vis.draw_boxes_and_labels_to_image`.",hi want crop image use visualization use,issue,negative,neutral,neutral,neutral,neutral,neutral
361336486,"For local training with multiple GPUs TensorFlow is adding new features to estimators that replicate the model once per GPU and them combine the gradients: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/estimator/python/estimator

The problem with distributed TensorFlow is the overhead of the network communication of the model over grpc. It is very slow to encode and decode the model. There are solutions as using DRMA over MPI or Verbs but those imply to compile TensorFlow with support for them and have specialized hardware in your machines. The solution TensorFlow is doing to solve this issues locally is to replicate the model one per GPU getting rid of the network communication and grpc.

To sum up, I don't think this is a good practice, and it would be better to use the TF estimators or something similar.",local training multiple new replicate model per combine problem distributed overhead network communication model slow encode decode model imply compile support specialized hardware solution solve locally replicate model one per getting rid network communication sum think good practice would better use something similar,issue,positive,positive,positive,positive,positive,positive
360461324,"Seems good enough, maybe you should modify the raise error: 

I will open a PR with a change ""proposal"" for the error raised:  #290

**Current Exception:**
```text
Exception: Layer 'my_layer_name' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

**Proposed Exception:**
```text
Exception: Layer 'my_layer_name' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
Additional Informations: http://tensorlayer.readthedocs.io/en/latest/modules/layers.html?highlight=clear_layers_name#tensorlayer.layers.clear_layers_name
```

@zsdonghao I close this one, I think we don't need more info, if you wan't to discuss about this one, I stay available on #290 

Thanks a lot for your help and once again: very nice work !

Jonathan",good enough maybe modify raise error open change proposal error raised current exception text exception layer already please choice reuse layer hint use different name different name used control parameter exception text exception layer already please choice reuse layer hint use different name different name used control parameter additional close one think need wa discus one stay available thanks lot help nice work,issue,positive,positive,positive,positive,positive,positive
360458872,"I would actually support the raised issues, what are the key reasons to keep in memory and manage a list of layers name ? All of this should already be managed by TF internally. Maybe I don't understand smthg, however I don't see the benefits of doing this.",would actually support raised key keep memory manage list name already internally maybe understand however see,issue,negative,neutral,neutral,neutral,neutral,neutral
360444423,"Thank you for your advise, I will update it now, please let me know or submit a PR if you find my edition is not good enough. 

BTW, https://github.com/tensorlayer/tensorlayer/issues/214 this issue may also helpful for you
",thank advise update please let know submit find edition good enough issue may also helpful,issue,positive,positive,positive,positive,positive,positive
359836126,"@luomai No, I don't convert ImageNet to TFrecords, I create a txt where each line has a path to the image and the labels.",convert create line path image,issue,negative,neutral,neutral,neutral,neutral,neutral
359801935,"@jorgemf In the distributed_inception_v3 example, you are using the kaggle or imagenet website to download training data. Are these data in the same format and structure with the one downloaded by the tf-slim: https://github.com/tensorflow/models/blob/master/research/slim/datasets/download_and_convert_imagenet.sh",example training data data format structure one,issue,negative,neutral,neutral,neutral,neutral,neutral
359460718,"Hi, thanks. we can modify `BiRNNLayer` to support different `cell_fn` for fw and dw, if it is common in practice.",hi thanks modify support different common practice,issue,positive,negative,neutral,neutral,negative,negative
357983867,"I think shuffle, sharding and interleave can be a little confusing in distributed training, we can provide something to make it simpler for users.",think shuffle interleave little distributed training provide something make simpler,issue,negative,negative,negative,negative,negative,negative
357983102,"done with core lib files.

tutorial files remained untouched.",done core tutorial untouched,issue,negative,neutral,neutral,neutral,neutral,neutral
357943222,"dont change the tutorial, it will make the code difficult to read",dont change tutorial make code difficult read,issue,negative,negative,negative,negative,negative,negative
357877187,I thought this before. Is the default style sufficient for our needs?,thought default style sufficient need,issue,negative,neutral,neutral,neutral,neutral,neutral
357849796,"@jorgemf @chaos5958 I have been checking the dataset API in these days. It looks pretty intuitive and the APIs are easy to use. Given the non-invasive nature of TL, it should be pretty straightforward to use the dataset APIs interchangeably with the TL code.  I want to ask for some hints here. What are the missed API that you would like TL to provide on top of the dataset?

We could come up with the API requirement and then develop a PR together. Thanks!",chaos day pretty intuitive easy use given nature pretty straightforward use interchangeably code want ask would like provide top could come requirement develop together thanks,issue,positive,positive,positive,positive,positive,positive
357749997,"Hi, I just find a way to train different model setting in one script, hope it helps in some case.

```python
for .... (different hyper-parameter):
    with tf.Graph().as_default() as graph:  # clear all variables of TF
           tl.layers.clear_layers_name()         # clear all layer name of TL
           sess = tf.InteractiveSession()
           # train a model here
```",hi find way train different model setting one script hope case python different graph clear clear layer name sess train model,issue,positive,positive,neutral,neutral,positive,positive
357614508,"I'm sorry to reopen this issue because I meet some problems when trying to initialize a Conv2d layer with two or more my own filters.

```python
import tensorflow as tf
import tensorlayer as tl
import numpy as np

sess = tf.InteractiveSession()
x = tf.placeholder(tf.float32, shape=[None, 256,256,1])

F0 = np.array([[-1, 2, -2, 2, -1],
               [2, -6, 8, -6, 2],
               [-2, 8, -12, 8, -2],
               [2, -6, 8, -6, 2],
               [-1, 2, -2, 2, -1]], dtype=np.float32)
F1 = F0 / 12.
Filter = [F0, F1]# 2 filter
Filter = np.array(Filter, dtype=np.float32)
my_filter = tf.constant_initializer(value=Filter, dtype=tf.float32)

net = tl.layers.InputLayer(x, name='input')
net = tl.layers.Conv2dLayer(net,
                  act=tf.identity,
                  shape=[5, 5, 1, len(Filter)],
                  strides=[1, 1, 1, 1],
                  padding='SAME',
                  W_init=my_filter,
                  b_init=tf.constant_initializer(value=0.0),
                  name='layer1')
tl.layers.initialize_global_variables(sess)

tl.visualize.CNN2d(net.all_params[0].eval(), second=1, saveable=False, 
                   name='filter', fig_idx=42)
```

![tim 20180115161618](https://user-images.githubusercontent.com/7754393/34932868-9f89a9da-fa0f-11e7-8c24-dd3721a00d96.png)

I think it may be caused by this part, but I donn't know what to do.
```
F1 = F0 / 12.
Filter = [F0, F1]# 2 filter
Filter = np.array(Filter, dtype=np.float32)
my_filter = tf.constant_initializer(value=Filter, dtype=tf.float32)
``` 

And when use tl.layers.ConcatLayer (every layer have only one filter), it works well.
![56454](https://user-images.githubusercontent.com/7754393/34933454-d223e7fa-fa11-11e7-8c90-5a945110645a.png)

Thank you!",sorry reopen issue meet trying initialize layer two python import import import sess none filter filter filter filter net net net filter sess think may part know filter filter filter filter use every layer one filter work well thank,issue,positive,negative,negative,negative,negative,negative
356868175,"Yes, that's true. I mean the SELU drop module rather than the SELU activation function. Actually, I don't know the different between [SELU dropout](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) module and dense(selu) + dropout.",yes true mean drop module rather activation function actually know different dropout module dense dropout,issue,positive,positive,neutral,neutral,positive,positive
356862210,"TL supports SELU by default, just simply pass `tf.nn.selu` into your layers, like `tf.nn.relu`.",default simply pas like,issue,negative,neutral,neutral,neutral,neutral,neutral
356799024,"I met this error earlier before. To solve this problem, you may need to pass a parameter to ```np.load()``` function.

It should be changed to ```np.load(encoding='latin1')```
I think the function you mentioned is in ```tensorlayer/files.py```
",met error solve problem may need pas parameter function think function,issue,negative,neutral,neutral,neutral,neutral,neutral
356630976,"@lgarithm I am adding a message now, let me test it and upload the change.",message let test change,issue,negative,neutral,neutral,neutral,neutral,neutral
356629998,"@luomai I'm OK with the current PR.
@jorgemf handling that error would be nice, but if it is tricky to handle, you can just add more comments in the beginning of file, saying that data must be properly prepared in order to run it.",current handling error would nice tricky handle add beginning file saying data must properly prepared order run,issue,negative,positive,positive,positive,positive,positive
356626655,"@lgarithm https://github.com/jorgemf/tensorlayer/blob/3c027c7c7fece77efba9f1c24944ba2b38f20522/example/tutorial_imagenet_inceptionV3_distributed.py#L4-L7

You need to accept the terms and conditions before downloading the dataset. I can add a message to handle that error.",need accept add message handle error,issue,negative,neutral,neutral,neutral,neutral,neutral
356621819,"how to prepare the dataset?

I got the following error when running in a clean repository
```
$ python example/tutorial_imagenet_inceptionV3_distributed.py 
2018-01-10 14:39:53,067 Batch size: 32
2018-01-10 14:39:53,067 Epochs: 100
Traceback (most recent call last):
  File ""example/tutorial_imagenet_inceptionV3_distributed.py"", line 429, in <module>
    might_create_training_set()
  File ""example/tutorial_imagenet_inceptionV3_distributed.py"", line 82, in might_create_training_set
    shuffle=True)
  File ""example/tutorial_imagenet_inceptionV3_distributed.py"", line 64, in might_create_dataset
    for filename in tf.gfile.Glob(os.path.join(annotations_dir, suffix)):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.py"", line 335, in get_matching_files
    compat.as_bytes(single_filename), status)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.NotFoundError: ./ILSVRC/Annotations/CLS-LOC/train; No such file or directory
```
",prepare got following error running clean repository python batch size recent call last file line module file line file line suffix file line status file line file directory,issue,negative,positive,neutral,neutral,positive,positive
356352163,"Hi @nickbent, the process for transfer learning I do is as follow:

1. Create the model with the desired number of outputs (a number different from 1001 most probably)
2. Tell the optimizer to not train the variables that are not in the last layer, this is, train only the variables of the last layer (you might also want to retrain the whole graph with the initialization of an already trained model, it speeds up the training and give good results too)
3. Start the session and initilize the graph variables
4. Load from the checkpoint all the graph without the last layer (you can set the variables in the saver, even do a mapping between the names of the variables in the graph and in the file)
5. Do the normal training",hi process transfer learning follow create model desired number number different probably tell train last layer train last layer might also want retrain whole graph already trained model training give good start session graph load graph without last layer set saver even graph file normal training,issue,positive,positive,positive,positive,positive,positive
356340323,"actions items:
* fix import orders
* fix spacing, line break, etc.
* fix other warnings produced by a linter (e.g. pep8, pylint) as much as we can
* enforce style check on code review",fix import fix spacing line break fix produced linter pep much enforce style check code review,issue,negative,positive,positive,positive,positive,positive
355750463,@tomtung @mitar @haiy we are preparing for a PR to replace all the print with logging in the library. ,replace print logging library,issue,negative,neutral,neutral,neutral,neutral,neutral
355745390,"yes, and I think the best place to add it is in the Dockerfile.",yes think best place add,issue,positive,positive,positive,positive,positive,positive
355745033,"@jorgemf totally agree. A more fundamental solution would be to use logger instead of print in, at least, the distributed and docker examples. ",totally agree fundamental solution would use logger instead print least distributed docker,issue,positive,negative,negative,negative,negative,negative
355741924,"Thank you for reporting this, I just fixed it here https://github.com/tensorlayer/tensorlayer/commit/3beebb9c0dd67dc07cc137912a2f714c87d07ed8

Best wishes,
Hao",thank fixed best hao,issue,positive,positive,positive,positive,positive,positive
355741326,"Thanks for reporting this, I just fixed it by using previous np.float32 in master branch",thanks fixed previous master branch,issue,negative,positive,neutral,neutral,positive,positive
355572691,Another way to unbuffer the python output is to set the environment variable `PYTHONUNBUFFERED=1`. So we don't need to touch any code,another way python output set environment variable need touch code,issue,negative,neutral,neutral,neutral,neutral,neutral
355455748,"@luomai could you help to review it?

BTW, to add new API to the documentation, see `docs/modules/xxx.rst`.",could help review add new documentation see,issue,negative,positive,positive,positive,positive,positive
355395058,"I notice this issue too. I also read the normal behavior in Linux is to buffer the output (it sounds strange for me but I bought it). My solution was to use the logging package for printing more info and to configure it to flush the output more often. I am working on another example that does this, once I am completely sure it works I will do the PR (also fix other bugs in the distributed mode).
",notice issue also read normal behavior buffer output strange bought solution use logging package printing configure flush output often working another example completely sure work also fix distributed mode,issue,negative,positive,positive,positive,positive,positive
355206872,"Hi @prabhjot-singh-gogana , sorry for my late response, the graduate application takes much of my time. 

I might not be so familiar with TensorLayer now since I have switched to Tensorflow and PyTorch. But I think [onnx](https://github.com/onnx/onnx) could help converting the model, you may wanna try it.",hi sorry late response graduate application much time might familiar since switched think could help converting model may wan na try,issue,negative,negative,neutral,neutral,negative,negative
354609590,"Hi, you may need to do something with `slim.arg_scope` 

Here is an example to remove some final layers from inception_v3, hope it solves your problem.

https://github.com/zsdonghao/Image-Captioning/blob/master/model.py

Happy new year!",hi may need something example remove final hope problem happy new year,issue,positive,positive,positive,positive,positive,positive
354400696,"I got a similar problem.  However, I used the two TimedistributedLayer but they aren't  stacked. Do you get the  TraceBack ""Variable timedense1/dense1/W"" does not exit, or was not created with tf.get_variable(). Did you mean to set reuse=Node in varScope""? Have you coped with it ? ",got similar problem however used two get variable exit mean set,issue,negative,negative,negative,negative,negative,negative
353729562,"@mshockwave thank you for the quick notice.
looks like this restriction is introduced in python3.
I will change it to `os.fdopen(sys.stdout.fileno(), 'wb', 0)`, it should also work for python2.",thank quick notice like restriction python change also work python,issue,negative,positive,positive,positive,positive,positive
353716386,"It should behave equivalently as calling `setvbuf` to `_IONBF` for stdout.
For normal use case, there should be no visible difference.",behave equivalently calling normal use case visible difference,issue,negative,positive,positive,positive,positive,positive
352304946,"For the `DropoutLayer`, I suggest to set `is_fix` to True to build separated graphs for training and testing (instead of using placeholder to control train/test), see this example https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mlp_dropout2.py",suggest set true build training testing instead control see example,issue,negative,positive,positive,positive,positive,positive
352217615,"The current API uses `tl.util.fit` in TensorFlow most functionality has been added to hooks in the session. A minimum example would be (I wrote this for the distributed API):
```
 task_spec = TaskSpec()
 # dataset is a :class:`tf.data.Dataset` with the raw data
 dataset = create_dataset()
 if task_spec is not None:
     dataset = dataset.shard(task_spec.num_workers, task_spec.shard_index)
 # shuffle or apply a map function to the new sharded dataset, for example:
 dataset = dataset.shuffle(buffer_size=10000)
 dataset = dataset.batch(batch_size)
 dataset = dataset.repeat(num_epochs)
 # create the iterator for the dataset and the input tensor
 iterator = dataset.make_one_shot_iterator()
 next_element = iterator.get_next()
 with tf.device(task_spec.device_fn()):
      # next_element is the input for the graph
      tensors = create_graph(next_element)
 with tl.DistributedSession(task_spec=task_spec,
                            checkpoint_dir='/tmp/ckpt') as session:
      while not session.should_stop():
           session.run(tensors)
```

The only issue with this code is that it will show an error after the number of epochs you set. But you can set up most things in the distributed session (http://tensorlayer.readthedocs.io/en/latest/modules/distributed.html#distributed-session-object ) and you can add more functionality with hooks: https://www.tensorflow.org/api_guides/python/train#Training_Hooks
I have already added some hooks for the distributed API and I think it can be worth to add more functionality this way http://tensorlayer.readthedocs.io/en/latest/modules/distributed.html#session-hooks

The Dataset class in TensorFlow offers a lot of flexibility, I would add our own Dataset class to TensorLayer where we trade some flexibility for something easier to use. Maybe for some uses cases as reading data from text files (for example each line contains a description of a data sample and then we use a map function to load the data) and a custom generator were the samples are generated on the fly. I have already code for this that I could add to TensorLayer. I think it is also important the dataset class is related with the distributed API as we can use things like sharding to use different data samples per worker server.",current functionality added session minimum example would wrote distributed class raw data none shuffle apply map function new sharded example create input tensor input graph session issue code show error number set set distributed session add functionality already added distributed think worth add functionality way class lot flexibility would add class trade flexibility something easier use maybe reading data text example line description data sample use map function load data custom generator fly already code could add think also important class related distributed use like use different data per worker server,issue,positive,positive,positive,positive,positive,positive
352216982,"Hi @wagamamaz, I have just added some methods to use distributed training in master. It doesn't matter what you want to train because the only change is in the session you need to use for it. Take a look to the example: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist_distributed.py and also to the documentation online: http://tensorlayer.readthedocs.io/en/latest/modules/distributed.html

Let me know if you have any issues with it and I will fix them",hi added use distributed training master matter want train change session need use take look example also documentation let know fix,issue,negative,neutral,neutral,neutral,neutral,neutral
352181204,"@zsdonghao, Actually, this is not error, but suggestion for improving Tensorlayer.
I found Tensorlayer is extremely useful for people who want to use parts of Tensorflow semantics and control tensorflow semantics in detail.
However, above example might be limitation of Tensorlayer, since we can't control errors occurred from Tensorflow if we don't modify APIs itself as you suggested. 

Then, let's close this issue and, I'll send a pull request if I can design better code.

Thank you for answering me,
Hyunho",actually error suggestion improving found extremely useful people want use semantics control semantics detail however example might limitation since ca control modify let close issue send pull request design better code thank,issue,positive,positive,positive,positive,positive,positive
352178021,"@chaos5958 , to make it easy, I suggest you to copy the code inside this API. Could I know what is the error about?",chaos make easy suggest copy code inside could know error,issue,negative,positive,positive,positive,positive,positive
352177702,"Hi, I think your implementation is correct.",hi think implementation correct,issue,negative,neutral,neutral,neutral,neutral,neutral
352169207,"for gpu image, use the following commands to build:

```
docker build -t tensorlayer/tensorlayer:latest-gpu  -f Dockerfile.gpu .
docker build -t tensorlayer/tensorlayer:1.7.2-gpu  -f Dockerfile.gpu .
```",image use following build docker build docker build,issue,negative,neutral,neutral,neutral,neutral,neutral
352165465,"https://github.com/zsdonghao/tensorlayer/pull/249 added the Dockerfile, so a docker image can be built with:

`docker build -t tensorlayer/tensorlayer .`

or 

`docker build -t tensorlayer/tensorlayer:1.7.2 .`

for specific tag.
",added docker image built docker build docker build specific tag,issue,negative,neutral,neutral,neutral,neutral,neutral
352076612,"Hi all, I wonder whether TensorLayer supports training RNN model in distributed way?",hi wonder whether training model distributed way,issue,negative,neutral,neutral,neutral,neutral,neutral
351956939,"```
network = Conv2d(net_in, df_dim, (k, k), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2), padding='SAME', W_init=w_init, name='h0/conv2d')
tf.summary.histogram('h0/conv2d',tf.get_collection(tf.GraphKeys.VARIABLES, 'h0/conv2d'))
```
how to get the variable of network? the tensorboard shows nothing.
",network get variable network nothing,issue,negative,neutral,neutral,neutral,neutral,neutral
351096707,"@jorgemf thanks, i corrected the format error and syn it to chinese docs.",thanks corrected format error,issue,negative,positive,positive,positive,positive,positive
351095706,"@jorgemf hi, great job, thanks, I corrected the format error and syn it to the chinese docs.",hi great job thanks corrected format error,issue,positive,positive,positive,positive,positive,positive
351004416,"@zsdonghao I have made a PR, please take a look and let me know what should be changed: https://github.com/zsdonghao/tensorlayer/pull/245",made please take look let know,issue,negative,neutral,neutral,neutral,neutral,neutral
349758417,"Hi @jorgemf , your way of organizing the distributed API is great. I also suggest you to import `distributed.py` in `__init__.py`, so users can call the API easily.

As you only add the distributed API, which would not affect the original APIs, so
for testing, just simply run [tutorial_mnist_simple.py](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist_simple.py). To help others understand how to use your APIs, you can put a distributed example [here](https://github.com/zsdonghao/tensorlayer/tree/master/example).

For the online documentation, you can create a `distributed.rst` [here](https://github.com/zsdonghao/tensorlayer/tree/master/docs/modules) and add the index into [here](https://github.com/zsdonghao/tensorlayer/edit/master/docs/index.rst) (I can help to do this part if you wish), then we can have a new tag in the [online documentation](http://tensorlayer.readthedocs.io/en/latest/).

Thank you for your contribution.",hi way distributed great also suggest import call easily add distributed would affect original testing simply run help understand use put distributed example documentation create add index help part wish new tag documentation thank contribution,issue,positive,positive,positive,positive,positive,positive
348703283,"@ExonRen Hi, I just release an example for float16 model using Conv2d, BatchNorm and Dense

https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist_float16.py

Hope it can solve your problem~",hi release example float model dense hope solve,issue,positive,neutral,neutral,neutral,neutral,neutral
348574138,"As the error show, you don’t have permission to create or open file, I suggest to run your script as administrator in windows, or use sudo in Linux ",error show permission create open file suggest run script administrator use,issue,negative,neutral,neutral,neutral,neutral,neutral
346952544,"@vyokky TF1.4 doesn't matter, probably adding the following code in deformable CNN API can give some hint to others when error happen.
 
```python
if tf.__version__ < ""1.4"":
      raise Exception(""xxx"")
```",matter probably following code deformable give hint error happen python raise exception,issue,negative,neutral,neutral,neutral,neutral,neutral
346950179,"Hi, I find that the deformable CNN depends on tensorflow 1.4. If this is a problem I can fix later.",hi find deformable problem fix later,issue,negative,neutral,neutral,neutral,neutral,neutral
346906702,Thanks. I will continue to contribute to this project in the future.,thanks continue contribute project future,issue,negative,positive,neutral,neutral,positive,positive
346906579,"this is an amazing contribution @vyokky thank you very much.

btw, I updated the code for documentation.",amazing contribution thank much code documentation,issue,positive,positive,positive,positive,positive,positive
345445421,"@ShieLian Em, let me merge it first, it should be right.",em let merge first right,issue,negative,positive,positive,positive,positive,positive
345444829,"@zsdonghao Yep, I just tested it on my trained SRResNet, and the image producted before and after modified even have the same SHA-256 hash value. But Since I just have the params of SRResNet under new SubPixelConv2d, I didn't test the old one. ",yep tested trained image producted even hash value since new test old one,issue,positive,positive,positive,positive,positive,positive
345443610,"Thank you very much @ShieLian . Before I merge it, could I confirm you have tried it on [SRGAN project](https://github.com/zsdonghao/SRGAN) , and have the same performance?
",thank much merge could confirm tried project performance,issue,negative,positive,positive,positive,positive,positive
343686467,"you need to make it yourself.

```
The _build folder can be generated in docs using make html
```",need make folder make,issue,negative,neutral,neutral,neutral,neutral,neutral
343393130,"hi @JindongJiang , Could you please tell how to convert .npz model into .caffemodel. Thanks in advance",hi could please tell convert model thanks advance,issue,positive,positive,positive,positive,positive,positive
342123036,"In stackoverflow I find the same problem(https://stackoverflow.com/questions/45046992/how-to-upgrade-cudnn5-1-to-cudnn6) and the questioner give a solution (Now I palced cudnn directory singlely and add path setting then the problem solved). 


OH! YEAH!!!
I copy cudnn-8.0-windows7-x64-v6.0 to my cuda directory and then the problem solved!!!
",find problem questioner give solution directory add path setting problem oh yeah copy directory problem,issue,negative,neutral,neutral,neutral,neutral,neutral
342059078," No,I mean that:
```
for i in range(3):  # number of mini-batch (step)
        print(""Step %d"" % i)
        val, l = sess.run([img_batch, label_batch])
        print(val.shape, l)
        tl.visualize.images2d(val, second=1, saveable=False, name='batch', dtype=None, fig_idx=2020121)
```
this program means that run one epoch on the whole data. But I want to run 100 epochs. Can I use the blow 
```
for j in range(100):
     for i in range(3):  # number of mini-batch (step)
            print(""Step %d"" % i)
            val, l = sess.run([img_batch, label_batch])
            print(val.shape, l)
            tl.visualize.images2d(val, second=1, saveable=False, name='batch', dtype=None, fig_idx=2020121)
```",mean range number step print step print program run one epoch whole data want run use blow range range number step print step print,issue,negative,negative,neutral,neutral,negative,negative
341979565,"do you mean how to define the end of an epoch?

I usually use : `n_step` in an epoch = `n_examples/batch_size`",mean define end epoch usually use epoch,issue,negative,negative,negative,negative,negative,negative
341465515,"Thanks for your recommendation! 
Before you suggest, I regard that tensorlayer just has `tl.layers` API...
Very appreciate!!! ",thanks recommendation suggest regard appreciate,issue,positive,positive,positive,positive,positive,positive
341357971,"hi, TL doesn't have a layer to transfer the output vectors to the index form.
For me, I usually get the index from a vector by `tl.nlp.sample` or `tl.nlp.sample_top`.",hi layer transfer output index form usually get index vector,issue,negative,negative,negative,negative,negative,negative
340676214,"The elastric_tranform function uses map_coordinates to do pixel mapping, the function ""map_coordinates"" has a parameter ""mode"" which determines what value will be used to fill the missing pixels. By default, constant mode and 0 filling will be used.",function function parameter mode value used fill missing default constant mode filling used,issue,negative,negative,neutral,neutral,negative,negative
340675016,"Thank you for your response.
I have revised my code, then tensorlayer has made npz file successfully!

```
    with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)
      if ckpt: 
        target_list = []
        for v in tf.global_variables():
          if v.shape != ():
            target_list.append(v.name)
        last_model = ckpt.model_checkpoint_path
        tl.files.load_ckpt(sess=sess, mode_name=last_model, printable=True)
    tl.files.save_npz(target_list, name='{}/cifar10.npz'.format(FLAGS.output_dir), sess=sess)
```",thank response code made file successfully sess,issue,positive,positive,positive,positive,positive,positive
340495473,"yes, you can load ckpt model and then save it to npz.",yes load model save,issue,positive,neutral,neutral,neutral,neutral,neutral
338356773," Got your point and I  added feed_dict in my code as attachment file shows; Program raised: TypeError('The value of a feed cannot be a tf.Tensor object. 'TypeError: The value of a feed cannot be a tf.Tensor object. Acceptable feed values include Python scalars, strings, lists, numpy ndarrays, or TensorHandles. )
So I check Tensorflow API, the return type of iterator.get_next() is Tensor as I though to be.   
So it must be wrong somewhere I don't know. Maybe caused by type of compatibility between Your API and TF1.3.   All that ,Thank you~
[TL_issue.zip](https://github.com/zsdonghao/tensorlayer/files/1403693/TL_issue.zip),
",got point added code attachment file program raised value feed object value feed object acceptable feed include python check return type tensor though must wrong somewhere know maybe type compatibility thank,issue,positive,negative,negative,negative,negative,negative
338201651,"hi, did you enable/disable the dropout layer in feed_dict? like [tutorial_mlp_dropout1.py](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mlp_dropout1.py)

another way is to build 2 graphs for training and testing seperatly. like [tutorial_mlp_dropout2.py](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mlp_dropout2.py)",hi dropout layer like another way build training testing like,issue,positive,neutral,neutral,neutral,neutral,neutral
336462415,"@nicknign i also update the master version on github, please have a try and let us know whether it solve your problem or not.~  thx",also update master version please try let u know whether solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
336400626,"it is a bug, thank you for reporting it.

please have a try on this one.

```python
class Seq2Seq(Layer):
    def __init__(
        self,
        net_encode_in = None,
        net_decode_in = None,
        cell_fn = None,#tf.nn.rnn_cell.LSTMCell,
        cell_init_args = {'state_is_tuple':True},
        n_hidden = 256,
        initializer = tf.random_uniform_initializer(-0.1, 0.1),
        encode_sequence_length = None,
        decode_sequence_length = None,
        initial_state_encode = None,
        initial_state_decode = None,
        dropout = None,
        n_layer = 1,
        # return_last = False,
        return_seq_2d = False,
        name = 'seq2seq',
    ):
        Layer.__init__(self, name=name)
        if cell_fn is None:
            raise Exception(""Please put in cell_fn"")
        if 'GRU' in cell_fn.__name__:
            try:
                cell_init_args.pop('state_is_tuple')
            except:
                pass
        # self.inputs = layer.outputs
        print(""  [**] Seq2Seq %s: n_hidden:%d cell_fn:%s dropout:%s n_layer:%d"" %
              (self.name, n_hidden, cell_fn.__name__, dropout, n_layer))

        with tf.variable_scope(name) as vs:#, reuse=reuse):
            # tl.layers.set_name_reuse(reuse)
            # network = InputLayer(self.inputs, name=name+'/input')
            network_encode = DynamicRNNLayer(net_encode_in,
                     cell_fn = cell_fn,
                     cell_init_args = cell_init_args,
                     n_hidden = n_hidden,
                     initial_state = initial_state_encode,
                     dropout = dropout,
                     n_layer = n_layer,
                     sequence_length = encode_sequence_length,
                     return_last = False,
                     return_seq_2d = True,
                     name = name+'_encode')
            # vs.reuse_variables()
            # tl.layers.set_name_reuse(True)
            network_decode = DynamicRNNLayer(net_decode_in,
                     cell_fn = cell_fn,
                     cell_init_args = cell_init_args,
                     n_hidden = n_hidden,
                     initial_state = (network_encode.final_state if initial_state_decode is None else initial_state_decode),
                     dropout = dropout,
                     n_layer = n_layer,
                     sequence_length = decode_sequence_length,
                     return_last = False,
                     return_seq_2d = return_seq_2d,
                     name = name+'_decode')
            self.outputs = network_decode.outputs

            rnn_variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)

        # Initial state
        self.initial_state_encode = network_encode.initial_state
        self.initial_state_decode = network_decode.initial_state

        # Final state
        self.final_state_encode = network_encode.final_state
        self.final_state_decode = network_decode.final_state

        # self.sequence_length = sequence_length
        self.all_layers = list(network_encode.all_layers)
        self.all_params = list(network_encode.all_params)
        self.all_drop = dict(network_encode.all_drop)

        self.all_layers.extend(list(network_decode.all_layers))
        self.all_params.extend(list(network_decode.all_params))
        self.all_drop.update(dict(network_decode.all_drop))

        self.all_layers.extend( [self.outputs] )
        # self.all_params.extend( rnn_variables )

        self.all_layers = list_remove_repeat(self.all_layers)
        self.all_params = list_remove_repeat(self.all_params)
```",bug thank please try one python class layer self none none none true none none none none dropout none false false name self none raise exception please put try except pas print dropout dropout name reuse network dropout dropout false true name true none else dropout dropout false name initial state final state list list list list,issue,negative,negative,neutral,neutral,negative,negative
336368121,@donproc I also met with this problem. Could you give me a detailed discription on how to finetuning?,also met problem could give detailed,issue,negative,positive,positive,positive,positive,positive
335747935,"Quoting my comment from https://github.com/zsdonghao/tensorlayer/issues/207#issuecomment-335683070:

> Also not sure why we need global variables to prevent layer name collisions, since after all the use of TensorFlow's variable scopes already handles that.
>
> Maybe it's also easier to just pass reuse as a parameter to the constructors of layers, instead of maintaining it as the global state set_keep['name_reuse'], which is almost not respected anywhere (except for TimeDistributedLayer) anyways.

To summarize, I think it might be cleaner to not worry about layer name collisions, and just leave it to TF's varaible scopes. For variable sharing, add a `reuse` boolean flag to the constructors of all layers that supports it. Currently, for example, it doesn't seem possible to have 2 `DenseLayer` instances sharing the same `W` and `b`, but it should be really easy with the extra constructor argument.",comment also sure need global prevent layer name since use variable already maybe also easier pas reuse parameter instead global state almost anywhere except anyways summarize think might cleaner worry layer name leave variable add reuse flag currently example seem possible really easy extra constructor argument,issue,positive,positive,positive,positive,positive,positive
335683070,"Also not sure why we need global variables to prevent layer name collisions, since after all the use of TensorFlow's variable scopes already handles that.

Maybe it's also easier to just pass `reuse` as a parameter to the constructors of layers, instead of maintaining it as the global state `set_keep['name_reuse']`, which is almost not respected anywhere (except for `TimeDistributedLayer`) anyways.",also sure need global prevent layer name since use variable already maybe also easier pas reuse parameter instead global state almost anywhere except anyways,issue,positive,positive,positive,positive,positive,positive
335682156,"Totally agree with the concerns over using print instead of logging. Things like `suppress_stdout` and `disable_print` suppresses stdout globally regardless of whether the prints are from the library, which feels quite overreaching. Logging seems to be more suitable here; see [When to use logging](https://docs.python.org/3/howto/logging.html#when-to-use-logging).

I think we could just add the following to all the modules that needs logging (to create a [logger hierarchy](https://docs.python.org/3/library/logging.html#logger-objects)):
```py
import logging
logger = logging.getLogger(__name__)
```
And use `logger.{debug, info, warning, ...}` instead of `print`, giving users full control over how and the logs are written.",totally agree print instead logging like globally regardless whether library quite overreaching logging suitable see use logging think could add following need logging create logger hierarchy import logging logger use logger warning instead print giving full control written,issue,positive,positive,positive,positive,positive,positive
335619383,"First, make sure you can run TensorFlow in GPU mode.
Then, remove `tf.device` to see whether it run with GPU or not.",first make sure run mode remove see whether run,issue,negative,positive,positive,positive,positive,positive
333116485,"sorry! but the lastest debug ouputs  says ""Start training the network ..."" but no any other info",sorry start training network,issue,negative,negative,negative,negative,negative,negative
333115033,"the exception already told you the reason `Cannot feed value of shape (1, 32, 32, 1) for Tensor 'x_:0', which has shape '(13, 32, 32, 1)`  you should set `batch_size=13` in `fit`",exception already told reason feed value shape tensor shape set fit,issue,positive,positive,positive,positive,positive,positive
332813252,"@haiy For production, as I know, people usually use [TensorFlow Serving](https://www.tensorflow.org/serving/), threading is not necessary. May be I didn't get you point? or there are some reasons threading is better?",production know people usually use serving necessary may get point better,issue,negative,positive,neutral,neutral,positive,positive
332808644,"Hi, if we train different models in different threads we should define different models with different names, then the global name list in `tl.layers` would not effect each other right?

Could you tell me a case that the global variables will effect each other in different threads?

If it is necessary, to manage those global variables, I may use `tl.global_dict` as follow:

`{  'sess_id' : { 'layer_name_list' : [...], 'name_resue': ... }}`",hi train different different define different different global name list would effect right could tell case global effect different necessary manage global may use follow,issue,negative,positive,neutral,neutral,positive,positive
332709114,I second everything @haiy said. Both of those two pieces have bitten us as well.,second everything said two bitten u well,issue,negative,neutral,neutral,neutral,neutral,neutral
332707084,"hi @zsdonghao. it's not about how to suppress print, it's about logging info with more control.If we use logging, then we can config what and where to output .When we want to deploy tl to production environment as service, we need  it to log the normal and exception messages.Besides we found some globals defined in  [layers.py](https://github.com/zsdonghao/tensorlayer/blob/master/tensorlayer/layers.py)

```python
set_keep = globals()
set_keep['_layers_name_list'] =[]
set_keep['name_reuse'] = False

```
,this piece of code make tl hard to deploy as service for multiple users.",hi suppress print logging use logging output want deploy production environment service need log normal exception found defined python false piece code make hard deploy service multiple,issue,negative,negative,negative,negative,negative,negative
332514031,"Hi, to disable the printing, you can use:

```python
>>> print(""You can see me"")
>>> with tl.ops.suppress_stdout():
>>>     print(""You can't see me"")
>>> print(""You can see me"")
```",hi disable printing use python print see print ca see print see,issue,negative,neutral,neutral,neutral,neutral,neutral
331833483,"Thanks a lot, but could you please add some kind of util function to convert npz_dict in old version(which are 0,1,2,3..... named) to new format(named by param names)? This may be done by 1) use the network model, load the weights of old format  2) save those weights in new format",thanks lot could please add kind function convert old version new format param may done use network model load old format save new format,issue,positive,positive,positive,positive,positive,positive
331816760,"Hi, yes.
This is a example to restore and save G and D via `tl.files.load_and_assign_npz` and `tl.files.save_npz`.

https://github.com/zsdonghao/SRGAN/blob/master/main.py
",hi yes example restore save via,issue,positive,neutral,neutral,neutral,neutral,neutral
329430800,"@zsdonghao Thanks for the reply! Sorry that I didn't make it clear. What I meant is 

```
        self.outputs = tf.contrib.layers.layer_norm(self.inputs,
                center=center,
                scale=scale,
                activation_fn=act,
                reuse=reuse,
                variables_collections=variables_collections,
                outputs_collections=outputs_collections,
                trainable=trainable,
                begin_norm_axis=begin_norm_axis,
                begin_params_axis=begin_params_axis,
                scope=name,
                )
        with tf.variable_scope(name) as vs:
            variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)
```",thanks reply sorry make clear meant name,issue,positive,negative,neutral,neutral,negative,negative
329418301,"Hi, use `tf.variable_scope` can help us get the variables using `variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)`, I now change the `scope=None` to `scope='var'`, it looks better",hi use help u get change better,issue,positive,positive,positive,positive,positive,positive
329370048,"@zsdonghao I think the previous implementation that doesn't use tf.variable_scope is more reasonable. 

Current fix would produce `layernorm/LayerNorm/beta` and `layernorm/LayerNorm/gamma` variable names if the `name='layernorm'`. It would be different from other normalization layer behaviors, like BatchNormLayer.",think previous implementation use reasonable current fix would produce variable would different normalization layer like,issue,negative,positive,neutral,neutral,positive,positive
329124440,"Thanks, just fix it in master version",thanks fix master version,issue,negative,positive,positive,positive,positive,positive
328312098,"@zsdonghao I'm sorry that I miss the activation arg in `batchNormLayer`......
It's my fault.....
Thanks for your model script!
And the trick link is a really good demonstration (or tutorial).",sorry miss activation fault thanks model script trick link really good demonstration tutorial,issue,negative,positive,positive,positive,positive,positive
328284249,"Hi, you can put the activation function into `BatchNormLayer`, this is how I do :

- https://github.com/zsdonghao/SRGAN/blob/master/model.py

But I am not sure what do you mean by ""`concatenate the other part of the network`"", here is some tricks summary by others:

- https://github.com/wagamamaz/tensorlayer-tricks

Hope it can solve your problem.",hi put activation function sure mean concatenate part network summary hope solve problem,issue,positive,positive,neutral,neutral,positive,positive
327424856,"Hey - that works great, thank you!",hey work great thank,issue,positive,positive,positive,positive,positive,positive
327331544,"Hi, here is a simple Seq2Seq chatbot example without using `tf.contrib.seq2seq` and attention.
You can clearly see how to iterate the data during training and inferencing.

https://github.com/zsdonghao/seq2seq-chatbot",hi simple example without attention clearly see iterate data training,issue,negative,positive,neutral,neutral,positive,positive
327331332,"You are welcome, feel free to let me know if you want discussion, I also want to know how to make it work~.",welcome feel free let know want discussion also want know make,issue,positive,positive,positive,positive,positive,positive
327330026,"Alright... In fact BatchNorm plays an important role in my network, so maybe I'll just keep using float32 currently.

Thank you anyway and expecting any fix.",alright fact important role network maybe keep float currently thank anyway fix,issue,positive,positive,positive,positive,positive,positive
327327010,"em... I think a better way is to try without BatchNorm, that should be some small issue here.
",em think better way try without small issue,issue,negative,positive,positive,positive,positive,positive
327313456,"Hi,

If I only change the dtype of weight/bias in Conv/DeConv to float16 and left BatchNorm float32, it will throw the `tf.cast()` error. I even tried leave any of the four variables (`beta`, `gamma`, `moving_mean` and `moving_variance`) unchanged but find all of them have to be changed or there will come `tf.cast()` error.

And the placeholder dtype is always set to float16 in all occasions I mentioned above.",hi change float left float throw error even tried leave four beta gamma unchanged find come error always set float,issue,negative,neutral,neutral,neutral,neutral,neutral
327308368,"Hi, i believe changing the weight/bias in Conv/DeConv to float16 would not lead to this error, could you use float32 in BatchNormLayer to see whether it raise this error?

What is your placeholder dtype?",hi believe float would lead error could use float see whether raise error,issue,negative,neutral,neutral,neutral,neutral,neutral
327302002,"Cool thanks! 

Your explanation of using single sequence lengths iteratively to construct the output sequence also made sense. I have something rough working but I'm still tidying it up.",cool thanks explanation single sequence iteratively construct output sequence also made sense something rough working still,issue,positive,positive,neutral,neutral,positive,positive
327300072,"@dbusbridge I am reimplementing my previous Twitter Chatbot, will send the link to you later.",previous twitter send link later,issue,negative,negative,neutral,neutral,negative,negative
327219178,"Hi, i have had a script for this, but can't find it, and I just have a new commit for Seq2Seq.

For inferencing, you will need to reuse the model parameters and set the `decode_sequence_length` to 1 (just like the text generation example). Then after you get the `final_state_encode` of RNN encoder by feeding the encoding text, you feed the `final_state_encode` to Seq2Seq as the `initial_state_decode` of the RNN decoder, by doing this, you can get a new output and a new `final_state_decode`, so you can continuous to feed the `final_state_decode` as `initial_state_decode` and new output as `decode_seqs` to get the next output and `final_state_decode`.

Let me know if you still have problem.",hi script ca find new commit need reuse model set like text generation example get feeding text feed get new output new continuous feed new output get next output let know still problem,issue,negative,positive,positive,positive,positive,positive
327208423,"Hi - thanks for the response!

I'm really sorry, I feel like I'm missing something. In the docs, the main example given
```
>>>     net = Seq2Seq(net_encode, net_decode,
...             cell_fn = tf.contrib.rnn.BasicLSTMCell,
...             n_hidden = 200,
...             initializer = tf.random_uniform_initializer(-0.1, 0.1),
...             encode_sequence_length = retrieve_seq_length_op2(encode_seqs),
...             decode_sequence_length = retrieve_seq_length_op2(decode_seqs),
...             initial_state = None,
...             dropout = None,
...             n_layer = 1,
...             return_seq_2d = True,
...             name = 'seq2seq')
```
sets up a dependency of `net.outputs` upon both `net_encode` and `net_decode`. In that case, it is not possible for the graph to give me a value for `net.outputs` unless `net_encode.outputs` and `net_decode.outputs` both have values.

I would like the `DynamicRNNLayer` acting on `net_encode`, using the inputs to `net_encode` to give a final hidden state. Using this final hidden state, I would like the decoder `DynamicRNNLayer` to give me what it thinks should be the `target_sequence`, without being explicitly fed a target sequence. Absolutely, during training I will feed it the encode, decode and target sequences as appropriate (I have no masks as all of my sequences are the same length), but at inference time, I only want to feed it the encoder sequence, and it's not clear to me how to resolve this. I see that the `tutorial_translate` example can achieve this behaviour, but unfortunately it depends on functionality deprecated since TF 0.12.

Any help you can offer would be greatly appreciated, thanks again!

Dan",hi thanks response really sorry feel like missing something main example given net none dropout none true name dependency upon case possible graph give value unless would like acting give final hidden state final hidden state would like give without explicitly fed target sequence absolutely training feed encode decode target appropriate length inference time want feed sequence clear resolve see example achieve behaviour unfortunately functionality since help offer would greatly thanks dan,issue,positive,positive,neutral,neutral,positive,positive
327201893,"Hi, you can use the standard Seq2seq to do this http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#simple-seq2seq
there is a example in the docs.

However, you still need to tokenize your sentence and feed the data during training by following this link https://arxiv.org/pdf/1409.3215v3.pdf

Hope it helps",hi use standard example however still need sentence feed data training following link hope,issue,negative,neutral,neutral,neutral,neutral,neutral
326922182,"Oh, thank you very much, it would be great if you can make a push request, or you want me to modify it?",oh thank much would great make push request want modify,issue,positive,positive,positive,positive,positive,positive
326841661,"If use `load_and_assign_npz_dict` to restore the parameters saved by `save_npz_dict`. 
In the source code, it only restores the parameters can be found with FLAG `tf.GraphKeys.TRAINABLE_VARIABLES`. So, it can not restore the parameters such as (moving_mean, moving_variance) in BatchNormalization Layer.  

How about replacing it with ""varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=key)""

```python
def load_and_assign_npz_dict(name='model.npz', sess=None):
    """"""Restore the parameters saved by ``tl.files.save_npz_dict()``.

    Parameters
    ----------
    name : a string
        The name of the .npz file.
    sess : Session
    """"""
    assert sess is not None
    params = np.load(name)
    if len(params.keys()) != len(set(params.keys())):
        raise Exception(""Duplication in model npz_dict %s"" % name)
    ops = list()
    for key in params.keys():
        try:
            # tensor = tf.get_default_graph().get_tensor_by_name(key)
            # varlist = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=key)
            # How about check the var in GLOBAL_VARIABLES ?
            varlist = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=key)
            if len(varlist) > 1:
                raise Exception(""[!] Multiple candidate variables to be assigned for name %s"" % key)
            elif len(varlist) == 0:
                raise KeyError
            else:
                ops.append(varlist[0].assign(params[key]))
                print(""[*] params restored: %s"" % key)
        except KeyError:
            print(""[!] Warning: Tensor named %s not found in network."" % key)

    sess.run(ops)
    print(""[*] Model restored from npz_dict %s"" % name)
```
In inference model, with the trainable flag as False, then (beta, gamma) parameters in BN also cannot be restored. ",use restore saved source code found flag restore layer python restore saved name string name file sess session assert sess none name set raise exception duplication model name list key try tensor key check raise exception multiple candidate assigned name key raise else key print key except print warning tensor found network key print model name inference model trainable flag false beta gamma also,issue,positive,negative,neutral,neutral,negative,negative
326836611,"save_npz_dict() solved. 
I found a reference [post](https://stackoverflow.com/questions/32909619/numpy-array-1-9-2-getting-valueerror-could-not-broadcast-input-array-from-shape) from StackOverflow might give some points.",found reference post might give,issue,negative,neutral,neutral,neutral,neutral,neutral
326836281,"someone reported it previously, it is a bug of numpy, so that guy contributed `save_npz_dict`, you can have a look.",someone previously bug guy look,issue,negative,negative,negative,negative,negative,negative
325247837,"GraphQL looks interesting, maybe it has to compare with  ProtocolBuffer : 
   TF is using ProtocolBuffer and it looks better for large data serialization: 
   https://developers.google.com/protocol-buffers/

Additionnally, there is already blaze/odo who supports a certain number of backend including mongo and SQLDB :  https://goo.gl/Vy2859

Few other questions to consider:
   Interface API TensorLayer with TensorDB.
   Interface API TensorDB with Backend (abstraction of backend).
   Scope in term of data pre-processing :  how much in TL side, TensorDB side and Backend side ?
   Data fetching :  Sync / async
                             Local storage of data  : format(s) ?

Maybe, it might easier to discuss in gitter for the details, 
https://gitter.im/

Thanks for starting the contribution, it would help from reinventing the ""wheel"" every time 
we do a new training....









",interesting maybe compare better large data serialization already certain number consider interface interface abstraction scope term data much side side side data fetching sync local storage data format maybe might easier discus thanks starting contribution would help wheel every time new training,issue,positive,positive,positive,positive,positive,positive
325199573,"Difficulty would be to define correctly the abstract layer for differenr back end DB:
   SQLDB  vs NoSQL DB vs filestorage,    
   Streaming data vs No Streaming.

Also differenciattion of :
    Model storage :  
```
           issue with TF dependency ... loading model with TF 2.0 when saved with TF 1.2
           Is it Google TF issue (.ckpt) or external issue ?
           Believe Google would be in better position to manage dependency when saving models....
```

    Input data type :   image/video/text/numerical/
    Input data batch processor : Shuffle / no Shuffle / ...  (batch_data_id)
    

Is there a doc/a way to see the abstract interface methods ?




",difficulty would define correctly abstract layer back end streaming data streaming also model storage issue dependency loading model saved issue external issue believe would better position manage dependency saving input data type input data batch processor shuffle shuffle way see abstract interface,issue,positive,positive,positive,positive,positive,positive
325196966,"Dear arita:
thank you for the comment,
that is exactly I'm doing now.
we have not merge the db code to the Tensorlayer
https://github.com/fangde/TensorLab


",dear thank comment exactly merge code,issue,positive,positive,positive,positive,positive,positive
324309590,"Yes, I will summarise the examples.
I usually use the traditional one, I didn't find any big performance differences on different topologies on image applications.",yes usually use traditional one find big performance different image,issue,negative,negative,neutral,neutral,negative,negative
323730753,"Yes, that is nicely compact and clear, thanks! Maybe it is simpler to just add ResNet example (one that includes the downsampling and padding)? Off topic, but have you found that different ResNet block topologies are more suited to different types of data?


",yes nicely compact clear thanks maybe simpler add example one padding topic found different block different data,issue,positive,positive,positive,positive,positive,positive
323725702,"I am thinking about it as well~ 
But I found there are many kinds of resnet, for me, I found it is easier to build residual blocks by using a for loop. This is a open topic, please let me know, if anyone have suggestion.
 
BTW, I think this resnet example is clear than the one you read.

```python

    n = Conv2d(256, (1,1), (1,1), act=None, W_init=w_init, b_init=None)(n)
    n = BatchNorm2d(decay=0.9, act=tf.nn.relu, gamma_init=g_init)(n)

    # res blocks
    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(n)
    nn = BatchNorm2d(decay=0.9, act=tf.nn.relu, gamma_init=g_init)(nn)
    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(nn)
    nn = BatchNorm2d(decay=0.9, act=None, gamma_init=g_init)(nn)
    n = Elementwise(tf.add)([n, nn])

    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(n)
    nn = BatchNorm2d(decay=0.9, act=tf.nn.relu, gamma_init=g_init)(nn)
    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(nn)
    nn = BatchNorm2d(decay=0.9, act=None, gamma_init=g_init)(nn)
    n = Elementwise(tf.add)([n, nn])

    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(n)
    nn = BatchNorm2d(decay=0.9, act=tf.nn.relu, gamma_init=g_init)(nn)
    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(nn)
    nn = BatchNorm2d(decay=0.9, act=None, gamma_init=g_init)(nn)
    n = Elementwise(tf.add)([n, nn])

    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(n)
    nn = BatchNorm2d(decay=0.9, act=tf.nn.relu, gamma_init=g_init)(nn)
    nn = Conv2d(256, (3,3), (1,1), act=None, W_init=w_init, b_init=None)(nn)
    nn = BatchNorm2d(decay=0.9, act=None, gamma_init=g_init)(nn)
    n = Elementwise(tf.add)([n, nn])
```

More info : 
- https://github.com/zsdonghao/tensorlayer/issues/85
- https://github.com/wagamamaz/tensorlayer-tricks",thinking found many found easier build residual loop open topic please let know anyone suggestion think example clear one read python,issue,positive,positive,positive,positive,positive,positive
322352744,"From the API doc
                      
![image](https://user-images.githubusercontent.com/11583292/29298617-b8ba7db8-819b-11e7-900b-d1a54598f1a5.png)

The problem can be solved by using your local dataset like this:
X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784),path='/home/wuzheng/PycharmProjects/Grammar/data/mnist')

",doc image problem local like,issue,negative,neutral,neutral,neutral,neutral,neutral
321858128,feel free to reopen the discussion,feel free reopen discussion,issue,positive,positive,positive,positive,positive,positive
321515364,"However, after the fix, the behavior of scope is same as other layers.
(You can see the picture in #191 ,where name scope of this layer is toooo much(uncorrect)",however fix behavior scope see picture name scope layer much uncorrect,issue,negative,positive,positive,positive,positive,positive
321507936,"Thank you for the PR~ but this modification will make the layer name incorrect (loss the previous scope), let me have a check~ ",thank modification make layer name incorrect loss previous scope let,issue,negative,negative,negative,negative,negative,negative
321463916,"![image](https://user-images.githubusercontent.com/6689783/29157296-da8255e0-7dd8-11e7-8b59-53554ea3dc91.png)
After the fix.
By the way, it seems like that CI is not configured rightly.   :P",image fix way like rightly,issue,negative,positive,positive,positive,positive,positive
321274543,"@lei-li i make a new commit, https://github.com/zsdonghao/tensorlayer/commit/8b2eaa41215fe6e682d8c69e1a6ce75e4b289ba5

please have a try.",make new commit please try,issue,positive,positive,positive,positive,positive,positive
320984025,"OK, will do a PR tomorrow.

Side note : I went quickly through your docs and you mention in the Testing section of Development that you want a test case for each bug fix. I'm guessing this is no longer required (since i can't find any test in the code)",tomorrow side note went quickly mention testing section development want test case bug fix guessing longer since ca find test code,issue,negative,positive,positive,positive,positive,positive
320955782,"Hi, in my view the changes are all correct, thank you.",hi view correct thank,issue,negative,neutral,neutral,neutral,neutral,neutral
320630628,@geometrikal anything news? let me have a try.,anything news let try,issue,negative,neutral,neutral,neutral,neutral,neutral
320602259,"We use database to handle This

?取 Outlook for Android<https://aka.ms/ghei36>

________________________________
From: Kaiyin Zhong <notifications@github.com>
Sent: Monday, August 7, 2017 9:23:45 AM
To: zsdonghao/tensorlayer
Cc: Subscribed
Subject: [zsdonghao/tensorlayer] Load data from folder (#187)


Would be nice to have something to load folder structures like this:

train_images
├── class1
│   ├── img1
│   ├── img2
│   ├── img3
│   └── img4
└── class2
    ├── img1
    ├── img2
    ├── img3
    └── img4


—
You are receiving this because you are subscribed to this thread.
Reply to this email directly, view it on GitHub<https://github.com/zsdonghao/tensorlayer/issues/187>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AGMKGA_N2Mri8B7TSng_q7gtZbnIcNY4ks5sVsmRgaJpZM4OvIbT>.
",use handle outlook android sent august subject load data folder would nice something load folder like class class thread reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
320004926,"@arita37 hi, if you read TL's code you will find the ""flexibility"" of layer is for TF only at the moment.

### WHY

A small example as follow, you can pass the activation function into a layer like that, so when you have a customized activation function, you can also pass it to the layer.

```python
n = InputLayer(x, name='in')
n = DenseLayer(n, 100, tf.nn.relu, name='d1')
```
But if you want this layer to support other backend like Chainer, you can't pass TF's activation function. So if the layer supports both Chainer and TF, the code to build a layer would become `DenseLayer(n, 100, ""relu"", name='d1')`, and it is not ""transparent"" to TF any more. (to support both TF and Theao, which is similar to the Keras design).

The design philosophy of TL as follow, the goal of our flexibility is for TF only.

- Simplicity : TensorLayer lifts the low-level dataflow interface of TensorFlow to high-level deep learning modules. These modules come with detailed examples that can be deployed in minutes. A user may find it easy to bootstrap with TensorLayer, and then dive into module implementation if need.
- Composability : If possible, deep learning modules should be composed, not built. By offering connectors to TF-Slim and Keras, TensorLayer can be used to glue existing pieces together. This yields a much better time to develop ideas and allows easy module plug-in.
- Flexibility : A deep learning workflow can require many careful tunings. TensorLayer provides the access to the native APIs of TensorFlow and therefore help users to achieve a flexible control within the engine.
- Performance : TensorLayer provides zero-cost compatibility for TensorFlow. It can easily run on heterogeneous platforms or multiple servers while offering native TensorFlow performance.


### SOLUTION

I think the better way to support other backends is to build other sets of layer:

`tl.layers` for TF
`tl.layers_xx` for others

so users can still able to use other toolboxs like `tl.prepro` etc.
",hi read code find flexibility layer moment small example follow pas activation function layer like activation function also pas layer python want layer support like chainer ca pas activation function layer chainer code build layer would become transparent support similar design design philosophy follow goal flexibility simplicity interface deep learning come detailed user may find easy bootstrap dive module implementation need possible deep learning composed built offering used glue together much better time develop easy module flexibility deep learning require many careful access native therefore help achieve flexible control within engine performance compatibility easily run heterogeneous multiple offering native performance solution think better way support build layer still able use like,issue,positive,positive,positive,positive,positive,positive
319959941,"Hello,

You should precise the meaning of ""flexible"" because it says it aims at different backend for DB...


The question why people develop different Deep Learning framework:
Torch, Theano, Neon, Nabble, Tensorflow, Chainer, Caffe, ....
since a CNN / RNN have same equations and  definitions in all of them....????

Reason:
   Computation strategy are different.
   There are benefits from different computation strategy even for same NN.

  Which the same for Database.
  That's why, SQLachemy exists and SQL request exist : to unify data request whatever database software you have behind.


At the definition level (layer), ex: CNN,
from mathematical/formalism, this is same. This is the most important point.


Why not extend TensorLayer as network generator which make it a very versatile framework, even for future not yet released backend ?



















On 3 Aug 2017, at 20:18, Aaron <notifications@github.com> wrote:

@arita37 This is not the purpose of TensorLayer to provide support for different backend. If you do serious deep learning, there is no matter of using which backend. Developers for TensorLayer are aiming at include as many opensource codes as possible to cover different models. If you are interested in particular function in PyTorch, why not contribute for TensorLayer? When TensorLayer mentioned flexible, it means it can build a network architecture easily definitely not support everything.

―
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello precise meaning flexible different question people develop different deep learning framework torch neon chainer since reason computation strategy different different computation strategy even request exist unify data request whatever behind definition level layer ex important point extend network generator make versatile framework even future yet wrote purpose provide support different serious deep learning matter aiming include many possible cover different interested particular function contribute flexible build network architecture easily definitely support everything reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
319941104,"@arita37 This is not the purpose of TensorLayer to provide support for different backend. If you do serious deep learning, there is no matter of using which backend. Developers for TensorLayer are aiming at include as many opensource codes as possible to cover different models. If you are interested in particular function in PyTorch, why not contribute for TensorLayer? When TensorLayer mentioned flexible, it means it can build a network architecture easily definitely not support everything. ",purpose provide support different serious deep learning matter aiming include many possible cover different interested particular function contribute flexible build network architecture easily definitely support everything,issue,positive,positive,neutral,neutral,positive,positive
319939543,"Hello,

That a bit the opposite of tensorlayer purpose....., having flexible framework for deep learning process...

This is not very flexible.....

Can you create layer to add other backend ? (Like PyTorch, Chainer,...)








On 3 Aug 2017, at 19:57, Hao <notifications@github.com> wrote:

Hi @arita37 , TensorLayer is a fully transparent library for TensorFlow only, so I think it may very difficult to have other backend~

―
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub, or mute the thread.

",hello bit opposite purpose flexible framework deep learning process flexible create layer add like chainer hao wrote hi fully transparent library think may difficult reply directly view mute thread,issue,positive,negative,neutral,neutral,negative,negative
319937136,"Hi @arita37 ,  TensorLayer is a fully transparent library for TensorFlow only, so I think it may very difficult to have other backend~
",hi fully transparent library think may difficult,issue,negative,negative,negative,negative,negative,negative
319141908,"great, please make another PR~",great please make another,issue,positive,positive,positive,positive,positive,positive
319141166,"If we return the vocabulary size, i think the back compability may have problem.",return vocabulary size think back may problem,issue,negative,neutral,neutral,neutral,neutral,neutral
319117799,@zsdonghao I will try the relevant scripts when I return to work. I can modify tutorial _cifar10 to use the new method if you agree.,try relevant return work modify tutorial use new method agree,issue,negative,positive,positive,positive,positive,positive
318897822,"@luomai we dont need test case, just make sure all tutorial scripts can run normally.",dont need test case make sure tutorial run normally,issue,negative,positive,positive,positive,positive,positive
318895049,"This PR looks good to me. Thanks for your contribution! @geometrikal 

Could you also provide a simple test case for your program?

@zsdonghao Where shall we put the test case in your project?",good thanks contribution could also provide simple test case program shall put test case project,issue,positive,positive,positive,positive,positive,positive
318776106,"Hi, thanks for your contribution.

As we should maintain back compatibility, apart from `tutorial_cifar10`, are there other tutorial codes need to be modified? (If it is impossible to maintain back compatibility, we can consider to have a new function, like `threading_data_xxx`.)

If your are sure your PR can maintain back compatibility, I will merge it now ~",hi thanks contribution maintain back compatibility apart tutorial need impossible maintain back compatibility consider new function like sure maintain back compatibility merge,issue,positive,positive,neutral,neutral,positive,positive
318429589,"Three ways can do this.

1. get current output `network.outputs`
2. get all outputs `network.all_params`
3. get all outputs by a given layer name `tl.layers.get_layers_with_name(network, ""dense"")`",three way get current output get get given layer name network dense,issue,negative,neutral,neutral,neutral,neutral,neutral
318418965,"@zsdonghao 

Is it possible to go from
    threading_data(self, data=None, fn=None, **kwargs):
to:
    threading_data(self, data=None, thread_count=8, fn=None, **kwargs):

If the user was not using named arguments?

e.g. in the examples you have `threading_data(X, zoom_multi, zoom_range=[0.5, 1], is_random=True)` which I have changed to `threading_data(X, None, zoom_multi, zoom_range=[0.5, 1], is_random=True)`

Or better to have a new function entirely?


",possible go self self user none better new function entirely,issue,negative,positive,positive,positive,positive,positive
318417134,"hi, feel free to reopen this discussion.",hi feel free reopen discussion,issue,positive,positive,positive,positive,positive,positive
318416660,"Hi, if you use CNN just simply follow the CNN example in `tutorial_mnist.py`, make your shape to [3000, 32, 32, 1].

If you use MLP, flatten your data into [3000, 32x32].",hi use simply follow example make shape use flatten data,issue,negative,neutral,neutral,neutral,neutral,neutral
318415334,"@geometrikal Cool, that is great~

Please make sure your new function can compatible with old code, otherwise, other user will need to change their codes.

Many thanks.",cool please make sure new function compatible old code otherwise user need change many thanks,issue,positive,positive,positive,positive,positive,positive
318307330,"I can get even more performance gain using multiprocessing.Pool, will try to add another function
",get even performance gain try add another function,issue,positive,neutral,neutral,neutral,neutral,neutral
318288703,"@luomai batch size of 200 images with size 64 x 64 x 1. I followed the training part of the code from `tutorial_cifar10.py'

 @zsdonghao Yes, batch size of 200, but not that different to batch size of 128 used in `tutorial_cifar10.py'. I will try to make a PR.
",batch size size training part code yes batch size different batch size used try make,issue,negative,neutral,neutral,neutral,neutral,neutral
318102206,"Hi, do you mean if the batch size is big, your implementation is better than the existing one?

If yes, could you make a PR to merge your implementation with the existing one? 

```python
def threading_data(self, data=None, thread_count=None, fn=None, **kwargs):
    if thread_count is None:
        use existing one.
   else:
        use your one.
```

Many thanks.
  ",hi mean batch size big implementation better one yes could make merge implementation one python self none use one else use one many thanks,issue,positive,positive,positive,positive,positive,positive
318096595,"This is an expected behaviors of threading_data as it is not created for applying functions onto massive objects. What is the length of data in your use case?

Yours approach looks good to me, when we are handling a big data array. Could you create a PR for your change?",onto massive length data use case approach good handling big data array could create change,issue,positive,positive,positive,positive,positive,positive
317569176,"@LimberenceCheng , feel free to reopen it if necessary. if you like, you can create your own repo, then I can put a link in this repo.",feel free reopen necessary like create put link,issue,positive,positive,positive,positive,positive,positive
317369659,"@ShieLian  @subzerofun , I re-train the SRGAN with simplified version for 1400 epochs, please have a look. 

From the result, it seems the simplified version is correct.

## Simplified layer:
![train_1400](https://user-images.githubusercontent.com/10713581/28517035-09688d24-705b-11e7-9f32-e2864f49ca8b.png)

## Previous version:
![train_1400_ok](https://user-images.githubusercontent.com/10713581/28517055-1725e27c-705b-11e7-8667-fcbe7e06b7e4.png)

",simplified version please look result simplified version correct simplified layer previous version,issue,negative,negative,negative,negative,negative,negative
317282992,"I think you misunderstood me, i did delete **all model weights – all .npz files** before starting the training again. The comparison from above is a newly trained model with the only difference being the changed tensorlayer version (this PR). Starting from epoch nr. 0 – 175, compared to the old Subpixel-function.

There has to be a difference between the two functions (new/old) – unfortunately i don't understand the math behind it. @ShieLian But what could be the reason – if you say that the (numerical) outcome should be the same? Thank you for the patch, i will test your new version and compare it to the other results (will delete the old weights of course!).

@zsdonghao  Lucky you – the two days on your Titan sound great. It took me 5-6 days on my 1080 Ti to get to epoch 2000.",think misunderstood delete model starting training comparison newly trained model difference version starting epoch old difference two unfortunately understand math behind could reason say numerical outcome thank patch test new version compare delete old course lucky two day sound great took day ti get epoch,issue,positive,positive,positive,positive,positive,positive
317266437,"@subzerofun @subzerofun 
The version compatible with the original version is here: [patch-2](https://github.com/ShieLian/tensorlayer/tree/patch-2)",version compatible original version,issue,negative,positive,positive,positive,positive,positive
317264637,"@subzerofun 
If you train the network starting with original weights, it may have very bad performance.
Try to reset the weights of Conv layer before the first SubConv and weights behind the first SubConv. Train those weights  only.
实际上是因为channel顺序变了，用原来的权值训练很难扳回来。
第一层SubConv之前的Conv之前的权值都可以用，但是这一层Conv要用的话要调一下channel顺序。。",train network starting original may bad performance try reset layer first behind first train,issue,negative,negative,neutral,neutral,negative,negative
317260128,"@subzerofun i am training SRGAN as well, may finish in 1 to 2 days, I will let you know when training finish~ 😊",training well may finish day let know training,issue,negative,neutral,neutral,neutral,neutral,neutral
317259929,"I'm sorry, but i have no idea about the mathematics behind the changes. But i have started the SRGAN training from scratch – with the new code. And when i compare the quality and output progress of the new generated images to the old ones, the newer ones clearly are worse. Don't know why that is, but it has to be because of this change – since i did not modify anything else in the SRGAN repo.

Take a look for yourself: here are two pictures, both from training epoch nr. 175. The new output image has a lot more artifacts and some parts again show the checkerboard pattern. Just look at the palm leaves – the results are clearly not the same. I will leave the training running for a while, but i don't think that the new model will reach the same fidelity as the old one. 

### Old pixelshuffler code:
![0175_2000 _train](https://user-images.githubusercontent.com/17863119/28500471-34f01c92-6fc9-11e7-9bb5-aef9d18bda92.png)

### New code:
![new](https://user-images.githubusercontent.com/17863119/28500476-44dd2a1e-6fc9-11e7-8d0d-08f541cc3a22.png)


Could you please check the output again?",sorry idea mathematics behind training scratch new code compare quality output progress new old clearly worse know change since modify anything else take look two training epoch new output image lot show checkerboard pattern look palm leaf clearly leave training running think new model reach fidelity old one old code new code new could please check output,issue,positive,negative,neutral,neutral,negative,negative
317249895,"Hi all, please cite this paper if you find it is useful in your project.

```
@article{TensorLayer2017,
author = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},
journal = {ACM Multimedia},
title = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},
url = {http://tensorlayer.org},
year = {2017}
}
```",hi please cite paper find useful project article author dong hao journal title versatile library efficient deep learning development year,issue,positive,positive,positive,positive,positive,positive
316974554,"thanks for your link, i already merged this function into `SubpixelConv1d`, please have a try.",thanks link already function please try,issue,positive,positive,positive,positive,positive,positive
316964728,"![image](https://user-images.githubusercontent.com/6689783/28459505-55ca27c6-6e40-11e7-8867-5b256f7f3fba.png)
According to the paper, PS(T)_1,0,0=T_0,0,1=1
So accurately my implement is strictly same as the paper's. 
But I will try to provide the version act the same as tensorlayer.
It's your turn to determine use which method. @zsdonghao ",image according paper accurately implement strictly paper try provide version act turn determine use method,issue,negative,positive,positive,positive,positive,positive
316962862,"For my local implement, If the Input(1x2x2x4,Batch,H,W,C) is
[
[ [0,1,2,3] , [4,5,6,7] ],
[ [8,9,10,11] , [12,13,14,15] ]
]
Then the output is
[
 [ [0], [1], [4], [5]],
 [ [2], [3], [6], [7]],
 [ [8], [9],[12],[13]],
 [[10],[11],[14],[15]]
]
While the original output is
[
 [ [0], [2], [4], [6]],
 [ [1], [3], [5], [7]],
 [ [8], [10],[12],[14]],
 [ [9],[11],[13],[15]]
]
May that's why the same weights differ @subzerofun 
But for a completely new trained model, it will give the equivalent result.",local implement input batch output original output may differ completely new trained model give equivalent result,issue,negative,positive,positive,positive,positive,positive
316927880,"@ShieLian @zsdonghao I'm still experimenting with the [SRGAN project](https://github.com/zsdonghao/SRGAN) (the results are amazing!) and have also mentioned the slow model building time in the Subpixel-Conv2D layers in an issue. But i thought that was because of a CPU-core/speed bottleneck on my side (i have an older i7 Quad core and thought you need a bazillion-core xeon-machine for this kind of computation).

So i was glad to see a solution to the problem! But as @zsdonghao is assuming – @ShieLian: your fix changes the image quality of the GANs output drastically. You can see a **checkerboard pattern** in the generated sample images as well as **color artifacts** and an overall **low res, blurry output**. I have tried letting the training run anew for a few epochs because the model weights from the previous training (checkpoint) seem to reset after the code change. But the resolution (sharp edges, high frequency details) does not get better and the checkerboard pattern is also persistent in all further results.

@ShieLian If your code would produce the same output as before i should see no change at all in my generated images and should be able to use the same weights as before – or am i wrong here?

If i revert the Subpixel-code (and model weights) the output images are clear and sharp again. I currently don't have access to the machine with my local repo, but will upload the images later so you can maybe check why the artifacts happen.

The model buildup time is nearly instantaneous with this PR – would love to see it producing the same results as the original code!",still project amazing also slow model building time issue thought bottleneck side older quad core thought need kind computation glad see solution problem assuming fix image quality output drastically see checkerboard pattern sample well color overall low blurry output tried training run anew model previous training seem reset code change resolution sharp high frequency get better checkerboard pattern also persistent code would produce output see change able use wrong revert model output clear sharp currently access machine local later maybe check happen model buildup time nearly instantaneous would love see original code,issue,positive,positive,positive,positive,positive,positive
316578977,"Yes, Please refer to this one **AUDIO SUPER-RESOLUTION USING NEURAL NETS**. And, here is their implementation of 1d subpixel, https://github.com/kuleshov/audio-super-res/blob/master/src/models/layers/subpixel.py",yes please refer one audio neural implementation,issue,positive,neutral,neutral,neutral,neutral,neutral
316391390,Do you have any paper that relate to subpixel convolution on 1D data?,paper relate convolution data,issue,negative,neutral,neutral,neutral,neutral,neutral
315557230,"Great. What I worry is your implementation have different shuffle method with the original paper. Please double check, many thanks.",great worry implementation different shuffle method original paper please double check many thanks,issue,positive,positive,positive,positive,positive,positive
315556468,"emm...
Sorry for didn't check the code before commit.(for readability, codes here are different from my local version)
w and h should be a and b, or just change a and  b to w and h.
If you'd like to do this quick fix, it'will be helpful. Or I'll do it torrow.(东八区)",sorry check code commit readability different local version change like quick fix helpful,issue,positive,negative,neutral,neutral,negative,negative
315555483,"Hi, are you sure the code is correct? i cant found where to define `h` and `w` in the code.
To test whether the implementation is correct, I suggest to test it on [SRGAN example](https://github.com/zsdonghao/SRGAN), do their also output the image with checkerboard artifact at the beginning?",hi sure code correct cant found define code test whether implementation correct suggest test example also output image checkerboard artifact beginning,issue,negative,positive,positive,positive,positive,positive
315156947,"Thank you for your advice. As it should be your credit, It is possible that you can contribute your code via push request?
Many thanks.",thank advice credit possible contribute code via push request many thanks,issue,positive,positive,positive,positive,positive,positive
314817606,"thank you for reporting this, i make a new commit to fix this problem, please check [this](https://github.com/zsdonghao/tensorlayer/commit/2a2422934b33ec189453cbc54e442b910c3cc2f1) if you like.",thank make new commit fix problem please check like,issue,positive,positive,positive,positive,positive,positive
313779218,"Thanks for reply, please can you give more details because I am new in this environment. ",thanks reply please give new environment,issue,positive,positive,positive,positive,positive,positive
313718022,"make the image shape to be 3 D
img = np.asarray(img)
img.shape = 32, 32, 1

and make the dataset to be 4 D:  [n_examples, 32, 32, 1]
",make image shape make,issue,negative,neutral,neutral,neutral,neutral,neutral
309426113,"this problem is the same as this one :  https://github.com/zsdonghao/text-to-image/issues/2

please have a look.

actually if you use TF1.2 , it will automatically reuse the RNN cell.~",problem one please look actually use automatically reuse,issue,negative,neutral,neutral,neutral,neutral,neutral
309349424,"In the other words, EmbeddingAttentionSeq2seqWrapper function which like the project ""easy_seq2seq"" is too low in TF-version",function like project low,issue,negative,neutral,neutral,neutral,neutral,neutral
309347994,"if I want to input the embedding matrix which trained by myself with the other API(like Gensim), I don't want use embedding layer in TL even TF, I want call the clear attention seq2seq or Peekly seq2seq directly.",want input matrix trained like want use layer even want call clear attention directly,issue,positive,positive,neutral,neutral,positive,positive
308640289,@zsdonghao https://github.com/zsdonghao/tensorlayer/blob/ReFUEL4/tensorlayer/files.py#L681 saves anything and everything in compressed format.,anything everything compressed format,issue,negative,neutral,neutral,neutral,neutral,neutral
308383381,"i mark this issue as `help wanted`, if any one used TF's dynamic RNN encoder and attention seq2seq, feel free to contribute ~",mark issue help one used dynamic attention feel free contribute,issue,positive,positive,positive,positive,positive,positive
308377715,"hi, you are going to save params into a dict, and load from the dict?

could we build two new functions for that? like `save_npz_dict` and `load_npz_dict`?",hi going save load could build two new like,issue,positive,positive,positive,positive,positive,positive
308300522,"It is confused when network is set to be None and params is empty. It may be good when sess is the last param, but it will break old code in this case.

Yes, in my test case:)  There is 5 param case in my test, they works well when assigned many times.",confused network set none empty may good sess last param break old code case yes test case param case test work well assigned many time,issue,negative,positive,positive,positive,positive,positive
308152132,"we may need to set default sess to None, e.g. `def assign_params(sess=None, params=[], network=None)` ~?

but one problem is that if we re-run this ops, it will assign the same params right?",may need set default sess none one problem assign right,issue,negative,positive,positive,positive,positive,positive
308090388,"Always return ops, auto run when session is given:)

```python
def assign_params(sess, params, network):
    """"""Assign the given parameters to the TensorLayer network.

    Parameters
    ----------
    sess : TensorFlow Session
    params : a list
        A list of parameters in order.
    network : a :class:`Layer` class
        The network to be assigned

    Examples
    --------
    >>> Save your network as follow:
    >>> tl.files.save_npz(network.all_params, name='model_test.npz')
    >>> network.print_params()
    ...
    ... Next time, load and assign your network as follow:
    >>> sess.run(tf.initialize_all_variables()) # re-initialize, then save and assign
    >>> load_params = tl.files.load_npz(name='model_test.npz')
    >>> tl.files.assign_params(sess, load_params, network)
    >>> network.print_params()

    References
    ----------
    - `Assign value to a TensorFlow variable <http://stackoverflow.com/questions/34220532/how-to-assign-value-to-a-tensorflow-variable>`_
    """"""
    ops = []
    for idx, param in enumerate(params):
        ops.append(network.all_params[idx].assign(param))
    if sess is not None:
        sess.run(ops)
    return ops
```",always return auto run session given python sess network assign given network sess session list list order network class layer class network assigned save network follow next time load assign network follow save assign sess network assign value variable param enumerate param sess none return,issue,positive,neutral,neutral,neutral,neutral,neutral
308086253,"i see, when `sess=None` what will the code look like?",see code look like,issue,negative,neutral,neutral,neutral,neutral,neutral
308085662,@zsdonghao the error would still be raised. ,error would still raised,issue,negative,neutral,neutral,neutral,neutral,neutral
308082328,"Or something like: sess=None, when user want run the ops manually? Is this a good idea?",something like user want run manually good idea,issue,positive,positive,positive,positive,positive,positive
308081479,"thank you for reporting this.

Could we set all params to list before saving them?

`np.savez('test.npz', [list(t1),list(t2),list(t3)])`",thank could set list saving list list list,issue,negative,neutral,neutral,neutral,neutral,neutral
308080941,"yes, we could set a default `assign_ops=None`",yes could set default,issue,negative,neutral,neutral,neutral,neutral,neutral
306462754,"Thanks, I think I understand. Your suggested method makes more sense when other layers that also need to be disabled are used.
",thanks think understand method sense also need disabled used,issue,negative,neutral,neutral,neutral,neutral,neutral
306440438,"With multiple RNN layer, we only need to add `input_keep_prob ` to the hidden outputs between different RNN layers. In the last RNN layer, we can see we add `output_keep_prob` as below:

```python
        if dropout:
            self.cell = DropoutWrapper_fn(self.cell,
                      input_keep_prob=1.0, output_keep_prob=out_keep_prob)
```",multiple layer need add hidden different last layer see add python dropout,issue,negative,negative,neutral,neutral,negative,negative
306151162,"thank you for your report ~
could you tell me your operation system, python version, tensorflow and tensorlayer version? 
then i can have a check.
many thanks",thank report could tell operation system python version version check many thanks,issue,positive,positive,positive,positive,positive,positive
305785996,"Hi, you can set the dropout keep prob in `DropoutLayer` without placeholder, simply set `is_fix` to True, please check: https://github.com/wagamamaz/tensorlayer-tricks

",hi set dropout keep prob without simply set true please check,issue,negative,positive,positive,positive,positive,positive
302664409,"OK, thanks, if my paper can be published soon, you're welcome to cite it. ",thanks paper soon welcome cite,issue,positive,positive,positive,positive,positive,positive
302663497,"==== new answer ====

```
@article{TensorLayer2017,
author = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},
journal = {ACM Multimedia},
title = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},
url = {http://tensorlayer.org},
year = {2017}
}
```


==== old answer ====
it's ok ~ 

we are writing a white paper, so we can also cite your work if you like ~


",new answer article author dong hao journal title versatile library efficient deep learning development year old answer writing white paper also cite work like,issue,positive,positive,neutral,neutral,positive,positive
302661988,"I mimic the way how to cite Keras like this:
```
@misc{hao2016tensorlayer,
	title={TensorLayer},
	author={Dong, Hao and others},
	year={2016},
	publisher={GitHub},
	howpublished={\url{https://github.com/zsdonghao/tensorlayer}},
}
```
Is this OK?
",mimic way cite like dong hao,issue,negative,neutral,neutral,neutral,neutral,neutral
301666480,@zsdonghao Sorry for the deletion of the blank lines. I did not do it intentionally. I guess it's somehow done by the IDE (Pycharm) I used. I have fixed it now. And I have run it on TensorFlow 1.0. It went well. Thanks.,sorry deletion blank intentionally guess somehow done ide used fixed run went well thanks,issue,positive,negative,neutral,neutral,negative,negative
301605545,"@CTTC , Many thanks for your quick PR.

After a quick check, I find you delete all blank lines between `Parameters`, `Examples`, `Variables` etc, it will cause readthedocs fail to build the document~  Could you keep the original document format (keep the blank lines)?

For the `cell_creator` can it also well work under TF1.0?

Many thanks",many thanks quick quick check find delete blank cause fail build could keep original document format keep blank also well work many thanks,issue,positive,positive,positive,positive,positive,positive
301342544,"don't worry.
if you got error in second run, this mean you have not clear everything (variables, layer names etc) before your second one.

to remove layer names please use `tl.layers.clear_layers_name()` before the second run, if you got some expections about variable exists , you also need to clear the tensorflow variables.

however, if people want to run a script twice, people usually close the python process and run again.",worry got error second run mean clear everything layer second one remove layer please use second run got variable also need clear however people want run script twice people usually close python process run,issue,negative,negative,neutral,neutral,negative,negative
301319788,"i m really sorry to disturb you,i just copy the file 'tutorial_cifar10.py' in your examples,the first time i run the code everything is ok,while the second gives an error just like i said.In the second time i just change the learning rate ,so if there is nothing wrong,does it means i have to reopen my spdyer another time if i  just want to change the params.It is so troublesome. <br>
Then i find it worked  through changing the `with tf.variable_scope(""mode2"", reuse=reuse)`,from model1 to model2 .<br>
Just wonder anyother methods to solve this questions.thank you for responsing when i asked so stupid questions.I love tensorlayer. @zsdonghao ",really sorry disturb copy file first time run code everything second error like second time change learning rate nothing wrong reopen another time want change troublesome find worked mode model model wonder solve stupid love,issue,negative,negative,negative,negative,negative,negative
301318125,"there is not a bug, you need to reuse your parameters if you want to use a model twice. please check the link i send you.",bug need reuse want use model twice please check link send,issue,negative,neutral,neutral,neutral,neutral,neutral
301315626,"thanks for your concern.it seems that i can only run this for a single time ,in the next time i run the code,it get the same error.if i close my spyder and reopen it ,the code can be run. is it a bug?@zsdonghao",thanks run single time next time run code get close reopen code run bug,issue,negative,positive,neutral,neutral,positive,positive
301315257,"you need to change your code from:
```python
network, cost, _ = model(x, y_,is_train=True,reuse=True)
_, cost_test, acc = model(x, y_, is_train=False,reuse=True)
```
to:
```python
network, cost, _ = model(x, y_,is_train=True,reuse=False)
_, cost_test, acc = model(x, y_, is_train=False,reuse=True)
```

and from:
```python
    with tf.variable_scope(""model1"", reuse=reuse):
        network = tl.layers.InputLayer(x,name='input')
```
to:
```python
    with tf.variable_scope(""model1"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        network = tl.layers.InputLayer(x,name='input')
```
      

please check: https://github.com/wagamamaz/tensorlayer-tricks
",need change code python network cost model model python network cost model model python model network python model reuse network please check,issue,negative,neutral,neutral,neutral,neutral,neutral
301299551,"@cai-lw thank you for your report, do you mean you would like to change 
`self.outputs = tf.reshape(tf.concat(outputs, 1), [-1, n_hidden])` to `self.outputs = tf.reshape(tf.concat(outputs, 1), [-1, tf.shape(outputs)[-1]])` ?",thank report mean would like change,issue,positive,negative,negative,negative,negative,negative
301297265,@zsdonghao  Sorry for the late reply. I'm happy to make a pull request. I will submit it ASAP.,sorry late reply happy make pull request submit,issue,positive,neutral,neutral,neutral,neutral,neutral
301059122,"@CTTC Many thanks for your report.

Could you make a push request, because this is your contribution.

To make it works for all version, you may like to change as follow.
```python
try: # TF1.1
     xxx
except:  # TF1.0
     xxx
```

Thanks in advance.",many thanks report could make push request contribution make work version may like change follow python try except thanks advance,issue,positive,positive,positive,positive,positive,positive
299879836,"oh, i forgot to send you this link : https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_inceptionV3_tfslim.py",oh forgot send link,issue,negative,neutral,neutral,neutral,neutral,neutral
299610751,"Never mind, the connection to keras works, but it is still better if the upsampling 3d layer could be implemented. ",never mind connection work still better layer could,issue,negative,positive,positive,positive,positive,positive
299304732,"Thanks! This doesn't work very well for me, because I set up multiple independent running separately, the scripts don't whether the model is available or not. Anyway, at last I figured out the solution, that is, just reset the default graph using:
```
tf.reset_default_graph()
```",thanks work well set multiple independent running separately whether model available anyway last figured solution reset default graph,issue,positive,positive,positive,positive,positive,positive
299301838,"if you want to reuse your model, see section 3 in this repo: https://github.com/wagamamaz/tensorlayer-tricks",want reuse model see section,issue,negative,neutral,neutral,neutral,neutral,neutral
299301548,"if you get a variables list in order, you can use `tl.files.assign`;  if you get a ckpt, you can use tensorflow 's method.",get list order use get method,issue,negative,neutral,neutral,neutral,neutral,neutral
299080989,"Yes, I could successfully run mnist and other nlp examples (ptb_lstm, generate_text, & translate etc.) on GPUs.",yes could successfully run translate,issue,positive,positive,positive,positive,positive,positive
298892737,i have no idea about it. can you make sure you machine work well with the mnist first?  https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist_simple.py,idea make sure machine work well first,issue,positive,positive,positive,positive,positive,positive
298864113,"It works by disabling GPUs. 
sess = tf.InteractiveSession(config=tf.ConfigProto(device_count={'GPU':0}))
Is it possible to fix the errors and make it work on GPUs?",work sess possible fix make work,issue,negative,neutral,neutral,neutral,neutral,neutral
298842536,"I logged the device placement. 
sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True))
-----------------------
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:02:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a47a50
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:03:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a4b3d0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:82:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
W tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x3a4ed50
I tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: 
name: TITAN X (Pascal)
major: 6 minor: 1 memoryClockRate (GHz) 1.531
pciBusID 0000:83:00.0
Total memory: 11.90GiB
Free memory: 11.76GiB
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 2
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 3
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 2 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 0
I tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 3 and 1
I tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y N N 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   N N Y Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0)
I tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0)
Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0
/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0
/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0
/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0
I tensorflow/core/common_runtime/direct_session.cc:257] Device mapping:
/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0
/job:localhost/replica:0/task:0/gpu:1 -> device: 1, name: TITAN X (Pascal), pci bus id: 0000:03:00.0
/job:localhost/replica:0/task:0/gpu:2 -> device: 2, name: TITAN X (Pascal), pci bus id: 0000:82:00.0
/job:localhost/replica:0/task:0/gpu:3 -> device: 3, name: TITAN X (Pascal), pci bus id: 0000:83:00.0

Load or Download matt_mahoney_text8 Dataset> data/mm_test8/
('Data size', 17005207)
132853 Steps a Epoch, total Epochs 20
   learning_rate: 1.000000
   batch_size: 128
()
Real vocabulary size    253854
Limited vocabulary size 50000
('Most 5 common words (+UNK)', [['_UNK', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)])
('Sample data', [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156], ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'])
()
(12, 'as', '->', 195, 'term')
(12, 'as', '->', 5239, 'anarchism')
(12, 'as', '->', 6, 'a')
(12, 'as', '->', 3084, 'originated')
(6, 'a', '->', 2, 'of')
(6, 'a', '->', 3084, 'originated')
(6, 'a', '->', 195, 'term')
(6, 'a', '->', 12, 'as')
(195, 'term', '->', 12, 'as')
(195, 'term', '->', 2, 'of')
(195, 'term', '->', 6, 'a')
(195, 'term', '->', 3137, 'abuse')
(2, 'of', '->', 3137, 'abuse')
(2, 'of', '->', 195, 'term')
(2, 'of', '->', 46, 'first')
(2, 'of', '->', 6, 'a')
(3137, 'abuse', '->', 195, 'term')
(3137, 'abuse', '->', 2, 'of')
(3137, 'abuse', '->', 59, 'used')
(3137, 'abuse', '->', 46, 'first')
()
  [TL] Word2vecEmbeddingInputlayer word2vec_layer: (50000, 128)
()
word2vec_layer/nce_biases/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Adagrad/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/TruncatedNormal: (TruncatedNormal)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal: (Add)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/RandomUniform: (RandomUniform): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/RandomUniform: (RandomUniform)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform: (Add)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Assign: (Assign): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Assign: (Assign)/job:localhost/replica:0/task:0/gpu:0
init: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] init: (NoOp)/job:localhost/replica:0/task:0/gpu:0
Const_4: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Const_4: (Const)/job:localhost/replica:0/task:0/gpu:0
Const_3: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Const_3: (Const)/job:localhost/replica:0/task:0/gpu:0
Const_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Const_2: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases/Initializer/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Initializer/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal/stddev: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/stddev: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal/mean: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/mean: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Initializer/truncated_normal/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Initializer/truncated_normal/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/max: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/max: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/min: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/min: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Initializer/random_uniform/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Initializer/random_uniform/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
_send_word2vec_layer/embeddings_0: (_Send): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/embeddings_0: (_Send)/job:localhost/replica:0/task:0/cpu:0
  param   0: (50000, 128)    (mean: -0.000195725995582, median: -0.000534415245056, std: 0.577156186104    )   word2vec_layer/embeddings:0
word2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
_send_word2vec_layer/nce_weights_0: (_Send): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/nce_weights_0: (_Send)/job:localhost/replica:0/task:0/cpu:0
  param   1: (50000, 128)    (mean: -9.69215670921e-06, median: 2.75921775028e-05 , std: 0.0777502208948   )   word2vec_layer/nce_weights:0
word2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
_send_word2vec_layer/nce_biases_0: (_Send): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _send_word2vec_layer/nce_biases_0: (_Send)/job:localhost/replica:0/task:0/cpu:0
  param   2: (50000,)        (mean: 0.0               , median: 0.0               , std: 0.0               )   word2vec_layer/nce_biases:0
  num of params: 12850000
  layer 0: Tensor(""word2vec_layer/embedding_lookup:0"", shape=(128, 128), dtype=float32)
50000 vocab saved to vocab_text8.txt in /home/omnisky/app/sourceCode/tensorlayer/example
word2vec_layer/nce_biases/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/Adagrad: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/Adagrad: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Prod_1: (Prod): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Prod_1: (Prod)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Maximum: (Maximum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Maximum: (Maximum)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Prod: (Prod): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Prod: (Prod)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/floordiv: (FloorDiv): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/floordiv: (FloorDiv)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Cast: (Cast): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Cast: (Cast)/job:localhost/replica:0/task:0/gpu:0
gradients/Fill: (Fill): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Fill: (Fill)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Tile: (Tile): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Tile: (Tile)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/truediv: (RealDiv): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/truediv: (RealDiv)/job:localhost/replica:0/task:0/gpu:0
gradients/Reshape_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Reshape_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
ones: (Fill): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] ones: (Fill)/job:localhost/replica:0/task:0/gpu:0
gradients/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ones_like: (Fill): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like: (Fill)/job:localhost/replica:0/task:0/gpu:0
nce_loss/truediv: (RealDiv): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/truediv: (RealDiv)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/mod: (FloorMod): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/mod: (FloorMod)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/ConcatOffset: (ConcatOffset): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/ConcatOffset: (ConcatOffset)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_3: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack_2: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_2: (Pack)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack: (Pack)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_biases/read: (Identity): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_biases/read: (Identity)/job:localhost/replica:0/task:0/cpu:0
word2vec_layer/nce_weights: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/nce_weights/read: (Identity): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/nce_weights/read: (Identity)/job:localhost/replica:0/task:0/cpu:0
word2vec_layer/embeddings: (VariableV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings: (VariableV2)/job:localhost/replica:0/task:0/gpu:0
word2vec_layer/embeddings/read: (Identity): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embeddings/read: (Identity)/job:localhost/replica:0/task:0/cpu:0
nce_loss/Cast: (Cast): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Cast: (Cast)/job:localhost/replica:0/task:0/gpu:0
nce_loss/LogUniformCandidateSampler: (LogUniformCandidateSampler): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/LogUniformCandidateSampler: (LogUniformCandidateSampler)/job:localhost/replica:0/task:0/cpu:0
nce_loss/Log_1: (Log): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Log_1: (Log)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Log: (Log): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Log: (Log)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat: (ConcatV2): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat: (ConcatV2)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/embedding_lookup_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/nce_weights/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/embedding_lookup_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/nce_biases/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0
nce_loss/embedding_lookup_1: (Gather): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/embedding_lookup_1: (Gather)/job:localhost/replica:0/task:0/cpu:0
nce_loss/Slice_3: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_3: (Slice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice_1: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_1: (Slice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_5: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_5: (Reshape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/embedding_lookup: (Gather): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/embedding_lookup: (Gather)/job:localhost/replica:0/task:0/cpu:0
nce_loss/Slice_2: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_2: (Slice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice: (Slice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/concat: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/concat: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape_2: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_2: (Shape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_1: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_2: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_1: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Shape_1: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Shape_1: (Shape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/embeddings/Unique: (Unique): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/Unique: (Unique)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/Shape: (Shape): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/Shape: (Shape)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/strided_slice: (StridedSlice): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice: (StridedSlice)/job:localhost/replica:0/task:0/cpu:0
word2vec_layer/embedding_lookup: (Gather): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] word2vec_layer/embedding_lookup: (Gather)/job:localhost/replica:0/task:0/cpu:0
nce_loss/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
nce_loss/add_1: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/add_1: (Add)/job:localhost/replica:0/task:0/gpu:0
nce_loss/sub_1: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/sub_1: (Sub)/job:localhost/replica:0/task:0/gpu:0
nce_loss/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_4: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_4: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ExpandDims: (ExpandDims): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ExpandDims: (ExpandDims)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_2: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_2: (Reshape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape_3: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_3: (Shape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_2: (StridedSlice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2: (StridedSlice)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack_1: (Pack): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_1: (Pack)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ones: (Fill): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones: (Fill)/job:localhost/replica:0/task:0/gpu:0
nce_loss/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_3_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_3_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_3: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_3: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_4_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_4_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_4: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_4: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Shape: (Shape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Shape: (Shape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/BroadcastGradientArgs: (BroadcastGradientArgs)/job:localhost/replica:0/task:0/gpu:0
nce_loss/add: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/add: (Add)/job:localhost/replica:0/task:0/gpu:0
nce_loss/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_3: (ConcatV2): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_3: (ConcatV2)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/zeros_like: (ZerosLike): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/zeros_like: (ZerosLike)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/GreaterEqual: (GreaterEqual): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/GreaterEqual: (GreaterEqual)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/Exp: (Exp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Exp: (Exp)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/Log1p: (Log1p): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Log1p: (Log1p)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/Select: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/Select: (Select)/job:localhost/replica:0/task:0/gpu:0
sampled_losses/sub: (Sub): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses/sub: (Sub)/job:localhost/replica:0/task:0/gpu:0
sampled_losses: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] sampled_losses: (Add)/job:localhost/replica:0/task:0/gpu:0
gradients/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Log1p_grad/add: (Add): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/add: (Add)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Log1p_grad/Reciprocal: (Reciprocal): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/Reciprocal: (Reciprocal)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Log1p_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Exp_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Exp_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/Select: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/Select: (Select)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Neg_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Neg_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_grad/Select_1: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/Select_1: (Select)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_grad/Select: (Select): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/Select: (Select)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Select_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Select_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/AddN: (AddN): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN: (AddN)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/Slice_1: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Slice_1: (Slice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/Slice: (Slice): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Slice: (Slice)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_1_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_1_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_1_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_1_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_1_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_1_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Neg: (Neg): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Neg: (Neg)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_5_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_5_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0
gradients/AddN_1: (AddN): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_1: (AddN)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/nce_biases/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/add_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_4_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_4_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_3_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_3_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_grad/MatMul_1: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/MatMul_1: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_grad/MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/MatMul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/MatMul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_2_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_2_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/mul_1: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/mul_1: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Sum_1: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Sum_1: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Reshape_1: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Reshape_1: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/mul: (Mul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/mul: (Mul)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Sum: (Sum): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Sum: (Sum)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/tuple/group_deps: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/group_deps: (NoOp)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/tuple/control_dependency_1: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/control_dependency_1: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_1_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_1_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Pad: (Pad): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Pad: (Pad)/job:localhost/replica:0/task:0/gpu:0
gradients/AddN_3: (AddN): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_3: (AddN)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/nce_weights/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/Mul_grad/tuple/control_dependency: (Identity): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/tuple/control_dependency: (Identity)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/ExpandDims_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/ExpandDims_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
gradients/AddN_2: (AddN): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/AddN_2: (AddN)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum: (UnsortedSegmentSum): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum: (UnsortedSegmentSum)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/SparseApplyAdagrad: (SparseApplyAdagrad): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/SparseApplyAdagrad: (SparseApplyAdagrad)/job:localhost/replica:0/task:0/cpu:0
Adagrad: (NoOp): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad: (NoOp)/job:localhost/replica:0/task:0/gpu:0
MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
Reshape: (Reshape): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Reshape: (Reshape)/job:localhost/replica:0/task:0/gpu:0
Mean: (Mean): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Mean: (Mean)/job:localhost/replica:0/task:0/gpu:0
_recv_Placeholder_0: (_Recv): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _recv_Placeholder_0: (_Recv)/job:localhost/replica:0/task:0/cpu:0
_recv_Placeholder_1_0: (_Recv): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _recv_Placeholder_1_0: (_Recv)/job:localhost/replica:0/task:0/cpu:0
_send_Mean_0: (_Send): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] _send_Mean_0: (_Send)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_biases/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/nce_weights/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/update_word2vec_layer/embeddings/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/update_word2vec_layer/embeddings/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/cpu:0
Adagrad/learning_rate: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Adagrad/learning_rate: (Const)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/embedding_lookup_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/Slice_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/word2vec_layer/embedding_lookup_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/word2vec_layer/embedding_lookup_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/ExpandDims_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/ExpandDims_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Mul_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_2_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_2_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/Size: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Size: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/embedding_lookup_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/embedding_lookup_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/cpu:0
gradients/nce_loss/Slice_2_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_2_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_2_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_1_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_1_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/concat/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/concat/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Slice_3_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Slice_3_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/Reshape_5_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/Reshape_5_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/add_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/add_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_1_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_1_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/sub_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/sub_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/nce_loss/concat_3_grad/Rank: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/nce_loss/concat_3_grad/Rank: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/mul_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/mul_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/sub_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/sub_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Reshape_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Reshape_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Maximum/y: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Maximum/y: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Const_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Const_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Tile/multiples: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Tile/multiples: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Mean_grad/Reshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Mean_grad/Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
Const_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Const_1: (Const)/job:localhost/replica:0/task:0/gpu:0
Reshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
ones/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] ones/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_4/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_4/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/truediv/y: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/truediv/y: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ones_like/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ones_like/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones_like/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_3/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_3/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice_3/size: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_3/size: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape_5: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_5: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice_2/size: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_2/size: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack_2/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_2/1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_3/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_3/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_3/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_3/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape_4: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_4: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_5/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_5/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_4/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_4/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Reshape_3/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape_3/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ones/Const: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ones/Const: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack_1/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack_1/1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_2/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_2/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_2/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_2/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_2/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_2/values_0: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_2/values_0: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/ExpandDims/dim: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/ExpandDims/dim: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_1/axis: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1/axis: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat_1/values_0: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat_1/values_0: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_1/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_1/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice_1/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice_1/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice_1/begin: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice_1/begin: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape_1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Slice/begin: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Slice/begin: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/stack/1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/stack/1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice/stack_2: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack_2: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice/stack_1: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack_1: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/strided_slice/stack: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/strided_slice/stack: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/Shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Shape: (Const)/job:localhost/replica:0/task:0/gpu:0
nce_loss/concat/axis: (Const): /job:localhost/replica:0/task:0/cpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/concat/axis: (Const)/job:localhost/replica:0/task:0/cpu:0
nce_loss/Reshape/shape: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] nce_loss/Reshape/shape: (Const)/job:localhost/replica:0/task:0/gpu:0
gradients/sampled_losses/Log1p_grad/add/x: (Const): /job:localhost/replica:0/task:0/gpu:0
I tensorflow/core/common_runtime/simple_placer.cc:841] gradients/sampled_losses/Log1p_grad/add/x: (Const)/job:localhost/replica:0/task:0/gpu:0
E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]
Traceback (most recent call last):
  File ""tutorial_word2vec_basic.py"", line 369, in <module>
    main_word2vec_basic()
  File ""tutorial_word2vec_basic.py"", line 246, in main_word2vec_basic
    _, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
	 for attr 'tensor_type'
	; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
	 [[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]
",logged device placement sess successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally library use available machine could speed library use available machine could speed library use available machine could speed library use available machine could speed library use available machine could speed library use available machine could speed found device name major minor total memory free memory context one currently active found device name major minor total memory free memory context one currently active found device name major minor total memory free memory context one currently active found device name major minor total memory free memory peer access device peer access device peer access device peer access device peer access device peer access device peer access device peer access device device device name bus id device device name bus id device device name bus id device device name bus id device device name bus id device name bus id device name bus id device name bus id device device name bus id device name bus id device name bus id device name bus id load size epoch total real vocabulary size limited vocabulary size common data assign assign assign assign assign assign assign assign add add assign assign sub sub add add assign assign noop noop param mean median param mean median param mean median layer tensor saved pack pack pack pack sub sub pack pack sub sub pack pack prod prod maximum maximum prod prod cast cast fill fill reshape reshape tile tile reshape reshape stack pack stack pack fill fill fill fill sub sub reshape reshape reshape reshape pack pack reshape reshape sub sub reshape reshape reshape reshape reshape reshape pack pack identity identity identity identity identity identity cast cast log log log log reshape reshape reshape reshape unique unique shape shape reshape reshape unique unique shape shape gather gather slice slice slice slice reshape reshape gather gather slice slice shape shape sub sub sub sub reshape reshape slice slice shape shape sub sub sub sub reshape reshape shape shape shape shape reshape reshape shape shape reshape reshape unique unique shape shape gather gather shape shape add add sub sub reshape reshape shape shape pack pack fill fill shape shape reshape reshape shape shape reshape reshape shape shape add add sub sub select select select select sub sub add add noop noop identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity add add reciprocal reciprocal select select select select noop noop identity identity identity identity identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity identity identity select select select select noop noop identity identity slice slice slice slice noop noop identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity pad pad identity identity noop noop identity identity pad pad identity identity identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity sum sum reshape reshape sum sum reshape reshape noop noop identity identity reshape reshape pad pad reshape reshape identity identity reshape reshape reshape reshape noop noop identity identity reshape reshape sum sum reshape reshape sum sum reshape reshape noop noop identity identity reshape reshape pad pad reshape reshape identity identity reshape reshape reshape reshape noop noop reshape reshape reshape reshape mean mean mean mean shape shape create kernel invalid argument must reference type value tensor type string string string bool executor create kernel invalid argument must reference type value tensor type string string string bool node recent call last file line module file line cost file line run file line file line file line raise type message must reference type value tensor type string string string bool node,issue,positive,positive,positive,positive,positive,positive
298511125,"Of course, I will make another PR as I finish my job. thx.",course make another finish job,issue,negative,neutral,neutral,neutral,neutral,neutral
298234242,"Cool, but this PR seem to be a unfinished work, can you finish all of it and push it again?
Alternative option is to make your own repository, then I can link TL repository to it.",cool seem unfinished work finish push alternative option make repository link repository,issue,negative,positive,positive,positive,positive,positive
298230930,"I am about to make jupyter notebook for the example so it could look better, can I contribute my work to TensorLayer? Coz I really love your work.",make notebook example could look better contribute work coz really love work,issue,positive,positive,positive,positive,positive,positive
298206152,Oh thank you for the answer. It does make sense.,oh thank answer make sense,issue,negative,neutral,neutral,neutral,neutral,neutral
298199680,"@gitmcdull it is weird ...  i just test it with TF1.0.1, TL1.4.2 and python3 , it works well..",weird test python work well,issue,negative,negative,negative,negative,negative,negative
298199547,"Hi, both are the same, because `moving_mean` and `moving_variance` are not trainable variable and only be used during training.",hi trainable variable used training,issue,negative,neutral,neutral,neutral,neutral,neutral
297861994,"@gitmcdull what is your platform, TF version, TL version, python version ?",platform version version python version,issue,negative,neutral,neutral,neutral,neutral,neutral
297861847,"@wangg12 thanks for your advice. 
actually i knew this when i pick the name, however, as deconv is the most common name especially for beginner, i decide to use this name.",thanks advice actually knew pick name however common name especially beginner decide use name,issue,negative,negative,neutral,neutral,negative,negative
297262160,"Thanks a lot for your reply. 

Following your comments, I had two more tries to fix the problem but failed anyway. 

1st try: strictly follow your comments
 sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))

Cannot evaluate tensor using `eval()`: No default session is registered. Use `with sess.as_default()` or pass an explicit session to `eval(session=sess)`
Traceback (most recent call last):
  File ""tutorial_word2vec_basic.py"", line 366, in <module>
    main_word2vec_basic()
  File ""tutorial_word2vec_basic.py"", line 226, in main_word2vec_basic
    emb_net.print_params()
  File ""/usr/local/lib/python2.7/dist-packages/tensorlayer/layers.py"", line 282, in print_params
    raise Exception(""Hint: print params details after tl.layers.initialize_global_variables(sess) or use network.print_params(False)."")
Exception: Hint: print params details after tl.layers.initialize_global_variables(sess) or use network.print_params(False).

2nd try: use InteractiveSession instead
sess = tf.InteractiveSession(config=tf.ConfigProto(allow_soft_placement=True))

E tensorflow/core/framework/op_segment.cc:53] Create kernel failed: Invalid argument: AttrValue must not have reference type value of float_ref
for attr 'tensor_type'
; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
E tensorflow/core/common_runtime/executor.cc:594] Executor failed to create kernel. Invalid argument: AttrValue must not have reference type value of float_ref
for attr 'tensor_type'
; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
[[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]
Traceback (most recent call last):
  File ""tutorial_word2vec_basic.py"", line 366, in <module>
    main_word2vec_basic()
  File ""tutorial_word2vec_basic.py"", line 244, in main_word2vec_basic
    _, loss_val = sess.run([train_op, cost], feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 767, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 965, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1015, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1035, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: AttrValue must not have reference type value of float_ref
for attr 'tensor_type'
; NodeDef: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique); Op<name=_Recv; signature= -> tensor:tensor_type; attr=tensor_type:type; attr=tensor_name:string; attr=send_device:string; attr=send_device_incarnation:int; attr=recv_device:string; attr=client_terminated:bool,default=false; is_stateful=true>
[[Node: word2vec_layer/embeddings/Adagrad/_71 = _Recv[_start_time=0, client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_100_word2vec_layer/embeddings/Adagrad"", tensor_type=DT_FLOAT_REF, _device=""/job:localhost/replica:0/task:0/cpu:0""](^Adagrad/learning_rate, ^Adagrad/update_word2vec_layer/embeddings/UnsortedSegmentSum, ^Adagrad/update_word2vec_layer/embeddings/Unique)]]

I am new to tf and tl. :-(
Could you help? Thank you in advance. ",thanks lot reply following two fix problem anyway st try strictly follow sess evaluate tensor default session registered use pas explicit session recent call last file line module file line file line raise exception hint print sess use false exception hint print sess use false try use instead sess create kernel invalid argument must reference type value tensor type string string string bool executor create kernel invalid argument must reference type value tensor type string string string bool node recent call last file line module file line cost file line run file line file line file line raise type message must reference type value tensor type string string string bool node new could help thank advance,issue,positive,negative,neutral,neutral,negative,negative
297074927,"it seems you may need to instantiate the `sess` as follow
```python
sess = tf.Session(config=tf.ConfigProto(
                          allow_soft_placement=True)
      #                    log_device_placement=FLAGS.log_device_placement))
```",may need sess follow python sess,issue,negative,neutral,neutral,neutral,neutral,neutral
296696076,"yes, it only return the corresponding `layer.outputs`, if you want it to be a `Layer` class, you can put it into `InputLayer`.",yes return corresponding want layer class put,issue,negative,neutral,neutral,neutral,neutral,neutral
296370365,"Thank you for your report. 
Could you make a push request, this update is your contribution.",report could make push request update contribution,issue,negative,neutral,neutral,neutral,neutral,neutral
294408874,"hi @boscotsang ,
you can use LambdaLayer or simply apply activation function on net.outputs, see:
[resnet](https://github.com/zsdonghao/tensorlayer/issues/85)
",hi use simply apply activation function see,issue,negative,neutral,neutral,neutral,neutral,neutral
293529482,"@zsdonghao Thank you. Besides conv2d, PoolLayer has the similar problem. I also found that the function MeanPoolxx/MaxPoolxx is a wrapper of tf.contrib.layers and the default argument `data_format` of MeanPoolxx/MaxPoolxx is ""channels_last"". However, according https://www.tensorflow.org/versions/r1.1/api_docs/python/tf/contrib/layers/avg_pool2d the argument `data_format` is either `NCHW` nor `NHWC`.",thank besides similar problem also found function wrapper default argument however according argument either,issue,negative,neutral,neutral,neutral,neutral,neutral
293516831,"@boscotsang hi , thank you for your suggestion, please check my new commit ~",hi thank suggestion please check new commit,issue,positive,positive,positive,positive,positive,positive
293208284,"Here you go, if it works, let me know and i will add it to the new features.


```
class TransposeLayer(Layer):  # <-- Layer is tl.layers.Layer
    def __init__(
        self,
        layer = None,
        perm = None,
        name ='dense_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        print(""  [TL] TransposeLayer  %s: perm=%s"" % (self.name, perm))
        with tf.variable_scope(name) as vs:
                self.outputs =  tf.transpose(self.inputs, perm= perm)
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
```

reference: https://www.tensorflow.org/api_docs/python/tf/transpose

",go work let know add new class layer layer self layer none perm none name self print perm name perm list list reference,issue,negative,positive,positive,positive,positive,positive
290915533,"Hi Hansheng, please have a try the following StackLayer, if it work we can add it in TL.
```
class StackLayer(Layer):
    def __init__(
        self,
        layer = [],
        axis = 0,
        name ='stack_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = []
        for l in layer:
            self.inputs.append(l.outputs)
        
        if tf.__version__ < ""1.0"":
            raise Exception(""please update to TF 1.0+"")

        self.outputs = tf.stack(self.inputs, axis= axis, name=name)

        print(""  [TL] StackLayer %s: axis: %d"" % (self.name, axis))

        self.all_layers = list(layer[0].all_layers)
        self.all_params = list(layer[0].all_params)
        self.all_drop = dict(layer[0].all_drop)

        for i in range(1, len(layer)):
            self.all_layers.extend(list(layer[i].all_layers))
            self.all_params.extend(list(layer[i].all_params))
            self.all_drop.update(dict(layer[i].all_drop))

        self.all_layers = list_remove_repeat(self.all_layers)
        self.all_params = list_remove_repeat(self.all_params)
```",hi please try following work add class layer self layer axis name self layer raise exception please update axis print axis axis list layer list layer layer range layer list layer list layer layer,issue,positive,neutral,neutral,neutral,neutral,neutral
290908085,"Stack layer increases rank by 1. If you expand dim by 1 and then concat, it will be the same.",stack layer rank expand dim,issue,negative,negative,negative,negative,negative,negative
288306900,"It can be retrieved via layers.all_params, but it's inconvenient, as we have to keep track the index of which layer the current layer points to. It'd be better if it just returns current layer, like the outputs does.
Perhaps something called layers.params that returns [W,b]?",via inconvenient keep track index layer current layer better current layer like perhaps something,issue,negative,negative,neutral,neutral,negative,negative
288280719,"Maybe you can use layers.all_params to get a list of Tensor, which all network variables in order.",maybe use get list tensor network order,issue,negative,neutral,neutral,neutral,neutral,neutral
288092920,"we don't have plan to support this feature, as we suggest to build different inferenec for testing and training, [click](https://github.com/wagamamaz/tensorlayer-tricks)",plan support feature suggest build different testing training click,issue,negative,neutral,neutral,neutral,neutral,neutral
287596620,"If I simply replace the code to use tensorlayer's Layers, it may cause problem about variable's name. 
In fit, evaluate and predict, the model_fn would be called, construct the graph and load the trained parameters. 
So maybe it should be used outside the model_fn? 

An model_fn example: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/estimators/abalone.py",simply replace code use may cause problem variable name fit evaluate predict would construct graph load trained maybe used outside example,issue,negative,positive,positive,positive,positive,positive
287596287,"
Thanks for replying~

The example use model_fn to create tensorlayer's Layer. 
Could it use in the opposite way? 
I would like to construct model_fn with Layers. (use Layers inside the model_fn)
In this way I could generate the an tensorflow.contrib.learn.Estimator instance.

The reason I choose the Estimator is that it works as sklearn's Estimator (fit and evaluate), and it can be use it sklearn's Pipeline. Moreover, it accepts monitors to record and control the processing. So I think it may be easier to fine-tune the parameters.

I'm a beginner tensorflow and tensorlayer, I wonder if there is a more automatic way (or pipeline as grid search) to fine-tune the parameters?",thanks example use create layer could use opposite way would like construct use inside way could generate instance reason choose estimator work estimator fit evaluate use pipeline moreover record control think may easier beginner wonder automatic way pipeline grid search,issue,positive,positive,positive,positive,positive,positive
287591258,"Yes, i think you can use this function [keraslayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#connect-keras), [estimatorlayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#estimator-layer),
[example](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_keras.py)",yes think use function example,issue,negative,neutral,neutral,neutral,neutral,neutral
285930183,"@zsdonghao I am sorry that maybe I had not describe my confusion clearly. 
Before `TimeDistributedLayer` I use some functions from `tl.nlp` to transfer words-in-sentences to integer index and  `EmbeddingInputLayer` to become embedded word vector, so the inputs to `TimeDistributedLayer` is not text input but vector input with `[batch_size, n_steps, n_dim]`. So I think `TimeDistributedLayer` maybe helpful. But the problem is that `ValueError: Cannot infer num from shape (5, ?, 200)` and I didn't know why.
As for `DynamicRNNLayer`, the first one I used as a `layer_class` in `TimeDistributedLayer` in order to encode at the sentence_level (That means encode tokens into sentences). The other `DynamicRNNLayer` I used was for encoding at the document_level (That means encode sentences into paragraph/document). 
",sorry maybe describe confusion clearly use transfer integer index become word vector text input vector input think maybe helpful problem infer shape know first one used order encode encode used encode,issue,negative,negative,neutral,neutral,negative,negative
285920543,"@Zsank as you set `return_last=True`, you can encode a batch of sentences simultaneously, but what is the other `DynamicRNNLayer` for?

I think `TimeDistributedLayer` have nothing to do in your case, because it is for non-text input.",set encode batch simultaneously think nothing case input,issue,negative,neutral,neutral,neutral,neutral,neutral
285920101,"Sure that the example from DynamicRNNLayer can work. However, what I want is to input more than one sentences and apply DynamicRNNLayer on each of them simultaneously. Then use their outputs as input to the next DynamicRNNLayer, just like my defined name in layers ""TokensToSeqs"" ""Seq2ToText"". So, if TimeDistributedLayer can be used on DynamicRNNLayer? Or should I use a loop for it?",sure example work however want input one apply simultaneously use input next like defined name used use loop,issue,positive,positive,positive,positive,positive,positive
285919623,"@JoelKronander as you already implemented that part and push to the master, i close this issues now ~

",already part push master close,issue,negative,neutral,neutral,neutral,neutral,neutral
285919550,Automatically closing due to lack of recent activity. Please reopen when additional information becomes available. Thanks!,automatically due lack recent activity please reopen additional information becomes available thanks,issue,negative,positive,positive,positive,positive,positive
285919431,"@boscotsang thanks for your answer ~ 
@AllenCX i close the issue now, reopen it if your problem cannot be solved",thanks answer close issue reopen problem,issue,negative,positive,positive,positive,positive,positive
285919194,"if your input is a sentence, just use `DynamicRNNLayer`.

the example from [DynamicRNNLayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#id26) should work in your case.
",input sentence use example work case,issue,negative,neutral,neutral,neutral,neutral,neutral
285786904,"you may find this function can help [tl.layers.list_remove_repeat](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#remove-repeated-items-in-a-list)
and instead of adding loss one by one, you can also use a for loop.

As I understand, the following script can solve your problem.

cost = 0
for p in tl.layers.list_remove_repeat(net1.all_params + net2.all_params)
     cost = cost + tl.cost.maxnorm_regularizer(1.0)(p)

",may find function help instead loss one one also use loop understand following script solve problem cost cost cost,issue,negative,neutral,neutral,neutral,neutral,neutral
285512686,"The code for ""losses"" seems correct in im2txt. I think ""tf.reduce_mean"" in line 251 of cost.py should be removed.",code correct think line removed,issue,negative,neutral,neutral,neutral,neutral,neutral
285430012,"Thank you ~
but I borrow this code from [tensorflow/models/im2txt](https://github.com/tensorflow/models/blob/master/im2txt/im2txt/show_and_tell_model.py)
```
# Compute losses.
losses = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets,
                                                              logits=logits)
batch_loss = tf.div(tf.reduce_sum(tf.multiply(losses, weights)),
                          tf.reduce_sum(weights),
                          name=""batch_loss"")
```",thank borrow code compute,issue,negative,neutral,neutral,neutral,neutral,neutral
285004323,"It seems that the statement you comment `train_op = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.99)` before pass a MomentumOptimizer instance instead of an operation to the tl.utils.fit. 
",statement comment pas instance instead operation,issue,negative,neutral,neutral,neutral,neutral,neutral
285000583," #define network
    sess = tf.InteractiveSession()
    
    x = tf.placeholder(tf.float32,shape= [None,227,227,1],name= 'x')
    y_ = tf.placeholder(tf.float32,shape= [None,],name= 'y_')
   
    w_init = tf.truncated_normal_initializer(stddev=0.01)
    b_init = tf.constant_initializer(value=0.0)

    network = tl.layers.InputLayer(inputs = x,n_features=1)                 #227*227
    #1
    network = tl.layers.Conv2d(network,32,(9,9),(2,2),act = tf.nn.relu,padding='VALID',
                        W_init=w_init, b_init=b_init, name='conv1')                 #110*110
    network = tl.layers.MeanPool2d(network,(2,2),padding ='VALID',name = 'pool1')   #55*55
    #2
    network = tl.layers.Conv2d(network,64,(7,7),(1,1),act = tf.nn.relu,padding='VALID',   #49*49
                        W_init=w_init, b_init=b_init, name='conv2')
    network = tl.layers.MeanPool2d(network,(2,2),padding ='VALID',name = 'pool2')    #24*24
    #3
    network = tl.layers.Conv2d(network,128,(5,5),(1,1),act = tf.nn.relu,padding='VALID',   #20*20
                        W_init=w_init, b_init=b_init, name='conv3')
    network = tl.layers.MeanPool2d(network,(2,2),padding ='VALID',name = 'pool3')    #10*10
    #4
    network = tl.layers.Conv2d(network,128,(3,3),(1,1),act = tf.nn.relu,padding='VALID',
                        W_init=w_init, b_init=b_init, name='conv4')                  #8*8
    network = tl.layers.MeanPool2d(network,(1,1),padding ='SAME',name = 'pool4')     #4*4
    #flatten layer
    network = tl.layers.FlattenLayer(network, name='flatten_layer')
    #fully connected
    network = tl.layers.DenseLayer(network,n_units=500,act=tf.nn.relu,W_init=w_init,name='relu_layer1')
    network = tl.layers.DenseLayer(network,n_units=100,act=tf.nn.relu,W_init=w_init,name='relu_layer2')
    network = tl.layers.DenseLayer(network,n_units=1,act=tf.nn.relu,W_init=w_init,name='out_layer')
    
    y = network.outputs
    cost =tl.cost.mean_squared_error(y, y_)
    acc = tf.reduce_mean(abs(y-y_))
    y_op = y
    
    train_params = network.all_params
    
    #train_op = tf.train.MomentumOptimizer(learning_rate=0.001, momentum=0.99)
    mse = tf.reduce_mean(tf.square(y - y_))
    train_op = tf.train.GradientDescentOptimizer(0.3).minimize(mse)
    
    sess.run(tf.global_variables_initializer())
    
    network.print_params()
    network.print_layers()
    
    sess.run(train_op,feed_dict={x: x_train, y_: y_train})",define network sess none none network network network act network network padding name network network act network network padding name network network act network network padding name network network act network network padding name flatten layer network network fully connected network network network network network network cost,issue,negative,neutral,neutral,neutral,neutral,neutral
284978374,The argument  `is_train` should be type of python bool instead of tf variable.,argument type python bool instead variable,issue,negative,neutral,neutral,neutral,neutral,neutral
284977803,"Could you show us the definition of network, train_op, x and y_?",could show u definition network,issue,negative,neutral,neutral,neutral,neutral,neutral
284290260,"Thank you for reply @zsdonghao

I had the same problem during inference time as described in http://stackoverflow.com/questions/40879967/how-to-use-batch-normalization-correctly-in-tensorflow.

 The inference result is much better when I set the is_training=True than when I set the is_training=False.
Initially I used the default decay value 0.9, so I guess it is the default decay value's problem. Then I tried 0.99 and 0.999, but both of these two values also have the same problem. The inference performance degrades significantly when is_training = False.  Has you had the same problem before?

Never mind, it turns out to be my network's problem.",thank reply problem inference time inference result much better set set initially used default decay value guess default decay value problem tried two also problem inference performance significantly false problem never mind turn network problem,issue,negative,positive,neutral,neutral,positive,positive
284286484,"I see that you have committed to master branch. I just pulled and tested, and it works now. Thanks.",see master branch tested work thanks,issue,negative,positive,positive,positive,positive,positive
283948971,"@Jackqu Hi, the default setting of TF is 0.999.

However, set decay to be a smaller value (decay=0.9 or decay=0.99) than default value (default is 0.999) if your dataset is small or your total training updates/steps are not that large.

reference: http://stackoverflow.com/questions/40879967/how-to-use-batch-normalization-correctly-in-tensorflow",hi default setting however set decay smaller value default value default small total training large reference,issue,negative,negative,neutral,neutral,negative,negative
283522378,"@zsdonghao  
for the output `print(network.outps)  Tensor(""output_layer/Identity:0"", shape=(?, 3), dtype=float32)`
and the y_train `print(y_train.shape,y_val.shape)  (278, 3) (93, 3)`
is this correct?",output print tensor print correct,issue,negative,neutral,neutral,neutral,neutral,neutral
283400248,"Oh I see, I used to think `name=args['name']+str(i))` as layer_class parameter.",oh see used think parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
283381876,"@danlutan yeah, I considered parameter sharing. https://github.com/zsdonghao/tensorlayer/blob/master/tensorlayer/layers.py#L2910

here is how I apply a dense layer on every step:
```
batch_size = 32
timestep = 20
input_dim = 100
x = tf.placeholder(dtype=tf.float32, shape=[batch_size, timestep,  input_dim], name=""encode_seqs"")
net = InputLayer(x, name='input')
net = TimeDistributedLayer(net, layer_class=DenseLayer, args={'n_units':50, 'name':'dense'}, name='time_dense')

print(net.outputs._shape)
net.print_params(False)
```

feel free to let me know if you have problem.",yeah considered parameter apply dense layer every step net net net print false feel free let know problem,issue,negative,negative,neutral,neutral,negative,negative
283356286,"The implement of TimeDistributedLayer are different from  TimeDistributed in Keras . And I use it on my code , it has a bad effect . Do you consider the weight sharing when create a TimeDistributedLayer?
",implement different use code bad effect consider weight create,issue,negative,negative,negative,negative,negative,negative
283334305,"@Aceb1shmael I see, if `tutorial_mnist_simple.py` work well, that should be a problem of your network output. Can you check the shape of the outputs? ```print(network.outputs)```

As `tl.cost.cross_entropy` uses `tf.nn.sparse_softmax_cross_entropy_with_logits`, you should make sure the output and target shapes match it's requirement.",see work well problem network output check shape print make sure output target match requirement,issue,negative,positive,positive,positive,positive,positive
283326096,"I'm using tf 1.0 and tl 1.3.10, and i use pip to install package.
i think both package work fine because the tutorial_mnist_simple.py works,
the problem might be my data doesn`t conform the rules, but i don't know why it stop at this line or how to fix it",use pip install package think package work fine work problem might data conform know stop line fix,issue,negative,positive,positive,positive,positive,positive
283316982,"hi, which TF version are you using. but I think it should work for both TF1.0 and others,
how did you install TL? could you install TL again?",hi version think work install could install,issue,negative,neutral,neutral,neutral,neutral,neutral
283233926,"@danlutan Hi, thank you for your advice. 

I release a [TimeDistributedLayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#time-distributed-layer) in the master version, please have a try~

",hi thank advice release master version please,issue,positive,neutral,neutral,neutral,neutral,neutral
283092192,"@danlutan i see, let me think and get back to you shortly, i think there should be some ways without using special layer. if special layer is necessary, i will create one.

thanks",see let think get back shortly think way without special layer special layer necessary create one thanks,issue,negative,positive,positive,positive,positive,positive
283024615,"For example , the shape is [ 5,6,8 ] .When I use Flatten , it become [ 5 , 48 ] ,and the use denselayer which dim_new is 4 .It become [ 5 , 4 ] . It's failed to reshape [5 ,6 ,4] . Moreover , I need to  map the dim of every  length to dim_new ，not to map length*dim to dim_new",example shape use flatten become use become reshape moreover need map dim every length map length dim,issue,negative,positive,neutral,neutral,positive,positive
283003153,"use  [TimeDistributedLayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#time-distributed-layer)

------------ previous ------------------------

@danlutan I think you can do this:

```
x = tf.placeholder('float32', [batch_size, length, dim], name = 'input')
net = InputLayer(x, name='input')
net = Flatten(net, name='flatten')
net = DenseLayer(net, dim_new, name='dense')
net = Reshape(net,  [batch_size , length , dim_new], name='reshape')
```",use previous think length dim name net net flatten net net net net reshape net length,issue,negative,negative,neutral,neutral,negative,negative
282551252,"Hmm, yes u are right. Thanks for the clarification.",yes right thanks clarification,issue,positive,positive,positive,positive,positive,positive
282491066,"@joekr552 That's a tricky yet reasonable confusion, thank you so much for mention it here. But after a second thought, I figure that maybe the author's method is rational. Let me put it in this way:
Just looking at the input layer I_rgb with RGB format and the weight W_rgb (conv kernel) along with the first hidden layer H.
```
H = I_rgb * W_rgb
```
That is, all neurons in the H receive some particular area of `I` convoluted by `W`. While it seems that to get the same value of neuron when the order of the channel of `I` change, all we need to do is modify the order of  `W`.
After we make sure that
```
 I_bgr * W_bgr ==  I_rgb * W_rgb
```
in the first hidden layer, we know the later hidden layer wouldn't face the problem you mentioned.",tricky yet reasonable confusion thank much mention second thought figure maybe author method rational let put way looking input layer format weight kernel along first hidden layer receive particular area convoluted get value neuron order channel change need modify order make sure first hidden layer know hidden layer would face problem,issue,negative,positive,positive,positive,positive,positive
282487371,"I am referring to my comment at the bottom of the page :
> ""Joel Kronander  to Davi Frossard • a day ago
> To me it seems weird/wrong to just reorganize the filter channels in the first layers?
> Later layers assume that they were assigned in that order to specific color channels?! 
> Ie if a higher order filter wants to get high activity for say a red edge, by re-ordering first layer filter weights this is effectively turned into a blue edge""

My issue was that in the blog post the author said he post-processed the weights after running the caffe-tensorflow code, which included a step explained as: ""..reorder the filters in the first layer so that they are in RGB order (instead of the default BGR. "" 

To me that is a strange thing to do as consecutive higher filters will assume that the first layer filters where in BGR order. An extreme example is that a ""red cat"" detector would become all confused.

The approach you mentioned above, where the input is first transformed to BGR from RGB should be correct though. ",comment bottom page day ago reorganize filter first later assume assigned order specific color ie higher order filter get high activity say red edge first layer filter effectively turned blue edge issue post author said running code included step reorder first layer order instead default strange thing consecutive higher assume first layer order extreme example red cat detector would become confused approach input first correct though,issue,negative,positive,positive,positive,positive,positive
282465831,Thank you very much . I re-examined the code and solved the problem . I'm really excited to receive your reply.,thank much code problem really excited receive reply,issue,negative,positive,positive,positive,positive,positive
282441898,"You're welcome. But the post in your link seems more like a tutorial than an issue, where are your confusion.",welcome post link like tutorial issue confusion,issue,positive,positive,positive,positive,positive,positive
282391158,"Awesome, thanks a lot! Since the original question & extra API questions are both resolved very well I'll close the issue. ",awesome thanks lot since original question extra resolved well close issue,issue,positive,positive,positive,positive,positive,positive
282331015,"Hi all, a easy way to test before push request is:

Run this scripts :tutorial_mnist_simple.py, tutorial_cifar10_tfrecord.py and tutorial_ptb_lstm_state_is_tuple.py under python2.7, 3.4, 3.5",hi easy way test push request run,issue,negative,positive,positive,positive,positive,positive
282322647,"@danlutan Hi, when did you get this error? during training?

Did you disable or enable `DropoutLayer`?  If nope, you may need to do sth like this: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist.py
```
dp_dict = tl.utils.dict_to_one( network.all_drop )    # disable dropout layers
feed_dict = {x: X_train_a, y_: y_train_a}
feed_dict.update(dp_dict)
```
Another way is to `is_fix=True` in `DropoutLayer`, see [Training/Testing switching](https://github.com/wagamamaz/tensorlayer-tricks)",hi get error training disable enable nope may need like disable dropout another way see switching,issue,negative,neutral,neutral,neutral,neutral,neutral
282302293,"Actually I am a bit confused on whatever the approach used to get the weights used in the vgg16 example in TL is a good approach: I don't think it is a valid approach, the approach you mentioned above does seem correct however.

See my discussion post here> https://www.cs.toronto.edu/~frossard/post/vgg16/",actually bit confused whatever approach used get used example good approach think valid approach approach seem correct however see discussion post,issue,negative,positive,neutral,neutral,positive,positive
282279108,"@joekr552 I just check caffe-tensorflow code for you. The good news is that they did transpose the c, w, h. 
```python
def transform_data(self):
        if self.params is None:
            transformers = [

                # Reshape the parameters to TensorFlow's ordering
                DataReshaper({
                    # (c_o, c_i, h, w) -> (h, w, c_i, c_o)
                    NodeKind.Convolution: (2, 3, 1, 0),

                    # (c_o, c_i) -> (c_i, c_o)
                    NodeKind.InnerProduct: (1, 0)
                }),

                # Pre-process batch normalization data
                BatchNormPreprocessor(),

                # Convert parameters to dictionaries
                ParameterNamer(),
            ]
            self.graph = self.graph.transformed(transformers)
            self.params = {node.name: node.data for node in self.graph.nodes if node.data}
        return self.params
```

But I can't find the code where they transpose the BGR into RGB, so I think they did not. And that also explain why we should do the transpose by hand at the TL [vgg19 example](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_vgg19.py)
```python
if tf.__version__ <= '0.11':
        red, green, blue = tf.split(3, 3, rgb_scaled)
    else: # TF 1.0
        print(rgb_scaled)
        red, green, blue = tf.split(rgb_scaled, 3, 3)
    assert red.get_shape().as_list()[1:] == [224, 224, 1]
    assert green.get_shape().as_list()[1:] == [224, 224, 1]
    assert blue.get_shape().as_list()[1:] == [224, 224, 1]
    if tf.__version__ <= '0.11':
        bgr = tf.concat(3, [
            blue - VGG_MEAN[0],
            green - VGG_MEAN[1],
            red - VGG_MEAN[2],
        ])
    else:
        bgr = tf.concat([
            blue - VGG_MEAN[0],
            green - VGG_MEAN[1],
            red - VGG_MEAN[2],
        ], axis=3)
```",check code good news transpose python self none reshape batch normalization data convert node return ca find code transpose think also explain transpose hand example python red green blue else print red green blue assert assert assert blue green red else blue green red,issue,positive,negative,neutral,neutral,negative,negative
282274231,"Aha, ok! thanks!

Did you check the GBR and RGB channel ordering as well?",aha thanks check channel well,issue,positive,positive,positive,positive,positive,positive
282267914,"@joekr552 The graph I use comes from one of the [TensorLayer examples](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_vgg19.py).

Well, without going deeper into caffe-tensorflow's code, I found by experimenting on jupyter notebook that caffe-tensorflow already did that for us.",graph use come one well without going code found notebook already u,issue,negative,neutral,neutral,neutral,neutral,neutral
282241924,"@JindongJiang what network graph for VGG19 do you use with that loading code? 

I guess one could also need to transpose the caffe weights as often in tensorflow/tensorlayer the network uses (width, height, channels) format instead of (channels, width, height) as in caffe for example?",network graph use loading code guess one could also need transpose often network width height format instead width height example,issue,negative,neutral,neutral,neutral,neutral,neutral
282183801,"@ahundt Thanks. `ReduceLROnPlateau` is for changing the learning rate right?

TL does not has the opt equivalent to this, because to control the training deeply we hope people to use TF APIs.

I suggest you to define a optimizer like that:
```
with tf.variable_scope('learning_rate'):
        lr = tf.Variable(0.0, trainable=False)
optimizer = tf.train.GradientDescentOptimizer(lr).minimize(cost)
```
then you can update your learning rate during training like that: 
```
sess.run(tf.assign(lr, your_new_lr)
```
The PTB example use this method: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py",thanks learning rate right opt equivalent control training deeply hope people use suggest define like cost update learning rate training like example use method,issue,positive,positive,positive,positive,positive,positive
282178613,"Amazing :-) that's the fastest I've ever seen a feature implemented, you deserve a medal 🏅! I'm definitely going to install TL now!

Perhaps TL has its own equivalent some of the [training callbacks](https://github.com/fchollet/keras/blob/keras-2/keras/callbacks.py) like ReduceLROnPlateau etc?",amazing ever seen feature deserve medal definitely going install perhaps equivalent training like,issue,positive,positive,positive,positive,positive,positive
282173311,"Hi, TensorLayer is compatible with Keras in the master verison now!

here is a small example: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_keras.py",hi compatible master small example,issue,negative,negative,negative,negative,negative,negative
282164524,"@ahundt  good idea. As TL is now compatible with TF-Slim, we can also make it compatible with keras ~ let me think .",good idea compatible also make compatible let think,issue,negative,positive,positive,positive,positive,positive
282031923,"@ruzrobert 
Thanks, that is useful ~  my colleague is implementing layer normalization.  http://r2rt.com/recurrent-neural-networks-in-tensorflow-ii.html

Let me mark this issue as `help wanted`.",thanks useful colleague layer normalization let mark issue help,issue,positive,positive,positive,positive,positive,positive
281794533,"Hello!
@guoying1030  , @zsdonghao  . Look what I have found today:
https://github.com/tensorflow/tensorflow/issues/1736
""there is an implementation here of batch normalized lstm: https://github.com/OlavHN/bnlstm/blob/master/lstm.py
explained in the article http://olavnymoen.com/2016/07/07/rnn-batch-normalization
inspired from the paper https://arxiv.org/abs/1603.09025""

So it is okay for RNN, but it is different.
Code for that RNN batch normalization is available in https://github.com/OlavHN/bnlstm",hello look found today implementation batch article inspired paper different code batch normalization available,issue,negative,positive,positive,positive,positive,positive
281318660,I  rewrite it (in vim ) and solve the problem.,rewrite vim solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
281317965,"@wagamamaz   Please don't close so soon. U don't investigate it deeply. As far as I know, it is the encoding problem of the file and not is my configure problem.    ",please close soon investigate deeply far know problem file configure problem,issue,negative,positive,neutral,neutral,positive,positive
280869433,"@guoying1030 do you mean apply batch norm on the RNN output? 
my colleague say it is not good for RNN, you should use layer normalization",mean apply batch norm output colleague say good use layer normalization,issue,negative,positive,positive,positive,positive,positive
280869338,"do you mean PadLayer?  http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#padding-layer

set the mode='CONSTANT' , it is zero padding.",mean set zero padding,issue,negative,negative,negative,negative,negative,negative
280869273,"@essank if you are using TF 1.0, please update TL to master version, and put a name to the cost function like this https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist_simple.py


",please update master version put name cost function like,issue,positive,neutral,neutral,neutral,neutral,neutral
280866497,"I'm using TensorFlow version 1.0, and I got same error:
```    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)```
```TypeError: run() got an unexpected keyword argument 'argv'```

How to fix it?",version got error unparsed run got unexpected argument fix,issue,negative,positive,neutral,neutral,positive,positive
280299740,"for TF 1.0 please update to master version `pip install git+https://github.com/zsdonghao/tensorlayer.git`

",please update master version pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
279968959,"@ssdf93 Many thanks, for compatibility with old version, could you change a little bit as follow and push again?~

```
if tf.__version__ <= ""0.12"":  # TF 0.10, 0.11, 0.12
        bidirectional_rnn_fn = tf.nn.bidirectional_rnn
else: # TF 1.0
        bidirectional_rnn_fn = tf.contrib.rnn.static_bidirectional_rnn
outputs, fw_state, bw_state = bidirectional_rnn_fn(....
```",many thanks compatibility old version could change little bit follow push else,issue,negative,positive,positive,positive,positive,positive
279677508,"@toori67 Thanks ~
Do you think the following solution is better?  a key problem in your PR is that the beta should be initialized by `tf.zeros_initializer`.

```
class BatchNormLayer(Layer):
    """"""
    The :class:`BatchNormLayer` class is a normalization layer, see ``tf.nn.batch_normalization`` and ``tf.nn.moments``.
    Batch normalization on fully-connected or convolutional maps.
    Parameters
    -----------
    layer : a :class:`Layer` instance
        The `Layer` class feeding into this layer.
    decay : float, default is 0.9.
        A decay factor for ExponentialMovingAverage, use larger value for large dataset.
    epsilon : float
        A small float number to avoid dividing by 0.
    act : activation function.
    is_train : boolean
        Whether train or inference.
    beta_init : beta initializer
        The initializer for initializing beta
    gamma_init : gamma initializer
        The initializer for initializing gamma
    name : a string or None
        An optional name to attach to this layer.
    References
    ----------
    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`_
    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`_
    """"""
    def __init__(
        self,
        layer = None,
        decay = 0.9,
        epsilon = 0.00001,
        act = tf.identity,
        is_train = False,
        beta_init = tf.zeros_initializer,
        gamma_init = tf.random_normal_initializer(mean=1.0, stddev=0.002),
        name ='batchnorm_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        print(""  tensorlayer:Instantiate BatchNormLayer %s: decay: %f, epsilon: %f, act: %s, is_train: %s"" %
                            (self.name, decay, epsilon, act.__name__, is_train))
        x_shape = self.inputs.get_shape()
        params_shape = x_shape[-1:]

        from tensorflow.python.training import moving_averages
        from tensorflow.python.ops import control_flow_ops

        with tf.variable_scope(name) as vs:
            axis = list(range(len(x_shape) - 1))

            ## 1. beta, gamma
            try: # TF12
                  beta = tf.get_variable('beta', shape=params_shape,
                               initializer=beta_init(),
                               trainable=is_train)#, restore=restore)
           except: # TF11
                  beta = tf.get_variable('beta', shape=params_shape,
                               initializer=beta_init,
                               trainable=is_train)#, restore=restore)

            gamma = tf.get_variable('gamma', shape=params_shape,
                                initializer=gamma_init, trainable=is_train,
                                )

            ## 2. moving variables during training (not update by gradient!)
            try: # TF12
                  moving_mean = tf.get_variable('moving_mean',
                                      params_shape,
                                      initializer=tf.zeros_initializer(),
                                      trainable=False,)
            except: # TF11
                  moving_mean = tf.get_variable('moving_mean',
                                      params_shape,
                                      initializer=tf.zeros_initializer,
                                      trainable=False,)
            moving_variance = tf.get_variable('moving_variance',
                                          params_shape,
                                          initializer=tf.constant_initializer(1.),
                                          trainable=False,)

            ## 3.
            # These ops will only be preformed when training.
            mean, variance = tf.nn.moments(self.inputs, axis)
            try:    # TF12
                update_moving_mean = moving_averages.assign_moving_average(
                                moving_mean, mean, decay, zero_debias=False)     # if zero_debias=True, has bias
                update_moving_variance = moving_averages.assign_moving_average(
                                moving_variance, variance, decay, zero_debias=False) # if zero_debias=True, has bias
                # print(""TF12 moving"")
            except Exception as e:  # TF11
                update_moving_mean = moving_averages.assign_moving_average(
                                moving_mean, mean, decay)
                update_moving_variance = moving_averages.assign_moving_average(
                                moving_variance, variance, decay)
                # print(""TF11 moving"")

            def mean_var_with_update():
                with tf.control_dependencies([update_moving_mean, update_moving_variance]):
                    return tf.identity(mean), tf.identity(variance)

            ## 4. behaviour for training and testing
            if is_train:
                mean, var = mean_var_with_update()
                self.outputs = act( tf.nn.batch_normalization(self.inputs, mean, var, beta, gamma, epsilon) )
            else:
                self.outputs = act( tf.nn.batch_normalization(self.inputs, moving_mean, moving_variance, beta, gamma, epsilon) )    # Simiao
         
            variables = [beta, gamma, moving_mean, moving_variance]

        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
        self.all_params.extend( variables )
```
i suggest to use only batch norm layer instead of creating more layers.",thanks think following solution better key problem beta class layer class class normalization layer see batch normalization convolutional layer class layer instance layer class feeding layer decay float default decay factor use value large epsilon float small float number avoid dividing act activation function whether train inference beta beta gamma gamma name string none optional name attach layer source self layer none decay epsilon act false name self print decay epsilon act decay epsilon import import name axis list range beta gamma try beta except beta gamma moving training update gradient try except preformed training mean variance axis try mean decay bias variance decay bias print moving except exception mean decay variance decay print moving return mean variance behaviour training testing mean act mean beta gamma epsilon else act beta gamma epsilon beta gamma list list suggest use batch norm layer instead,issue,negative,negative,negative,negative,negative,negative
279596491,"@zsdonghao I tested with tensorflow 0.12.1 with python 3.5 on Ubuntu 16.04. 
`BatchNormalLayer5`, `BatchNormalLayer_TF` also has problem. 

And for version compatibility issue, how about creating another class like `BatchNormLayer_12` to support tf-0.12.1? (But I'll fix my pull request as you suggested :) )",tested python also problem version compatibility issue another class like support fix pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
279499900,"@toori67 hi, are you sure you are using tf 0.12 ? because the `BatchNormalLayer ` work well in my machine.

Besides, I think `beta` should be initialized to ones, and can you use `""try except"" `instead of just change the code? otherwise, old version would not be compatible.",hi sure work well machine besides think beta use try except instead change code otherwise old version would compatible,issue,positive,positive,positive,positive,positive,positive
279246929,"@wagamamaz you can use for loop to define it.

```
lrelu = lambda x: tl.act.lrelu(x, 0.2)
...
net_h1 = Conv2d(net_h0, df_dim*2, (4, 4), (2, 2), act=None,             
        padding='SAME', W_init=w_init, b_init=b_init, name='e_h1/conv2d')
net_h1 = BatchNormLayer(net_h1, #act=lrelu, 
       is_train=is_train, gamma_init=gamma_init, name='e_h1/batchnorm')

for i in range(1000):
      net = Conv2d(net_h1, df_dim*1, (1, 1), (1, 1),
          padding='SAME', W_init=w_init, b_init=b_init, name='e{}/c1'.format(i))
      net = BatchNormLayer(net, act=lrelu,
          is_train=is_train, gamma_init=gamma_init, name='e{}/b1'.format(i))
      net = Conv2d(net, df_dim*1, (3, 3), (1, 1),
          padding='SAME', W_init=w_init, b_init=b_init, name='e{}/c2'.format(i))
      net = BatchNormLayer(net, act=lrelu,
          is_train=is_train, gamma_init=gamma_init, name='e{}/b2'.format(i))
      net = Conv2d(net, df_dim*2, (3, 3), (1, 1),
          padding='SAME', W_init=w_init, b_init=b_init, name='e{}/c3'.format(i))
      net = BatchNormLayer(net, #act=tf.nn.relu,
          is_train=is_train, gamma_init=gamma_init, name='e{}/b3'.format(i))
      net_h1 = ElementwiseLayer(layer=[net_h1, net], combine_fn=tf.add, name='e{}/add'.format(i))
      net_h1.outputs = lrelu(net_h1.outputs)

net_h2 = Conv2d(net_h1, df_dim*4, (4, 4), (2, 2), act=None,               
      padding='SAME', W_init=w_init, b_init=b_init, name='e_h2/conv2d')
net_h2 = BatchNormLayer(net_h2,                           
      is_train=is_train, gamma_init=gamma_init, name='e_h2/batchnorm')
....
```",use loop define lambda range net net net net net net net net net net net net,issue,negative,neutral,neutral,neutral,neutral,neutral
279138124,"@guoying1030 https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py

just simply change `tl.layers.RNNLayer` to `tl.layers.BiRNNLayer `, it automatically do bidirectional staff for you.",simply change automatically bidirectional staff,issue,negative,neutral,neutral,neutral,neutral,neutral
279138046,"@guoying1030 please DONOT post REPEATED issue !!!

You can ask people for advice on your previous issue https://github.com/zsdonghao/tensorlayer/issues/83

Many thanks!
",please post repeated issue ask people advice previous issue many thanks,issue,positive,positive,positive,positive,positive,positive
279113606,close due to repeated issue ,close due repeated issue,issue,negative,negative,negative,negative,negative,negative
278949221,"@zsdonghao  
I would like to use tensorlayer realize neon inside the function of Birnn, neon inside the BiRnn you can set split_input, the meaning of this parameter is: split_inputs (bool): to expect the input coming from the same source of separate Sources, but tensorlayer which birnn inside, I do not know how to set up this parameter? ?? This parameter from the code inside the neon see, assuming that the input is [32,1000,2304], respectively, on behalf of the [batch_size, n_step, hidden_size], then how split_input parameter is set to true, the function is running, [32,1000,1152] is used for forward propagation operations, and [32,1000,1152] is used for backward propagation operations. Then tensorlayer inside how to set up?",would like use realize neon inside function neon inside set meaning parameter bool expect input coming source separate inside know set parameter parameter code inside neon see assuming input respectively behalf parameter set true function running used forward propagation used backward propagation inside set,issue,positive,positive,positive,positive,positive,positive
278944567,"@zsdonghao ,,
I would like to use multilayer birnn, and should meet the following conditions:
1. Each layer uses batch normal, and clip relu to 20
2.to expect the input coming from the same source of separate Sources？


What should I do? Use tensorlayer",would like use meet following layer batch normal clip expect input coming source separate use,issue,negative,positive,neutral,neutral,positive,positive
278809508,"@xjtuljy in that case, you may need to extend `minibatches`.

```
def minibatches(inputs=None, targets=None, batch_size=None, shuffle=False):
    """"""Generate a generator that input a group of example in numpy.array and
    their labels, return the examples and labels by the given batchsize.
    Parameters
    ----------
    inputs : numpy.array
        (X) The input features, every row is a example.
    targets : numpy.array
        (y) The labels of inputs, every row is a example.
    batch_size : int
        The batch size.
    shuffle : boolean
        Indicating whether to use a shuffling queue, shuffle the dataset before return.
    Examples
    --------
    >>> X = np.asarray([['a','a'], ['b','b'], ['c','c'], ['d','d'], ['e','e'], ['f','f']])
    >>> y = np.asarray([0,1,2,3,4,5])
    >>> for batch in tl.iterate.minibatches(inputs=X, targets=y, batch_size=2, shuffle=False):
    >>>     print(batch)
    ... (array([['a', 'a'],
    ...        ['b', 'b']],
    ...         dtype='<U1'), array([0, 1]))
    ... (array([['c', 'c'],
    ...        ['d', 'd']],
    ...         dtype='<U1'), array([2, 3]))
    ... (array([['e', 'e'],
    ...        ['f', 'f']],
    ...         dtype='<U1'), array([4, 5]))
    """"""
    assert len(inputs) == len(targets)
    if shuffle:
        indices = np.arange(len(inputs))
        np.random.shuffle(indices)
    for start_idx in range(0, len(inputs) - batch_size + 1, batch_size):
        if shuffle:
            excerpt = indices[start_idx:start_idx + batch_size]
        else:
            excerpt = slice(start_idx, start_idx + batch_size)
        yield inputs[excerpt], targets[excerpt]
```",case may need extend generate generator input group example return given input every row example every row example batch size shuffle whether use shuffling queue shuffle return batch print batch array array array array array array assert shuffle index index range shuffle excerpt index else excerpt slice yield excerpt excerpt,issue,negative,neutral,neutral,neutral,neutral,neutral
278805612,"I wonder can we use tl.iterate.minibatches to generate ""feed_dict""  to feed multiple inputs into the session? I tried but it seems that minibatches() only takes 4 augments, which mean single input",wonder use generate feed multiple session tried mean single input,issue,negative,negative,negative,negative,negative,negative
278795508,"@guoying1030 both BiRNNLayer and BiDynamicRNNLayer have a args `n_layer`, you can just simply set the number of RNN layers .

Alternatively, you can stack layers like this example shows: https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py",simply set number alternatively stack like example,issue,negative,neutral,neutral,neutral,neutral,neutral
278255591,Is there any way to specify the weights variable to PS server while the matmul to worker server in a layer? Thank you.,way specify variable server worker server layer thank,issue,negative,neutral,neutral,neutral,neutral,neutral
278239284,"@wagamamaz 

```
net = tl.layers.BiRNNLayer(net, cell_fn=tf.nn.rnn_cell.LSTMCell,
                                              n_hidden=hidden_size,n_steps=1000,
                                              initializer=tf.random_uniform_initializer(0, 0.01),
                                              name='DeepBiRnn_' + str(i),return_seq_2d=isReturn2d)
print 'a'
net = tl.layers.BatchNormLayer(net, act=lambda x: tl.act.ramp(x,0,20),name='BatchNormal_'+str(i))
```

is this it???",net net print net net,issue,negative,neutral,neutral,neutral,neutral,neutral
277955891,"@zsdonghao  Thanks for your reply!

I agree with you. They are conceptually same.

Since in tensorflow, all noise layers, such as dropout, have the parameter is_training, I think it is easier to just pass the is_training to control all the noise layers.
",thanks reply agree conceptually since noise dropout parameter think easier pas control noise,issue,positive,positive,positive,positive,positive,positive
277393267,"@benwu232 sorry for the late reply...

Yes, it is the same idea. by defualt, tensorlayer uses placeholder to control the training and testing states ~  and all placeholders are stored in `network.all_drop` ",sorry late reply yes idea control training testing,issue,negative,negative,negative,negative,negative,negative
276973843,It solved the problem perfectly! Thanks a lot!!!,problem perfectly thanks lot,issue,positive,positive,positive,positive,positive,positive
275633288,"Yes. By using [caffe-tensorflow](https://github.com/ethereon/caffe-tensorflow) .

First, follow the instructions in caffe-tensorflow e.g. :
```
convert.py VGG_ILSVRC_19_layers_deploy.prototxt --caffemodel VGG_ILSVRC_19_layers.caffemodel --data-output-path=vgg19.npy
```
You can get prototxt and caffemodel here: [Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo).

Then you get `vgg19.npy` that stores pre-trained weights. Since `npy` here share different structure with `npz` which was commonly used in TensorLayer, you still need another step to extract the weighted (python3):
```pyhton3
npy = np.load(vgg_weights_dir, encoding='latin1')
params = []
for val in sorted(npy.item().items()):
    if val[0] == 'conv5_2':
        break
    params.append(val[1]['weights'])
    params.append(val[1]['biases'])
tl.files.assign_params(sess, params, network)
```
The `if val[0] == 'conv5_2': break` means to skip the weights after conv5_2, which is just for my own project.
",yes first follow get model zoo get since share different structure commonly used still need another step extract weighted python sorted break sess network break skip project,issue,positive,negative,neutral,neutral,negative,negative
274677481,"@zsdonghao  Thanks for the answer!

I just tried to use a placeholder to pass the is_training parameter to the network (written by tensorflow or slim) to control it switching between training and testing states. And seems it works. It is something like this:

is_training_ph = tf.placeholder(tf.bool)
...

net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')
...

",thanks answer tried use pas parameter network written slim control switching training testing work something like net net,issue,positive,positive,neutral,neutral,positive,positive
274100039,"@benwu232 It is correct.
tf-slim don't use placeholder to control the keep probs, it equivalent to 
```python
if is_train:
    network = DropoutLayer(network, 0.8, is_fix=True, name='xxx')
```
then by setting  `is_train` you control training and testing phase",correct use control keep equivalent python network network setting control training testing phase,issue,negative,neutral,neutral,neutral,neutral,neutral
273748571,"@zergxyz  Good idea, I already label this issue as `help wanted`.

If anyone want to work on this, I can help.",good idea already label issue help anyone want work help,issue,positive,positive,positive,positive,positive,positive
273138780,"Set the environment variable PYTHONPATH in eclipse RunConfiguration, this problem is solved. thanks!",set environment variable eclipse problem thanks,issue,negative,positive,positive,positive,positive,positive
272627749,@jjkke88 can't help without your code,ca help without code,issue,negative,neutral,neutral,neutral,neutral,neutral
272290757,@boscotsang the lastest implementation use the fixed mean and variance of training data.,implementation use fixed mean variance training data,issue,negative,negative,negative,negative,negative,negative
272179727,I've adding the path argument to the load dataset functions. I think you can replace the origin download and extract function with your maybe_download_and_extract() function.,path argument load think replace origin extract function function,issue,negative,neutral,neutral,neutral,neutral,neutral
272178993,Can you combine this with PR 60 Please? Cleaner code that way imho,combine please cleaner code way,issue,negative,neutral,neutral,neutral,neutral,neutral
272178714,Could you merge this with PR 60 and that func to clean up code a bit?,could merge clean code bit,issue,negative,positive,positive,positive,positive,positive
272177280,"@boscotsang good idea, go ahead. please make sure previous codes can run normally.

Thank you ~~",good idea go ahead please make sure previous run normally thank,issue,positive,positive,positive,positive,positive,positive
271979029,Thanks! @ritchieng 's code is quite helpful!,thanks code quite helpful,issue,positive,positive,positive,positive,positive,positive
271977229,"@xjtuljy You can connect TF-Slim into TensorLayer, as TF-Slim have pre-trained resnet, inception , vgg models .. see `tutorial_inceptionV3_tfslim.py` in example folder.

Besides, here is a [resnet example](https://github.com/ritchieng/wideresnet-tensorlayer) implemented by @ritchieng 

",connect inception see example folder besides example,issue,negative,neutral,neutral,neutral,neutral,neutral
271957119,"Are sure you have correctly installed tensorlayer and that your PYTHONPATH enviroment variable is set correctly?

comapre the PYTHONPATH in eclipse and Pycharm for example.",sure correctly variable set correctly eclipse example,issue,negative,positive,positive,positive,positive,positive
271892599,"Here I found the changes may cause this problem. 
[changes](https://github.com/tensorflow/tensorflow/commit/b8101a2b0a7ff1b96999c8e7ad213d1f36b4a4ab)

The rnn_cell was moved to tf.contrib instead after Dec 5. Why did the document not mention this change? 

@wagamamaz @Bishopxu ",found may cause problem instead document mention change,issue,negative,neutral,neutral,neutral,neutral,neutral
271795023,"I think the problem is not resolved by tensorflow's update. I try a lot these days.
when I installed tensoflow by bazel+tensorflow CMake build, I always got the  AttributeError: 'module' object has no attribute 'rnn_cell'.
When I used pip install, I didn't encounter this problem.",think problem resolved update try lot day build always got object attribute used pip install encounter problem,issue,negative,neutral,neutral,neutral,neutral,neutral
271783758,"@wagamamaz same problem with those above: **AttributeError: 'module' object has no attribute 'rnn_cell'**

And for me, it seems the only way to import functions in rnn_cell is: 
**from tensorflow.contrib.rnn.python ops import core_rnn_cell_impl**

I compile the master version from source following the stander instruction in the tutorial. Is there any compile procedure changed? ",problem object attribute way import import compile master version source following stander instruction tutorial compile procedure,issue,negative,neutral,neutral,neutral,neutral,neutral
271654081,"I think contributors may like to use built-in library, and just simply excute a .py program ~",think may like use library simply program,issue,negative,neutral,neutral,neutral,neutral,neutral
271641024,"I also met the problem and update to 0.12.1 does not work for me! Any ideas? 
Thanks! ",also met problem update work thanks,issue,negative,positive,positive,positive,positive,positive
271613739,";) What kind of approach would be best?
using pytest or built in unittest?",kind approach would best built,issue,positive,positive,positive,positive,positive,positive
271613166,"Is something like this interface what you had in mind? Or should I change it to something else?
I could also clean up some code in the files.py to use this new function instead of repeated definitions of private download functions if you like.",something like interface mind change something else could also clean code use new function instead repeated private like,issue,positive,positive,positive,positive,positive,positive
271360205,"@boscotsang 
I have two implementation at the moment : `BatchNormLayer` and `BatchNormLayer5` in https://github.com/zsdonghao/tensorlayer/blob/master/tensorlayer/layers.py

Now, I save the beta and gamma only. Yes, I agree with you, "" it seems that the mean and variance of test phase should take the mean and variance estimated during training "" instead of ""using mean and variance of the testing data"".",two implementation moment save beta gamma yes agree mean variance test phase take mean variance training instead mean variance testing data,issue,positive,negative,negative,negative,negative,negative
271320645,"@zsdonghao How can I share the variables of BN in training and evaluating phase to evaluate while training? Besides, it seems that the mean and variance of test phase should take the mean and variance estimated during training because during the test phase there may be only one image so that it's hard to estimate the mean and variance of one image.",share training phase evaluate training besides mean variance test phase take mean variance training test phase may one image hard estimate mean variance one image,issue,negative,negative,negative,negative,negative,negative
271200698,"Just updated a new version with an optional extract=True/False option. If set to True, detects if .tar or .zip files and uncompresses.",new version optional option set true,issue,negative,positive,positive,positive,positive,positive
271192603,"@JoelKronander could we simplify the args of function ? e.g. use `format='zip'` instead of `is_tar, is_zip`? do you think it is better?",could simplify function use instead think better,issue,negative,positive,positive,positive,positive,positive
271155307,"Ok, I will submit a pull request adding a function to get such an initializer in layers.py.
Where would it be appropriate to submit a corresponding test case of the function?
At some point I could also look into submitting a FCN segmantic segmentation example.",submit pull request function get would appropriate submit corresponding test case function point could also look segmentation example,issue,negative,positive,positive,positive,positive,positive
271133618,"I think it should work, but it that possible don't add new code into every layers?",think work possible add new code every,issue,negative,positive,neutral,neutral,positive,positive
271131830,"@zsdonghao Thx. And w.r.t network.get_layers_by_name maybe :
```python
self.all_layers = layer.all_layers
...
self.all_layers[name] = self.output
```
and:
```python
def get_layers_by_name(self, name):
    print(""  Get layers in a network with %s"" % name)
    d_layers = self.all_layers.get(name, None)
    if d_layers is None:
        print("" %s not implemented "".format(name))
    return d_layers
```
Could work ?",maybe python name python self name print get network name name none none print name return could work,issue,negative,neutral,neutral,neutral,neutral,neutral
271127122,"@JindongJiang Good idea, indeed.

I think we can create 2 functions for `Layer` class : `network.get_layers_by_name(""conv2"")` and `network.get_params_by_name(""W"")`.

`network.get_params_by_name(""W"")` should be similar with `tl.layer. get_variables_with_name()` as follow:
```
def get_variables_with_name(name, train_only=True, printable=False):
    """"""Get variable list by a given name scope.

    Examples
    ---------
    >>> dense_vars = get_variable_with_name('dense', True, True)
    """"""
    print(""  Get variables with %s"" % name)
    t_vars = tf.trainable_variables() if train_only else tf.all_variables()
    d_vars = [var for var in t_vars if name in var.name]
    if printable:
        for idx, v in enumerate(d_vars):
            print(""  got {:3}: {:15}   {}"".format(idx, v.name, str(v.get_shape())))
    return d_vars
```

You can try this code by adding it to `Layer`:
```
def get_params_by_name(self, name, printable=False):
    print(""  Get params in a network with %s"" % name)
    t_vars = self.all_params
    d_vars = [var for var in t_vars if name in var.name]
    if printable:
        for idx, v in enumerate(d_vars):
            print(""  got {:3}: {:15}   {}"".format(idx, v.name, str(v.get_shape())))
    return d_vars
```",good idea indeed think create layer class similar follow name get variable list given name scope true true print get name else name printable enumerate print got return try code layer self name print get network name name printable enumerate print got return,issue,positive,positive,positive,positive,positive,positive
271126926,"@JoelKronander oh, great you did it.
for contribution, you can put it into layers.py ;  when we collect a lot of special initializer , we may create init.py ",oh great contribution put collect lot special may create,issue,positive,positive,positive,positive,positive,positive
271126273,"Hi, 

By bilinear weights I mean initializing the DeConvLayer weights so it replicates a standard bilinear image upsampling (resampling). This is used in some Semantic Segmentation papers, such as [FCN](https://arxiv.org/abs/1605.06211) e.g. 

Below is some code to do that by creating bilinear weights in numpy and then passing them to a constant_initializer, then passed as the W_init parameter when creating the DeConv2dLayer. 
Not sure if/where it would make sense to add/contribute something like this to tensorlayer, custom initializers somewhere?

```
import tensorflow as tf
import tensorlayer as tl
import numpy as np
import skimage
from scipy.misc import imread, imresize, imsave

sess = tf.InteractiveSession()

imsize = 100
scale = 2
filter_size = (2 * scale - scale % 2)
num_channels = 3

#Create bilinear weights in numpy array
bilinear_kernel = np.zeros([filter_size, filter_size], dtype=np.float32)
scale_factor = (filter_size + 1) // 2
if filter_size % 2 == 1:
    center = scale_factor - 1
else:
    center = scale_factor - 0.5
for x in range(filter_size):
    for y in range(filter_size):
        bilinear_kernel[x,y] = (1 - abs(x - center) / scale_factor) * \
                               (1 - abs(y - center) / scale_factor)
weights = np.zeros((filter_size, filter_size, num_channels, num_channels))
for i in range(num_channels):
    weights[:, :, i, i] = bilinear_kernel

#assign numpy array to constant_initalizer and pass to get_variable
bilinear_init = tf.constant_initializer(value=weights, dtype=tf.float32)

x = tf.placeholder(tf.float32, [1, imsize, imsize, num_channels])

network = tl.layers.InputLayer(x, name='input_layer')
network = tl.layers.DeConv2dLayer(network,
                            shape = [filter_size, filter_size, num_channels, num_channels],
                            output_shape = [1, imsize*scale, imsize*scale, num_channels],
                            strides=[1, scale, scale, 1],
                            W_init=bilinear_init,
                            padding='SAME',
                            act=tf.identity, name='g/h1/decon2d')
y=network.outputs
sess.run(tf.global_variables_initializer())

#Random test image
img = np.random.random_sample((imsize,imsize,num_channels))

deconv_result = sess.run(y, feed_dict={x: [img]})
#skimag
upsampled_skimage = skimage.transform.rescale(img,
                                     scale,
                                     mode='constant', #zero padding as in tf conv2d_transpose
                                     cval=0,
                                     order=1, #corresponds to bilinear upsampling
                                     preserve_range=False)

print(np.allclose(upsampled_skimage,deconv_result))

imsave('skim.png',upsampled_skimage)
imsave('deconv2d.png', np.squeeze(deconv_result))
```",hi bilinear mean standard bilinear image used semantic segmentation code bilinear passing parameter sure would make sense something like custom somewhere import import import import import sess scale scale scale create bilinear array center else center range range center center range assign array pas network network network shape scale scale scale scale random test image scale zero padding bilinear print,issue,positive,negative,neutral,neutral,negative,negative
271086120,"@boscotsang To evaluate the performance, you need a inference with `is_train=False`.

e.g.
```python
network = inference(x, is_train=True, reuse=False)
network_test = inference(x, is_train=False, reuse=True)
```
Donot use the `network` to evaluate the performance,
and @zsdonghao just update `BatchNormLayer` for TF12, it works in my case, please try yours.",evaluate performance need inference python network inference inference use network evaluate performance update work case please try,issue,negative,neutral,neutral,neutral,neutral,neutral
271085487,"@wagamamaz  The following is the my code. The image read is the tensorflow pipeline. The data is the cifar10 binary and is put in the dataset directory.
```
# cifar10_input.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os

from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf

IMAGE_SIZE = 32

# Global constants describing the CIFAR-10 data set.
NUM_CLASSES = 10
NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000
NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000


def read_cifar10(filename_queue):
    class CIFAR10Record(object):
        pass

    result = CIFAR10Record()

    label_bytes = 1
    result.height = 32
    result.width = 32
    result.depth = 3
    image_bytes = result.height * result.width * result.depth
    record_bytes = label_bytes + image_bytes
    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)
    result.key, value = reader.read(filename_queue)
    record_bytes = tf.decode_raw(value, tf.uint8)
    result.label = tf.cast(
        tf.slice(record_bytes, [0], [label_bytes]), tf.int32)
    depth_major = tf.reshape(tf.slice(record_bytes, [label_bytes], [image_bytes]),
                             [result.depth, result.height, result.width])
    result.uint8image = tf.pad(tf.transpose(depth_major, [1, 2, 0]), [[1, 1], [1, 1], [0, 0]])
    return result


def _generate_image_and_label_batch(image, label, min_queue_examples,
                                    batch_size, shuffle):
    num_preprocess_threads = 24
    if shuffle:
        images, label_batch = tf.train.shuffle_batch(
            [image, label],
            batch_size=batch_size,
            num_threads=num_preprocess_threads,
            capacity=min_queue_examples + 3 * batch_size,
            min_after_dequeue=min_queue_examples)
    else:
        images, label_batch = tf.train.batch(
            [image, label],
            batch_size=batch_size,
            num_threads=num_preprocess_threads,
            capacity=min_queue_examples + 3 * batch_size)
    return images, tf.reshape(label_batch, [batch_size])


def distorted_inputs(data_dir, batch_size):
    filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)
                 for i in xrange(1, 6)]
    for f in filenames:
        if not tf.gfile.Exists(f):
            raise ValueError('Failed to find file: ' + f)
    filename_queue = tf.train.string_input_producer(filenames)
    read_input = read_cifar10(filename_queue)
    reshaped_image = tf.cast(read_input.uint8image, tf.float32)
    height = IMAGE_SIZE
    width = IMAGE_SIZE
    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])
    distorted_image = tf.image.random_flip_left_right(distorted_image)
    distorted_image = tf.image.random_brightness(distorted_image,
                                                 max_delta=63)
    distorted_image = tf.image.random_contrast(distorted_image,
                                               lower=0.2, upper=1.8)
    float_image = tf.image.per_image_standardization(distorted_image)
    min_fraction_of_examples_in_queue = 0.4
    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN *
                             min_fraction_of_examples_in_queue)
    print('Filling queue with %d CIFAR images before starting to train. '
          'This will take a few minutes.' % min_queue_examples)
    return _generate_image_and_label_batch(float_image, read_input.label,
                                           min_queue_examples, batch_size,
                                           shuffle=True)


def inputs(eval_data, data_dir, batch_size):
    if not eval_data:
        filenames = [os.path.join(data_dir, 'data_batch_%d.bin' % i)
                     for i in xrange(1, 6)]
        num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN
    else:
        filenames = [os.path.join(data_dir, 'test_batch.bin')]
        num_examples_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_EVAL

    for f in filenames:
        if not tf.gfile.Exists(f):
            raise ValueError('Failed to find file: ' + f)
    filename_queue = tf.train.string_input_producer(filenames)
    read_input = read_cifar10(filename_queue)
    reshaped_image = tf.cast(read_input.uint8image, tf.float32)
    height = IMAGE_SIZE
    width = IMAGE_SIZE
    resized_image = tf.image.resize_image_with_crop_or_pad(reshaped_image,
                                                           width, height)
    float_image = tf.image.per_image_standardization(resized_image)
    # float_image = tf.image.per_image_standardization(reshaped_image)
    min_fraction_of_examples_in_queue = 0.4
    min_queue_examples = int(num_examples_per_epoch *
                             min_fraction_of_examples_in_queue)
    return _generate_image_and_label_batch(float_image, read_input.label,
                                           min_queue_examples, batch_size,
                                           shuffle=False)

```

```
# cifar10_resnet.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import numpy as np
import tensorflow as tf
import tensorlayer as tl
import cifar10_input

FLAGS = tf.app.flags.FLAGS

# Basic model parameters.
tf.app.flags.DEFINE_integer('batch_size', 128,
                            """"""Number of images to process in a batch."""""")
tf.app.flags.DEFINE_string('data_dir', 'datasets/cifar10_data',
                           """"""Path to the CIFAR-10 data directory."""""")
tf.app.flags.DEFINE_boolean('use_fp16', False,
                            """"""Train the model using fp16."""""")

IMAGE_SIZE = cifar10_input.IMAGE_SIZE
NUM_CLASSES = cifar10_input.NUM_CLASSES
NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN
NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = cifar10_input.NUM_EXAMPLES_PER_EPOCH_FOR_EVAL

MOVING_AVERAGE_DECAY = 0.9999  # The decay to use for the moving average.
NUM_EPOCHS_PER_DECAY = 350.0  # Epochs after which learning rate decays.


# LEARNING_RATE_DECAY_FACTOR = 0.1  # Learning rate decay factor.


# INITIAL_LEARNING_RATE = 0.05  # Initial learning rate.


def distorted_inputs():
    if not FLAGS.data_dir:
        raise ValueError('Please supply a data_dir')
    data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')
    images, labels = cifar10_input.distorted_inputs(data_dir=data_dir,
                                                    batch_size=FLAGS.batch_size)
    if FLAGS.use_fp16:
        images = tf.cast(images, tf.float16)
        labels = tf.cast(labels, tf.float16)
    return images, labels


def inputs(eval_data):
    if not FLAGS.data_dir:
        raise ValueError('Please supply a data_dir')
    data_dir = os.path.join(FLAGS.data_dir, 'cifar-10-batches-bin')
    images, labels = cifar10_input.inputs(eval_data=eval_data,
                                          data_dir=data_dir,
                                          batch_size=FLAGS.batch_size)
    if FLAGS.use_fp16:
        images = tf.cast(images, tf.float16)
        labels = tf.cast(labels, tf.float16)
    return images, labels

def inference(images, reuse=False, is_train=True):
    def cblock(x, in_filter, nb_filter, stride, stage):
        with tf.variable_scope(""CBlock{}"".format(stage), reuse=reuse) as scope:
            shortcut = x
            x = tl.layers.BatchNormLayer(x, act=tf.nn.relu, is_train=is_train, name=scope.name + ""BN1"")
            # x = tl.layers.PReluLayer(x, name=scope.name + ""_PRELU1"")
            _shortcut = x
            x = tl.layers.Conv2d(x, nb_filter // 4, (1, 1), (stride, stride), padding='SAME',
                                 W_init=tf.contrib.layers.variance_scaling_initializer(
                                     factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32),
                                 name=scope.name + ""CONV1"")
            x = tl.layers.BatchNormLayer(x, act=tf.nn.relu, is_train=is_train, name=scope.name + ""BN2"")
            # x = tl.layers.PReluLayer(x, name=scope.name + ""_PRELU2"")
            x = tl.layers.Conv2d(x, nb_filter // 4, (3, 3), (1, 1),
                                 padding=""SAME"",
                                 W_init=tf.contrib.layers.variance_scaling_initializer(
                                     factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32),
                                 name=scope.name + ""CONV2"")
            x = tl.layers.BatchNormLayer(x, act=tf.nn.relu, is_train=is_train, name=scope.name + ""BN3"")
            # x = tl.layers.PReluLayer(x, name=scope.name + ""_PRELU3"")
            x = tl.layers.Conv2d(x, nb_filter, (1, 1), (1, 1),
                                 padding=""SAME"",
                                 W_init=tf.contrib.layers.variance_scaling_initializer(
                                     factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32),
                                 name=scope.name + ""CONV3"")
            if nb_filter != in_filter:
                shortcut = tl.layers.Conv2d(_shortcut, nb_filter, (1, 1), (stride, stride), padding='VALID',
                                            W_init=tf.contrib.layers.variance_scaling_initializer(
                                                factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32),
                                            name=scope.name + ""IDENTITY"")
            out = tl.layers.ElementwiseLayer((x, shortcut), tf.add, name=scope.name + ""_Add"")
            return out

    with tf.variable_scope(""Net"", reuse=reuse) as scope:
        tl.layers.set_name_reuse(reuse)
        x = tl.layers.InputLayer(images, name=scope.name + ""_INPUT"")
        x = tl.layers.BatchNormLayer(x, act=tf.nn.relu, is_train=is_train, name=scope.name + ""_BN"")
        # x = tl.layers.PReluLayer(x, name=scope.name + ""_CONVOUT"")
        x = tl.layers.Conv2d(x, 16, (3, 3), (1, 1),
                             padding='SAME',
                             W_init=tf.contrib.layers.variance_scaling_initializer(
                                 factor=2.0, mode='FAN_IN', uniform=False, seed=None, dtype=tf.float32),
                             name=scope.name + ""_CONV"")
        start, n = 0, 18
        for i in range(start, start + n):
            x = cblock(x, 16, 64, 1, i)
        start += n
        for i in range(start, start + n):
            if i == start:
                x = cblock(x, 64, 128, 2, i)
            else:
                x = cblock(x, 64, 128, 1, i)
        start += n
        for i in range(start, start + n):
            if i == start:
                x = cblock(x, 128, 256, 2, i)
            else:
                x = cblock(x, 128, 256, 1, i)
        x = tl.layers.BatchNormLayer(x, act=tf.nn.relu, is_train=is_train, name=scope.name + ""_OUTBN"")
        # x = tl.layers.PReluLayer(x, name=scope.name + ""_OUTPRELU"")
        pool = tl.layers.MeanPool2d(x, (8, 8), (1, 1), padding='VALID')
        out = tl.layers.FlattenLayer(pool, name=scope.name + ""_Flatten"")
        fc = tl.layers.DenseLayer(out, 10)
        return fc


def train(total_loss, global_step):
    num_batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / FLAGS.batch_size
    # decay_steps = int(num_batches_per_epoch * NUM_EPOCHS_PER_DECAY)
    boundaries = (2000, 30000, 80000)
    values = (0.01, 0.1, 0.01, 0.001)
    lr = tf.train.piecewise_constant(global_step, boundaries, values)
    tf.summary.scalar('learning_rate', lr)
    opt = tf.train.AdamOptimizer(lr)
    grads = opt.compute_gradients(total_loss)
    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)
    for var in tf.trainable_variables():
        tf.summary.histogram(var.op.name, var)
    for grad, var in grads:
        if grad is not None:
            tf.summary.histogram(var.op.name + '/gradients', grad)
    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)
    variables_averages_op = variable_averages.apply(tf.trainable_variables())
    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):
        train_op = tf.no_op(name='train')
    return train_op


def correct(logits, labels):
    return tf.nn.in_top_k(logits.outputs, labels, 1)


def loss(logits, labels, l2=0.0001):
    l2_v = tf.Variable(l2, trainable=False, dtype=tf.float32)
    l2_loss = tf.add_n([tf.nn.l2_loss(x) for x in logits.all_params if x.name.endswith(""W_conv2d:0"")])
    return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits.outputs, labels)) + l2_v*l2_loss
```

```
# cifar10_resnet.py
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from datetime import datetime
import os.path
import time

import numpy as np
from six.moves import xrange  # pylint: disable=redefined-builtin
import tensorflow as tf

import cifar10_resnet

FLAGS = tf.app.flags.FLAGS

tf.app.flags.DEFINE_string('train_dir', 'C:/Users/zgj/.keras/datasets/cifar10_train',
                           """"""Directory where to write event logs """"""
                           """"""and checkpoint."""""")
tf.app.flags.DEFINE_integer('max_steps', 100000,
                            """"""Number of batches to run."""""")
tf.app.flags.DEFINE_boolean('log_device_placement', False,
                            """"""Whether to log device placement."""""")


def train():
    with tf.Graph().as_default():
        global_step = tf.Variable(0, trainable=False)
        images, labels = cifar10_resnet.distorted_inputs()
        images_eval, labels_eval = cifar10_resnet.inputs(True)

        logits = cifar10_resnet.inference(images, False, True)
        logits_eval = cifar10_resnet.inference(images_eval, True, False)
        correct = cifar10_resnet.correct(logits_eval, labels_eval)

        # Calculate loss.
        loss = cifar10_resnet.loss(logits, labels)
        loss_eval = cifar10_resnet.loss(logits_eval, labels_eval)
        train_op = cifar10_resnet.train(loss, global_step)
        saver = tf.train.Saver(tf.all_variables())
        summary_op = tf.summary.merge_all()

        init = tf.global_variables_initializer()

        sess = tf.Session(config=tf.ConfigProto(log_device_placement=FLAGS.log_device_placement))
        sess.run(init)
        tf.train.start_queue_runners(sess=sess)
        summary_writer = tf.summary.FileWriter(FLAGS.train_dir, sess.graph)
        correct_cnt = 0.
        for step in xrange(FLAGS.max_steps):
            start_time = time.time()
            _, loss_value, loss_eval_value, correct_val = sess.run([train_op, loss, loss_eval, correct])
            # _, loss_value = sess.run([train_op, loss])
            correct_cnt += np.sum(correct_val)
            duration = time.time() - start_time
            assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
            if step % 100 == 0:
                sec_per_batch = float(duration)

                format_str = ('%s: step %d, loss = %.4f, eval_loss = %.4f, eval_accuracy = %.4f (%.3f '
                              'sec/batch)')
                print(format_str % (datetime.now(), step, loss_value, loss_eval_value,
                                    correct_cnt / (100 * FLAGS.batch_size), sec_per_batch))
                summary_str = sess.run(summary_op)
                summary_writer.add_summary(summary_str, step)
                correct_cnt = 0
            if step % 1000 == 0 or (step + 1) == FLAGS.max_steps:
                checkpoint_path = os.path.join(FLAGS.train_dir, 'model.ckpt')
                saver.save(sess, checkpoint_path, global_step=step)

def main(argv=None):  # pylint: disable=unused-argument
    if tf.gfile.Exists(FLAGS.train_dir):
        tf.gfile.DeleteRecursively(FLAGS.train_dir)
    tf.gfile.MakeDirs(FLAGS.train_dir)
    train()


if __name__ == '__main__':
    tf.app.run()
```",following code image read pipeline data binary put directory import import division import import o import import global data set class object pas result reader value value return result image label shuffle shuffle image label else image label return raise find file height width height width print queue starting train take return else raise find file height width width height return import import division import import o import import import import basic model number process batch path data directory false train model decay use moving average learning rate learning rate decay factor initial learning rate raise supply return raise supply return inference stride stage stage scope stride stride stride stride identity return net scope reuse start range start start start range start start start else start range start start start else pool pool return train opt grad grad none grad return correct return loss return import import division import import import import time import import import import directory write event number run false whether log device placement train true false true true false correct calculate loss loss loss saver sess step loss correct loss duration assert loss step float duration step loss print step step step step sess main train,issue,negative,negative,neutral,neutral,negative,negative
271085025,"@JoelKronander sorry of the late reply.
I am not quite understand what bilinear weights stand for, do you mean given a matrix
`[[1, 4], [1, 4]]`, you can upsample to `[[1, 2, 3, 4], [1, 2, 3, 4]]`?",sorry late reply quite understand bilinear stand mean given matrix,issue,negative,negative,negative,negative,negative,negative
271084773,"This is a good example provided by @wagamamaz 
```
#! /usr/bin/python
# -*- coding: utf8 -*-

import numpy as np
import tensorflow as tf
import tensorlayer as tl
from tensorlayer.layers import set_keep
import time

is_test_only = False # if True, restore and test without training

X_train, y_train, X_val, y_val, X_test, y_test = \
                tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))

X_train = np.asarray(X_train, dtype=np.float32)[0:10000]#<-- small training set for fast debugging
y_train = np.asarray(y_train, dtype=np.int64)[0:10000]
X_val = np.asarray(X_val, dtype=np.float32)
y_val = np.asarray(y_val, dtype=np.int64)
X_test = np.asarray(X_test, dtype=np.float32)
y_test = np.asarray(y_test, dtype=np.int64)

print('X_train.shape', X_train.shape)
print('y_train.shape', y_train.shape)
print('X_val.shape', X_val.shape)
print('y_val.shape', y_val.shape)
print('X_test.shape', X_test.shape)
print('y_test.shape', y_test.shape)
print('X %s   y %s' % (X_test.dtype, y_test.dtype))

sess = tf.InteractiveSession()

batch_size = 128
x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])
y_ = tf.placeholder(tf.int64, shape=[None,])

def inference(x, is_train, reuse=None):
    # gamma_init = tf.random_normal_initializer(1., 0.02)
    with tf.variable_scope(""CNN"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        network = tl.layers.InputLayer(x, name='input_layer')

        network = tl.layers.Conv2d(network, n_filter=32, filter_size=(5, 5), strides=(1, 1),
                act=None, b_init=None, padding='SAME', name='cnn_layer1')
        network = tl.layers.BatchNormLayer(network, act=tf.identity,#tf.nn.relu,
                # gamma_init=gamma_init,
                is_train=is_train, name='batch1')
        check = network.outputs
        network.outputs = tf.nn.relu(network.outputs)
        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer1')

        network = tl.layers.Conv2d(network, n_filter=64, filter_size=(5, 5), strides=(1, 1),
                act=None, b_init=None, padding='SAME', name='cnn_layer2')
        network = tl.layers.BatchNormLayer(network, act=tf.nn.relu,
                # gamma_init=gamma_init,
                is_train=is_train, name='batch2')
        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer2')

        ## end of conv
        network = tl.layers.FlattenLayer(network, name='flatten_layer')
        # if is_train:
        #     network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, name='drop1')
        network = tl.layers.DenseLayer(network, n_units=256,
                                        act = tf.nn.relu, name='relu1')
        # if is_train:
        #     network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, name='drop2')
        network = tl.layers.DenseLayer(network, n_units=10,
                                        act = tf.identity, name='output_layer')
    return network, check


# train phase
network, check = inference(x, is_train=True, reuse=False)
y = network.outputs
cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
correct_prediction = tf.equal(tf.argmax(y, 1), y_)
acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# test phase
network_test, check_t = inference(x, is_train=False, reuse=True)
y_t = network_test.outputs
cost_t = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y_t, y_))
correct_prediction_t = tf.equal(tf.argmax(y_t, 1), y_)
acc_t = tf.reduce_mean(tf.cast(correct_prediction_t, tf.float32))

# train
n_epoch = 5
learning_rate = 0.0001
print_freq = 1

train_params = network.all_params
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
    epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

tl.layers.initialize_global_variables(sess)

if is_test_only:
    load_params = tl.files.load_npz(name='_model_test.npz')
    tl.files.assign_params(sess, load_params, network)

# network.print_params(True)
network.print_layers()

# tl.layers.print_all_variables(train_only=True)

for i, p in enumerate(tf.all_variables()):
    print(""  Before {:3}: {:15} (mean: {:<18}, median: {:<18}, std: {:<18})   {}"".format(i, str(p.eval().shape), p.eval().mean(), np.median(p.eval()), p.eval().std(), p.name))

print('   learning_rate: %f' % learning_rate)
print('   batch_size: %d' % batch_size)

if not is_test_only:
    for epoch in range(n_epoch):
        start_time = time.time()
        for X_train_a, y_train_a in tl.iterate.minibatches(
                                    X_train, y_train, batch_size, shuffle=True):
            _, c = sess.run([train_op, check], feed_dict={x: X_train_a, y_: y_train_a})
            # print('bn out train:', np.mean(c), np.std(c))

        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:
            print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))
            train_loss, train_acc, n_batch = 0, 0, 0
            for X_train_a, y_train_a in tl.iterate.minibatches(
                                    X_train, y_train, batch_size, shuffle=True):
                err, ac = sess.run([cost_t, acc_t], feed_dict={x: X_train_a, y_: y_train_a})
                train_loss += err; train_acc += ac; n_batch += 1
            print(""   train loss: %f"" % (train_loss/ n_batch))
            print(""   train acc: %f"" % (train_acc/ n_batch))
            val_loss, val_acc, n_batch = 0, 0, 0
            for X_val_a, y_val_a in tl.iterate.minibatches(
                                        X_val, y_val, batch_size, shuffle=True):
                err, ac = sess.run([cost_t, acc_t], feed_dict={x: X_val_a, y_: y_val_a})
                val_loss += err; val_acc += ac; n_batch += 1
            print(""   val loss: %f"" % (val_loss/ n_batch))
            print(""   val acc: %f"" % (val_acc/ n_batch))

print('Evaluation')
test_loss, test_acc, n_batch = 0, 0, 0
for X_test_a, y_test_a in tl.iterate.minibatches(
                            X_test, y_test, batch_size=1, shuffle=True):
    err, ac, c = sess.run([cost_t, acc_t, check_t], feed_dict={x: X_test_a, y_: y_test_a})
    # print('bn out test:', np.mean(c), np.std(c))
    test_loss += err; test_acc += ac; n_batch += 1
print(""   test loss: %f"" % (test_loss/n_batch))
print(""   test acc: %f"" % (test_acc/n_batch))

# network.print_param#! /usr/bin/python
# -*- coding: utf8 -*-

import numpy as np
import tensorflow as tf
import tensorlayer as tl
from tensorlayer.layers import set_keep
import time

is_test_only = False # if True, restore and test without training

X_train, y_train, X_val, y_val, X_test, y_test = \
                tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))

X_train = np.asarray(X_train, dtype=np.float32)[0:10000]#<-- small training set for fast debugging
y_train = np.asarray(y_train, dtype=np.int64)[0:10000]
X_val = np.asarray(X_val, dtype=np.float32)
y_val = np.asarray(y_val, dtype=np.int64)
X_test = np.asarray(X_test, dtype=np.float32)
y_test = np.asarray(y_test, dtype=np.int64)

print('X_train.shape', X_train.shape)
print('y_train.shape', y_train.shape)
print('X_val.shape', X_val.shape)
print('y_val.shape', y_val.shape)
print('X_test.shape', X_test.shape)
print('y_test.shape', y_test.shape)
print('X %s   y %s' % (X_test.dtype, y_test.dtype))

sess = tf.InteractiveSession()

batch_size = 128
x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])
y_ = tf.placeholder(tf.int64, shape=[None,])

def inference(x, is_train, reuse=None):
    # gamma_init = tf.random_normal_initializer(1., 0.02)
    with tf.variable_scope(""CNN"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        network = tl.layers.InputLayer(x, name='input_layer')

        network = tl.layers.Conv2d(network, n_filter=32, filter_size=(5, 5), strides=(1, 1),
                act=None, b_init=None, padding='SAME', name='cnn_layer1')
        network = tl.layers.BatchNormLayer(network, act=tf.identity,#tf.nn.relu,
                # gamma_init=gamma_init,
                is_train=is_train, name='batch1')
        check = network.outputs
        network.outputs = tf.nn.relu(network.outputs)
        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer1')

        network = tl.layers.Conv2d(network, n_filter=64, filter_size=(5, 5), strides=(1, 1),
                act=None, b_init=None, padding='SAME', name='cnn_layer2')
        network = tl.layers.BatchNormLayer(network, act=tf.nn.relu,
                # gamma_init=gamma_init,
                is_train=is_train, name='batch2')
        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer2')

        ## end of conv
        network = tl.layers.FlattenLayer(network, name='flatten_layer')
        # if is_train:
        #     network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, name='drop1')
        network = tl.layers.DenseLayer(network, n_units=256,
                                        act = tf.nn.relu, name='relu1')
        # if is_train:
        #     network = tl.layers.DropoutLayer(network, keep=0.5, is_fix=True, name='drop2')
        network = tl.layers.DenseLayer(network, n_units=10,
                                        act = tf.identity, name='output_layer')
    return network, check


# train phase
network, check = inference(x, is_train=True, reuse=False)
y = network.outputs
cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
correct_prediction = tf.equal(tf.argmax(y, 1), y_)
acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# test phase
network_test, check_t = inference(x, is_train=False, reuse=True)
y_t = network_test.outputs
cost_t = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y_t, y_))
correct_prediction_t = tf.equal(tf.argmax(y_t, 1), y_)
acc_t = tf.reduce_mean(tf.cast(correct_prediction_t, tf.float32))

# train
n_epoch = 5
learning_rate = 0.0001
print_freq = 1

train_params = network.all_params
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
    epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

tl.layers.initialize_global_variables(sess)

if is_test_only:
    load_params = tl.files.load_npz(name='_model_test.npz')
    tl.files.assign_params(sess, load_params, network)

# network.print_params(True)
network.print_layers()

# tl.layers.print_all_variables(train_only=True)

for i, p in enumerate(tf.all_variables()):
    print(""  Before {:3}: {:15} (mean: {:<18}, median: {:<18}, std: {:<18})   {}"".format(i, str(p.eval().shape), p.eval().mean(), np.median(p.eval()), p.eval().std(), p.name))

print('   learning_rate: %f' % learning_rate)
print('   batch_size: %d' % batch_size)

if not is_test_only:
    for epoch in range(n_epoch):
        start_time = time.time()
        for X_train_a, y_train_a in tl.iterate.minibatches(
                                    X_train, y_train, batch_size, shuffle=True):
            _, c = sess.run([train_op, check], feed_dict={x: X_train_a, y_: y_train_a})
            # print('bn out train:', np.mean(c), np.std(c))

        if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:
            print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))
            train_loss, train_acc, n_batch = 0, 0, 0
            for X_train_a, y_train_a in tl.iterate.minibatches(
                                    X_train, y_train, batch_size, shuffle=True):
                err, ac = sess.run([cost_t, acc_t], feed_dict={x: X_train_a, y_: y_train_a})
                train_loss += err; train_acc += ac; n_batch += 1
            print(""   train loss: %f"" % (train_loss/ n_batch))
            print(""   train acc: %f"" % (train_acc/ n_batch))
            val_loss, val_acc, n_batch = 0, 0, 0
            for X_val_a, y_val_a in tl.iterate.minibatches(
                                        X_val, y_val, batch_size, shuffle=True):
                err, ac = sess.run([cost_t, acc_t], feed_dict={x: X_val_a, y_: y_val_a})
                val_loss += err; val_acc += ac; n_batch += 1
            print(""   val loss: %f"" % (val_loss/ n_batch))
            print(""   val acc: %f"" % (val_acc/ n_batch))

print('Evaluation')
test_loss, test_acc, n_batch = 0, 0, 0
for X_test_a, y_test_a in tl.iterate.minibatches(
                            X_test, y_test, batch_size=1, shuffle=True):
    err, ac, c = sess.run([cost_t, acc_t, check_t], feed_dict={x: X_test_a, y_: y_test_a})
    # print('bn out test:', np.mean(c), np.std(c))
    test_loss += err; test_acc += ac; n_batch += 1
print(""   test loss: %f"" % (test_loss/n_batch))
print(""   test acc: %f"" % (test_acc/n_batch))

# network.print_params(True)

if not is_test_only:
    tl.files.save_npz(network.all_params, name='_model_test.npz', sess=sess)

for i, p in enumerate(tf.all_variables()):
    print(""  After {:3}: {:15} (mean: {:<18}, median: {:<18}, std: {:<18})   {}"".format(i, str(p.eval().shape), p.eval().mean(), np.median(p.eval()), p.eval().std(), p.name))
s(True)

if not is_test_only:
    tl.files.save_npz(network.all_params, name='_model_test.npz', sess=sess)

for i, p in enumerate(tf.all_variables()):
    print(""  After {:3}: {:15} (mean: {:<18}, median: {:<18}, std: {:<18})   {}"".format(i, str(p.eval().shape), p.eval().mean(), np.median(p.eval()), p.eval().std(), p.name))
```",good example provided import import import import import time false true restore test without training small training set fast print print print print print print print sess none none inference reuse network network network network network check network network network network network network network network end network network network network network network act network network network network act return network check train phase network check inference cost test phase inference train cost sess sess network true enumerate print mean median print print epoch range check print train epoch epoch print epoch took epoch err err print train loss print train err err print loss print print err print test err print test loss print test import import import import import time false true restore test without training small training set fast print print print print print print print sess none none inference reuse network network network network network check network network network network network network network network end network network network network network network act network network network network act return network check train phase network check inference cost test phase inference train cost sess sess network true enumerate print mean median print print epoch range check print train epoch epoch print epoch took epoch err err print train loss print train err err print loss print print err print test err print test loss print test true enumerate print mean median true enumerate print mean median,issue,positive,positive,neutral,neutral,positive,positive
271084299,"@boscotsang Thank you for your advice, please do that.

BTW, I am updating BatchNormLayer for TensorFlow 0.12, please check the lastest version in Github.

https://github.com/zsdonghao/tensorlayer/issues/57",thank advice please please check version,issue,positive,neutral,neutral,neutral,neutral,neutral
271084103,"please follow the lastest implementatiom `BatchNormLayer` and `BatchNormLayer5` in https://github.com/zsdonghao/tensorlayer/blob/master/tensorlayer/layers.py

---previous answer ---
Hi, I just make a commit for TF12, please have a try and let me know if there are any other issues.

```python

class BatchNormLayer5(Layer):   #
    """"""
    The :class:`BatchNormLayer` class is a normalization layer, see ``tf.nn.batch_normalization`` and ``tf.nn.moments``.

    Batch normalization on fully-connected or convolutional maps.

    Parameters
    -----------
    layer : a :class:`Layer` instance
        The `Layer` class feeding into this layer.
    decay : float
        A decay factor for ExponentialMovingAverage.
    epsilon : float
        A small float number to avoid dividing by 0.
    act : activation function.
    is_train : boolean
        Whether train or inference.
    beta_init : beta initializer
        The initializer for initializing beta
    gamma_init : gamma initializer
        The initializer for initializing gamma
    name : a string or None
        An optional name to attach to this layer.

    References
    ----------
    - `Source <https://github.com/ry/tensorflow-resnet/blob/master/resnet.py>`_
    - `stackoverflow <http://stackoverflow.com/questions/38312668/how-does-one-do-inference-with-batch-normalization-with-tensor-flow>`_
    """"""
    def __init__(
        self,
        layer = None,
        decay = 0.999,
        epsilon = 0.00001,
        act = tf.identity,
        is_train = False,
        beta_init = tf.zeros_initializer,
        # gamma_init = tf.ones_initializer,
        gamma_init = tf.random_normal_initializer(mean=1.0, stddev=0.002),
        name ='batchnorm_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        print(""  tensorlayer:Instantiate BatchNormLayer %s: decay: %f, epsilon: %f, act: %s, is_train: %s"" %
                            (self.name, decay, epsilon, act.__name__, is_train))
        x_shape = self.inputs.get_shape()
        params_shape = x_shape[-1:]

        from tensorflow.python.training import moving_averages
        from tensorflow.python.ops import control_flow_ops

        with tf.variable_scope(name) as vs:
            axis = list(range(len(x_shape) - 1))

            ## 1. beta, gamma
            beta = tf.get_variable('beta', shape=params_shape,
                               initializer=beta_init,
                               trainable=is_train)#, restore=restore)

            gamma = tf.get_variable('gamma', shape=params_shape,
                                initializer=gamma_init, trainable=is_train,
                                )#restore=restore)

            ## 2. moving variables during training (not update by gradient!)
            moving_mean = tf.get_variable('moving_mean',
                                      params_shape,
                                      initializer=tf.zeros_initializer,
                                      trainable=False,)#   restore=restore)
            moving_variance = tf.get_variable('moving_variance',
                                          params_shape,
                                          initializer=tf.constant_initializer(1.),
                                          trainable=False,)#   restore=restore)

            batch_mean, batch_var = tf.nn.moments(self.inputs, axis)
            ## 3.
            # These ops will only be preformed when training.
            def mean_var_with_update():
                try:    # TF12
                    update_moving_mean = moving_averages.assign_moving_average(
                                    moving_mean, batch_mean, decay, zero_debias=False)     # if zero_debias=True, has bias
                    update_moving_variance = moving_averages.assign_moving_average(
                                    moving_variance, batch_var, decay, zero_debias=False) # if zero_debias=True, has bias
                    # print(""TF12 moving"")
                except Exception as e:  # TF11
                    update_moving_mean = moving_averages.assign_moving_average(
                                    moving_mean, batch_mean, decay)
                    update_moving_variance = moving_averages.assign_moving_average(
                                    moving_variance, batch_var, decay)
                    # print(""TF11 moving"")

                with tf.control_dependencies([update_moving_mean, update_moving_variance]):
                    # return tf.identity(update_moving_mean), tf.identity(update_moving_variance)
                    return tf.identity(batch_mean), tf.identity(batch_var)

            if is_train:
                mean, var = mean_var_with_update()
            else:
                mean, var = (batch_mean, batch_var) # hao

            normed = tf.nn.batch_normalization(
              x=self.inputs,
              mean=mean,
              variance=var,
              offset=beta,
              scale=gamma,
              variance_epsilon=epsilon,
              name=""tf_bn""
            )
            self.outputs = act( normed )
            variables = [beta, gamma]

        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
        self.all_params.extend( variables )
```",please follow answer hi make commit please try let know python class layer class class normalization layer see batch normalization convolutional layer class layer instance layer class feeding layer decay float decay factor epsilon float small float number avoid dividing act activation function whether train inference beta beta gamma gamma name string none optional name attach layer source self layer none decay epsilon act false name self print decay epsilon act decay epsilon import import name axis list range beta gamma beta gamma moving training update gradient axis preformed training try decay bias decay bias print moving except exception decay decay print moving return return mean else mean hao act beta gamma list list,issue,negative,negative,negative,negative,negative,negative
271061897,"@boscotsang Can you show your code?

The test accuracy increase in my case
```
Epoch 1 of 200 took 133.758089s
   train loss: 0.711337
   train acc: 0.874659
   val loss: 0.662424
   val acc: 0.889323
```

Can you run your code again under TensorFlow 12 ? just to see whether your test accuracy increase.
or change the variables of `BatchNormLayer` to `variables = [beta, gamma, moving_mean, moving_variance]`?

",show code test accuracy increase case epoch took train loss train loss run code see whether test accuracy increase change beta gamma,issue,negative,neutral,neutral,neutral,neutral,neutral
271056440,"Yes, I'm using TF0.12r and I found that when I use BatchNormLayer and share variables between train and test as you code in my ResNet 164 on Cifar10  the training cost drops normally while the test cost nearly doesn't change. Did you have this issue?",yes found use share train test code training cost normally test cost nearly change issue,issue,positive,positive,positive,positive,positive,positive
270955249,"@boscotsang hi, are you using TL with TF12?

I found a interesting thing, when I use TF11, a `BatchNormLayer` only have 4 parameters, but have 8 parameters when using TF12. Do you have any idea about that? 

Thank you in advance.

TF11  TL1.3
```
  param   0: (5, 5, 1, 32)      CNN/cnn_layer1/W_conv2d:0
  param   1: (32,)              CNN/batch1/beta:0
  param   2: (32,)              CNN/batch1/gamma:0
  param   3: (32,)              CNN/batch1/moving_mean:0
  param   4: (32,)              CNN/batch1/moving_variance:0
  param   5: (5, 5, 32, 64)     CNN/cnn_layer2/W_conv2d:0
  param   6: (64,)              CNN/cnn_layer2/b_conv2d:0
  param   7: (64,)              CNN/batch2/beta:0
  param   8: (64,)              CNN/batch2/gamma:0
  param   9: (64,)              CNN/batch2/moving_mean:0
  param  10: (64,)              CNN/batch2/moving_variance:0
  param  11: (3136, 256)        CNN/relu1/W:0
  param  12: (256,)             CNN/relu1/b:0
  param  13: (256, 10)          CNN/output_layer/W:0
  param  14: (10,)              CNN/output_layer/b:0
```
TF12  TL1.3
```
  param   0: (5, 5, 1, 32)      CNN/cnn_layer1/W_conv2d:0
  param   1: (32,)              CNN/batch1/beta:0
  param   2: (32,)              CNN/batch1/gamma:0
  param   3: (32,)              CNN/batch1/moving_mean:0
  param   4: (32,)              CNN/batch1/moving_variance:0
  param   5: (32,)              CNN/batch1/CNN/batch1/moving_mean/biased:0
  param   6: ()                 CNN/batch1/CNN/batch1/moving_mean/local_step:0
  param   7: (32,)              CNN/batch1/CNN/batch1/moving_variance/biased:0
  param   8: ()                 CNN/batch1/CNN/batch1/moving_variance/local_step:0
  param   9: (5, 5, 32, 64)     CNN/cnn_layer2/W_conv2d:0
  param  10: (64,)              CNN/cnn_layer2/b_conv2d:0
  param  11: (64,)              CNN/batch2/beta:0
  param  12: (64,)              CNN/batch2/gamma:0
  param  13: (64,)              CNN/batch2/moving_mean:0
  param  14: (64,)              CNN/batch2/moving_variance:0
  param  15: (64,)              CNN/batch2/CNN/batch2/moving_mean/biased:0
  param  16: ()                 CNN/batch2/CNN/batch2/moving_mean/local_step:0
  param  17: (64,)              CNN/batch2/CNN/batch2/moving_variance/biased:0
  param  18: ()                 CNN/batch2/CNN/batch2/moving_variance/local_step:0
  param  19: (3136, 256)        CNN/relu1/W:0
  param  20: (256,)             CNN/relu1/b:0
  param  21: (256, 10)          CNN/output_layer/W:0
  param  22: (10,)              CNN/output_layer/b:0
```

Code
```
X_train, y_train, X_val, y_val, X_test, y_test = \
                tl.files.load_mnist_dataset(shape=(-1, 28, 28, 1))

X_train = np.asarray(X_train, dtype=np.float32)
y_train = np.asarray(y_train, dtype=np.int64)
X_val = np.asarray(X_val, dtype=np.float32)
y_val = np.asarray(y_val, dtype=np.int64)
X_test = np.asarray(X_test, dtype=np.float32)
y_test = np.asarray(y_test, dtype=np.int64)

print('X_train.shape', X_train.shape)
print('y_train.shape', y_train.shape)
print('X_val.shape', X_val.shape)
print('y_val.shape', y_val.shape)
print('X_test.shape', X_test.shape)
print('y_test.shape', y_test.shape)
print('X %s   y %s' % (X_test.dtype, y_test.dtype))

sess = tf.InteractiveSession()

# Define the batchsize at the begin, you can give the batchsize in x and y_
# rather than 'None', this can allow TensorFlow to apply some optimizations
# – especially for convolutional layers.
batch_size = 128

x = tf.placeholder(tf.float32, shape=[batch_size, 28, 28, 1])   # [batch_size, height, width, channels]
y_ = tf.placeholder(tf.int64, shape=[batch_size,])

def inference(x, is_train, reuse=None):
    with tf.variable_scope(""CNN"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)
        network = tl.layers.InputLayer(x, name='input_layer')
        network = tl.layers.Conv2d(network, n_filter=32, filter_size=(5, 5), strides=(1, 1),
                act=None, b_init=None, padding='SAME', name='cnn_layer1')
        network = tl.layers.BatchNormLayer(network, act=tf.nn.relu, is_train=True, name='batch1')

        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer1')
        network = tl.layers.Conv2d(network, n_filter=64, filter_size=(5, 5), strides=(1, 1),
                act=None, padding='SAME', name='cnn_layer2')
        network = tl.layers.BatchNormLayer(network, act=tf.nn.relu, is_train=True, name='batch2')

        network = tl.layers.MaxPool2d(network, filter_size=(2, 2), strides=(2, 2),
                padding='SAME', name='pool_layer2')
        ## end of conv
        network = tl.layers.FlattenLayer(network, name='flatten_layer')   # output: (?, 3136)
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop1') # output: (?, 3136)
        network = tl.layers.DenseLayer(network, n_units=256,
                                        act = tf.nn.relu, name='relu1')   # output: (?, 256)
        network = tl.layers.DropoutLayer(network, keep=0.5, name='drop2') # output: (?, 256)
        network = tl.layers.DenseLayer(network, n_units=10,
                                        act = tf.identity,
                                        name='output_layer')    # output: (?, 10)
    return network

network = inference(x, is_train=True, reuse=False)
y = network.outputs

ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
cost = ce

correct_prediction = tf.equal(tf.argmax(y, 1), y_)
acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# train
n_epoch = 200
learning_rate = 0.0001
print_freq = 1

train_params = network.all_params
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
    epsilon=1e-08, use_locking=False).minimize(cost, var_list=train_params)

tl.layers.initialize_global_variables(sess)
network.print_params(False)
network.print_layers()

print('   learning_rate: %f' % learning_rate)
print('   batch_size: %d' % batch_size)

for epoch in range(n_epoch):
    start_time = time.time()
    for X_train_a, y_train_a in tl.iterate.minibatches(
                                X_train, y_train, batch_size, shuffle=True):
        feed_dict = {x: X_train_a, y_: y_train_a}
        feed_dict.update( network.all_drop )        # enable noise layers
        sess.run(train_op, feed_dict=feed_dict)

    if epoch + 1 == 1 or (epoch + 1) % print_freq == 0:
        print(""Epoch %d of %d took %fs"" % (epoch + 1, n_epoch, time.time() - start_time))
        train_loss, train_acc, n_batch = 0, 0, 0
        for X_train_a, y_train_a in tl.iterate.minibatches(
                                X_train, y_train, batch_size, shuffle=True):
            dp_dict = tl.utils.dict_to_one( network.all_drop )    # disable noise layers
            feed_dict = {x: X_train_a, y_: y_train_a}
            feed_dict.update(dp_dict)
            err, ac = sess.run([cost, acc], feed_dict=feed_dict)
            train_loss += err; train_acc += ac; n_batch += 1
        print(""   train loss: %f"" % (train_loss/ n_batch))
        print(""   train acc: %f"" % (train_acc/ n_batch))
        val_loss, val_acc, n_batch = 0, 0, 0
        for X_val_a, y_val_a in tl.iterate.minibatches(
                                    X_val, y_val, batch_size, shuffle=True):
            dp_dict = tl.utils.dict_to_one( network.all_drop )    # disable noise layers
            feed_dict = {x: X_val_a, y_: y_val_a}
            feed_dict.update(dp_dict)
            err, ac = sess.run([cost, acc], feed_dict=feed_dict)
            val_loss += err; val_acc += ac; n_batch += 1
        print(""   val loss: %f"" % (val_loss/ n_batch))
        print(""   val acc: %f"" % (val_acc/ n_batch))

print('Evaluation')
test_loss, test_acc, n_batch = 0, 0, 0
for X_test_a, y_test_a in tl.iterate.minibatches(
                            X_test, y_test, batch_size, shuffle=True):
    dp_dict = tl.utils.dict_to_one( network.all_drop )    # disable noise layers
    feed_dict = {x: X_test_a, y_: y_test_a}
    feed_dict.update(dp_dict)
    err, ac = sess.run([cost, acc], feed_dict=feed_dict)
    test_loss += err; test_acc += ac; n_batch += 1
print(""   test loss: %f"" % (test_loss/n_batch))
print(""   test acc: %f"" % (test_acc/n_batch))
```",hi found interesting thing use idea thank advance param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param param code print print print print print print print sess define begin give rather allow apply especially convolutional height width inference reuse network network network network network network network network network network network network network end network network output network network output network network act output network network output network network act output return network network inference ce cost ce train cost sess false print print epoch range enable noise epoch epoch print epoch took epoch disable noise err cost err print train loss print train disable noise err cost err print loss print print disable noise err cost err print test loss print test,issue,negative,positive,neutral,neutral,positive,positive
270369611,"No it is not the same as a regular DeConv2DLayer, which is not using the indicies from a particular max pooling layer. Often in Sementation Networks the Unpooling layer is followed by convolutions, but it is not the same as a single DeConv2DLayer.
A more formal description is available in the torch documentation:
https://github.com/torch/nn/blob/master/doc/convolution.md#spatialmaxunpooling",regular particular layer often layer single formal description available torch documentation,issue,negative,positive,positive,positive,positive,positive
270323661,"I feel so sorry for not replying to you in time. This error has been solved by install the tensorflow 0.12.1.
Thank you for your great help!",feel sorry time error install thank great help,issue,positive,positive,positive,positive,positive,positive
270285246,"@JoelKronander from the top left of page 6 , the author said: `Deconvolution for upsampling` , so as my understand it should be `DeConv2d`, for professional version you can use  `DeConv2dLayer`",top left page author said understand professional version use,issue,negative,positive,positive,positive,positive,positive
270184324,"Sorry for being a bit vague. 
UpSammpling2dLayer or Deconv2D is not what I am looking for. 
I am looking for an ""Unpooling"" layer, explicitly linked to the pooling indicies in an earlier max-pooling layer, for example used in encoder-decoder type models. The ""unpooling"" upsamples the feature map by coping the input to the location resulting from the max pooling operation and fills in zeros for the other values in the upsampled feature map.
See e.g. the Segnet paper https://arxiv.org/abs/1511.00561 on top left of page 6 for an illustration. This ""Unpooling"" layer thus needs to access the stored max pooling indicies in a corresponding max_pooling layer.   ",sorry bit vague looking looking layer explicitly linked layer example used type feature map coping input location resulting operation feature map see paper top left page illustration layer thus need access corresponding layer,issue,negative,negative,negative,negative,negative,negative
269585055,"Please check https://www.tensorflow.org/versions/master/api_docs/python/nn/convolution#conv1d

The strides should be an integer, and the input shape should be 3D, i.e. [batch_size, n_width, n_channel]",please check integer input shape,issue,negative,neutral,neutral,neutral,neutral,neutral
269460423,"@sunbohit yes, now it works well with both python 3.4 and 3.5. well done.",yes work well python well done,issue,positive,neutral,neutral,neutral,neutral,neutral
269452354,"@sunbohit Thanks, but this modification cannot run under python 3.4 and python 2.7.
```
     return f.read().replace(*replace).split()
TypeError: Can't convert 'bytes' object to str implicitly
```
I suggest you to fix it in compatible way.",thanks modification run python python return replace ca convert object implicitly suggest fix compatible way,issue,negative,positive,positive,positive,positive,positive
269444298,"@zsdonghao the enviroment as follows:

> Windows10, python 3.5.2.3, tensorflow 0.12.0, tensorlayer 1.2.8

maybe the error caused by system? I haven't run it in Linux yet
@wagamamaz I train a toy 'en-fr' model which use 1k sentences for training data, it seems work well. ",python maybe error system run yet train toy model use training data work well,issue,negative,neutral,neutral,neutral,neutral,neutral
269342051,Is this a problem with tensorflow installation? I have seen a similar question in this:https://github.com/tensorflow/skflow/issues/98,problem installation seen similar question,issue,negative,neutral,neutral,neutral,neutral,neutral
269341992,"# IMPORTANT

@lucidfrontier45 @wagamamaz the latest version of TL has an args of `is_fix`, so you can do as follow:

```python
if is_train:
    network = DropoutLayer(network, 0.8, is_fix=True, name='xxx')
```",important latest version follow python network network,issue,negative,positive,positive,positive,positive,positive
269341260,"_>>> import tensorflow as tf_
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcublas.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcudnn.so.5 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcufft.so.8.0 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcuda.so.1 locally
I tensorflow/stream_executor/dso_loader.cc:125] successfully opened CUDA library libcurand.so.8.0 locally
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
W tensorflow/core/platform/cpu_feature_guard.cc:95] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
_>>> print(tf.nn.rnn_cell.BasicRNNCell)_
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'module' object has no attribute 'rnn_cell'
>>> ",import successfully library locally successfully library locally successfully library locally successfully library locally successfully library locally library use available machine could speed library use available machine could speed library use available machine could speed library use available machine could speed print recent call last file line module object attribute,issue,positive,positive,positive,positive,positive,positive
269339208,"@Bishopxu can you try this:
```python
import tensorflow as tf
print(tf.nn.rnn_cell.BasicRNNCell)
```
It is work?
",try python import print work,issue,negative,neutral,neutral,neutral,neutral,neutral
269314892,"@todtom after you fix the bugs, can it work well in English-French translation task?",fix work well translation task,issue,negative,neutral,neutral,neutral,neutral,neutral
269302989,"@todtom Hi, thank you for your contribution. I wonder which TF version are you using? As the code seem to work fine previously, we need to make sure other users can also run their scripts. Thanks~",hi thank contribution wonder version code seem work fine previously need make sure also run,issue,positive,positive,positive,positive,positive,positive
269301864,I run it for the Chinese-English translation task. When use 'basic_tokenizer' it often reports the error like 'TypeError: cannot use a bytes pattern on a string-like object'. It works for me after fix the bugs. I'm not sure whether the modified 'initital_vocabulary'  works for some other tokenizers,run translation task use often error like use pattern object work fix sure whether work,issue,negative,positive,positive,positive,positive,positive
268940215,"@zsdonghao That's right, I'll get it done later today.",right get done later today,issue,negative,positive,positive,positive,positive,positive
268937319,"@sczhengyabin thank you for your contribution. I think you are using TF12 right?
For compatibility with TF11, I think the following approach is better? 
If you agree, could you change the codes and make a new push request?

Thank you very much. 

```python
try:   # TF12
   rnn_variables = [tf.GraphKeys.GLOBAL_VARIABLES, RESNET_VARIABLES]
except: # TF11
   rnn_variables = [tf.GraphKeys.VARIABLES, RESNET_VARIABLES]
```",thank contribution think right compatibility think following approach better agree could change make new push request thank much python try except,issue,positive,positive,positive,positive,positive,positive
268746048,"@zsdonghao Thanks for your work!
Yeah, I've tried again, and it does work now!",thanks work yeah tried work,issue,positive,positive,positive,positive,positive,positive
268733144,"@tobymu Hi, I ran you code and fixed the bug in ConcatLayer, see [last commit](https://github.com/zsdonghao/tensorlayer/commit/400d717ca2e6e3ee4d9ba2e5e14754d859882310).

Please reinstall TL from git, and run your script again, it should work.

Thanks",hi ran code fixed bug see last commit please reinstall git run script work thanks,issue,positive,positive,positive,positive,positive,positive
268699580,"Thanks for your comment.
But I don't think the key point is the wrong use of function ConcatLayer.

If I do nothing after concat, just directly output, the programme runs well.
If I do any operation, such as add a denselayer or a droplayer, it will cause the Error `TypeError: cannot convert dictionary update sequence element #0 to a sequence`

I have re-edited the code as below:

```
import numpy as np
import tensorflow as tf
import tensorlayer as tl
from tensorlayer.layers import set_keep
import time


X_train, y_train, X_val, y_val, X_test, y_test = tl.files.load_mnist_dataset(shape=(-1,784))
X_train = np.asarray(X_train, dtype=np.float32)
y_train = np.asarray(y_train, dtype=np.int64)
X_val = np.asarray(X_val, dtype=np.float32)
y_val = np.asarray(y_val, dtype=np.int64)
X_test = np.asarray(X_test, dtype=np.float32)
y_test = np.asarray(y_test, dtype=np.int64)
    
sess = tf.InteractiveSession()

# placeholder
x = tf.placeholder(tf.float32, shape=[None, 784], name='x')
y_ = tf.placeholder(tf.int64, shape=[None, ], name='y_')

print(""Building Network"")
        
"""""" nn model """"""
net_1 = tl.layers.InputLayer(x, name='nn_input_layer')
net_1 = tl.layers.DropoutLayer(net_1, keep=0.8, name='nn_drop1')
net_1 = tl.layers.DenseLayer(net_1, n_units=800, act = tf.nn.relu, name='nn_relu1')
net_1 = tl.layers.DropoutLayer(net_1, keep=0.5, name='nn_drop2')
net_1 = tl.layers.DenseLayer(net_1, n_units=800, act = tf.nn.relu, name='nn_relu2')
#net_1 = tl.layers.DropoutLayer(net_1, keep=0.5, name='nn_drop3')

"""""" cnn model """"""
x_image = tf.reshape(x, shape=[-1,28,28,1])
net_2 = tl.layers.InputLayer(x_image, name='cnn_input_layer')
net_2 = tl.layers.Conv2dLayer(net_2,
                        act = tf.nn.relu,
                        shape = [5, 5, 1, 32],  # 32 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer1')     # output: (?, 28, 28, 32)
net_2 = tl.layers.PoolLayer(net_2,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='cnn_pool_layer1',)   # output: (?, 14, 14, 32)
net_2 = tl.layers.Conv2dLayer(net_2,
                        act = tf.nn.relu,
                        shape = [5, 5, 32, 64], # 64 features for each 5x5 patch
                        strides=[1, 1, 1, 1],
                        padding='SAME',
                        name ='cnn_layer2')     # output: (?, 14, 14, 64)
net_2 = tl.layers.PoolLayer(net_2,
                        ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1],
                        padding='SAME',
                        pool = tf.nn.max_pool,
                        name ='cnn_pool_layer2',)   # output: (?, 7, 7, 64)
net_2 = tl.layers.FlattenLayer(net_2, name='cnn_flatten_layer')   # output: (?, 3136)
net_2 = tl.layers.DropoutLayer(net_2, keep=0.5, name='cnn_drop1') # output: (?, 3136)
net_2 = tl.layers.DenseLayer(net_2, n_units=256, act = tf.nn.relu, name='cnn_relu1')   # output: (?, 256)
#net_2 = tl.layers.DropoutLayer(net_2, keep=0.5, name='drop2') # output: (?, 256)

"""""" mix model """"""         
network = tl.layers.ConcatLayer(layer = [net_1, net_2], name ='concat_layer')
network = tl.layers.DropoutLayer(network, keep=0.5, name='mix_drop1') # output: (?, 800+256)
network = tl.layers.DenseLayer(network, n_units=1000, act = tf.nn.relu, name='mix_relu1')   # output: (?, 1056)
network = tl.layers.DropoutLayer(network, keep=0.5, name='mix_drop2') # output: (?, 1056)
network = tl.layers.DenseLayer(network, n_units=10, act = tf.identity, name='output_layer')    # output: (?, 10)
    
y = network.outputs
y_op = tf.argmax(tf.nn.softmax(y), 1)
cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(y, y_))
params = network.all_params
    
# train
n_epoch = 100
batch_size = 128
learning_rate = 0.0001
print_freq = 5
train_op = tf.train.AdamOptimizer(learning_rate, beta1=0.9, beta2=0.999,
                                epsilon=1e-08, use_locking=False).minimize(cost)

sess.run(tf.initialize_all_variables()) # initialize all variables

network.print_params()
network.print_layers()

print('   learning_rate: %f' % learning_rate)
print('   batch_size: %d' % batch_size)
```

",thanks comment think key point wrong use function nothing directly output well operation add cause error convert dictionary update sequence element sequence code import import import import import time sess none none print building network model act act model act shape patch name output pool name output act shape patch name output pool name output output output act output output mix model network layer name network network output network network act output network network output network network act output cost train cost initialize print print,issue,negative,negative,neutral,neutral,negative,negative
268529280,a new push by @boscotsang may solve this problem.,new push may solve problem,issue,negative,positive,positive,positive,positive,positive
268500822,"The key function of [ConcatLayer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#concat-layer) is `tf.concat`, to concat the outputs of CNN and FC together, you will need to flatten the CNN output to `[batch_size, n_feature1]` then concat it to the FC output of `[batch_size, n_feature2]` with concat_dim =1 (default is 1).  Please have a look at the example in [RTD](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#concat-layer).

However, `TypeError: cannot convert dictionary update sequence element #0 to a sequence` doesn't seem to be a TensorFlow error. Your code format is broken, cannot be read.",key function together need flatten output output default please look example however convert dictionary update sequence element sequence seem error code format broken read,issue,negative,negative,negative,negative,negative,negative
268163565,"yeah, my train_dir is  a relative file path `wmt`, isn't it true? i'll try to set `is_npz=True`, should i remove the generated data and train the translate model again?",yeah relative file path true try set remove data train translate model,issue,positive,positive,positive,positive,positive,positive
268149631,"It seem that you have problem with ckpt. 
Did you set a correct`train_dir`?
Alternatively, can you set `is_npz = True` then it will save the model into npz file instead of ckpt, it may help.",seem problem set correct alternatively set true save model file instead may help,issue,positive,positive,positive,positive,positive,positive
267713334,"y_op = tf.argmax(tf.nn.softmax(y), 1) do nothing in this tutorial, we put it here just for telling users how to get integer output from softmax output.",nothing tutorial put telling get integer output output,issue,negative,neutral,neutral,neutral,neutral,neutral
267601669,"What data are you going to read? How did you create these data?
I think this is the key part: 
```
W c:\tf_jenkins\home\workspace\release-win\device\gpu\os\windows\tensorflow\core\framework\op_kernel.cc:975] Invalid argument: Input to DecodeRaw has length 154587 that is not a multiple of 4, the size of float
[[Node: DecodeRaw = DecodeRawlittle_endian=true, out_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]]
```",data going read create data think key part invalid argument input length multiple size float node,issue,negative,neutral,neutral,neutral,neutral,neutral
267601213,I moved data folder into example folder.,data folder example folder,issue,negative,neutral,neutral,neutral,neutral,neutral
267518475,Can you shift it to site-packages/tensorlayer/data instead of site-packages/data? At the moment if I try to import from my own data.py file the data package installed by tensorlayer is used instead.  ,shift instead moment try import file data package used instead,issue,negative,neutral,neutral,neutral,neutral,neutral
267474175,"`data/imagenet_classes.py` contains imagenet label list which will be used in VGG, Inception examples.",label list used inception,issue,negative,neutral,neutral,neutral,neutral,neutral
266694297,">it is unusual to apply act just after RNNs.

It's good to know that I am doing something wrong.

Thank you. ",unusual apply act good know something wrong thank,issue,negative,positive,positive,positive,positive,positive
266692973,"we don't have plan to include `act` to `RNNLayer`, it is unusual to apply `act` just after RNNs.",plan include act unusual apply act,issue,negative,positive,positive,positive,positive,positive
266683528,"@zsdonghao 

Thank you. 

I think `act` should be included in `RNNLayer` so that the API is consistent to other Layers like Dense or Convolution.
 
Will `RNNLayer` be modified to have `act` in the future? 
",thank think act included consistent like dense convolution act future,issue,positive,positive,positive,positive,positive,positive
266278479,"Thank for your advice, I think we should leave it there, it helps beginner to know `var_list`.",thank advice think leave beginner know,issue,negative,neutral,neutral,neutral,neutral,neutral
266265575,I have resolved the error and make a pull request then. The init_ops.one_initializer() function is rewritten on tensorflow 0.12. Maybe 'tf.one_initializer' should be replaced by 'tf.one_initializer()' in tensorlayer for tensorflow 0.12. But i did not check it for tensorflow 0.11 backend,resolved error make pull request function maybe check,issue,negative,neutral,neutral,neutral,neutral,neutral
266241264,"@michuanhaohao Can you remove the line of `dtype=dtype` and try again? If it works, please make a push request. Thx.",remove line try work please make push request,issue,negative,neutral,neutral,neutral,neutral,neutral
265735918,"@zsdonghao 

```py
if is_train:
    network = DropoutLayer(network, 0.8, name='xxx')
```
This looks fine to me.  Thank you. ",network network fine thank,issue,positive,positive,positive,positive,positive,positive
265480035,"FYI, the lastest version of `DropoutLayer` have a `is_fix` setting, you can fix the keeping probability by setting it to `True`.",version setting fix keeping probability setting true,issue,negative,positive,positive,positive,positive,positive
264879388,@qjc937044867 I suggest you to find help on [QQ group](https://github.com/zsdonghao/tensorlayer/blob/master/img/img_qq.png) or [waffle](https://waffle.io/zsdonghao/tensorlayer).,suggest find help group waffle,issue,negative,neutral,neutral,neutral,neutral,neutral
264878558,"@zsdonghao  sorry,i haven't found the right way to use the pre-trained embedings in those file. And when i try to use RNNLayer in PTB way,i got another problem, my code as follow:

    class OPR_LSTM():
    def __init__(self, d_hidden, nodes, embs, active_function='sigmoid', loss='mean_squared_error',
                 training_epochs=100, display_step=10, batch=10, regular='L2', decay=0.8, optimizer='sgd', keep_prob = 0.9):
        self.d_hidden, self.nodes = d_hidden, nodes
        self.embs, self.active_function, self.keep_prob = embs, active_function, keep_prob
        self.training_epochs,self.batch,self.display_step = training_epochs,batch,display_step
        self.regular,self.lr_decay, self.lr = regular, decay, 1.0
        self.max_epoch = 14
        self.max_max_epoch = 55
        self.num_steps = 30
        #self.sess = tf.InteractiveSession()
        self.x, self.y = self._define_train_inputs()

    def _projection(self, *inputs):
        #'''
        xa, xp, x1, x2, x3, xd, xm = inputs
        xa = tl.utils.pad_sequences(xa, maxlen=5,truncating='post')
        xp = tl.utils.pad_sequences(xp, maxlen=5,truncating='post')
        x1 = tl.utils.pad_sequences(x1, maxlen=5,truncating='post')
        x2 = tl.utils.pad_sequences(x2, maxlen=5,truncating='post')
        x3 = tl.utils.pad_sequences(x3, maxlen=10,truncating='post')
        xt = np.hstack([xa, xp, x1, x2, x3])
        #return [x,np.array(xd).astype(dtype='float32'),np.array(xm).astype(dtype='float32')]
        return xt

    def _define_train_inputs(self):
        # Define placeholder
        '''
        x1 = tf.placeholder(tf.int32, shape=[None, 5], name='x1')
        x2 = tf.placeholder(tf.int32, shape=[None, 5], name='x2')
        x3 = tf.placeholder(tf.float32, shape=[None, 2], name='x3')
        '''
        x = tf.placeholder(tf.int32, shape=[None, self.num_steps], name='x')
        y = tf.placeholder(tf.float32, shape=[None, 1], name='y')
        return x,y

    def _define_layers(self,is_training=False,reuse=None):
        print '\n  building model:'
        with tf.variable_scope(""model"", reuse=reuse):
            tl.layers.set_name_reuse(reuse)
            network = tl.layers.EmbeddingInputlayer(
                inputs=self.x,
                vocabulary_size=self.embs.n_words,
                embedding_size=self.embs.d_vector,
                E_init=tf.constant_initializer(self.embs.embs),
                name='embedding_layer',
                istrain=False)
            network = tl.layers.RNNLayer(network,
                cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
                cell_init_args={'forget_bias': 0.0, 'state_is_tuple': True},
                n_hidden=self.embs.d_vector,
                initializer=tf.random_uniform_initializer(-0.001, 0.001),
                n_steps=self.num_steps,
                return_last=False,
                return_seq_2d=True,
                name='basic_lstm')
            lstm = network
            network = tl.layers.DenseLayer(network, n_units=self.nodes[-1], act=tf.sigmoid, name='output')
        return network,lstm

    def fit(self,x_data,y_data,print_model=False):
        sess = tf.InteractiveSession()
        network, lstm = self._define_layers()
        x,y = self.x,self.y
        with tf.variable_scope('learning_rate'):
            lr = tf.Variable(0.0, trainable=False)
        pred = network.outputs
        cost = tf.reduce_mean(tf.pow(pred - y, 2)/self.batch)
        # define optimizer
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),5)
        optimizer = tf.train.GradientDescentOptimizer(self.lr)
        train_op = optimizer.apply_gradients(zip(grads, tvars))
        # initialize_all_variables
        sess.run(tf.initialize_all_variables())
        # print model information
        if print_model:
            network.print_params()
            network.print_layers()
        # model training
        for i in range(self.max_max_epoch):
            # decreases the initial learning rate after several
            # epoachs (defined by ``max_epoch``), by multipling a ``lr_decay``.
            new_lr_decay = self.lr_decay ** max(i - self.max_epoch, 0.0)
            sess.run(tf.assign(lr, self.lr * new_lr_decay))

            # Training
            print(""Epoch: %d/%d Learning rate: %.3f"" % (i + 1, self.max_max_epoch, sess.run(lr)))
            epoch_size = ((len(x_data) // self.batch) - 1) // self.num_steps
            start_time = time.time()
            costs,iters = 0.0, 0

            # reset all states at the begining of every epoch
            state = tl.layers.initialize_rnn_state(lstm.initial_state) # ERROR

            for step, (x_, y_) in enumerate(tl.iterate.minibatches(x_data, self.batch, self.num_steps)):
                feed_dict = {x: x_, y: y_,
                             lstm.initial_state: state,
                             }
                # For training, enable dropout
                feed_dict.update(network.all_drop)
                _cost, state, _ = sess.run([cost,lstm.final_state,train_op],feed_dict=feed_dict)
                costs += _cost
                iters += self.num_steps

                if step % (epoch_size // 10) == 10:
                    print(""%.3f perplexity: %.3f speed: %.0f wps"" %
                          (step * 1.0 / epoch_size, np.exp(costs / iters),
                           iters * self.batch / (time.time() - start_time)))
            train_perplexity = np.exp(costs / iters)
            print(""Epoch: %d/%d Train Perplexity: %.3f"" % (i + 1, self.max_max_epoch,
                                                           train_perplexity))

Error report as :
Traceback (most recent call last):
  File ""/home/qjc/文档/zero_anaphora/main/overt_pronoun_resolution1.py"", line 54, in <module>
    md.train(n_epoch=40,evaluation=True,save=True)
  File ""/home/qjc/文档/zero_anaphora/main/libs/models.py"", line 188, in train
    self.clf.fit(x,y)
  File ""/home/qjc/文档/zero_anaphora/main/lstm_units.py"", line 120, in fit
    state = tl.layers.initialize_rnn_state(lstm.initial_state)
  File ""/home/qjc/文档/zero_anaphora/main/tensorln/layers.py"", line 59, in initialize_rnn_state
    c = state.c.eval(session=session)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 575, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3633, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: <exception str() failed>

i know i should try to solve this problem by myself,but i have to finish it in two days, so please help me,thank you very much!",sorry found right way use file try use way got another problem code follow class self batch regular decay self return return self define none none none none none return self print building model model reuse network network network true network network network return network fit self sess network cost define cost zip print model information model training range initial learning rate several defined training print epoch learning rate reset every epoch state error step enumerate state training enable dropout state cost step print perplexity speed step print epoch train perplexity error report recent call last file line module file line train file line fit state file line file line return self session file line return file line run file line file line file line raise type message exception know try solve problem finish two day please help thank much,issue,negative,positive,positive,positive,positive,positive
264864018,"you can find this example in [here](https://github.com/zsdonghao/tensorlayer/tree/master/example) as well.

[reuse embedding matrix](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_generate_text.py)
[ptb](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py) ",find example well reuse matrix,issue,negative,neutral,neutral,neutral,neutral,neutral
264863429,"ok, i will try . And i have find another problem in EmbeddingInputlayer, how can i use the embedings which has been pre-trained with wiki. Now i use like this, where self.embs.embs is the embedings_weights

network = tl.layers.EmbeddingInputlayer(
            inputs=self.x,
            vocabulary_size=self.embs.n_words,
            embedding_size=self.embs.d_vector,
            E_init=tf.constant_initializer(self.embs.embs),
            name='embedding_layer',
            istrain=False)

but i got the problem as follow,and i don't know why:
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
run_metadata_ptr)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
feed_dict_string, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
target_list, options, run_metadata)
File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: <exception str() failed>

thank you very much!",try find another problem use use like network got problem follow know file line run file line file line file line raise type message exception thank much,issue,negative,positive,positive,positive,positive,positive
264859221,"As you are doing cross validation, actually you have different models, you can use the following script to creates different inference for different fold.

```python
def inference(x, reuse=False, name='fold1'):
       with tf.variable_scope(""model"", reuse=reuse):
           tl.layers.set_name_reuse(reuse)
           network = tl.layers.InputLayer(inputs=x,name=name+'input_layer')
           network = tl.layers.DropoutLayer(network,keep=0.8,name=name+'drop1')
           network = tl.layers.DenseLayer(network,n_units=32,act=tf.nn.tanh,name=name+'relu1')
           ....
      return network
```
TensorLayer forces user to use different name for every layers so as to prevent bugs.
",cross validation actually different use following script different inference different fold python inference model reuse network network network network network return network user use different name every prevent,issue,negative,neutral,neutral,neutral,neutral,neutral
264855867,"@qjc937044867 you cannot use `utils` functions, when you use RNN. please see the PTB example.",use use please see example,issue,negative,neutral,neutral,neutral,neutral,neutral
264851682,"@zsdonghao  because i had packaged all network and fit/preditc into a class, so i can use them just like 'sklearn' way. As follow:

    class OPR_LSTM():
        def __init__(self, d_hidden, nodes, embs, active_function='relu', loss='mean_squared_error', lr=.0001, training_epochs=100, display_step=10, batch=10, regular='L2', optimizer='sgd', keep_prob = 0.9):
        self.d_hidden, self.nodes = d_hidden, nodes
        self.embs, self.active_function, self.lr, self.keep_prob = embs, active_function, lr, keep_prob
        self.training_epochs,self.batch,self.display_step = training_epochs,batch,display_step
        self.regular = regular
        self.sess = tf.InteractiveSession()
        self.x, self.y = self._define_train_inputs()
        self.network = self._define_layers()

    def _projection(self, *inputs):
        #'''
        xa, xp, x1, x2, x3, xd, xm = inputs
        xa = tl.utils.pad_sequences(xa, maxlen=5,truncating='post')
        xp = tl.utils.pad_sequences(xp, maxlen=5,truncating='post')
        x1 = tl.utils.pad_sequences(x1, maxlen=5,truncating='post')
        x2 = tl.utils.pad_sequences(x2, maxlen=5,truncating='post')
        x3 = tl.utils.pad_sequences(x3, maxlen=10,truncating='post')
        xt = np.hstack([xa, xp, x1, x2, x3])
        return xt

    def _define_train_inputs(self):
        # Define placeholder
        x = tf.placeholder(tf.int32, shape=[self.batch, 30], name='x')
        y = tf.placeholder(tf.float32, shape=[self.batch, 1], name='y')
        return x,y

    def _define_layers(self,is_training=False):
        print '\n  building model:'
        network = tl.layers.EmbeddingInputlayer(
            inputs=self.x,
            vocabulary_size=self.embs.n_words,
            embedding_size=self.embs.d_vector,
            E_init=self.embs.embs,
            name='embedding_layer',
            istrain=False)
        '''
        network = tl.layers.DynamicRNNLayer(network,
            cell_fn = tf.nn.rnn_cell.BasicLSTMCell,
            n_hidden = self.embs.d_vector,
            dropout = 0.8,
            sequence_length = tl.layers.retrieve_seq_length_op2(self.x),
            return_last=True,
            return_seq_2d = False,  # stack denselayer or compute cost after it
            name = 'dynamic_rnn')
        network = tl.layers.DenseLayer(network, n_units=self.nodes[-1], act=tf.sigmoid, name='output')
        '''
        return network

    def fit(self,x_data,y_data,print_model=False):
        y_data = np.array(y_data)
        network = self.network
        x,y = self.x,self.y

        pred = network.outputs
        cost = tf.reduce_mean(tf.pow(pred - y, 2)/self.batch)
        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        # define optimizer
        #train_params = network.all_params
        #train_op = tf.train.AdamOptimizer(learning_rate=self.lr, epsilon=1e-08,use_locking=False).minimize(cost, var_list=train_params)
        tvars = tf.trainable_variables()
        grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),5)
        optimizer = tf.train.GradientDescentOptimizer(self.lr)
        train_op = optimizer.apply_gradients(zip(grads, tvars))
        # initialize_all_variables
        self.sess.run(tf.initialize_all_variables())
        # print model information
        if print_model:
            network.print_params()
            network.print_layers()
        # model training
        tl.utils.fit(self.sess, network, train_op, cost, x_data, y_data, x, y, acc=acc,
                            batch_size=self.batch, n_epoch=self.training_epochs, print_freq=self.display_step)
        #tl.utils.test(self.sess, network, acc=acc,cost=cost, X_test=x_data, y_test=y_data, x=x, y_=y, batch_size=self.batch)

    def predict(self,x_test):
        # Predict surviving chances (class 1 results)
        y_op = self.network.outputs
        return tl.utils.predict(self.sess, self.network, x_test, self.x, y_op)

    def save_model(self,name):
        # save model as '.npz' file
        tl.files.save_npz(self.network.all_params, name=name)

    def load_model(self,name):
        # load model by name
        tl.files.load_npz(name=name)



And i have find another problem in EmbeddingInputlayer, how can i use the embedings which has been pre-trained with wiki.   Now i use like this, where self.embs.embs is the embedings_weights

    network = tl.layers.EmbeddingInputlayer(
            inputs=self.x,
            vocabulary_size=self.embs.n_words,
            embedding_size=self.embs.d_vector,
            E_init=self.embs.embs,
            name='embedding_layer',
            istrain=False)

but i got the problem as follow,and i don't know why:
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 766, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 964, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1014, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1034, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.FailedPreconditionError: <exception str() failed>


Thanks for you attention, and looking forward to receiving your soonest reply.",network class use like way follow class self batch regular self return self define return self print building model network network network dropout false stack compute cost name network network return network fit self network cost define cost cost zip print model information model training network cost network predict self predict surviving class return self name save model file self name load model name find another problem use use like network got problem follow know file line run file line file line file line raise type message exception thanks attention looking forward reply,issue,positive,positive,neutral,neutral,positive,positive
264833788,@HzFu you can do exactly like the script I send you.,exactly like script send,issue,negative,positive,positive,positive,positive,positive
264831787,"I means how to input  two different data. 
For example, for a RGB-D image classification network, I want to input RGB image and its depth map to the separate layers.",input two different data example image classification network want input image depth map separate,issue,negative,neutral,neutral,neutral,neutral,neutral
264829583,"you may want to use `ConcatLayer` for multi-input, check this.
```python
>>> x = tf.placeholder(tf.float32, shape=[None, 784])
>>> inputs = tl.layers.InputLayer(x, name='input_layer')
>>> x2 = tf.placeholder(tf.float32, shape=[None, 784])
>>> inputs2 = tl.layers.InputLayer(x, name='input_layer2')
>>> net1 = tl.layers.DenseLayer(inputs, n_units=800, act = tf.nn.relu, name='relu1_1')
>>> net2 = tl.layers.DenseLayer(inputs2, n_units=300, act = tf.nn.relu, name='relu2_1')
>>> network = tl.layers.ConcatLayer(layer = [net1, net2], name ='concat_layer')
```

for multi-output, just simply reuse a layer.
```python
>>> net1 = tl.layers.DenseLayer(inputs, n_units=800, act = tf.nn.relu, name='relu1_1')
>>> net_out1 = tl.layers.DenseLayer(net1, n_units=800, name='out1')
>>> net_out2 = tl.layers.DenseLayer(net1, n_units=800, name='out2')
```",may want use check python none none net act net act network layer net net name simply reuse layer python net act net net,issue,negative,neutral,neutral,neutral,neutral,neutral
264757574,"hello，i hava the same problem with you ,but i use the RNNLayer:

def _define_layers(self,is_training=False):
        print '\n  building model:'
        network = tl.layers.EmbeddingInputlayer(
            inputs=self.x,
            vocabulary_size=self.embs.n_words,
            embedding_size=self.embs.d_vector,
            E_init=self.embs.embs,
            name='embedding_layer',
            istrain=False)
        network = tl.layers.RNNLayer(network,
                                     cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
                                     cell_init_args={'forget_bias': 0.0, 'state_is_tuple': True},
                                     n_hidden=self.embs.d_vector,
                                     initializer=tf.random_uniform_initializer(-0.001, 0.001),
                                     n_steps=5,
                                     return_last=True,
                                     name='basic_lstm')
        #define NN network
        for i in range(len(self.nodes)-1):
            network = tl.layers.DenseLayer(network, n_units=self.nodes[i], act=tf.nn.relu,
                                               name='hidden' + str(i))
        network = tl.layers.DenseLayer(network, n_units=self.nodes[-1], act=tf.identity, name='output')
        return network
    def fit(self,x_data,y_data,print_model=True):
        network = self.network
        x,y = self.x,self.y
        pred = network.outputs
        cost = tf.reduce_mean(tf.pow(pred - y, 2))
        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))
        acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
        # define optimizer
        train_params = network.all_params
        train_op = tf.train.AdamOptimizer(learning_rate=self.lr, epsilon=1e-08,use_locking=False).minimize(cost, var_list=train_params)
        # initialize_all_variables
        self.sess.run(tf.initialize_all_variables())
        # model training
        tl.utils.fit(self.sess, network, train_op, cost, x_data, y_data, x, y, acc=acc,
                            #batch_size=self.batch, n_epoch=self.training_epochs, print_freq=self.display_step)
        #tl.utils.test(self.sess, network, acc=acc,cost=cost, X_test=x_data, y_test=y_data, x=x, y_=y, batch_size=self.batch)


my input shape also have padding with zero just like:
 x = [[5, 4, 8, 9, 2]
      [0, 0, 0, 2, 4]
      [0, 0, 3, 5, 7] 
      [0, 0, 2, 3, 3]]
y = [[1],[0],[0],[1]]
but when i try to run this session,always get the problem as follow:

File ""main/lstm_units.py"", line 90, in fit
    tl.utils.test(self.sess, network, acc=acc,cost=cost, X_test=x_data, y_test=y_data, x=x, y_=y, batch_size=self.batch)
  File ""main/tensorln/utils.py"", line 149, in test
    err, ac = sess.run([cost, acc], feed_dict=feed_dict)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 710, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 908, in _run
    feed_dict_string, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 958, in _do_run
    target_list, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 978, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.InvalidArgumentError: <exception str() failed>

so i want to know where the problem is? Thank you very much",problem use self print building model network network network true define network range network network network network return network fit self network cost define cost model training network cost network input shape also padding zero like try run session always get problem follow file line fit network file line test err cost file line run file line file line file line raise type message exception want know problem thank much,issue,positive,positive,positive,positive,positive,positive
262864350,"Thank you.
I see.
""y = network.outputs""  translate tensorlayer instance into tensorflow instance,
and
""network = tl.layers.InputLayer(x, name='input_layer')""  ranslate tensorflow instance into tensorlayer instance.",thank see translate instance instance network instance instance,issue,negative,neutral,neutral,neutral,neutral,neutral
262779104,"@luoruisichuan `network` is the object of a TensorLayer's layer, `network.outputs` is the tensor output of TensorFlow.

Again, you may find the following links will help:
[Understanding basic layer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html)
[Understanding dense layer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#understand-dense-layer)
[How to learn TensorLayer](http://tensorlayer.readthedocs.io/en/latest/user/more.html#how-to-effectively-learn-tensorlayer)
",network object layer tensor output may find following link help understanding basic layer understanding dense layer learn,issue,negative,neutral,neutral,neutral,neutral,neutral
262778452,"@lucidfrontier45 Hi, your suggestion is good.

Now, `BatchNormLayer` have `is_train`, but `DropoutLayer` doesn't. However, if a model contails `BatchNormLayer`, to build inferences for training and testing, we need to use the way in [PTB example](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm.py). In that case, we can use
```python
if is_train:
    network = DropoutLayer(network, 0.8, name='xxx')
```
instead of put the `is_train` inside the `DropoutLayer`, or we can also enable/disable dropout layer by setting `feed_dict` see [mnist cnn](https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_mnist.py).

Please let me know, if you have any suggestion.",hi suggestion good however model build training testing need use way example case use python network network instead put inside also dropout layer setting see please let know suggestion,issue,positive,positive,positive,positive,positive,positive
262447867,"@wagamamaz 

Before talking about my code or @zsdonghao 's I want to make clear how to use batch normalization.
```python
class tensorlayer.layers.BatchNormLayer(
        layer = None,
        decay = 0.999,
        epsilon = 0.00001,
        act = tf.identity,
        is_train = None,
        beta_init = tf.zeros_initializer,
        gamma_init = tf.ones_initializer,
        name ='batchnorm_layer')
```

`BatchNormLayer` accepts `is_train` as arg for constructor. It's compile time but not run time.
I couldn't find any example of batch normalization except in DCGAN example. It makes two net, one with is `is_train=True` passed and `is_train=False`  the other. Is this the intended usage of `BatchNormLayer` ?

If so, I think it's confusing that `DropoutLayer` and `BatchNormLayer` has different API for switching training/test phase and should make it unify.

One way is my `Dropout` implementation that accepts `is_train` argument.
Is @zsdonghao 's code for switching training/test phase?

",talking code want make clear use batch normalization python class layer none decay epsilon act none name constructor compile time run time could find example batch normalization except example two net one intended usage think different switching phase make unify one way dropout implementation argument code switching phase,issue,negative,positive,neutral,neutral,positive,positive
262363767,"@lucidfrontier45 Is @zsdonghao 's code work for you? if yes, you can make a push request.",code work yes make push request,issue,negative,neutral,neutral,neutral,neutral,neutral
262221252,"[NEW] FYI, the lastest version of `DropoutLayer` have a `is_fix` setting, you can fix the keeping probability by setting it to `True`.

---------

Previous answer:

This may be better?
```python
class Dropout(Layer):
    def __init__(self,
                layer = None,
                keep = 0.5,
                is_fix = False,
                name = 'dropout_layer'):
        
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        print(""  tensorlayer:Instantiate Dropout %s: keep: %f"" % (self.name, keep))

        if is_fix:
            self.outputs = tf.nn.dropout(self.inputs, keep, name=name) 
        else:
           set_keep[name] = tf.placeholder(tf.float32)
           self.outputs = tf.nn.dropout(self.inputs, set_keep[name], name=name)
            
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        if not is_fix:
              self.all_drop.update( {set_keep[name]: keep} )
        self.all_layers.extend( [self.outputs] )
```",new version setting fix keeping probability setting true previous answer may better python class dropout layer self layer none keep false name self print dropout keep keep keep else name name list list name keep,issue,positive,positive,neutral,neutral,positive,positive
262134892,"I think one way to unify the API is add a new `Dropout` layer that receives `is_train` argument.
See by test implementation below.

```python
class Dropout(Layer):
    def __init__(self,
                layer = None,
                keep = 0.5,
                is_train = True,
                name = 'dropout_layer'):
        
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs
        print(""  tensorlayer:Instantiate Dropout %s: keep: %f"" % (self.name, keep))

        set_keep[name] = tf.constant(keep, dtype=tf.float32)
        if is_train:
            self.outputs = tf.nn.dropout(self.inputs, set_keep[name], name=name) # 1.2
        else:
            self.outputs = self.inputs
            
        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_drop.update( {set_keep[name]: keep} )
        self.all_layers.extend( [self.outputs] )
```",think one way unify add new dropout layer argument see test implementation python class dropout layer self layer none keep true name self print dropout keep keep name keep name else list list name keep,issue,negative,positive,positive,positive,positive,positive
261912927,"Em... I find another way to fix this problem: 
adding 
`import matplotlib`
`matplotlib.use('Agg')`
before 
`import tensorlayer as tl`
in our own script instead of tensorlayer's visualize.py.

Maybe this workaround can be shown as a hint in the document. :D ",em find another way fix problem import import script instead maybe shown hint document,issue,negative,neutral,neutral,neutral,neutral,neutral
261909168,"@wagamamaz  Thank you for your replay! Sorry I haven't read the document carefully. I just found that forcing backend to Agg will make figures not display on screen, so it is better not to do this. ",thank replay sorry read document carefully found forcing make display screen better,issue,positive,negative,neutral,neutral,negative,negative
261903622,"Did it work well, under other OS like mac?

FQA have this answer, http://tensorlayer.readthedocs.io/en/latest/user/more.html#visualization

But I wonder whether we should force it to Agg or not.",work well o like mac answer wonder whether force,issue,positive,neutral,neutral,neutral,neutral,neutral
260623632,"Did you reuse the layer ?  I don't have this problem.

In the source code, PReluLayer uses `name_scope`, it should be fine and similar with `with tf.variable_scope(name) as vs:`

```
with tf.name_scope(name) as scope:
     alphas = tf.get_variable(name='alphas', shape=w_shape, initializer=a_init, **a_init_args )
     self.outputs = tf.nn.relu(self.inputs) + tf.mul(alphas, (self.inputs - tf.abs(self.inputs))) * 0.5
```

To reuse an inference, I usually do as follow:

```
def inference(input_images, is_train=True, reuse=False):
    w_init = tf.random_normal_initializer(stddev=0.02)
    gamma_init=tf.random_normal_initializer(1., 0.02)

    with tf.variable_scope(""cnn"", reuse=reuse):
        tl.layers.set_name_reuse(reuse)

        net_in = InputLayer(input_images, name='input/images')
        net_h0 = Conv2d(net_in, df_dim, (5, 5), (2, 2), act=lambda x: tl.act.lrelu(x, 0.2),
                padding='SAME', W_init=w_init, name='hidden0/conv2d') 
        ...

network_train = inference(x, is_train=True, reuse=False)
network_test = inference(x, is_train=False, reuse=True)
```

If you still have this problem, can you change the soruce code from `with tf.name_scope(name) as scope:` to `with tf.variable_scope(name) as vs:`?
",reuse layer problem source code fine similar name name scope reuse inference usually follow inference reuse inference inference still problem change code name scope name,issue,negative,positive,neutral,neutral,positive,positive
260310692,"The `network.outputs` is the output tensor of the current layer.

The `tf.nn.lrn` is the shortcut of `tf.nn.local_response_normalization`, which means apply local response normalization to the output.

We provide `LocalResponseNormLayer`, but as local response normalization does not have any weights and arguments, you can also apply `tf.nn.lrn` on `network.outputs`.

You may find the following links will help:
[Understanding basic layer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html)
[Understanding dense layer](http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#understand-dense-layer)
[How to learn TensorLayer](http://tensorlayer.readthedocs.io/en/latest/user/more.html#how-to-effectively-learn-tensorlayer)
",output tensor current layer apply local response normalization output provide local response normalization also apply may find following link help understanding basic layer understanding dense layer learn,issue,negative,neutral,neutral,neutral,neutral,neutral
259961702,"@narrator-wong Gitter may be the better place to discuss algorithm, we normally report bugs on Issues.
",may better place discus algorithm normally report,issue,negative,positive,positive,positive,positive,positive
259955479,"@zsdonghao Thanks...
i saw this, but i mean.. for example, set max_grad_norm = 10 or 20 or 50 ...? what's the difference?
i set 'max_grad_norm' according to my num_step? 
if the num_step of my LSTM is 100, and my max_grad_norm also 100?
",thanks saw mean example set difference set according also,issue,negative,negative,neutral,neutral,negative,negative
259937613,"@narrator-wong you can find the example in PTB tutorials.

```
    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),
                                      max_grad_norm)
    optimizer = tf.train.GradientDescentOptimizer(lr)
    train_op = optimizer.apply_gradients(zip(grads, tvars))
```
",find example cost zip,issue,negative,neutral,neutral,neutral,neutral,neutral
259771317,"@narrator-wong You don't need to stack DynamicRNNLayer like that, you can use `n_layer` to set the number of RNN layers you want, and use `dropout` to set the input and output keeping probabilities of all RNN layers.

the dropout is implemented by `tf.nn.rnn_cell.DropoutWrapper`, stacking layers is implemented by `tf.nn.rnn_cell.MultiRNNCell`.
",need stack like use set number want use dropout set input output keeping dropout,issue,negative,neutral,neutral,neutral,neutral,neutral
259768714,"@zsdonghao how to write stack DynamicRNN? 
1. DynamicRNNLayer : n_layer = 2 ?
2.  

```
            network = tl.layers.EmbeddingInputlayer(
                        inputs = inputs,
                        vocabulary_size = vocab_size,
                        embedding_size = hidden_size,
                        E_init = tf.random_uniform_initializer(-init_scale, init_scale),
                        name ='embedding_layer')

            if is_training:
                network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop1')

            network = tl.layers.DynamicRNNLayer(network,
                        cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
                        cell_init_args={'forget_bias': 0.0, 'state_is_tuple': True},
                        n_hidden=hidden_size,
                        initializer=tf.random_uniform_initializer(-init_scale, init_scale),
                        sequence_length = tl.layers.retrieve_seq_length_op2(inputs),
                        return_last=False,
                        name='dynamic_lstm_layer1')
            lstm1 = network

            if is_training:
                network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop2')

            network = tl.layers.DynamicRNNLayer(network,
                        cell_fn=tf.nn.rnn_cell.BasicLSTMCell,
                        cell_init_args={'forget_bias': 0.0, 'state_is_tuple': True},
                        n_hidden=hidden_size,
                        initializer=tf.random_uniform_initializer(-init_scale, init_scale),
                        sequence_length = tl.layers.retrieve_seq_length_op2(inputs),
                        return_last=True,
                        name='dynamic_lstm_layer2')
            lstm2 = network

            if is_training:
                network = tl.layers.DropoutLayer(network, keep=keep_prob, name='drop3')

            network = tl.layers.DenseLayer(network,
                        n_units=2,
                        W_init=tf.random_uniform_initializer(-init_scale, init_scale),
                        b_init=tf.random_uniform_initializer(-init_scale, init_scale),
                        act = tf.identity,
                        name='output_layer')
```
",write stack network name network network network network true network network network network network true network network network network network act,issue,positive,positive,positive,positive,positive,positive
259694724,"@narrator-wong Hi, I have an Image Captioning example for TensorLayer, hope it help https://github.com/zsdonghao/Image-Captioning 
",hi image example hope help,issue,positive,neutral,neutral,neutral,neutral,neutral
259607473,"@wagamamaz `max_length = tf.shape(outputs)[1]` is not my work, i just find it, and report it to @zsdonghao , i saw the new code is right, appreciate to your speeeeeed! Thanks, and i wanna 
write an example for DynamicRNN by using `tl.layers.DynamicRNNLayer`, Thanks for your answer
",work find report saw new code right appreciate thanks wan na write example thanks answer,issue,positive,positive,positive,positive,positive,positive
259411000,"@narrator-wong Does `max_length = tf.shape(outputs)[1]` work for you? 

The `shape=[batch_size, None]` means you have `batch_size` of sentences, and the `max_length` can be any number.

If your number of step is fixed, `RNNLayer` is easier than `DynamicRNNLayer`, so in your case,  you can define your placeholder as `shape=[4, 5]`.

For dynamic rnn, In your case, as your batch_size is 4, you can define your placeholder as follow.

```
input_seqs = tf.placeholder(dtype=tf.int64, shape=[4, None], name=""input_seqs"")
network = tl.layers.EmbeddingInputlayer(
             inputs = input_seqs,
             vocabulary_size = vocab_size,
             embedding_size = embedding_size,
             name = 'seq_embedding')
 network = tl.layers.DynamicRNNLayer(network,
             cell_fn = tf.nn.rnn_cell.BasicLSTMCell,
             n_hidden = embedding_size,
             dropout = 0.7,
             sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),
             return_seq_2d = True,     # stack denselayer or compute cost after it
             name = 'dynamic_rnn',)
 network = tl.layers.DenseLayer(network, n_units=vocab_size,
             act=tf.identity, name=""output"")
```

`tl.layers.retrieve_seq_length_op2(input_seqs)` computes the sequence lengths of every sentence after zero padding, in your case, they are `[5, 2, 4, 3]`. (you are correct)

But in your case, it seem that, every sentences have a single label (you don't have a  target sequence), I think you should use the last output for classification ? i.e. `return_last=True`. This [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) may help you better understand either to use the last output or all the outputs.

If you use all the outputs (you have an input sequence and target sequence), you will need a mask to define the cost function, here is an example from Google [im2txt](https://github.com/tensorflow/models/blob/master/im2txt/im2txt/show_and_tell_model.py).

```
def batch_with_dynamic_pad(images_and_captions,
                           batch_size,
                           queue_capacity,
                           add_summaries=True):
  """"""Batches input images and captions, returns the images, input sequence and
  output sequence.

  This function splits the caption into an input sequence and a target sequence,
  where the target sequence is the input sequence right-shifted by 1. Input and
  target sequences are batched and padded up to the maximum length of sequences
  in the batch. A mask is created to distinguish real words from padding words.

  Example 1
  -----------

    Actual captions in the batch ('-' denotes padded character):
    |      [
    |        [ 1 2 5 4 5 ],
    |        [ 1 2 3 4 - ],
    |        [ 1 2 3 - - ],
    |      ]
    |
    |    input_seqs:
    |      [
    |        [ 1 2 3 4 ],
    |        [ 1 2 3 - ],
    |        [ 1 2 - - ],
    |      ]
    |
    |    target_seqs:
    |      [
    |        [ 2 3 4 5 ],
    |        [ 2 3 4 - ],
    |        [ 2 3 - - ],
    |      ]
    |
    |    mask:
    |      [
    |        [ 1 1 1 1 ],
    |        [ 1 1 1 0 ],
    |        [ 1 1 0 0 ],
    |      ]

  Example 2
  -----------
  - input_seqs - <S> a figurine with a plastic witches head is standing in front of a computer keyboard . a
  - target_seqs - a figurine with a plastic witches head is standing in front of a computer keyboard . </S> a
  - input_mask - [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]

  Parameters
  -----------
    images_and_captions : A list of pairs [image, caption], where image is a
      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of
      any length. Each pair will be processed and added to the queue in a
      separate thread.
    batch_size : Batch size.
    queue_capacity : Queue capacity.
    add_summaries : If true, add caption length summaries.

  Returns
  --------
    images : A Tensor of shape [batch_size, height, width, channels].
    input_seqs : An int32 Tensor of shape [batch_size, padded_length].
    target_seqs : An int32 Tensor of shape [batch_size, padded_length].
    mask : An int32 0/1 Tensor of shape [batch_size, padded_length].
  """"""
  enqueue_list = []
  for image, caption in images_and_captions:
    caption_length = tf.shape(caption)[0]
    input_length = tf.expand_dims(tf.sub(caption_length, 1), 0)

    input_seq = tf.slice(caption, [0], input_length)
    target_seq = tf.slice(caption, [1], input_length)
    indicator = tf.ones(input_length, dtype=tf.int32)
    enqueue_list.append([image, input_seq, target_seq, indicator])

  images, input_seqs, target_seqs, mask = tf.train.batch_join(
      enqueue_list,
      batch_size=batch_size,
      capacity=queue_capacity,
      dynamic_pad=True,
      name=""batch_and_pad"")

  if add_summaries:
    lengths = tf.add(tf.reduce_sum(mask, 1), 1)
    tf.scalar_summary(""caption_length/batch_min"", tf.reduce_min(lengths))
    tf.scalar_summary(""caption_length/batch_max"", tf.reduce_max(lengths))
    tf.scalar_summary(""caption_length/batch_mean"", tf.reduce_mean(lengths))

  return images, input_seqs, target_seqs, mask

queue_capacity = (2 * num_preprocess_threads * batch_size)
images, input_seqs, target_seqs, input_mask = (
              batch_with_dynamic_pad(images_and_captions,
                                               batch_size=batch_size,
                                               queue_capacity=queue_capacity))

batch_loss, losses, weights, _ = tl.cost.cross_entropy_seq_with_mask(logits, target_seqs, input_mask, return_details=True)
```

If you feel the code from Google is difficult, you can define the mask by yourself, and use `tl.cost.cross_entropy_seq_with_mask()` to compute the cost.

More details about [dynamic_rnn ops](https://github.com/dennybritz/tf-rnn/blob/master/dynamic_rnn.ipynb),  [rnn vs dynamic_rnn](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)

Feel free to let us know when you have problem.
",work none number number step fixed easier case define dynamic case define follow none network name network network dropout true stack compute cost name network network output sequence every sentence zero padding case correct case seem every single label target sequence think use last output classification may help better understand either use last output use input sequence target sequence need mask define cost function example input input sequence output sequence function caption input sequence target sequence target sequence input sequence input target maximum length batch mask distinguish real padding example actual batch character mask example figurine plastic head standing front computer keyboard figurine plastic head standing front computer keyboard list image caption image tensor shape height width caption tensor length pair added queue separate thread batch size queue capacity true add caption length tensor shape height width tensor shape tensor shape mask tensor shape image caption caption caption caption indicator image indicator mask mask return mask feel code difficult define mask use compute cost feel free let u know problem,issue,positive,positive,positive,positive,positive,positive
259245584,"@narrator-wong Thank you for reporting bug. I just make a new commit.
The outputs should be reshaped by the max length as follow.

```
max_length = tf.shape(self.outputs)[1]
self.outputs = tf.reshape(tf.concat(1, outputs), [-1, max_length, n_hidden])
```

The idea is the same with `advanced_indexing_op()`.
",thank bug make new commit length follow idea,issue,positive,positive,positive,positive,positive,positive
259241290,"@zsdonghao Great implementation！but, ""tensorlayer(1.2.5)/layers.py"", 
class DynamicRNNLayer(Layer),
line 2236:  ""self.outputs = tf.reshape(tf.concat(1, outputs), [-1, **n_steps**, n_hidden])""
**n_steps** is not defined, actually, in DynamicRNNLayer, just have n_steps(max), no **n_steps**, 
i think maybe something wrong???
",great class layer line defined actually think maybe something wrong,issue,negative,positive,positive,positive,positive,positive
258429189,"Hi, @EncodeTS .
You may need to get the list of variables you want to update, TensorLayer provides two ways to get the variables list.

The first way is to use the `all_params` of a network, by default, it will store the variables in order.
You can print the variables information via
`tl.layers.print_all_variables(train_only=True)` or `network.print_params(details=False)`
To choose which variables to update, you can do as below.

```
train_params = network.all_params[3:]
```

The second way is to get the variables by a given name. For example, if you want to get all variables which the layer name contail `dense`, you can do as below.

```
train_params = tl.layers.get_variables_with_name('dense', train_only=True, printable=True)
```

After you get the variable list, you can define your optimizer like that so as to update only a part of the variables.

```
train_op = tf.train.AdamOptimizer(0.001).minimize(cost, var_list= train_params)
```
",hi may need get list want update two way get list first way use network default store order print information via choose update second way get given name example want get layer name dense get variable list define like update part cost,issue,negative,positive,positive,positive,positive,positive
258118637,"@GuangmingZhu As TensorLayer is fully transparent to TensorFlow, you can just simply define your layers under `with tf.device('/cpu:0'):`.

This tutorial can help you understand how to put variables on cpu. `tutorial_cifar10_tfrecord.py`
https://github.com/zsdonghao/tensorlayer/blob/master/tutorial_cifar10_tfrecord.py
",fully transparent simply define tutorial help understand put,issue,negative,neutral,neutral,neutral,neutral,neutral
258055856,"@zsdonghao Thank you! How can I put variables on cpu when I define layers using tensorlayer APIs, since tensorlayer APIs define the variables within the APIs?
",thank put define since define within,issue,negative,neutral,neutral,neutral,neutral,neutral
257997167,"@GuangmingZhu  this is a script to run multi-gpu hope it can help you.

```
    num_gpus = 2
    opt = tf.train.AdamOptimizer(5e-5, beta1=0.9, beta2=0.999, epsilon=1e-08, use_locking=False)
    tower_grads = []
    for i in range(num_gpus):
        with tf.device('/gpu:%d' % i):
            with tf.name_scope('%s_%d' % (""gpu"", i)) as scope:
                tl.layers.set_name_reuse(True)
                ## inference
                network, outputs = u_net_2d(x, batch_size)
                ## cost
                dice_loss = 1 - tl.cost.dice_coe(outputs[:,:,:,0], y_[:,:,:,0], epsilon=1e-10)
                ## Reuse variables for the next tower.
                tf.get_variable_scope().reuse_variables()
                ## compute grads for every tower
                grads = opt.compute_gradients(dice_loss, var_list=network.all_params)
                tower_grads.append(grads)
    grads = average_gradients(tower_grads)
    train_op = opt.apply_gradients(grads)
```

the `average_gradients` is borrowed from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py
",script run hope help opt range scope true inference network cost reuse next tower compute every tower,issue,positive,positive,positive,positive,positive,positive
257633363,"Hi, TensorLayer naturally supported all features of TensorFlow.
To use multi-gpu, just simply follow this tutorial https://www.tensorflow.org/versions/r0.11/how_tos/using_gpu/index.html
i.e. create graph with TL under ""multi-tower"" `with tf.device(xx):`
",hi naturally use simply follow tutorial create graph,issue,negative,positive,neutral,neutral,positive,positive
257145862,"@lijiaman  your `W_int` ann `b_int` are wrong, this is not the way to use TensorLayer.

The `network.all_params` is the list of all parameters in network.

you can find an example to load a list of numpy array into tensorlayer as follow:  
1. http://tensorlayer.readthedocs.io/en/latest/_modules/tensorlayer/files.html#load_npz
2. `main_restore_embedding_layer()` 
   https://github.com/zsdonghao/tensorlayer/blob/master/tutorial_generate_text.py
",ann wrong way use list network find example load list array follow,issue,negative,negative,negative,negative,negative,negative
257085851,"It is obvious.

```
'Conv2dLayer' object has no attribute 'get_shape'
```

you can get the shape of input and output by `network.inputs.get_shape()` or `network.outputs.get_shape()`
",obvious object attribute get shape input output,issue,negative,neutral,neutral,neutral,neutral,neutral
257065390,"@zsdonghao There's a need for `arguments: optional dictionary of keyword arguments to be passed to the function.`

What do you recommend? Is this how it would work?

```
import tensorflow as tf
import tensorlayer as tl

Layer = tl.layers.LambdaLayer(x, function_name, arguments={'key': value})

class LambdaLayer(Layer):
    """"""
    """"""
    def __init__(
        self,
        layer = None,
        fn = None,
        arguments = None,
        name = 'lambda_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs

        print(""  tensorlayer:Instantiate LambdaLayer  %s"" % self.name)
        with tf.variable_scope(name) as vs:
            self.outputs = fn(self.inputs, **arguments)

        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
```

My Wide ResNet Repo in TensorLayer: [https://github.com/ritchieng/wideresnet-tensorlayer](https://github.com/ritchieng/wideresnet-tensorlayer). There are 2 scripts. One in Keras, one in TensorLayer. The Keras version is working and verified.

I'm facing this error too. Although I would think that I would be able to call `get_shape()`. I'm unable to decipher why my Conv2dLayer does not have that attribute. I can't seem to figure out why because the error message does indicate it's a Conv2dLayer. And it should be able to return a shape being a Tensor? 

```
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0)
  tensorlayer:Instantiate InputLayer  input_layer: (64, 32, 32, 3)
  tensorlayer:Instantiate Conv2dLayer cnn_layer_first: [3, 3, 3, 16], [1, 1, 1, 1], SAME, relu
Traceback (most recent call last):
  File ""cifar_wide_resnet_tl.py"", line 258, in <module>
    a.step()
  File ""cifar_wide_resnet_tl.py"", line 179, in step
    x = residual_block(x, count, nb_filters=nb_filters, subsample_factor=1)
  File ""cifar_wide_resnet_tl.py"", line 80, in residual_block
    prev_nb_channels = x.get_shape()[3]
AttributeError: 'Conv2dLayer' object has no attribute 'get_shape'
```

Thanks :) 
",need optional dictionary recommend would work import import layer value class layer self layer none none none name self print name list list wide one one version working facing error although would think would able call unable decipher attribute ca seem figure error message indicate able return shape tensor device device name bus id recent call last file line module file line step count file line object attribute thanks,issue,negative,positive,neutral,neutral,positive,positive
257032787,"@zsdonghao Thanks, you're really helpful! 

This is a [working Keras version](https://gist.github.com/ritchieng/019fc573a3024b42a76432ceb94e63f5) I modified and tested for Wide ResNet. Wide ResNet is a CNN that outperforms Deep ResNets with fewer layers (a lot fewer). 

There's no implementation in TensorLayer yet. I'm trying to map it over to TensorLayer to show the flexibility of TensorLayer in building more complex systems. Writing a [guide](http://www.ritchieng.com/) and will be attributing your help and TensorLayer on this particular part!

Thanks for supporting again.
",thanks really helpful working version tested wide wide deep lot implementation yet trying map show flexibility building complex writing guide help particular part thanks supporting,issue,positive,positive,neutral,neutral,positive,positive
257010203,"Hi, we didn't have lambda layer, but I just created one which is able to use the provided function.

Let me know if it helps.

```
import tensorflow as tf
import tensorlayer as tl

Layer = tl.layers.Layer

class LambdaLayer(Layer):
    """"""
    """"""
    def __init__(
        self,
        layer = None,
        fn = None,
        name = 'lambda_layer',
    ):
        Layer.__init__(self, name=name)
        self.inputs = layer.outputs

        print(""  tensorlayer:Instantiate LambdaLayer  %s"" % self.name)
        with tf.variable_scope(name) as vs:
            self.outputs = fn(self.inputs)

        self.all_layers = list(layer.all_layers)
        self.all_params = list(layer.all_params)
        self.all_drop = dict(layer.all_drop)
        self.all_layers.extend( [self.outputs] )
```
",hi lambda layer one able use provided function let know import import layer class layer self layer none none name self print name list list,issue,negative,positive,positive,positive,positive,positive
256916349,"Hi, you should use `pip install -e .`.
In term of loading model, I think you can find the answer from here : http://tensorlayer.readthedocs.io/en/latest/user/more.html#fqa
",hi use pip install term loading model think find answer,issue,negative,neutral,neutral,neutral,neutral,neutral
252206300,"the default visualization of ReconLayer is for MNIST dataset, if you want to modify the pretrain function, please have a look at the following tutorial on our website:

http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#modifying-pre-train-behaviour

However, if you change the training data shape it will **NOT** crash, as **pretrain** function uses **try** as follow.

```
if save:
      try:
            visualize.W(self.train_params[0].eval(), second=10, saveable=True, shape=[28,28], name=save_name+str(epoch+1), fig_idx=2012)
            files.save_npz([self.all_params[0]] , name=save_name+str(epoch+1)+'.npz')
      except:
            raise Exception(""You should change visualize.W(), if you want to save the feature images for different dataset"")
```

We primarily use github issues to track bugs and installation issues. This is a question better suited for StackOverflow. Please ask it there and tag it with the `tensorlayer` tag. Thanks!
",default visualization want modify pretrain function please look following tutorial however change training data shape crash pretrain function try follow save try except raise exception change want save feature different primarily use track installation question better please ask tag tag thanks,issue,positive,positive,positive,positive,positive,positive
251920352,"Ah, I see, the shape should be [1,1].
It's only a test program, and I was trying to make things simple.
",ah see shape test program trying make simple,issue,negative,neutral,neutral,neutral,neutral,neutral
251915694,"Hi, your shape is incorrect, and why you use DensLayer when X only have one feature?
Check you model pls.
",hi shape incorrect use one feature check model,issue,negative,neutral,neutral,neutral,neutral,neutral
251914365,"@wagamamaz 
the network code is:

'''define placeholders'''
x = tf.placeholder(tf.float32, shape=[None, 1], name='x')
y_= tf.placeholder(tf.float32, shape=[None, 1], name='y_')

'''define the network'''
network = tl.layers.InputLayer(x, name='input_layer')
network = tl.layers.DenseLayer(network, n_units=16, act = tf.nn.relu, name='relu1')
network = tl.layers.DenseLayer(network, n_units=4, act = tf.nn.relu, name='relu2')
network = tl.layers.DenseLayer(network, n_units=1, act = tf.nn.relu, name='result')

and after training, 

> > > network.all_params[0].eval()
> > > array([[-0.12620708, -0.13378434,  0.05406925, -0.06829847,  0.12863488,
> > >         -0.0354087 , -0.01825967,  0.06813091, -0.05230081, -0.09361359,
> > >         -0.08705429,  0.08989979, -0.17979668,  0.01737362,  0.11124372,
> > >          0.04816928]], dtype=float32)
> > > 
> > > tl.visualize.W(network.all_params[0].eval(), second=10, saveable=True, shape=[10, 10], name='test')
> > > Traceback (most recent call last):
> > >   File ""<pyshell#4>"", line 1, in <module>
> > >     tl.visualize.W(network.all_params[0].eval(), second=10, saveable=True, shape=[10, 10], name='test')
> > >   File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorlayer/visualize.py"", line 59, in W
> > >     plt.imshow(np.reshape(feature ,(shape[0],shape[1])),
> > >   File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/numpy/core/fromnumeric.py"", line 225, in reshape
> > >     return reshape(newshape, order=order)
> > > ValueError: total size of new array must be unchanged
",network code none none network network network network act network network act network network act training array recent call last file line module file line feature shape shape file line reshape return reshape total size new array must unchanged,issue,negative,positive,neutral,neutral,positive,positive
251375898,"@haigu12 can you show your code, when you got this error ?
How is your weight matrix look like ?
",show code got error weight matrix look like,issue,negative,neutral,neutral,neutral,neutral,neutral
250427694,"@llan-ml  Yes, indeed. `Layer` cannot track the parameters outside TensorLayer, I suggest you to use`train_params = layer.all_params + [you parameters]` if you want to update the parameters outside TensorLayer .

For `layers.all_drop`, if you control the keep probabilities by placeholder, then you will need to feed the values into `feed_dict`, but in this case, to keep everything simple, I suggest you to create different graph for training and evaluating :
e.g. https://github.com/zsdonghao/tensorlayer/blob/master/tutorial_ptb_lstm_state_is_tuple.py
",yes indeed layer track outside suggest use want update outside control keep need feed case keep everything simple suggest create different graph training,issue,positive,neutral,neutral,neutral,neutral,neutral
250426347,"@zsdonghao thx, the solution is the same as mine.
But this solution causes that newly created layers cannot track components,which are located before `tl.layers.InputLayer`, using methods such as `layer.all_params` and `layer.all_drop`.
",solution mine solution newly track,issue,positive,positive,positive,positive,positive,positive
250411763,"Hi, you can feed your `tf.Tensor` into `tl.layers.InputLayer` and then feed the layer into Dense and Conv layer.

If you look into the source code, the InputLayer just creat a `Layer.outputs`  , more details can be found on the documentation : http://tensorlayer.readthedocs.io/en/latest/modules/layers.html#understand-layer 
",hi feed feed layer dense layer look source code creat found documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
247983003,"@zsdonghao Thanks, the bug has been fixed, and the code runs well.  ^^
",thanks bug fixed code well,issue,positive,positive,positive,positive,positive,positive
247971265,"Hi, it is a bug of ReconLayer, can you update TensorLayer and try again?
",hi bug update try,issue,negative,neutral,neutral,neutral,neutral,neutral
237401681,"you need to cd into the docs folder, and then use ""make html"" to build the documentation.
",need folder use make build documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
