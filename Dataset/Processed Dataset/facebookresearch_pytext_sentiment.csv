id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
1293851349,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Meta Open Source project. Thanks!,thank contributor license agreement accept code meta open source project thanks,issue,positive,positive,neutral,neutral,positive,positive
1258825883,"Dependabot tried to update this pull request, but something went wrong. We're looking into it, but in the meantime you can retry the update by commenting `@dependabot rebase`.",tried update pull request something went wrong looking retry update rebase,issue,negative,negative,negative,negative,negative,negative
1252935757,This pull request was **exported** from Phabricator. Differential Revision: [D39639475](https://www.internalfb.com/diff/D39639475),pull request differential revision,issue,negative,neutral,neutral,neutral,neutral,neutral
1238783312,"@hudeven has imported this pull request. If you are a Meta employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D39295908).",pull request meta employee view,issue,negative,neutral,neutral,neutral,neutral,neutral
1192020388,apparently this is a bug left in the source. installing from pypi does not have this problem. ,apparently bug left source problem,issue,negative,positive,neutral,neutral,positive,positive
1191990984,"Hi, I have the same questions both 1 & 2. Could you explain why you it's meaningless?",hi could explain meaningless,issue,negative,negative,negative,negative,negative,negative
1179623708,"I get the same error following the README in this repo by running

pytext export < demo/configs/docnn.json",get error following running export,issue,negative,neutral,neutral,neutral,neutral,neutral
1135721574,"Hi @michael-marlen! 

Thank you for your pull request. 

We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.

You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%231554). Thanks!",hi thank pull request require sign contributor license agreement need attention currently record system longer valid need process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,neutral,neutral,positive,positive
1049481308,I guess our gpu is not supported from cuda or you don't have the right cuda version installed for torch.,guess right version torch,issue,negative,positive,positive,positive,positive,positive
1041333948,"Hi @neo315! 

Thank you for your pull request. 

We **require** contributors to sign our **Contributor License Agreement**, and yours needs attention.

You currently have a record in our system, but the CLA is no longer valid, and will need to be **resubmitted**.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%23901). Thanks!",hi neo thank pull request require sign contributor license agreement need attention currently record system longer valid need process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,neutral,neutral,positive,positive
970206574,"> same issue here. Torch version: 1.5.1 TorchText version: 0.6.0 Install pytext from source

same here, did you figure it out?",issue torch version version install source figure,issue,negative,neutral,neutral,neutral,neutral,neutral
936945907,"@seemethere has imported this pull request. If you are a Facebook employee, you can view this diff [on Phabricator](https://www.internalfb.com/diff/D31445282).",pull request employee view,issue,negative,neutral,neutral,neutral,neutral,neutral
936945668,@seemethere has updated the pull request. You must [reimport the pull request](https://www.internalfb.com/intern/opensource/github/pull_request/confirm/import/1028287884626311/) before landing.,pull request must reimport pull request landing,issue,negative,neutral,neutral,neutral,neutral,neutral
903605209,"same issue here. 
Torch version: 1.5.1
TorchText version: 0.6.0
Install pytext from source",issue torch version version install source,issue,negative,neutral,neutral,neutral,neutral,neutral
887153892,"@sy-LeeAP  Did you find a fix for this issue?

I am having similar errors while importing pytext when installed from source: https://github.com/facebookresearch/pytext/issues/1706",find fix issue similar source,issue,negative,neutral,neutral,neutral,neutral,neutral
887149864,"@hudeven I came across a commit in June 2021 where you committed changes adding this import statement: https://github.com/facebookresearch/pytext/commit/7ec49a8aa26dbc174436da07ab78f1a591d4c2f0 
```
from pytorch.text.fb.utils import PATH_MANAGER as PathManager  # noqa
```

Could you help with this issue? 

Is torchtext's installation from source supported for non-facebook (external) users? It will be great to update the README.md if that is no longer the case.

Thanks!",came across commit june import statement import could help issue installation source external great update longer case thanks,issue,positive,positive,positive,positive,positive,positive
876337159,I have the same problem when running Pytext in a new Google CoLab environment.,problem running new environment,issue,negative,positive,positive,positive,positive,positive
818702388,Thank you for signing our Contributor License Agreement. We can now accept your code for this (and any) Facebook open source project. Thanks!,thank contributor license agreement accept code open source project thanks,issue,positive,positive,neutral,neutral,positive,positive
818691367,"Hi @DatBhim! 

Thank you for your pull request and welcome to our community. 

# Action Required

In order to merge **any pull request** (code, docs, etc.), we **require** contributors to sign our **Contributor License Agreement**, and we don't seem to have one on file for you.

# Process

In order for us to review and merge your suggested changes, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

Once the CLA is signed, our tooling will perform checks and validations. Afterwards, the **pull request will be tagged** with `CLA signed`. The tagging process may take up to 1 hour after signing. Please give it that time before contacting us about it.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%231673). Thanks!",hi thank pull request welcome community action order merge pull request code require sign contributor license agreement seem one file process order u review merge please sign behalf someone else employer individual may sufficient employer may need sign corporate tooling perform afterwards pull request tagged process may take hour please give time u received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
804459611,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
757068686,"Hi @donkeyfire! 

Thank you for your pull request. We require contributors to sign our Contributor License Agreement, and yours needs attention.

You currently have a record in our system, but we do not have a signature on file. 

In order for us to review and merge your code, please sign at <https://code.facebook.com/cla>. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need to sign the corporate CLA.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%23334). Thanks!",hi thank pull request require sign contributor license agreement need attention currently record system signature file order u review merge code please sign behalf someone else employer individual may sufficient employer may need sign corporate received error please contact u thanks,issue,positive,positive,neutral,neutral,positive,positive
712542871,This is already upgraded. We need to create diff internally and export here.,already need create internally export,issue,negative,neutral,neutral,neutral,neutral,neutral
684840590,"Also, how would we like this feature to be? Passed as a new system argument say `--sentences`, `--all` etc, and provide a List of batches or a file consisting of batches in json?",also would like feature new system argument say provide list file,issue,negative,positive,positive,positive,positive,positive
684837991,@Titousensei I would like to work on this. Is there anyone else working on this? If no then please help me with the code Infrastructure so that I can focus on correct files to make changes.,would like work anyone else working please help code infrastructure focus correct make,issue,positive,neutral,neutral,neutral,neutral,neutral
661950006,"@salkola Thanks for pointing that question out. I have the same segmentation fault. However, the upgrading onnx and torch solution doesn't work on my anaconda environment. The latest version of torch is `1.5.1` but the requirement for pytext is `1.5.0`. I was using the same command as you mentioned in the related issue but `pip install pytext-nlp` would remove everything that is not compatible with the requirement. 

Some important configuration are shown as below:
```
Package                            Version
---------------------------------- -------------------
torch                              1.5.0
onnx                               1.7.0
click                              7.1.2
fairseq                            0.9.0
future                             0.18.2
fvcore                             0.1.1.post20200608
hypothesis                         3.88.3
joblib                             0.15.1
numpy                              1.18.5
python-dateutil                    2.8.0
pandas                             1.0.4
pytorch-dp                         0.1b1
pytorch-pretrained-bert            0.6.2
regex                              2019.11.1
requests                           2.23.0
scipy                              1.4.1
sentencepiece                      0.1.92
tensorboard                        1.14.0
torchtext                          0.6.0
```

",thanks pointing question segmentation fault however torch solution work anaconda environment latest version torch requirement command related issue pip install would remove everything compatible requirement important configuration shown package version torch click future post hypothesis,issue,positive,positive,positive,positive,positive,positive
617307506,Closing this issue as stale. Please feel free to reopen if the problem persists even with the config change suggested above.,issue stale please feel free reopen problem even change,issue,negative,negative,neutral,neutral,negative,negative
617306234,"Closing this as we haven't heard back, and the link to the config file is broken. Feel free to reopen if the problem persists.",back link file broken feel free reopen problem,issue,negative,neutral,neutral,neutral,neutral,neutral
613936939,"I'm working on something similar. Simpletransformers make it simpler to use transformers, but its design is too simple, hard to make any change. I want to use pytext to do what it does.",working something similar make simpler use design simple hard make change want use,issue,negative,negative,neutral,neutral,negative,negative
613214483,[@hudeven](https://github.com/hudeven) has updated the pull request. [Re-import the pull request](https://our.intern.facebook.com/intern/opensource/github/pull_request/confirm/import/2592949144284604/),pull request pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
579768538,"> @Titousensei, I had resolved three above by adding the following in `add_include` method in `builtin_task.py`
> 
> ```
>     import sys
>     sys.path.append('')
> ```

@Titousensei  Can I give a merge request for this? Or is there a better way to do this? 
I am getting the same error, which is resolved by adding current dir to sys path.",resolved three following method import give merge request better way getting error resolved current path,issue,negative,positive,positive,positive,positive,positive
579176378,"@pietern 1.4.0 does not have it. How could pytext release include this unstable feature ???  And even make any suggestion ? 
![image](https://user-images.githubusercontent.com/1797038/73255014-6b892180-41fa-11ea-8f86-ddfc91245537.png)
",could release include unstable feature even make suggestion image,issue,negative,neutral,neutral,neutral,neutral,neutral
579150959,"It's in the nightly releases and perhaps also in 1.4 (not sure).

See https://github.com/pytorch/pytorch/commit/0282c5ae6996584b908946cb694b02443dfd3c9a for the commit.",nightly perhaps also sure see commit,issue,positive,positive,positive,positive,positive,positive
579138833,"so which pytorch version should I use ? @pietern @chenyangyu1988 
I cannot find this method in pytorch's master branch also.",version use find method master branch also,issue,negative,neutral,neutral,neutral,neutral,neutral
579103538,"It's available on unstable. If PyText supports 1.3.1, it should gracefully degrade if it can't find the function.",available unstable gracefully degrade ca find function,issue,negative,positive,positive,positive,positive,positive
578390647,"@chenyangyu1988 you actually import '_round_robin_process_groups' from torch, but indeed it dosenot exist.

```
(pytext-nlp) luning@luning-mate:~/dev/tools/pytext$ pytext train < demo/configs/distributed_docnn.json 
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
No config file specified, reading from stdin
WARNING - Applying old config adapter for version=0. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=1. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=2. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=3. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=4. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=5. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=6. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=7. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=8. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=9. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=10. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=11. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=12. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=13. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=14. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=15. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=16. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=17. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=18. Please consider migrating your old configs to the latest version.

===Starting training...

=== Starting training, World size is 2
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])

Parameters: PyTextConfig:
    auto_resume_from_snapshot: False
    debug_path: /tmp/model.debug
    distributed_world_size: 2
    export_caffe2_path: None
    export_onnx_path: /tmp/model.onnx
    export_torchscript_path: None
    gpu_streams_for_distributed_training: 1
    include_dirs: None
    load_snapshot_path: 
    modules_save_dir: 
    random_seed: None
    report_eval_results: False
    save_all_checkpoints: False
    save_module_checkpoints: False
    save_snapshot_path: /tmp/model.pt
    task: DocumentClassificationTask.Config:
        data: Data.Config:
            batcher: PoolingBatcher.Config:
                eval_batch_size: 16
                num_shuffled_pools: 1
                pool_num_batches: 10000
                test_batch_size: 16
                train_batch_size: 16
            in_memory: True
            sort_key: None
            source: TSVDataSource.Config:
                column_mapping: {}
                delimiter: 	
                drop_incomplete_rows: False
                eval_filename: base_dir/test_tiny.tsv
                field_names: ['text', 'doc_label']
                quoted: False
                test_filename: base_dir/test_tiny.tsv
                train_filename: base_dir/train_tiny.tsv
        metric_reporter: ClassificationMetricReporter.Config:
            additional_column_names: []
            model_select_metric: ComparableClassificationMetric.ACCURACY
            output_path: /tmp/test_out.txt
            pep_format: False
            recall_at_precision_thresholds: [0.2, 0.4, 0.6, 0.8, 0.9]
            target_label: None
            text_column_names: ['text']
        model: DocModel.Config:
            decoder: MLPDecoder.Config:
                activation: Activation.RELU
                dropout: 0.0
                freeze: False
                hidden_dims: []
                layer_norm: False
                load_path: None
                out_dim: None
                save_path: None
                shared_module_key: None
            embedding: WordEmbedding.Config:
                cpu_only: False
                delimiter:  
                embed_dim: 100
                embedding_init_range: None
                embedding_init_strategy: EmbedInitStrategy.RANDOM
                export_input_names: ['tokens_vals']
                freeze: False
                load_path: None
                lowercase_tokens: True
                min_freq: 1
                mlp_layer_dims: []
                padding_idx: None
                pretrained_embeddings_path: 
                save_path: None
                shared_module_key: None
                skip_header: True
                vocab_file: 
                vocab_from_all_data: False
                vocab_from_pretrained_embeddings: False
                vocab_from_train_data: True
                vocab_size: 0
            inputs: ModelInput:
                dense: None
                labels: LabelTensorizer.Config:
                    allow_unknown: False
                    column: doc_label
                    is_input: False
                    label_vocab: None
                    pad_in_vocab: False
                tokens: TokenTensorizer.Config:
                    add_bos_token: False
                    add_eos_token: False
                    column: text
                    is_input: True
                    max_seq_len: None
                    tokenizer: Tokenizer.Config:
                        lowercase: True
                        split_regex: \s+
                    use_eos_token_for_bos: False
                    vocab: VocabConfig:
                        build_from_data: True
                        size_from_data: 0
                        vocab_files: []
                    vocab_file_delimiter:  
            output_layer: ClassificationOutputLayer.Config:
                freeze: False
                label_weights: None
                load_path: None
                loss: CrossEntropyLoss.Config:
                save_path: None
                shared_module_key: None
            representation: BiLSTMDocAttention.Config:
                dropout: 0.4
                freeze: False
                load_path: None
                lstm: BiLSTM.Config:
                    bidirectional: True
                    dropout: 0.4
                    freeze: False
                    load_path: None
                    lstm_dim: 32
                    num_layers: 1
                    pack_sequence: True
                    save_path: None
                    shared_module_key: None
                mlp_decoder: None
                pooling: SelfAttention.Config:
                    attn_dimension: 64
                    dropout: 0.4
                save_path: None
                shared_module_key: None
        trainer: TaskTrainer.Config:
            do_eval: True
            early_stop_after: 0
            epochs: 10
            fp16_args: FP16OptimizerFairseq.Config:
                init_loss_scale: 128
                min_loss_scale: 0.0001
                scale_tolerance: 0.0
                scale_window: None
                threshold_loss_scale: None
            load_best_model_after_train: True
            max_clip_norm: None
            num_accumulated_batches: 1
            num_batches_per_epoch: None
            num_samples_to_log_progress: 1000
            optimizer: Adam.Config:
                eps: 1e-08
                lr: 0.001
                weight_decay: 1e-05
            report_train_metrics: True
            scheduler: None
            sparsifier: None
            target_time_limit_seconds: None
    test_out_path: /tmp/test_out.txt
    torchscript_quantize: False
    use_config_from_snapshot: True
    use_cuda_for_testing: True
    use_cuda_if_available: True
    use_deterministic_cudnn: False
    use_fp16: False
    use_tensorboard: True
    version: 19


        # for debug of GPU
        use_cuda_if_available: True
        device_id: 0
        world_size: 2
        torch.cuda.is_available(): True
        cuda.CUDA_ENABLED: True
        cuda.DISTRIBUTED_WORLD_SIZE: 2
        
# for debug of FP16: fp16_enabled=False
Traceback (most recent call last):
  File ""/home/luning/.pyenv/versions/pytext-nlp/bin/pytext"", line 8, in <module>
    sys.exit(main())
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/main.py"", line 369, in train
    train_model_distributed(config, metric_channels)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/main.py"", line 91, in train_model_distributed
    config.distributed_world_size,
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 171, in spawn
    while not spawn_context.join():
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 118, in join
    raise Exception(msg)
Exception: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/torch/multiprocessing/spawn.py"", line 19, in _wrap
    fn(i, *args)
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/main.py"", line 114, in run_single
    metadata=metadata,
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/workflow.py"", line 101, in train_model
    config, dist_init_url, device_id, rank, world_size, metric_channels, metadata
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/workflow.py"", line 130, in prepare_task
    config.gpu_streams_for_distributed_training,
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/workflow.py"", line 73, in _set_distributed
    rank, world_size, dist_init_url, device_id, gpu_streams=gpu_streams
  File ""/home/luning/.pyenv/versions/3.7.5/envs/pytext-nlp/lib/python3.7/site-packages/pytext/utils/distributed.py"", line 42, in dist_init
    _round_robin_process_group = dist_c10d._round_robin_process_groups(
AttributeError: module 'torch.distributed' has no attribute '_round_robin_process_groups'
```",actually import torch indeed exist train passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource file reading warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version training starting training world size passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource false none none none none false false false task data batcher true none source delimiter false false false none model activation dropout freeze false false none none none none false delimiter none freeze false none true none none none true false false true dense none false column false none false false false column text true none true false true freeze false none none loss none none representation dropout freeze false none bidirectional true dropout freeze false none true none none none dropout none none trainer true none none true none none true none none none false true true true false false true version true true true recent call last file line module main file line return file line main file line invoke return file line invoke return file line invoke return file line return file line train file line file line spawn file line join raise exception exception process following error recent call last file line file line file line rank file line file line rank file line module attribute,issue,positive,positive,neutral,neutral,positive,positive
576830706,"diff was https://github.com/facebookresearch/pytext/commit/151be724b91f41f446f131754e5b21451312c81a, either @chenyangyu1988 or the pytorch docs might be able to help further.",either might able help,issue,negative,positive,positive,positive,positive,positive
576829633,"I don't think we have such a parameter (at least in the new design), as we don't use the pytorch dataloader. `distributed_world_size` specifies the number of workers for both training and loading data.",think parameter least new design use number training loading data,issue,negative,negative,neutral,neutral,negative,negative
574865823,"If I install the nightly version of PyTorch (currently 1.5) the pytest tests complete without aborting, but with errors.
PyTorch Linux-GPU install:
`pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html`

pytest run:
[pytext_pytest.txt](https://github.com/facebookresearch/pytext/files/4067632/pytext_pytest.txt)

",install nightly version currently complete without install pip install torch run,issue,negative,positive,neutral,neutral,positive,positive
574421376,"Still has error with current master branch (0.3.1) of pytext.

### Install
```
$ git clone git@github.com:facebookresearch/pytext.git
$ cd pytext
$ python3 -m venv pytext_venv
$ source pytext_venv/bin/activate
$ pip install --upgrade pip
$ pip install torch
$ ./install_deps
$ export LANG=en_US.utf8
$ export LC_ALL=en_US.utf8
```

### Config file (top.json)
```
{
  ""task"": {
    ""SemanticParsingTask"": {
      ""data"": {
        ""batcher"": {
          ""PoolingBatcher"": {
            ""eval_batch_size"": 1,
            ""test_batch_size"": 1,
            ""train_batch_size"": 1
          }
        },
        ""source"": {
          ""TSVDataSource"": {
            ""field_names"": [""text"", ""tokenized_text"", ""seqlogical""],
            ""train_filename"": ""/home/user/top/train.tsv"",
            ""test_filename"": ""/home/user/top/test.tsv"",
            ""eval_filename"": ""/home/user/top/eval.tsv""
          }
        }
      },
      ""model"": {
        ""lstm"": {
          ""dropout"": 0.34,
          ""lstm_dim"": 16,
          ""num_layers"": 2,
          ""bidirectional"": true
        },
        ""ablation"": {
          ""use_buffer"": true,
          ""use_stack"": true,
          ""use_action"": true,
          ""use_last_open_NT_feature"": false
        },
        ""constraints"": {
          ""intent_slot_nesting"": true,
          ""ignore_loss_for_unsupported"": false,
          ""no_slots_inside_unsupported"": true
        },
        ""max_open_NT"": 10,
        ""dropout"": 0.34,
        ""compositional_type"": ""sum""
      },
      ""metric_reporter"": {
        ""text_column_name"": ""tokenized_text""
      },
      ""trainer"": {
        ""real_trainer"": {
          ""report_train_metrics"": false,
          ""epochs"": 1
        }
      }
    }
  },
  ""version"": 12
}
```
### Training
```
$ pytext train < top.json
```
### Predict output using tutorial (key is ""text"")
```
$ pytext predict-py --model-file=/tmp/model.pt
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Install apex from https://github.com/NVIDIA/apex/.
Loading model from /tmp/model.pt
Loaded checkpoint...
Use config saved in snapshot
Creating task: SemanticParsingTask...
Skipped initializing tensorizers since they are loaded from a previously saved state.
Loading model from model state dict...
Loaded!
please input a json example, the names should be the same with column_to_read in model training config:
{""text"": ""order coffee from starbucks""}
Traceback (most recent call last):
  File ""/home/user/pytext/pytext/pytext_venv/bin/pytext"", line 11, in <module>
    load_entry_point('pytext-nlp', 'console_scripts', 'pytext')()
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/user/pytext/pytext/pytext/main.py"", line 447, in predict_py
    pprint.pprint(task.predict([json.loads(line)])[0])
  File ""/home/user/pytext/pytext/pytext/task/new_task.py"", line 233, in predict
    _, inputs = next(pad_and_tensorize_batches(self.data.tensorizers, batches))
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 157, in pad_and_tensorize_batches
    for raw_batch, numberized_batch in batches:
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 140, in batchify
    for super_pool in self._group_iter(iterable, super_pool_size, None):
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 78, in _group_iter
    for group in itertools.zip_longest(*iterators):
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 300, in numberize_rows
    for name, tensorizer in self.tensorizers.items()
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 300, in <dictcomp>
    for name, tensorizer in self.tensorizers.items()
  File ""/home/user/pytext/pytext/pytext/data/tensorizers.py"", line 454, in numberize
    tokens, start_idx, end_idx = self._lookup_tokens(row[self.text_column])
KeyError: 'tokenized_text'
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
$
```
### Predict output using column_to_read (key is ""tokenized_text"")
```
$ pytext predict-py --model-file=/tmp/model.pt
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Install apex from https://github.com/NVIDIA/apex/.
Loading model from /tmp/model.pt
Loaded checkpoint...
Use config saved in snapshot
Creating task: SemanticParsingTask...
Skipped initializing tensorizers since they are loaded from a previously saved state.
Loading model from model state dict...
Loaded!
please input a json example, the names should be the same with column_to_read in model training config:
{""tokenized_text"": ""order coffee""}
Traceback (most recent call last):
  File ""/home/user/pytext/pytext/pytext_venv/bin/pytext"", line 11, in <module>
    load_entry_point('pytext-nlp', 'console_scripts', 'pytext')()
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/user/pytext/pytext/pytext_venv/lib/python3.6/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/user/pytext/pytext/pytext/main.py"", line 447, in predict_py
    pprint.pprint(task.predict([json.loads(line)])[0])
  File ""/home/user/pytext/pytext/pytext/task/new_task.py"", line 233, in predict
    _, inputs = next(pad_and_tensorize_batches(self.data.tensorizers, batches))
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 157, in pad_and_tensorize_batches
    for raw_batch, numberized_batch in batches:
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 140, in batchify
    for super_pool in self._group_iter(iterable, super_pool_size, None):
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 78, in _group_iter
    for group in itertools.zip_longest(*iterators):
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 300, in numberize_rows
    for name, tensorizer in self.tensorizers.items()
  File ""/home/user/pytext/pytext/pytext/data/data.py"", line 300, in <dictcomp>
    for name, tensorizer in self.tensorizers.items()
  File ""/home/user/pytext/pytext/pytext/data/tensorizers.py"", line 1578, in numberize
    annotation = Annotation(row[self.column])
KeyError: 'seqlogical'
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
Destroying TSV object
Total number of rows read: 0
Total number of rows dropped: 0
```",still error current master branch install git clone git python source pip install upgrade pip pip install torch export export file task data batcher source text model dropout bidirectional true ablation true true true false true false true dropout sum trainer false version training train predict output tutorial key text passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource install apex loading model loaded use saved snapshot task since loaded previously saved state loading model model state loaded please input example model training text order coffee recent call last file line module file line return file line main file line invoke return file line invoke return file line invoke return file line return file line line file line predict next file line file line iterable none file line group file line name file line name file line row object total number read total number object total number read total number object total number read total number predict output key passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type quint passing type synonym type future version understood type type passing type synonym type future version understood type type resource install apex loading model loaded use saved snapshot task since loaded previously saved state loading model model state loaded please input example model training order coffee recent call last file line module file line return file line main file line invoke return file line invoke return file line invoke return file line return file line line file line predict next file line file line iterable none file line group file line name file line name file line annotation annotation row object total number read total number object total number read total number object total number read total number,issue,positive,positive,neutral,neutral,positive,positive
574362276,"Turns out since the ""dense"" field can only be ""FloatListTensorizer"", you don't need to include it. So

""dense"": {
""column"": ""dense_feat"",
""dim"": 48,
""error_check"": false,
""normalize"": false
}

should work.",turn since dense field need include dense column dim false normalize false work,issue,negative,negative,negative,negative,negative,negative
574342454,"batch sizes are in `data.batcher.{train,eval,test}_batch_size`. What do you mean by num_workers?",batch size train test mean,issue,negative,negative,negative,negative,negative,negative
563376895,"If you mean printing during validation (every epoch of training), then https://github.com/facebookresearch/pytext/blob/master/pytext/metrics/__init__.py#L232 is where the prints are happening. The `print` method for the `macro_prf1_metrics` attribute is already implemented in https://github.com/facebookresearch/pytext/blob/master/pytext/metrics/__init__.py#L178, so you could probably just call that.",mean printing validation every epoch training happening print method attribute already could probably call,issue,negative,negative,negative,negative,negative,negative
562866209,"@habout632 hey even i want to print the prf1 score, did you manage to get it working?",hey even want print score manage get working,issue,negative,neutral,neutral,neutral,neutral,neutral
562861325,"@JiesiZhao077 I noticed that we dont print the macro and micro f1 score , how can i print those?",dont print macro micro score print,issue,negative,neutral,neutral,neutral,neutral,neutral
560542319,"Roughly speaking, yes that's right. Your data should be raw text and the data tensorizer should create something like you showed.

More specifically, follow the code in https://github.com/facebookresearch/pytext/blob/master/pytext/data/bert_tensorizer.py#L136 to see how the data is tensorized. The exact output format depends on the config values and the subclass of BERTTensorizerBase you're using. (You can drop a pdb into that method to inspect).

finally, the data is split into chunks of fixed length in https://github.com/facebookresearch/pytext/blob/master/pytext/data/packed_lm_data.py#L150, and you can also use pdb there to see the final data chunks.",roughly speaking yes right data raw text data create something like specifically follow code see data exact output format subclass drop method inspect finally data split fixed length also use see final data,issue,positive,positive,neutral,neutral,positive,positive
559924691,Same problem with predict `(AttributeError: 'SemanticParsingTask' object has no attribute 'data_handler')` - Any hint on what the issue is?,problem predict object attribute hint issue,issue,negative,neutral,neutral,neutral,neutral,neutral
558149429,"There is a fix concerning the training failing and a pull request to add it to master:
[https://github.com/facebookresearch/pytext/pull/1151](url)
But the predict is still failing. Any known solution?
",fix concerning training failing pull request add master predict still failing known solution,issue,negative,neutral,neutral,neutral,neutral,neutral
557275882,I was able to train a hierarchical intent and slot filling model using this fix.  Thanks!,able train hierarchical intent slot filling model fix thanks,issue,negative,positive,positive,positive,positive,positive
543042346,"@snisarg  when the set is ""target_prob"": true ,the error raised as the title mentioned.",set true error raised title,issue,negative,positive,positive,positive,positive,positive
542958034,Thanks for the detailed traces. I can reproduce it now. There's something up with the tensorizer that I can't point by looking at the code. I'll prioritize this and debug to get results about this soon. ,thanks detailed reproduce something ca point looking code get soon,issue,negative,positive,positive,positive,positive,positive
541592023,"I met the same bug.
in pytext/data/tensorizers.py around line 1313,

       try:
            while True:
                row = yield
                annotation = Annotation(row[self.column])
                actions = annotation.tree.to_actions()
                self.vocab_builder.add_all(actions)
        except GeneratorExit:
            self.vocab = self.vocab_builder.make_vocab()
            print(self.vocab.idx)   # maybe the bug is here, self.vocab.idx is {}
            self.shift_idx = self.vocab.idx[SHIFT]
            self.reduce_idx = self.vocab.idx[REDUCE]
",met bug around line try true row yield annotation annotation row except print maybe bug shift reduce,issue,negative,positive,positive,positive,positive,positive
541261109,"Thank you for your pull request. We require contributors to sign our Contributor License Agreement, and yours may need attention.

You currently have a CLA record in our system (either from being a former employee or from a previous collaboration) that may require an update. **Please email [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%23631)** with your name, GitHub id, email address, screenshot of your attempt to sign the CLA at code.facebook.com/cla, and whether you are a former employee, so we can update your status.

Note that if you feel this message is in error, occasionally the message appears transiently and the CLA bot will catch up and comment that your CLA is still in tact. So please give a few minutes before sending the email. Thank you.",thank pull request require sign contributor license agreement may need attention currently record system either former employee previous collaboration may require update please name id address attempt sign whether former employee update status note feel message error occasionally message transiently bot catch comment still tact please give sending thank,issue,positive,negative,neutral,neutral,negative,negative
540099060,@Titousensei  could you try rebasing? The circleCI error is caused by ONNX == 1.6.0 which is fixed in https://github.com/facebookresearch/pytext/pull/1014,could try error fixed,issue,negative,positive,neutral,neutral,positive,positive
539298491,"@snisarg 
Thanks a lot . BYW, how to assign the weight value, for example:
```
{
'pos': value1, 
'neutral': value2,
'neg': value3
}
```
the value1, value2, value3 should be assigned as '0.2, 0.6, 0.2' or ' 1.0, 3.0, 2.0' ?
Are weights set in intervals?

",thanks lot assign weight value example value value value value value value assigned set,issue,positive,positive,positive,positive,positive,positive
539274506,"@snisarg 
the config likes :
```
{
  ""config"": {
    ""task"": {
      ""DocClassificationTask"": {
        ""features"": {
          ""word_feat"": {
            ""embed_dim"": 128,
            ""embedding_init_strategy"": ""zero"",
            ""pretrained_embeddings_path"": ""train/token_vec_128.bin""
          }
        },
        ""featurizer"": {
          ""SimpleFeaturizer"": {
            ""sentence_markers"": null,
            ""lowercase_tokens"": true,
            ""split_regex"": ""\\s+"",
            ""convert_to_bytes"": false
          }
        },
        ""data_handler"": {
          ""columns_to_read"": [
            ""doc_label"",
            ""text""
          ],
          ""shuffle"": true,
          ""sort_within_batch"": true,
          ""train_path"": ""train.tsv"",
          ""eval_path"": ""eval.tsv"",
          ""test_path"": ""test.tsv"",
          ""train_batch_size"": 128,
          ""eval_batch_size"": 128,
          ""test_batch_size"": 128,
          ""column_mapping"": {},
          ""max_seq_len"": -1
        },
        ""trainer"": {
          ""epochs"": 2,
          ""optimizer"": {
            ""Adam"": {
              ""lr"": 0.001,
              ""weight_decay"": 1e-05
            }
          }
        },
        ""exporter"": null,
        ""model"": {
          ""representation"": {
            ""BiLSTMDocAttention"": {
              ""lstm"": {
                ""dropout"": 0.5,
                ""lstm_dim"": 64,
                ""num_layers"": 2,
                ""bidirectional"": true
              },
              ""pooling"": {
                ""SelfAttention"": {
                  ""attn_dimension"": 64,
                  ""dropout"": 0.4
                }
              },
              ""mlp_decoder"": null
            }
          },
          ""decoder"": {
            ""load_path"": null,
            ""save_path"": null,
            ""freeze"": false,
            ""shared_module_key"": null,
            ""hidden_dims"": [],
            ""out_dim"": null,
            ""layer_norm"": true
          },
          ""output_layer"": {
            ""load_path"": null,
            ""save_path"": null,
            ""freeze"": false,
            ""shared_module_key"": null,
            ""loss"": {
              ""CrossEntropyLoss"": {}
            },
            ""label_weights"": null
          }
        },
        ""labels"": {
          ""export_output_names"": [
            ""doc_scores""
          ],
          ""label_weights"": {},
          ""target_prob"": true 
        }
      }
    },
    ""save_snapshot_path"": ""m.pt"",
    ""export_caffe2_path"": ""m.c2"",
    ""export_onnx_path"": ""m.nx""
  }
}
```

the  set is ""target_prob"": true 


",task zero null true false text shuffle true true trainer exporter null model representation dropout bidirectional true dropout null null null freeze false null null true null null freeze false null loss null true set true,issue,positive,positive,positive,positive,positive,positive
537776329,"demo folder was in the root folder, it's moved to /pytext/demo internally to add OSS integration tests which work both internally and externally by https://github.com/facebookresearch/pytext/pull/1022 . The repo is expected to be out of sync as I updated the Github export config. Now I'm re-syncing Github with internal repo and demo will be moved back to /demo as it was in Github repo",folder root folder internally add integration work internally externally sync export internal back,issue,negative,neutral,neutral,neutral,neutral,neutral
537595659,Hi @snisarg .  The issue is in the github master.  Please refer to line 75 of https://github.com/facebookresearch/pytext/files/3664614/pytext_apex_bug.txt or lines 1 through 12 of https://github.com/facebookresearch/pytext/files/3664372/pytext_bug.txt,hi issue master please refer line,issue,negative,neutral,neutral,neutral,neutral,neutral
537582632,"This is a TensorBoard warning. I'm not entirely sure if the work we're doing on TensorBoard will fix this, but let's go back to this in case the error persists. I'm hoping updates will fix it. ",warning entirely sure work fix let go back case error fix,issue,negative,positive,positive,positive,positive,positive
537581050,"Since we're transitioning to TorchScript, we're in a weird state where we may not support Caffe2 and for newer models even TorchScript for exporting. Note that you can still use PyTorch models for inference. ",since weird state may support even note still use inference,issue,negative,negative,negative,negative,negative,negative
537314554,"label_weights is a ""string: float"" dictionary format. There are code references in the project. I urge you to have a look.",string float dictionary format code project urge look,issue,negative,neutral,neutral,neutral,neutral,neutral
537313383,@ButteredGroove can you try from github master instead of the pip release and let us know? ,try master instead pip release let u know,issue,negative,neutral,neutral,neutral,neutral,neutral
536105405,"I get the same error if I have apex installed as well.
[pytext_apex_bug.txt](https://github.com/facebookresearch/pytext/files/3664614/pytext_apex_bug.txt)
",get error apex well,issue,negative,neutral,neutral,neutral,neutral,neutral
536081987,I split off the problem of the demo failing to train with v0.2.2 into it's own issue.  Once that's fixed maybe we can revisit the issue here about predictions not working.,split problem failing train issue fixed maybe revisit issue working,issue,negative,positive,neutral,neutral,positive,positive
534271042,@Titousensei Rebasing to most recent master did get rid of the ` build_docs` error. Please help merge the PR. Thank you!,recent master get rid error please help merge thank,issue,positive,neutral,neutral,neutral,neutral,neutral
533752657,"Thank you for your pull request. We require contributors to sign our Contributor License Agreement, and yours has expired.

Before we can review or merge your code, **we need you to email [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%23871)** with your details so we can update your status.",thank pull request require sign contributor license agreement review merge code need update status,issue,positive,neutral,neutral,neutral,neutral,neutral
533340128,I think build_docs is was broken in master when you submitted your PR. Please try rebasing to  the most recent master.,think broken master please try recent master,issue,negative,negative,negative,negative,negative,negative
532250000,"I have similar issue too, but my task is SquadQATask

```

===Starting training...

Parameters: PyTextConfig:
    debug_path: /tmp/model.debug
    distributed_world_size: 1
    export_caffe2_path: /pytext_models/qna.predictor.c2
    export_onnx_path: /pytext_models/qna.predictor.onnx
    export_torchscript_path: None
    load_snapshot_path: 
    modules_save_dir: 
    random_seed: None
    report_eval_results: False
    save_all_checkpoints: False
    save_module_checkpoints: False
    save_snapshot_path: /pytext_models/qna.predictor.pt
    task: SquadQATask.Config:
        data: Data.Config:
            batcher: PoolingBatcher.Config:
                eval_batch_size: 16
                pool_num_batches: 10000
                test_batch_size: 16
                train_batch_size: 16
            in_memory: True
            sort_key: None
            source: SquadDataSource.Config:
                eval_filename: /data/dev-v2.0.json
                ignore_impossible: True
                test_filename: /dev-v2.0.json
                train_filename: /train-v2.0.json
        metric_reporter: SquadMetricReporter.Config:
            false_label: False
            ignore_impossible: True
            max_answer_length: 16
            n_best_size: 5
            output_path: /tmp/test_out.txt
            pep_format: False
        model: DrQAModel.Config:
            doc_rnn: StackedBidirectionalRNN.Config:
                bidirectional: True
                concat_layers: True
                dropout: 0.4
                freeze: False
                hidden_size: 32
                load_path: None
                num_layers: 1
                rnn_type: RnnType.LSTM
                save_path: None
                shared_module_key: None
            dropout: 0.4
            embedding: WordEmbedding.Config:
                embed_dim: 300
                embedding_init_range: None
                embedding_init_strategy: EmbedInitStrategy.RANDOM
                export_input_names: ['tokens_vals']
                freeze: False
                load_path: None
                lowercase_tokens: True
                min_freq: 1
                mlp_layer_dims: []
                padding_idx: None
                pretrained_embeddings_path: 
                save_path: None
                shared_module_key: None
                vocab_file: 
                vocab_from_all_data: False
                vocab_from_pretrained_embeddings: False
                vocab_from_train_data: True
                vocab_size: 0
            inputs: ModelInput:
                has_answer: LabelTensorizer.Config:
                    allow_unknown: False
                    column: has_answer
                    label_vocab: None
                    pad_in_vocab: False
                squad_input: SquadTensorizer.Config:
                    add_bos_token: False
                    add_eos_token: False
                    answer_starts_column: answer_starts
                    answers_column: answers
                    column: text
                    doc_column: doc
                    max_doc_seq_len: 256
                    max_ques_seq_len: 64
                    max_seq_len: None
                    ques_column: question
                    tokenizer: Tokenizer.Config:
                        lowercase: True
                        split_regex: \W+
                    use_eos_token_for_bos: False
                    vocab: VocabConfig:
                        build_from_data: True
                        size_from_data: 0
                        vocab_files: []
            output_layer: SquadOutputLayer.Config:
                false_label: False
                freeze: False
                has_answer_loss_weight: 0.5
                ignore_impossible: True
                load_path: None
                loss: CrossEntropyLoss.Config:
                pos_loss_weight: 0.5
                save_path: None
                shared_module_key: None
            ques_rnn: StackedBidirectionalRNN.Config:
                bidirectional: True
                concat_layers: True
                dropout: 0.4
                freeze: False
                hidden_size: 32
                load_path: None
                num_layers: 1
                rnn_type: RnnType.LSTM
                save_path: None
                shared_module_key: None
        trainer: TaskTrainer.Config:
            do_eval: True
            early_stop_after: 0
            epochs: 10
            max_clip_norm: None
            num_accumulated_batches: 1
            num_batches_per_epoch: None
            num_samples_to_log_progress: 1000
            optimizer: Adam.Config:
                eps: 1e-08
                lr: 0.001
                weight_decay: 1e-05
            report_train_metrics: True
            scheduler: None
            sparsifier: None
            target_time_limit_seconds: None
    test_out_path: /tmp/test_out.txt
    torchscript_quantize: False
    use_cuda_if_available: True
    use_deterministic_cudnn: False
    use_fp16: False
    use_tensorboard: True
    version: 14


    # for debug of GPU
    use_cuda_if_available: True
    device_id: 0
    world_size: 1
    torch.cuda.is_available(): True
    cuda.CUDA_ENABLED: True
    cuda.DISTRIBUTED_WORLD_SIZE: 1
    
# for debug of FP16: fp16_enabled=False
Num trainable parameters: 5571983
```

***I install pytext using `pip install pytext-nlp` (pytext-nlp==0.2.2)

I also tried using `pytext export --output-path qna.predictor.c2 < myconfig.json`,
turns out that nothing happen except this message
```
Install apex from https://github.com/NVIDIA/apex/.
No config file specified, reading from stdin
Exporting /pytext_models/qna.predictor.pt to caffe2 file: /pytext_models/qna.predictor.c2 and onnx file: /pytext_models/qna.predictor.onnx
Loading model from /media/datawow/harddrive8TB/qna/pytext_models/qna.predictor.pt...
Skipped initializing tensorizers since they are loaded from a previously saved state.
```
caffe2 and onnx models are still not found :(",similar issue task training none none false false false task data batcher true none source true false true false model bidirectional true true dropout freeze false none none none dropout none freeze false none true none none none false false true false column none false false false column text doc none question true false true false freeze false true none loss none none bidirectional true true dropout freeze false none none none trainer true none none true none none none false true false false true version true true true trainable install pip install also tried export turn nothing happen except message install apex file reading file file loading model since loaded previously saved state still found,issue,positive,negative,neutral,neutral,negative,negative
530252813,"@mwu1993 I used the sst2.json format. the command as following:
```json
{
  ""config"": {
    ""task"": {
      ""DocClassificationTask"": {
        ""model"": {
          ""representation"": {
            ""BiLSTMDocAttention"": {
              ""lstm"":{
                ""num_layers"": 2
              }
            }
          },
          ""decoder"": {
            ""activation"": ""gelu""
          }
        },
        ""data_handler"": {
          ""columns_to_read"": [
            ""doc_label"",
            ""text""
          ],
          ""train_path"": ""train.tsv"",
          ""eval_path"": ""eval.tsv"",
          ""test_path"": ""test.tsv""
        },
        ""trainer"": {
          ""epochs"": 100 
        }
      }
    },
    ""save_snapshot_path"":""m.pt"",
    ""export_caffe2_path"":""m.c2"",
    ""export_onnx_path"":""m.nx""
  }
}

```

```
pytext train < sst2.json
pytext test < sst2.json
pytext export < sst2.json
```
everything seems will.

but when to make a simple app.
(https://pytext.readthedocs.io/en/master/execute_your_first_model.html)

the error turns out.



",used format command following task model representation activation text trainer train test export everything make simple error turn,issue,negative,neutral,neutral,neutral,neutral,neutral
526780103,Wondering if there's more news on this front?  Thanks!,wondering news front thanks,issue,negative,positive,positive,positive,positive,positive
525690238,"Thanks @hudeven, for the PR.

A similar change `raw_text => text` is needed in the `flask_app` example https://pytext.readthedocs.io/en/master/execute_your_first_model.html at the following line:
```
    # Pass the inputs to PyText's prediction API
    result = predictor({""raw_text"": text})
```
",thanks similar change text example following line pas prediction result predictor text,issue,negative,positive,neutral,neutral,positive,positive
525671872,"One more thing, there is an additional bracket at the following line - 
`words_tensors, seq_len_tensors = tensorizer.tensorize(numberized_rows))` ",one thing additional bracket following line,issue,negative,neutral,neutral,neutral,neutral,neutral
525029035,"the fix in FairSeq repo is working. https://circleci.com/gh/facebookresearch/pytext/12894
Closing this pull request.",fix working pull request,issue,negative,neutral,neutral,neutral,neutral,neutral
525011326,"@deepali-c thanks for your report! With pytorch v1.2.0, I saw the similar error message but am able to complete training and eval the model. Does it block you from train/eval/export the model? If it does, please share me 1) your OS type, version 2) how you install PyText, from pypi or source code. Try PyText v0.2.2 if you haven't  3) full log of your error messages",thanks report saw similar error message able complete training model block model please share o type version install source code try full log error,issue,negative,positive,positive,positive,positive,positive
524616661,"Add the following to the pip command-line to ignore the uninstall:

 --ignore-installed certifi",add following pip ignore,issue,negative,neutral,neutral,neutral,neutral,neutral
524227670,"Updating code locally to correctly compare the tensors at the end helps here. 
The example in the document needs to be updated likewise:

 ``` 
assert torch.all(torch.eq(words_tensors, tensor([[2, 3, 4, 5], [6, 7, 8, 1]])))
assert torch.all(torch.eq(seq_len_tensors, tensor([4, 3])))
```

",code locally correctly compare end example document need likewise assert tensor assert tensor,issue,negative,neutral,neutral,neutral,neutral,neutral
524203580,"I tried this with `v0.2.2` and observed the following:
#1 is resolved
#2 is now observed as:
 ```
python3 my_classifier/source.py atis | head -n 3
WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.
Install apex from https://github.com/NVIDIA/apex/.
TRAIN {'query': 'i want to fly from boston at 838 am and arrive in denver at 1110 in the morning', 'intent': ''}
TRAIN {'query': 'what is the arrival time in san francisco for the 755 am flight leaving washington', 'intent': ''}
TRAIN {'query': 'cheapest airfare from tacoma to orlando', 'intent': ''}
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='ANSI_X3.4-1968'>
BrokenPipeError: [Errno 32] Broken pipe
```
#3 is still seen and I changed `builtin_task.py` to get it to work. 
#4 is not seen, training is successful.
",tried following resolved python atis head warning root python run support run mode install apex train want fly boston arrive morning train arrival time san flight leaving train exception broken pipe still seen get work seen training successful,issue,negative,positive,positive,positive,positive,positive
523136628,"@deepali-c thanks for reporting this issue. The root cause is: data_processor.py is outdated. It doesn't generate column 'doc_weight' and 'word_weight' in the training data. 

A quick walk-around is to add them manually to each data file(with doc_weight=0.2, word_weight=0.5). e.g. 
awk '{print $0""\011""0.2""\011""0.5}' demo/atis_joint_model/atis.processed.val.csv > demo/atis_joint_model/atis.processed.val.with_weights.csv

I will fix the data_processor.py soon. ",thanks issue root cause outdated generate column training data quick add manually data file print fix soon,issue,negative,positive,neutral,neutral,positive,positive
522925414,"This has been fixed by the following PR on master branch - https://github.com/facebookresearch/pytext/pull/915. 

applying this patch to `v0.2.2` leads to the following issue at predict:

```
$ pytext --config-file demo/atis_joint_model/atis_joint_config.json   predict --exported-model /tmp/atis_joint_model.c2 <<< '{""raw_text"": ""flights from colorado""}'
Install apex from https://github.com/NVIDIA/apex/.
WARNING - Applying old config adapter for version=0. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=1. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=2. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=3. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=4. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=5. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=6. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=7. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=8. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=9. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=10. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=11. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=12. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=13. Please consider migrating your old configs to the latest version.
WARNING - Applying old config adapter for version=14. Please consider migrating your old configs to the latest version.
Loading model from /tmp/atis_joint_model.c2
Model loaded, reading example JSON from stdin
Traceback (most recent call last):
  File ""<myenv>/bin/pytext"", line 11, in <module>
    load_entry_point('pytext-nlp==0.2.2', 'console_scripts', 'pytext')()
  File ""<myenv>/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""<myenv>/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""<myenv>/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""<myenv>/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""<myenv>/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""<myenv>/lib/python3.6/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""<myenv>/lib/python3.6/site-packages/pytext/main.py"", line 450, in predict
    predictions = predictor(input)
  File ""<myenv>/lib/python3.6/site-packages/pytext/__init__.py"", line 74, in <lambda>
    workspace_id, predict_net, new_task.model, input_tensorizers, input
  File ""<myenv>/lib/python3.6/site-packages/pytext/__init__.py"", line 27, in _predict
    for name, tensorizer in tensorizers.items()
  File ""<myenv>/lib/python3.6/site-packages/pytext/__init__.py"", line 27, in <dictcomp>
    for name, tensorizer in tensorizers.items()
  File ""<myenv>/lib/python3.6/site-packages/pytext/data/tensorizers.py"", line 258, in prepare_input
    tokenized_texts, start_idx, end_idx = self._tokenize(row[self.text_column])
KeyError: 'text'
```

This worked when tried with the example - ```'{""text"": ""flights from colorado""}'```",fixed following master branch patch following issue predict predict colorado install apex warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version warning old adapter please consider old latest version loading model model loaded reading example recent call last file line module file line return file line main file line invoke return file line invoke return file line invoke return file line return file line predict predictor input file line lambda input file line name file line name file line row worked tried example text colorado,issue,negative,positive,positive,positive,positive,positive
521115737,Looking forward. Model could be exported to caffe2 and onnx is one of the great feature.,looking forward model could one great feature,issue,positive,positive,positive,positive,positive,positive
521071563,"@i4never thanks for reporting this issue! I'm working on a fix for it.

Root cause: caffe2_export() is not implemented for some models like PairwiseModel. https://github.com/facebookresearch/pytext/blob/af1d1dca4db373601c9dbb84dfb0767ed435a840/pytext/models/pair_classification_model.py#L103",thanks issue working fix root cause like,issue,positive,positive,positive,positive,positive,positive
520611070,"I had been installing from pip, not the repo.  So, I did a fresh install of pytext following the instructions here: https://pytext.readthedocs.io/en/master/installation.html#install-from-source

pytest and pytest --cov worked fine.

However, when I run pytext I'm getting the following error:
```
$ pytext train < rnng.json
8<--- snip --->8
Traceback (most recent call last):
  File ""/home/user/.local/bin/pytext"", line 11, in <module>
    load_entry_point('pytext-nlp', 'console_scripts', 'pytext')()
  File ""/home/user/.local/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/user/.local/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/user/.local/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/user/.local/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/user/.local/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/user/.local/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/gpfs-volume/pytext/pytext/pytext/main.py"", line 350, in train
    train_model(config, metric_channels=metric_channels)
  File ""/gpfs-volume/pytext/pytext/pytext/workflow.py"", line 89, in train_model
    config, dist_init_url, device_id, rank, world_size, metric_channels, metadata
  File ""/gpfs-volume/pytext/pytext/pytext/workflow.py"", line 125, in prepare_task
    config.task, metadata=metadata, rank=rank, world_size=world_size
  File ""/gpfs-volume/pytext/pytext/pytext/task/task.py"", line 43, in create_task
    world_size=world_size,
  File ""/gpfs-volume/pytext/pytext/pytext/config/component.py"", line 154, in create_component
    return cls.from_config(config, *args, **kwargs)
  File ""/gpfs-volume/pytext/pytext/pytext/task/new_task.py"", line 100, in from_config
    tensorizers, data = cls._init_tensorizers(config, tensorizers, rank, world_size)
  File ""/gpfs-volume/pytext/pytext/pytext/task/new_task.py"", line 142, in _init_tensorizers
    init_tensorizers=init_tensorizers,
  File ""/gpfs-volume/pytext/pytext/pytext/config/component.py"", line 154, in create_component
    return cls.from_config(config, *args, **kwargs)
  File ""/gpfs-volume/pytext/pytext/pytext/data/data.py"", line 243, in from_config
    **kwargs,
  File ""/gpfs-volume/pytext/pytext/pytext/data/data.py"", line 270, in __init__
    initialize_tensorizers(self.tensorizers, full_train_data)
  File ""/gpfs-volume/pytext/pytext/pytext/data/tensorizers.py"", line 1306, in initialize_tensorizers
    for row in data_source:
  File ""/gpfs-volume/pytext/pytext/pytext/data/sources/data_source.py"", line 243, in _convert_raw_source
    example = self._read_example(row)
  File ""/gpfs-volume/pytext/pytext/pytext/data/sources/data_source.py"", line 217, in _read_example
    example[name] = self.load(value, self.schema[name])
  File ""/gpfs-volume/pytext/pytext/pytext/data/sources/data_source.py"", line 264, in load
    return converter(value)
  File ""/gpfs-volume/pytext/pytext/pytext/data/sources/data_source.py"", line 328, in load_json
    return json.loads(s)
  File ""/usr/local/lib/python3.7/json/__init__.py"", line 348, in loads
    return _default_decoder.decode(s)
  File ""/usr/local/lib/python3.7/json/decoder.py"", line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/local/lib/python3.7/json/decoder.py"", line 355, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1)
Exception ignored in: <generator object AnnotationNumberizer.initialize at 0x7fbd6004b9a8>
Traceback (most recent call last):
  File ""/gpfs-volume/pytext/pytext/pytext/data/tensorizers.py"", line 1211, in initialize
    self.shift_idx = self.vocab.idx[SHIFT]
KeyError: 'SHIFT'
```",pip fresh install following worked fine however run getting following error train snip recent call last file line module file line return file line main file line invoke return file line invoke return file line invoke return file line return file line train file line rank file line file line file line return file line data rank file line file line return file line file line file line row file line example row file line example name value name file line load return converter value file line return file line return file line decode end file line raise value none value line column char exception generator object recent call last file line initialize shift,issue,positive,negative,neutral,neutral,negative,negative
520563811,"BERT models are now supported for document classification, although export support is still coming.  Look for bert_classification_models.py",document classification although export support still coming look,issue,negative,neutral,neutral,neutral,neutral,neutral
520561804,"Actually, the related issue seems to be closed, are you still seeing the issue based on the latest code? https://github.com/facebookresearch/pytext/issues/701",actually related issue closed still seeing issue based latest code,issue,negative,positive,positive,positive,positive,positive
520560912,"pytext predict-py is currently broken.  Thanks for reporting the issue, we're working on fixing it.",currently broken thanks issue working fixing,issue,negative,negative,neutral,neutral,negative,negative
520559790,"Could you provide a minimal example with data, possibly using one of the demo configs, so I can try to reproduce it?",could provide minimal example data possibly one try reproduce,issue,negative,negative,neutral,neutral,negative,negative
520550905,Hi! I've updated the tutorial and you should be able to follow the documentation and get non-zero accuracy now. Let me know if you encounter any issue :),hi tutorial able follow documentation get accuracy let know encounter issue,issue,negative,positive,positive,positive,positive,positive
519795500,I am not able to submit a PR at the moment could you please add a change for this too. ,able submit moment could please add change,issue,negative,positive,positive,positive,positive,positive
518906527,I forgot to mention: this is based on the [hierarchical intent and slot filling tutorial](https://pytext.readthedocs.io/en/master/hierarchical_intent_slot_tutorial.html).,forgot mention based hierarchical intent slot filling tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
517795989,"@ishalyminov, this is not currently possible, but it's coming soon with pull request #856 where you will be able to do:

  `pytext gen-default-config IntentSlotTask eval_batch_size=1`

In the mean time, you need to manually edit your config to set `""eval_batch_size"": 1` in the json file.",currently possible coming soon pull request able mean time need manually edit set file,issue,negative,positive,neutral,neutral,positive,positive
517759925,"@Titousensei thanks! So with the `IntentSlotTask` example working for me, here's my original question. Say, I want to generate this config but override some of the parameters right away via the `options` argument, e.g.:

```bash
pytext gen-default-config task.IntentSlotTask.data.Data.batcher.PoolingBatcher.eval_batch_size=1 IntentSlotTask
```

for which I get:
```bash
File ""/home/t-igsha/miniconda3/envs/dstc8/lib/python3.7/site-packages/pytext/main.py"", line 124, in gen_config_impl
    f""Unknown task class: {task_name} "" ""(try fully qualified class name?)""
Exception: Unknown task class: task.IntentSlotTask.data.Data.batcher.PoolingBatcher.eval_batch_size=1 (try fully qualified class name?)
```
(The above example might look weird but I have my specific use-case in which it makes more sense.)
What is the correct way to provide such an option?",thanks example working original question say want generate override right away via argument bash get bash file line unknown task class try fully qualified class name exception unknown task class try fully qualified class name example might look weird specific sense correct way provide option,issue,negative,positive,neutral,neutral,positive,positive
517427986,"This article you linked is using an older version of PyText, probably v0.1.0. The new version of PyText v0.2.0 will try and load this older version of a config and convert it to the latest version, but his is not always possible to do automatically, and it seems to be the case here.

As you suggest, it's better to start fresh from a new config with the latest version. `JointTextTask` is not supported anymore and was renamed `JointTextTask_Deprecated`. The new version is called `IntentSlotTask`. You can get a config by running the command:

  `pytext gen-default-config IntentSlotTask > my_config.json`

You will need to edit the file `my_config.json` to fill in the `field_names` (previously called `columns_to_read`), `train_filename` (and other filenames, previously called `train_path`), `pretrained_embeddings_path`, and maybe others.

Look at `demo/configs/new_joint.json` for a minimal working example.",article linked older version probably new version try load older version convert latest version always possible automatically case suggest better start fresh new latest version new version get running command need edit file fill previously previously maybe look minimal working example,issue,positive,positive,positive,positive,positive,positive
516633651,"I think the 80% here is misleading, this is mainly because these PRs were downstream of merged push which broke the build. This happens mainly for ModelInputs, so we can special case this rule for ModelInputs, or work towards fixing configs generally.",think misleading mainly downstream push broke build mainly special case rule work towards fixing generally,issue,negative,positive,positive,positive,positive,positive
516249313,@Titousensei can you please help Igor with the correct way to supply config params?,please help correct way supply,issue,positive,neutral,neutral,neutral,neutral,neutral
516096918,"We should fix the build_docs issue rather than relaxing the requirements for quality, especially since this is open source. This will only incentivize lowered standards for checking in. It gets very hard to recovery from there IMHO. ",fix issue rather quality especially since open source hard recovery,issue,negative,negative,neutral,neutral,negative,negative
513732629,"@Titousensei, thank you very much
I tried v0.2.0 according to your advise, unfortunately I got this error message

> tokens, seq_lens = tensor_dict[""tokens""]
> ValueError: too many values to unpack (expected 2)

my config file is 

            ""TSVDataSource"": {
              ""column_mapping"": {},
              ""delimiter"": ""\t"",
              ""eval_filename"": ""file"",
              ""field_names"": [""text""],
              ""test_filename"": ""file"",
              ""train_filename"": ""file""
            }

After that I tried
$pytext update-config <my_config_file>
to transform my old version of config file to match with v0.2.0 and now above errors disappear. Anyway, my task has been killed because my data (only 10k text lines) cannot fit in the memory (32G of RAM)",thank much tried according advise unfortunately got error message many unpack file delimiter file text file file tried transform old version file match disappear anyway task data text fit memory ram,issue,negative,positive,positive,positive,positive,positive
513625834,turns out it was the problem of the system's environment,turn problem system environment,issue,negative,neutral,neutral,neutral,neutral,neutral
513048690,"I can't reproduce the issue with ""WordTaggingTask"", which is ""NewWordTaggingTask"" renamed in the last version. What command did you run? Was it `pytext train < ner.json`? Is the issue still happening to you?",ca reproduce issue last version command run train issue still happening,issue,negative,neutral,neutral,neutral,neutral,neutral
512987877,Issue is fixed in source. Will be in upcoming release v0.2.1. Closing.,issue fixed source upcoming release,issue,negative,positive,neutral,neutral,positive,positive
512986684,"We're looking at the list of known issues prioritizing them for the next releases. The fix predict command is already merged in source and will be included in the next release, v0.2.1, which should be released in early August at the latest.",looking list known next fix predict command already source included next release early august latest,issue,negative,positive,positive,positive,positive,positive
512678909,"Thanks @Titousensei, for your response. I had observed some issues and had read in the FB group that more changes are needed in the Predict API. 

By when can we expect `v0.2.1` with the fixes. ",thanks response read group predict expect,issue,negative,positive,positive,positive,positive,positive
512678407,"@Titousensei, I had resolved three above by adding the following in `add_include` method in `builtin_task.py`

```
    import sys
    sys.path.append('')
```",resolved three following method import,issue,negative,neutral,neutral,neutral,neutral,neutral
512553082,I think this is resolved. (It works for me on a fresh install from source.) @megupta can you confirm?,think resolved work fresh install source confirm,issue,negative,positive,positive,positive,positive,positive
512544049,On a mac the directory /tmp should exist (virtual env or not should not make a difference). You can try the following command in a terminal: `ls /tmp/`. If you exported your model you should see a file file `/tmp/model.caffe2.predictor`. The test results should be in `/tmp/test_out.txt`.,mac directory exist virtual make difference try following command terminal model see file file test,issue,negative,neutral,neutral,neutral,neutral,neutral
512437072,"@Titousensei Thank you for reaching out. I'm not very familiar with build_docs, do you have any hint why it might fail?",thank reaching familiar hint might fail,issue,negative,negative,neutral,neutral,negative,negative
512059846,Hi @Jongchan! I also come to question (2). Could you please explain why it's meaningless? Thank!,hi also come question could please explain meaningless thank,issue,negative,negative,negative,negative,negative,negative
512054815,Thanks for the PR. Please fix build_docs so we can include it.,thanks please fix include,issue,positive,positive,positive,positive,positive,positive
512033147,"Thanks for your fix. This tutorial was completely re-written in the last release, but one of the typos remains. I made PR #786 to fix the new issue as a resolution, which replaces this PR. Closing.",thanks fix tutorial completely last release one remains made fix new issue resolution,issue,negative,positive,positive,positive,positive,positive
511001657,"No I'm using a mac

On Fri, Jul 12, 2019, 11:56 PM Eric Gaudet <notifications@github.com> wrote:

> I'm guessing you're on Windows. Can you look in C:\temp\ to see if the
> files are here?
>
> 
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/facebookresearch/pytext/issues/719?email_source=notifications&email_token=AHWNJ6A3PICUIQ7KMF44IDLP7DEELA5CNFSM4H2ZCXZKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ2QQTY#issuecomment-510986319>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AHWNJ6DFO62D7T3HLSG5253P7DEELANCNFSM4H2ZCXZA>
> .
>
",mac eric wrote guessing look see thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
510993393,We don't currently have a way to load ELMo embeddings.  We have support for BERT models.  Would that serve your use case?,currently way load support would serve use case,issue,negative,neutral,neutral,neutral,neutral,neutral
510992361,"I would suggest just chunking you input into fixed-sized lengths and training on that.  As you mention, it's difficult for RNN's to models very long context.  So I doubt you'll lose anything by doing this.  
That being said, the BiLSTM representation module does support passing in state:  https://github.com/facebookresearch/pytext/blob/f35987dc7c9c4e5f26aad9bb2ac1ae398063c788/pytext/models/representations/bilstm.py#L75
We used to use this for language modeling in the past, but removed the functionality because it did not improve results. ",would suggest input training mention difficult long context doubt lose anything said representation module support passing state used use language modeling past removed functionality improve,issue,negative,negative,negative,negative,negative,negative
510975005,"Thanks for the feedback.
- as you observed, issue 1 is already solved.
- for issue 2, BrokenPipeError is often caused by truncated files. Handling it as an exception might hide the fact that your files are corrupted. Could you verify that yours are complete? Mine have 4978 rows for train and 893 for test.
- how did you resolve issue 3? --include my_classifier tells PyText to look into the directory my_classifier/ for your custom python files. Knowing the problem you had and how you resolved it will help me print a better error message.
- issue 4 is a known problem and the updated tutorial should be published soon. In the mean time, you just need to revert the two arguments:
  $ pytext --include my_classifier gen-default-config DocumentClassificationTask AtisIntentDataSource

",thanks feedback issue already issue often truncated handling exception might hide fact corrupted could verify complete mine train test resolve issue include look directory custom python knowing problem resolved help print better error message issue known problem tutorial soon mean time need revert two include,issue,negative,positive,positive,positive,positive,positive
510679658,The new data design with streaming is out with v0.2.0. Please try it and let us know if it solves your issue.,new data design streaming please try let u know issue,issue,negative,positive,positive,positive,positive,positive
510679341,"You can use `--include` to your command line to import your own components. There's a full example in the tutorial ""Creating A New Model"".",use include command line import full example tutorial new model,issue,negative,positive,positive,positive,positive,positive
510677679,@deepali-c we'll be happy to receive a push request with the updated documentation. :) ,happy receive push request documentation,issue,positive,positive,positive,positive,positive,positive
510675886,"The new API implementation is complete as of 0.2.0. Is there anything you think is missing?
The next v0.2.1 will be a bug fix without breaking changes (hopefully). ",new implementation complete anything think missing next bug fix without breaking hopefully,issue,negative,positive,neutral,neutral,positive,positive
510659969,"The new PyText version (released as 0.2.0) should give you the capabilities to do that. Look at the documentation for ""Custom Data Format"" and ""Creating A New Model"", that looks like what you're asking.",new version give look documentation custom data format new model like,issue,negative,positive,positive,positive,positive,positive
509533337,Just found out that the term is meaning less. closing the issue.,found term meaning le issue,issue,negative,neutral,neutral,neutral,neutral,neutral
509479797,I also observed the broken pipe issue. Seems it can be avoided if we handle it in the script. ,also broken pipe issue handle script,issue,negative,negative,negative,negative,negative,negative
507296761,"Even I ran into this issue. 

I was just trying to understand the reason here and observed that earlier the Task being used had features attribute - https://github.com/facebookresearch/pytext/blob/master/pytext/task/tasks.py#L167

but in the new task features is indeed not there. https://github.com/facebookresearch/pytext/blob/master/pytext/task/tasks.py#L190.

Probably this needs to be handled somewhere at -https://github.com/facebookresearch/pytext/blob/master/pytext/__init__.py#L85",even ran issue trying understand reason task used attribute new task indeed probably need handled somewhere,issue,negative,positive,positive,positive,positive,positive
506727450,"I think this is resolved by the PR - https://github.com/facebookresearch/pytext/pull/433.
But looks like document is yet to be updated. Shall I do that?",think resolved like document yet shall,issue,negative,neutral,neutral,neutral,neutral,neutral
504729085,"just tried it with the latest from github:

```
(pytext_venv) pc@debian:~/workarea/experiments/pytext$ pytext export < pytext/demo/configs/docnn.json
Install apex from https://github.com/NVIDIA/apex/.
No config file specified, reading from stdin
Exporting /tmp/model.pt to caffe2 file: /tmp/model.caffe2.predictor and onnx file: /tmp/model.onnx
Loading model from /tmp/model.pt...
Saving caffe2 model to: /tmp/model.caffe2.predictor
Saving onnx model to: /tmp/model.onnx
Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.Warning: ATen was a removed  experimental ops. In the future, we may directly reject this operator. Please update your model as soon as possible.(pytext_venv) pc@debian:~/workarea/experiments/pytext$ pytext --config-file demo/configs/docnn.json predict <<< '{""raw_text"": ""c^Cate an alarm for 1:30 pm""}'

(pytext_venv) pc@debian:~/workarea/experiments/pytext$ pytext --config-file pytext/demo/configs/docnn.json predict <<< '{""raw_text"": ""create an alarm for 1:30 pm""}'
Install apex from https://github.com/NVIDIA/apex/.
Loading model from /tmp/model.caffe2.predictor
Traceback (most recent call last):
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/bin/pytext"", line 10, in <module>
    sys.exit(main())
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/pytext/main.py"", line 381, in predict
    predictor = create_predictor(config, exported_model)
  File ""/home/pc/workarea/experiments/pytext/pytext_venv/lib/python3.7/site-packages/pytext/__init__.py"", line 85, in create_predictor
    feature_config = task.features
AttributeError: 'DocumentClassificationTask.Config' object has no attribute 'features'
```
",tried latest export install apex file reading file file loading model saving model saving model warning removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon removed experimental future may directly reject operator please update model soon possible predict alarm predict create alarm install apex loading model recent call last file line module main file line return file line main file line invoke return file line invoke return file line invoke return file line return file line predict predictor file line object attribute,issue,negative,positive,neutral,neutral,positive,positive
500310215,@Yuhuishishishi No I didn't... Could you let me know if you find a way? Thanks! ,could let know find way thanks,issue,negative,positive,positive,positive,positive,positive
499976601,"Yes, I am encountering the same problem. Did you find a way to resolve this @jianwolf ?",yes problem find way resolve,issue,negative,neutral,neutral,neutral,neutral,neutral
499354589,"Hi @hikushalhere,
Your explanation makes sense and I see some improvements that can be achieved there. But issue is ocuring even if the comma is not present. For examples, 
""_**golden gate bridge**_""  only ""_**gate bridge**_"" gets detected as entity. So there may be something else going on as well which I'm not able to figure out.",hi explanation sense see issue even comma present golden gate bridge gate bridge entity may something else going well able figure,issue,negative,positive,positive,positive,positive,positive
497418886,"@Sanjeev2487 , there's no limit to the number of tokens that the slot tagging model can predict. This could be an issue with tokenization. Are you using the default featurizer implementation in PyText, i.e., SimpleFeaturizer? If so, please be aware that SimpleFeaturizer's default tokenization strategy is splitting on whitespaces. See https://github.com/facebookresearch/pytext/blob/master/pytext/data/featurizer/simple_featurizer.py#L20. If so, tokens followed by comma might not be extracted correctly, e.g., the token you're getting from "" to 203, lincoln avenue"" is `203,` and not `203`. Using the correct regex will solve your problem.",limit number slot model predict could issue default implementation please aware default strategy splitting see comma might extracted correctly token getting avenue correct solve problem,issue,negative,positive,positive,positive,positive,positive
492594515,"I faced the same issue too
my training set is less than 5G and I have 32G for memories :(",faced issue training set le,issue,negative,neutral,neutral,neutral,neutral,neutral
492529874,"DocModel is deprecated now. 
What I find is :
![image](https://user-images.githubusercontent.com/10850020/57754792-dd2f4700-7721-11e9-8389-ff74c03a97fa.png)
Maybe ``create_fn`` is what u need if you want custom representation module.
From an old friend :)
",find image maybe need want custom representation module old friend,issue,positive,positive,neutral,neutral,positive,positive
491492307,"I guess something like  train.MonitoredTrainingSession should be added as seen #463 also talks about the same issue . 
@seayoung1112 are u looking to it ?
@Titousensei ",guess something like added seen also issue looking,issue,negative,neutral,neutral,neutral,neutral,neutral
487789697,"This doesn't actually fix the tests,  #530 incorporates this fix but also fixes the test, so I'm closing this.",actually fix fix also test,issue,negative,neutral,neutral,neutral,neutral,neutral
486898676,"I have created PR to pin onnx to 1.5 already: https://github.com/facebookresearch/pytext/pull/509

I think that should be good enough for us :-)",pin already think good enough u,issue,negative,positive,positive,positive,positive,positive
486842742,"We're working on a similar model called BiTransformer which is not ready to be open sourced, but we have tentative plans to.

We don't have active plans on integrating HuggingFace, but would welcome contributions to add it, as I think that would be particularly valuable to PyText users.

To use these models, you generally need to replace the Embedding component in a DocClassification or WordTagging task with the ""embedder"" or ""decoder"" (depending on the project nomenclature, whatever component contains the transformer network), and then train the task as normal.",working similar model ready open tentative active would welcome add think would particularly valuable use generally need replace component task depending project nomenclature whatever component transformer network train task normal,issue,positive,positive,positive,positive,positive,positive
486450391,"I'm not very familiar with this model and label names, but maybe we can join consecutive spans automatically? Something like

```
city_token_ranges = []
for token_idx, label_idx in enumerate(predicted_labels):
    label = labels[label_idx]
    if ""city_name"" in label:
        current_start, current_end = token_ranges[token_idx]
        if city_token_ranges:
            prev_start, prev_end = city_token_ranges[-1]
            if current_start == prev_end + 1:
                city_token_ranges[-1] = prev_start, current_end
                continue
        city_token_ranges.append((current_start, current_end))
```

If this works for you, I can make a short PR.",familiar model label maybe join consecutive automatically something like enumerate label label continue work make short,issue,positive,positive,positive,positive,positive,positive
485057926,"sorry, wrong posted

I closed it.",sorry wrong posted closed,issue,negative,negative,negative,negative,negative,negative
484910488,"Hi @hikushalhere, 

I cannot figure how to make this work. Would it be possible to have a working example ? with all the modified files ? 

",hi figure make work would possible working example,issue,negative,neutral,neutral,neutral,neutral,neutral
483355359,"Currently, you can specify the ""save_path"" and ""load_path"" of any module, which will resume training with the same parameters for those modules. (This can be done for any module whose config is a subclass of ModuleConfig - https://github.com/facebookresearch/pytext/blob/master/pytext/config/module_config.py).

Full model checkpointing isn't supported yet to my knowledge, but that's something that is planned to be added.",currently specify module resume training done module whose subclass full model yet knowledge something added,issue,negative,positive,positive,positive,positive,positive
483224105,"I added the additional condition for batch_size >1 in the function get_predict_iter, but still facing the same issue. 

**Prediction** fails due to a ValueError : ValueError: not enough values to unpack (expected 2, got 1) 

@aneuraz ",added additional condition function still facing issue prediction due enough unpack got,issue,negative,negative,neutral,neutral,negative,negative
482771303,I think Control + C may be used to resume .,think control may used resume,issue,negative,neutral,neutral,neutral,neutral,neutral
482375630,"Unfortunately, this seems possible at the moment, as the framework is not super memory-efficient. In the new data design (e.g. https://github.com/facebookresearch/pytext/blob/master/demo/configs/new_docnn.json), the data should be streamed instead of being fully loaded into memory, which should help. The LM task hasn't been migrated though :(",unfortunately possible moment framework super new data design data instead fully loaded memory help task though,issue,positive,positive,positive,positive,positive,positive
481154774,"Even I observed this issue while trying the [Hierarchical intent and slot filling](https://pytext-pytext.readthedocs-hosted.com/en/latest/hierarchical_intent_slot_tutorial.html) example:
`ValueError: not enough values to unpack (expected 2, got 1)`

@aneuraz, suggestion to use additional condition for `batch_size >1` helps avoid this error.
However, the output observed is different from what is expected:
```
{'prediction': [30,
                62,
                30,
                49,
                30,
                49,
                30,
                49,
                30,
                49,
                0,
                1,
                62,
                0,
                1,
                62,
                0,
                1,
                62,
                0,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1,
                1],
 'score': [0.017170962849315855,
           0.01713114094327962,
           0.01710875675970051,
           0.017116134324335577,
           0.017193583296607088,
           0.0172605314803626,
           0.01728651777654214,
           0.017326478527635934,
           0.017318777285967674,
           0.017348990170670715,
           0.017331626423169565,
           0.017242485192126036,
           0.017204381327541405,
           0.017171213022784588,
           0.017148975548483596,
           0.017230069169606222,
           0.017200853263866153,
           0.017153064756518906,
           0.017255196577326184,
           0.017212376927325475,
           0.017152578249105238,
           0.017245572405417756,
           0.017286284225179204,
           0.017306541493034937,
           0.017302698164254268,
           0.017312835070079583,
           0.017314965547133992,
           0.017344222273397488,
           0.01735800409260928,
           0.01740321076733627]}
```
",even issue trying hierarchical intent slot filling example enough unpack got suggestion use additional condition avoid error however output different,issue,negative,neutral,neutral,neutral,neutral,neutral
480736064,"Hi all,
it seems that I have the same question as @habout632.

- When I train the model:
```bash
pytext train < textcnn_with_common_crawl_w2v.json
```

It takes long time loading embedding (the file is 5.3G, so it should take long time), and prints:
```
load pretrained embeddings from word_embedding/glove.840B.300d.txt
```

- but when I test the model:
```bash
pytext test < textcnn_with_common_crawl_w2v.json
```
It's super fast and doesn't print load embedding message

- I also try to load model and config in code as following:
```python
from pytext import load_config, create_predictor
predictor =  create_predictor(load_config(CONFIG_PATH), MODEL_PATH)
```
It also loads very fast.

- In the predictor function, there is no use of the embedding path in word_feat config, featurizer.featurize just do tokenization:
```python
def _predict(workspace_id, feature_config, predict_net, featurizer, input):
    workspace.SwitchWorkspace(workspace_id)
    features = featurizer.featurize(InputRecord(**input))
    if feature_config.word_feat:
        for blob_name in feature_config.word_feat.export_input_names:
            converted_blob_name = convert_caffe2_blob_name(blob_name)
            workspace.blobs[converted_blob_name] = np.array(
                [features.tokens], dtype=str
            )
        workspace.blobs[""tokens_lens""] = np.array([len(features.tokens)], dtype=np.int_)
```

- I check that my model file is 21MB, it's not possible to include the large embedding in it.

what confuses me is that the model still works, output good results...",hi question train model bash train long time loading file take long time load test model bash test super fast print load message also try load model code following python import predictor also fast predictor function use path python input input check model file possible include large model still work output good,issue,positive,positive,positive,positive,positive,positive
479410157,"thanks @jingfeidu. the error removed but when I run predict commend like this : 
`pytext --config-file lmlstm.2.json  predict --exported-model lmlstm.2.c2 <<< '{""raw_text"": ""the""}'`
the output is : 
`Install apex from https://github.com/NVIDIA/apex/.
Loading model from lmlstm.2.c2
Model loaded, reading example JSON from stdin
{}`
the output is empty. I need to generate text but it dosen't give any information for do that. 
How can I generate a random text by language model trained by pytext ? ",thanks error removed run predict commend like predict output install apex loading model model loaded reading example output empty need generate text dose give information generate random text language model trained,issue,positive,negative,negative,negative,negative,negative
478780672,"Hi @abb4s, could you add labels in your config and try to export it again? For example,
`
{
""task"": {
""LMTask"": {
""features"": {
""shared_module_key"": ""SHARED_EMBEDDING"",
""word_feat"": {
""vocab_file"": ""vocab.txt"",
""vocab_size"": 2715,
""vocab_from_train_data"": false
}
},

**""labels"": {},**

        ""data_handler"": {
          ""LanguageModelDataHandler"": {
            ""train_path"": ""shekspeare2.txt"",
            ""eval_path"": ""shekspeare2.txt"",
            ""test_path"": ""shekspeare2.txt"",
            ""train_batch_size"": 16,
            ""eval_batch_size"": 16,
            ""test_batch_size"": 16
          }
        },
        ""exporter"":{},
        ""trainer"": {
          ""epochs"": 1
        },
        ""model"": {
          ""representation"": {
            ""shared_module_key"": ""SHARED_LSTM""
          },
          ""decoder"": {
            ""hidden_dims"": [
              10
            ]
          }
        }
        
      }
}
}`
",hi could add try export example task false exporter trainer model representation,issue,negative,negative,negative,negative,negative,negative
477977731,it is 10 day that you've not answered to me(is Facebook  in Holiday !?) please some one correct me!,day holiday please one correct,issue,positive,neutral,neutral,neutral,neutral,neutral
477705883,"I adding wheel for python 3.7 as well. Hopefully, that may solve the problem.",wheel python well hopefully may solve problem,issue,negative,neutral,neutral,neutral,neutral,neutral
470709707,"This PR supports the scenario where the featurizer implementation you use is capable of generating a vector of floats for the sentence passed to the it. `SimpleFeaturizer` in PyText doesn't support it. You have to implement your own that can run a model or call a service or look up from a file to get the token embeddings for a given sentence ,e.g., ELMo.

Configuring pretrained embeddings in your feature_config is supported. See https://github.com/facebookresearch/pytext/blob/master/pytext/config/field_config.py#L78.

The field definition of PretrainedEmbeddingField to hold the embeddings is here: https://github.com/facebookresearch/pytext/blob/master/pytext/fields/pretrained_model_embedding_field.py#L12.

Given your feature_config definition the featurizer will be instantiated. Then the call to `featurizer.featurize()` should return the embeddings in addition to other data structures. This happens here: https://github.com/facebookresearch/pytext/blob/master/pytext/fields/pretrained_model_embedding_field.py#L12

Let me know if this clears how to use pretrained token embeddings that can be augmented with trainable word embeddings.",scenario implementation use capable generating vector sentence support implement run model call service look file get token given sentence see field definition hold given definition call return addition data let know use token augmented trainable word,issue,positive,positive,positive,positive,positive,positive
469023695,"A similar issue came up with the `DocClassificationTask.Config` @ https://pytext-pytext.readthedocs-hosted.com/en/latest/configs/pytext.task.tasks.DocClassificationTask.Config.html

In the newest version of pytext it seems the default values for the trainer parameter needs to remove the random seed parameter and change from:
```
    ""trainer"": {
        ""random_seed"": 0,
        ""epochs"": 10,
        ""early_stop_after"": 0,
        ""max_clip_norm"": null,
        ""report_train_metrics"": true
    },
```
to
```
    ""trainer"": {
        ""epochs"": 10,
        ""early_stop_after"": 0,
        ""max_clip_norm"": null,
        ""report_train_metrics"": true
    },
```",similar issue came version default trainer parameter need remove random seed parameter change trainer null true trainer null true,issue,positive,positive,neutral,neutral,positive,positive
466559484,"@swaroopgrs Yeah, we are working on mixed precision training using Apex.amp and Apex.fp16_utils.",yeah working mixed precision training,issue,negative,neutral,neutral,neutral,neutral,neutral
466074540,Just read the paper and there is a plan to add FP16 support. If you think mixed-precision is covered there you can close the issue.,read paper plan add support think covered close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
464716021,"if you perform the same test with 2 example entries, you have the same issue :(

returning the Iterator is an issue in itself",perform test example issue issue,issue,negative,neutral,neutral,neutral,neutral,neutral
464165606,"@ahhegazy Another quick question, does multilingual modeling support is still on the radar? 
Also really interested regarding fairseq timing question. 

And thanks for sharing this codebase with the community!",another quick question multilingual modeling support still radar also really interested regarding timing question thanks community,issue,positive,positive,positive,positive,positive,positive
463151157,"On macosx 10.13, python 3.6.5, pytext-nlp 0.1.2, tensorboardX 1.6
with or without config.use_tensorboard boolean flag positioned in configuration
no ""./runs"" directory created, nor tensorboard events files created

",python without flag positioned configuration directory,issue,negative,neutral,neutral,neutral,neutral,neutral
462777162,Is there a specific question you are asking? Can you fill in the description?,specific question fill description,issue,negative,neutral,neutral,neutral,neutral,neutral
462627401,"Yeah, I am trying to use torch_nightly package in CI",yeah trying use package,issue,negative,neutral,neutral,neutral,neutral,neutral
462463623,"We'll track the code fix in the referenced issue.
Thanks for bringing this up. ",track code fix issue thanks,issue,negative,positive,positive,positive,positive,positive
462462074,"I looked into the code. Looks like we are reading the config off of the model file, so don't really need it if the ""--model"" is specified. 

I'll file a task for this. We welcome your contribution too. :) In the meantime, just pass any config to unblock yourself. ",code like reading model file really need model file task welcome contribution pas unblock,issue,positive,positive,positive,positive,positive,positive
462237069,"From the doc : https://pytext-pytext.readthedocs-hosted.com/en/latest/execute_your_first_model.html#export-the-model.

*You can also pass in a configuration **to infer some of these options.** In this case lets do that because depending on how youre following along your snapshot might be in different places!*

Instead of passing the model through config, I am passing it through options given. What's the difference?
Does it need more than the pytorch model and output model name ? If yes, then why isn't mentioned?",doc also pas configuration infer case let depending following along snapshot might different instead passing model passing given difference need model output model name yes,issue,negative,neutral,neutral,neutral,neutral,neutral
462165464,"It's waiting for the model config (which is required, besides the pytorch model) from the stdin.

You can redirect your config like it's done in https://pytext-pytext.readthedocs-hosted.com/en/latest/execute_your_first_model.html#export-the-model.",waiting model besides model redirect like done,issue,negative,neutral,neutral,neutral,neutral,neutral
461926460,Feel free to reopen the issue if you still face it. Thanks.,feel free reopen issue still face thanks,issue,positive,positive,positive,positive,positive,positive
461659304,Thank you @xtagon for figuring out the issue. This issue should go away once you update to the latest version v0.1.4,thank issue issue go away update latest version,issue,negative,positive,positive,positive,positive,positive
461658239,"Hey @Vivicai1005 

The RNNG model currently doesn't support ONNX export because of its dynamic graph. As an alternative, we plan to make PyTorch 1.0 JIT support available in time.",hey model currently support export dynamic graph alternative plan make support available time,issue,positive,positive,positive,positive,positive,positive
461159353,"Hello @ahhegazy, and thanks for sharing this amazing work, just a quick question, can we have an idea about timing regarding fairseq integration ? Thanks a lot !",hello thanks amazing work quick question idea timing regarding integration thanks lot,issue,positive,positive,positive,positive,positive,positive
460890148,"It's my fault, I force the LM task to have char_feat which I found out later that it's not available (correct me if I'm wrong)
I employed char_feat to single Doc classification task and it works smoothly. By the way, when it comes to multitask (multiple doc classification tasks) I got this error
`RuntimeError: CUDA error: device-side assert triggered`
quickly guess I just have had my own resource allocation issue.
Thanks and sorry if this issue opening disturb you.",fault force task found later available correct wrong employed single doc classification task work smoothly way come multiple doc classification got error error assert triggered quickly guess resource allocation issue thanks sorry issue opening disturb,issue,negative,positive,neutral,neutral,positive,positive
460675727,"FYI
I also tried the most recent commit
`commit 70c1b4361eed09b11c32a8c4a1def8807e573351
Author: Barlas Oguz <barlaso@fb.com>
Date:   Mon Feb 4 11:01:33 2019 -0800

    export target model to default path

    Summary: Pull Request resolved: https://github.com/facebookresearch/pytext/pull/264

    Differential Revision: D13933847

    fbshipit-source-id: 40a0c576d729d6c2ce1b6c7028ecaabcc42f19d7`

I found this error instead
`Traceback (most recent call last):
  File ""/home/jod/pytext/pytext_venv/bin/pytext"", line 11, in <module>
    load_entry_point('pytext-nlp', 'console_scripts', 'pytext')()
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/home/jod/pytext/pytext_venv/lib/python3.7/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/home/jod/pytext/pytext/pytext/main.py"", line 272, in train
    train_model(config, summary_writer=summary_writer)
  File ""/home/jod/pytext/pytext/pytext/workflow.py"", line 69, in train_model
    config, dist_init_url, device_id, rank, world_size, summary_writer, metadata
  File ""/home/jod/pytext/pytext/pytext/workflow.py"", line 97, in prepare_task
    task = create_task(config.task, metadata=metadata)
  File ""/home/jod/pytext/pytext/pytext/task/task.py"", line 38, in create_task
    return create_component(ComponentType.TASK, task_config, metadata, model_state)
  File ""/home/jod/pytext/pytext/pytext/config/component.py"", line 144, in create_component
    return cls.from_config(config, *args, **kwargs)
  File ""/home/jod/pytext/pytext/pytext/task/disjoint_multitask.py"", line 93, in from_config
    for name, task in task_config.tasks.items()
  File ""/home/jod/pytext/pytext/pytext/task/disjoint_multitask.py"", line 93, in <genexpr>
    for name, task in task_config.tasks.items()
File ""/home/jod/pytext/pytext/pytext/config/component.py"", line 164, in create_model
    return create_component(ComponentType.MODEL, model_config, *args, **kwargs)
  File ""/home/jod/pytext/pytext/pytext/config/component.py"", line 144, in create_component
    return cls.from_config(config, *args, **kwargs)
  File ""/home/jod/pytext/pytext/pytext/models/language_models/lmlstm.py"", line 79, in from_config
    model = super().from_config(model_config, feat_config, metadata)
  File ""/home/jod/pytext/pytext/pytext/models/model.py"", line 161, in from_config
    feat_config, create_fn=cls.create_embedding, metadata=metadata
  File ""/home/jod/pytext/pytext/pytext/models/module.py"", line 44, in create_module
    create_fn(module_config, *args, **kwargs),
  File ""/home/jod/pytext/pytext/pytext/models/model.py"", line 151, in create_embedding
    sub_emb_module_dict = cls.create_sub_embs(feat_config, metadata)
  File ""/home/jod/pytext/pytext/pytext/models/model.py"", line 123, in create_sub_embs
    config, metadata=meta
KeyError: 'char_feat'`

again I think the main error should be `KeyError: 'char_feat'`.

Any idea?",also tried recent commit commit author date mon export target model default path summary pull request resolved differential revision found error instead recent call last file line module file line return file line main file line invoke return file line invoke return file line invoke return file line return file line train file line rank file line task file line return file line return file line name task file line name task file line return file line return file line model super file line file line file line file line think main error idea,issue,positive,negative,neutral,neutral,negative,negative
460509526,"This issue is resolved on making the following change in the source code:

In bilstm_slot_attn.py changing  self.representation_dim: int = self.lstm.representation_dim to  self.representation_dim: int = word_representation_dim  resolved this issue.

The decoder dimension is set from representation_dim whose dimension is changed if slot attention sub-layer is included in the representation layer. 

Please correct if anything is wrong in changing the above code.
Thanks",issue resolved making following change source code resolved issue dimension set whose dimension slot attention included representation layer please correct anything wrong code thanks,issue,positive,negative,neutral,neutral,negative,negative
460147467,"Any resolution for this incidence? even we are facing the same issue.
Our config data is:
""model"": {
          ""representation"": {
            ""BiLSTMSlotAttention"": {
              ""lstm"": {
                ""dropout"": 0,
                ""lstm_dim"": 366,
                ""num_layers"": 2,
                ""bidirectional"": true
              },
              ""slot_attention"":{
                ""attention_type"": ""dot"",
                ""attn_dimension"": 133
              }
            }
          },
          ""output_layer"": {
            ""WordTaggingOutputLayer"": {
              
            }
          }

The error is:
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/pytext/models/decoders/mlp_decoder.py"", line 54, in forward
    return self.mlp(torch.cat(input, 1))
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/torch/nn/modules/container.py"", line 92, in forward
    input = module(input)
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/torch/nn/modules/module.py"", line 489, in __call__
    result = self.forward(*input, **kwargs)
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/torch/nn/modules/linear.py"", line 67, in forward
    return F.linear(input, self.weight, self.bias)
  File ""/datadrive/ner_poc/anaconda/lib/python3.6/site-packages/torch/nn/functional.py"", line 1354, in linear
    output = input.matmul(weight.t())
RuntimeError: size mismatch, m1: [420 x 1464], m2: [732 x 6] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:940

Any assistance would be appreciated.
Thanks",resolution incidence even facing issue data model representation dropout bidirectional true dot error file line forward return input file line result input file line forward input module input file line result input file line forward return input file line linear output size mismatch assistance would thanks,issue,negative,positive,positive,positive,positive,positive
460026342,I'm also getting `AttributeError: 'NoneType' object has no attribute 'export_output_names'`,also getting object attribute,issue,negative,neutral,neutral,neutral,neutral,neutral
459856044,"Yes, we are working on integrating with translate: https://github.com/pytorch/translate/tree/master/pytorch_translate which is Fairseq models with production support",yes working translate production support,issue,positive,neutral,neutral,neutral,neutral,neutral
459844003,"> Thanks for the answer @ahhegazy! Does integration with fairseq fall under the 1st area?

I am curious about the fairest integration as well. Any plans on that?",thanks answer integration fall st area curious integration well,issue,positive,positive,neutral,neutral,positive,positive
459749365,"Thanks for the answer @ahhegazy! Does integration with fairseq fall under the 1st area?
",thanks answer integration fall st area,issue,negative,positive,positive,positive,positive,positive
459477344,"Neat, this is all good to know, thanks Ahmed.

I'm looking forward to seeing more model and task examples, particularly advanced ones.

Also that release blog post link appears to be broken for me, or at least inaccessible. I'm assuming it's the same content as this one? https://code.fb.com/ai-research/pytext-open-source-nlp-framework/.",neat good know thanks looking forward seeing model task particularly advanced also release post link broken least inaccessible assuming content one,issue,positive,positive,positive,positive,positive,positive
459470067,"Great questions :)

As for the road map, we are mainly working on 5 main areas (roughly sorted according to priority):

- Enriching and Improving our stack with more tasks, models and techniques
- Improve Usability and have better integration with ipython notebooks
- Performance optimizations for training and inference
- Explore model interpretability techniques

For the release cadence, we will do our best to have a release every month. And yeah as we mentioned in our release blog post [here](https://code.fb.com/ai-research/pytext-open-source-nlp-framework/) this project powers some of the core projects  in production at Facebook and it indeed has a substantial backing from different teams in Facebook AI.
",great road map mainly working main roughly sorted according priority enriching improving stack improve usability better integration performance training inference explore model interpretability release cadence best release every month yeah release post project core production indeed substantial backing different ai,issue,positive,positive,positive,positive,positive,positive
459209698,"uninstalling onnx 1.4.1:
pip uninstall onnx
then doing a manual install of 1.3.1:
pip install onnx==1.3.0
allows to export docnn to onnx but with 2 warnings : 
Saving onnx model to: /tmp/model.onnx
repos/pytext/pytext_venv/lib64/python3.6/site-packages/torch/nn/modules/module.py:487: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = self._slow_forward(*input, **kwargs)
repos/pytext/pytext_venv/lib64/python3.6/site-packages/pytext/models/representations/pooling.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  alphas = self.softmax(alphas)  # (bsz, sent_len)

So there are actually 3 issues:
1- pip install pytext-nlp seems to install onnx 1.4.1 despite the 1.3.1 requirements (I guess a pip bug)
2- onnx export with onnx 1.4.1 uses the unsupported ConstantFill op (deprecated op ?). Anyway pytext does nt seem to support onnx 1.4.1 yet. Could be worth it in pytext to just assert that onnx version = 1.3.0
3- these warnings about softmax implicit dim choice (pooling.py) : 'just' deprecation.

Up to you to close that ticket.
",pip manual install pip install export saving model implicit dimension choice change call include argument result input implicit dimension choice change call include argument actually pip install install despite guess pip bug export unsupported anyway seem support yet could worth assert version implicit dim choice deprecation close ticket,issue,negative,positive,positive,positive,positive,positive
458265134,"Hum, requirements.txt indeed requests onnx 1.3 : 
onnx==1.3.0
but 
pip install pytext-nlp
installed 1.4.1.
Tks.",hum indeed pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
458256319,There is an open issue with the example code seg faulting from an incompatibility with onnx 1.4.1 and the solution was to downgrade to 1.3.0. I think the requirements.txt has onnx 1.3.0 now as well. But that seems to cause this problem.,open issue example code seg faulting incompatibility solution downgrade think well cause problem,issue,negative,neutral,neutral,neutral,neutral,neutral
458233835,"Tks. Well, looks like 
pip install pytext-nlp
did install the latest onnx, meaning 1.4.1 : 
$ more pytext_venv/lib64/python3.6/site-packages/onnx/version.py 
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
from __future__ import unicode_literals
version = '1.4.1'
git_version = '9e55ace55aad1ada27516038dfbdc66a8a0763db'

Did nt pytorch/pytext use it when exporting ? ",well like pip install install latest meaning import import division import import version use,issue,positive,positive,positive,positive,positive,positive
458210542,A work around for now is just to upgrade onnx to 1.4.1 while you are exporting.  I would guess if we fix the compatibility issues with the latest version of onnx and pytext then this issue would also be fixed as a result.,work around upgrade would guess fix compatibility latest version issue would also fixed result,issue,negative,positive,positive,positive,positive,positive
458020490,"The generated onnx file is openable through netron 
https://lutzroeder.github.io/netron/
and I indeed see 2 ContantFill ops, just after a Concat op and just before the LSTM cell.
Node prop:
input_as_shape : 1
inputs : id 75
outputs : id 76",file openable indeed see cell node prop id id,issue,negative,neutral,neutral,neutral,neutral,neutral
457725396,"No information in the description, that's bad. Likely duplicate of #240.",information description bad likely duplicate,issue,negative,negative,negative,negative,negative,negative
457562506,"My system had onnx 1.4.1. Once I pinned it to 1.3.0, it still segfaults with python 3.7.2; it completes successfully with python 3.6.8. ",system pinned still python successfully python,issue,negative,positive,positive,positive,positive,positive
457404377,"We recently removed `ConstantFill` from onnx operator set, since it's an experimental operator. We also stop emitting `ConstantFill` in PyTorch as well. However, the release version is not out yet. So short term solution is install the previous release version of ONNX. I am wondering how the onnx is installed on your system. if you use pip, could you try `pip install onnx==1.3.0` and check whether you still have the problem or not.",recently removed operator set since experimental operator also stop well however release version yet short term solution install previous release version wondering system use pip could try pip install check whether still problem,issue,negative,negative,neutral,neutral,negative,negative
457356612,"FWIW:

```sh
  ~ cat /etc/redhat-release
CentOS Linux release 7.5.1804 (Core)
```",sh cat release core,issue,negative,neutral,neutral,neutral,neutral,neutral
457345966,Thanks for the suggestion. I'll add that to the list of future improvements.,thanks suggestion add list future,issue,negative,positive,neutral,neutral,positive,positive
457334827,"Downgrading to Python 3.6 yields:

```sh
(pytext)   pytext git:(master)  pytext export --output-path exported_model.c2 < demo/configs/docnn.json
No config file specified, reading from stdin
Exporting /tmp/model.pt to caffe2 file: exported_model.c2 and onnx file: /tmp/model.onnx
Loading model from /tmp/model.pt...
Task parameters:

{'data_handler': {'columns_to_read': ['doc_label', 'text', 'dict_feat'],
                  'eval_batch_size': 128,
                  'eval_path': 'tests/data/test_data_tiny.tsv',
                  'max_seq_len': -1,
                  'shuffle': True,
                  'sort_within_batch': True,
                  'test_batch_size': 128,
                  'test_path': 'tests/data/test_data_tiny.tsv',
                  'train_batch_size': 128,
                  'train_path': 'tests/data/train_data_tiny.tsv'},
 'exporter': None,
 'features': {'char_feat': None,
              'dense_feat': None,
              'dict_feat': None,
              'freeze': False,
              'load_path': None,
              'pretrained_model_embedding': None,
              'save_path': None,
              'shared_module_key': None,
              'word_feat': {'embed_dim': 100,
                            'embedding_init_range': None,
                            'embedding_init_strategy': 'random',
                            'export_input_names': ['tokens_vals'],
                            'freeze': False,
                            'lowercase_tokens': True,
                            'min_freq': 1,
                            'mlp_layer_dims': [],
                            'pretrained_embeddings_path': '',
                            'vocab_file': '',
                            'vocab_from_all_data': False,
                            'vocab_from_pretrained_embeddings': False,
                            'vocab_from_train_data': True,
                            'vocab_size': 0}},
 'featurizer': {'SimpleFeaturizer': {'convert_to_bytes': False,
                                     'lowercase_tokens': True,
                                     'sentence_markers': None,
                                     'split_regex': '\\s+'}},
 'labels': {'export_output_names': ['doc_scores'],
            'label_weights': {},
            'target_prob': False},
 'metric_reporter': {'output_path': '/tmp/test_out.txt'},
 'model': {'decoder': {'freeze': False,
                       'hidden_dims': [],
                       'load_path': None,
                       'save_path': None,
                       'shared_module_key': None},
           'output_layer': {'freeze': False,
                            'load_path': None,
                            'loss': {'CrossEntropyLoss': {}},
                            'save_path': None,
                            'shared_module_key': None},
           'representation': {'BiLSTMDocAttention': {'dropout': 0.4,
                                                     'freeze': False,
                                                     'load_path': None,
                                                     'lstm': {'bidirectional': True,
                                                              'dropout': 0.4,
                                                              'freeze': False,
                                                              'load_path': None,
                                                              'lstm_dim': 32,
                                                              'num_layers': 1,
                                                              'save_path': None,
                                                              'shared_module_key': None},
                                                     'mlp_decoder': None,
                                                     'pooling': {'SelfAttention': {'attn_dimension': 64,
                                                                                   'dropout': 0.4}},
                                                     'save_path': None,
                                                     'shared_module_key': None}}},
 'optimizer': {'lr': 0.001,
               'momentum': 0.0,
               'type': 'adam',
               'weight_decay': 1e-05},
 'scheduler': {'T_max': 1000,
               'cooldown': 0,
               'cut_frac': 0.1,
               'eta_min': 0.0,
               'gamma': 0.1,
               'lm_gradual_unfreezing': True,
               'lm_lr_multiplier': 1.0,
               'lm_use_per_layer_lr': False,
               'non_pretrained_param_groups': 2,
               'patience': 5,
               'ratio': 32,
               'step_size': 30,
               'threshold': 0.0001,
               'threshold_is_absolute': False,
               'type': 'none'},
 'trainer': {'early_stop_after': 0,
             'epochs': 10,
             'max_clip_norm': None,
             'random_seed': 0,
             'report_train_metrics': True}}
creating field TextFeatureField
creating field DocLabelField

Loading data...
load_path is not a config of embedding, skipping
save_path is not a config of embedding, skipping
freeze is not a config of embedding, skipping
shared_module_key is not a config of embedding, skipping
dict_feat is not a config of embedding, skipping
char_feat is not a config of embedding, skipping
pretrained_model_embedding is not a config of embedding, skipping
dense_feat is not a config of embedding, skipping
Model has 2 param groups (1 from embedding module) for optimizer
Saving caffe2 model to: exported_model.c2
Saving onnx model to: /tmp/model.onnx
/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/torch/nn/modules/module.py:487: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = self._slow_forward(*input, **kwargs)
Traceback (most recent call last):
  File ""/data/erippeth/anaconda3/envs/pytext/bin/pytext"", line 11, in <module>
    sys.exit(main())
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/core.py"", line 764, in __call__
    return self.main(*args, **kwargs)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/core.py"", line 717, in main
    rv = self.invoke(ctx)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/core.py"", line 1137, in invoke
    return _process_result(sub_ctx.command.invoke(sub_ctx))
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/core.py"", line 956, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/core.py"", line 555, in invoke
    return callback(*args, **kwargs)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/click/decorators.py"", line 17, in new_func
    return f(get_current_context(), *args, **kwargs)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/pytext/main.py"", line 303, in export
    export_saved_model_to_caffe2(model, output_path, output_onnx_path)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/pytext/workflow.py"", line 130, in export_saved_model_to_caffe2
    task.export(task.model, export_caffe2_path, export_onnx_path=output_onnx_path)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/pytext/task/task.py"", line 189, in export
    self.exporter.export_to_caffe2(model, export_path, export_onnx_path)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/pytext/exporters/exporter.py"", line 192, in export_to_caffe2
    export_onnx_path,
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/pytext/utils/onnx_utils.py"", line 50, in pytorch_to_caffe2
    onnx.checker.check_model(onnx_model)
  File ""/data/erippeth/anaconda3/envs/pytext/lib/python3.6/site-packages/onnx/checker.py"", line 86, in check_model
    C.check_model(model.SerializeToString())
onnx.onnx_cpp2py_export.checker.ValidationError: No Op or Function registered for ConstantFill with domain_version of 9

==> Context: Bad node spec: input: ""75"" output: ""76"" op_type: ""ConstantFill"" attribute { name: ""input_as_shape"" i: 1 type: INT }
```",python sh git master export file reading file file loading model task true true none none none none false none none none none none false true false false true false true none false false none none none false none none none false none true false none none none none none none true false false none true field field loading data skipping skipping freeze skipping skipping skipping skipping skipping skipping model param module saving model saving model implicit dimension choice change call include argument result input recent call last file line module main file line return file line main file line invoke return file line invoke return file line invoke return file line return file line export model file line file line export model file line file line file line function registered context bad node spec input output attribute name type,issue,positive,negative,neutral,neutral,negative,negative
457071956,"> Hi @Vivicai1005, RNNG model is trained by Hogwild trainer which only runs on CPU now

Thank you for answering! 

By the way, does the exporter attribute work in the RNNG model? When I tried to set exporter in the demo/configs/rnng.json,  it showed ""AttributeError: 'NoneType' object has no attribute 'export_output_names'"" .  And How can I export the trained PyTorch model to Caffe2 model?
",hi model trained trainer thank way exporter attribute work model tried set exporter object attribute export trained model model,issue,negative,neutral,neutral,neutral,neutral,neutral
457056992,"Hi @Vivicai1005, RNNG model is trained by Hogwild trainer which only runs on CPU now ",hi model trained trainer,issue,negative,neutral,neutral,neutral,neutral,neutral
456924190,"That's correct, by default the metric reporter emits to all relevant channels. Please elaborate: what's your use case and an example of what you would like to do?",correct default metric reporter relevant please elaborate use case example would like,issue,positive,positive,positive,positive,positive,positive
456791494,Please tell me if there is any further information you might need to reproduce the bug. ,please tell information might need reproduce bug,issue,negative,neutral,neutral,neutral,neutral,neutral
456739266,"> The `prediction` values are indices of the predicted actions. Given the actions vocabulary the ids should be converted to actual actions and given the action the tree structure is derived naturally. We will add the translation from id to string to make the predicted actions readable. Will that help?

Hi~ Have you added this translation function?",prediction index given vocabulary converted actual given action tree structure derived naturally add translation id string make readable help added translation function,issue,negative,positive,neutral,neutral,positive,positive
456608555,"Thank you for the feedback, we will take a look at it",thank feedback take look,issue,negative,neutral,neutral,neutral,neutral,neutral
455754313,"this is unfortunate in that it means if we cancel training mid-way through we don't get a saved model with which to experiment.

could this be an option that's off by default?",unfortunate cancel training get saved model experiment could option default,issue,negative,negative,negative,negative,negative,negative
455417265,"> @yanqiangmiffy check out joint_data_handler_test.py, there is sample data for test, will it help?

thanks",check sample data test help thanks,issue,positive,positive,positive,positive,positive,positive
455389841,New pip wheel pytext-nlp version 0.1.3 is uploaded. It should fix the issue. Please try it and re-open the issue if it does not work.,new pip wheel version fix issue please try issue work,issue,negative,positive,positive,positive,positive,positive
455385523,"The new hypothesis package break the test, we will build a new pip wheel to resolve the hypothesis error.",new hypothesis package break test build new pip wheel resolve hypothesis error,issue,negative,positive,positive,positive,positive,positive
455384801,"@yanqiangmiffy check out joint_data_handler_test.py, there is sample data for test, will it help?",check sample data test help,issue,negative,neutral,neutral,neutral,neutral,neutral
454840979,"Thanks @m3rlin45, I added the correct value of ""columns_to_read"" to the configuration file and the problem went away! By the way @hikushalhere the dataset has a size of 835MB.",thanks added correct value configuration file problem went away way size,issue,negative,positive,positive,positive,positive,positive
454697851,"Could you share your config and pytext version? We only recently added tensorboard support for SemanticParsingTask, so if you installed via pip and are using that task, tensorboard event files will not be generated",could share version recently added support via pip task event,issue,positive,neutral,neutral,neutral,neutral,neutral
454695852,tensorboard doesnt generate runs directory dont know the reason,doesnt generate directory dont know reason,issue,negative,neutral,neutral,neutral,neutral,neutral
454565077,"@hikushalhere what i meant was that _if_ i'm using the attention matrix in that calculation, i can't pass it in via the clone-detach, it can't be done via the context dict, it has to be done more directly.",meant attention matrix calculation ca pas via ca done via context done directly,issue,negative,positive,neutral,neutral,positive,positive
454484439,"@silky Are you referring to the orthogonality constraint that's enforced by the penalty on attention matrix? If so, the penalty tensor should be user created and it shouldn't be a problem with deepcopy.",silky orthogonality constraint enforced penalty attention matrix penalty tensor user problem,issue,negative,neutral,neutral,neutral,neutral,neutral
454479774,"@fsonntag You're right that the DOT mode is only a transformation.

I am closing this issue since there isn't any actionable item here.",right dot mode transformation issue since actionable item,issue,negative,positive,positive,positive,positive,positive
454465247,"Hi there,

Would there be any interest of using a coverage tool such as https://codecov.io to report generated code coverage to the repository? If so, I can help with that.

Best",hi would interest coverage tool report code coverage repository help best,issue,positive,positive,positive,positive,positive,positive
454286372,"I worked with @habout632 on this in the support group, he had set up his data file columns as: [text, class], the default setting for ""columns_to_read"" is [class, text, dict_feat]. Once they set columns_to_read correctly, the problem went away. @dsalfran, can you confirm you don't have the same issue?",worked support group set data file text class default setting class text set correctly problem went away confirm issue,issue,negative,neutral,neutral,neutral,neutral,neutral
454277792,"happens to me 
dataset is 0.036GB=36MB
machine configuration is 32G(10G used) pytext will cost rest 22G when started",machine configuration used cost rest,issue,negative,neutral,neutral,neutral,neutral,neutral
454225331,"@hikushalhere cool okay; that's pretty much what i did

i had also tried to use the context to pass a tensor that i needed to be included in the gradients (when computing the penalty for the attention in the self-attention paper); in that case this approach would fail, right?",cool pretty much also tried use context pas tensor included penalty attention paper case approach would fail right,issue,negative,positive,positive,positive,positive,positive
454224303,"@silky I was able to repro the exception locally. Yes, `context` is the way to pass batch level data for downstream usage (e.g., metric calculation). However, given the limitation that only Tensors created explicitly by the user can be used with deepcopy, one way to do this is to clone the Tensor.
`context[""logits""] = [l.clone().detach() for l in logits]`",silky able exception locally yes context way pas batch level data downstream usage metric calculation however given limitation explicitly user used one way clone tensor context,issue,negative,positive,positive,positive,positive,positive
454187957,@dsalfran Thanks for reporting the issue. Can you give an estimate of how big your dataset is (in GB) and if you expect it to fit in memory given your machine configuration?,thanks issue give estimate big expect fit memory given machine configuration,issue,positive,positive,positive,positive,positive,positive
453211385,"Should now be handled by https://github.com/facebookresearch/pytext/pull/197.
Add an export config to any sub-task to export the model.",handled add export export model,issue,negative,neutral,neutral,neutral,neutral,neutral
452942781,"@borguz  thanks once again for this suggested work around.

I followed your suggestion saving modules and load them (via each `load_path`) in single-task training. The issue for now may not about transferring weights but about freezing the weights while executing training single-task. I tried setting `freeze = true` for all layers (modules) however, the single-task log shows that loss values or reported matrices had been updated. 

Anyway, even if I employed single-task mode (i.e., DocClassificationTask), unfortunately caffe2 export function still unworkable (only .pt came out as output). Then I tried to export by,
`$pytext export --output-path <my_caffe2> < <my_single_task_config>`
and it works :)",thanks work around suggestion saving load via training issue may transferring freezing training tried setting freeze true however log loss matrix anyway even employed mode unfortunately export function still unworkable came output tried export export work,issue,negative,positive,neutral,neutral,positive,positive
452885282,"@hikushalhere,  thanks for your comment.
I did look at PairClassification, it solves a different problem. For PairClassification, the inputs are {label, doc1, doc2}. The goal is to classify the two documents as a 'pair'. Eg: the documents ""agree"", or ""contradict"", or neither agree nor contradict.

For SearchPairwiseRanking, the input is three documents: {query, response1, response2}. We want to find which response is better for the query in question. In this case (unlike PairClassification), the query is a document and needs to go through an embedding-->representation(LSTM, eg)-->MLP layers. 

Fundamentally, PairClassification is solving a classification problem, computing something like similarity(doc1, doc2) and mapping that to a label (eg., via a Softmax layer)
SearchPairwiseRanking is solving a relevance ranking problem, computing s1=similarity(query, doc1); s2=similarity(query, doc2), and then using f(s1, s2) as a loss function.

Hope this makes sense!",thanks comment look different problem label doc doc goal two agree contradict neither agree contradict input three query response response want find response better query question case unlike query document need go representation fundamentally classification problem something like similarity doc doc label via layer relevance problem query doc query doc loss function hope sense,issue,negative,positive,positive,positive,positive,positive
452878915,@kmalik22 Thanks a lot for the PR. It looks like you want a two-tower model that can represent a query and a document and learns by using pairwise ranking loss. Have you checked out `PairClassificationTask`: https://github.com/facebookresearch/pytext/blob/master/pytext/task/tasks.py#L162? I wonder if you can just add a new output layer that implements the pairwise loss (that you have provided a link to) and then add it to `PairClassificationTask` as an option for its `output_layer`. Pls let me know what you think.,thanks lot like want model represent query document pairwise loss checked wonder add new output layer pairwise loss provided link add option let know think,issue,negative,positive,positive,positive,positive,positive
452790393,"You can set modules_save_dir in the PyText config, then can set save_path for each module you want to save.  save_path is relative to modules_save_dir.",set set module want save relative,issue,positive,neutral,neutral,neutral,neutral,neutral
452735137,"Thanks @Titousensei !

Having hacked a solution to #1 I then came across #2 and have been struggling to find a solution!  Thanks for your work in addressing this.

And of course, thank you for your being so open to the community's troubles and requests.  It's much appreciated.  I look forward to building my own custom `Task`s with ease in the future.  ",thanks hacked solution came across struggling find solution thanks work course thank open community much look forward building custom task ease future,issue,positive,positive,positive,positive,positive,positive
452543768,"Thanks both of you @jayrodge @borguz, by the way how to save individual modules? I tried put save_snapshot_path under tasks{...} but, unfortunately it would work that way. I think I might did something wrong.",thanks way save individual tried put unfortunately would work way think might something wrong,issue,negative,negative,negative,negative,negative,negative
452510743,"Thanks for reporting the issue and providing a fix. Yes, there's 2 issues here:
1) we only look at the module ""pytext"". That's a mistaked.
2) the new tasks are not registered in pytext main.
I'll submit a fix soon.",thanks issue providing fix yes look module new registered main submit fix soon,issue,positive,positive,positive,positive,positive,positive
452497078,@hikushalhere unfortunately not; i don't have time to work on this one at the moment; but we are working on the #173 ; i just have to see if i can do a PR of it once i'm done.,unfortunately time work one moment working see done,issue,negative,negative,negative,negative,negative,negative
452460014,@silky Do you have plans to send a PR for decoupling dummy input from exporter?,silky send dummy input exporter,issue,negative,neutral,neutral,neutral,neutral,neutral
452430978,@silky We had implemented single hop attention to get a simpler version working. Please feel free to send us a PR to add multi-hop attention.,silky single hop attention get simpler version working please feel free send u add attention,issue,positive,positive,positive,positive,positive,positive
452429771,Closing this since the task being #187.  Feel free to reopen if the issue is being closed in error.,since task feel free reopen issue closed error,issue,negative,positive,positive,positive,positive,positive
452426841,@ZhuoranLyu Please feel free to send us a PR for this.,please feel free send u,issue,positive,positive,positive,positive,positive,positive
452423791,"Currently caffe2 export for multitask is not implemented.  I'll take this up this week.
For now, it's possible to save individual modules and load them into a single-task pytext run to export the model as workaround.",currently export take week possible save individual load run export model,issue,negative,neutral,neutral,neutral,neutral,neutral
452146950,"The .pt one will be save but not for .c2, anyway how can we use the model by tasks then?",one save anyway use model,issue,negative,neutral,neutral,neutral,neutral,neutral
452093026,"I'm adding following line in the config.json:
`""save_snapshot_path"": ""./model.pt"",
    ""export_caffe2_path"": ""./model.c2""`

But the model is not being saved, any idea?",following line model saved idea,issue,negative,neutral,neutral,neutral,neutral,neutral
451345654,"@seayoung1112 I make a new PR in https://github.com/facebookresearch/pytext/pull/187 , I have made some fixes according to what you have mentioned. However, in onnx_utils.py the parameter 'export_onnx_path' is considered as an optional, so the onnx file will not be created once it's not assigned in an explicit way.",make new made according however parameter considered optional file assigned explicit way,issue,negative,positive,positive,positive,positive,positive
451208860,This is correct. The current implementation doesn't account for multi-hop attention. In effect it doesn't  implement equation (6) in the paper but stops at equation (5). Thanks for pointing it out. We will get back on the fix which requires introducing a new parameter `r` which controls the number of attention hops.,correct current implementation account attention effect implement equation paper equation thanks pointing get back fix new parameter number attention,issue,negative,positive,neutral,neutral,positive,positive
451047941,"> @asp188 feel free to add an additional ""export_onnx_path"" param to config, I can merge your PR once it's updated

I make a PR in https://github.com/facebookresearch/pytext/pull/185 , please check.",asp feel free add additional param merge make please check,issue,positive,positive,positive,positive,positive,positive
451038383,"seq_word_feat is only used for seq models right now (SeqNN and ContextualIntentSlot), the FeatureConfig class is a set of all possible features for all models right now, we'll break it down to let each model having it's own feature definition in future. ",used right class set possible right break let model feature definition future,issue,negative,positive,positive,positive,positive,positive
451026607,"Thank you for your pull request and welcome to our community. We require contributors to sign our Contributor License Agreement, and we don't seem to have you on file. In order for us to review and merge your code, please sign up at https://code.facebook.com/cla. **If you are contributing on behalf of someone else (eg your employer)**, the individual CLA may not be sufficient and your employer may need the corporate CLA signed.

If you have received this in error or have any questions, please contact us at [cla@fb.com](mailto:cla@fb.com?subject=CLA%20for%20facebookresearch%2Fpytext%20%23181). Thanks!",thank pull request welcome community require sign contributor license agreement seem file order u review merge code please sign behalf someone else employer individual may sufficient employer may need corporate received error please contact u thanks,issue,positive,positive,positive,positive,positive,positive
450983292,"@asp188 feel free to add an additional ""export_onnx_path"" param to config, I can merge your PR once it's updated",asp feel free add additional param merge,issue,positive,positive,positive,positive,positive,positive
450787810,"> @bethebunny the exported model file is a C2 model file rather than the original ONNX file, the ONNX file is not supposed to be big since no vocab is in it. We reuse the same ""export_path"" param to store the ONNX file temporarily and override it with the final C2 file. The assumption was ONNX file itself doesn't have much value without the str2id ops prepended. But if users do need the raw ONNX file, it should be done either implicitly in this PR or explicitly by adding a config param like onnx_export_path

You are right, and I hope the maintainer should reconsider my PR and make an additional parameter is preferable.",model file model file rather original file file supposed big since reuse param store file temporarily override final file assumption file much value without strid need raw file done either implicitly explicitly param like right hope maintainer reconsider make additional parameter preferable,issue,positive,positive,positive,positive,positive,positive
450786275,"@bethebunny the exported model file is a C2 model file rather than the original ONNX file, the ONNX file is not supposed to be big since no vocab is in it. We reuse the same ""export_path"" param to store the ONNX file temporarily and override it with the final C2 file. The assumption was ONNX file itself doesn't have much value without the str2id ops prepended. But if users do need the raw ONNX file, it should be done either implicitly in this PR or explicitly by adding a config param like onnx_export_path",model file model file rather original file file supposed big since reuse param store file temporarily override final file assumption file much value without strid need raw file done either implicitly explicitly param like,issue,positive,positive,neutral,neutral,positive,positive
450259314,"Its also not clear from the labels that one corresponds to train and one corresponds to eval, so I think a good solution will make that clear.",also clear one train one think good solution make clear,issue,positive,positive,positive,positive,positive,positive
450258808,"i just set tensorboard to run against the `./runs` folder:

`tensorboard --logdir runs`, then i pick the run-name from the side:

![image](https://user-images.githubusercontent.com/129525/50497972-eaa8a500-0a90-11e9-8e8d-69ab22111986.png)

i think definitely having some kind of naming scheme for the runs definable in the config is nice, but not necessary (i've always liked seeing some of the main hyper-params in the name of the runs)

i'll see how i go about making this change; will give it a shot if i get some time today :)



",set run folder pick side image think definitely kind naming scheme definable nice necessary always seeing main name see go making change give shot get time today,issue,positive,positive,positive,positive,positive,positive
450254643,"So they will go to two event files in two folders?
Currently it's also kind of cumbersome that the user has to manually specify the [`log_dir`](https://tensorboardx.readthedocs.io/en/latest/tensorboard.html#tensorboardX.SummaryWriter.__init__) that is date/time stamped when launching the visualization server, because we use the default constructor of `summary_writer()`. One suggestion I can think of is if we could specify in the config the name of the log_dir, it would make loading multiple event files easier?

If you have an idea on how to do this cleanly, feel free to take a stab at this and send a PR, with a screenshot on how the result looks.",go two event two currently also kind cumbersome user manually specify visualization server use default constructor one suggestion think could specify name would make loading multiple event easier idea cleanly feel free take stab send result,issue,positive,positive,positive,positive,positive,positive
450253847,"We have a work item to decouple the dummy input from exporter, but it's not high priority fo us at the moment. If you want, you can take a stab at it and send a PR.",work item dummy input exporter high priority u moment want take stab send,issue,negative,positive,positive,positive,positive,positive
450219892,"Update:

I was able to get it to work with a slight modification to `pytext.utils.documentation_helper.find_config_class()` (comments include the errors that I encountered):

```
def find_config_class(class_name):
    """"""
        Return the set of PyText classes matching that name.
        Handles fully-qualified `class_name` including module.
    """"""
    module_part = None
    if ""."" in class_name:
        m = class_name.split(""."")
        class_name = m[-1]
        module_part = ""."".join(m[:-1])

    ret = set()
    for mod_name, mod in list(modules.items()):            # avoids this error: RuntimeError: dictionary changed size during iteration
        # if not mod_name.startswith(""pytext""):                # removed to allow looking for custom Tasks
        #     continue
        try:                                                                        # avoids this error: ModuleNotFoundError: No module named '_gdbm'
            getmembers(mod, isclass)
        except:
            continue
        for name, obj in getmembers(mod, isclass):
            if name == class_name:
                if not module_part or obj.__module__ == module_part:
                    ret.add(obj)
    return ret
```

As @elans2 noted, however, this only required *after* I imported the custom `Task`.  I'm not knowledgeable enough to know how to import those custom `Task`s before running `pytext gen-default-config` from the command line however.",update able get work slight modification include return set class matching name none ret set list error dictionary size iteration removed allow looking custom continue try error module except continue name name return ret noted however custom task knowledgeable enough know import custom task running command line however,issue,negative,positive,positive,positive,positive,positive
450206267,"The [ATIS tutorial](https://pytext-pytext.readthedocs-hosted.com/en/latest/atis_tutorial.html#download-pre-trained-word-embeddings) has an example of using the [Stanford GloVe embeddings](https://nlp.stanford.edu/projects/glove/):

The format for these files is space-separated lines, where the first column is the word, and the remaining columns are ascii-encoded float values. For instance, for a 5-dimension embedding,

```
the -0.038194 -0.24487 0.72812 -0.39961 0.083172
of -0.1529 -0.24279 0.89837 0.16996 0.53516
to -0.1897 0.050024 0.19084 -0.049184 -0.089737
```",atis tutorial example glove format first column word float instance,issue,negative,positive,positive,positive,positive,positive
450096832,"This change is to create a copy of the onnx model file that has the suffix "".onnx"", correct? There are a couple of ways to accomplish this now without code changes:

1. Add this copy to your builds between when you call pytext export and when you execute your java code
2. Update your config json file, change export_caffe2_path to change the name to be ""whatever.onnx""
3. Change the path in your Java code to reference the file that you are exporting in your config rather than that file plus the .onnx suffix

I think all of these things are preferable to changing the code to duplicate model files, which can be quite large.",change create copy model file suffix correct couple way accomplish without code add copy call export execute code update file change change name change path code reference file rather file plus suffix think preferable code duplicate model quite large,issue,positive,positive,positive,positive,positive,positive
449834303,"Thanks @snisarg. Updating `torchtext` to the latest version fix the problem.

Will close the issue for now.",thanks latest version fix problem close issue,issue,negative,positive,positive,positive,positive,positive
449822156,"We will have better support and a tutorial on how to use TensorBoardX soon, stay tuned.",better support tutorial use soon stay tuned,issue,positive,positive,positive,positive,positive,positive
449806920,"@snisarg Thanks, I fix this. I upgrade torchtext to the newest version.",thanks fix upgrade version,issue,negative,positive,positive,positive,positive,positive
449806551,@ZhuoranLyu what's the version of torchtext though? Try removing and adding that? ,version though try removing,issue,negative,neutral,neutral,neutral,neutral,neutral
449806296,"@snisarg same error,  using pip install with version pytext-nlp==0.1.2",error pip install version,issue,negative,neutral,neutral,neutral,neutral,neutral
449797775,"@elans2 what error do you see when you comment that part of the code? 
@Titousensei any reason to only allow pytext Tasks only? ",error see comment part code reason allow,issue,negative,neutral,neutral,neutral,neutral,neutral
449788145,"This is the __init__() from torchtext.data.field: https://github.com/pytorch/text/blob/master/torchtext/data/field.py#L156, called from https://github.com/facebookresearch/pytext/blob/master/pytext/fields/field.py#L54

Are you using an older version of torchtext? Can you try updating it please.",older version try please,issue,negative,positive,positive,positive,positive,positive
449695304,"I have the same problem, can't generate config file using user task.

but if comment the code,  also doesn't work.   must import  user class in code.",problem ca generate file user task comment code also work must import user class code,issue,negative,neutral,neutral,neutral,neutral,neutral
449638153,"@snisarg Thanks. I am using Python 3.6.0. The docs says it works from Python 3.6 or above.

I have installed `Python 3.6.8rc1`. Now it works fine. The documentation has to be updated.
",thanks python work python python work fine documentation,issue,positive,positive,positive,positive,positive,positive
449604622,"Alright, thanks a for the explanation!
So do I understand it then correctly that the Dot mode is not really an attention in the sense that we weigh different parts of the input? Because it does not have any trainable weights, but the output is just a transformation of the input",alright thanks explanation understand correctly dot mode really attention sense weigh different input trainable output transformation input,issue,positive,positive,positive,positive,positive,positive
449592292,"This is a class from the standard library, see 
https://docs.python.org/3/library/typing.html#typing.Counter. It was introduced in Python 3.6.1. Can you use a newer version please.",class standard library see python use version please,issue,negative,neutral,neutral,neutral,neutral,neutral
449530492,"thanks for that tip @bethebunny ; using the graph i was able to figure out which tensor it was

![image](https://user-images.githubusercontent.com/129525/50368486-4f6d8680-05dd-11e9-87b2-e590615ffffe.png)

(i picked node 110).

i'm happy with this hack for now, but also happy to be directed on how to add this into the codebase via a pr so that it's much nicer in the future :)",thanks tip graph able figure tensor image picked node happy hack also happy directed add via much future,issue,positive,positive,positive,positive,positive,positive
449524710,"Hey, we've only recently gotten this as a feature request, so we haven't really done anything explicit for making the attention weights exported. You're on the right track with looking in the caffe2 workspace blobs as the quickest way to get this to actually work without making code changes, although I can't give you a great answer about where these will be in the workspace blobs as I haven't had time to look into it yet.

If you do something like this: https://github.com/facebookresearch/pytext/blob/master/pytext/__init__.py#L80
or the similar ""load_from_db"" function in caffe2, you can actually print out the caffe2 graph structure. That will give you a bit of a better idea of which blobs are the outputs of the attention operators, although reading these structures is a bit obtuse.

It's also possible that your model is not learning something particularly explainable from the attention inputs which is why the values are confusing; I'm not a particularly good person to answer whether this is likely to be valuable, but I've heard claims both ways.",hey recently gotten feature request really done anything explicit making attention right track looking way get actually work without making code although ca give great answer time look yet something like similar function actually print graph structure give bit better idea attention although reading bit obtuse also possible model learning something particularly explainable attention particularly good person answer whether likely valuable way,issue,positive,positive,positive,positive,positive,positive
449383518,"Oh, I understand `black pytext` reformat codebase to the current style.
I'm sorry, I close this PR.",oh understand black current style sorry close,issue,negative,negative,negative,negative,negative,negative
449249255,"i've now gotten to the point that i've looked at the shapes of all the numpy arrays in `workspace.blobs`. there's one, with the informative name ""107"" that has the right dimension, but the values don't look right. nothing else has the right size

am i missing something here?

near as i can tell, attention _is_ turned on in my model...",gotten point one informative name right dimension look right nothing else right size missing something near tell attention turned model,issue,negative,positive,positive,positive,positive,positive
449165090,This is very exciting and looks lots of new features will be there with Pytext. Thanks all for the great discussions. I am closing the issue. ,exciting lot new thanks great issue,issue,positive,positive,positive,positive,positive,positive
448799372,"from the last run, you can click on details->artifacts and see the docs here:
https://668-143080897-gh.circle-artifacts.com/0/docs/index.html",last run click see,issue,negative,neutral,neutral,neutral,neutral,neutral
448714467,"SlotAttention doesn't have great documentation, and should probably be split out into a few classes. The best way to understand it for now is to read the code: https://github.com/facebookresearch/pytext/blob/master/pytext/models/representations/slot_attention.py

Essentially there are 3 supported attention modes, Dot, Multiply and Add (called CONCAT in the code).
The core line in building the attention states is https://github.com/facebookresearch/pytext/blob/master/pytext/models/representations/slot_attention.py#L42

All of the attention types construct a weight map, weight the attention state by the weight map, and concatenate it onto the original state.
- For Dot Attention, we simply dot the two states together
- For Multiply, we add in a single linear layer to Dot attention
- For Add, we concatenate the states together and then execute a 2 layer tanh MLP",great documentation probably split class best way understand read code essentially attention dot multiply add code core line building attention attention construct weight map weight attention state weight map concatenate onto original state dot attention simply dot two together multiply add single linear layer dot attention add concatenate together execute layer tanh,issue,positive,positive,positive,positive,positive,positive
448572966,"The tutorial text itself does not include code that uses wikitext data, but by looking more closely into the [config file](https://github.com/facebookresearch/pytext/blob/0d1872312d4fd670ce3e388dd08c7deb85bbec8d/demo/configs/multitask_sst_lm.json#L54) that the tutorial points to, everything becomes clear, thanks.  ",tutorial text include code data looking closely file tutorial everything becomes clear thanks,issue,positive,positive,positive,positive,positive,positive
448457334,"This is failing on a facebook internal e2e test of our RNNG pipeline (we're working on separating these tests from internal infra so we can run them on CircleCI)

The issue in particular is:
we set `pad_token=None` here: https://github.com/facebookresearch/pytext/blob/master/pytext/data/compositional_data_handler.py#L67

We then read this here:
https://github.com/facebookresearch/pytext/blob/master/pytext/fields/field.py#L61

and do not set pad_token_idx on the metadata field, which causes an explosion when we try to read it in this PR.

@seayoung1112 @bethebunny  do you guys have any thoughts?",failing internal test pipeline working separating internal infra run issue particular set read set field explosion try read,issue,negative,positive,neutral,neutral,positive,positive
448425600,"If you mean in the sense of unifying the projects, no, there are no plans. If you mean supporting fasttext models in PyText, we are evaluating that, as it is definitely a feature request we've heard.",mean sense mean supporting definitely feature request,issue,positive,negative,neutral,neutral,negative,negative
448383484,"This has not been pulled into differential. There's some confusion with the ""export to github"" feature that's prevent updates. Will do once the diff is accepted. ",differential confusion export feature prevent accepted,issue,negative,neutral,neutral,neutral,neutral,neutral
448372923,"Do we need to talk about ATIS on this page at all? As you said, coming to this page from ATIS are advanced users who know that the config file will change to the one they're currently playing with instead of the DocNN one. 

I propose we get rid if any mention of ATIS on https://pytext-pytext.readthedocs-hosted.com/en/latest/execute_your_first_model.html, and replace the ""Evaluate the model"" output with the DocNN one. ",need talk atis page said coming page atis advanced know file change one currently instead one propose get rid mention atis replace evaluate model output one,issue,negative,positive,positive,positive,positive,positive
448319838,"Thanks @snisarg !
Can you maybe also give me a small hint about the question I asked in the last paragraph?",thanks maybe also give small hint question last paragraph,issue,negative,negative,neutral,neutral,negative,negative
448319452,"This has been checked in, thank you so much for pointing out. ",checked thank much pointing,issue,negative,positive,positive,positive,positive,positive
448313853,"Currently, we have a very simple Featurizer which can work for English and similar languages only. We'll look into adding one for Chinese soon, but don't have immediate plans for it. We'll appreciate if you can send us a pull request though! 

See a more detailed answer from @m3rlin45 here: https://www.facebook.com/groups/pytext/permalink/929836947224147/",currently simple work similar look one soon immediate appreciate send u pull request though see detailed answer,issue,negative,positive,positive,positive,positive,positive
448294342,"I like it. Here's mine. And I'm using PyCharm which renders it next to the file editor, and is pretty accurate. :P 

![screen shot 2018-12-18 at 09 03 00](https://user-images.githubusercontent.com/683812/50170053-dd770380-02a3-11e9-8a70-cd0424b9da4b.png)
",like mine next file editor pretty accurate screen shot,issue,positive,positive,positive,positive,positive,positive
448292205,"For documentation changes can we make a habit of including a screenshot or HTML of the change? This is obviously gonna be fine because it's simple but it's hard to get a feel for what it looks like :)

Also for ""how has this been tested"" for documentation you can always check what the README looks like on your fork, or run sphinx build, etc.",documentation make habit change obviously gon na fine simple hard get feel like also tested documentation always check like fork run sphinx build,issue,positive,positive,neutral,neutral,positive,positive
448289970,"Created https://github.com/facebookresearch/pytext/pull/132 to update README. 

@moabaom from your trace, I see that you're using python 3.5. Please try with 3.6 or 3.7. Reopen this issue in case you're still seeing this error. ",update trace see python please try reopen issue case still seeing error,issue,negative,neutral,neutral,neutral,neutral,neutral
448272182,"What version of Python do you have installed? We require python 3.6+, as you can see here: https://pytext-pytext.readthedocs-hosted.com/en/latest/installation.html

We should restore the warning on the README though, I guess it got lost in the shuffle of document creation.",version python require python see restore warning though guess got lost shuffle document creation,issue,negative,neutral,neutral,neutral,neutral,neutral
448007147,"@fsonntag Thanks for submitting the fix, could you rebase to make the test pass?",thanks fix could rebase make test pas,issue,negative,positive,positive,positive,positive,positive
447966548,"@hikushalhere Yes that helps, thank you so much for the quick reply  ",yes thank much quick reply,issue,positive,positive,positive,positive,positive,positive
447965072,The `prediction` values are indices of the predicted actions. Given the actions vocabulary the ids should be converted to actual actions and given the action the tree structure is derived naturally. We will add the translation from id to string to make the predicted actions readable. Will that help?,prediction index given vocabulary converted actual given action tree structure derived naturally add translation id string make readable help,issue,negative,positive,neutral,neutral,positive,positive
447960287,Changes look good to me. @seayoung1112 can you please merge it?,look good please merge,issue,positive,positive,positive,positive,positive,positive
447956357,"The tutorial explains how to do multitasking, where each task has its own (read disjoint) dataset.  This is in contrast to the case where there is a single dataset with each input text having multiple labels, one for each task.  Does this clear up the confusion?  If so I can add an explanation to the tutorial.  Thanks!",tutorial task read disjoint contrast case single input text multiple one task clear confusion add explanation tutorial thanks,issue,positive,positive,neutral,neutral,positive,positive
447946919,"I'm closing this, please open a new issue if you have a question.",please open new issue question,issue,negative,positive,neutral,neutral,positive,positive
447932343,"Sorry, that was unnecessary. I'm going to remove that in a bit. ",sorry unnecessary going remove bit,issue,negative,negative,negative,negative,negative,negative
447906331,"The config is something you can generate on your own according to your use. The config.json we've provided is a sample one. It doesn't exist in the project, it's a generic file, you point it from wherever it exists. ",something generate according use provided sample one exist project generic file point wherever,issue,negative,neutral,neutral,neutral,neutral,neutral
447827191,@ahhegazy Thanks for the comparison. Is there plan to bring `fasttext` also under `PyText`,thanks comparison plan bring also,issue,negative,positive,positive,positive,positive,positive
447709020,"Does the file 'config.json' exist in the project from the beginning or is it generated later?I had the same problem 'no such file: config.json', Did the readme  miss any steps, or could you detail the process?thank you!",file exist project beginning later problem file miss could detail process thank,issue,negative,neutral,neutral,neutral,neutral,neutral
447665218,"If you don't specify the '--model' option, it'll pick up the model location from config's save_snapshot_path; which defaults to '/tmp/model.pt'. Look at https://github.com/facebookresearch/pytext/blob/b79a93b510ca0ace5e73f4cad399a11c902594a9/pytext/config/pytext_config.py#L73
The idea of the config is to have 1 file which specifies all these details, including where the model file was created and use that for exporting in this case.",specify model option pick model location look idea file model file use case,issue,negative,neutral,neutral,neutral,neutral,neutral
447643548,"My understanding is that you train from a configuration file, this will then save the output in a temporary location. Then, any further commands (such as `export`) using the same configuration file reference that temporary location somehow. ",understanding train configuration file save output temporary location export configuration file reference temporary location somehow,issue,negative,neutral,neutral,neutral,neutral,neutral
447627551,"@Youlike708 Can you please fill in the issue template, otherwise I will close it as it is not actionable by us.",please fill issue template otherwise close actionable u,issue,negative,neutral,neutral,neutral,neutral,neutral
447609896,"In readme,   suggest show the export example as:

 (venv) $ pytext export  < demo/configs/docnn.json  ",suggest show export example export,issue,negative,neutral,neutral,neutral,neutral,neutral
447600117,"> Could you try only
> `pip install pytext-nlp`
> 
> Caffe2 is merged with PyTorch 1.0, which is already in the dependency list. Manually installing Caffe2 might introduce a wrong version

It works~
I tried to uninstall `caffe2` `pytext-nlp` and `torch`
then `pip install pytext-nlp`, then it works~
thank you very much~",could try pip install already dependency list manually might introduce wrong version tried torch pip install thank,issue,negative,negative,negative,negative,negative,negative
447598060,"Could you try only 
`pip install pytext-nlp`

Caffe2 is merged with PyTorch 1.0, which is already in the dependency list. Manually installing Caffe2 might introduce a wrong version",could try pip install already dependency list manually might introduce wrong version,issue,negative,negative,negative,negative,negative,negative
447583608,"config.json is the config you used for training, not the trained model file",used training trained model file,issue,negative,neutral,neutral,neutral,neutral,neutral
447551700,"**Torchtext** Provides a set of data-abstractions that helps reading and processing raw text data into PyTorch tensors, at the moment we use Torchtext in PyText for training-time data reading and preprocessing.

**AllenNLP** is a great NLP modeling library that is aimed at providing reference implementations and prebuilt state-of-the-art models, and make it easy to iterate on and research with models for different NLP tasks.

**PyText**
We've built PyText to be a rich NLP modeling library (along the lines of AllenNLP) but with production capabilities baked in the design from day 1. Examples are:

- We provide interfaces to make sure data preprocessing can be consistent between training and runtime 
- The model interfaces are compatible with ONNX and torch.jit 
- A core goal for us in the next few month is to be able to run models trained in PyText on mobile.

Among other differences like supporting distributed training and multi-task training.

That being said, so far our library of models has been mostly influenced by our current production use-cases, we are actively working on enriching this library with more models and tasks while keeping production capabilities and inference speed in mind. ",set reading raw text data moment use data reading great modeling library providing reference make easy iterate research different built rich modeling library along production baked design day provide make sure data consistent training model compatible core goal u next month able run trained mobile among like supporting distributed training training said far library mostly current production actively working enriching library keeping production inference speed mind,issue,positive,positive,positive,positive,positive,positive
447540729,"In my opinion 
###  Pytext

* based on torchtext;
* supports distributed training;
* supports multi-task training;
* currently only several tasks are out of the box.


### Allennlp
* Provides a lot of handful modules like Seq2VecEncoder, Seq2SeqEncoder.
* Provides a lot of handful utilities like  masked_softmax, masked_log_softmax .....  (You can get a lot of mask-related helper functions in AllenNLP, but there is no such thing in pytext).
* Provides the latest embeddings and tokenization out of the box, such as ELMo, BERT.
* configs are based on jsonnet, which is much more powerful than json (pytext uses json I think).
* Provides out of the box model/data_reader for a lot of popular NLP tasks.
* No distributed training.
* Multi-task training is not very straightforward in some cases.

(I just read the pytext doc, so I might be wrong)
",opinion based distributed training training currently several box lot handful like lot handful like get lot helper thing latest box based much powerful think box lot popular distributed training training straightforward read doc might wrong,issue,positive,positive,positive,positive,positive,positive
447263365,Thanks! We will link to the right section in our [clouds](https://pytext-pytext.readthedocs-hosted.com/en/latest/installation.html#cloud-vm-setup) and [installation](https://pytext-pytext.readthedocs-hosted.com/en/latest/installation.html) docs from our README.,thanks link right section installation,issue,negative,positive,positive,positive,positive,positive
