id,original_comment,processed_comment,source,sentiment_VADER,sentiment_textblob,sentiment_pattern,sentiment_bert,sentiment_spacy,max_voted_sentiment
2040517244,"This does not work. Not only that this does not work, this fails silently, i.e, no rotation is applied to any key other than `image`. I was able to work by explicitly adding targets. Using the nomenclature of above example, following works for version `1.3.1`
```
# only this new line needs to be added. 
aug.add_targets({'image2':'image'})

aug_input = {""image"" : image1, ""image2"" : image2}
augmented_data = aug(**aug_input)
i1 = augmented_data[""image""]
i2 = augmented_data[""image2""]

```",work work silently rotation applied key image able work explicitly nomenclature example following work version new line need added image image image image image image,issue,negative,positive,positive,positive,positive,positive
2040218369,We also need to add test that shows that tiles are indeed shuffled. ,also need add test indeed,issue,negative,neutral,neutral,neutral,neutral,neutral
2038076792,We are not planning adding `torchvision` dependency at the moment. But reading images with `opencv` is indeed twice slower than in `torchvision`,dependency moment reading indeed twice,issue,negative,neutral,neutral,neutral,neutral,neutral
2038073226,"Thanks!

Inspiration - it is almost this notebook: https://albumentations.ai/docs/examples/example_multi_target/

1. Video is split into frames.
2. For every frame we use example from the notebook to apply augmentation pipeline with the same parameters (could be very complicated). In the example about multi target we have:

```python
import albumentations as A

transform = A.Compose(
    [HorizontalFlip(p=0.5), ...],
    additional_targets={
        'image1': 'image',
        'image2': 'image',
        ...
        'imageN': 'image',

        'bboxes1': 'bboxes',
        'bboxes1': 'bboxes',
        ...
        'bboxesM': 'bboxes',

        'keypoints1': 'keypoints',
        'keypoints2': 'keypoints',
        ...
        'keypointsK': 'keypoints',

        'mask1': 'mask',
        'mask2': 'mask',
        ...
        'maskL': 'mask'
    })
)
```

but make `N = M = L = K = <the number of frames>` and the definition of `additional_targets` dictionary could be done as a one liner.
 
 For video - probably person / people dancing. This would allow to visualize on image, mask, bounding boxes + keypoints
  
 ---
 Way that we create examples =>
 we create notebooks to: `https://github.com/albumentations-team/albumentations_examples` and they are authomatically parsed to the documentation to the website.
 
 If you create such a notebook and commit it to the `albumentations_examples` it would be great. 
 
 P.S. If you have questions - feel free to ask here, ask in [Discord](https://discord.gg/AKPrrDYNAt), or just ping me in [DM](https://www.linkedin.com/in/iglovikov/)
 
 Thank you for helping!
 ",thanks inspiration almost notebook video split every frame use example notebook apply augmentation pipeline could complicated example target python import transform make number definition dictionary could done one liner video probably person people dancing would allow visualize image mask bounding way create create documentation create notebook commit would great feel free ask ask discord ping thank helping,issue,positive,positive,positive,positive,positive,positive
2036569181,"Hello. I will have some free time starting the week of the 15th April and I would like to help with this.
What type of open-source video dataset you would suggest for creating the tutorial notebook?
Is there a similar notebook to get inspiration?
Thanks. 😸 ",hello free time starting week th would like help type video would suggest tutorial notebook similar notebook get inspiration thanks,issue,positive,positive,positive,positive,positive,positive
2030215150,"> word `TODO`, we should probably remove as well, having `Deprecated` in the code should be enough.
> 
> And to pass linting / type checks it is recommended to set up pre-commit hook following https://github.com/albumentations-team/albumentations/blob/main/CONTRIBUTING.md

changed to NOTE.",word probably remove well code enough pas type set hook following note,issue,negative,neutral,neutral,neutral,neutral,neutral
2030071192,"word `TODO`, we should probably remove as well, having `Deprecated` in the code should be enough.

And to pass linting / type checks it is recommended to set up pre-commit hook following https://github.com/albumentations-team/albumentations/blob/main/CONTRIBUTING.md  ",word probably remove well code enough pas type set hook following,issue,negative,neutral,neutral,neutral,neutral,neutral
2026647098,It is recommended to normalize image at the end as unit8 images are processed faster than float32,normalize image end unit faster float,issue,negative,neutral,neutral,neutral,neutral,neutral
2026520041,"> @jeanmonet @AdvancingEther
> 
> We will look into it, but it would be easier if you provided an example to reproduce the issue.
> 
> Do you have one?

I have solved it. When I normalized a picture in uint8 format, I turned it into float64 format., tks!!!",look would easier provided example reproduce issue one picture format turned float,issue,negative,neutral,neutral,neutral,neutral,neutral
2025432617,"It is hard to debug if there is no way to reproduce the issue.

Could you please visually check that after the augmentations, the pipeline bounding boxes are where they should be?

If this is not the case and you can provide an example of images + bounding boxes that were processed incorrectly, we will be happy to fix the issue ASAP. 

But if bounding boxes are where they should be, the issue with convergence will be in the later phase of training.
",hard way reproduce issue could please visually check pipeline bounding case provide example bounding incorrectly happy fix issue bounding issue convergence later phase training,issue,positive,positive,positive,positive,positive,positive
2021483424,"@jeanmonet @AdvancingEther 

We will look into it, but it would be easier if you provided an example to reproduce the issue. 

Do you have one? ",look would easier provided example reproduce issue one,issue,negative,neutral,neutral,neutral,neutral,neutral
2016941683,"Updated bench result in images/sec

Bbboxes
```
                imgaug albumentations torchvision
HorizontalFlip   10214           4394       11750
VerticalFlip     12331           4628       22442
Rotate            1489            835        2514
RandomRotate90    1662           3954           -
Pad                910            796           -
Perspective        432           2279        1893
Crop               362            954           -
Affine            1525            447        2654
PiecewiseAffine      3              7           -
Sequence           197            184           -
```

Keypoints
```
                      imgaug albumentations
HorizontalFlip          7833           4545
VerticalFlip            9179           4940
Flip                    7881           4291
Rotate                  2625           2101
SafeRotate                 -           1340
RandomRotate90          5165           3958
ShiftScaleRotate           -           2047
Transpose                  -           4621
Pad                     2806           2252
Perspective             1280           3044
RandomCropNearBBox         -           5284
CenterCrop                 -           5143
Crop                    1409           5617
CropAndPad                 -           3878
RandomCropFromBorders      -           5247
Affine                  2825            510
PiecewiseAffine           12             12
Sequence                 492            318
```",bench result rotate pad perspective crop affine sequence flip rotate transpose pad perspective crop affine sequence,issue,negative,neutral,neutral,neutral,neutral,neutral
2011001644,Looks like all the support is long added.,like support long added,issue,positive,negative,neutral,neutral,negative,negative
2010999203,"We removed all dependencies from imgaug and quidda that caused issues with OpenCV,

Let me know if the issue with OpenCV dependency still exists. ",removed let know issue dependency still,issue,negative,neutral,neutral,neutral,neutral,neutral
2010976629,"> Possibly related. If you use PadIfNeeded with RandomScale, it calculates the padding for OpenCVs copyMakeBorder for the initial image/mask size, not from the incoming image/mask from the Compose pipeline. Since RandomScale doesn't preserve that input shape, it'll be padded to a random size. Which caused me a hell of a headache trying to figure out why my crops were failing.

Could you please share an example that I could reproduce?",possibly related use padding initial size incoming compose pipeline since preserve input shape random size hell headache trying figure failing could please share example could reproduce,issue,negative,negative,negative,negative,negative,negative
2010975620,"I believe this issue was fixed in https://github.com/albumentations-team/albumentations/pull/1570 

I cannot reproduce this bug in version 1.4.2",believe issue fixed reproduce bug version,issue,negative,positive,neutral,neutral,positive,positive
2005087817,"It looks like it was decided that Frost is the same as Template but with frost images.

Although this non-obvious fast should obviously covered in the blog post / tutorial.",like decided frost template frost although fast obviously covered post tutorial,issue,negative,positive,neutral,neutral,positive,positive
2005079005,"The main reason to optimize is similar to why hackers do it - **""Because we can""** :)

Benchmark is used as a profiler to show obvious areas of growth.",main reason optimize similar used profiler show obvious growth,issue,positive,positive,neutral,neutral,positive,positive
2005070371,"Thank you for your contribution!

By the way, would you or your company consider becoming a sponsor for the project?

https://github.com/sponsors/albumentations-team
",thank contribution way would company consider becoming sponsor project,issue,negative,positive,positive,positive,positive,positive
2004776065,"Removed an unused import.
Ruff checks should pass now :)",removed unused import ruff pas,issue,negative,neutral,neutral,neutral,neutral,neutral
2004661663,"Yes, you are right. We trigger this warning even if user initializes the transform in non-deprecated way. Your PR is welcome.",yes right trigger warning even user transform way welcome,issue,positive,positive,positive,positive,positive,positive
2004301236,"> I prefer something similar to:
> 
> ## [github.com/albumentations-team/albumentations/blob/0cb8de5022a484e4586ce573040fc6e58885b094/albumentations/augmentations/blur/transforms.py#L349 ](https://github.com/albumentations-team/albumentations/blob/0cb8de5022a484e4586ce573040fc6e58885b094/albumentations/augmentations/blur/transforms.py#L349)
A little off topic, but isn't the warning always going to be triggered in this specific case? Line 336: `sigmaX_limit: ScaleFloatType = (0.2, 1.0),  # noqa: N803` indicates that sigmaX_limit will always have a value, hence the program will always think the user is using that parameter and print a warning, should I PR a fix?",prefer something similar little topic warning always going triggered specific case line always value hence program always think user parameter print warning fix,issue,negative,negative,neutral,neutral,negative,negative
2004047572,"I prefer something similar to:

[github.com/albumentations-team/albumentations/blob/0cb8de5022a484e4586ce573040fc6e58885b094/albumentations/augmentations/blur/transforms.py#L349
](https://github.com/albumentations-team/albumentations/blob/0cb8de5022a484e4586ce573040fc6e58885b094/albumentations/augmentations/blur/transforms.py#L349)
---
Not a big fan of `*args` in the code, as it complicates readability and type checking.

=>
Just add `size`  When people use `height` and `width,` a deprecation warning will appear, but the transform should still work.

It will break compatibility for people who call transform without argument names, but I don't know how big the issue will be.

@Dipet, what do you think?
",prefer something similar big fan code readability type add size people use height width deprecation warning appear transform still work break compatibility people call transform without argument know big issue think,issue,negative,neutral,neutral,neutral,neutral,neutral
2002134779,"While option 1 is the safer choice, I prefer option 2 more, but it needs a deprecation warning for using separate height and width arguments",option choice prefer option need deprecation warning separate height width,issue,negative,neutral,neutral,neutral,neutral,neutral
2001700241,Based on the PRs seems like the implementation is actually missing Frost only,based like implementation actually missing frost,issue,negative,negative,negative,negative,negative,negative
2001693261,"There are two simple options to consider:

1. Proceed without mandating that arguments be named after the third parameter. This approach can make the function calls appear verbose and potentially less readable, especially if there are many parameters. The code might end up looking quite wordy, which could affect maintainability and clarity:

```python
class RandomResizedCrop(_BaseRandomSizedCrop):
...
  def __init__(
          self,
          size: Union[int, Tuple[int, int]],
          *args,
          ratio: Tuple[float, float] = (0.75, 1.3333333333333333),
          interpolation: int = cv2.INTER_LINEAR,
          always_apply: bool = False,
          p: float = 1.0,
      ):
          if isinstance(size, tuple):
              if len(size) != 2:
                  raise ValueError(""Size must be a tuple of two integers (height, width)."")
              height, width = size
              if len(args) >= 1:
                  scale = args[0]
              else:
                  scale = (0.08, 1.0)
              if len(args) > 1:
                  ratio = args[1]
          else:
              if len(args) < 1:
                  raise ValueError(""Width must be provided when the first argument is an integer."")
              width = args[0]
              if len(args) >= 2:
                  scale = args[1]
              else:
                  scale = (0.08, 1.0)
              if len(args) > 2:
                  ratio = args[2]
              warn(
                  ""Initializing with separate height and width is deprecated and will be removed in a future version. ""
                  ""Please pass a tuple (height, width) as the first argument."",
                  DeprecationWarning,
                  stacklevel=2
              )
              height = size
  
          super().__init__(height=height, width=width, interpolation=interpolation, always_apply=always_apply, p=p)
          self.scale = scale
          self.ratio = ratio
```

2. Go with enforcing named arguments for all parameters after the third one. This approach enhances clarity and readability but might break existing code for users who have not used named arguments in their calls, as illustrated below:

```python
A.RandomResizedCrop(513, 450, (0.08, 1.0)) # Called without argument names
```


implementation [cleaner]:
```python
class RandomResizedCrop(_BaseRandomSizedCrop):
...
  def __init__(
          self,
          size: Union[int, Tuple[int, int]],
          width: Optional[int] = None,
          *,
          scale: Tuple[float, float] = (0.08, 1.0),
          ratio: Tuple[float, float] = (0.75, 1.3333333333333333),
          interpolation: int = cv2.INTER_LINEAR,
          always_apply: bool = False,
          p: float = 1.0,
      ):
          if isinstance(size, tuple):
              if len(size) != 2:
                  raise ValueError(""Size must be a tuple of two integers (height, width)."")
              height, width = size
          else:
              if width is None:
                  raise ValueError(""Width must be provided when 'size' is an integer."")
              height = size
  
          super().__init__(height=height, width=width, interpolation=interpolation, always_apply=always_apply, p=p)
          self.scale = scale
          self.ratio = ratio
 ```",two simple consider proceed without third parameter approach make function appear verbose potentially le readable especially many code might end looking quite wordy could affect clarity python class self size union ratio float float interpolation bool false float size size raise size must two height width height width size scale else scale ratio else raise width must provided first argument integer width scale else scale ratio warn separate height width removed future please pas height width first argument height size super scale ratio go third one approach clarity readability might break code used python without argument implementation cleaner python class self size union width optional none scale float float ratio float float interpolation bool false float size size raise size must two height width height width size else width none raise width must provided integer height size super scale ratio,issue,positive,positive,positive,positive,positive,positive
2000871836,Look like this issue is relevant to https://github.com/albumentations-team/albumentations/issues/1585,look like issue relevant,issue,negative,positive,positive,positive,positive,positive
1998288085,"Already implemented this for our augmentation pipeline. The results look like this:

<div align=""center"">
  <a href=""https://i.imgur.com/tUiymXQ.jpeg"">
    <img src=""https://i.imgur.com/tUiymXQ.jpeg"" width=""30%"">
  </a>
  <a href=""https://i.imgur.com/Jc2b5kG.jpeg"">
    <img src=""https://i.imgur.com/Jc2b5kG.jpeg"" width=""30%"">
  </a>
  <a href=""https://i.imgur.com/OGhQCTf.jpeg"">
    <img src=""https://i.imgur.com/OGhQCTf.jpeg"" width=""30%"">
  </a>
  <p>
    <b>Left:</b> Original, <b>Middle:</b> Subtle chromatic aberration, <b>Right:</b> Strong chromatic aberration
    <br>(Image is from our internal mobile mapping dataset)
  </p>
</div>

Waiting for feedback from @albumentations-team, then I'll make a PR :)",already augmentation pipeline look like div center left original middle subtle chromatic aberration right strong chromatic aberration image internal mobile waiting feedback make,issue,positive,positive,neutral,neutral,positive,positive
1998053903,"Great to hear back on this! 
You are totally right, my implementation was an overly simplistic one and lateral chromatic aberration as you described would be much more realistic. If it looks something like (https://smallpond.ca/jim/photomicrography/ca/index.html) then you have me super excited. Please implement it, by all means, looking forward trying it out.",great hear back totally right implementation overly simplistic one lateral chromatic aberration would much realistic something like super excited please implement looking forward trying,issue,positive,positive,positive,positive,positive,positive
1997142410,"Simply shifting the 2 channels (blue and red) would apply the chromatic aberration effect to the whole image.
This might not be the desired effect since the color fringing typically occurs only at the edges of the image.

To simulate lateral chromatic aberration, the shift should be applied to the individual channels separately with respect to the radial distance from the center of the image.

This can be done by splitting the image into its RGB channels, applying a slight amount of distortion to the blue and red channels, and then merging the channels back together.
The border mode should be set to `cv2.BORDER_REFLECT` to avoid black borders in the channels, if the distortion gets quite large.

If this transform seems useful, I would like to implement it as an `ImageOnlyTransform`.
",simply shifting blue red would apply chromatic aberration effect whole image might desired effect since color fringing typically image simulate lateral chromatic aberration shift applied individual separately respect radial distance center image done splitting image slight amount distortion blue red back together border mode set avoid black distortion quite large transform useful would like implement,issue,positive,positive,neutral,neutral,positive,positive
1992822471,"Torchvision augmentations support boxes, while imgaug is not supported.

I suspect people would be more interested in how `Albumentations` are with respect to the popular `Torchvison`",support suspect people would interested respect popular,issue,positive,positive,positive,positive,positive,positive
1989521618,"You may apply the same transform to a set of videos through the additional targets functionality: 

https://albumentations.ai/docs/examples/example_multi_target/",may apply transform set additional functionality,issue,negative,neutral,neutral,neutral,neutral,neutral
1988945411,"Thanks for your donation. Every dollar counts :)

I will look at metrics, but I cannot promise. 

It was generated a long time back by @creafz , who focuses on other projects.

But I will take a look, anyway.
",thanks donation every dollar look metric promise long time back take look anyway,issue,positive,positive,neutral,neutral,positive,positive
1988938858,@ternaus thanks and I did a one-time donation because I am a student. Can you help check the first reference link for CIFAR-10 dataset at [this URL](https://tensorboard.dev/experiment/ZleMHe73QCGzPeDCRpFLfA/) in the following URL https://albumentations.ai/docs/autoalbument/metrics/#examples-for-metric-values? It is no longer accessible because TensorBoard.dev service has been stopped. ,thanks donation student help check first reference link following longer accessible service stopped,issue,positive,positive,positive,positive,positive,positive
1988928660,"@ryoryon66 Thank you for your work.

Would you consider becoming a sponsor of Albumentations? 

https://github.com/sponsors/albumentations-team

",thank work would consider becoming sponsor,issue,negative,positive,positive,positive,positive,positive
1986698643,"Fixed in https://github.com/albumentations-team/albumentations/pull/1570

@margilt, would you consider becoming a sponsor for the library? Lowest tier is only `$2 / month`

https://github.com/sponsors/albumentations-team",fixed would consider becoming sponsor library tier month,issue,negative,positive,positive,positive,positive,positive
1986690943,"@domef would you consider becoming a sponsor? 

https://github.com/sponsors/albumentations-team",would consider becoming sponsor,issue,negative,positive,positive,positive,positive,positive
1986690776,"As of Version 1.4.1 everything works:

```python
In [1]: import albumentations as A

In [2]: d = {
   ...:     ""__version__"": ""1.3.0"",
   ...:     ""transform"": {
   ...:         ""__class_fullname__"": ""Compose"",
   ...:         ""additional_targets"": {},
   ...:         ""p"": 1.0,
   ...:         ""transforms"": [
   ...:             {
   ...:                 ""__class_fullname__"": ""albumentations.augmentations.transforms.ToRGB"",
   ...:                 ""p"": 1.0,
   ...:             }
   ...:         ],
   ...:     },
   ...: }

In [3]: A.from_dict(d)
Out[3]:
Compose([
  ToRGB(always_apply=True, p=1.0),
], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}, is_check_shapes=True)
```",version everything work python import transform compose compose,issue,negative,neutral,neutral,neutral,neutral,neutral
1986571170,"@saigontrade88 fixed.

Would you consider becoming a sponsor for Albumentations? 

https://github.com/sponsors/albumentations-team
",fixed would consider becoming sponsor,issue,negative,positive,positive,positive,positive,positive
1985918090,"Hello,
 Today I have got an issue when accessing autoalbument documentation at https://albumentations.ai/docs/autoalbument/. Can you help check if this issue can be fixed? Many thanks. ",hello today got issue documentation help check issue fixed many thanks,issue,positive,positive,positive,positive,positive,positive
1982140459,"Thanks!

Going to add a check that verifies that all docstrings in Tranforms match Google Docstrings to the level that your parser works without issues.",thanks going add check match level parser work without,issue,negative,positive,positive,positive,positive,positive
1979749415,"## [Codecov](https://app.codecov.io/gh/albumentations-team/albumentations/pull/1566?src=pr&el=h1&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team) Report
All modified and coverable lines are covered by tests :white_check_mark:
> Project coverage is 91.04%. Comparing base [(`9e61897`)](https://app.codecov.io/gh/albumentations-team/albumentations/commit/9e61897a78be4090d55bb8218812d6fa0d190eae?el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team) to head [(`78de8d3`)](https://app.codecov.io/gh/albumentations-team/albumentations/pull/1566?src=pr&el=desc&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team).

> :exclamation: Current head 78de8d3 differs from pull request most recent head b028796. Consider uploading reports for the commit b028796 to get more accurate results


<details><summary>Additional details and impacted files</summary>


```diff
@@            Coverage Diff             @@
##             main    #1566      +/-   ##
==========================================
- Coverage   91.10%   91.04%   -0.07%     
==========================================
  Files          54       54              
  Lines        8310     8310              
==========================================
- Hits         7571     7566       -5     
- Misses        739      744       +5     
```



</details>

[:umbrella: View full report in Codecov by Sentry](https://app.codecov.io/gh/albumentations-team/albumentations/pull/1566?src=pr&el=continue&utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team).   
:loudspeaker: Have feedback on the report? [Share it here](https://about.codecov.io/codecov-pr-comment-feedback/?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team).
",report covered project coverage base head exclamation current head pull request recent head consider commit get accurate summary additional impacted coverage main coverage umbrella view full report sentry feedback report share,issue,positive,positive,neutral,neutral,positive,positive
1979607094,"## Welcome to [Codecov](https://codecov.io?utm_medium=referral&utm_source=github&utm_content=comment&utm_campaign=pr+comments&utm_term=albumentations-team) :tada:

Once merged to your default branch, Codecov will compare your coverage reports and display the results in this comment.

Thanks for integrating Codecov - We've got you covered :open_umbrella:",welcome default branch compare coverage display comment thanks got covered,issue,positive,positive,positive,positive,positive,positive
1977036112,"Added check for `---` in docstrings and cleaned from the existing code. It is a hack, rather than solution, but hopefully will help for some time: https://github.com/albumentations-team/albumentations/pull/1558",added check code hack rather solution hopefully help time,issue,positive,neutral,neutral,neutral,neutral,neutral
1976391771,"The current albumentation dosctring mixes two docstring section styles (google and numpy).
The mkdocs support two of them but can not handle mixed cases in a single project.
I found the following issue discussing more flexible style support in the mkdocs, but the issue is still open and is inconclusive.

https://github.com/mkdocstrings/griffe/issues/5

It would be preferable to format docstring with tools like ruff or black, but currently, they do not seem able to handle docstring styles.
And I could not find other tools that support formatting multiple docstring styles.

If possible, I think writing a custom hook would be a practical workaround, and I expect deleting ""---"" is sufficient.
I could confirm that the mkdocs correctly rendered the parameter table by deleting lines ""----"" in a transform docstring on my local machine.

Ex: 
![screenshot_params](https://github.com/albumentations-team/albumentations/assets/9190086/b01a5e45-4b97-42c4-9c84-8893bf68d19f)
",current two section support two handle mixed single project found following issue flexible style support issue still open inconclusive would preferable format like ruff black currently seem able handle could find support multiple possible think writing custom hook would practical expect sufficient could confirm correctly parameter table transform local machine ex,issue,positive,positive,neutral,neutral,positive,positive
1975270976,"I was not able to set up ruff to check for ""----"", it does not flag it as violation of google docstrings format.

I will try to upgrade version of mkdocs, maybe it will be more robust toward formatting of the docstring.

I can also have a custom hook, that just look for ""---"" and longer in the code.

What exactly don't you like in the existing version of parsed docstrings?


",able set ruff check flag violation format try upgrade version maybe robust toward also custom hook look longer code exactly like version,issue,negative,positive,positive,positive,positive,positive
1975166186,"@ternaus Thank you for your quick response.

But I could not confirm the fix in the documetation site.

Lines ""----"" appear in commit #1529, where the lines are inserted at some sections (not all sections) of every transform's docstrings.
I think we need to remove those inserted lines “----"" to fix the issue.",thank quick response could confirm fix site appear commit inserted every transform think need remove inserted fix issue,issue,positive,positive,positive,positive,positive,positive
1974916491,"FIxed in https://github.com/albumentations-team/albumentations/pull/1556

The issue was that script that generated docs did not distinguish between Deprecated class and deprecated parameter in the class and did not use it.",fixed issue script distinguish class parameter class use,issue,negative,positive,neutral,neutral,positive,positive
1965280893,Could you please share a piece of code that will allow us to reproduce the issue?,could please share piece code allow u reproduce issue,issue,positive,neutral,neutral,neutral,neutral,neutral
1964701203,"In version 1.4.0 transforms that depend on imgaug were deprecated.

* `Imgaug` is not supported for many years which causes issues with dependencies.
* We have the same transforms that are faster natively implemented.
---
You may use [Affine](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine) instead of `iaa.affine`

And `DualTransform` instead of `DualIAATransform` 

",version depend many faster natively may use affine instead instead,issue,negative,positive,positive,positive,positive,positive
1964581109,"@Matthew-J-Payne, thank you for addressing this issue.

Docstrings and documentation, in general, a big areas for growth.",thank issue documentation general big growth,issue,positive,positive,neutral,neutral,positive,positive
1958888497,"Opening this up.this code
```
    bbox_params=A.BboxParams(format=""pascal_voc"", min_visibility=0.3),
```

Can produce mask without a box
![image](https://github.com/albumentations-team/albumentations/assets/149590311/474bfd31-0a92-4bf7-a92f-eb274c4c424b)

and even produces mismatched outputs

the bounding boxes has 1 output while the mask has 6, and not even properly ordered.",opening code produce mask without box image even bounding output mask even properly ordered,issue,negative,neutral,neutral,neutral,neutral,neutral
1955145354,"Thank you for reporting.

We will move quida code base to the library, which will remove this issue..",thank move code base library remove issue,issue,negative,negative,negative,negative,negative,negative
1955131038,"I suspect that it will be hard to debug behavior in which the output of the Crop transform is smaller than the (crop_height, crop_width) 

Typically we use PadIfNeeded before Crop to ensure that crop size does not exceed the size of the image. 

Will this work?

P.S. Anyway, this is a good question to add to the FAQ
",suspect hard behavior output crop transform smaller typically use crop ensure crop size exceed size image work anyway good question add,issue,negative,positive,neutral,neutral,positive,positive
1951649607,"ok, i understand - benchmark was wrong... My fail
we need to change default flag - impact with another order of args  we have 2x speedup.
```python
import random
import time


def call_with_random(flag: bool = True):
    if random.random() < 0.5 or flag:
        return True
    return False


def call_without_random(flag: bool = True):
    if flag or random.random() < 0.5:
        return True
    return False


def bench(func, n: int = int(1e8)) -> float:
    s = time.time()
    for _ in range(n):
        func()
    return time.time() - s


if __name__ == '__main__':
    import timeit
    print(timeit.timeit(""call_with_random()"", setup=""from __main__ import call_with_random"", number=int(1e8)))
    print(timeit.timeit(""call_without_random()"", setup=""from __main__ import call_without_random"", number=int(1e8)))
    print(bench(call_with_random, int(1e8)))
    print(bench(call_without_random, int(1e8)))

# 8.865977536999992
# 3.8430869389999884
# 8.653024435043335
# 3.906151294708252
```",understand wrong fail need change default flag impact another order python import random import time flag bool true flag return true return false flag bool true flag return true return false bench float range return import print import print import print bench print bench,issue,positive,negative,negative,negative,negative,negative
1951647767,"Interesting.
The results are really inconsistent - it looks like the impact is very low, and background tasks have a big impact on this benchmark",interesting really inconsistent like impact low background big impact,issue,positive,positive,positive,positive,positive,positive
1951640654,"If there is small, but nonzero improvement -> why not? Let's merge it.",small nonzero improvement let merge,issue,negative,negative,negative,negative,negative,negative
1951635407,"Sounds logical. I am not sure how expensive a random call, but if we want to get high rps it's better to avoid uneccesery calls.
But in general we very rarely use `always_apply` flag and we call random in 90% of cases.
Additionally, this benchmark shows a very low impact of random call - aroud 0.7%. But custom code show 5% impact. I am not sure in what test believe more... But 5% is a huge impact.
I think we have nothing to lose with these changes, and merging it will be ok - in a good case we will get 5% speedup.

```python
import random
import time


def call_with_random(flag: bool = False):
    if flag or random.random() < 0.5:
        return True
    return False


def call_without_random(flag: bool = False):
    if random.random() < 0.5 or flag:
        return True
    return False


def bench(func, n: int = int(1e8)) -> float:
    s = time.time()
    for _ in range(n):
        func()
    return time.time() - s


if __name__ == '__main__':
    import timeit
    print(timeit.timeit(""call_with_random()"", setup=""from __main__ import call_with_random"", number=int(1e8)))
    print(timeit.timeit(""call_without_random()"", setup=""from __main__ import call_without_random"", number=int(1e8)))
    print(bench(call_with_random, int(1e8)))
    print(bench(call_without_random, int(1e8)))


# 8.588571457
# 8.528335685000002
# 8.731735467910767
# 8.299729824066162
```",logical sure expensive random call want get high better avoid general rarely use flag call random additionally low impact random call custom code show impact sure test believe huge impact think nothing lose good case get python import random import time flag bool false flag return true return false flag bool false flag return true return false bench float range return import print import print import print bench print bench,issue,positive,negative,neutral,neutral,negative,negative
1951315852,"Also, the new order prioritizes the access of a local variable first (`force_apply`), then an instance variable (`self.always_apply`; slower than the local variable), and finally, the most expensive one (creation of a random variable (`random.random()`) and its comparison to an instance variable (`self.p`)).",also new order access local variable first instance variable local variable finally expensive one creation random variable comparison instance variable,issue,negative,negative,negative,negative,negative,negative
1951310512,"> In practice variable `force_apply or self.always_apply` is typically equal to `false and false = false`
> 
> => What is important is the value of `random.random() < self.p`

If we move `random.random() < self.p` to the last, `random.random()` needn't be called when `force_apply` or `self.always_apply` is True. We fall back to creating a random variable (the most expensive operation for this comparison) only when both `force_apply` and `self.always_apply` are False. 

It's just a minor performance nitpick.
",practice variable typically equal false false false important value move last need true fall back random variable expensive operation comparison false minor performance,issue,positive,negative,negative,negative,negative,negative
1950285152,"How would to i did this (augmentation from scratch) for segmentation task in yolo format. <class id> <x> <y>. I looking for albumentations, but i dont find nothing about this, annotations following polygon yolo format ",would augmentation scratch segmentation task format class id looking dont find nothing following polygon format,issue,negative,neutral,neutral,neutral,neutral,neutral
1947521765,"In practice variable `force_apply or self.always_apply` is typically equal to `false and false = false`

=> What is important is the value of `random.random() < self.p`",practice variable typically equal false false false important value,issue,negative,negative,negative,negative,negative,negative
1942220608,"Possibly related.  If you use PadIfNeeded with RandomScale, it calculates the padding for OpenCVs copyMakeBorder for the initial image/mask size, not from the incoming image/mask from the Compose pipeline.  Since RandomScale doesn't preserve that input shape, it'll be padded to a random size.  Which caused me a hell of a headache trying to figure out why my crops were failing.",possibly related use padding initial size incoming compose pipeline since preserve input shape random size hell headache trying figure failing,issue,negative,negative,negative,negative,negative,negative
1931637950,"Again facing this issue in docker container `nvcr.io/nvidia/pytorch:24.01-py3`. After running `pip install --upgrade pip` followed by `pip install -r requirements.txt`, following content is in `requirements.txt`:

```
timm==0.9.12
hydra-core==1.3.2
wandb==0.16.2
lightning==2.1.4
scikit-image==0.22.0
black==24.1.1
albumentations==1.3.1 --no-binary qudida,albumentations
```

Following versions are installed:

```
$ pip list | grep opencv
opencv                    4.7.0
opencv-python-headless    4.9.0.80
```

Maybe albumentations struggles to use customly compiled opencv?",facing issue docker container running pip install upgrade pip pip install following content following pip list maybe use,issue,negative,neutral,neutral,neutral,neutral,neutral
1917357130,"I was missing
```
additional_targets={""image0"": ""image""},
```
inside `Compose`",missing image image inside compose,issue,negative,negative,negative,negative,negative,negative
1911523673," I've implemented a temporary one, not sure if there will be any potential risks. @ternaus 
```python
class PadToSquare(A.DualTransform):
    """"""Pad image to a square shape (max(height, width) x max(height, width))""""""

    def __init__(self, always_apply=False, p=1.0, value=[128, 128, 128, 128]):
        super().__init__(always_apply, p)
        self.value = value

    def apply(self, img, **params):
        height, width = params['rows'], params['cols']
        max_dim = max(height, width)

        resize = A.LongestMaxSize(max_size=max_dim, always_apply=True)
        pad = A.PadIfNeeded(min_height=max_dim, min_width=max_dim, border_mode=cv2.BORDER_CONSTANT, value=self.value, always_apply=True)
        transform_func = A.Compose(
            [resize, pad],
        )
        img = transform_func(image=img)['image']

        return img
    
    def apply_to_bbox(self, bbox, **params):
        height, width = params['rows'], params['cols']
        max_dim = max(height, width)

        resize = A.LongestMaxSize(max_size=max_dim, always_apply=True)
        pad = A.PadIfNeeded(min_height=max_dim, min_width=max_dim, border_mode=cv2.BORDER_CONSTANT, value=self.value, always_apply=True)
        transform_func = A.Compose(
            [resize, pad],
        )
        bbox = transform_func(bbox=bbox)['bbox']

        return bbox
    
    def apply_to_keypoint(self, keypoint, **params):
        height, width = params['rows'], params['cols']
        max_dim = max(height, width)

        resize = A.LongestMaxSize(max_size=max_dim, always_apply=True)
        pad = A.PadIfNeeded(min_height=max_dim, min_width=max_dim, border_mode=cv2.BORDER_CONSTANT, value=self.value, always_apply=True)
        transform_func = A.Compose(
            [resize, pad],
        )
        keypoint = transform_func(keypoint=keypoint)['keypoint']

        return keypoint

    def get_transform_init_args_names(self):
        return ()
```",temporary one sure potential python class pad image square shape height width height width self super value apply self height width height width resize pad resize pad return self height width height width resize pad resize pad return self height width height width resize pad resize pad return self return,issue,positive,positive,positive,positive,positive,positive
1911318292,"What updates have been recently? I also really want the `PadToSquare` feature, I think it is a common image processing method.",recently also really want feature think common image method,issue,negative,negative,neutral,neutral,negative,negative
1905781632,Is this project still maintained at all or am I howling in the wind here?,project still howling wind,issue,negative,neutral,neutral,neutral,neutral,neutral
1889680615,"Issue persists into the new year, even when no augmentations are applied to yolo format bounding boxes.  Original bounding boxes are within the range 0 to 1. Bounding boxes are transformed into ranges outside of 0 to 1. Likely due to internal normalization of albumentations. 
Replacing the internal normalization process, in favor of standard open cv format to transform both image and bounding box in coco, yolo, or pascal voc format would be a sensible change, given the persistent nature of this bug. ",issue new year even applied format bounding original bounding within range bounding outside likely due internal normalization internal normalization process favor standard open format transform image bounding box coco format would sensible change given persistent nature bug,issue,positive,positive,neutral,neutral,positive,positive
1884010325,"Likewise, despite installing from an environment it seems to perform a local installation, moving the folders to the place indicated by @iswarup solves the problem

",likewise despite environment perform local installation moving place problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1881693405,"yes, it will be really helpful to have a keypoint safe crop mechanism. ",yes really helpful safe crop mechanism,issue,positive,positive,positive,positive,positive,positive
1869195526,"> ## 🐛 Bug
> Hi, when i use Albumentations.Perspective() class to enhance my image, i tryed to set fit_output=True, will get an erro with below: OpenCV(4.8.1) D:\a\opencv-python\opencv-python\opencv\modules\core\src\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 5291741479792 bytes in function 'cv::OutOfMemoryError'
> 
> the images in the list select_obj are not same shape but all of them are very small in range(200, 200, 1).It should take very little time to process one image, but sometimes, it takes a long time for one image
> 
> when i set fit_output=False, the erro is gone
> 
> I tyed in two compute with different os: windows and linux, have the same issue please anyone can help me to fit it, thanks!
> 
> please see the screenshor with my code and erro:
> 
> ## To Reproduce
> ![屏幕截图 2023-11-17 172838](https://private-user-images.githubusercontent.com/23455362/283749403-00c1a697-8739-42e7-af2a-358f14d2e140.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTEiLCJleHAiOjE3MDM1NTYyNDgsIm5iZiI6MTcwMzU1NTk0OCwicGF0aCI6Ii8yMzQ1NTM2Mi8yODM3NDk0MDMtMDBjMWE2OTctODczOS00MmU3LWFmMmEtMzU4ZjE0ZDJlMTQwLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFJV05KWUFYNENTVkVINTNBJTJGMjAyMzEyMjYlMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjMxMjI2VDAxNTkwOFomWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPTgxMTA3MmZkOTBmYjkxNDYyYTI3Zjc1ODAyMDY0NmIxNWEzZjU2MWYxY2QxYjIzMjg1NmU5NTVhZTI1M2JiNzcmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0JmFjdG9yX2lkPTAma2V5X2lkPTAmcmVwb19pZD0wIn0.qbYHc1gZBb1PUO1cbpyU4_XJaFGFjlWZZCBosP8IUuA)
> 
> Steps to reproduce the behavior:
> 
> 
> ## Expected behavior
> ## Environment
> * Albumentations version (e.g., 1.3.1):
> * Python version (e.g., 3.10):
> * OS (e.g., Linux&Windows):
> * How you installed albumentations (`conda`, `pip`,):
> * Any other relevant information:
>  
> ## Additional context

I found the reason, in perspective transform, the max_height and max_width is too large.  I print the matrix,image shape, max_height and max_width. You can see how large them are.
![image](https://github.com/albumentations-team/albumentations/assets/2825718/045aa33d-13bc-4ddd-a13b-5d657753225d)

My solution is  to add this code in __call__ function in BasicTransform class
`            if ('max_height' in params and params['max_height'] > self.max_side) or \
                    ('max_width' in params and params['max_width'] > self.max_side):  # error
                params['matrix'] = np.eye(3)
                params['max_height'] = kwargs['image'].shape[0]
                params['max_width'] = kwargs['image'].shape[1]`
I set max_side to 10000, if the params are too large,  this code will keep the image unchanged




",bug bug hi use class enhance image set get error insufficient memory allocate function list shape small range take little time process one image sometimes long time one image set gone two compute different o issue please anyone help fit thanks please see code reproduce reproduce behavior behavior environment version python version o pip relevant information additional context found reason perspective transform large print matrix image shape see large image solution add code function class error set large code keep image unchanged,issue,positive,positive,positive,positive,positive,positive
1868732960,"I  got same problem, and the imgaug library has the same problem when I use perspective transform.",got problem library problem use perspective transform,issue,negative,neutral,neutral,neutral,neutral,neutral
1861718376,"One reason this could be helpful: We put ground truth labels (bounding boxes) on the full image, then use a tiling service to break that image into smaller tiles. The ground truth labels that are centered in a tile get bundled with that tile, even though they may extend beyond the boundaries of the tile. Then we augment the tiles. If we were to clip them to the image, then the models we train might draw a prediction box around part of the objects we are interested in.

Currently we convert the bounding boxes to keypoints, augment those using  `keypoint_params=A.KeypointParams(format=""xy"", remove_invisible=False)` then convert back to bounding boxes.",one reason could helpful put ground truth bounding full image use tiling service break image smaller ground truth centered tile get tile even though may extend beyond tile augment clip image train might draw prediction box around part interested currently convert bounding augment convert back bounding,issue,positive,positive,positive,positive,positive,positive
1845257105,"I've tried this method and it was totally correct:
```
def read_keypoints(keypoints_path):
    df = pd.read_csv(keypoints_path, delimiter=',', names=[""labels"", ""x"", ""y""])
    # Convert 'x' and 'y' to integer if they are not NaN
    df['x'] = pd.to_numeric(df['x'], errors='coerce').fillna(0).astype(int)
    df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(int)
    return list(zip(df[""x""], df[""y""]))
```
```
def get_augmentation_pipeline():
    return A.Compose([
        A.HorizontalFlip(always_apply=True),
    ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=True))

augmentation_pipeline = get_augmentation_pipeline()
transformed = augmentation_pipeline(image=image, keypoints=keypoints)
print(f""transformed keypoints: {transformed['keypoints']}"")

```",tried method totally correct convert integer nan return list zip return print,issue,negative,neutral,neutral,neutral,neutral,neutral
1837411508,"`BboxProcessor` is an internal class an does not propagated to the top import level.
`from albumentations.core.bbox_utils import BboxProcessor`",internal class top import level import,issue,negative,positive,positive,positive,positive,positive
1823810495,"> Since there are versions of yolo who can have OBB (i.e. https://github.com/hukaixuan19970627/yolov5_obb), it would be nice if someone can give this a little push.. what do you think?

[ultralytics obb](https://github.com/ultralytics/ultralytics/blob/aa3926ce15862921f8a9be8990213f0fd5e8909b/ultralytics/data/augment.py#L779) They haven't achieved it yet",since would nice someone give little push think yet,issue,negative,positive,positive,positive,positive,positive
1821327563,"I found my mistake, which tensor.reshape does not swap dimensions. I made a very serious mistake.",found mistake swap made serious mistake,issue,negative,negative,negative,negative,negative,negative
1821233065,"I don't know what happened，(ノ﹏ヽ)
img plot fn
```
def plot_img(img: np.array = None, title='image', axis='on', color_bar=False):
    plt.figure()
    plt.imshow(img)
    plt.axis(axis)  # 关掉坐标轴为 off
    plt.title(title)  # 图像题目
    if color_bar:
        plt.colorbar()
    plt.show()
```",know plot none axis title,issue,negative,neutral,neutral,neutral,neutral,neutral
1819475390,"I had the same concern as well ~~and I feel like a similar issue occurs with ShiftScaleRotate as the center of the image is set as `(cols - 1) * 0.5, (rows - 1) * 0.5` instead of just `cols * 0.5, rows * 0.5`~~ (it seems that the shift has to happen for ShiftScaleRotate so that the rotation matrix that is retrieved from cv2 is accurate)",concern well feel like similar issue center image set instead shift happen rotation matrix accurate,issue,positive,positive,positive,positive,positive,positive
1806030323,"Not entirely true. As most of my transformation are specialized coordinate system transformation, regardless of the library used, I need to define custom transformation modules. I have done exactly that with albumentations and it works quite well, creating my own  set of pytorch albumentation modules. 

Albumentations has also allowed me to consolidate my codebase, defining modules with paired transformations specific to images and labels at each stage of my data processing pipeline. This is a big benefit. ",entirely true transformation specialized system transformation regardless library used need define custom transformation done exactly work quite well set also consolidate paired specific stage data pipeline big benefit,issue,positive,positive,positive,positive,positive,positive
1805182613,"So you want to use pytorch instead of numpy and opencv for image transformaion? Unfortunately this is impossible now, because all the library logic based on numpy and opencv.
If you wnat transform you images using pytorch look to https://github.com/kornia/kornia or https://github.com/NVIDIA/DALI for fast image processing using gpu",want use instead image unfortunately impossible library logic based transform look fast image,issue,negative,negative,negative,negative,negative,negative
1804247849,"Any updates for this issue? It is quite crucial for border reflections to reflect annotations too as not doing it results in 2 side by side   objects, one annotated and one not",issue quite crucial border reflect side side one one,issue,negative,neutral,neutral,neutral,neutral,neutral
1798878108,"**Pre-Processing:** 
I have computationally expensive image transformations to both prepare the images and augment them. It is preferable to perform these operations on type Tensor as:
- Equivalent Tensor operations are generally much faster (and not forced to float64) 
- The operations can be performed on GPU (incredibly faster)
- Pytorch models run on Tensors and it is preferable to cache data in its final format.

**Post-Processing:** 
I need to undo the transformations and prepare the data for visualization. The same applies, except the input is Tensor format to begin with.  ",expensive image prepare augment preferable perform type tensor equivalent tensor generally much faster forced float incredibly faster run preferable cache data final format need undo prepare data visualization except input tensor format begin,issue,negative,positive,neutral,neutral,positive,positive
1796522873,"> to give somebody a hint based on my own experiments, for image only argumentations. Key to use custom transforms is based on it's interface https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/transforms_interface.py
> 
> see following pipline examples
> 
> ```python
> import cv2
> from albumentations.core.transforms_interface import ImageOnlyTransform
> import albumentations as A
> 
> class RandomA(ImageOnlyTransform):
>     """"""
>     RamdomA transformation
>     """"""
>     def __init__(self, safe_db_lists=[], prob =0.5) -> None:
>         super(RandomA, self).__init__()
>         self.safe_db_lists = safe_db_lists
>         self.prob = prob
> 
>     def apply(self, img, copy=True, **params):
>         if np.random.uniform(0, 1) > self.prob:
>             return img
>         if copy:
>             img = img.copy()
>         # some of your logic here
>         return img
> 
> 
> class RandomB(ImageOnlyTransform):
>     """"""
>     RandomB transformation
>     """"""
>     def __init__(self, safe_db_lists=[], prob =0.5) -> None:
>         super(RandomB, self).__init__()
>         self.safe_db_lists = safe_db_lists
>         self.prob = prob
> 
>     def apply(self, img, copy=True, **params):
>         if np.random.uniform(0, 1) > self.prob:
>             return img
>         if copy:
>             img = img.copy()
>         # some of your logic here
>         return img
> 
> 
> # test compose pipeline
> image = cv2.imread('test.jpg')
> transforms = A.Compose(transforms=[
>     RrandomA(prob=1.0),
>     RandomB(prob=1.0),
> ])
> 
> 
> # and new_image is the target image dict
> new_image_dict = transforms(image=image)
> new_image = new_image_dict['image']
> ```

This works but note that  `ImageOnlyTransform` extends `BasicTransform` which does not get applied to every image. To apply to every input you need to pass `always_apply=True` to 
`super(...).__init__()`
",give somebody hint based image key use custom based interface see following python import import import class transformation self prob none super self prob apply self return copy logic return class transformation self prob none super self prob apply self return copy logic return test compose pipeline image target image work note get applied every image apply every input need pas super,issue,negative,positive,positive,positive,positive,positive
1794423155,"You can catch the error and print or save all the neccesary data, including random seed.
For example:
```python
try:
    seed = random.randrange(sys.maxsize)
    random.seed(seed)
    np.random.seed(seed)
    res = albu_pipe(**data)
except Exception:
    with open(path_to_save, ""wb) as file:
        pickle.dump({""seed"": seed, data: data, ""albu_pipe"": albu_pipe.to_dict()}, file)
    raise
````

If everything is clear to you, please create a PR and make sure that the problem has disappeared.",catch error print save data random seed example python try seed seed seed data except exception open file seed seed data data file raise everything clear please create make sure problem,issue,positive,positive,neutral,neutral,positive,positive
1794268627,"This is a RANDOM behavior resulting from the `randint` in that function so in order to reproduce it you need some 1000 images and a large `max_part_shift` param (say 0.7, but the higher you give the quicker it will happen, say 1.0).

But really, just READ the code...
If `max_part_shift` (in lines 2-3 of the function below) is bigger than 0.5, then `h_max_shift` and `w_max_shift` are basically bigger than half the height/width of the original box. 
These values then dictate the boundaries of the `roundint` (lines 5-6, 8-9 below) for the shift of the new crop box and in very likely random scenarios the resulting boundaries may simply ""switch"" with one another and yield a non-positive box. For example, x_min may be bigger than x_max.

It's really quite clear.

```
    def get_params_dependent_on_targets(self, params: Dict[str, Any]) -> Dict[str, int]:
        bbox = params[self.cropping_bbox_key]
        h_max_shift = round((bbox[3] - bbox[1]) * self.max_part_shift[0])
        w_max_shift = round((bbox[2] - bbox[0]) * self.max_part_shift[1])

        x_min = bbox[0] - random.randint(-w_max_shift, w_max_shift)
        x_max = bbox[2] + random.randint(-w_max_shift, w_max_shift)

        y_min = bbox[1] - random.randint(-h_max_shift, h_max_shift)
        y_max = bbox[3] + random.randint(-h_max_shift, h_max_shift)

        x_min = max(0, x_min)
        y_min = max(0, y_min)
```",random behavior resulting function order reproduce need large param say higher give happen say really read code function bigger basically bigger half original box dictate shift new crop box likely random resulting may simply switch one another yield box example may bigger really quite clear self round round,issue,positive,negative,neutral,neutral,negative,negative
1793834315,Does this problem still exist? Tests work correctly,problem still exist work correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1793832741,"> Firstly, there is a small chance that transformed['image'] is an empty list, 

It looks like problem with your dataloader because error occured before albumentations pipeline call.

> Another issue is that in the original labels, there might be ten instances, but after the transformation, the number of instances could decrease.

Check docs: https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#step-2-define-an-augmentation-pipeline
You can control this behaviour with `BboxParams`
",firstly small chance empty list like problem error pipeline call another issue original might ten transformation number could decrease check control behaviour,issue,negative,positive,neutral,neutral,positive,positive
1793826747,"Unfortunately, without a reproducible example I can not help with this. Please provide an example of data : image shapes, bboxes, labels, keypoints",unfortunately without reproducible example help please provide example data image,issue,negative,negative,negative,negative,negative,negative
1793825847,Right now all transforms expect `image` to be `np.ndarray`. Could you give a minimal example of your approach?,right expect image could give minimal example approach,issue,negative,positive,neutral,neutral,positive,positive
1793825119,Please provide a reproducible example. I  can not reproduce this behaviour.,please provide reproducible example reproduce behaviour,issue,negative,neutral,neutral,neutral,neutral,neutral
1793807777,Just specify a mask for each class and use `masks` instead of `mask`,specify mask class use instead mask,issue,negative,neutral,neutral,neutral,neutral,neutral
1793721930,"Have you tested this feature? I can't get the correct output. Does it work with uint8, uint32, float32?",tested feature ca get correct output work float,issue,negative,neutral,neutral,neutral,neutral,neutral
1780989269,@Ciaran1981 were you able to figure something that worked in your case?,able figure something worked case,issue,negative,positive,positive,positive,positive,positive
1752251717,"> ```python
>  bbox=list(bbox)
>     for i in range(4):
>       if (bbox[i]<0) :
>         bbox[i]=0
>       elif (bbox[i]>1) :
>         bbox[i]=1
>     bbox=tuple(bbox)
> ```

This solution worked for me however I recommend making the following change.
``` python
bbox=list(bbox)
for i in range(4):
  if (bbox[i]<=0) :
    bbox[i]=np.finfo(np.float32).tiny
  elif (bbox[i]>1.0) :
    bbox[i]=1.0
bbox=tuple(bbox)
```

- Use 0.0 and 1.0 to explicitly label the bbox coordinates as float types.
- Add the <= because there is a check that if a yolo-style bounding box ==0, an error is thrown. A ==0 condition also applied to check the input so make sure you don't have 0 as an input for bounding boxes if you're using `yolo` format.",python range solution worked however recommend making following change python range use explicitly label float add check bounding box error thrown condition also applied check input make sure input bounding format,issue,positive,positive,positive,positive,positive,positive
1736873231,"I see in the albumentations tests that `lambda x: x` is passed to `read_fn` but I think that it is not very clear, also `reference_images` could contain any object as long `read_fn` returns a `np.array`.
Maybe only the documentation and type hinting should be updated.",see lambda think clear also could contain object long maybe documentation type,issue,negative,positive,neutral,neutral,positive,positive
1730191412,"@Baakchsu I already managed to fix the issue long ago, but unfortunately I can't reproduce it anymore, even in the case when I have simultaneous installations of `opencv-python` and `opencv-python-headless` (at v4.8.0.76). I'm thinking of closing this issue now as this issue is better explained at https://github.com/albumentations-team/albumentations/issues/1471. The issue has something to do with albumentations being incompatible with a previous version of opencv-python, but that issue no longer occurred to me when I used opencv-python at v4.8.0.76.",already fix issue long ago unfortunately ca reproduce even case simultaneous thinking issue issue better issue something incompatible previous version issue longer used,issue,negative,negative,neutral,neutral,negative,negative
1709260379,Successfully installed it with pip. Seems something wrong with conda install when under conda environments.,successfully pip something wrong install,issue,negative,positive,positive,positive,positive,positive
1704953071,"Hey @starkfire! I once ran into something similar because of dependency issues. Could you please try to manually remove all the pip dependency packages of albumentations that it installed when you ran `pip install albumentations`? 
After that, I would suggest you first do ""pip install opencv-python==4.6.0.66"". For some reason, this version installs quicker than the recent ones. Then, force albumentations using `pip install -U albumentations --no-binary qudida,albumentations`.",hey ran something similar dependency could please try manually remove pip dependency ran pip install would suggest first pip install reason version recent force pip install,issue,negative,positive,neutral,neutral,positive,positive
1696987038,never mind found the problem,never mind found problem,issue,negative,neutral,neutral,neutral,neutral,neutral
1685565239,"Yeah, better to use keras-cv at this point instead of albumentations (for these augmentations)",yeah better use point instead,issue,positive,positive,positive,positive,positive,positive
1680667389,"Couldn't we modify albumentations to augment the RGB channels only for 4 channel images (RGBA and RGB-D?)

Like 

```
def apply(self, image, hue_shift=0, sat_shift=0, val_shift=0, **params):
    if not is_rgb_image(image) and not is_grayscale_image(image) and not is_rgba_image(image):
        raise TypeError(""HueSaturationValue transformation expects 1-channel, 3-channel, or 4-channel images."")
    if is_rgba_image(image):
         return np.concatenate((F.shift_hsv(image[0:3], hue_shift, sat_shift, val_shift), image[3]), axis=0)
    else:
         return F.shift_hsv(image, hue_shift, sat_shift, val_shift)
```

instead of 

```
def apply(self, image, hue_shift=0, sat_shift=0, val_shift=0, **params):
    if not is_rgb_image(image) and not is_grayscale_image(image):
        raise TypeError(""HueSaturationValue transformation expects 1-channel or 3-channel images."")
    return F.shift_hsv(image, hue_shift, sat_shift, val_shift)
```
here: https://github.com/albumentations-team/albumentations/blob/e3b47b3a127f92541cfeb16abbb44a6f8bf79cc8/albumentations/augmentations/transforms.py#L1005C21-L1005C21

I created a comment about it [here,](https://github.com/albumentations-team/albumentations/issues/1473) but found this discussion after deeper google searching",could modify augment channel like apply self image image image image raise transformation image return image image else return image instead apply self image image image raise transformation return image comment found discussion searching,issue,negative,neutral,neutral,neutral,neutral,neutral
1679429445,"The bug is defiantly there, I'm assuming some numbers after decimal point somehow is messing up the calculation in the backed, you can try limiting the numbers (your bbox label values) after decimal point to 6 or 10 and try again. 

Library has critical bugs for sure. You might have to find a new library for augmentation if the issue persists for you",bug defiantly assuming decimal point somehow messing calculation backed try limiting label decimal point try library critical sure might find new library augmentation issue,issue,negative,positive,positive,positive,positive,positive
1674215764,"> @sainisatish me too....
> 
> ```
> # Define transform pipeline
> transform = A.Compose([
>     A.RandomResizedCrop(width=1280, height=1280, scale=(0.2, 2)),
>     A.HorizontalFlip(p=0.5)
> ], bbox_params=A.BboxParams(format='yolo', min_visibility=0.8))
> ```
@PawelKinczyk As of now I have dropped the images that display the mention error.
",define transform pipeline transform display mention error,issue,negative,neutral,neutral,neutral,neutral,neutral
1673451992,"@sainisatish me too.... 
```
# Define transform pipeline
transform = A.Compose([
    A.RandomResizedCrop(width=1280, height=1280, scale=(0.2, 2)),
    A.HorizontalFlip(p=0.5)
], bbox_params=A.BboxParams(format='yolo', min_visibility=0.8))
```",define transform pipeline transform,issue,negative,neutral,neutral,neutral,neutral,neutral
1669751200,"I am facing below error ->
Version - 1.3.1
Bounding Box = [[0.56195, 0.738133, 0.125, 0.181567],
                           [0.046181, 0.64675, 0.092363, 0.131267],
                           [0.271012, 0.6556, 0.375, 0.457233]]
Format - Yolo
Error - Expected x_min for bbox (-5.000000000005e-07, 0.5811165, 0.0923625, 0.7123835000000001, 5) to be in the range [0.0, 1.0], got -5.000000000005e-07.",facing error version bounding box format error range got,issue,negative,neutral,neutral,neutral,neutral,neutral
1661297854,"Hi, does this 'boundary checking' solution also work for Affine transformation like rotation, translation or even shearing?",hi solution also work affine transformation like rotation translation even shearing,issue,positive,neutral,neutral,neutral,neutral,neutral
1649659222,Any update on the AugMix as well as other automatic DA methods?,update well automatic da,issue,negative,neutral,neutral,neutral,neutral,neutral
1635411795,"An addition to this bug, the commit actually introduced some inconsistencies between the kwargs of `transform`. If one introduce bboxes with the modification bellow:

```python
import numpy as np
from albumentations.core.composition import Compose
from albumentations.augmentations import Crop

img_shape = (100, 100)

image = np.ones(img_shape)
masks = None
bboxes = None

transform = Compose([Crop(always_apply=False, p=1.0, x_min=20, y_min=50, x_max=30, y_max=60)],
                    bbox_params={'format': 'pascal_voc', min_area': 0.0, 'min_visibility': 0.1, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True})
res = transform(image=image, masks=masks, bboxes=bboxes)
print(res)
```

One get the following error:
```
Traceback (most recent call last):
  File ""test.py"", line 13, in <module>
    res = transform(image=image, masks=masks, bboxes=bboxes)
  File ""/home/moj2rng/Code/aps_training/.venv/lib/python3.8/site-packages/albumentations/core/composition.py"", line 199, in __call__
    p.ensure_data_valid(data)
  File ""/home/moj2rng/Code/aps_training/.venv/lib/python3.8/site-packages/albumentations/core/bbox_utils.py"", line 112, in ensure_data_valid
    data_exists = data_name in data and len(data[data_name])
TypeError: object of type 'NoneType' has no len()
```

Therefore with albumentations 1.3.1 the way to work with unlabelled data would be:
```python
import numpy as np
from albumentations.core.composition import Compose
from albumentations.augmentations import Crop

img_shape = (100, 100)

image = np.ones(img_shape)
masks = None
bboxes = []

transform = Compose([Crop(always_apply=False, p=1.0, x_min=20, y_min=50, x_max=30, y_max=60)],
                    bbox_params={'format': 'pascal_voc', 'min_area': 0.0, 'min_visibility': 0.1, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True})
res = transform(image=image, masks=masks, bboxes=bboxes)
print(res)
```

I think one way should be chosen, applied everywhere, and tested.

",addition bug commit actually transform one introduce modification bellow python import import compose import crop image none none transform compose crop true transform print one get following error recent call last file line module transform file line data file line data data object type therefore way work data would python import import compose import crop image none transform compose crop true transform print think one way chosen applied everywhere tested,issue,positive,positive,positive,positive,positive,positive
1612522697,"to give somebody a hint based on my own experiments, for image only argumentations. Key to use custom transforms is based on it's interface
https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/transforms_interface.py

see following pipline examples
```python
import cv2
from albumentations.core.transforms_interface import ImageOnlyTransform
import albumentations as A

class RandomA(ImageOnlyTransform):
    """"""
    RamdomA transformation
    """"""
    def __init__(self, safe_db_lists=[], prob =0.5) -> None:
        super(RandomA, self).__init__()
        self.safe_db_lists = safe_db_lists
        self.prob = prob

    def apply(self, img, copy=True, **params):
        if np.random.uniform(0, 1) > self.prob:
            return img
        if copy:
            img = img.copy()
        # some of your logic here
        return img


class RandomB(ImageOnlyTransform):
    """"""
    RandomB transformation
    """"""
    def __init__(self, safe_db_lists=[], prob =0.5) -> None:
        super(RandomB, self).__init__()
        self.safe_db_lists = safe_db_lists
        self.prob = prob

    def apply(self, img, copy=True, **params):
        if np.random.uniform(0, 1) > self.prob:
            return img
        if copy:
            img = img.copy()
        # some of your logic here
        return img


# test compose pipeline
image = cv2.imread('test.jpg')
transforms = A.Compose(transforms=[
    RrandomA(prob=1.0),
    RandomB(prob=1.0),
])


# and new_image is the target image dict
new_image_dict = transforms(image=image)
new_image = new_image_dict['image']
```",give somebody hint based image key use custom based interface see following python import import import class transformation self prob none super self prob apply self return copy logic return class transformation self prob none super self prob apply self return copy logic return test compose pipeline image target image,issue,negative,positive,positive,positive,positive,positive
1604950419,"I will pick this up again if the `multi-input / single-output` PR gets merged. Not worth investing the time of adapting this, if it is not happening",pick worth time happening,issue,negative,positive,positive,positive,positive,positive
1604934580,"> I see, yes, makes sense to have `Compose` for multi-input / single-output images.
> 
> > I have been thinking of this issue for the past few days and have just started writing a PoC. I will let you know when it is ready.
> 
> I can try it out when it is done 😄

Did you manage to work this PR out with [https://github.com/albumentations-team/albumentations/pull/1420](url) implementation?

Any plans to merge this?",see yes sense compose thinking issue past day writing let know ready try done manage work implementation merge,issue,positive,negative,neutral,neutral,negative,negative
1604933639,"> Any plans of getting this merged any time soon?

I'm also very interested on this matter. Are there any plans on merging this PR?",getting time soon also interested matter,issue,negative,positive,positive,positive,positive,positive
1588862630,"This should be fixed now with the change from `np.int` to `np.int32` here: 

```python
height_split = np.linspace(0, height, n + 1, dtype=np.int32)
width_split = np.linspace(0, width, m + 1, dtype=np.int32)
```
in https://github.com/albumentations-team/albumentations/blob/e3b47b3a127f92541cfeb16abbb44a6f8bf79cc8/albumentations/augmentations/transforms.py#LL138C1-L139C67 so this can be closed.",fixed change python height width closed,issue,negative,neutral,neutral,neutral,neutral,neutral
1588831609,"> 

when you are passing **""bboxes""** in **label_feilds** of **A.Bboxparams()** then it understands that you have kept your labels just after the bounding box details e.g: your boxes looks like `boxes=[[0.1, 0.1, 0.01, 0.01,<your class label>],[0.1, 0.1, 0.01, 0.01,<your class label>]]` but you are not providing the class labels in last of each box hence it trims hence it assumes the last  element as class label that is why you are only getting three values in each box in output bboxes.

If you want to provide the labels in seperate list you have to give **""class_labels""** in the label_feilds. means `label_feilds=[""class_labels""]`
<------------------------------------**_1st Method_**--------------------------------------------->
#You just have to change the label_feilds value to ""class_labels"" 
#label_fields=[""class_labels""] -> it means that you are providing the labels externally 
import numpy as np
import albumentations as A

album=A.Compose([
A.Resize(height=64, width=64)
, A.RandomScale(p=1.0, scale_limit=(-0.5, 0.5))
],bbox_params=A.BboxParams(format=""yolo"", label_fields=[""class_labels""]))

img=np.random.randint(112, 128, [1280, 1280, 3], dtype=np.uint8)

#you can also do like 
boxes = [[0.1, 0.1, 0.01, 0.01],[0.3, 0.5, 0.01, 0.01]]
cls=[0,1]

#now you do not need to pass class_labels here as you have already included it into bboxes
r = album(image=img, bboxes=boxes,class_labels=cls, cropping_bbox=boxes[0] )
print(""bboxes are : "",r[""bboxes""])
print(""labels are : "",r[""class_labels""])

**Output: -**
bboxes are :  [(0.1, 0.1, 0.009999999999999995, 0.009999999999999995), (0.3, 0.5, 0.010000000000000009, 0.010000000000000009)]
labels are :  [0, 1]
<------------------------------------------**_2nd Method_**-------------------------------------->
#Here is your code if you are passing label_fields=[""bboxes""] 
#label_fields=[""bboxes""] -. it means that you are providing labels along with the box not separately.
#so if you want to do like this then the below code will do it
#You have to pass labels along with each box
import numpy as np
import albumentations as A

album=A.Compose([
A.Resize(height=64, width=64)
, A.RandomScale(p=1.0, scale_limit=(-0.5, 0.5))
],bbox_params=A.BboxParams(format=""yolo"", label_fields=[""bboxes""]))

img=np.random.randint(112, 128, [1280, 1280, 3], dtype=np.uint8)

#you can also do like 
boxes = [[0.1, 0.1, 0.01, 0.01],[0.3, 0.5, 0.01, 0.01]]
cls=[0,1]
#you have to add class of each box at last of box itself (as you are passing ""bboxes"" in label_feilds in A.BboxParams) 
boxes=[box+[label] for box,label in zip(boxes,cls)]

#now you do not need to pass class_labels here as you ave already included it into bboxes
r = album(image=img, bboxes=boxes,class_labels=cls, cropping_bbox=boxes[0] )
print(""bboxes are : "",r[""bboxes""])
print(""labels are : "",r[""class_labels""])

**Output :-**
bboxes are :  [[0.1, 0.1, 0.01, 0.01], [0.3, 0.5, 0.01, 0.01]]
labels are :  [0, 1]
<------------------------------------------------------------------------------------------>

Please visit this link for more details: [https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/](url)",passing kept bounding box like class label class label providing class last box hence hence last element class label getting three box output want provide list give change value providing externally import import also like need pas already included album print print output code passing providing along box separately want like code pas along box import import also like add class box last box passing label box label zip need pas ave already included album print print output please visit link,issue,positive,neutral,neutral,neutral,neutral,neutral
1588785454,"@rpgit12 or maybe you can help, seems like @albu isn't available. ",maybe help like available,issue,positive,positive,positive,positive,positive,positive
1584428344,"I noticed that my error from above has already been fixed with PR #1370 some time ago, so I'd say you can close this issue. We'll just have to wait for the next albumentations release and the incompatibilities with numpy >= 1.24 should be resolved.",error already fixed time ago say close issue wait next release resolved,issue,negative,positive,neutral,neutral,positive,positive
1581901543,"I have deleted the conda environment and started from scratch to work around the problem.

Was gonna close the issue, but I will leave it open for now.",environment scratch work around problem gon na close issue leave open,issue,negative,neutral,neutral,neutral,neutral,neutral
1580403593,"Importing works fine for me, but I get the same deprecation error message when applying a transformation using NumPy >= 1.24:

```
albumentations              1.3.0
numpy                       1.24.3
python                      3.10.6
qudida                      0.0.4
```

```
File /usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py:205, in Compose.__call__(self, force_apply, *args, **data)
    202     p.preprocess(data)
    204 for idx, t in enumerate(transforms):
--> 205     data = t(**data)
    207     if check_each_transform:
    208         data = self._check_data_post_transform(data)

File /usr/local/lib/python3.10/dist-packages/albumentations/core/transforms_interface.py:109, in BasicTransform.__call__(self, force_apply, *args, **kwargs)
    105     assert all(key in kwargs for key in self.targets_as_params), ""{} requires {}"".format(
    106         self.__class__.__name__, self.targets_as_params
    107     )
    108     targets_as_params = {k: kwargs[k] for k in self.targets_as_params}
--> 109     params_dependent_on_targets = self.get_params_dependent_on_targets(targets_as_params)
    110     params.update(params_dependent_on_targets)
    111 if self.deterministic:

File /usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:143, in RandomGridShuffle.get_params_dependent_on_targets(self, params)
    140 if n > height // 2 or m > width // 2:
    141     raise ValueError(""Incorrect size cell of grid. Just shuffle pixels of image"")
--> 143 height_split = np.linspace(0, height, n + 1, dtype=np.int)
    144 width_split = np.linspace(0, width, m + 1, dtype=np.int)
    146 height_matrix, width_matrix = np.meshgrid(height_split, width_split, indexing=""ij"")

File /usr/local/lib/python3.10/dist-packages/numpy/__init__.py:305, in __getattr__(attr)
    300     warnings.warn(
    301         f""In the future `np.{attr}` will be defined as the ""
    302         ""corresponding NumPy scalar."", FutureWarning, stacklevel=2)
    304 if attr in __former_attrs__:
--> 305     raise AttributeError(__former_attrs__[attr])
    307 # Importing Tester requires importing all of UnitTest which is not a
    308 # cheap import Since it is mainly used in test suits, we lazy import it
    309 # here to save on the order of 10 ms of import time for most users
    310 #
    311 # The previous way Tester was imported also had a side effect of adding
    312 # the full `numpy.testing` namespace
    313 if attr == 'testing':

AttributeError: module 'numpy' has no attribute 'int'.
`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
```",work fine get deprecation error message transformation python file self data data enumerate data data data data file self assert key key file self height width raise incorrect size cell grid shuffle image height width file future defined corresponding scalar raise tester cheap import since mainly used test lazy import save order import time previous way tester also side effect full module attribute alias avoid error code use modify behavior safe may wish use specify precision wish review current use check release note link additional information originally guidance see original release note,issue,positive,positive,positive,positive,positive,positive
1580032830,"This is weird. I have the same version of numpy. What exact versions of libs do you have?
For me:
```
qudida==0.0.4
numpy==1.24.3
```

To check `qudida` version write:
```
pip freeze | grep qudida
```",weird version exact check version write pip freeze,issue,negative,negative,negative,negative,negative,negative
1577802064,"@Dipet
Hi, is there anything that still needs to be changed or that I can help with?",hi anything still need help,issue,negative,neutral,neutral,neutral,neutral,neutral
1575642628,"Try to update python. In python3.11.3 everything works fine.
It looks like you are using one of the very first versions of python3.11 and it is not stable.",try update python python everything work fine like one first python stable,issue,positive,positive,positive,positive,positive,positive
1575542314,"```(venv) aj@aj-laptop:~/repo/deep_fashion_2$ ipython
Python 3.11.0rc1 (main, Aug 12 2022, 10:02:14) [GCC 11.2.0]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.14.0 -- An enhanced Interactive Python. Type '?' for help.

In [1]: %cpaste
Pasting code; enter '--' alone on the line to stop or use Ctrl-D.
:import albumentations as A
import cv2
from albumentations.pytorch import ToTensorV2
import numpy as np

transforms = A.Compose(
    [
        A.MedianBlur(p=1.),
        A.Normalize(),
        ToTensorV2(),
    ]
)

image = np.empty([100, 100, 3], dtype=np.uint8)
image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
transformed = transforms(image=image):::::::::::::::
:--
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[1], line 16
     14 image = np.empty([100, 100, 3], dtype=np.uint8)
     15 image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
---> 16 transformed = transforms(image=image)

File ~/repo/deep_fashion_2/venv/lib/python3.11/site-packages/albumentations/core/composition.py:205, in Compose.__call__(self, force_apply, *args, **data)
    202     p.preprocess(data)
    204 for idx, t in enumerate(transforms):
--> 205     data = t(**data)
    207     if check_each_transform:
    208         data = self._check_data_post_transform(data)

File ~/repo/deep_fashion_2/venv/lib/python3.11/site-packages/albumentations/core/transforms_interface.py:102, in BasicTransform.__call__(self, force_apply, *args, **kwargs)
     99     return kwargs
    101 if (random.random() < self.p) or self.always_apply or force_apply:
--> 102     params = self.get_params()
    104     if self.targets_as_params:
    105         assert all(key in kwargs for key in self.targets_as_params), ""{} requires {}"".format(
    106             self.__class__.__name__, self.targets_as_params
    107         )

File ~/repo/deep_fashion_2/venv/lib/python3.11/site-packages/albumentations/augmentations/blur/transforms.py:44, in Blur.get_params(self)
     43 def get_params(self) -> Dict[str, Any]:
---> 44     return {""ksize"": int(random.choice(np.arange(self.blur_limit[0], self.blur_limit[1] + 1, 2)))}

File /usr/lib/python3.11/random.py:369, in Random.choice(self, seq)
    367 def choice(self, seq):
    368     """"""Choose a random element from a non-empty sequence.""""""
--> 369     if not seq:
    370         raise IndexError('Cannot choose from an empty sequence')
    371     return seq[self._randbelow(len(seq))]

ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

In [2]: A.__version__
Out[2]: '1.3.0'

In [3]: 
```

Here comes the full error log that I get when running your snippet. Did you run it on python3.11 and albumentations=1.3.0?",python main type information enhanced interactive python type help pasting code enter alone line stop use import import import import image image image recent call last cell line image image image file self data data enumerate data data data data file self return assert key key file self self return file self choice self choose random element sequence raise choose empty sequence return truth value array one element ambiguous use come full error log get running snippet run python,issue,negative,negative,neutral,neutral,negative,negative
1575408026,"Can not reproduce the problem
```python
import albumentations as A
import cv2
from albumentations.pytorch import ToTensorV2
import numpy as np

transforms = A.Compose(
    [
        A.MedianBlur(p=1.),
        A.Normalize(),
        ToTensorV2(),
    ]
)

image = np.empty([100, 100, 3], dtype=np.uint8)
image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
transformed = transforms(image=image)
```",reproduce problem python import import import import image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
1572529188,"This is still an Issue today which is absolutely strange!
",still issue today absolutely strange,issue,negative,negative,neutral,neutral,negative,negative
1572207528,Please provide a reproducible example,please provide reproducible example,issue,negative,neutral,neutral,neutral,neutral,neutral
1568212093,@albu Can you please also give an example with fixed commit-hash links? Looks like there were a lot of changes since you've posted above and now links in that comment pointing to something completely different :( ,please also give example fixed link like lot since posted link comment pointing something completely different,issue,positive,positive,neutral,neutral,positive,positive
1565672857,Apparently the problem is not related to Albumentation clearly the source of the issue comes from a datapoint that has to be of None type.,apparently problem related clearly source issue come none type,issue,negative,positive,neutral,neutral,positive,positive
1565199833,Check your data or give reproducible example. It looks like a problem with your data,check data give reproducible example like problem data,issue,negative,neutral,neutral,neutral,neutral,neutral
1565195881,"Thank you for your work, but we already have a repo with our demo: https://github.com/albumentations-team/albumentations-demo
If you want to add some features, please create a PR in that repository.
It looks like your app has some bugs.",thank work already want add please create repository like,issue,positive,neutral,neutral,neutral,neutral,neutral
1565194524,"Yes it is expected. In common user waits that images after transforms will be contiguous, also some transform could work incorrectly with non contiguous images.",yes common user contiguous also transform could work incorrectly non contiguous,issue,negative,negative,negative,negative,negative,negative
1565194100,Now everything is working fine. There may have been a server shutdown for a short time.,everything working fine may server shutdown short time,issue,negative,positive,positive,positive,positive,positive
1545901202,"@Dipet , @creafz , @ternaus @BloodAxe, @albu Could you please look into this and if everything is fine, can you merge this PR ? 

This would help others to test, explore and play with different filters of Albumentations.",could please look everything fine merge would help test explore play different,issue,positive,positive,positive,positive,positive,positive
1541444287,"@ynhuhu 
did you use any thing like transpose or ToTorchV2 layers in your Compose?
or probably you can provide that piece of code? Thanks~
",use thing like transpose compose probably provide piece code,issue,negative,neutral,neutral,neutral,neutral,neutral
1540703015,"I guess the solution is to apply `A.Normalize(mean=[0, 0, 0], std=[1, 1, 1])` to divide by `max_pixel_value=255`",guess solution apply divide,issue,negative,neutral,neutral,neutral,neutral,neutral
1540683704,"@Dipet no, this is not true: 👎 https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensor

I have had the same issue

I just need to apply a divide by 255 on the image, not to normalize it to bring it to [0, 1]

There is no way today to do it in albumentations. You need to transform your image to [0, 1] using PIL or numpy or cv2 before passing it to the Compose of transforms",true issue need apply divide image normalize bring way today need transform image passing compose,issue,negative,positive,positive,positive,positive,positive
1535321743,"After some investigation, I figure it out: Albuminations does not take the same channel format as Pytorch -> image should be (W ,H ,C).
It works that way. ",investigation figure take channel format image work way,issue,negative,neutral,neutral,neutral,neutral,neutral
1534492161,"I ended up defining my own class for the augmentation, so this question is no longer relevant.
For completeness, my StackOverflow question (with my own answer) can be found here: https://stackoverflow.com/questions/76125174/custom-mask-dependent-augmentation-in-albumentations/",ended class augmentation question longer relevant completeness question answer found,issue,negative,positive,positive,positive,positive,positive
1527301424,"I didn't describe clearly. Before dataloader, my input image shape is (224,224,12), mask shape is (224,224,1).",describe clearly input image shape mask shape,issue,negative,positive,positive,positive,positive,positive
1525880026,"In albumentations, usually an image or mask is defined in channel-last style, thus you'll have to transpose it like
```python
image = image.transpose(1, 2, 0)
```",usually image mask defined style thus transpose like python image,issue,negative,negative,negative,negative,negative,negative
1521458442,"I have same issues with opencv, 
I can't use cv2.imwrite() although I can use cv2.imread() ",ca use although use,issue,negative,neutral,neutral,neutral,neutral,neutral
1512244861,"I would like to add more randomness to the augmentation process.

In this regard, Is there a way to apply different albumentation pipeline to each training image for instance? Currently all the parameters are fixed before training begins.",would like add randomness augmentation process regard way apply different pipeline training image instance currently fixed training,issue,negative,positive,neutral,neutral,positive,positive
1508007689,"Though one should ideally do something like https://github.com/albumentations-team/albumentations/pull/490/files ie modify functional.py, test_transforms.py and transforms but if you already have a long albumentation augmentations pipeline and just want to add aug in between than one can use something like:

Depending upon the use case one can inherit ImageOnlyTransform, DualTransform etc
There are a lot of other methods that one can too use like apply_to_keypoints etc

 ```
class MyCustomAug(DualTransform):
    def __init__(self, parameter_1, parameter_2, always_apply=True, p=1.0):
        super().__init__(always_apply, p)
        self.parameter_1 = parameter_1
        self.parameter_2 = parameter_2
    def apply(self, img, **params):
        #apply augmentation to images
    def apply_to_mask(self, mask, **params):
        #apply augmentation to masks
```",though one ideally something like ie modify already long pipeline want add one use something like depending upon use case one inherit lot one use like class self super apply self apply augmentation self mask apply augmentation,issue,positive,positive,positive,positive,positive,positive
1506696109,"> Hi, I installed the master branch from Github to fix the bug. But now, I obtain the error:
> 
> ```
>     transformed = self.transforms(image=image, keypoints=annos[:, 1:3], labels=annos[:, 0], shapes=annos[:, 3:5])
>   File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/composition.py"", line 210, in __call__
>     data = t(**data)
>   File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/transforms_interface.py"", line 118, in __call__
>     return self.apply_with_params(params, **kwargs)
>   File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/transforms_interface.py"", line 131, in apply_with_params
>     res[key] = target_function(arg, **dict(params, **target_dependencies))
>   File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/augmentations/dropout/coarse_dropout.py"", line 170, in apply_to_keypoints
>     result = set(keypoints)
> TypeError: unhashable type: 'numpy.ndarray'
> ```
> 
> Any idea? The error doesn't occur when the key point is not covered by CoarseDropout.

@bartoszptak you are probably using `np.ndarray` as keypoints, you should be using list of tuples.
Check https://albumentations.ai/docs/getting_started/keypoints_augmentation/ the code works correctly with such keypoints.",hi master branch fix bug obtain error file line data data file line return file line key file line result set type idea error occur key point covered probably list check code work correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1506459289,"Hi, I installed the master branch from Github to fix the bug. But now, I obtain the error:
```
    transformed = self.transforms(image=image, keypoints=annos[:, 1:3], labels=annos[:, 0], shapes=annos[:, 3:5])
  File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/composition.py"", line 210, in __call__
    data = t(**data)
  File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/transforms_interface.py"", line 118, in __call__
    return self.apply_with_params(params, **kwargs)
  File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/core/transforms_interface.py"", line 131, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""/home/bartosz/miniconda3/envs/count/lib/python3.10/site-packages/albumentations/augmentations/dropout/coarse_dropout.py"", line 170, in apply_to_keypoints
    result = set(keypoints)
TypeError: unhashable type: 'numpy.ndarray'
```

Any idea? The error doesn't occur when the key point is not covered by CoarseDropout.",hi master branch fix bug obtain error file line data data file line return file line key file line result set type idea error occur key point covered,issue,negative,neutral,neutral,neutral,neutral,neutral
1503793731,"I am getting the error as mentioned, i am trying to build a deep face recognition system using open cv and tensor flow, this error is occuring while data augmentation
line of code :
augmented = augmentor(image=img, bboxes=[coords], class_labels=['Saumya'])

![Screenshot 2023-04-11 223952](https://user-images.githubusercontent.com/114299936/231238238-2bff9cbe-6d30-4ec9-be79-625de7ce9bd6.png)
",getting error trying build deep face recognition system open tensor flow error data augmentation line code augmented,issue,negative,neutral,neutral,neutral,neutral,neutral
1501010682,"Hi @tobycollins, the `color` parameter was added in PR #1305, which was merged into the master branch after the release of version 1.3.0. Maybe wait for the next release version or clone the repo.",hi color parameter added master branch release version maybe wait next release version clone,issue,negative,neutral,neutral,neutral,neutral,neutral
1501008579,"Hi @aminzakizebarjad, version 1.3.0 was released in on 20th September 2022, but the ToRGB class was added on 25th October 2022 to the repo. So wait for a future release or clone the repo, if you want to use this new feature.",hi version th class added th wait future release clone want use new feature,issue,negative,positive,neutral,neutral,positive,positive
1497683381,"The problem is probably that, inside `A.Compose`, you used a transform function which does not come from `albumentations`, this external transform function does not have the keyword argument ""force_apply"". 

To solve this problem, you can apply this external transform function outside `A.Compose`",problem probably inside used transform function come external transform function argument solve problem apply external transform function outside,issue,negative,neutral,neutral,neutral,neutral,neutral
1490797948,After much debugging the problem was due to negative values in bounding boxes data for their `x` and `y` coordinates as well as heights and widths of bounding boxes.,much problem due negative bounding data well bounding,issue,negative,negative,neutral,neutral,negative,negative
1489234510,"This problem is still present in version `1.3.0`:
```
ValueError(f""Expected {name} for bbox {bbox} to be in the range [0.0, 1.0], got {value}."")
ValueError: Expected y_max for bbox (0.14792899408284024, 0.9671641791044776, 0.28205128205128205, 1.0029850746268656, tensor(2)) to be in the range [0.0, 1.0], got 1.0029850746268656.
```

 I am using the following augmentations for an object detection problem:
```
def get_train_transforms():
    return A.Compose(
        [
            A.Resize(height=config.RESOLUTION, width=config.RESOLUTION, p=1),
            A.Normalize(p=1),
            ToTensorV2(p=1.0),
        ], 
        p=1.0, 
        bbox_params=A.BboxParams(
            format='pascal_voc',
            min_area=0, 
            min_visibility=0,
            label_fields=['labels']
        )
    )

def get_valid_transforms():
    return A.Compose(
        [
            A.Resize(height=config.RESOLUTION, width=config.RESOLUTION, p=1.0),
            A.Normalize(p=1),
            ToTensorV2(p=1.0),
        ], 
        p=1.0, 
        bbox_params=A.BboxParams(
            format='pascal_voc',
            min_area=0, 
            min_visibility=0,
            label_fields=['labels']
        )
    )
```",problem still present version name range got value tensor range got following object detection problem return return,issue,negative,neutral,neutral,neutral,neutral,neutral
1487877221,"Code:
```
mask = np.load(label['mask_file'])
mask1 = cv2.resize(mask, (600,600), interpolation=cv2.INTER_NEAREST)
mask2 = cv2.resize(mask, (600,600), cv2.INTER_NEAREST)
```
Results:
![mask](https://user-images.githubusercontent.com/8948724/228415111-87dcc791-172f-467e-bcf0-fcf77766c949.png)
![mask1](https://user-images.githubusercontent.com/8948724/228415152-456005fb-a3a0-43c8-908e-8664cfefad2f.png)
![mask2](https://user-images.githubusercontent.com/8948724/228415167-8f6884a9-cb28-465c-9dfe-f406372c3a0b.png)
",code mask label mask mask mask mask mask mask mask,issue,negative,neutral,neutral,neutral,neutral,neutral
1487867787,"Maybe it can be done by replacing  ```mask_new = cv2.resize(mask_old, (w, h), cv2.INTER_NEAREST)```  with ```mask_new = cv2.resize(mask_old, (w, h), interpolation=cv2.INTER_NEAREST)```,  but I have not found the reason yet.",maybe done found reason yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1486693599,"We have switched to imgaug for augmentations now. I might not respond on this issue, but the bug remains",switched might respond issue bug remains,issue,negative,neutral,neutral,neutral,neutral,neutral
1483206797,This is the basic idea behind normalization - to normalize values around 0 and 1. It must change dtype to float,basic idea behind normalization normalize around must change float,issue,negative,negative,negative,negative,negative,negative
1483203906,It looks like your file is corrupted. That not albumentations issue,like file corrupted issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1483079467,"@victor1cea, is there a way to receive the bboxes even if it's ouside the image? I can clip the bboxes to the max image size later. I'm constantly getting empty bboxes when the image is rotated.",way receive even image clip image size later constantly getting empty image rotated,issue,negative,negative,neutral,neutral,negative,negative
1482380462,"I've faced the same issue and have a more straightforward solution. The idea is to use indices with label fields for transformation. Here is what it looks like:

```

transformations = A.Compose([
...
    ],
        bbox_params=A.BboxParams(format='pascal_voc', label_fields=[..., 'indices'], min_area=16,
                                 min_visibility=0.1))

...

sample = self.transform(image=image,
                                bboxes=target['boxes'],
                                masks=instance_masks,
                                ...,
                                indices=np.array(list(range(len(target['boxes'])))))

transformed_masks = np.array(sample['masks'])[sample['indices']]
```
",faced issue straightforward solution idea use index label transformation like sample list range target sample sample,issue,positive,positive,positive,positive,positive,positive
1481830368,I'm also facing this issue and agree ^^^,also facing issue agree,issue,negative,neutral,neutral,neutral,neutral,neutral
1473308545,I agree with @jveitchmichaelis that being able to disable box filtering or being able to filter masks along with the bounding boxes would be a nice feature to have. ,agree able disable box filtering able filter along bounding would nice feature,issue,positive,positive,positive,positive,positive,positive
1472439054,I'm facing the same issue when using Mask RCNN from torchvision (instance segmentation). It could be nice to have an option to remove empty masks in that particular use case,facing issue mask instance segmentation could nice option remove empty particular use case,issue,negative,positive,positive,positive,positive,positive
1470038903,"@lilianabrandao My workaround looks to be working in Colab.
What pipeline did you use?
The code I tried is here

```py
import torch
import numpy as np
import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2
import cv2

def toGray(image, **params):
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    return gray

transform_A = A.ReplayCompose([
    A.Lambda(name='togray',image=toGray),
    A.Resize(256,256),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.2, rotate_limit=30, p=0.5, border_mode=cv2.BORDER_CONSTANT),
    A.RandomBrightnessContrast(p=0.5),
    A.Normalize((0.5,),(0.5,), always_apply=True),
    ToTensorV2()
])

image = np.random.randint(0, 256, size=(100, 100, 3), dtype=np.uint8)

re_param = transform_A(image=image)['replay']
lambda_transforms = {lam.name : lam for lam in transform_A if isinstance(lam, A.Lambda)}
augs = transform_A._restore_for_replay(re_param, lambda_transforms=lambda_transforms)
augs(force_apply=True, image=image)

# re_param = transform_A(image=image)['replay']
# transform_A.replay(re_param,image=image)
```",working pipeline use code tried import torch import import import import image gray image return gray image lam lam lam,issue,negative,neutral,neutral,neutral,neutral,neutral
1469296117,Closed as I accidentally pushed to origin repo instead of my fork.,closed accidentally origin instead fork,issue,negative,negative,neutral,neutral,negative,negative
1466875144,"> Adding my solution (might help someone else ) If you are sure there is no problem in your bounding boxes ,only floating point precision issue then you can add this block to the function
> 
> ```python
> def check_bbox(bbox):
>     """"""Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums""""""
>    #my added block 
>     bbox=list(bbox)
>     for i in range(4):
>       if (bbox[i]<0) :
>         bbox[i]=0
>       elif (bbox[i]>1) :
>         bbox[i]=1
>     bbox=tuple(bbox)
>    #end of block
>    #rest of the code as it is
> ```
> 
> the directory
> 
> ```
> /usr/local/lib/python3.6/dist-packages/albumentations/augmentations/bbox_utils.py
> ```
> 
> N.B : also check the indentation when add the code segment

For those of you of you running into this issue when using the [Cut and Paste Augmentation Technique](https://www.kaggle.com/code/slavkoprytula/detectron2-copy-paste-augmentation), make the change to `check_bbox()` in the following script:

` anaconda/envs/your_environment/lib/python3.8/site-packages/albumentations/core/bbox_utils.py` 

In addition, if you're performing this CopyPaste augmentation, it might be advantageous to have it perform first, followed by other augmentations that change the image (resize, rotate) 

```python
augmentation_list = [
                     CopyPaste(always_apply=True, p=1.0, blend=True, sigma=15, pct_objects_paste=1.0, max_paste_objects=5),
                     A.Resize(800, 1333, p=1.0),
                     A.HorizontalFlip(p=0.5),
                     A.Rotate(limit=(-15, 15), p=0.5),
                     A.OneOf([A.Blur(), A.MotionBlur(), A.GaussNoise()], p=0.1),
                    ]
```",solution might help someone else sure problem bounding floating point precision issue add block function python check range lesser added block range end block rest code directory also check indentation add code segment running issue cut paste augmentation technique make change following script addition augmentation might advantageous perform first change image resize rotate python,issue,negative,positive,positive,positive,positive,positive
1466052201,I'm having the same issue as @ColaWithIce [and the workaround suggested by @i-aki-y is not working]. Any news regarding this bug? Thanks.,issue working news regarding bug thanks,issue,negative,positive,positive,positive,positive,positive
1464997492,"Since there are versions of yolo who can have OBB (i.e. https://github.com/hukaixuan19970627/yolov5_obb), it would be nice if someone can give this a little push.. what do you think?",since would nice someone give little push think,issue,negative,positive,positive,positive,positive,positive
1463371512,"Wow, this looks fantastic @i-aki-y. Should I adapt MixUp in my PR and make it inherit `BatchBasedTransform` if this is the new way of working with multi-image augmentations @Dipet?",wow fantastic adapt make inherit new way working,issue,positive,positive,positive,positive,positive,positive
1463253209,"May be it can refer to the
    `torchvision.transforms.Grayscale(num_output_channels=1)`
```python
def to_gray(img, num_output_channels=3):
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    if num_output_channels==3:
        return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    elif num_output_channel==1:
        return gray
    else:
        raise ImplementError# rasie an error
```
By setting the default argument 'num_output_channels' to 3, we can maintain the same visualization as before.",may refer python gray return gray return gray else raise error setting default argument maintain visualization,issue,negative,neutral,neutral,neutral,neutral,neutral
1460733029,"Please, provide a minimal reproducible example.",please provide minimal reproducible example,issue,negative,negative,neutral,neutral,negative,negative
1458125701,"Is there a way to add variables to the class Albumentations?
I want to use A.CenterCrop(width=h,height=h, p=1)  , where h is the height of the image (so I would always get a square image), but I don´t find how to pass this parameter to the function as the class never opens the image. I could hardcode it to a fix number, but I'm not sure if all my images are the same size. Do you know if there is a solution?",way add class want use height image would always get square image find pas parameter function class never image could fix number sure size know solution,issue,positive,positive,positive,positive,positive,positive
1457996884,"Hi @Dipet 
I think the version of scikit-image is the reason why the unittest failed.
Should I fix its version to `scikit-image>=0.16.1,<0.20.0` in the setup file?
Or should I open an issue for this?",hi think version reason fix version setup file open issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1457595248,"@GdoongMathew Please, fix it. I can not properly checkout to your branch and push to this PR from my workstation.",please fix properly branch push,issue,negative,neutral,neutral,neutral,neutral,neutral
1457506741,"Hi @Dipet 
I think there was a mistake when merging master into this branch, accidentally renamed `convert_bboxes_to_albumentations` to `a` in bbox_utils.py?",hi think mistake master branch accidentally,issue,negative,neutral,neutral,neutral,neutral,neutral
1457244181,Closing this to open a new PR with a cleaner (squashed) commit history.,open new cleaner commit history,issue,negative,positive,neutral,neutral,positive,positive
1455883453,"> Problem with recursive `label_fields`. Remove from this list `keypoints` and `bboxes`

Thanks, you're right, and this solved the first issue. I'm removing this from the description and updating title. Also changed the code a bit. The second issue remains",problem recursive remove list thanks right first issue removing description title also code bit second issue remains,issue,negative,positive,positive,positive,positive,positive
1455715218,Problem with recursive `label_fields`. Remove from this list `keypoints` and `bboxes` ,problem recursive remove list,issue,negative,neutral,neutral,neutral,neutral,neutral
1455012925,"I see, yes, makes sense to have `Compose` for multi-input / single-output images.

> I have been thinking of this issue for the past few days and have just started writing a PoC. I will let you know when it is ready.

I can try it out when it is done :smile: ",see yes sense compose thinking issue past day writing let know ready try done smile,issue,positive,positive,neutral,neutral,positive,positive
1454964908,"@mikel-brostrom My mosaic augmentation's PR have some difficulties, and these are two of them:

1. We can not define the whole transform as a single `Compose`.
As you did in the above example, we need to define multiple transforms; one is for preprocessing, and the second is for the mosaic.
This weakens an advantage defined by the declaration. Ideally, I want to define the following ways:

```
Compose([
    A.Normalize(),
    A.Resize(),
    A.Mosaic(),
    A.RandomCrop(),
    A.MixUp(),
    ...
])
```

But it is difficult because the `Compose` could not know how to handle the additional targets required from Mosaic (and MixUp).

2. The situation will be more complicated if the transform introduces an additional bboxes target as the Mosaic did.
The `Compose` internally applies some pre- and post-processings, but the auxiliary targets introduced by individual transform bypass these operations.
So the author of such a transform needs to re-implement the same pre- and post-processes for the additional targets. But this is a bad practice because such a code duplication reduces maintainability.
Even worse、I have no idea how to implement some features implemented in the `Compose` such as `label_fields` feature because it is difficult to access the parameter information from the individual transform. The same situation exists for KeypointsParams.

There still exist other minor problems. Anyway, I think I need to extend the `Compose` to fix issues like those described above.
I have been thinking of this issue for the past few days and have just started writing a PoC. I will let you know when it is ready.",mosaic augmentation two define whole transform single compose example need define multiple one second mosaic advantage defined declaration ideally want define following way compose difficult compose could know handle additional mosaic situation complicated transform additional target mosaic compose internally auxiliary individual transform bypass author transform need additional bad practice code duplication even idea implement compose feature difficult access parameter information individual transform situation still exist minor anyway think need extend compose fix like thinking issue past day writing let know ready,issue,negative,negative,neutral,neutral,negative,negative
1454842054,"Maybe an argument to specify 1/3 channels to return would be enough?

```python
def to_gray(img, gray2rgb=False):
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    if gray2rgb:
        return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
    else:
        return gray
```",maybe argument specify return would enough python gray return gray else return gray,issue,negative,neutral,neutral,neutral,neutral,neutral
1454042205,"> But now I do not think introducing auxiliary targets like 'image_cache' and 'image1' is a good approach

Yes, I read you comment in your MR that is why I though I could upload mine. But is a multi image compose really needed? You can simply have a single target image and several other as input to complete the mosaic right? What could we gain by a multi-image Compose @i-aki-y ?",think auxiliary like good approach yes read comment though could mine image compose really simply single target image several input complete mosaic right could gain compose,issue,positive,positive,positive,positive,positive,positive
1453408155,"> Making a new Compose class that handles multi and single-image targets seems more flexible

Yup, I agree here",making new compose class flexible agree,issue,positive,positive,positive,positive,positive,positive
1453399915,"> I consider this ready for review.

I think you need to consider the case when the input is grayscale `len(image.shape) == 2`.

> Don't want to steal the spotlight here @i-aki-y but should I put a PR up for Mosaic as well? I implemented it using the same approach as in this. Will you update yours? 😄

Sure, you can make your PR.
But now I do not think introducing auxiliary targets like 'image_cache' and 'image1' is a good approach. Making a new Compose class that handles multi and single-image targets seems more flexible, and we can make the API more straightforward.",consider ready review think need consider case input want steal spotlight put mosaic well approach update sure make think auxiliary like good approach making new compose class flexible make straightforward,issue,positive,positive,positive,positive,positive,positive
1452647776,"Thanks for the solution, it would be nice to have a new release with this fixed or a warning in the CoarseDropout docs. 👍 
Also, while in https://albumentations.ai/docs/getting_started/keypoints_augmentation/#keypoints-augmentation is stated that you should provide labels to your keypoints if you need to ""recognize"" them, before seeing this particular fix I thought I could count on the fact that keypoints would be returned with same order as they were provided to augmentations.
Maybe the documentation might state clearly that order might change.
",thanks solution would nice new release fixed warning also stated provide need recognize seeing particular fix thought could count fact would returned order provided maybe documentation might state clearly order might change,issue,positive,positive,positive,positive,positive,positive
1451785669,"I consider this ready for review.

Don't want to steal the spotlight here @i-aki-y  but should I put a PR up for Mosaic as well? I implemented it using the same approach as in this. Will you update yours? :smile: ",consider ready review want steal spotlight put mosaic well approach update smile,issue,negative,positive,positive,positive,positive,positive
1451781632,"> What will happen if you remove the ""additional_targets""?

Thanks @i-aki-y! That simple change solved it! Updated the usage example",happen remove thanks simple change usage example,issue,negative,positive,neutral,neutral,positive,positive
1451742651,"Albumentation has a mapping list from the argument key to the associated functions in the `targets` variable:

```
targets = {
    ""image"": apply_image,
    ""bboxes"": apply_bboxes,
    ...
}
```

The functions specified in the `targets` will be executed one by one when you apply `trasform(image=image, bboxes=bboxes)`.

You have added new entries into the `targets` variable by using `additional_targets` 

```
targets = {
    ""image"": apply_image,
    ""bboxes"": apply_bboxes,
    ...
    ""image1"": apply_image,
    ...
}
```

This means apply_image will be called twice; the first is for the ""image"", and the second is for the ""image1"".
I think this is the expected behavior when the `additional_targets` is used.

What will happen if you remove the ""additional_targets""?",list argument key associated variable image executed one one apply added new variable image image twice first image second image think behavior used happen remove,issue,negative,positive,neutral,neutral,positive,positive
1451373894,"Thank for the feedback @i-aki-y! Branch updated based on your comments:
- stochastic operations moved to `get_params`
- apply is now deterministic
- alpha and beta defining the distribution are now input arguments

 Any suggestions on how to avoid calling `apply` twice when having multiple targets @i-aki-y?",thank feedback branch based stochastic apply deterministic alpha beta distribution input avoid calling apply twice multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1451341730,"@mikel-brostrom I advise making the `apply_*` functions deterministic. Existing transforms put stochastic operations like `np.random.*` into get_params or get_params_dependent_on_targets. This practice makes the result reproducible and debugging and testing easy.
And why not use an argument for the alpha (the parameter of the beta function) instead of the hardcoding `32`?",advise making deterministic put stochastic like practice result reproducible testing easy use argument alpha parameter beta function instead,issue,negative,positive,positive,positive,positive,positive
1450350292,"Hi @Dipet, 
since I haven't got further comments on [this change request](https://github.com/albumentations-team/albumentations/pull/1396#discussion_r1103777429), I think currently it's best to just add the correct type hint instead of combining `combine_bboxes_from_albumentations`, `convert_to_original_type` & `combine_bboxes_to_albumentations`, `convert_to_internal_type`.

Feel free to give any further suggestions.",hi since got change request think currently best add correct type hint instead combining feel free give,issue,positive,positive,positive,positive,positive,positive
1449525701,"Thank you very much for the details!
3. can obviously be quickly ""fixed"" by using p=1 where needed.",thank much obviously quickly fixed,issue,negative,positive,neutral,neutral,positive,positive
1446123862,Mixup gives consistent boosts on image classification when using the loss presented in the paper. It also helps on COCO for object detection in the case of large models that tend to overfit. For smaller models this augmentation tends to be detrimental and should be avoided.,consistent image classification loss paper also coco object detection case large tend overfit smaller augmentation detrimental,issue,negative,positive,positive,positive,positive,positive
1445986377,"@mikel-brostrom No, the filters are applied in the post-processing, while the error occurs in pre-processing validation. 

I think they have different purposes. 
The filters are necessary because some transforms make bbox with zero or tiny areas by design. 
But invalid data in the input suggest something was wrong in the previous process, which should be fixed.",applied error validation think different necessary make zero tiny design invalid data input suggest something wrong previous process fixed,issue,negative,negative,neutral,neutral,negative,negative
1445748453,"1. `Sequential` is a simplified version of `Compose` without logic for pre- and post-processing of bboxes and keypoints.
2. You can use any transform as top level transform, but only `Compose` do pre- and post-processing for targets such as bboxes and keypoints. We recommend only using `Compose` as your top-level transform.
3. Looks like a bug in the documentation or implementation...",sequential simplified version compose without logic use transform top level transform compose recommend compose transform like bug documentation implementation,issue,positive,positive,positive,positive,positive,positive
1445695381,"@mikel-brostrom 

> ValueError: y_max is less than or equal to y_min for bbox

I found some coco annotations have bboxes with height == 0.0.
This is the cause of the error.

```py
import json
import pathlib
coco_annot_path = pathlib.Path(""coco/annotations/instances_train2017.json"")
with open(coco_annot_path) as f:
    coco_annots = json.load(f)
for item in coco_annots[""annotations""]:
    x, y, w, h = item[""bbox""]
    if w == 0 or h == 0:
        print(item)

> {'segmentation': [[296.65, 388.33, 296.65, 388.33, 297.68, 388.33, 297.68, 388.33]], 'area': 0.0, 'iscrowd': 0, 'image_id': 200365, 'bbox': [296.65, 388.33, 1.03, 0.0], 'category_id': 58, 'id': 918}
> {'segmentation': [[9.98, 188.56, 15.52, 188.56, 15.52, 188.56, 11.09, 188.56]], 'area': 0.0, 'iscrowd': 0, 'image_id': 550395, 'bbox': [9.98, 188.56, 5.54, 0.0], 'category_id': 1, 'id': 2206849}
```

",le equal found coco height cause error import import open item item print item,issue,negative,neutral,neutral,neutral,neutral,neutral
1443181917,"Btw, I have implemented this in a slightly different manner.

```python 
class Mosaic(DualTransform):
    def __init__(
        self,
        height,
        width,
        replace=True,
        fill_value=0,
        bboxes_format=""coco"",
        always_apply=False,
        p=0.5,
    ):
        super().__init__(always_apply=always_apply, p=p)
        self.height = height
        self.width = width
        self.replace = replace
        self.fill_value = fill_value
        self.bboxes_format = bboxes_format
        self.images = []
        self.bboxes = []

    def get_transform_init_args_names(self) -> Tuple[str, ...]:
        return (""height"", ""width"", ""replace"", ""fill_value"", ""bboxes_format"")

    def apply(self, image, **params):
        return mosaic4(self.images, self.height, self.width, self.fill_value)

    def apply_to_keypoint(self, **params):
        pass  # TODO
    
    def apply_to_bbox(self, bbox, image_shape, position, height, width, **params):
        rows, cols = image_shape[:2]
        return bbox_mosaic4(bbox, rows, cols, position, height, width)
    
    def apply_to_bboxes(self, bboxes, **params):
        new_bboxes = []
        for i, (bbox, im) in enumerate(zip(self.bboxes, self.images)):
            im_shape = im.shape
            h, w, _ = im_shape
            for b in bbox:
                new_bbox = self.apply_to_bbox(b, im_shape, i, self.height, self.width)
                new_bboxes.append(new_bbox)
        return new_bboxes

    def get_params_dependent_on_targets(self, params: Dict[str, Any]) -> Dict[str, Any]:
        self.images = [params['image'], params['image1'], params['image2'], params['image3']]
        self.bboxes = [params['bboxes'], params['bboxes1'], params['bboxes2'], params['bboxes3']]
        images_bboxes = list(zip(self.images, self.bboxes))
        random.shuffle(images_bboxes)
        self.images, self.bboxes = zip(*images_bboxes)
        return {}
        
    @property
    def targets_as_params(self) -> List[str]:
        return [
            ""image"", ""image1"", ""image2"", ""image3"",
            ""bboxes"", ""bboxes1"", ""bboxes2"", ""bboxes3""
        ]
```

Trying to follow the recommended way of working with multiple images and bboxes. I however see that `apply` and `apply_to_bbox` is called equally many times as there are targets. Any ideas on how to circumvent this @i-aki-y ?",slightly different manner python class mosaic self height width coco super height width replace self return height width replace apply self image return mosaic self pas self position height width return position height width self enumerate zip return self list zip zip return property self list return image image image image trying follow way working multiple however see apply equally many time circumvent,issue,positive,positive,positive,positive,positive,positive
1441399884,Feel free to check out my MixUp implementation [here](https://github.com/albumentations-team/albumentations/pull/1409) @i-aki-y . Any feedback is appreciated. It works nicely with this Mosaic implementation :smile:. I am going for `CutMix` :rocket: ,feel free check implementation feedback work nicely mosaic implementation smile going rocket,issue,positive,positive,positive,positive,positive,positive
1439609031,"@mikel-brostrom 

> Yes. I had multiple augmentations in my augmentation stack but it always appeared during Mosiac. The case was that when the error emerged y_max was always equal to y_min. This couldn't be fixed by setting min_area in A.BboxParams btw...

Thanks, I will investigate it.

> I have it working locally so I could create pull request with what I have @i-aki-y . I also have MixUP working as you suggested:

Great!

> Maybe this should go into a separate PR?

Yes.",yes multiple augmentation stack always case error always equal could fixed setting thanks investigate working locally could create pull request also working great maybe go separate yes,issue,positive,positive,positive,positive,positive,positive
1439552657,"> This means that some bboxes have zero or minus heights.
> Did you get this error only when you used mosaic transform?

Yes. I had multiple augmentations in my augmentation stack but it always appeared during Mosiac. The case was that when the error emerged  `y_max` was always equal to `y_min`. This couldn't be fixed by setting `min_area` in `A.BboxParams` btw...

",zero minus get error used mosaic transform yes multiple augmentation stack always case error always equal could fixed setting,issue,negative,neutral,neutral,neutral,neutral,neutral
1439550201,"I have it working locally so I could create pull request with what I have @i-aki-y . I also have MixUP working as you suggested:

```
mosaic1 = mosaic_aug(image=image1, image_cache=image_cache, bboxes=bboxes1, bboxes_cache=bboxes_cache)
mosaic2 = mosaic_aug(image=image2, image_cache=image_cache, bboxes=bboxes2, bboxes_cache=bboxes_cache)
mosaic_mixup = mixup_aug(image=mosaic1[""image""], bboxes=mosaic1[""bboxes""], image_cache=mosaic2[""image""], bboxes_cache=mosaic2[""bboxes""])
```

Maybe this should go into a separate PR?",working locally could create pull request also working mosaic mosaic image image maybe go separate,issue,negative,neutral,neutral,neutral,neutral,neutral
1439534001,"@mikel-brostrom Sorry for delaying and thank you for your feedback!

> ValueError: y_max is less than or equal to y_min for bbox 

This means that some bboxes have zero or minus heights. 
Did you get this error only when you used mosaic transform?

> Moreover, the structure of the repo must have changed since the PR was created as some refactoring was needed.

OK. I will check and update the PR.

> Couldn't this be solved by using additional_targets like here?
> This would allow the loaded images to be augmented in different ways before mosaic 🚀

I missed this feature! I will look into this if I can use this for this transform.",sorry delaying thank feedback le equal zero minus get error used mosaic transform moreover structure must since check update could like would allow loaded augmented different way mosaic rocket feature look use transform,issue,negative,negative,negative,negative,negative,negative
1438793649,Setting the `width` and `height` for `Mosaic` requires some basic knowledge regarding the dataset your are working with as most part of the image could be left outside otherwise. Maybe this should be reflected in the docstrings as well. I guess that a good set of initial values could be the average width and height of the dataset you work with @i-aki-y ?,setting width height mosaic basic knowledge regarding working part image could left outside otherwise maybe reflected well guess good set initial could average width height work,issue,positive,positive,neutral,neutral,positive,positive
1437746666,"I think that describing a vector using an ""x-scale""/""y-scale"" or using ""scale""/""angle"" are both equally valid and expected. Each is more or less useful in different situations. However, the order of priorities are:

1. Maintain current interfaces.
2. Minimise changes to ""x-scale""/""y-scale"" internally or for passing to other libraries

So, I think it wouldn't make sense to change it to be represented as ""x-scale""/""y-scale"".

As for ""different repo""... The albumenatations.ai website is in its own repo. https://github.com/albumentations-team/albumentations.ai I opened an issue on it providing the current diagram. https://github.com/albumentations-team/albumentations.ai/issues/22

So, that's my *new* diagram you're complaining about. :sweat_smile: Previously it looked like it was using an ""x-scale""/""angle"" representation, rather than what it actually does: uses ""scale""/""angle"" representation. This issue was about the accuracy of the diagram; not asking the code to change.

",think vector scale angle equally valid le useful different however order maintain current internally passing think would make sense change different issue providing current diagram new diagram previously like angle representation rather actually scale angle representation issue accuracy diagram code change,issue,positive,positive,neutral,neutral,positive,positive
1437371107,"> Since current albumentations do not support multiple image sources, I introduced helper targets, image_cache, bboxes_cache, as additional data sources

Couldn't this be solved by using `additional_targets` like [here](https://albumentations.ai/docs/examples/example_multi_target/)?
This would allow the loaded images to be augmented in different ways before mosaic :rocket: ",since current support multiple image helper additional data could like would allow loaded augmented different way mosaic rocket,issue,positive,neutral,neutral,neutral,neutral,neutral
1437352635,"I tried this out together with some rotation augmentations and seems to work @i-aki-y .

![proc](https://user-images.githubusercontent.com/18719680/220160642-60f104a7-3246-4030-bb1d-3a4195a58c23.png)

However, from time to time this error arise:

`ValueError: y_max is less than or equal to y_min for bbox `

when using COCO. Any idea how to fix this @i-aki-y?

Moreover, the structure of the repo must have changed since the PR was created as some refactoring was needed.
",tried together rotation work however time time error arise le equal coco idea fix moreover structure must since,issue,negative,neutral,neutral,neutral,neutral,neutral
1437252721,@Multihuntr I agree with your observation. What do you mean by different repo? Wouldn't it make sense to get the scale for both directions: x and y.,agree observation mean different would make sense get scale,issue,negative,negative,negative,negative,negative,negative
1433053046,"This is known issue with OpenCV: https://github.com/albumentations-team/albumentations#comments
I can not imagine common fix.",known issue imagine common fix,issue,negative,negative,negative,negative,negative,negative
1432991142,"+1 here. Same problem fixed with `cv2.setNumThreads(0)`.
I didn't notice such behaviour when using Albumentations 6 month ago (~June-July). Did something changes this then? 
",problem fixed notice behaviour month ago something,issue,negative,positive,neutral,neutral,positive,positive
1431823611,"This doesn't seem to work, I still get a reduced set of bounding boxes out.

e.g.

```python
transform = A.Compose([
    A.RandomCrop(width=1024, height=1024)
], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], check_each_transform=False))
```

```python
# succeeds
res = transform(image=image, masks=masks, bboxes=boxes, labels=labels)

# fails
assert len(res['masks']) == len(res['bboxes'])
```

The check is here(?)

https://github.com/albumentations-team/albumentations/blob/87b1b7d009bcff12d9cc7a482c14cac0b1300ac8/albumentations/core/composition.py#L212-L213

which calls filter:

https://github.com/albumentations-team/albumentations/blob/87b1b7d009bcff12d9cc7a482c14cac0b1300ac8/albumentations/core/composition.py#L229

However, I think filtering is actually performed again in postprocessing, which is always called: https://github.com/albumentations-team/albumentations/blob/87b1b7d009bcff12d9cc7a482c14cac0b1300ac8/albumentations/core/composition.py#L216-L219

https://github.com/albumentations-team/albumentations/blob/87b1b7d009bcff12d9cc7a482c14cac0b1300ac8/albumentations/core/utils.py#L68-L76",seem work still get reduced set bounding python transform python transform assert check filter however think filtering actually always,issue,negative,neutral,neutral,neutral,neutral,neutral
1431762628,"Current PyPi version does not have these changes. This is issue with documentation - it shows current master state, not latest PyPi version.
You can install latest version of library from github directly `pip install git+https://github.com/albumentations-team/albumentations.git`",current version issue documentation current master state latest version install latest version library directly pip install,issue,negative,positive,positive,positive,positive,positive
1431191915,"I've added a few fixes.
1. converting bboxes / keypoints to internal type when testing transforming function.
2. add targets back in the unittests.
3. some requested changes.

point 1 and 2 were missing from the initial review, sorry for the additional corrections.
",added converting internal type testing transforming function add back point missing initial review sorry additional,issue,negative,negative,negative,negative,negative,negative
1430277052,"Is there a recommended solution here for instance segmentation?

A standard approach for model inputs is a list of bounding boxes and full image masks for each object. I don't think this is strange, it's a different input expectation than either object detection or semantic segmentation. In this case the individual mask is explicitly tied to the bounding box.

@Dipet Is it possible to disable bounding box/object filtering? Then we could do a coordinate check on the returned list (e.g. coordinate < 0 or > image size) and ignore masks where that's true. This would enable relatively cheap filtering versus recomputing the bounding box for the new mask.",solution instance segmentation standard approach model list bounding full image object think strange different input expectation either object detection semantic segmentation case individual mask explicitly tied bounding box possible disable bounding filtering could check returned list image size ignore true would enable relatively cheap filtering versus bounding box new mask,issue,negative,positive,positive,positive,positive,positive
1427154209,"```x = np.random.rand(256, 256)```
```tr = A.Compose([ToTensorV2()])```
```z = tr(image=x)['image']```
```z.shape --> torch.Size([1, 256, 256])```

According to the official documentation: ""If the image is in HW format (grayscale image), it will be converted to pytorch HW tensor."" I think it should be corrected to ""... to pytorch CHW, C=1"".
",according official documentation image format image converted tensor think corrected,issue,negative,neutral,neutral,neutral,neutral,neutral
1426247403,Is this still planned? It would super useful to have this feature.,still would super useful feature,issue,positive,positive,positive,positive,positive,positive
1419990597,"That'll will be more reliable, thanks for all the suggestions.
I'll start make those changes.",reliable thanks start make,issue,negative,positive,positive,positive,positive,positive
1419470355,"I think we must to provide only 4 coordinates into `apply_to_bbox`, withouts targets. This is the way how it is works now. I do not think that we must to change this logic.

I think better to write:
```python
    def apply_to_bbox(self, bbox: Tuple[float, float, float, float], **params) -> BBoxesInternalType:
        raise NotImplementedError(""Method apply_to_bbox is not implemented in class "" + self.__class__.__name__)

    def apply_to_bboxes(self, bboxes: BBoxesInternalType, **params) -> BBoxesInternalType:
        bbox: BBoxesInternalType
        for i, bbox in enumerate(bboxes.array):  # type: ignore[arg-type]
            bboxes[i] = self.apply_to_bbox(bbox, **params)
        return bboxes
```",think must provide way work think must change logic think better write python self float float float float raise method class self enumerate type ignore return,issue,negative,positive,positive,positive,positive,positive
1419465854,"The performance of scipy.signal.medfilt2d(image, kernel_size) is significantly worse than with opencv. While it does support larger kernel sizes, it takes roughly 30 times as long to process identical arrays with the same kernel sizes. At least, this is what I found with my limited testing. ",performance image significantly worse support kernel size roughly time long process identical kernel size least found limited testing,issue,negative,negative,negative,negative,negative,negative
1419367348,"Yes, currently I'm working on something like that as well.
* in transforms_interface:
```python
@dataclass
class BBoxesInternalType(BatchInternalType):
    array: BoxesArray
    targets: List[Any] = field(default_factory=list)
    def __getitem__(self, item) -> ""BBoxesInternalType"":
        _bboxes = self.array[item].astype(float)
        if isinstance(item, int):
            _bboxes = _bboxes[np.newaxis, ...]
            _target = [self.targets[item]] if self.targets else []
        else:
            _target = [self.targets[i] for i in item] if self.targets is not None else None
        return BBoxesInternalType(array=_bboxes, targets=_target)
    def __setitem__(self, idx, value: ""BBoxesInternalType""):
        self.array[idx] = value.array
        if isinstance(idx, int):
            self.targets[idx] = value.targets[0]
        else:
            for i, target in zip(idx, value.targets):
                self.targets[i] = target
```

* in DualTransform:
```python
    def apply_to_bbox(self, bbox: BBoxesInternalType, **params) -> BBoxesInternalType:
        raise NotImplementedError(""Method apply_to_bbox is not implemented in class "" + self.__class__.__name__)

    def apply_to_bboxes(self, bboxes: BBoxesInternalType, **params) -> BBoxesInternalType:
        bbox: BBoxesInternalType
        for i, bbox in enumerate(bboxes):  # type: ignore[arg-type]
            bboxes[i] = self.apply_to_bbox(bbox, **params)
        return bboxes
```
If implemented in such way, then developers well only have to be aware that in `apply_to_bbox`, its input bbox has `array` with shape `1 x 4` and targets with type `[Any]`, which indicate that there's only one bbox and its corresponding target labeling.
And also, they can choose to use BboxesInternalType.array in their numpy operation if they want to.
Same principle applies to `KeypointsInternalType`.
",yes currently working something like well python class array list field self item item float item item else else item none else none return self value else target zip target python self raise method class self enumerate type ignore return way well aware input array shape type indicate one corresponding target also choose use operation want principle,issue,positive,positive,positive,positive,positive,positive
1418929805,This PR might fix this issue #1394. Thank you so much @i-aki-y,might fix issue thank much,issue,negative,positive,positive,positive,positive,positive
1418643081,"> Black bbox is a result of a generalized ellipse rotation. It looks much better than the gray box (largest_box).

This looks fantastic @i-aki-y!",black result generalized ellipse rotation much better gray box fantastic,issue,positive,positive,positive,positive,positive,positive
1416684302,"For the next one that comes here, these errors are generally caused by mismatched bbox sizes that could've been the result of incorrect labelling. Please verify your data or the specific image that's causing the error before blaming the developers of not correcting the issue.",next one come generally size could result incorrect please verify data specific image causing error blaming correcting issue,issue,negative,positive,neutral,neutral,positive,positive
1415057307,"> > In most cases, your original suggestion make sense since most operation does not discard bboxes during transformation. In dropout (more specifically CoarseDropout) this might cause inconsistency though.
> 
> I can not see any problem there because CoarseDropout reimplements `apply_to_keypoints` and it will get ours new internal type.

Ah that's correct.

> > And if we still keep apply_to_bbox, then it's input type hint to bbox will be very different compare to apply_to_bboxes?
> 
> It will be the same as now: `Tuple[float, float, float, float]`.
> 
That's my concern.
Original type hint to bbox and bboxes is very intuitive.`BoxInternalType` and `Sequence[BoxInternalType]`, they can be directly interpreted as one bbox and a sequence of bboxes. However, if we change them to `InternalBboxes` and `BoxInternalType`, then they have different ways to interact with the internal data, and those type hints become less intuitive?
But also, we can still say that its developers own responsibility to recognize the differences? Not sure if i'm worrying to much.
Would love to know about your opinion on this. Thanks.",original suggestion make sense since operation discard transformation dropout specifically might cause inconsistency though see problem get new internal type ah correct still keep input type hint different compare float float float float concern original type hint sequence directly one sequence however change different way interact internal data type become le intuitive also still say responsibility recognize sure worrying much would love know opinion thanks,issue,positive,positive,positive,positive,positive,positive
1414181497,"> And if we still keep apply_to_bbox, then it's input type hint to bbox will be very different compare to apply_to_bboxes?

It will be the same as now: `Tuple[float, float, float, float]`.

>  In most cases, your original suggestion make sense since most operation does not discard bboxes during transformation. In dropout (more specifically CoarseDropout) this might cause inconsistency though.

I can not see any problem there because CoarseDropout reimplements `apply_to_keypoints` and it will get ours new internal type.",still keep input type hint different compare float float float float original suggestion make sense since operation discard transformation dropout specifically might cause inconsistency though see problem get new internal type,issue,negative,positive,positive,positive,positive,positive
1414134358,"> Why you think we can not still fall back to single bbox/keypoint transformation? I think this approach will be work fine:
> 
> ```python
> def apply_bboxes(self, bboxes, *args, **kwargs):
>     res = bboxes.copy()
>     for i, box in enumerate(bboxes):
>         res[i] = self.apply_bbox(box, *args, **kwargs)
>     return res
> ```
> 
> In that case we might reimplement `apply_bboxes` or we can change bboxes one by one.

Since if we're doing full ndarray to bboxes and keypoints, then there's one problems we might encounter if we still keep those two methods.
1. the default `apply_to_bboxes` will become something like this
```python
@dataclass
class InternalBboxes:
    bboxes: np.ndarray
    targets: List[Any]

    def __getitem__(self, item):
        return bboxes[item].tolist() + targets[item]


def apply_to_bboxes(self, bboxes: InternalBboxes, *args, **kwargs) -> InternalBboxes:
    bboxes = [self.apply_to_bboxes(bbox, *args, **kwargs) for bbox in bboxes]
    bboxes = list(filter(lambda _x: _x is not None, bboxes)
    return InternalBboxes(bboxes[:4], bboxes[4:])

```
the reason for that is to make sure that the consistency still can be assured after apply_to_bbox. In most cases, your original suggestion make sense since most operation does not discard bboxes during transformation. In dropout (more specifically `CoarseDropout`) this might cause inconsistency though.
And if we still keep `apply_to_bbox`, then it's input type hint to `bbox` will be very different compare to `apply_to_bboxes`?",think still fall back single transformation think approach work fine python self box enumerate box return case might change one one since full one might encounter still keep two default become something like python class list self item return item item self list filter lambda none return reason make sure consistency still assured original suggestion make sense since operation discard transformation dropout specifically might cause inconsistency though still keep input type hint different compare,issue,positive,positive,positive,positive,positive,positive
1414076577,"Why you think we can not still fall back to single bbox/keypoint transformation?
I think this approach will be work fine:
```python
def apply_bboxes(self, bboxes, *args, **kwargs):
    res = bboxes.copy()
    for i, box in enumerate(bboxes):
        res[i] = self.apply_bbox(box, *args, **kwargs)
    return res
```
In that case we might reimplement `apply_bboxes` or we can change bboxes one by one.",think still fall back single transformation think approach work fine python self box enumerate box return case might change one one,issue,negative,positive,positive,positive,positive,positive
1413650778,"The reason why I convert bboxes from Sequence[BoxTyep] to ndarray and back is because I didn't want to cause any conflicts on the current users. By doing so, we can still have the flexibility of transforming one or multiple bboxes based on their own desire.

However, if you agree on just removing the single bbox/keypoint transformation, probably we should currently close this PR first, and start a new PR once I finished development on the new branch.",reason convert sequence back want cause current still flexibility transforming one multiple based desire however agree removing single transformation probably currently close first start new finished development new branch,issue,positive,positive,neutral,neutral,positive,positive
1413317036,"@mikel-brostrom I think there is no easy workaround for using shear transform with ellipse rotation.
As I showed above, a shear operation introduces extra spaces between the target and the bbox.
So if you include shear operation in random affine transform, some extra spaces will appear.
I think we need to generalize the ellipse rotation to account for the shear effect.",think easy shear transform ellipse rotation shear operation extra target include shear operation random affine transform extra appear think need generalize ellipse rotation account shear effect,issue,negative,negative,neutral,neutral,negative,negative
1413279265,"@Dipet 
I'm currently working on another branch, exactly like what you're talking about, converting bboxes and keypoints once inside the compose. However, I didn't store the additional targets in the `InternalBboxes` representation, the internal bboxes / keypoints representations are ndarray only, which in most transformations are fine since they filter bboxes/keypoints **after** each transformation layer. The only transformation that I encounter problem with, is `CoarseDropout` since it remove keypoints during the transformation, which can cause inconsistency between keypoints and the additional targets.
You suggestion, converting bboxes/keypoints to dataclasses as internal representation, is a good solution for my problem, thanks. My only problem left is that do we still need to keep `apply_to_bbox` and `apply_to_keypoint`, or can we just remove them?",currently working another branch exactly like talking converting inside compose however store additional representation internal fine since filter transformation layer transformation encounter problem since remove transformation cause inconsistency additional suggestion converting internal representation good solution problem thanks problem left still need keep remove,issue,positive,positive,positive,positive,positive,positive
1413232320,"It is huge PR and amazing work!
I think we need convert bboxes to ndarrays once inside `Compose` or when we call transform [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/transforms_interface.py#L102) call something named `targets_to_internal_format` and call on function exit `targets_to_original_format` if they are were in non internal format.
Would be great if you will create internal representation of bboxes such as:
```python
@dataclass
class InternalBboxes:
    bboxes: np.ndarray
    targets: list
```
And we will use this representation inside all transforms.",huge amazing work think need convert inside compose call transform call something call function exit non internal format would great create internal representation python class list use representation inside,issue,positive,positive,positive,positive,positive,positive
1410398087,"> > `np.random`
> 
> Do you know if there's any issue open in numpy for this issue?
> 
> I'm not sure because I don't know that much about parallelization, but could there be any hint in here?
> 
> https://numpy.org/doc/stable/reference/random/parallel.html

Everything is working, and I just merged the latest stuff from master, again. The problem was in the test itself because the test was not expecting something using a random state, but I fixed it long time ago.",know issue open issue sure know much parallelization could hint everything working latest stuff master problem test test something random state fixed long time ago,issue,negative,positive,positive,positive,positive,positive
1410380237,"> `np.random`

Do you know if there's any issue open in numpy for this issue?

I'm not sure because I don't know that much about parallelization, but could there be any hint in here? 

https://numpy.org/doc/stable/reference/random/parallel.html",know issue open issue sure know much parallelization could hint,issue,negative,positive,positive,positive,positive,positive
1410378810,"Ah yes, I confused with #1377 
my bad hehe

> hope one day it will get accepted

inshallah! it's a major upgrade 

> For this reason we used random istead of np.random as base random generator.

Would it be possible to use `random.Random` instead of `np.random.RandomState`?

",ah yes confused bad hope one day get accepted major upgrade reason used random base random generator would possible use instead,issue,negative,negative,negative,negative,negative,negative
1410286225,"@jpcbertoldo, this PR is still alive and kicking :smile: 
#1377 was closed because it depends on this one, but this one never gets pulled and I need to keep updating it in the hope one day it will get accepted :cry: ",still alive kicking smile closed one one never need keep hope one day get accepted cry,issue,negative,positive,positive,positive,positive,positive
1409883632,"It looks strange. Try to trace imports by calling your test script like this:
```
python -v -m test.py
```",strange try trace calling test script like python,issue,negative,negative,neutral,neutral,negative,negative
1409880359,"@pneycho 
About what commit are you talking? I do not see any pull request there. Try to create it please",commit talking see pull request try create please,issue,positive,neutral,neutral,neutral,neutral,neutral
1407664495,On a different note. After this conversation it is clear to me that rotate_method='ellipse' is clearly superior to 'largest_box' for most applications. Maybe it should be the standard in albumentations for handling bbox transformations?,different note conversation clear clearly superior maybe standard handling,issue,positive,positive,positive,positive,positive,positive
1407591597,"I see, thanks again. So, `Affine` is comprised of:

```text
- Translation (""move"" image on the x-/y-axis)
- Rotation
- Scaling (""zoom"" in/out)
- Shear (move one side of the image, turning a square into a trapezoid)
```

by using `ShiftScaleRotate` three of these operations would be covered and it has the `rotate_method` option. Couldn't find any `Shear` operation with `rotate_method`... Do you have a better suggestion @i-aki-y?",see thanks affine comprised text translation move image rotation scaling zoom shear move one side image turning square trapezoid three would covered option could find shear operation better suggestion,issue,positive,positive,positive,positive,positive,positive
1407590127,"> So you suggestion is to not rotate by Affine which don't have rotate_method option and instead use Rotate (which has this option)?

Yes

> Can shear also lead to these type of behavior or is it only rotate?

See below:

![bbox_shear](https://user-images.githubusercontent.com/9190086/215312722-3d8dc120-1af1-4391-8ab4-d00fab922a21.jpg)
",suggestion rotate affine option instead use rotate option yes shear also lead type behavior rotate see,issue,negative,neutral,neutral,neutral,neutral,neutral
1407477622,"> I think this is not a bug. As pointed out in #746, to fit the rotated bounding box on the targets, information about the shape of the target is needed. Setting rotate_method=""ellipse"" might mitigate your issue, but if you have rectangle targets, it might make too small a bounding box because the corners will be cut off. See also #1203 or https://openaccess.thecvf.com/content/ICCV2021/papers/Kalra_Towards_Rotation_Invariance_in_Object_Detection_ICCV_2021_paper.pdf

Very helpful comment. Really appreciate it @i-aki-y . So you suggestion is to not rotate by `Affine` which don't have `rotate_method` option and instead use `Rotate` (which has this option)? Can shear also lead to these type of behavior or is it only rotate?",think bug pointed fit rotated bounding box information shape target setting ellipse might mitigate issue rectangle might make small bounding box cut see also helpful comment really appreciate suggestion rotate affine option instead use rotate option shear also lead type behavior rotate,issue,positive,positive,positive,positive,positive,positive
1407470743,"What if you substitute `SafeRotate` by `Rotate` @pillai-karthik, and you set:

https://github.com/albumentations-team/albumentations/blob/592991f42bcc72f7e6ae06a398850291a35a88d2/albumentations/augmentations/geometric/rotate.py#L71

to ""ellipse"" as suggested here: https://github.com/albumentations-team/albumentations/issues/1388#issuecomment-1407283872",substitute rotate set ellipse,issue,negative,neutral,neutral,neutral,neutral,neutral
1407283872,"I think this is not a bug. As pointed out in #746, to fit the rotated bounding box on the targets, information about the shape of the target is needed.
Setting rotate_method=""ellipse"" might mitigate your issue, but if you have rectangle targets, it might make too small a bounding box because the corners will be cut off.
See also #1203 or https://openaccess.thecvf.com/content/ICCV2021/papers/Kalra_Towards_Rotation_Invariance_in_Object_Detection_ICCV_2021_paper.pdf

",think bug pointed fit rotated bounding box information shape target setting ellipse might mitigate issue rectangle might make small bounding box cut see also,issue,negative,positive,neutral,neutral,positive,positive
1406851832,"@mikel-brostrom 
I don't think that's a bug, those transformations are properly implemented.
The reason for that is because the result bounding boxes are based on the transformation on boxes' corner keypoints. 
A bbox will be dilated more if it's processed by more rotation transformations. However, it can be acquired in a more precise way if the corresponding instance mask is given. 
some tips tho, if you really need to acquire precise bbox, and you happen to have instance mask labeling, then you can apply mask transformation, and use opencv boundingrect to acquire the bbox result.",think bug properly reason result bounding based transformation corner dilated rotation however acquired precise way corresponding instance mask given tho really need acquire precise happen instance mask apply mask transformation use acquire result,issue,negative,positive,positive,positive,positive,positive
1406533222,"@mikel-brostrom.
 no, I didn't realize those bugs exist. I'll dig into those later.",realize exist dig later,issue,negative,neutral,neutral,neutral,neutral,neutral
1406069993,"Hi @Dipet, It'd be great if you could take a look at the commit. There is an issue I fixed, about the gray image inversion if the mean pixel value of the resultant value is over 127 (which is mentioned was mentioned in the docstring but not implemented). Is there any specific reason why that is being done? The only thing I could think of is visibility enhancement of specific structures. Am I correct in assuming that?",hi great could take look commit issue fixed gray image inversion mean value resultant value specific reason done thing could think visibility enhancement specific correct assuming,issue,positive,positive,positive,positive,positive,positive
1405159107,"



> What version of the library are you using? What shows `print(A.__version__)`?

It shows 1.3.0 version for me",version library print version,issue,negative,neutral,neutral,neutral,neutral,neutral
1399249140,"@Dipet Hi,
I've added a few fixes to the CI errors. Can you help me rerun the workflow again? Thanks.",hi added help rerun thanks,issue,positive,positive,positive,positive,positive,positive
1396597561,I just found out that the `ToRGB` has been merged on master with this PR https://github.com/albumentations-team/albumentations/pull/1323 but the last version of albumentations on pypi is older. However the documentations https://albumentations.ai/docs/ says that it refers to version 1.3.0 even if it actually refers to the master branch.,found master last version older however version even actually master branch,issue,negative,positive,neutral,neutral,positive,positive
1382869904,P.s.: I have only moved `update_params` from `apply_with_params` to `__call__` and it doesn't get called anymore when the transformer is in replay mode.,get transformer replay mode,issue,negative,neutral,neutral,neutral,neutral,neutral
1382868749,"Hi,

managed to fix the issue by overriding the `BasicTransform` class:

```python
""""""Bugfix: `albumentations.core.transforms_interface.BasicTransform`.

https://github.com/albumentations-team/albumentations/issues/1382
""""""

from albumentations.core.transforms_interface import (
    BasicTransform,
    random,
    warn,
    deepcopy,
    Any,
    Dict,
)
from albumentations import CropNonEmptyMaskIfExists


class FixedBasicTransform(BasicTransform):
    def __call__(self, *args, force_apply: bool = False, **kwargs) -> Dict[str, Any]:
        if args:
            raise KeyError(
                ""You have to pass data to augmentations as named arguments, for example: aug(image=image)""
            )
        if self.replay_mode:
            if self.applied_in_replay:
                return self.apply_with_params(self.params, **kwargs)

            return kwargs

        if (random.random() < self.p) or self.always_apply or force_apply:
            params = self.get_params()

            if self.targets_as_params:
                assert all(
                    key in kwargs for key in self.targets_as_params
                ), ""{} requires {}"".format(
                    self.__class__.__name__, self.targets_as_params
                )
                targets_as_params = {k: kwargs[k] for k in self.targets_as_params}
                params_dependent_on_targets = self.get_params_dependent_on_targets(
                    targets_as_params
                )
                params.update(params_dependent_on_targets)
            if params is not None:
                params = self.update_params(params, **kwargs)
            if self.deterministic:
                if self.targets_as_params:
                    warn(
                        self.get_class_fullname()
                        + "" could work incorrectly in ReplayMode for other input data""
                        "" because its' params depend on targets.""
                    )
                kwargs[self.save_key][id(self)] = deepcopy(params)
            return self.apply_with_params(params, **kwargs)

        return kwargs

    def apply_with_params(self, params: Dict[str, Any], **kwargs) -> Dict[str, Any]:  # skipcq: PYL-W0613
        if params is None:
            return kwargs
        res = {}
        for key, arg in kwargs.items():
            if arg is not None:
                target_function = self._get_target_function(key)
                target_dependencies = {
                    k: kwargs[k] for k in self.target_dependence.get(key, [])
                }
                res[key] = target_function(arg, **dict(params, **target_dependencies))
            else:
                res[key] = None
        return res


class FixedCropNonEmptyMaskIfExists(CropNonEmptyMaskIfExists, FixedBasicTransform):
    pass

```

If that fix makes any sense, then it also means that all other transformers that rely on `apply_with_params` are affected by this bug.",hi fix issue class python import random warn import class self bool false raise pas data example return return assert key key none warn could work incorrectly input data depend id self return return self none return key none key key key else key none return class pas fix sense also rely affected bug,issue,negative,negative,negative,negative,negative,negative
1379401018,"I ran your code and it really works. 

However, `print(im.shape)` outputs (10, 25, 3) because the cv2.resize function has a different order of width and height.

Change the line containing cv2.resize function to the following:
`
im = cv2.resize(cv2.cvtColor(cv2.imread(""test.jpg""), cv2.COLOR_BGR2RGB), (10, 25))
`

and the program should hang",ran code really work however print function different order width height change line function following program,issue,negative,positive,neutral,neutral,positive,positive
1378265809,"> I think it is difficult to judge the presence of Gauss noise in your example. What will happen if you plot the diff between the original and the transformed?
> 
> ```python
> plt.imshow(im - tfs_im[""image""])
> ```
I think it works properly, meaning that Gaussian noise is added to the original image but it is not very noticeable when the noisy image is concatenated with the original one. When plotted the noise only (subtracting noisy image from the original image), noise is clear.

![image](https://user-images.githubusercontent.com/50166164/211726410-374fcb5b-fc17-4dc5-84ac-fd422dc196ab.png)

",think difficult judge presence gauss noise example happen plot original python image think work properly meaning noise added original image noticeable noisy image original one plotted noise noisy image original image noise clear image,issue,positive,positive,positive,positive,positive,positive
1378206307,"I think it is difficult to judge the presence of Gauss noise in your example. What will happen if you plot the diff between the original and the transformed?

```py
plt.imshow(im - tfs_im[""image""])
```
",think difficult judge presence gauss noise example happen plot original image,issue,negative,negative,neutral,neutral,negative,negative
1378142554,"@AL3708 and @Dipet 
I've implemented some of the bboxes transforms with numpy in [my repo](https://github.com/GdoongMathew/albumentations). But there's a little problem in this, unless you have more then 20 bounding boxes in one image, you can't really observe any true improvements, or even encounter some speed degradation since the boxes will have to be converted back and forth within each apply_to_bboxes. 
Here's some experiments.

|methods|for loop with 5 bboxes|numpy ops with 5 bboxes|for loop with 10 bboxes|numpy ops with 10 bboxes|for loop with 20 bboxes|numpy ops with 20 bboxes|
|---|---|---|---|---|---|---|
|HorizontalFlip|3151|2490|1802|2409|1148|1995|
|VerticalFlip|3195|2601|1864|2460|1147|2112|
|Flip|3223|2448|1558|2449|1132|2161|
|Rotate|349|342|312|355|277|337|
|SafeRotate|266|264|243|254|218|253|
|RandomRotate90|778|785|745|801|596|766|
|ShiftScaleRotate|333|330|311|336|256|340|
|Transpose|677|638|626|640|515|631|
|Pad|700|625|639|608|502|629|
|Perspective|269|264|241|280|193|283|
|RandomCropNearBBox|6327|3152|3532|3015|1970|2623|
|BBoxSafeRandomCrop|2241|1931|1293|1849|740|1708|
|CenterCrop|5309|2885|3019|2710|1626|2431|
|Crop|5323|2751|3044|2534|1729|2335|
|CropAndPad|809|728|697|722|546|712|
|RandomCropFromBorders|3141|2212|1942|2184|1096|1985|
|Affine|373|417|321|429|227|424|

Overall, I think the performance can be improved further if someone can figure out a way to remove the converting process while still do the label_field filtering.

By the way, the average bboxes / image in COCO Dataset is 7.19 based on [this](https://www.researchgate.net/figure/Dataset-statistics-Number-of-images-and-objects-per-split-average-number-of-objects_tbl1_324717066), so unless your application is in some manufacturing industry, which might have dozens of bboxes in one image waiting for prediction, other than that, this numpy operation currently won't help.
",al little problem unless bounding one image ca really observe true even encounter speed degradation since converted back forth within loop loop loop overall think performance someone figure way remove converting process still filtering way average image coco based unless application industry might one image waiting prediction operation currently wo help,issue,negative,positive,neutral,neutral,positive,positive
1376757258,"With Albumentations 1.3.0, python 3.9, and linux everything works properly.

```python
import albumentations as A
import cv2
import matplotlib.pyplot as plt

im = cv2.resize(cv2.cvtColor(cv2.imread(""test.jpg""), cv2.COLOR_BGR2RGB), (25, 10))
display(Image.fromarray(im))

plt.figure(figsize=(15,10))
display_ims = 20
var_limit = (0.0, 60.0)
p = 0.5

for i in range(display_ims):   
    tfs = A.Compose([A.RandomFog(fog_coef_lower=0.3, fog_coef_upper=0.3, alpha_coef=0.40, always_apply=True)])
    tfs_im = tfs(image=im)
    plt.subplot(4, display_ims // 4, i+1)
    plt.imshow(tfs_im[""image""])
    plt.axis(""off"")
```
",python everything work properly python import import import display range image,issue,negative,neutral,neutral,neutral,neutral,neutral
1376645215,"Check if your bbox is inside the image. Here's a short piece of code I wrote to check that. 

```
def check_bbox_in_image(fileName, bbox_tl, bbox_br):
    # open the image
    image = Image.open(os.path.join('./images', fileName))

    # bounding box to be checked
    bbox = box(bbox_tl[0], bbox_tl[1], bbox_br[0],bbox_br[1])

    # image dimensions
    width, height = image.size

    # check if the bounding box is inside the image
    if bbox[0] >= 0 and bbox[1] >= 0 and bbox[2] <= width and bbox[3] <= height:
        # the bounding box is inside the image
        print(""Bounding box is inside the image."")
    else:
        # the bounding box is outside the image
        print(""Bounding box is outside the image."")
```",check inside image short piece code wrote check open image image bounding box checked box image width height check bounding box inside image width height bounding box inside image print bounding box inside image else bounding box outside image print bounding box outside image,issue,negative,neutral,neutral,neutral,neutral,neutral
1373241143,"Many non related changes. Please, create new PR with changes that related only to your description.",many non related please create new related description,issue,positive,positive,positive,positive,positive,positive
1369845025,"I've been using this modified version passing a Numpy RandomState object and it is super useful for reproducibility. Albumentations is amazing, and I love using it, so I am here offering my help. What else should I do to get it merged?",version passing object super useful reproducibility amazing love offering help else get,issue,positive,positive,positive,positive,positive,positive
1365304215,"I can write the pipeline above using only the `additional_targets` parameter from `Compose`, but I still have the same issue. Or, for a simpler pipeline:

```
import albumentations as A
t1 = A.Flip()
t2 = A.ToGray()
c2 = A.Compose([t2], additional_targets={'image_for_t2': ""image""})
c = A.Compose([t1, c2], additional_targets={'image_for_t1_t2': ""image""})

print(c.transforms[1].transforms[0]._additional_targets)
```
I would expect the targets of `t1` to be ""image_for_t1_t2"", and the targets of `t2` to be both ""image_for_t1_t2"" and ""image_for_t2"". 
But the targets of `t2` is only {""image_for_t1_t2"": ""image""}. I'm guessing that, applying only part of the pipeline to some inputs, is not supported either. Which is fair.

Using `update` inside `add_targets` solve my issue. But I guess the less hacky way for me to do this is to implement this outside of Albumentations: apply the first part of the pipeline, add some targets, and apply the second part of the pipeline.
",write pipeline parameter compose still issue simpler pipeline import image image print would expect image guessing part pipeline either fair update inside solve issue guess le hacky way implement outside apply first part pipeline add apply second part pipeline,issue,negative,positive,positive,positive,positive,positive
1365292878,This method designed for internal use only. How to use additional targets you could read there: https://albumentations.ai/docs/examples/example_multi_target/,method designed internal use use additional could read,issue,negative,neutral,neutral,neutral,neutral,neutral
1362659014,"> Are you sure that you use correct environment? Try pip install -e . inside repo directory.

@Dipet, still same issue after checking the pip.",sure use correct environment try pip install inside directory still issue pip,issue,negative,positive,positive,positive,positive,positive
1362538208,Are you sure that you use correct environment? Try `pip install -e .` inside repo directory.,sure use correct environment try pip install inside directory,issue,negative,positive,positive,positive,positive,positive
1362533559,I do not know if this approach has some name or not. We copied this aproach from torchvision.transforms https://pytorch.org/vision/0.9/transforms.html,know approach name copied,issue,negative,neutral,neutral,neutral,neutral,neutral
1362496914,"Hi @Dipet,
How can I fix the documentation? Error output states to run: `python3 tools/make_transforms_docs.py make 
`

However when I run it, the output is not related to my code.

```
Traceback (most recent call last):
  File ""tools/make_transforms_docs.py"", line 194, in <module>
    main()
  File ""tools/make_transforms_docs.py"", line 174, in main
    transforms_info = get_transforms_info()
  File ""tools/make_transforms_docs.py"", line 80, in get_transforms_info
    if issubclass(cls, albumentations.DualIAATransform):
AttributeError: module 'albumentations' has no attribute 'DualIAATransform'
```",hi fix documentation error output run python make however run output related code recent call last file line module main file line main file line module attribute,issue,negative,positive,neutral,neutral,positive,positive
1362465896,"Hi! @Dipet,
It is little bit out of the topic but, is the design pattern for the communication between functional.py and transforms.py files has a specific name? transforms.py functions (Such as RandomShadow) calls the transforms functions. It is very interesting and cool approach for a newbie like me, I would love to learn more.

",hi little bit topic design pattern communication specific name interesting cool approach like would love learn,issue,positive,positive,positive,positive,positive,positive
1362454000,"I observed the same problem. It consumed most of my cpu usage and the training is very slow.
The cv2 trick worked for me",problem usage training slow trick worked,issue,negative,negative,negative,negative,negative,negative
1360928245,"Hi @Dipet,
I inspected the test failures, they do not look like comes from my side. Can you eloborate? I see bunch of numpy errors which are not related to my commit.",hi test look like come side see bunch related commit,issue,positive,neutral,neutral,neutral,neutral,neutral
1357689259,"Okay, serialization tests are fixed, however the ""test_additional_targets_for_image_only"" still fails, any input is welcome.",serialization fixed however still input welcome,issue,negative,positive,positive,positive,positive,positive
1353076197,"@Dipet Hi!,
There are some test in the test_serialization which are failing when I run pytest. However the functionality that I added (RandomGravel) not in the except_augmentations or augmentation_cls for these tests, so I do not understand why these tests testing this functionality.

Furthermore all of the failures are related to params are out of their limit (see below.) But I already checking them (in the transforms.py, RandomGravel class, I raise exceptions for them).


![1](https://user-images.githubusercontent.com/20031290/207872325-3b0c9ca9-098c-4ef1-a56e-86d2f5466e7e.png)


Clearly I lack the structure of the testing system, so any help would appreciated.

Edit: Turns out my implementation was wrong, not that I missunderstand the system.",hi test failing run however functionality added understand testing functionality furthermore related limit see already class raise clearly lack structure testing system help would edit turn implementation wrong system,issue,negative,negative,negative,negative,negative,negative
1349253209,"My issue was that I:

`img = img.transpose((2,0,1))  # H X W C -> C X H X W `

before the transformations. Albumentations expect the image in `H X W X C` which messed up the bbox transformation for me",issue expect image transformation,issue,negative,neutral,neutral,neutral,neutral,neutral
1348477955,"@Dipet Thanks. That helped out a lot, I guess I should read the documentation more thouroughly. ",thanks lot guess read documentation,issue,negative,positive,positive,positive,positive,positive
1346577464,"Hi!

In order not to create a new issue, I am continuing this issue here!. With a small change to :

```
def prepare(ds, shuffle=False, augment=False):
    
    ds = ds.map(resize_and_rescale, num_parallel_calls=AUTOTUNE)
        
    if augment:
      ds = ds.map(process_data, num_parallel_calls=AUTOTUNE)
    
    if shuffle:
        ds = ds.shuffle(buffer_size=1000)
    
    #ds = ds.batch(BATCH_SIZE)
    ds = ds.cache()
    return ds.prefetch(buffer_size=AUTOTUNE)
```

Note, that I don't use `ds.batch` , but `ds.cache()` now.

And at the same time:
```
def data_gen(path_imgs, path_masks, seed):
    
    images = image_dataset_from_directory(
        path_imgs, batch_size=BATCH_SIZE, shuffle=False, label_mode=None)
    
    masks = image_dataset_from_directory(
        path_masks, batch_size=BATCH_SIZE, shuffle=False, label_mode=None,
        color_mode='grayscale')
....
```

I am using `batch_size=BATCH_SIZE`, instead of `batch_size=None` here.

And I receive the same error:

`cv2.error in function 'warpAffine'`

Any ideas?
",hi order create new issue issue small change prepare augment shuffle return note use time seed instead receive error function,issue,negative,negative,neutral,neutral,negative,negative
1344525038,"No, otherwise, It works with odd sizes only when `allow_shifted=False`. In that case we need odd value to make centered kernel.
Yes. it will work correctly with even kenel sizes, but it maybe a little bit confusing whe you set limit to odd value, but it newer will be reached. At least we need warning that `kernel` is even and real limit is lower to by 1.",otherwise work odd size case need odd value make centered kernel yes work correctly even size maybe little bit set limit odd value least need warning kernel even real limit lower,issue,negative,negative,negative,negative,negative,negative
1344520774,"@Dipet Do you mean it works only with odd kernel sizes in the case when allow_shifted=True? Because it seems to me that when allow_shifted is False, it will perform something like __make_odd_val__ for generated parameters from the kernel. I mean lines:
https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/blur/transforms.py#L97
https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/blur/transforms.py#L107",mean work odd kernel size case false perform something like kernel mean,issue,negative,negative,negative,negative,negative,negative
1344514204,"Ok, I understand.It looks like current implementation works fine with odd `blur_limit` but we need to change  error message.
Need to change `centered=True` to `allow_shifted=False` inside error message and add description into the docstring that augmentation works only with odd kenel sizes in this case.",like current implementation work fine odd need change error message need change inside error message add description augmentation work odd size case,issue,negative,positive,neutral,neutral,positive,positive
1344503802,"@Dipet Error says that __blur_limit__ should be odd when centered=True. I mean that this expression:
```
if not allow_shifted and self.blur_limit[0] % 2 != 1 or self.blur_limit[1] % 2 != 1:
```
doesn't take into account __allow_shifted__ parameter at all (If self.blur_limir[1] is even it will always raise an error if __allow_shifted__ is True or False). If Motion blur works only with odd blur_limit we should specify it somewhere and make this expression without __allow_shifted__ variable. What do you think?",error odd mean expression take account parameter even always raise error true false motion blur work odd specify somewhere make expression without variable think,issue,negative,negative,negative,negative,negative,negative
1344495767,"I can not understand what the problem?
Motion blur only works with odd `blur_limit`.",understand problem motion blur work odd,issue,negative,negative,negative,negative,negative,negative
1342945056,I have also posted an issue/feature request/example request to improve TTA here: https://github.com/albumentations-team/albumentations_examples/issues/25,also posted request improve,issue,negative,neutral,neutral,neutral,neutral,neutral
1342822884,I think main problem may be in Resize method #1024,think main problem may resize method,issue,negative,positive,positive,positive,positive,positive
1342815493,"`ToTensor` only transforms image to `torh.Tensor` and change from `HWC` to `CHW` format.
If you need normalize image use `Normalize` transform.",image change format need normalize image use normalize transform,issue,negative,neutral,neutral,neutral,neutral,neutral
1342764827,"I see two reasons that might be causing your problem. Firstly, there seems to be a bug in ToTensorV2, see the issue I created: https://github.com/albumentations-team/albumentations/issues/1360 Secondly, in torchvision you first convert to tensor and then normalize while in albumentations you first normalize and then convert to tensor.",see two might causing problem firstly bug see issue secondly first convert tensor normalize first normalize convert tensor,issue,negative,positive,positive,positive,positive,positive
1334259254,"The dataset is uint8 actually, so I do not understand where the error comes from. I'm having this problem on multiple (but not all) filters (such RandomShadow, RandomSnow).",actually understand error come problem multiple,issue,negative,neutral,neutral,neutral,neutral,neutral
1334165439,Library do not support `float64` as an image dtype. Use `float32` instead.,library support float image use float instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1331741980,Please change it inside `Blur` too.,please change inside blur,issue,negative,neutral,neutral,neutral,neutral,neutral
1331741351,It looks like we need to replace all `np.arange` with a `range`,like need replace range,issue,negative,neutral,neutral,neutral,neutral,neutral
1331737988,Idiot approach inside python3.11 for checking if a sequence is empty 🤦‍♂️ ,idiot approach inside python sequence empty,issue,negative,negative,negative,negative,negative,negative
1331713148,Could you provide a reproducible example of the error?,could provide reproducible example error,issue,negative,neutral,neutral,neutral,neutral,neutral
1331680716,"> Just add a `albumentations.ToTensorV2` as the last transform into your pipeline to switch channel shape. https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2

Hello!

I was having the same error as the OP with the same code, and from Google I've got here

But I'm having a different error in my code (I do have the ToTensorV2 in my pipeline):

`TypeError: pic should be Tensor or ndarray. Got <class 'str'>.`

PyTorch version: 1.12.1
Albumentation version: 1.3.0

Sample code:
     
       import albumentations.pytorch as album_pytorch

        WIDTH = 384
        HEIGHT = 380
        AR_INPUT = WIDTH / HEIGHT

        base_augmentations = [
        A.Rotate(crop_border=True, p=1.0),
        A.Resize(height=HEIGHT, width=WIDTH, interpolation=cv2.INTER_CUBIC),
        A.Flip(p=1.0),
        A.RandomBrightnessContrast(p=1.0),
        A.Perspective(p=1.0),
        A.Sharpen(p=1.0),
        album_pytorch.transforms.ToTensorV2()
        ]
        
        class Transforms:
             def init(self, transforms: A.Compose):
                 self.transforms = transforms
        
             def __call__(self, img, *args, **kwargs):
                 return self.transforms(image=np.array(img))
        
        
        train_data = torchvision.datasets.ImageFolder(
        root=TRAIN_DATA_PATH, transform=Transforms(transforms=A.Compose(base_augmentations)))",add last transform pipeline switch channel shape hello error code got different error code pipeline pic tensor got class version version sample code import width height width height class self self return,issue,negative,neutral,neutral,neutral,neutral,neutral
1328040445,"I am trying to run a code repository downloaded from GitHub as it is mentioned in the instruction of that but getting following error.

TypeError: __init__() missing 1 required positional argument: 'image_paths'
I am having this error at the code line 63 (preprocessing = preprocessing).
When I srat the program in debug mode I shows following error
unable to get repr for <class 'albumentations.core.composition.compose'>
```
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import torch

from skimage import io

from utils import adjust_sar_contrast, compute_building_score, plot_images

sys.path.append('/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/work')
from spacenet6_model.configs.load_config import get_config_with_previous_experiment
from spacenet6_model.datasets import SpaceNet6TestDataset
from spacenet6_model.models import get_model
from spacenet6_model.transforms import get_augmentation, get_preprocess

# select previous experiment to load
exp_id = 14
exp_log_dir = ""/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/logs""  # None: use default

# select device to which the model is loaded
cuda = True
if cuda:
    device = 'cuda'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
else:
    device = 'cpu'
    os.environ['CUDA_VISIBLE_DEVICES'] = ''

# overwrite default config with previous experiment
config = get_config_with_previous_experiment(exp_id=exp_id, exp_log_dir=exp_log_dir)

# overwrite additional hyper parameters
config.MODEL.DEVICE = device

config.WEIGHT_ROOT = ""/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/weights/""
config.MODEL.WEIGHT = f""/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/weights/exp_{exp_id:04d}/model_best.pth""
config.INPUT.MEAN_STD_DIR = ""/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/work/models/image_mean_std/""
config.INPUT.TEST_IMAGE_DIR = ""/home/salman/data/SN6_buildings_AOI_11_Rotterdam_test_public/test_public/AOI_11_Rotterdam/SAR-Intensity""
config.INPUT.SAR_ORIENTATION=""/home/salman/Downloads/SpaceNet_SAR_Buildings_Solutions-master/4-motokimura/tmp/work/static/SAR_orientations.txt""
config.TRAIN_VAL_SPLIT_DIR=""/home/salman/Downloads/data/spacenet6/split""
config.PREDICTION_ROOT=""/home/salman/Downloads/data/spacenet6/predictions""
config.POLY_CSV_ROOT=""/home/salman/Downloads/data/spacenet6/polygons""
config.CHECKPOINT_ROOT=""/home/salman/Downloads/data/spacenet6/ceckpoints""
config.POLY_OUTPUT_PATH=""/home/salman/Downloads/data/spacenet6/val_polygons""
config.freeze()

print(config)

model = get_model(config)
model.eval();

from glob import glob
image_paths = glob(os.path.join(config.INPUT.TEST_IMAGE_DIR, ""*.tif""))
#print(image_paths)

preprocessing = get_preprocess(config, is_test=True)
augmentation = get_augmentation(config, is_train=False)

test_dataset = SpaceNet6TestDataset(
    config,
    augmentation=augmentation,
    preprocessing=preprocessing
)

test_dataset_vis = SpaceNet6TestDataset(
    config,
    augmentation=augmentation,
    preprocessing=None
)

channel_footprint = config.INPUT.CLASSES.index('building_footprint')
channel_boundary = config.INPUT.CLASSES.index('building_boundary')

score_thresh = 0.5
alpha = 1.0

start_index = 900
N = 20

for i in range(start_index, start_index + N):
    image_vis = test_dataset_vis[i]['image']
    image = test_dataset[i]['image']

    x_tensor = image.unsqueeze(0).to(config.MODEL.DEVICE)
    pr_score = model.module.predict(x_tensor)
    pr_score = pr_score.squeeze().cpu().numpy()

    pr_score_building = compute_building_score(
        pr_score[channel_footprint],
        pr_score[channel_boundary],
        alpha=alpha
    )
    pr_mask = pr_score_building > score_thresh

    rotated = test_dataset[i]['rotated']
    if rotated:
        image_vis = np.flipud(np.fliplr(image_vis))
        pr_mask = np.flipud(np.fliplr(pr_mask))

    plot_images(
        SAR_intensity_0=(adjust_sar_contrast(image_vis[:, :, 0]), 'gray'),
        building_mask_pr=(pr_mask, 'viridis')
    )
```

The function which this code calls is given below:
```
def get_spacenet6_preprocess(config, is_test):
    """"""
    """"""
    mean_path = os.path.join(
        config.INPUT.MEAN_STD_DIR,
        config.INPUT.IMAGE_TYPE,
        'mean.npy'
    )
    mean = np.load(mean_path)
    mean = mean[np.newaxis, np.newaxis, :]

    std_path = os.path.join(
        config.INPUT.MEAN_STD_DIR,
        config.INPUT.IMAGE_TYPE,
        'std.npy'
    )
    std = np.load(std_path)
    std = std[np.newaxis, np.newaxis, :]

    if is_test:
        to_tensor = albu.Lambda(
            image=functools.partial(_to_tensor)
        )
    else:
        to_tensor = albu.Lambda(
            image=functools.partial(_to_tensor),
            mask=functools.partial(_to_tensor)
        )

    preprocess = [
        albu.Lambda(
            image=functools.partial(
                _normalize_image,
                mean=mean,
                std=std
            )
        ),
        to_tensor,
    ]
    return albu.Compose(preprocess)
```
",trying run code repository instruction getting following error missing positional argument error code line program mode following error unable get class import import import o import import torch import io import import import import import select previous experiment load none use default select device model loaded true device else device overwrite default previous experiment overwrite additional hyper device print model import print augmentation alpha range image rotated rotated function code given mean mean mean else return,issue,negative,negative,negative,negative,negative,negative
1326565073,Thanks for the quick reply. It was indeed a deeper problem on my end regarding passing the parameterization. I will close as not relevant.,thanks quick reply indeed problem end regarding passing close relevant,issue,negative,positive,positive,positive,positive,positive
1326462679,"Hi @Dipet, I just pushed a simple modification that has a test specifically for the situation where the user wants to use the random state. Could you please give me another code example where this implementation will work incorrectly with python multiprocessing since it passes the original test? I never used albumentations before and I will start using it just because it seemed easier to implement the use of random states compared to torchvision transforms :smiley: 
Thanks!

",hi simple modification test specifically situation user use random state could please give another code example implementation work incorrectly python since original test never used start easier implement use random thanks,issue,positive,negative,neutral,neutral,negative,negative
1325903196,"> And by the way, how to find out which of the transformations are applied using an instance created by my_instance = alb.Compose(transform_list), if all of the p in the transform_list are 0.5?

For this purpose try to use [`ReplayCompose`](https://albumentations.ai/docs/examples/replay/#debugging-an-augmentation-pipeline-with-replaycompose)",way find applied instance purpose try use,issue,negative,neutral,neutral,neutral,neutral,neutral
1325894387,"Current implementation will work incorrectly with python multiprocessing because each process wil use the same random seed.
For this reason we used `random` istead of `np.random` as base random generator.",current implementation work incorrectly python process use random seed reason used random base random generator,issue,negative,negative,negative,negative,negative,negative
1325889268,"Can not reproduce the problem
```python
import albumentations as A
import numpy as np

img = np.empty([100, 100, 3], dtype=np.uint8)
keypoint_params = A.KeypointParams(format=""xy"")
t = A.Compose([A.Flip(p=1)], keypoint_params=keypoint_params)
for i in range(100):
    t(image=img, keypoints=[[10, 10], [20, 20]])
```",reproduce problem python import import range,issue,negative,neutral,neutral,neutral,neutral,neutral
1325854219,"Updates: it appears the issue of this shift is due to the default interpolation method for the alb.Downscale function. If  interpolation=cv2.INTER_CUBIC or interpolation=cv2.INTER_LINEAR then the transformed image and mask overlap correctly.  

And by the way, how to find out which of the transformations are applied using an instance created by my_instance = alb.Compose(transform_list), if all of the p in the transform_list are 0.5? ",issue shift due default interpolation method function image mask overlap correctly way find applied instance,issue,negative,negative,negative,negative,negative,negative
1322608692,"The mask is for the detection task actually.  That's why non-discrete values.
Any chance there is another interpolation that works for both discrete and non-discrete values you could apply? ",mask detection task actually chance another interpolation work discrete could apply,issue,negative,neutral,neutral,neutral,neutral,neutral
1321676707,"You are using a strange segmentation mask that has non-discrete values (not just 0 and 1).
We use `INTER_NEAREST ` interpolation for all masks, so you will always see changes on masks. https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121
Unfortunately, we don't currently have the option to change the interpolation flag for masks.",strange segmentation mask use interpolation always see unfortunately currently option change interpolation flag,issue,negative,negative,neutral,neutral,negative,negative
1319302334,"@glenn-jocher Hi, I have been unable to use albumentaion in a GCP compute engine environment. It keep s saying package not found, but the requirement is already satisfied. Also, why is it commented in the latest requirements.txt?
![image](https://user-images.githubusercontent.com/13908241/202574798-47a2cb0a-6a7b-425c-8cfa-623fb3e6ef7e.png)
![image](https://user-images.githubusercontent.com/13908241/202575353-f9d650f3-e296-4b0d-8310-677a13e22b76.png)

",hi unable use compute engine environment keep saying package found requirement already satisfied also latest image image,issue,negative,positive,positive,positive,positive,positive
1308667488,"@Dipet, do review my implementation of the feature that can be run with Compose. Your inputs will help me progress in getting the feature implemented in albumentations",review implementation feature run compose help progress getting feature,issue,positive,neutral,neutral,neutral,neutral,neutral
1303136366,"The simplest way - load transforms using old library version and save them using ours serialization to json/yaml. You can then load these transforms from newer versions of the library.
https://albumentations.ai/docs/examples/serialization/#Serializing-an-augmentation-pipeline-to-a-JSON-or-YAML-file",way load old library version save serialization load library,issue,negative,positive,neutral,neutral,positive,positive
1298884546,"Problem fixed with latest release. Everything works fine, I can not reproduce any problem with clear virtual environment.",problem fixed latest release everything work fine reproduce problem clear virtual environment,issue,negative,positive,positive,positive,positive,positive
1298541107,Issue continues with latest pip version (1.3.0) as well. Installing from source did not fix too.,issue latest pip version well source fix,issue,negative,positive,positive,positive,positive,positive
1298385339,Thank you for the timely review and merge @jangop  @Dipet ,thank timely review merge,issue,negative,neutral,neutral,neutral,neutral,neutral
1298026590,"Sorry for the additional correction.
The current implementation assumes that the number of all objects in an image is less than 255(""the length of target masks"" + ""the number of pasted objects"" < 255).
So, I added some validation code and documentation about the limitation.
I think the number 255 is sufficiently large for a typical usecase, but if needed, it can be increased.",sorry additional correction current implementation number image le length target number pasted added validation code documentation limitation think number sufficiently large typical,issue,negative,negative,negative,negative,negative,negative
1297427312,Ran flake8 and black locally @Dipet ,ran flake black locally,issue,negative,negative,neutral,neutral,negative,negative
1297383278,"Also if anyone stumbles on this before the fix is put in, you can workaround this issue by limiting the range of the input to `tan` to `(.25, .75)` avoid the integer overflow (pick whatever arbitrary limit you want, but keep in mind `tan((.25 + epsilon)*2*pi)) = O(1/epsilon)`)

```
epsilon = .001
A.RandomSunFlare(..., angle_lower=.25 + epsilon, angle_upper=.75 - epsilon, ...)
```",also anyone fix put issue limiting range input tan avoid integer overflow pick whatever arbitrary limit want keep mind tan epsilon pi epsilon epsilon epsilon,issue,negative,negative,neutral,neutral,negative,negative
1296211267,I am not sure that this approach must be used in all tasks. But I agree that we must support this case. It is might be implemented with additional flag in transforms that could change aspect ratio.,sure approach must used agree must support case might additional flag could change aspect ratio,issue,positive,positive,positive,positive,positive,positive
1294787316,"> In that case, specify `scipy>=1.1.0` in `setup.py`. At the moment, no version is specified for `scipy` at all.

I've adapted the branch accordingly and rebased to master.",case specify moment version branch accordingly master,issue,negative,neutral,neutral,neutral,neutral,neutral
1292671928,"From what I can tell, the result is substantially the same, it's possible some randomness should be added to the `t` value, but it seems mostly fine.

new examples:

![out_63](https://user-images.githubusercontent.com/107961397/198141580-80ccc9c0-55dd-4ec0-993d-b2eb96d000f5.png)
![out_67](https://user-images.githubusercontent.com/107961397/198141619-c656f877-33ea-476e-8f04-9bd596fc5d1d.png)
![out_69](https://user-images.githubusercontent.com/107961397/198141625-79a317db-8e0b-4db8-b3bc-e9d399b971e4.png)

previous examples:

![out_56](https://user-images.githubusercontent.com/107961397/198141693-5828ec36-d937-4ffd-986d-a83d2a80b0fb.png)
![out_55](https://user-images.githubusercontent.com/107961397/198141702-3f9f9d62-09c9-4729-8101-02827b080b39.png)
![out_57](https://user-images.githubusercontent.com/107961397/198141715-fb498c22-ce0a-4cf5-b5ec-1b7fd4be38c1.png)

",tell result substantially possible randomness added value mostly fine new previous,issue,negative,positive,neutral,neutral,positive,positive
1292552598,"If I understand this correctly, basically when the angle is pi/2 or 3pi/2 the sun is directly above or below, so the sampling of centers along the x direction doesn't work.  My trivial fix to clip will at least remove the crashes but will leave behind a ""data"" bug at samples near pi/2 and 3pi/2.  I believe there might already be a bit of a data bug here because the range of tan is covered in pi, not 2pi, so I believe the current approach might be difficult to parameterize since it is double sampling the range.

A more complete fix would be to generate points from the the line defined by centroid and the angle and then sample along that line both in the positive and negative direction.",understand correctly basically angle sun directly sampling along direction work trivial fix clip least remove leave behind data bug near believe might already bit data bug range tan covered pi pi believe current approach might difficult since double sampling range complete fix would generate line defined centroid angle sample along line positive negative direction,issue,negative,negative,neutral,neutral,negative,negative
1292461538,"The root cause is https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L689

```
math.tan(pi/2-epsilon) --> inf
```

I'm not sure I understand the goal of the `tan` here well enough to offer a fix at the moment, but I'll see if I can put a PR together.",root cause sure understand goal tan well enough offer fix moment see put together,issue,positive,positive,positive,positive,positive,positive
1292444561,"@Dipet I wrote a script to just keep making new images until it broke printing out the center, and can say with some certainty that the problem has to do with python ints not being 64 bit, but opencv being implemented in cpp using 64 bit ints.

Full code here (repro for me took ~24 hours using this single threaded approach).

```
from PIL import Image
import albumentations as A
import numpy as np

class FakeDataset:
    def __init__(self):
        self.fake_len = 1000
        self.aug = A.RandomSunFlare(p=1.0)


    def __len__(self):
        return self.fake_len


    def __getitem__(self, idx):
        image = np.array(Image.new('RGB', (311,255)))
        return self.aug(image=image)


if __name__ == ""__main__"":
    import pdb 
    dataset = FakeDataset()

    while True:
        val = dataset[0]
```

Patch to add debug prints
```
diff --git a/albumentations/augmentations/functional.py b/albumentations/augmentations/functional.py
index 700bb79..ff74ac0 100644
--- a/albumentations/augmentations/functional.py
+++ b/albumentations/augmentations/functional.py
@@ -698,6 +698,7 @@ def add_sun_flare(img, flare_center_x, flare_center_y, src_radius, src_color, ci
     output = img.copy()
 
     for (alpha, (x, y), rad3, (r_color, g_color, b_color)) in circles:
+        print(f""center: ({x},{y})"")
         cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1) 
 
         cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)
```


Output (clipped obviously)
```
...
center: (180,184)
center: (90,-8)
center: (210,248)
center: (180,184)
center: (100,137)
center: (250,-199)
center: (70,204)
center: (280,-267)
center: (240,-177)
center: (100,137)
center: (60,227)
center: (40,-1444370434)
center: (0,-2194692778)
Traceback (most recent call last):
  File ""test_dl.py"", line 25, in <module>
    val = dataset[0]
  File ""test_dl.py"", line 17, in __getitem__
    return self.aug(image=image)
  File ""/home/jason.rock/albumentations/albumentations/core/transforms_interface.py"", line 118, in __call__
    return self.apply_with_params(params, **kwargs)
  File ""/home/jason.rock/albumentations/albumentations/core/transforms_interface.py"", line 131, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""/home/jason.rock/albumentations/albumentations/augmentations/transforms.py"", line 656, in apply
    return F.add_sun_flare(
  File ""/home/jason.rock/albumentations/albumentations/augmentations/utils.py"", line 107, in wrapped_function
    result = func(img, *args, **kwargs)
  File ""/home/jason.rock/albumentations/albumentations/augmentations/functional.py"", line 702, in add_sun_flare
    cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)
cv2.error: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'circle'
> Overload resolution failed:
>  - Can't parse 'center'. Sequence item with index 1 has a wrong type
>  - Can't parse 'center'. Sequence item with index 1 has a wrong type
```",wrote script keep making new broke printing center say certainty problem python bit bit full code took single threaded approach import image import import class self self return self image return import true patch add git index output alpha rad print center overlay rad overlay alpha output alpha output output clipped obviously center center center center center center center center center center center center center recent call last file line module file line return file line return file line key file line apply return file line result file line overlay rad error bad argument function overload resolution ca parse sequence item index wrong type ca parse sequence item index wrong type,issue,negative,negative,neutral,neutral,negative,negative
1291552008,"It is might be hard because MotionBlur is a convolution with some kernel. We can not just simple convolve mask too, because this  operation will change mask values. Maybe we could convolve mask, take new mask, make dilation with same filter size, after that take mask values from dilated mask based on convolved mask. But this operation very slow.

We need to do research to find a way how to properly change the mask.",might hard convolution kernel simple convolve mask operation change mask maybe could convolve mask take new mask make dilation filter size take mask dilated mask based mask operation slow need research find way properly change mask,issue,negative,negative,neutral,neutral,negative,negative
1291258211,"Thanks. 
But if I use the transformed mask and apply it to the motion blurred image, then there will be a lot of left overs from the grass in the mask and as well player on the grass. 
Is there any way to shift the mask the same amount that the player is shifted? ",thanks use mask apply motion blurred image lot left grass mask well player grass way shift mask amount player,issue,positive,positive,neutral,neutral,positive,positive
1290501711,"Thank you for finding this terrible bug. Please add this test case into tests.
```python
@pytest.mark.parametrize(
    [""keypoints"", ""expected_keypoints"", ""holes""],
    [
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [], [(40, 40, 60, 60), (70, 70, 80, 80), (10, 10, 20, 20)]],
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [], [(10, 10, 20, 20), (40, 40, 60, 60), (70, 70, 80, 80)]],
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [], [(40, 40, 60, 60), (10, 10, 20, 20), (70, 70, 80, 80)]],
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [(75, 75, 0, 0)], [(40, 40, 60, 60), (10, 10, 20, 20)]],
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [(50, 50, 0, 0)], [(70, 70, 80, 80), (10, 10, 20, 20)]],
        [[(50, 50, 0, 0), (75, 75, 0, 0)], [(50, 50, 0, 0), (75, 75, 0, 0)], [(10, 10, 20, 20)]],
    ]

)
def test_coarse_dropout_remove_keypoints(keypoints, expected_keypoints, holes):
    t = A.CoarseDropout()
    result_keypoints = t.apply_to_keypoints(keypoints, holes)

    assert set(result_keypoints) == set(expected_keypoints)
```",thank finding terrible bug please add test case python assert set set,issue,negative,negative,negative,negative,negative,negative
1290447675,I fixed the bug and created a pull request: https://github.com/albumentations-team/albumentations/pull/1330.,fixed bug pull request,issue,negative,positive,neutral,neutral,positive,positive
1290412175,"I noticed that with one single hole the `CoarseDropout` augmentation seems to work correctly:

```python
augmentations = A.Compose(
    [
        A.CoarseDropout(
            min_holes=1,
            max_holes=1,
            min_height=32,
            min_width=32,
            max_height=264,
            max_width=264,
            fill_value=0,
            p=1.0,
        )
    ],
    keypoint_params=A.KeypointParams(format=""xy"", remove_invisible=True),
)
```",one single hole augmentation work correctly python,issue,negative,negative,neutral,neutral,negative,negative
1290164273,"> [...] Specifically for scipy, I tested that the ""new"" import location (i.e. the one that is not deprecated, and which this PR starts using) has been around in scipy 1.1.0 from 2012 [...]

In that case, specify `scipy>=1.1.0` in `setup.py`. At the moment, no version is specified for `scipy` at all.
",specifically tested new import location one around case specify moment version,issue,negative,positive,positive,positive,positive,positive
1290118210,"> Does albumentations have a release schedule?

No, we don't have it.

> Any established way to mark the function as deprecated and scheduled for removal?

This function is not a transform and made for internal use. So we could remove it with any next PR with code refactoring.
In common, we trying do not remove any transform. We just add a deprecation warning and hide it from the documentation.",release schedule established way mark function removal function transform made internal use could remove next code common trying remove transform add deprecation warning hide documentation,issue,negative,negative,neutral,neutral,negative,negative
1290105796,"> You are right. I think we could add tracking shape before and after by add this logic [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py#L213) and [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py#L217). In first case we need to save shapes before transform call. In second case - save shapes at the top of `__call__`

Great suggestions. I do not know the code well enough to form a preference. 🤷 From my standpoint, either option seems fine.

> It looks like legacy, because I do not see any usage of this function.

👍 Does albumentations have a release schedule? Any established way to mark the function as deprecated and scheduled for removal?",right think could add shape add logic first case need save transform call second case save top great know code well enough form preference standpoint either option fine like legacy see usage function release schedule established way mark function removal,issue,positive,positive,positive,positive,positive,positive
1290045144,"You are right. I think we could add tracking shape before and after by add this logic [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py#L213) and [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py#L217).
In first case we need to save shapes before transform call. In second case - save shapes at the top of `__call__`

> One last point to add: there is also [filter_bboxes_by_visibility](https://github.com/albumentations-team/albumentations/blob/3904898e3bfe476137a75cad6ae3d1015e3d468e/albumentations/core/bbox_utils.py#L236) which seems to have been written with similar intent to filter_boxes, but is entirely untested and with a different signature. Can anyone recall why filter_bboxes_by_visibility exists?

It looks like legacy, because I do not see any usage of this function.",right think could add shape add logic first case need save transform call second case save top one last point add also written similar intent entirely untested different signature anyone recall like legacy see usage function,issue,positive,positive,positive,positive,positive,positive
1290027872,">  So, is there any way to do training with hard negatives using albumentations for data augmentation

Just add hard negatives after transform.",way training hard data augmentation add hard transform,issue,negative,negative,negative,negative,negative,negative
1288128667,"Sorry, I made a stupid mistake. Closing it for now. ",sorry made stupid mistake,issue,negative,negative,negative,negative,negative,negative
1285831431,"Revisiting this thread. 

For some experiments, we would like to train with hard negatives. The prescribed method for including those in the training is to have boxes with all zeros (for example : this is suggested by the author for [EfficientDet](https://github.com/rwightman/efficientdet-pytorch/issues/71)). 

Doing so raises the exception above (`ValueError: x_max is less than or equal to x_min for bbox (0.0, 0.0, 0.0, 0.0, 0)`). So, is there any way to do training with hard negatives using albumentations for data augmentation ?",thread would like train hard method training example author exception le equal way training hard data augmentation,issue,negative,negative,negative,negative,negative,negative
1283706323,Hey @neuronflow. Thank you for your report. I have fixed the site pages. These links should work now.,hey thank report fixed site link work,issue,negative,positive,neutral,neutral,positive,positive
1282617454,"> I think that we also need to update the version requirements for libs for these changes https://github.com/albumentations-team/albumentations/blob/master/setup.py#L8

Which dependency do you have in mind? Specifically for scipy, I tested that the ""new"" import location (i.e. the one that is not deprecated, and which this PR starts using) has been around in scipy 1.1.0 from 2012. I didn't run the full test suite to find out what the minimal version requirement actually are, but I doubt that this change would change that in practice.
That being said, I'm happy to run more tests or adapt version specs in this PR, if you let me know in what direction.",think also need update version dependency mind specifically tested new import location one around run full test suite find minimal version requirement actually doubt change would change practice said happy run adapt version spec let know direction,issue,negative,positive,positive,positive,positive,positive
1282387605,Now we check that image and mask shapes are equal. More info in #1310 ,check image mask equal,issue,negative,neutral,neutral,neutral,neutral,neutral
1281816874,I think that we also need to update the version requirements for libs for these changes https://github.com/albumentations-team/albumentations/blob/master/setup.py#L8,think also need update version,issue,negative,neutral,neutral,neutral,neutral,neutral
1281813799,"Looks like you trying to provide into transform tensorflow tensors, not numpy arrays. And looks like you have some problems with image shape. Check that you provide not batched images in `HWC` format.",like trying provide transform like image shape check provide format,issue,positive,neutral,neutral,neutral,neutral,neutral
1281467764,"I think that the main problem is that you are trying to augment images after you batch them (Albumentations process one image/mask at a time). The proper way would be:
```
def prepare(ds, shuffle=False, augment=False):
    
    ds = ds.map(resize_and_rescale, num_parallel_calls=AUTOTUNE)
    
    if augment:
      ds = ds.map(process_data, num_parallel_calls=AUTOTUNE)

    if shuffle:
        ds = ds.shuffle(buffer_size=1000)
    
    ds = ds.batch(BATCH_SIZE)
    return ds.prefetch(buffer_size=AUTOTUNE)
```

Also, you may find a bit better way to resize your images. Maybe you can try something from Albumentations like: __LongestMaxSize__ + __PadIfNeeded__. But this all is your choice. ",think main problem trying augment batch process one time proper way would prepare augment shuffle return also may find bit better way resize maybe try something like choice,issue,negative,positive,positive,positive,positive,positive
1281338428,@claverru Thanks! I hit almost exactly the same problem :)  ,thanks hit almost exactly problem,issue,negative,positive,positive,positive,positive,positive
1280700512,"So! The code works! If you use:

```
transforms= A.Compose(
[
    A.HorizontalFlip(p = 0.5), 
    A.OneOf(
        [
            A.RandomGamma(), 
            A.RandomBrightnessContrast(), 
        ],
        p = 0.5 
    ),

])  
```

it works fine!

If you add anyone of :

```
A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1),
A.CLAHE (p=0.5),
A.PadIfNeeded(min_height=4, min_width=4, p=1)
A.Rotate(p=0.3),
```

(I haven't tried something else also)

it crashes:
`
cv2.error: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'`",code work use work fine add anyone tried something else also error assertion function,issue,negative,positive,positive,positive,positive,positive
1280503770,"I am attaching a fully working code.

Right now, I am receiving:

`cv2.error: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/imgwarp.cpp:2595: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'`

I have changed the `process_data` function to return 2 arguments.

```
import albumentations as A
import tensorflow as tf
from tensorflow.keras.utils import image_dataset_from_directory
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, \
    Activation, MaxPooling2D, Dropout
from tensorflow.keras import backend as K
from tensorflow.keras.models import Model


def jaccard_coef(y_true,y_pred,smooth=1):
    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)
    jac = (intersection + smooth) / (sum_ - intersection + smooth)
    return jac

def jaccard_distance(y_true, y_pred, smooth=1):
    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)
    sum_ = K.sum(K.square(y_true), axis = -1) + K.sum(K.square(y_pred), axis=-1)
    jac = (intersection + smooth) / (sum_ - intersection + smooth)
    return (1 - jac)

IMG_HEIGHT = 256
IMG_WIDTH = 256
CHANNELS = 3
FILTERS = 16
BATCH_NORM = True
NB_CLASSES = 1
OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)
AUTOTUNE = tf.data.experimental.AUTOTUNE
BATCH_SIZE = 16
SEED = 123
LOSS = jaccard_distance #'binary_crossentropy'
METRIC =  [jaccard_coef]

path_imgs = ('./data/images/train')
path_masks =('./data/masks/train')

transforms= A.Compose(
[
    A.HorizontalFlip(p = 0.5), 
    A.Rotate(p=0.3),
])   

def aug_fn(image, mask):
    aug_data = transforms(image=image, mask=mask)
    aug_img = aug_data[""image""]
    aug_mask = aug_data[""mask""]
    print('IMAGE inside aug_fn func :', aug_img.shape)
    print('MASK inside aug_fn func :', aug_mask.shape)
    return aug_img, aug_mask

    
def process_data(image, mask):
    aug_img, aug_mask = tf.numpy_function(func=aug_fn, 
                                          inp=[image, mask],
                                          Tout=[tf.float32, tf.float32])
    print('IMAGE inside process_data func :', aug_img.shape)
    print('MASK inside process_data func:', aug_mask.shape)
    return aug_img, aug_mask

def resize_and_rescale(image, mask):
    image = tf.image.resize(image, (IMG_HEIGHT, IMG_WIDTH))
    mask = tf.image.resize(mask, (IMG_HEIGHT, IMG_WIDTH))
    image = image / 255.0
    # mask = tf.cast(mask, tf.uint8)
    return image, mask
    
def prepare(ds, shuffle=False, augment=False):
    
    ds = ds.map(resize_and_rescale, num_parallel_calls=AUTOTUNE)
    
    if shuffle:
        ds = ds.shuffle(buffer_size=1000)
    
    ds = ds.batch(BATCH_SIZE)
    
    if augment:
      ds = ds.map(process_data, num_parallel_calls=AUTOTUNE)
    
    return ds.prefetch(buffer_size=AUTOTUNE)


def split_train_test_val(ds,
                         seed,
                         train_split=0.8, 
                         val_split=0.1, 
                         test_split=0.1,
                         shuffle=True, 
                         shuffle_size=1000):
                              
    assert (train_split + test_split + val_split) == 1
    
    ds_size = len(ds)
    if shuffle:
        ds = ds.shuffle(shuffle_size, 
                        reshuffle_each_iteration=False,
                        seed=seed)
    
    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)
    
    train_ds = ds.take(train_size)    
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)
    
    return train_ds, val_ds, test_ds

def data_gen(path_imgs, path_masks, seed):
    
    images = image_dataset_from_directory(
        path_imgs, batch_size=None, shuffle=False, label_mode=None)
    
    masks = image_dataset_from_directory(
        path_masks, batch_size=None, shuffle=False, label_mode=None,
        color_mode='grayscale')
    
    dataset = tf.data.Dataset.zip((images, masks))
        
    train_ds, val_ds, test_ds = split_train_test_val(dataset,
                                                     SEED,
                                                     train_split=0.8, 
                                                     val_split=0.1, 
                                                     test_split=0.1,
                                                     shuffle=True, 
                                                     shuffle_size=1000)
   
    train_steps = len(train_ds)
    val_steps = len(val_ds)
    
    train_ds = prepare(train_ds, shuffle=True, augment=True)
    val_ds = prepare(val_ds)
    test_ds = prepare(test_ds)
    
    return train_ds, val_ds, test_ds, train_steps, val_steps


def build_model(height,
                width,
                channels,
                filters,
                batch_norm,
                nb_classes):
    inputs = Input((height, width, channels))
    x = Conv2D(filters=filters,
               kernel_size=(3, 3),
               kernel_initializer='he_normal',
               padding='same')(inputs)
    # second layer
    x = Conv2D(filters=filters,
               kernel_size=(3, 3),
               kernel_initializer='he_normal',
               padding='same')(x)
    if batch_norm:
        x = BatchNormalization()(x)
    x = Activation('relu')(x)
    x = Dropout(0.3)(x)
    
    outputs =  Conv2D(filters=nb_classes,
                     kernel_size=(1, 1), 
                     activation='sigmoid',
                     padding='same')(x)
    
    model = Model(inputs=inputs, outputs=outputs)
    return model

model = build_model(IMG_HEIGHT, 
                    IMG_WIDTH,
                    CHANNELS,
                    FILTERS,
                    BATCH_NORM,
                    NB_CLASSES)

model.compile(optimizer=OPTIMIZER,
              loss=LOSS,
              metrics=METRIC)

train_ds, val_ds, test_ds, steps_per_epoch, validation_steps  = data_gen(
    path_imgs,
    path_masks,
    SEED)

history = model.fit(train_ds,
                    validation_data=val_ds,
                    batch_size=BATCH_SIZE,
                    epochs=2)
```

the dataset I am using is: [here](https://www.kaggle.com/datasets/franciscoescobar/satellite-images-of-water-bodies)

You can find a small sample of the data [here](https://files.fm/f/ad7mv5m2m)",fully working code right error assertion function function return import import import import input activation dropout import import model intersection axis intersection smooth intersection smooth return intersection axis intersection smooth intersection smooth return true seed loss metric image mask image mask print inside print inside return image mask image mask print inside print inside return image mask image image mask mask image image mask mask return image mask prepare shuffle augment return seed assert shuffle return seed seed prepare prepare prepare return height width input height width second layer activation dropout model model return model model seed history find small sample data,issue,negative,positive,positive,positive,positive,positive
1280359965,"Yes, I am aware of this example but it uses only images, not masks. 
And the problem is in this function

` aug_img = tf.numpy_function(func=aug_fn, inp=[image, img_size], Tout=tf.float32)`

Where it uses the `aug_fn`. If I use `transforms(image=image, mask=mask)`  and then something like:

```
def aug_fn(image, mask):
    data = {""image"":image,
            'mask': mask}
    aug_data = transform(**data)
    aug_img = aug_data[""image""]
    aug_mask = aug_data[""mask""]
    return aug_img, aug_mask
```

And 

```
def process_data(image, mask):
    aug_img, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, mask], Tout=tf.float32)
    return aug_img, aug_mask
```

I receive:
```

  aug_img, aug_mask = tf.numpy_function(func=aug_fn, inp=[image, mask], Tout=tf.float32)

    OperatorNotAllowedInGraphError: Iterating over a symbolic `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```

I am not sure if I can return  2 parameters from the function `aug_img` and  `aug_mask`..

The example returns only `aug_img`.

That's why I tried using 2 aug_fn functions as in my first post.
",yes aware example problem function image use something like image mask data image image mask transform data image mask return image mask image mask return receive image mask symbolic graph execution use eager execution decorate function sure return function example tried first post,issue,positive,positive,positive,positive,positive,positive
1280062574,"Hi! I think that you have to take a look at this example https://albumentations.ai/docs/examples/tensorflow-example/ . Also it is possible to augment image and mask in one call. Something like this:
```
transforms(image=image, mask=mask)
```",hi think take look example also possible augment image mask one call something like,issue,negative,neutral,neutral,neutral,neutral,neutral
1280058318,"I would like to work on this one. If I understand it correctly we would like to add a new __pad_mode__ for __CropAndPad__  and __border_mode__ for __PadIfNeeded__ classes to allow users to use image mean color instead of predefined Opencv modes, am I right? ",would like work one understand correctly would like add new class allow use image mean color instead right,issue,positive,positive,neutral,neutral,positive,positive
1279219564,Looks like we need to add `rotate_method=ellipse` for `Affine`.,like need add affine,issue,negative,neutral,neutral,neutral,neutral,neutral
1278395616,"After mirroring in the key points of the human body, the left and right hands will be marked wrong. What is the return information, indicating that the picture has been mirrored? Thank you",key human body left right marked wrong return information picture mirrored thank,issue,negative,negative,neutral,neutral,negative,negative
1278394521,"I'm sorry, it's my use problem, I forgot to add label_fields=[""class_labels""], I suggest you can set this as an error, otherwise, it's really hard to debug. Thank you for your open source project.",sorry use problem forgot add suggest set error otherwise really hard thank open source project,issue,negative,negative,negative,negative,negative,negative
1277112436,"@Dipet , sorry for the late update. I have tried using the image only transform class for creating a new class named ""MetaData"" for the eraser feature. I have implemented some methods to it. Do review it and let me know if those methods are correct or not. Also I will be removing those images in my next commit once I get a reply from you regarding the methods of the new class.",sorry late update tried image transform class new class eraser feature review let know correct also removing next commit get reply regarding new class,issue,negative,negative,negative,negative,negative,negative
1274633442,The reason to check only the first mask from masks is the same as it is for types checking.,reason check first mask,issue,negative,positive,positive,positive,positive,positive
1274523973,"Result does not change unfortunately
<img width=""1269"" alt=""Screenshot 2022-10-11 at 13 15 26"" src=""https://user-images.githubusercontent.com/80039909/195076157-fe8e81e7-3046-46ff-acae-a160f8fe0697.png"">

`TRAIN_TRANSFORMS = A.Compose(
    [
        A.Resize(width=640, height=640, interpolation=cv2.INTER_LINEAR),
        A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.4, p=0.4),
        A.OneOf(
            [
                A.ShiftScaleRotate(
                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT, rotate_method=""ellipse""
                ),
                A.Affine(shear=15, p=0.5, mode=cv2.BORDER_CONSTANT),
            ],
            p=1.0,
        ),
        A.HorizontalFlip(p=0.5),
        A.Blur(p=0.1),
        A.CLAHE(p=0.1),
        A.Posterize(p=0.1),
        A.ToGray(p=0.1),
        A.ChannelShuffle(p=0.05),
        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format=""coco"", min_visibility=0.4, label_fields=[],),
)`
",result change unfortunately ellipse coco,issue,negative,negative,negative,negative,negative,negative
1274499290,"Adding evidence on a COCO img:
On the left ground_truth bboxes, on the right augmented bboxes
Looks quite horrible tbh, to you suggest any turnaround?
<img width=""1268"" alt=""Screenshot 2022-10-11 at 12 52 52"" src=""https://user-images.githubusercontent.com/80039909/195072016-43eda7c1-728b-41d1-abff-10eabb289b04.png"">

Here below the augmentations:

`TRAIN_TRANSFORMS = A.Compose(
    [
        A.Resize(width=640, height=640, interpolation=cv2.INTER_LINEAR),
        A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),
        A.OneOf(
            [
                A.ShiftScaleRotate(
                    rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT
                ),
                A.Affine(shear=15, p=0.5, mode=cv2.BORDER_CONSTANT),
            ],
            p=1.0,
        ),
        A.HorizontalFlip(p=0.5),
        A.Blur(p=0.1),
        A.CLAHE(p=0.1),
        A.Posterize(p=0.1),
        A.ToGray(p=0.1),
        A.ChannelShuffle(p=0.05),
        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format=""coco"", min_visibility=0.4, label_fields=[],),
)`


`VAL_TRANSFORM = A.Compose(
    [
        A.LongestMaxSize(max_size=IMAGE_SIZE),
        A.PadIfNeeded(
            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT
        ),
        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format=""coco"", min_visibility=0.4, label_fields=[]),
)`
",evidence coco left right augmented quite horrible suggest turnaround coco coco,issue,negative,negative,negative,negative,negative,negative
1274268716,I think check inside `Compose` is enough. In common usage of augs outside of `Compose` is incorrect.,think check inside compose enough common usage outside compose incorrect,issue,negative,negative,neutral,neutral,negative,negative
1272559434,"@Dipet I have two proposals for this issue:
* We can add a check for the same height and width inside __Compose__ class's method ___check_args__. It will work I think, but it will not cover the problem if augmentations are used without __Compose__ class 
* We can add a similar __check_args_ method to BasicTransform/DualTransform class. I think it will cover the usage of augs without Compose class but it will cause some overhead because __check_args_ for BasicTransform/DualTransform would be called for every augmentation call.

What do you think?",two issue add check height width inside class method work think cover problem used without class add similar method class think cover usage without compose class cause overhead would every augmentation call think,issue,negative,neutral,neutral,neutral,neutral,neutral
1272556380,"Hey, thank you, the company was added to the list of companies that use Albumentations - https://albumentations.ai/whos_using",hey thank company added list use,issue,negative,neutral,neutral,neutral,neutral,neutral
1272294149,I would like to take this one!,would like take one,issue,negative,neutral,neutral,neutral,neutral,neutral
1271800808,Hey! Would like to take this one. Will post PR soon.,hey would like take one post soon,issue,negative,neutral,neutral,neutral,neutral,neutral
1267042569,"Okey, not exactly what I wanted. It now only changes tone curve for the whole image, not per channel, so new augmentation is still needed. Or maybe just modify the `RandomToneCurve` with `per_channel` flag as for some other augmentations.",exactly tone curve whole image per channel new augmentation still maybe modify flag,issue,negative,positive,positive,positive,positive,positive
1263827537,The changes are too small and difference is only in the interpolation flag. Try to set the same `interpolation` and `mask_interpolation`.,small difference interpolation flag try set interpolation,issue,negative,negative,negative,negative,negative,negative
1263195091,"> I can not reproduce the problem on clear environment with python 3.8.10

Thanks for reply, I have solved it by upgrating the typing_extensions to the lastest version.",reproduce problem clear environment python thanks reply version,issue,negative,positive,positive,positive,positive,positive
1263174508,I can not reproduce the problem on clear environment with python 3.8.10,reproduce problem clear environment python,issue,negative,positive,positive,positive,positive,positive
1263162049,"> > 
> 
> I just encountered the same issue. I'am using Python 3.8.3. I tried typing_extensions-4.3.0 and -3.7.4, both of them do not work.

Update: I later tried command pip install -U typing_extensions, and it works. Hope it helps.",issue python tried work update later tried command pip install work hope,issue,negative,neutral,neutral,neutral,neutral,neutral
1263157374,"> 

I just encountered the same issue. I'am using Python 3.8.3. I tried  typing_extensions-4.3.0 and -3.7.4, both of them do not work.",issue python tried work,issue,negative,neutral,neutral,neutral,neutral,neutral
1257221806,"Попробуй поставить try except и при ошибке вывести параметры маски и
изображения. Скорее всего у тебя маска и изображение по шейпам на 1 пиксел
отличаются во время ошибки

вс, 25 сент. 2022 г., 21:44 Michael Monashev ***@***.***>:

> Так и у меня воспроизводится редко. Долго работает, а потом ошибку выдаёт.
> И возможно оно падает не на изображении, а на паре: картинка+маска.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/1295#issuecomment-1257221035>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABZKBOWNDROLMLOUYJ6D223WABXPTANCNFSM6AAAAAAQTFSG7Y>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",try except reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1257077085,"что-то не получается вопроизвести
```python
import albumentations as A
import numpy as np
import cv2
from tqdm import tqdm

t = A.Compose([
    A.PadIfNeeded(min_height=320, min_width=320, position=""random"", border_mode=cv2.BORDER_CONSTANT, value=(0,0,0), p=1.0),
    A.RandomCrop(320, 320, always_apply=True, p=1.0),
])

for i in tqdm(range(321 * 321)):
    x = i // 321
    y = i % 321
    img = np.empty([x, y, 3], dtype=np.uint8)
    t(image=img)

```",python import import import import random range,issue,negative,negative,negative,negative,negative,negative
1255980115,"@Dipet thank you for your reply.

Do You mean that the `paste_image_key` is used to set a path to the png file as a target?

```
transform(image=image, paste_image_key=""path/to/png"", ...)
```

If so, I think using `past_image_dir` is a better choice since the `past_image_dir` can be used as a parameter of the constructor. And we can avoid introducing new targets and follow the standard usage.

Ex.

```py 
transform Compose([
    CopyAndPaste(paste_image_dir=objects_dir, …),
...
])

transform(image=image, …)   # we can still use only the standard targets bboxes, and masks.
```

To get a feel for it, I made a workable example in PR #1297 (still working). ",thank reply mean used set path file target transform think better choice since used parameter constructor avoid new follow standard usage ex transform compose transform still use standard get feel made workable example still working,issue,negative,positive,neutral,neutral,positive,positive
1252349251,"Sure.
A.Rotate (limit=[45, 45], interpolation=1, border_mode=0, value=None, mask_value=None, method='largest_box', crop_border=True, always_apply=True, p=1). This command generated the first image (approximately ran in july).
![git_1](https://user-images.githubusercontent.com/102518682/191266474-4ef72f35-cc96-4618-82b5-9705b59eefe2.png)

A.Rotate (limit=[45, 45], interpolation=1, border_mode=0, value=None, mask_value=None, rotate_method='largest_box', crop_border=True, always_apply=True, p=1). This command generated the second image (ran these days).
![git_2](https://user-images.githubusercontent.com/102518682/191266811-fe264524-641d-47a2-b239-a4d5f68d596a.png)

Leaving aside the color of the images, these two start from the same amount of circular black artifact; despite the same treatment (run two months later), they are a bit different.
I thought the reason for this was a possible change in the Rotate function, but at this point I don't know what happened.",sure command first image approximately ran command second image ran day leaving aside color two start amount circular black artifact despite treatment run two later bit different thought reason possible change rotate function point know,issue,negative,positive,neutral,neutral,positive,positive
1252233305,"This PR do not change the logic of rotations. It is just change name of argument.
Also, this param used only for bbox, image is not affected by this argument.
Could you provide reproducible example and library versions, on which you apply augmentation?",change logic change name argument also param used image affected argument could provide reproducible example library apply augmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
1252188549,"Actually, in both cases I used a fixed angle of 45 degrees, because the
crop performed by Rotate was one of my goals.  With the same treatment, the
crop was more marked before than now;  so, kindly, do you confirm that what
I said is possible?

On Tue, 20 Sep 2022, 11:48 Mikhail Druzhinin, ***@***.***>
wrote:

> Rotate sample random rotation angle, so yes, they are might be different.
> Try to fix random seed and check result:
>
> import randomrandom.seed(0)
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/pull/1258#issuecomment-1252108944>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AYOE7GW63N2OPE2N5TPISB3V7GB7FANCNFSM565HV2WA>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",actually used fixed angle crop rotate one treatment crop marked kindly confirm said possible tue wrote rotate sample random rotation angle yes might different try fix random seed check result import reply directly view id,issue,positive,negative,neutral,neutral,negative,negative
1252177786,"Solve: this happens because the first time I've installed albumentations I didn't installed it with `--no-binary`. I've manually uninstalled relevant packages and then reinstalled as needed and it worked!

```
pip install opencv-python
...
pip install albumentations #this was the first mistake- this added opencv-python-headless altough I have opencv-python installed

# this is how to fix it
pip uninstall -y opencv-python-headless qudida albumentations
pip install -U albumentations --no-binary qudida,albumentations
```",solve first time manually uninstalled relevant worked pip install pip install first added fix pip pip install,issue,negative,positive,positive,positive,positive,positive
1252108944,"Rotate sample random rotation angle, so yes, they are might be different. Try to fix random seed and check result:
```python
import random
random.seed(0)
``` ",rotate sample random rotation angle yes might different try fix random seed check result python import random,issue,negative,negative,negative,negative,negative,negative
1252105917,"Greetings,
I wanted to ask for confirmation. I have previously used the rotate function with the old 'method' on some images. After some time, I rerun the rotate function with 'rotate_method' on the same photos. In both cases 'crop_border' was True. However, I noticed that the resulting crops are different. Is it possible that in the change of the function you also have changed how much the images are cropped? Thanks in advance.",ask confirmation previously used rotate function old time rerun rotate function true however resulting different possible change function also much thanks advance,issue,positive,positive,neutral,neutral,positive,positive
1251988335,"Unfortunately, now we don't have our own resources to add this support. If someone tries add this functionality to the library, we will be very happy.
If you need GPU augmentations you might look to https://github.com/NVIDIA/DALI and https://github.com/kornia/kornia",unfortunately add support someone add functionality library happy need might look,issue,positive,positive,positive,positive,positive,positive
1250642133,"> ```python
> if self.transform is not None:
>     try:
>         transformed = self.transform(image=image, mask=mask)
>     except Exception:
>         print(image, mask)
>         raise
> ```

Hi, Thanks for the reply. I get an error which is below:

```python
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Input In [3], in PikeDataset.__init__(self, images_directory, masks_directory, mask_filenames, transform)
      9 try:
---> 10     transformed = self.transform(image=image, mask=mask)
     11 except Exception:

NameError: name 'image' is not defined

During handling of the above exception, another exception occurred:

NameError                                 Traceback (most recent call last)
Input In [6], in <cell line: 12>()
      9 val_mask_filenames = mask_filenames[:5]
     10 train_mask_filenames = mask_filenames[5:]
---> 12 train_dataset = PikeDataset(image_small_dir, mask_small_dir, train_mask_filenames, 
     13                             transform=train_transform)
     15 val_dataset = PikeDataset(image_small_dir, mask_small_dir, val_mask_filenames, 
     16                           transform=val_transform)

Input In [3], in PikeDataset.__init__(self, images_directory, masks_directory, mask_filenames, transform)
     10     transformed = self.transform(image=image, mask=mask)
     11 except Exception:
---> 12     print(image, mask)
     13     raise

NameError: name 'image' is not defined
```
Not sure how to fix the issue.

Thanks & Best Regards
Schroter Michael",python none try except exception print image mask raise hi thanks reply get error python recent call last input self transform try except exception name defined handling exception another exception recent call last input cell line input self transform except exception print image mask raise name defined sure fix issue thanks best,issue,positive,positive,positive,positive,positive,positive
1250448129,"```python
if self.transform is not None:
    try:
        transformed = self.transform(image=image, mask=mask)
    except Exception:
        print(image, mask)
        raise
```",python none try except exception print image mask raise,issue,negative,neutral,neutral,neutral,neutral,neutral
1250446357,"> `resize` is the first transform in your pipeline. Try to check output of `cv.imread`

Hi, Thanks for your reply. The values are within a fucntion. Hence, I am not sure exactly how to test it. Nevertheless, I added:

`assert image is not None` after `cv.imread` but is gives me the same error. 

Thanks & Best Regards
Schroter Michael",resize first transform pipeline try check output hi thanks reply within hence sure exactly test nevertheless added assert image none error thanks best,issue,positive,positive,positive,positive,positive,positive
1250436355,`resize` is the first transform in your pipeline. Try to check output of `cv.imread`,resize first transform pipeline try check output,issue,negative,positive,positive,positive,positive,positive
1250435836,"> Looks like you provide an empty image into transofrm

Hi, Thanks for the reply. Could you please eloborate a bit more. Where the mistake has been made please. There are many 'transform' in the script. 
Thanks & Best Regards
Schroter Michael",like provide empty image hi thanks reply could please bit mistake made please many script thanks best,issue,positive,positive,positive,positive,positive,positive
1250413872,Looks like you provide an empty image into transofrm,like provide empty image,issue,negative,negative,neutral,neutral,negative,negative
1250412410,"This tansform is not released yet to PYPI. Try to install library from github: `pip install git+https://github.com/albumentations-team/albumentations`
We will releasea  new version to PYPI soon.",yet try install library pip install new version soon,issue,negative,positive,positive,positive,positive,positive
1250411683,">  I want to pad images to be at least 224 x 224, and then also have dimensions to be divisible by 32

Yes, looks like we need to add flag to support this kind of transform.
You are right - you can use 2 `PadIfNeeded`:
```python
t = A.Compose([
    A.PadIfNeeded(min_width=256, min_height=256),
    A.PadIfNeeded(pad_height_divisor=32, pad_width_divisor=32, min_width=None, min_height=None),
])
```",want pad least also divisible yes like need add flag support kind transform right use python,issue,positive,positive,positive,positive,positive,positive
1248845114,"> Have you tried [PadIfNeeded](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded) with `pad_height_divisor=32, pad_width_divisor=32`?
> 
> Example:
> 
> ```python
> import numpy as np
> import albumentations as A
> 
> t = A.Compose([A.PadIfNeeded(pad_height_divisor=32, pad_width_divisor=32, min_width=None, min_height=None)])
> 
> img = np.empty([123, 457, 3], dtype=np.uint8)
> res = t(image=img)[""image""]
> print(res.shape)
> ```

Thank you! I think this would do the job. Just to clarify one more point with you:  suppose I want to pad images to be at least 224 x 224, and then also have dimensions to be divisible by 32, can I compose 2 A.PadIfNeeded() transforms together? first one would have min_width/min_height set to 256, and the second will have pad_height_divisor/width_divisor set to 32, while min_width/min_height set to None. Reason being PadIfNeeded apparently does not allow me to specify both min_width and pad_width_divisor in the same transform. ",tried example python import import image print thank think would job clarify one point suppose want pad least also divisible compose together first one would set second set set none reason apparently allow specify transform,issue,negative,positive,neutral,neutral,positive,positive
1248029030,"> @Dipet I have the same the same issue as the first post:
> 
> ```
>   File ""/home/hoel/.local/lib/python3.10/site-packages/albumentations/augmentations/functional.py"", line 979, in add_sun_flare
>     cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)
> cv2.error: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'circle'
> > Overload resolution failed:
> >  - Can't parse 'center'. Sequence item with index 1 has a wrong type
> >  - Can't parse 'center'. Sequence item with index 1 has a wrong type
> ```
> 
> However I cannot make a reproducible example. During my trainings, the error appeared seemingly randomly after 40 and 28 epochs. I thought it might be a bad combination of augmentations, so I put all the probabilities at one in an attempt to reproduce it, but the error still does not appear straight away. From the error message, it's as if the x and y extremely rarely, randomly, have a wrong type like a float when creating the circle arguments ([here](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L687) I assume). But since the coordinates are explicitly casted to ints, I don't see how this is happening.
> 
> ### Environment
> * Albumentations version: 1.2.1
> * Python version: 3.10
> * OS: Linux
> * How you installed albumentations: pip

So was I! The error occurs at random in the training stage.",issue first post file line overlay rad error bad argument function overload resolution ca parse sequence item index wrong type ca parse sequence item index wrong type however make reproducible example error seemingly randomly thought might bad combination put one attempt reproduce error still appear straight away error message extremely rarely randomly wrong type like float circle assume since explicitly see happening environment version python version o pip error random training stage,issue,negative,negative,negative,negative,negative,negative
1247552776,"@Dipet I have the same the same issue as the first post:

```
  File ""/home/hoel/.local/lib/python3.10/site-packages/albumentations/augmentations/functional.py"", line 979, in add_sun_flare
    cv2.circle(overlay, (x, y), rad3, (r_color, g_color, b_color), -1)
cv2.error: OpenCV(4.6.0) :-1: error: (-5:Bad argument) in function 'circle'
> Overload resolution failed:
>  - Can't parse 'center'. Sequence item with index 1 has a wrong type
>  - Can't parse 'center'. Sequence item with index 1 has a wrong type
```

However I cannot make a reproducible example. During my trainings, the error appeared seemingly randomly after 40 and 28 epochs. I thought it might be a bad combination of augmentations, so I put all the probabilities at one in an attempt to reproduce it, but the error still does not appear straight away.
From the error message, it's as if the x and y extremely rarely, randomly, have a wrong type like a float when creating the circle arguments ([here](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L687) I assume). But since the coordinates are explicitly casted to ints, I don't see how this is happening.

### Environment
- Albumentations version: 1.2.1
- Python version: 3.10
- OS: Linux
- How you installed albumentations: pip",issue first post file line overlay rad error bad argument function overload resolution ca parse sequence item index wrong type ca parse sequence item index wrong type however make reproducible example error seemingly randomly thought might bad combination put one attempt reproduce error still appear straight away error message extremely rarely randomly wrong type like float circle assume since explicitly see happening environment version python version o pip,issue,negative,negative,negative,negative,negative,negative
1245859266,"Have you tried [PadIfNeeded](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.PadIfNeeded) with `pad_height_divisor=32, pad_width_divisor=32`?

Example:
```python
import numpy as np
import albumentations as A

t = A.Compose([A.PadIfNeeded(pad_height_divisor=32, pad_width_divisor=32, min_width=None, min_height=None)])

img = np.empty([123, 457, 3], dtype=np.uint8)
res = t(image=img)[""image""]
print(res.shape)
```",tried example python import import image print,issue,negative,neutral,neutral,neutral,neutral,neutral
1244851170,"To give some context - this is the transform i want to implement (i want to pad each image up to the nearest multiple of ""size_divisible"", e.g. 32) - which can be readily achieved using torchvision:
```python
class PadToNearestMultiple:
    """"""pad to nearest multiple of specified size""""""

    def __init__(self, size_divisible = 32):
        assert isinstance(size_divisible, int)
        self.sd = size_divisible

    def __call__(self, image):
        return pad_to_nearest_multiple(image, self.sd)

def pad_to_nearest_multiple(image, size_divisible = 32):
    if isinstance(image, np.ndarray):
        return_array = True
        image = Image.fromarray(image)
    else:
        return_array = False
    size = list(image.size) # w, h
    stride = float(size_divisible)
    new_w = int(math.ceil(float(size[0]) / stride) * stride) 
    new_h = int(math.ceil(float(size[1]) / stride) * stride)
#     print(new_w)
#     print(new_h)
    # left, top, right and bottom
    padding = (0, 0, new_w - size[0], new_h - size[1])
    image = TF.pad(image, padding = padding, fill = (0,0,0), padding_mode = 'constant')
    if return_array:
        return np.array(image)
    else:
        return image
        
transforms = PadToNearestMultiple(size_divisible=32)
img = Image.open(filename)
img = transforms(img)
```
However, I have been struggling to translate this into albumentions. Any help would be appreciated, thanks!       ",give context transform want implement want pad image nearest multiple readily python class pad nearest multiple size self assert self image return image image image true image image else false size list stride float float size stride stride float size stride stride print print left top right bottom padding size size image image padding padding fill return image else return image however struggling translate help would thanks,issue,positive,positive,positive,positive,positive,positive
1244842970,"> * `targets_as_params` - if you want to use some targets (arguments that you pass when call the augmentation pipeline) to produce some augmentation parameters on aug call, you need to list all of them here. When the transform is called, they will be provided in `get_params_dependent_on_targets`. For example: `image`, `mask`, `bboxes`, `keypoints` - are standard names for our targets.
> * `get_params_dependent_on_targets` - used to generate parameters based on some targets. If your transform doesn't depend on any target, only it's own arguments, you can use `get_params`.  These functions are used to produce params once per call, this is useful when you are producing some random or heavy params.
> * `get_transform_init_args_names` - used for serialization purposes. If params names in `__init__` are equal to the params names stored inside the transform, you can just enumerate them inside this function. Otherwise, if you have some custom serialization logic, you will have to override the `_to_dict` method. We may remove remove this function in the future when someone implements automatic parsing of the `__init__` call.

Unfortunately i have the exact same issue, and I get the following error when running the example you provided:
TypeError: apply() got an unexpected keyword argument 'cols'

This is on version '1.2.1'",want use pas call augmentation pipeline produce augmentation call need list transform provided example image mask standard used generate based transform depend target use used produce per call useful random heavy used serialization equal inside transform enumerate inside function otherwise custom serialization logic override method may remove remove function future someone automatic call unfortunately exact issue get following error running example provided apply got unexpected argument version,issue,negative,negative,neutral,neutral,negative,negative
1242149572,Looks like no one has worked on this bug yet.,like one worked bug yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1242146652,"Do not call `apply` directly.
```python
A.RandomResizedCrop(height=100, width=100)(image=np.ones((300, 300, 3)).astype(np.float32))
```",call apply directly python,issue,negative,positive,neutral,neutral,positive,positive
1239624559,"try again after you run the command below
`pip install ""opencv-python-headless<4.3""`",try run command pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
1238025822,"I have added the parameter `clamp_bbox=False` to `BBoxParams` which will control whether bboxes will be forcefully clamped to remain between 0 and 1.
Thanks.",added parameter control whether forcefully remain thanks,issue,negative,positive,positive,positive,positive,positive
1237893848,"We can not change bboxes silently, because these changes might be incorrect in some cases. Better to add flag `clamp_bbox=False` into this function and into [`BboxParams and BboxProcessor`](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/bbox_utils.py#L65)",change silently might incorrect better add flag function,issue,negative,positive,positive,positive,positive,positive
1236803398,That's great. I'll do a MR in the next few days. As soon as I get some free time,great next day soon get free time,issue,positive,positive,positive,positive,positive,positive
1236233327,"I made some minor changes to follow up on recent albumentation updates.
And I integrated bbox reflection functionality to ShiftScaleRotate transform by adding a new argument `reflec_annotation`",made minor follow recent reflection functionality transform new argument,issue,negative,positive,neutral,neutral,positive,positive
1236115405,"Hi. sorry that I did not write explanation.
Need to remove `sampleImages` directory. We decided do not store any binary data inside this repository. For examples and documentation we hav this repository https://github.com/albumentations-team/albumentations_examples4
The current implementaion does not inherit from `ImageOnlyTransform` or `DuatlTransorm` and can not work inside `Compose`. For these reason we can not now aprove this PR.
 ",hi sorry write explanation need remove directory decided store binary data inside repository documentation repository current inherit work inside compose reason,issue,negative,negative,negative,negative,negative,negative
1236114502,"I think we could support this:
- Add `past_image_key` param for transform - if this key is set we will use provided images
- Add `past_image_dir` parms for transform - is this key is set we will sample random image from this directory
- Otherwise sample random segment from the image.",think could support add param transform key set use provided add transform key set sample random image directory otherwise sample random segment image,issue,negative,negative,negative,negative,negative,negative
1236062676,"@Dipet, I request you to explain the label ""wontfix"". Does this mean that this feature is of no help to albumentations repo and won't be merged to the master?",request explain label mean feature help wo master,issue,negative,negative,negative,negative,negative,negative
1236022725,"@ternaus  Hi, I'm interested in the feature.

How can we specify the pasted object in an albumentation's transform? Do we need to introduce a new target keyword like 'paste_image':

```py
transform(image=image, paste_image=paste_image, ...)
```

, or sample segments from the same target image as yolov5 is doing?

https://github.com/ultralytics/yolov5/blob/15e82d296720d4be344bf42a34d60ffd57b3eb28/utils/dataloaders.py#L706",hi interested feature specify pasted object transform need introduce new target like transform sample target image,issue,positive,positive,positive,positive,positive,positive
1235456870,"Yes, like `clipped` function https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/utils.py#L67",yes like clipped function,issue,positive,neutral,neutral,neutral,neutral,neutral
1235426115,So if the dtype is `np.float32` then the max value is 1.0 and if it's any `int` dtype then the max is 255? And it should detect the type automatically as oppose to having a parameter that the user can specify?,value detect type automatically oppose parameter user specify,issue,negative,neutral,neutral,neutral,neutral,neutral
1235422862,Now this transform only supports images with a `np.uint8` dtype. Need to add support for any dtype and chose a max value based on the image dtype.,transform need add support chose value based image,issue,positive,neutral,neutral,neutral,neutral,neutral
1235419406,"Apologies, it seems I have misunderstood the task.
I would like to try again if that's alright.
To confirm, the task is to add the ability to invert `float32` dtype images. What would be the range of values? 0-1?
Thank you",misunderstood task would like try alright confirm task add ability invert float would range thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1235302717,"These changes are incorrect. They are only overwrites `BaseTransform` logic and do not affect support of float32.
Original issues says about support `np.float32` dtype images.",incorrect logic affect support float original support,issue,positive,positive,positive,positive,positive,positive
1235224271,We are always glad to see new functionality in the library. You are welcome to add this transform to  the library.,always glad see new functionality library welcome add transform library,issue,positive,positive,positive,positive,positive,positive
1234124032,"By the way, I'm actually using it with a custom version of albumentations, but would love to contribute to the project.",way actually custom version would love contribute project,issue,positive,positive,positive,positive,positive,positive
1233957063,"Hi,
I got it....I was using ver. 1.0.3
Now that I have updated to the latest one everything works fine
Thank you
D.
",hi got latest one everything work fine thank,issue,positive,positive,positive,positive,positive,positive
1233111923,"I can not reproduce the problem

```python
import albumentations as A
import numpy as np
import matplotlib.pyplot as plt
import random

random.seed(0)
np.random.seed(0)

img = np.full([100, 100, 3], 0, dtype=np.float32)

t = A.Compose(
    [
        A.LongestMaxSize(max_size=768, interpolation=1),
        A.PadIfNeeded(min_height=768, min_width=768, border_mode=0, value=(1, 1, 1)),
        A.HorizontalFlip(),
        A.VerticalFlip(),
        A.Rotate(limit=180, border_mode=0, value=(1, 1, 1), p=0.7),
        A.GridDropout(
            ratio=0.3,
            unit_size_min=128,
            unit_size_max=384,
            random_offset=True,
            fill_value=(0.5, 0.5, 0.5),
            mask_fill_value=None,
            always_apply=False,
            p=0.7,
        )
    ]
)
A.save(t, ""test.json"")
t = A.load(""test.json"")

res = t(image=img)[""image""]
plt.imshow(res)
plt.show()
```",reproduce problem python import import import import random image,issue,negative,negative,negative,negative,negative,negative
1233104310,"> Hey, I would like to take this on.

You are welcome.

> My guess is that the purpose of float p is to be the probability of returning the inverted image. Is that correct?

Correct.

> And if so, in what circumstances is there a need for this feature?

Because we want to apply augmentations randomly.

> After implementing it as such, one of the tests fail. I think this is because the test for InvertImg doesn't account for this probability p.

All tests must work with `p=1`. It they fails - something wrong inside you changes or you forget update tests. Read tests error messages carefully.",hey would like take welcome guess purpose float probability inverted image correct correct need feature want apply randomly one fail think test account probability must work something wrong inside forget update read error carefully,issue,negative,negative,negative,negative,negative,negative
1233097261,"Hi,
I guess there is a problem here.
I created a transformation with fill_value=(0.5,0.5,0.5) (see below) and everything works if I apply it to the image
However when saving the transformation to file and then loading it back, the fill_value is lost and the image is transformed using BLACK as fill_value!
Here is my transformation:
`transform = A.Compose(
    [
    A.LongestMaxSize(max_size=768, interpolation=1),
    A.PadIfNeeded(min_height=768, min_width=768, border_mode=0, value=(1,1,1)),
    A.HorizontalFlip(),
    A.VerticalFlip(),
    A.Rotate(limit=180, border_mode=0, value=(1,1,1),
             p=0.7),
    A.GridDropout(ratio=0.3, unit_size_min=128, unit_size_max=384,
                 random_offset=True,
                  fill_value=(0.5,0.5,0.5), mask_fill_value=None,
                  always_apply=False, p=0.7),
    ToTensorV2()
    ]
)`

Please let me know asap because I always used to ""load"" the transformation from file and this means the transformation I was using was wrong...
Thank you

Best
D.  ",hi guess problem transformation see everything work apply image however saving transformation file loading back lost image black transformation transform please let know always used load transformation file transformation wrong thank best,issue,negative,positive,neutral,neutral,positive,positive
1231648265,"I can not reproduce this problem and I do not have access to your notebook.
Please, provide a minimal reproducible code example.",reproduce problem access notebook please provide minimal reproducible code example,issue,negative,negative,neutral,neutral,negative,negative
1231624561,"Does the problem exist on the in latest version of the library?
Try call `np.ascontiguousarray` for image and mask before provide them into transforms.",problem exist latest version library try call image mask provide,issue,negative,positive,positive,positive,positive,positive
1231205554,the problem happens to me..how do u fix it? I am pretty sure that the input type is right,problem fix pretty sure input type right,issue,negative,positive,positive,positive,positive,positive
1229238120,"After implementing it as such, one of the tests fail. I think this is because the test for InvertImg doesn't account for this probability `p`.",one fail think test account probability,issue,negative,negative,negative,negative,negative,negative
1229228495,"Hey, I would like to take this on.
My guess is that the purpose of `float p` is to be the probability of returning the inverted image. Is that correct?
And if so, in what circumstances is there a need for this feature?
Thank you",hey would like take guess purpose float probability inverted image correct need feature thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1227165500,"> Try this:
> 
> from albumentations import BBoxSafeRandomCrop, HorizontalFlip, OpticalDistortion

Thanks! New to Python, didn't think of that ",try import thanks new python think,issue,negative,positive,positive,positive,positive,positive
1227151554,"It looks like website is taking sources from the github master branch and it is different to latest pip version. @creafz I'm right? Can we add to website select button to choose albumentations version based on github tags?
@KhorunzhyiAl Why are you importing augs using a direct module path?
Try this:
```python
from albumentations import BBoxSafeRandomCrop, HorizontalFlip, OpticalDistortion
```",like taking master branch different latest pip version right add select button choose version based direct module path try python import,issue,negative,positive,positive,positive,positive,positive
1225703324,"Sorry, that I didn't write the answer. Yes, `0.5,0.5,0.5` means grey color.
Example:
```python
import albumentations as A
import numpy as np
import matplotlib.pyplot as plt


img = np.full([100, 100, 3], 0, dtype=np.float32)
t = A.Compose([A.GridDropout(fill_value=(0.5, 0.5, 0.5), p=1)])
res = t(image=img)[""image""]
plt.imshow(res)
plt.show()
```

> Which is the exact format for this parameter?

Any value that represents image color. For grayscale image it will be single number (float or int), for multichannel images - single number or sequence of number with length equal to count of channels.  

I added these tags because we need enhance documentation and add correct types and descriptions for all functions that uses fill values.",sorry write answer yes grey color example python import import import image exact format parameter value image color image single number float single number sequence number length equal count added need enhance documentation add correct fill,issue,positive,negative,neutral,neutral,negative,negative
1225692851,"I saw this issue has been marked as ""enhancement"".
Because I am actually running tests, could someone meanwhile tell me if using fill_value=(0.5,0.5,0.5) is correct to represent grey color in RGB images normalized between 0 and 1?
Can I use this format?
Thank you

Davide",saw issue marked enhancement actually running could someone meanwhile tell correct represent grey color use format thank,issue,negative,positive,neutral,neutral,positive,positive
1221577784,Thanks a lot @iswarup I was facing the same problem and now it has been fixed,thanks lot facing problem fixed,issue,negative,positive,positive,positive,positive,positive
1220466840,"Yes, now you also have to provide an image too. It looks like we need to change this logic for ReplayCompose.

Try this:
```python
houghed_wireframe_data = A.ReplayCompose.replay(data['replay'], image=np.emty_like(img), houghed_wireframe=houghed_wireframe)
```",yes also provide image like need change logic try python data,issue,positive,neutral,neutral,neutral,neutral,neutral
1220263685,Do I need to provide `image` again? Will that change the augmentations from the first function call?,need provide image change first function call,issue,negative,positive,positive,positive,positive,positive
1219486949,"Now, when you call `A.ReplayCompose.replay` you must provide `image` to",call must provide image,issue,negative,neutral,neutral,neutral,neutral,neutral
1216100221,"Hi @dav-ell 
How to write albumentations pipeline wrapper? Can you provide your whole code?",hi write pipeline wrapper provide whole code,issue,negative,positive,positive,positive,positive,positive
1214838875,"No, I didn't have any issues further.
Make sure that the model is training (model sees some values) by checking once again the input bounding boxes.",make sure model training model input bounding,issue,negative,positive,positive,positive,positive,positive
1214773103,"@kulkarnikeerti 
Thanks again, I was able to fix the error.

But, now I see that all the AP values are 0. Did you encounter this issue?",thanks able fix error see encounter issue,issue,negative,positive,positive,positive,positive,positive
1214148015,"In my case, when I wrongly set a non-Transform object in the `A.Compose`, I got a similar TypeError.

An example:
```py
## `HorizontalFlip` is missing `()`
transforms = A.Compose([A.HorizontalFlip])
transforms(image=image)
#->  TypeError: __init__() got an unexpected keyword argument 'force_apply'

# fixed version
transforms = A.Compose([A.HorizontalFlip()])
transforms(image=image) # OK

```",case wrongly set object got similar example missing got unexpected argument fixed version,issue,negative,negative,neutral,neutral,negative,negative
1213182578,`from albumentations import Crop` or `from albumentations.augmentations.crops.transforms import Crop`,import crop import crop,issue,negative,neutral,neutral,neutral,neutral,neutral
1213035999,"- `targets_as_params` - if you want to use some targets (arguments that you pass when call the augmentation pipeline) to produce some augmentation parameters on aug call, you need to list all of them here. When the transform is called, they will be provided in `get_params_dependent_on_targets`. For example: `image`, `mask`, `bboxes`, `keypoints` - are standard names for our targets.
- `get_params_dependent_on_targets` - used to generate parameters based on some targets. If your transform doesn't depend on any target, only it's own arguments, you can use `get_params`.  These functions are used to produce params once per call, this is useful when you are producing some random or heavy params.
- `get_transform_init_args_names` - used for serialization purposes. If params names in `__init__` are equal to the params names stored inside the transform, you can just enumerate them inside this function. Otherwise, if you have some custom serialization logic, you will have to override the `_to_dict` method. We may remove remove this function in the future when someone implements automatic parsing of the `__init__` call.",want use pas call augmentation pipeline produce augmentation call need list transform provided example image mask standard used generate based transform depend target use used produce per call useful random heavy used serialization equal inside transform enumerate inside function otherwise custom serialization logic override method may remove remove function future someone automatic call,issue,negative,negative,neutral,neutral,negative,negative
1213024232,"Thank you for the example @Dipet . Can you please explain what are `get_transform_init_args_names`, `get_params_dependent_on_targets` and `targets_as_params` methods used for? It is way more verbose than I expected.",thank example please explain used way verbose,issue,positive,neutral,neutral,neutral,neutral,neutral
1211771218,"Try this:
```python
import numpy as np
import albumentations as A

from typing import List, Tuple, Dict, Any


class Test(A.ImageOnlyTransform):
    def __init__(self, param_key: str = ""param1"", always_apply=False, p=0.5):
        super().__init__(always_apply, p)
        self.param_key = param_key

    def get_transform_init_args_names(self) -> Tuple[str]:
        return (""param_key"",)

    @property
    def targets_as_params(self) -> List[str]:
        return [self.param_key]

    def get_params_dependent_on_targets(self, params: Dict[str, Any]) -> Dict[str, int]:
        return {""my_new_param"": params[self.param_key]}

    def apply(self, img: np.ndarray, my_new_param=None) -> np.ndarray:
        # do something
        return img + my_new_param


aug = A.Compose([Test()])
image = np.empty([100, 100, 3], dtype=np.uint8)
result = aug(image=image, param1=100)
```",try python import import import list class test self param super self return property self list return self return apply self something return test image result,issue,positive,positive,positive,positive,positive,positive
1210727570,Thank you so much for sharing the details. I shall try to fix it!,thank much shall try fix,issue,negative,positive,positive,positive,positive,positive
1210712483,"Inside docstring wee see that we have to use int values, so the user has to implement this logic themselves.",inside wee see use user implement logic,issue,negative,neutral,neutral,neutral,neutral,neutral
1210706250,We always need image as parameter for transform,always need image parameter transform,issue,negative,neutral,neutral,neutral,neutral,neutral
1210672791,A don't see a much difference from the current implementation of `CoarseDropout`,see much difference current implementation,issue,negative,positive,neutral,neutral,positive,positive
1210374308,"Everything is working fine today, so these changes are not necessary.",everything working fine today necessary,issue,negative,positive,positive,positive,positive,positive
1209205242,"@enaskopelja @vanderschuea I finally tried this today (rather than turning the polylines into a segmentation map and working with that)

I ran into the following issues:
1) keypoints allows 1 set of keypoints to be transformed . I have multiple polylines. 
2) The decision if to keep a point is per point rather than per polyline so I might end up with part of a polyline that should strech to the edge of the image but doesn't. There needs to be polyline aware logic for cutting the polyline - more similar to how the bounding box methods work. 
3) (minor) kyepoints expects int? what's wrong with floats?",finally tried today rather turning segmentation map working ran following set multiple decision keep point per point rather per might end part edge image need aware logic cutting similar bounding box minor wrong,issue,negative,negative,negative,negative,negative,negative
1209037140,"@invincible-28 You have to check your input bounding boxes if they are within the image and discard the ones which are outside the image boundary. For example, you could check it with:
`0 <= x_min <= 1 and 0 <= y_min <= 1 and 0 <= x_max <= 1 and 0 <= y_max <= 1`
If this is true then they are inside the image.

Other reason could be that the bounding boxes are very small. To filter these boxes you can use:
```
width = max_x - min_x
height = max_y - min_y
box_area = width * height
 # Ignore smaller bounding boxes
 if box_area > 0:
    # Here you can retain these bounding boxes 
```

I hope this helps",check input bounding within image discard outside image boundary example could check true inside image reason could bounding small filter use width height width height ignore smaller bounding retain bounding hope,issue,negative,positive,neutral,neutral,positive,positive
1208161353,"@kulkarnikeerti Could you please share in detail what changes you made to fix this error?
Even I have a similar issue.",could please share detail made fix error even similar issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1207942634,"Hi @ternaus, I would like to work on this issue.",hi would like work issue,issue,negative,neutral,neutral,neutral,neutral,neutral
1206692744,"Good question. I think that it should affect only the images, but in this case, it will have a different spirit than the CourseDropout.

It looks like it should be a separate transform.

@Dipet What do you think?

",good question think affect case different spirit like separate transform think,issue,positive,positive,positive,positive,positive,positive
1206582404,"@ternaus Hi, I'd like to work on this issue. Should it be image-only transform, or should mask and key points be affected like in CoarseDropout?",hi like work issue transform mask key affected like,issue,positive,neutral,neutral,neutral,neutral,neutral
1206206483,"> . However, the range of float32 images assumed to stay in [0..1] range, which is stated in the docs. For values outside this range, a clamping to [0...1] range will occur.

I didn't find what you mentioned either!",however range float assumed stay range stated outside range range occur find either,issue,negative,neutral,neutral,neutral,neutral,neutral
1205560513,"Oh, it seems to not work with Safari on Mac OS. I loaded it on Chrome and it worked. I think this is a problem out of this team's hands.",oh work safari mac o loaded chrome worked think problem team,issue,negative,neutral,neutral,neutral,neutral,neutral
1204937486,"Interesting question. We have [wrapper](https://github.com/albumentations-team/albumentations/blob/master/albumentations/random_utils.py) functions for numpy random module.
I think we need to create some base class that would represent sampling strategy. When we will create some random sampled value we will wrap this value into this wrapper and on call `get_params` we will just call `sample` method for this variable.
Something like random logic in imgaug https://github.com/aleju/imgaug/blob/0101108d4fed06bc5056c4a03e2bcb0216dac326/imgaug/random.py",interesting question wrapper random module think need create base class would represent sampling strategy create random value wrap value wrapper call call sample method variable something like random logic,issue,positive,negative,negative,negative,negative,negative
1204932478,"Now everything works fine for me, please check again. Please, write about the demo project on the project page https://github.com/albumentations-team/albumentations-demo",everything work fine please check please write project project page,issue,positive,positive,positive,positive,positive,positive
1204423748,"Feel free to contribute it.

Something like PadToSquare",feel free contribute something like,issue,positive,positive,positive,positive,positive,positive
1202988373,"@Dipet I'd like to work on this one. Any ideas how to better implement switching between different samplings? 

Ideally, this option should be available for all transforms out of the box. ",like work one better implement switching different ideally option available box,issue,positive,positive,positive,positive,positive,positive
1199318720,"I can not understand what wrong. Everythink must work ok from albumentations side.
```python
import random
import cv2 as cv
import albumentations as A
import matplotlib.pyplot as plt

random.seed(0)
flip_p = 0.5


def augment_function(image):
    transform = A.Compose(
        [
            A.RandomResizedCrop(height=256, width=256, scale=(0.2, 1.0), p=0.5),
            A.Flip(p=flip_p),
            A.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2, p=0.8)]
    )

    return transform(image=image)[""image""], transform(image=image)[""image""]


img = cv.imread(r""C:\Users\dipet\parrot.jpg"")
img = cv.cvtColor(img, cv.COLOR_BGR2RGB)

img1, img2 = augment_function(img)

plt.subplot(131, title=""original"")
plt.imshow(img)
plt.subplot(132, title=""img1"")
plt.imshow(img1)
plt.subplot(133, title=""img2"")
plt.imshow(img2)
plt.show()
```
![image](https://user-images.githubusercontent.com/7512250/181776002-9eeb02e5-5254-4f81-9a64-bb5eaef2c0ce.png)
",understand wrong must work side python import random import import import image transform return transform image transform image original image,issue,negative,negative,negative,negative,negative,negative
1199015263,Try to implement random fill value: https://github.com/albumentations-team/albumentations/pull/597,try implement random fill value,issue,negative,negative,negative,negative,negative,negative
1198959557,"Issue fixed, TensorBoard accepts images in torch.uint8 type, I was using torch.int32. Sorry.",issue fixed type sorry,issue,negative,negative,negative,negative,negative,negative
1193377999,"I face the same issue as @ZFTurbo: 
I have situations where the masks do not match the bboxes. The image has 25 annotations, bboxes and segmentations which are converted to masks using the COCO api (coco.annToMask(). Then the image and targets are passed through a few transforms, CenterCrop, Resize, Affine... 
The transformed dict only contains 9 boxes, but it still containes 25 masks.
When I filter out masks which only contain zeros `masks = [i for i in masks if i.any()]`, most of the time this works. But sometimes a bbox is still there where its mask is gone after I filtered ""empty"" masks.

> ...than create some strange code to augment both bboxes and mask and check them after each transformation.

I thought thats what DualTransforms for, applying the transforms to both image and target in the same way. But it seems that bbox and mask transforms might get inconsistent.
",face issue match image converted coco image resize affine still filter contain time work sometimes still mask gone empty create strange code augment mask check transformation thought thats image target way mask might get inconsistent,issue,negative,negative,neutral,neutral,negative,negative
1188825091,@Dipet sorry for the late update. Please do let me know if there are any problems being faced while running or reviewing the eraser module.,sorry late update please let know faced running eraser module,issue,negative,negative,negative,negative,negative,negative
1186941605,The author did not provide a reproducible example.,author provide reproducible example,issue,negative,neutral,neutral,neutral,neutral,neutral
1184805696,"Something like this:

```python
class RandomCropFromBorders(DualTransform):
    """"""Cuts out a rectangular portion of the image by randomly indenting each side. There is no resize of image at the end.

    Args:
        crop_value (float): float value in (0.0, 0.5) range. Default 0.1
        crop_left (float): float value in (0.0, 1.0) range. Default 0.1
        crop_right (float): float value in (0.0, 1.0) range. Default 0.1
        crop_top (float): float value in (0.0, 1.0) range. Default 0.1
        crop_bottom (float): float value in (0.0, 1.0) range. Default 0.1
        p (float): probability of applying the transform. Default: 1.

    Targets:
        image, mask, bboxes, keypoints

    Image types:
        uint8, float32
    """"""

    def __init__(
            self,
            crop_value=None,
            crop_left=None,
            crop_right=None,
            crop_top=None,
            crop_bottom=None,
            always_apply=False,
            p=1.0
    ):
        super(RandomCropFromBorders, self).__init__(always_apply, p)
        self.crop_left = 0.1
        self.crop_right = 0.1
        self.crop_top = 0.1
        self.crop_bottom = 0.1
        if crop_value is not None:
            self.crop_left = crop_value
            self.crop_right = crop_value
            self.crop_top = crop_value
            self.crop_bottom = crop_value
        if crop_left is not None:
            self.crop_left = crop_left
        if crop_right is not None:
            self.crop_right = crop_right
        if crop_top is not None:
            self.crop_top = crop_top
        if crop_bottom is not None:
            self.crop_bottom = crop_bottom

    def get_params_dependent_on_targets(self, params):
        img = params[""image""]
        x_min = random.randint(0, int(self.crop_left * img.shape[1]))
        x_max = random.randint(max(x_min + 1, int((1 - self.crop_right) * img.shape[1])), img.shape[1])
        y_min = random.randint(0, int(self.crop_top * img.shape[0]))
        y_max = random.randint(max(y_min + 1, int((1 - self.crop_bottom) * img.shape[0])), img.shape[0])
        return {""x_min"": x_min, ""x_max"": x_max, ""y_min"": y_min, ""y_max"": y_max}

    def apply(self, img, x_min=0, x_max=0, y_min=0, y_max=0, **params):
        return F_albu.clamping_crop(img, x_min, y_min, x_max, y_max)

    def apply_to_mask(self, mask, x_min=0, x_max=0, y_min=0, y_max=0, **params):
        return F_albu.clamping_crop(mask, x_min, y_min, x_max, y_max)

    def apply_to_bbox(self, bbox, x_min=0, x_max=0, y_min=0, y_max=0, **params):
        rows, cols = params[""rows""], params['cols']
        return F_albu.bbox_crop(bbox, x_min, y_min, x_max, y_max, rows, cols)

    def apply_to_keypoint(self, keypoint, x_min=0, x_max=0, y_min=0, y_max=0, **params):
        return F_albu.crop_keypoint_by_coords(
            keypoint,
            crop_coords=(x_min, y_min, x_max, y_max),
            crop_height=y_max - y_min,
            crop_width=x_max - x_min,
            rows=params[""rows""],
            cols=params[""cols""],
        )

    @property
    def targets_as_params(self):
        return [""image""]

    def get_transform_init_args_names(self):
        return ""crop_value"", ""crop_left"", ""crop_right"", ""crop_top"", ""crop_bottom""
```",something like python class rectangular portion image randomly side resize image end float float value range default float float value range default float float value range default float float value range default float float value range default float probability transform default image mask image float self super self none none none none none self image return apply self return self mask return mask self return self return property self return image self return,issue,positive,negative,neutral,neutral,negative,negative
1184145848,"It seems that the [`SomeOf`](https://albumentations.ai/docs/api_reference/core/composition/#albumentations.core.composition.SomeOf) work in this way.
Transform samples in random order https://github.com/albumentations-team/albumentations/blob/ed7626f8d0789d5c0d194bb0ca6fda563c69bc95/albumentations/core/composition.py#L339

But from the documentation it is not clear, I think we need to update the docs for this transform.",work way transform random order documentation clear think need update transform,issue,negative,negative,negative,negative,negative,negative
1183570147,@ternaus I don't think it's useful to add something that could be achieved by 2 lines of current augs. For me augmentations list is always pretty long and has 10+ augs. Saving one line wouldn't help. ,think useful add something could current list always pretty long saving one line would help,issue,positive,positive,positive,positive,positive,positive
1182722194,"I do not really understand, what are you trying to achieve.

if you want to disable a transform, you can just have `probability = 0`.


`A.Flip(p=0)` <- disabled
`A.Flip(p=0.5)` <- applied with probability 50%
",really understand trying achieve want disable transform probability disabled applied probability,issue,negative,neutral,neutral,neutral,neutral,neutral
1182715700,"![2022-07-13 10-54-17-930](https://user-images.githubusercontent.com/6490927/178640880-f50ef35e-3d03-4c95-b289-38b7f7a45e4b.jpg)

Checkbox

```python

Flip = True
Transpose = False
VerticalFlip = False
HorizontalFlip= True

transform = A.Compose([
    A.Flip(),
    A.Transpose(),
    A.VerticalFlip(),
    A.HorizontalFlip(),
])

another_transformed_image = transform(image=another_image)[""image""]
```

```python
D4 = A.Compose( [ A.HorizontalFlip(p=0.5), A.RandomRotate90(p=1)], p=1 )
```

create a new list ?
```python
newlist = [ A.HorizontalFlip(p=0.5), A.RandomRotate90(p=1)]
D4 = A.Compose( newlist , p=1 )
```

ideas
```python

Flip = True
Transpose = False
VerticalFlip = False
HorizontalFlip= True

newlist = [
    A.Flip(),
    A.Transpose(),
    A.VerticalFlip(),
    A.HorizontalFlip(),
]

if Flip == True:
    newlist.append(""A.Flip()"")
elif Transpose == True:
    newlist.append(""A.Transpose ()"")
elif VerticalFlip == True:
    newlist.append(""A.VerticalFlip ()"")

D4 = A.Compose(newlist , p=1)

```

I don't know how to make it more reasonable

`D4 = A.Compose( [ A.HorizontalFlip(p=0.5), A.RandomRotate90(p=0.5)], p=1 )`
It looks like p=1 is a global setting ？

@ternaus ",python flip true transpose false false true transform transform image python create new list python python flip true transpose false false true flip true transpose true true know make reasonable like global setting,issue,positive,positive,neutral,neutral,positive,positive
1178739124,"Just found out about it in the FAQ of the docs - [Augmentations have a parameter named p that sets the probability of applying that augmentation, but they also have the always_apply parameter that can either be True or False. What is the difference between p and always_apply? Is always_apply=True equals to p=1.0?](https://albumentations.ai/docs/faq/#augmentations-have-a-parameter-named-p-that-sets-the-probability-of-applying-that-augmentation-but-they-also-have-the-always_apply-parameter-that-can-either-be-true-or-false-what-is-the-difference-between-p-and-always_apply-is-always_applytrue-equals-to-p10)",found parameter probability augmentation also parameter either true false difference,issue,negative,negative,neutral,neutral,negative,negative
1173793060,"Build failing due to missing dependencies  https://github.com/conda-forge/albumentations-feedstock/pull/22
Fixed applied in this PR https://github.com/regro-cf-autotick-bot/albumentations-feedstock/pull/1",build failing due missing fixed applied,issue,negative,negative,neutral,neutral,negative,negative
1170647767,"@Dipet @ai4prod Silly me, by simply updating pip to the latest version `pip install -U albumentations --no-binary qudida,albumentations` now functions correctly.",silly simply pip latest version pip install correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
1169807046,"On a clean env and python3.8 with opencv-python installed all works fine and `opencv-python-headless` doesn't get installed.
```
Requirement already satisfied: opencv-python>=4.1.1 in ./test_venv/lib/python3.8/site-packages (from albumentations) (4.6.0.66)
````",clean python work fine get requirement already satisfied,issue,positive,positive,positive,positive,positive,positive
1169767598,"Hi @creafz,

i can confirm that i have the same issue. 
Thanks",hi confirm issue thanks,issue,negative,positive,positive,positive,positive,positive
1169638153,"Hi @creafz, I am installing albumentation v1.2.0 and I have `opencv-python` already installed, but `pip install -U albumentations --no-binary qudida,albumentations` still installs `opencv-python-headless`. Would you please take a look into this? Thank you.",hi already pip install still would please take look thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1168460304,"I am not sure if we can fix this problem, because I do not know how to check that numpy is prebuilt for your hardware.
Maybe it only took a few second to install the old version because you already installed numpy?
Have you tried installing v1.1 in a clean environment?
I don't have an Apple with M1 CPU, so I can not check and fix this issue.

The scikit-learn documentation says that the only way to get prebuilt libraries is to use conda https://scikit-learn.org/stable/install.html#installing-on-apple-silicon-m1-hardware",sure fix problem know check hardware maybe took second install old version already tried clean environment apple check fix issue documentation way get use,issue,negative,positive,positive,positive,positive,positive
1165831228,"Found repo, which basically solves the same problem - [correct White-Balance augmentation](https://github.com/mahmoudnafifi/WB_color_augmenter). 
Maybe will just port Python code from there (MIT license allows this) 🤔 
",found basically problem correct augmentation maybe port python code license,issue,negative,neutral,neutral,neutral,neutral,neutral
1165475825,"Quick workaround if anyone else stumbles across this and wants an instant fix:

```python
class CoarseDropout(A.CoarseDropout):
    """""" Workaround for https://github.com/albumentations-team/albumentations/issues/420 """"""
    def get_params_dependent_on_targets(self, params):
        if np.any(params['image'].shape[:2] < np.array([self.max_height, self.max_width])):
            return {'holes': []}
        return super().get_params_dependent_on_targets(params)
```

Just note that this will always skip any images smaller than the max hole dimensions. It won't attempt to adapt hole size to the image size or check if a randomly generated hole happens to fit into the image anyway.

Albumentations version 1.2.0",quick anyone else across instant fix python class self return return super note always skip smaller hole wo attempt adapt hole size image size check randomly hole fit image anyway version,issue,positive,positive,neutral,neutral,positive,positive
1165296282,"Before
![image](https://user-images.githubusercontent.com/7512250/175486949-19a94970-93ea-477a-9e41-3e5b6b8b3d4c.png)


After
![image](https://user-images.githubusercontent.com/7512250/175486878-fbe6e053-e09b-4eba-b742-c22f04af78aa.png)

<details>
<summary> Code to reproduce </summary><details>
```python
import random

import cv2 as cv
import numpy as np
import albumentations as A
import matplotlib.pyplot as plt

kp_params = A.KeypointParams(format='xya', remove_invisible=False)
img = np.zeros((100, 100, 3), dtype=np.uint8)
img[..., 0] = 255
cv.line(img, (25, 50), (75, 50), (255, 255, 255), 3)
cv.line(img, (75, 25), (75, 75), (255, 255, 255), 3)

kp = [(0, 0, 0)]


angle = 30
shift = 0
scale = 0
rotate = A.Rotate((angle, angle), p=1, border_mode=cv.BORDER_CONSTANT, value=0)
ssr = A.ShiftScaleRotate(
    shift_limit=(shift, shift),
    scale_limit=(0, 0),
    rotate_limit=(angle, angle),
    p=1, border_mode=cv.BORDER_CONSTANT, value=0
)
affine = A.Affine(
    rotate=angle,
    scale=1,
    translate_percent=shift,
    shear=None,
    p=1
)
perspective = A.Perspective(scale=[0.2, 0.2], p=1)
transform1 = A.Compose(
    [rotate],
    keypoint_params=kp_params
)
transform2 = A.Compose(
    [ssr],
    keypoint_params=kp_params
)
transform3 = A.Compose(
    [affine],
    keypoint_params=kp_params
)
transform4 = A.Compose(
    [
        affine,
        rotate
    ],
    keypoint_params=kp_params
)
random.seed(1)
transform5 = A.Compose(
    [perspective],
    keypoint_params=kp_params
)
random.seed(1)
transform6 = A.Compose(
    [
        perspective,
        rotate
    ],
    keypoint_params=kp_params
)


count = 331
plt.figure(figsize=(15, 15))
def show(res, name):
    global count
    print(res[""keypoints""][0][-1])
    img = res[""image""]
    plt.subplot(count, title=f""{name} {res['keypoints'][0][-1]:.2f}"")
    plt.imshow(img)
    count += 1


show({""image"": img, ""keypoints"": kp}, ""original"")
show(transform1(image=img, keypoints=kp), ""Rotate"")
show(transform2(image=img, keypoints=kp), ""ShiftScaleRotate"")
show(transform3(image=img, keypoints=kp), ""Affine"")
show(transform4(image=img, keypoints=kp), ""Affine + Rotate"")
show(transform5(image=img, keypoints=kp), ""Perspective + Rotate"")
show(transform6(image=img, keypoints=kp), ""Perspective + Rotate"")
plt.show()
```
<\details>",image image summary code reproduce python import random import import import import angle shift scale rotate angle angle shift shift angle angle affine perspective transform rotate transform transform affine transform affine rotate transform perspective transform perspective rotate count show name global count print image count name count show image original show transform rotate show transform show transform affine show transform affine rotate show transform perspective rotate show transform perspective rotate,issue,negative,negative,neutral,neutral,negative,negative
1163674979,"What library version did you use? 
Post output of `import albumentations; print(albumentations.__version__)`
Tried to replicate this issue on 1.1.0 and everything worked fine (except that `A.IAAAffine` is marked as deprecated now). 

```python
import albumentations as A
IMAGE_SIZE = 512
train_transforms = A.Compose([
    A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),
    A.PadIfNeeded(
        min_height=int(IMAGE_SIZE * scale),
        min_width=int(IMAGE_SIZE * scale),
        border_mode=cv2.BORDER_CONSTANT,
    ),
    A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),
    A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),
    A.OneOf([
        A.ShiftScaleRotate(rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT),
        A.Affine(shear=15, p=0.5, mode=""constant""),
    ], p=1.0,),
    A.HorizontalFlip(p=0.5),
    A.Blur(p=0.1),
    A.CLAHE(p=0.1),
    A.Posterize(p=0.1),
    A.ToGray(p=0.1),
    A.ChannelShuffle(p=0.05),
    A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
    ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format=""yolo"", min_visibility=0.4, label_fields=[],),
)
```",library version use post output import print tried replicate issue everything worked fine except marked python import scale scale scale constant,issue,negative,positive,positive,positive,positive,positive
1163516308,"Please, resolve branch conflict",please resolve branch conflict,issue,negative,neutral,neutral,neutral,neutral,neutral
1163354084,"I agree with @Trud09 here. The goal of ReplayCompose is not only to replay, but also to **look at the parameters of applied augmentations**.

Current implementation for ColorJitter doesn't provide such option.  This is the only transform that returns functions instead of parameter, which doesn't look consistent.

Possible fix I see is to return 
1. selected parameters 
2. indexes of functions to be applied, e.g. [0,2,1,3] and use this info in `apply` method. 

 ",agree goal replay also look applied current implementation provide option transform instead parameter look consistent possible fix see return selected applied use apply method,issue,negative,positive,neutral,neutral,positive,positive
1163125608,"Perfect, I'm able to achieve my use case with `A.Affine`. Thanks for your support.

> Maybe we should add a reference to Affine in the docstrings for all transforms like RandomRotate90 and write if you want some specific changes try to using Affine.

Yes, could be a valuable information for a beginner like me.",perfect able achieve use case thanks support maybe add reference affine like write want specific try affine yes could valuable information beginner like,issue,positive,positive,positive,positive,positive,positive
1163103722,"Ohh, ok, I got it.
Try this:
```python
import albumentations as A
import numpy as np
import cv2 as cv

img = np.full([2339, 1654, 3], 255, np.uint8)
transform = A.Affine(rotate=[90, 90], p=1, mode=cv.BORDER_CONSTANT, fit_output=True)
res = transform(image=img)[""image""]
print(res.shape)
```

Your second options looks good. This is the correct way to create custom augmentation and work with it.

> Is this use case not supported by RandomRotate90 because of ""Random"" word in RandomRotate90? Or are there other motivations?

Right, random means rotating through a random angle that divisible by 90 degrees.

> Do you think that this particular use case is only mine or could be interesting to also support a Rotate90WithFactor spatial-level transforms?

I think the current implementation of Affine transform is very useful and can cover all cases. Maybe we should add a reference to Affine in the docstrings for all transforms like RandomRotate90 and write if you want some specific changes try to using Affine.
",got try python import import import transform transform image print second good correct way create custom augmentation work use case random word right random rotating random angle divisible think particular use case mine could interesting also support think current implementation affine transform useful cover maybe add reference affine like write want specific try affine,issue,positive,positive,neutral,neutral,positive,positive
1163083166,"I've already tried it, but the problem is that `Rotate` doesn't switch height and width.

For example, my `image.shape` is `(2339, 1654, 3)` and after `A.Rotate([90,90])` it is always `(2339, 1654, 3)`. Instead, with `RandomRotate90` will be `(1654, 2339, 3)`.",already tried problem rotate switch height width example always instead,issue,negative,neutral,neutral,neutral,neutral,neutral
1163072105,"If you want to just rotate image 90 degrees try to use [`A.Rotate([90, 90])`](https://albumentations.ai/docs/api_reference/full_reference/#albumentations.augmentations.geometric.rotate.Rotate)",want rotate image try use,issue,negative,neutral,neutral,neutral,neutral,neutral
1161661150,"Thank you so much. I feel very, very embarrassed. It was a very stupid problem on my side. Namely, I polled the transform twice in my code, once to get the mask and once to get the image, hence the random augmentations were different. So sorry for this.",thank much feel embarrassed stupid problem side namely polled transform twice code get mask get image hence random different sorry,issue,negative,negative,negative,negative,negative,negative
1161634963,To solve this problem we need to process bboxes as batches using numpy. But since all of our transforms works with bboxes as tuples these changes are complicated.,solve problem need process since work complicated,issue,negative,negative,negative,negative,negative,negative
1161629660,"Hi, thanks for reporting this issue.
The library has a problem if you have a lot of bboxes, but since must people do not  usually use more than dozens of bboxes we didn't think that this is a real problem.
We will try to fix the problem, but it may take a long time, because a lot of changes needed.",hi thanks issue library problem lot since must people usually use think real problem try fix problem may take long time lot,issue,negative,positive,neutral,neutral,positive,positive
1161488059,"To be honest, not very often. Usually, once every few months. Maybe more often if there are a lot of changes.",honest often usually every maybe often lot,issue,positive,positive,positive,positive,positive,positive
1161481903,Thanks for the quick turnaround! Do you know how often patch versions are generally released?,thanks quick turnaround know often patch generally,issue,negative,positive,positive,positive,positive,positive
1161415342,"All works fine:
```python
import albumentations as A
import numpy as np
import matplotlib.pyplot as plt
import cv2 as cv

sat_img = np.zeros([400, 400, 3], np.uint8)
sat_img[:, :, 0] = 255

segmentation = np.zeros([400, 400], np.uint8)
segmentation += 255

aug = A.Rotate((45, 45), p=1, border_mode=cv.BORDER_CONSTANT, value=0, mask_value=0)
sample = aug(image=sat_img, mask=segmentation)

plt.subplot(211)
plt.imshow(sample[""image""])
plt.subplot(212)
plt.imshow(sample[""mask""], cmap=""gray"")
plt.show()
```

In your example you plot different data",work fine python import import import import segmentation segmentation sample sample image sample mask gray example plot different data,issue,negative,positive,positive,positive,positive,positive
1160416208,"Hello, thanks for your contribution to the library.
I think it is a good idea!
But I think it is better not to change the logic, but add a new option as a `method` or `mode`, something like `simple` - the current implementation and `advanced` - your proposal.",hello thanks contribution library think good idea think better change logic add new option method mode something like simple current implementation advanced proposal,issue,positive,positive,positive,positive,positive,positive
1160384559,"1. Yes, looks ok to have `method` argument.
2. Its's a good idea to reuse `bbox_shift_scale_rotate` inside `bbox_rotate` :+1: ",yes method argument good idea reuse inside,issue,negative,positive,positive,positive,positive,positive
1159596009,"@ternaus I would assume that OP has abandoned this PR, so I could try to implement this rotation method. Could I ask you some implementation details?
Regarding point 1:
I was thinking of adding to `bbox_rotate()` a new argument `method` that can be used to choose between the ""largest box"" rotation method (current one) and ""ellipsoid"" rotation method. Would this be ok?
Regarding 2:
There is an [official repo](https://github.com/akasha-imaging/ICCV2021) that I could used to generate some test cases against.
Another thing:
`bbox_rotate()` is basically `bbox_shift_scale_rotate()`, but with no scaling and shifting. However there are 2 explicit implementations. Why not call `bbox_shift_scale_rotate()` with no scaling and shifting inside of `bbox_rotate()`?

https://github.com/albumentations-team/albumentations/blob/0d98b4f46f04db271203381742c6abe113a8f269/albumentations/augmentations/geometric/functional.py#L123-L149

https://github.com/albumentations-team/albumentations/blob/0d98b4f46f04db271203381742c6abe113a8f269/albumentations/augmentations/geometric/functional.py#L209-L229

There is even this test case

https://github.com/albumentations-team/albumentations/blob/0d98b4f46f04db271203381742c6abe113a8f269/tests/test_functional.py#L113-L116

",would assume abandoned could try implement rotation method could ask implementation regarding point thinking new argument method used choose box rotation method current one ellipsoid rotation method would regarding official could used generate test another thing basically scaling shifting however explicit call scaling shifting inside even test case,issue,negative,positive,neutral,neutral,positive,positive
1155511425,It seems that the master branch was not properly formatted and I had some trailing whitespaces in my commit. Reformatted with black. ,master branch properly trailing commit black,issue,negative,negative,neutral,neutral,negative,negative
1152913643,"Please use [pre-commit hooks](https://albumentations.ai/docs/contributing/)
Please fix flake8 issues. You can check flake8 with this commands:
1. `pip install flake8==3.9.2 flake8-docstrings==1.6.0` - install flake8
2. `flake8` - call checker",please use please fix flake check flake pip install install flake flake call checker,issue,positive,neutral,neutral,neutral,neutral,neutral
1151256008,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`.

If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",wo notify release get touch new version available rather skip next major minor version let know ignore major version ignore minor version change mind resolve,issue,negative,positive,neutral,neutral,positive,positive
1151148132,"Looks like we forget it in [get_transform_init_args_names](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/dropout/grid_dropout.py#L143)
`fill_value` is used inside transform correctly, but serialization and deserialization of this transform works now incorrectly for custom values of `fill_value`.",like forget used inside transform correctly serialization transform work incorrectly custom,issue,negative,neutral,neutral,neutral,neutral,neutral
1148598047,The way albumentation treats keypoints shouldn't make this a problem since you also encounter that problem when having multiple different people in the same image or keypoints that aren't visible (out of frame). You can apply this same logic to polygons (set remove_invisible to False in A.compose though!): and then you simply clip the values of your polygon's vertices to the edges of your picture and you should obtain a transformed polygon ;),way make problem since also encounter problem multiple different people image visible frame apply logic set false though simply clip polygon vertex picture obtain polygon,issue,negative,negative,negative,negative,negative,negative
1148407681,"It seems that you need to update `Black` to a newer version in order to work with  newest `click==8.1.3`.
Or downgrade `click` to a previous version.

see: https://stackoverflow.com/a/71674345/11457061",need update black version order work downgrade click previous version see,issue,negative,negative,negative,negative,negative,negative
1148295785,@MaxTeselkin You should also have an empty bounding boxes list (`[]`) with the `pascal_voc` format. The problem is that `A.ShiftScaleRotate` shifts/scales/rotates the bounding box outside of the image. ,also empty bounding list format problem bounding box outside image,issue,negative,negative,neutral,neutral,negative,negative
1147297393,`img` is a name for local variable. In that case `img` will contain the mask.,name local variable case contain mask,issue,negative,neutral,neutral,neutral,neutral,neutral
1146579057,"I need this feature as well.
When I'm doing data transformations for rotated objects, I want to keep their original geometrical feature.
Although I set the shear parameter of affine to None, the result of rotated objects still seems to be sheared.

Original data:
![image](https://user-images.githubusercontent.com/28639476/171993839-889d6645-7993-4489-b80b-6ce8f1de4346.png)

Transformed one:
![image](https://user-images.githubusercontent.com/28639476/171993648-0e6804d1-c390-4fe8-933a-f296ef6823a7.png)
",need feature well data rotated want keep original geometrical feature although set shear parameter affine none result rotated still original data image one image,issue,positive,positive,positive,positive,positive,positive
1145333875,"> So I fixed the errors for me by changing the conversion in `bbox_utils` for the yolo case. Since yolo comes in already relative a conversion to absolute format and then back to relative will yield errors.
> 
> So instead of convertig to absolute I removed it and now with the coordinates I get from yolo
> 
> ```
> x, y, w, h = yolo_bbox
> 
> w_half, h_half = w / 2, h / 2
> x_min, x_max = x - w_half, x + w_half
> y_min, y_max = y - h_half, y + h_half
> ```
> 
> and vice versa in the `from_albumentation` case.

Could you please tell us in what function you propose this change?  Thank you!",fixed conversion case since come already relative conversion absolute format back relative yield instead absolute removed get vice case could please tell u function propose change thank,issue,positive,positive,neutral,neutral,positive,positive
1141775465,"Yes, I was able to get rid of the error by filtering the bounding boxes. Thanks!",yes able get rid error filtering bounding thanks,issue,negative,positive,positive,positive,positive,positive
1141729695,"You have not provided the size of your images. It could be the boxes are in fact outsize the bounds of the image, which would cause this behaviour. I was able to run your example without error when using
`image = np.zeros((4000,4000,3), dtype=np.float)`",provided size could fact outsize image would cause behaviour able run example without error image,issue,negative,positive,positive,positive,positive,positive
1140893511,"> 

I did try the upgrade. The problem isn't with the code or something because I tried force installing an earlier version on Google Colab and it worked fine with the proper openCV version. Yet, the problem is at the installation location and the version signature. The upgrade should remove the existing version and replace it with an earlier code. Actually, what is happening is quite different. The issue that @iswarup pointing too is exact and the behavior of the installation, updates, and upgrades doesn't seem to work correctly on managing the site-packages as it is supposed to do.

I hope that my comment clarified the problem",try upgrade problem code something tried force version worked fine proper version yet problem installation location version signature upgrade remove version replace code actually happening quite different issue pointing exact behavior installation seem work correctly supposed hope comment problem,issue,negative,positive,positive,positive,positive,positive
1140497977,"I found the solution that the package was getting installed at an unexpected location as below
C:\Users\Swarup\AppData\Roaming\Python\Python39\site-packages

The expected location for [pip install albumentations] was as below
C:\Users\Swarup\Anaconda3\Lib\site-packages

So I just copied the folders from the first location to second one. And the problem's solved! :D
Following six packages were installed so while copying them I skipped cv2 since I already had it at the dst 

![image](https://user-images.githubusercontent.com/10144957/170884776-511b0c0b-7b0c-4c66-ab19-350988708268.png)


Even so I'd like to understand why the first location was selected for the installation in the first place!
Both are user level installations. ",found solution package getting unexpected location location pip install copied first location second one problem following six since already image even like understand first location selected installation first place user level,issue,negative,positive,positive,positive,positive,positive
1140133913,"

    



    font{
        line-height: 1.6;
    }
    ul,ol{
        padding-left: 20px;
        list-style-position: inside;
    }


    
    
        Thank you! I have solved it.
        
    
        
            
        
    
    
     
    
        
                    ***@***.***
        
    
 

    On 5/27/2022 17:29，Mikhail ***@***.***> wrote： 



I am not sure I understand correctly what you want to do. Could you give a small example?

—Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you authored the thread.Message ID: ***@***.***>",font inside thank sure understand correctly want could give small example directly view id,issue,positive,positive,positive,positive,positive,positive
1139470461,"Now, if the image is in float32 format, its values should be in the range [0, 1]. I agree that we should describe it in the docs.",image float format range agree describe,issue,negative,neutral,neutral,neutral,neutral,neutral
1139452070,"Hi. Sorry for the late response. Please, remove pre-commit changes, they are not needed for your changes and they made a lot of changes in files that are not related to your target changes.
Now it is hard to analyze your changes",hi sorry late response please remove made lot related target hard analyze,issue,negative,negative,negative,negative,negative,negative
1139446009,I am not sure I understand correctly what you want to do. Could you give a small example?,sure understand correctly want could give small example,issue,negative,positive,positive,positive,positive,positive
1139434530,"Did you try to install or upgrade library using pip?
Try this command: `pip install --upgrade albumentations`",try install upgrade library pip try command pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1137078064,"I've also had this issue! And it was very hard to find with dataloader with multiple workers. I had to set workers=0 to even get told where the code was freezing. 
Pretty incredible that it's not fixed yet...",also issue hard find multiple set even get told code freezing pretty incredible fixed yet,issue,negative,positive,positive,positive,positive,positive
1137075420,"I still had this issue with randomfog in albumentations 1.1.0, with arguments lower 0, upper 0.2 and alpha 0.5.",still issue lower upper alpha,issue,negative,neutral,neutral,neutral,neutral,neutral
1135966084,"Hello people!
Any solution updates here?
I'm facing the same issue

Albumentations version 1.1.0
Python version 3.9.12
Windows 11
pip 21.2.4

",hello people solution facing issue version python version pip,issue,negative,neutral,neutral,neutral,neutral,neutral
1135121494,"@brownrc that's correct, albumentations are additive to existing augmentations. Though I think your conclusion is inconsistent with statistical additive noise, which always results in more noise and never cancels itself out statistically speaking.",correct additive though think conclusion inconsistent statistical additive noise always noise never statistically speaking,issue,negative,neutral,neutral,neutral,neutral,neutral
1135114799,"This is more of a clarification than an issue but... Assuming you have albumentations installed, then Yolo will apply both the hyperparameters in the hyp.scratch-low.yaml (default) AND apply the albumentation pipeline defined in utils/augmentations.py. Since there are overlaps in the two capabilities, you could possibly end up canceling out an effect or, at the very least, end up with unexpected results. For example, if I have fliplr set in the hyperparameters but then use A.HorzontalFlip in the albumentation side, there is a chance that I could flip the same image twice (effectively canceling out the flip). Is this correct or am I over thinking this? It is definitely something to consider when using albumentations.",clarification issue assuming apply default apply pipeline defined since two could possibly end effect least end unexpected example set use side chance could flip image twice effectively flip correct thinking definitely something consider,issue,positive,positive,neutral,neutral,positive,positive
1133252911,"An intersting observation: this problem disappeared when I set format='pascal_voc' instead of format='yolo'. So if your dataset has bounding boxes in yolo format, then the pipeline will be the following:
1. convert bounding boxes from yolo to pascal voc format
2. put converted boxes into Albumentations transform
3. convert augmented boxes from pascal voc to format required by architecture for neural network you are using.

For example, I am going to use Efficientdet architecture and my dataset has bounding boxes in yolo format. So I will convert boxes from yolo to pascal voc format -> put boxes in pascal voc format into transform -> convert transformed boxes from pascal voc to Efficientdet format.

Functions for converting:

```
def convert_to_voc(yolo_box, image_width, image_height):
  x_c, y_c, w, h = yolo_box
  x_tl = x_c - w / 2
  y_tl = y_c - h / 2
  x_tl *= image_width
  y_tl *= image_height
  w *= image_width
  h *= image_height
  x_br = x_tl + w
  y_br = y_tl + h
  voc_box = np.array([x_tl, y_tl, x_br, y_br], dtype=np.int64)
  voc_box = list(voc_box)
  return voc_box
```

```
def convert_to_effdet(box, image_width, image_height, format):
  if format == 'yolo':
    voc_box = convert_to_voc(box, image_width, image_height)
  elif format == 'pascal_voc':
    voc_box = box
  effdet_order = [1, 0, 3, 2]
  effdet_box = [voc_box[i] for i in effdet_order]
  return effdet_box
```


Nevertheless, it is still not normal that augmentations with bounding boxes in yolo format do not work correctly, so I will leave this issue opened.",observation problem set instead bounding format pipeline following convert bounding format put converted transform convert augmented format architecture neural network example going use architecture bounding format convert format put format transform convert format converting list return box format format box format box return nevertheless still normal bounding format work correctly leave issue,issue,negative,positive,neutral,neutral,positive,positive
1132570999,"I have the same issue.
**Environment:**
* Albumentations version (e.g., 0.1.8): `1.1.0`
* Python version (e.g., 3.7): `3.9.5`
* OS (e.g., Linux): Ubuntu `20.04`
* How you installed albumentations (conda, pip, source): `pip`

**Update**
The last pip version (1.1.0) was created on 4 Oct 2021. So you can install the newest version directly from github:
```
pip3 uninstall albumentations
pip3 install git+https://github.com/albumentations-team/albumentations.git
```
Then the augmentation works well.",issue environment version python version o pip source pip update last pip version install version directly pip pip install augmentation work well,issue,negative,positive,neutral,neutral,positive,positive
1131292391,@ternaus I hope its okay to tag you here. I'm tagging you here for a request to assign a reviewer to review my merge request.,hope tag request assign reviewer review merge request,issue,negative,neutral,neutral,neutral,neutral,neutral
1130077142,"Yes, but still the results are different between PILLOW, OpenCV and ResizeRight. If it ok I'll work on an update to let the resize using PILLOW or OpenCV based on a parameter",yes still different pillow work update let resize pillow based parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
1128913236,"If this is the intended behaviour, it should be clearly stated in the docs. ",intended behaviour clearly stated,issue,negative,positive,positive,positive,positive,positive
1123990270,Also interested in support for this feature.,also interested support feature,issue,positive,positive,positive,positive,positive,positive
1110929628,"I have the same issue with small images.

Could it be a solution to either ""skip"" the transformation or apply a `min_height = min(min_height, height)` to solve this?",issue small could solution either skip transformation apply min height solve,issue,negative,negative,negative,negative,negative,negative
1108508303,"Hi
I think your problem is for your library.


!pip install -U albumentations

pip install --upgrade opencv-python
import cv2
print(cv2.__version__)#4.5.5
import albumentations as A
print(A.__version__)#1.1.0
put them in 3 cell. I worked with this version

On Sat, Apr 23, 2022 at 7:26 AM Mehmet Niyazi Karacalar <
***@***.***> wrote:

> I try everything u said But I can't solve the problem.
> [image: image]
> <https://user-images.githubusercontent.com/83755124/164872572-134b696c-c3b8-4d63-a323-b7bf198656b0.png>
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/414#issuecomment-1107334444>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ATGXNJNRW65QT7QYPYHF7U3VGNRHPANCNFSM4I74BIQQ>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
",hi think problem library pip install pip install upgrade import print import print put cell worked version sat wrote try everything said ca solve problem image image reply directly view id,issue,negative,positive,neutral,neutral,positive,positive
1107334444,"I try everything u said But I can't solve the problem.
![image](https://user-images.githubusercontent.com/83755124/164872572-134b696c-c3b8-4d63-a323-b7bf198656b0.png)
",try everything said ca solve problem image,issue,negative,neutral,neutral,neutral,neutral,neutral
1105333757,"You can add this type of rotation to the existing rotation algorithms.

For this you need:

1. Add the desired functionality to the existing rotations transforms.
2. Write tests.
3. Make sure that everything works.",add type rotation rotation need add desired functionality write make sure everything work,issue,positive,positive,positive,positive,positive,positive
1103859852,"Hi, I think this cannot be done with `Affine`. `fit_output=True` enlarges the output image so that not a single pixel of the original image is lost (here, I find the current docs a bit confusing, see https://github.com/albumentations-team/albumentations/pull/1159/commits). But I understand you want to crop it so that no filling pixel is visible. Then, your issue seems to be a duplicate of https://github.com/albumentations-team/albumentations/issues/266.",hi think done affine output image single original image lost find current bit see understand want crop filling visible issue duplicate,issue,negative,positive,positive,positive,positive,positive
1103682287,"Thanks, will try this. I did not think of this as I did not see this ToFloat function be used before RandomBrightness in any documentation/example.",thanks try think see function used,issue,negative,positive,positive,positive,positive,positive
1103665578,"I believe RandomBrightness and RandomBrightnessContrast work only with float32 and not uint8. Therefore add `A.ToFloat(max_value=255)` as the first step in A.Compose and now you can skip line 5` aug_img = tf.cast(aug_img/255.0, tf.float32)`

```
def augmentor_dummy(image, img_size):
    transformations = A.Compose([
        A.ToFloat(max_value=255),
        A.RandomBrightness(limit=0.001)
    ])    
    aug_img = transformations(image=image)[""image""]
    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])
    return aug_img
```

",believe work float therefore add first step skip line image image return,issue,negative,positive,positive,positive,positive,positive
1100895241,"It seems SomeOf is missing an implementation of get_dict_with_id(), so BaseCompose's get_dict_with_id() is used which obviously doesn't return ""n"".

This function just needs to be added to SomeOf:
```
    def get_dict_with_id(self):
        dictionary = super(SomeOf, self).get_dict_with_id()
        dictionary.update({""n"": self.n, ""replace"": self.replace})
        return dictionary
```",missing implementation used obviously return function need added self dictionary super self replace return dictionary,issue,negative,positive,neutral,neutral,positive,positive
1100729283,"> cc. @awsaf49 
> Cutmix is available officially in `keras`, [HERE](https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing/cut_mix.py). The Mosaic one will be soon.

Thank you 😀",available officially mosaic one soon thank,issue,negative,positive,positive,positive,positive,positive
1100706748,"cc. @awsaf49 
Cutmix is available officially in `keras`, [HERE](https://github.com/keras-team/keras-cv/blob/master/keras_cv/layers/preprocessing/cut_mix.py). The Mosaic one will be soon.",available officially mosaic one soon,issue,negative,positive,positive,positive,positive,positive
1098193314,"Hi,

First of all, thanks for your work on this great package! Really enjoy using Albumentations.

I had the same problem with PixelDropout and AdvancedBlur. It would be convenient if the documentation (https://albumentations.ai/docs/api_reference/ ) clearly stated which version it's talking about. I assume the docs are always up to date with master, while the releases (of course) aren't. Even more, the ""go to repository"" button on the docs:

![image](https://user-images.githubusercontent.com/19475494/163215899-f6bdd744-df38-4c39-8328-20479fd139fa.png)

gave me the impression I was looking at the documentation of v1.1. Being able to choose the version in the documentation or it being tied with the latest release version would be quite convenient imo? ",hi first thanks work great package really enjoy problem would convenient documentation clearly stated version talking assume always date master course even go repository button image gave impression looking documentation able choose version documentation tied latest release version would quite convenient,issue,positive,positive,positive,positive,positive,positive
1090996827,"try to remove bbox with width/height=0, just set `min_area=1` in BboxParams
view https://albumentations.ai/docs/api_reference/core/composition/#albumentations.core.composition.BboxParams for detail",try remove set view detail,issue,negative,neutral,neutral,neutral,neutral,neutral
1087812049,"I fixed that problem using
`!pip uninstall opencv-python-headless`
`!pip install ""opencv-python-headless<4.3""`

![image](https://user-images.githubusercontent.com/47266164/161597466-6fe95e57-fd78-4ef6-a74b-d470f3d61960.png)
",fixed problem pip pip install image,issue,negative,positive,neutral,neutral,positive,positive
1073604221,"The `SafeRotate` transform is problematic also for the transformed bounding boxes.

The workaround I used to get past this behavior was to pad the images at all edges.

I debugged it by adding `keypoints` to each corner and plotting the transformed image with the keypoints on it.


It looked something like:
![16343644648013](https://user-images.githubusercontent.com/5475058/159225291-b82fe614-706c-406d-90a6-5eb804aaefaf.png)

",transform problematic also bounding used get past behavior pad corner plotting image something like,issue,negative,negative,negative,negative,negative,negative
1072347829,"
I checked what happened.

Default colab has:
```
opencv-contrib-python         4.1.2.30
opencv-python                 4.1.2.30
```

The albumentation requirements are given from the result of `pkginfo -f requires_dist albumentations-1.1.0-py3-none-any.whl`:
```
opencv-python-headless>=4.1.1
```


After the pip -u albumentations
```
opencv-contrib-python         4.1.2.30
opencv-python                 4.1.2.30
opencv-python-headless        4.5.5.64
```

So, multiple versions may cause conflicts. 

I think there is no simple solution to this problem.
The opencv team provides multiple package options and says, ""SELECT ONLY ONE OF THEM.""
- opencv-python
- opencv-contrib-python
- opencv-python-headless
- opencv-contrib-python-headless
(See also, https://pypi.org/project/opencv-python/ )

The package owner can not know which opencv package the user's environment has installed or none of them.
Unfortunately, the situation would be worse because there are environments where multiple openvcv packages have been installed, like Colab.

Installing all of them with the same version seems to work, but it introduces unnecessary and redundant dependencies.
I think such behavior would not be wanted for users who use packages in production.",checked default given result pip multiple may cause think simple solution problem team multiple package select one see also package owner know package user environment none unfortunately situation would worse multiple like version work unnecessary redundant think behavior would use production,issue,negative,negative,negative,negative,negative,negative
1072337641,"I made a PR to add Mosaic augmentation with as few changes to the existing implementation as possible #1147. 
Although some remaining works are to be done, please check it out if you are interested in this feature. 
I welcome any comments and feedback.",made add mosaic augmentation implementation possible although work done please check interested feature welcome feedback,issue,positive,positive,positive,positive,positive,positive
1071041728,"@BloodAxe following up on this. Albumentation seems to have a serious Colab problem that users are encountering i.e. https://github.com/ultralytics/yolov5/issues/7014, https://github.com/albumentations-team/albumentations/issues/1140. You can reproduce in Colab with:

```bash
!pip install -U albumentations
import cv2
```

<img width=""1398"" alt=""Screenshot 2022-03-17 at 17 18 29"" src=""https://user-images.githubusercontent.com/26833433/158845755-5ebf30ca-bb97-4e97-96dd-85f720041c00.png"">

",following serious problem reproduce bash pip install import,issue,negative,negative,negative,negative,negative,negative
1068088671,"I was able to create a custom version that fit my needs, but I couldn't get it to work with the albumentations pipeline. I ended up doing this (where `self.mosaic` :

```python
# in my datamodule

self.mosaic = Mosaic(img_files, bbox_files, p=0.9) # requires all image and label files to create random mosaics

def transform(image, bboxes, class_labels, **params):
    """"""A wrapper for the albumentations pipeline""""""
    image, bboxes, class_labels = self.mosaic(image, bboxes, class_labels)
    return self.transform(image=image, bboxes=bboxes, class_labels=class_labels, **params)

# create dataset, pass transform
```

That way my dataset could still call transform() with the usual albumentations API. However, it'd be nice to not have to use workarounds like this.",able create custom version fit need could get work pipeline ended python mosaic image label create random transform image wrapper pipeline image image return create pas transform way could still call transform usual however nice use like,issue,positive,positive,positive,positive,positive,positive
1067154760,"I've been working on a custom augment that performs mosaic by loading in 3 new images with whatever image is currently in the pipeline, but I got stuck at trying to return the new set of bounding boxes in the final moasic-ed image. Has anyone made progress on this?",working custom augment mosaic loading new whatever image currently pipeline got stuck trying return new set bounding final image anyone made progress,issue,negative,positive,neutral,neutral,positive,positive
1066818932,"@testproducts20 this seems unrelated to YOLOv5. If I run this Colab I see the same error:

```python
!pip install -U albumentations
import cv2

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
[<ipython-input-1-b305364478bf>](https://localhost:8080/#) in <module>()
      1 get_ipython().system('pip install -U albumentations')
      2 
----> 3 import cv2

3 frames
[/usr/local/lib/python3.7/dist-packages/cv2/__init__.py](https://localhost:8080/#) in <module>()
      7 
      8 from .cv2 import *
----> 9 from .cv2 import _registerMatType
     10 from . import mat_wrapper
     11 from . import gapi

ImportError: cannot import name '_registerMatType' from 'cv2.cv2' (/usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so)
```

@BloodAxe could you take a look at Colab compatibility for Albumentations? The opencv-headless install seems to be causing conflicts. To reproduce just run this in colab:
```python
!pip install -U albumentations
import cv2
```
",unrelated run see error python pip install import recent call last module install import module import import import import import name could take look compatibility install causing reproduce run python pip install import,issue,negative,neutral,neutral,neutral,neutral,neutral
1066062655,"on google colab : 

after installing ""pip install -U albumentations""
and trying to run the train.py an error is thrown up  related to cv2

Traceback (most recent call last):
  File ""train.py"", line 41, in <module>
    import val  # for end-of-epoch mAP
  File ""/content/yolov5/val.py"", line 38, in <module>
    from models.common import DetectMultiBackend
  File ""/content/yolov5/models/common.py"", line 14, in <module>
    import cv2
  File ""/usr/local/lib/python3.7/dist-packages/cv2/__init__.py"", line 9, in <module>
    from .cv2 import _registerMatType
ImportError: cannot import name '_registerMatType' from 'cv2.cv2' (/usr/local/lib/python3.7/dist-packages/cv2/cv2.cpython-37m-x86_64-linux-gnu.so)",pip install trying run error thrown related recent call last file line module import map file line module import file line module import file line module import import name,issue,negative,neutral,neutral,neutral,neutral,neutral
1061730636,"interesting.possibly. it depends how you defined key-points. 
Usually key-points types assume a consistent number of points. 
Polygons can be any number (> 3) of points. 
",defined usually assume consistent number number,issue,negative,neutral,neutral,neutral,neutral,neutral
1059158950,"The following paper also shows good improvements with MixUp/CutMix
https://arxiv.org/abs/2111.05328",following paper also good,issue,negative,positive,positive,positive,positive,positive
1056468428,"You should change the lines in ""normalize_bbox"" to the following. 

    x_min, x_max = max(0, x_min / cols), min(1, x_max / cols)
    y_min, y_max = max(0, y_min / rows), min(1, y_max / rows)",change following min min,issue,negative,neutral,neutral,neutral,neutral,neutral
1055297010,Is anyone working on this? I would be happy to take it and do a PR,anyone working would happy take,issue,positive,positive,positive,positive,positive,positive
1054945609,"@mb010  Sorry, I'm not sure about that. 

I think a simple workaround is to install the master version by using `git+https://...` syntax.
The drawback is that this may be unstable.

Another workaround would be to copy the related codes into your codebase, and replace them after the next albumentations version is released.
Since the code looks well isolated, see #1082, copying a `pixel_dropout` function and a `PixelDropout` class seem to work.
(For other dependencies, you can import from the current version by:`from albumentations import preserve_shape, DualTransform`.)",sorry sure think simple install master version syntax drawback may unstable another would copy related replace next version since code well isolated see function class seem work import current version import,issue,negative,neutral,neutral,neutral,neutral,neutral
1054171729,"> Try to provide `keypoint_params` inside `visualize_augmentations` when you create new `Compose` object

Thank you so much! It works!",try provide inside create new compose object thank much work,issue,positive,positive,positive,positive,positive,positive
1054093154,Try to provide `keypoint_params` inside `visualize_augmentations` when you create new `Compose` object,try provide inside create new compose object,issue,negative,positive,positive,positive,positive,positive
1054070221,"Unfortunately, I do not remember other such augs that could ensure keypoints don't be dropped.
That RandomSizedBBoxSafeCrop does not implement `apply_to_keypoint` looks like a bug.
As a hotfix you could create this aug and then assign method like this:
```python
import numpy as np
import albumentations as A

img = np.empty([100, 100, 3], dtype=np.uint8)
bboxes = [[10, 10, 20, 20, 1]]
keypoints = [[15, 15]]

crop_aug = A.RandomSizedBBoxSafeCrop(50, 50)
pipeline = A.Compose(
    [crop_aug], bbox_params=A.BboxParams(""pascal_voc""), keypoint_params=A.KeypointParams(""xy"")
)

def apply_to_keypoint(keypoint, crop_height=0, crop_width=0, h_start=0, w_start=0, rows=0, cols=0, **params):
    keypoint = A.keypoint_random_crop(keypoint, crop_height, crop_width, h_start, w_start, rows, cols)
    scale_x = crop_aug.width / crop_width
    scale_y = crop_aug.height / crop_height
    keypoint = A.keypoint_scale(keypoint, scale_x, scale_y)
    return keypoint

crop_aug.apply_to_keypoint = apply_to_keypoint

res = pipeline(image=img, keypoints=keypoints, bboxes=bboxes)

```",unfortunately remember could ensure implement like bug could create assign method like python import import pipeline return pipeline,issue,positive,negative,negative,negative,negative,negative
1054056169,Thanks! I didn't track that I had designed my code around the git installed version. Any ideas on when the next update might be released? @i-aki-y ,thanks track designed code around git version next update might,issue,negative,positive,neutral,neutral,positive,positive
1053904347,This feature looks to be added after the current version 1.1.0 was released and has not been released yet.,feature added current version yet,issue,negative,neutral,neutral,neutral,neutral,neutral
1053461411,"image will change, mask will choose change or unchange according to specific function.",image change mask choose change unchange according specific function,issue,negative,neutral,neutral,neutral,neutral,neutral
1044365355,"@BloodAxe 
Thank you!

This is my app https://share.streamlit.io/yoyoyo-yo/albumentations_sim_sl/main/app.py

If you find any problems about using albumentations, please tell me here.",thank find please tell,issue,positive,neutral,neutral,neutral,neutral,neutral
1042507608,"Of course, you can take your time.
I think this is not a simple patch and includes things to consider and need design decisions.
",course take time think simple patch consider need design,issue,negative,neutral,neutral,neutral,neutral,neutral
1042311982,"Thank you for your contribution. This is great and complex work!
Unfortunately, we need some time to look deeper into this PR and understand how it works.
Thus, reviewing this PR may take some time.",thank contribution great complex work unfortunately need time look understand work thus may take time,issue,positive,neutral,neutral,neutral,neutral,neutral
1037716797,"@SunQpark @Dipet @IlyaOvodov @bmabey @henrique Hi, if is possible, would you please check why this merge fails? It is 2022 now! Thank you.",hi possible would please check merge thank,issue,positive,neutral,neutral,neutral,neutral,neutral
1037079208,"@BloodAxe @Dipet  I fixed the PR.

I modified the test case to detect transforms that raise an error with non-contiguous input instead of checking contiguous output. 
As a result of the test, I found that the `add_shasow` was the only function with a problem. 
So in this new approach, fixes became simpler than the previous version.

I appreciate it if you review this modification.",fixed test case detect raise error input instead contiguous output result test found function problem new approach simpler previous version appreciate review modification,issue,negative,positive,neutral,neutral,positive,positive
1037003277,@BloodAxe Thank you for your suggestion. I will try your idea.,thank suggestion try idea,issue,negative,neutral,neutral,neutral,neutral,neutral
1035011025,"Hi! Thanks for your PR. This is indeed valuable improvement for the library. Yet i feel it may have impact on the performance.
Consider a following pipeline:
```
A.Compose([A.RandomCrop(), A.HorisontalFlip(), A.VerticalFlip()])
```
There are three consecutive augmentations that produce non-contiguous output. With the proposed change we will make 3 copies of the sliced data simply to make another slice. Which is clearly a waste of processing time. 

The initial reason why we want to have contiguous data is that OpenCV fails to handle image slices as input. With this knowledge in mind, I suggest to consider another solution:

1) Extend our unit tests to test all augmentations for non-contiguous inputs
2) For those, who fails, add a decorator function similar to `@preserve_shape` to ensure input for that function is contiguous:
```
def ensure_contiguous(func):

    @wraps(func)
    def wrapped_function(img, *args, **kwargs):
        img = np.require(img, requirements=np.C_CONTIGUOUS)
        result = func(img, *args, **kwargs)
        return result

    return wrapped_function
```
This change would ensure that we make a copy only when necessary and would be optimal from performance point of view.

3) As a final step - call a `_make_targets_contiguous` to ensure that outputs from the whole pipeline are also contiguous.


",hi thanks indeed valuable improvement library yet feel may impact performance consider following pipeline three consecutive produce output change make sliced data simply make another slice clearly waste time initial reason want contiguous data handle image input knowledge mind suggest consider another solution extend unit test add decorator function similar ensure input function contiguous result return result return change would ensure make copy necessary would optimal performance point final step call ensure whole pipeline also contiguous,issue,positive,positive,neutral,neutral,positive,positive
1034937774,"@Dipet Hi, there is another use case in document analysis, where some boxes of tokens will be shrink to a point to represent a special token. Current version thinks such boxes are invalid.

May I ask how is it going now? ",hi another use case document analysis shrink point represent special token current version invalid may ask going,issue,negative,positive,positive,positive,positive,positive
1031858275,Hi. Thanks. Celsus was added to the list of companies that use Albumentations at https://albumentations.ai/.,hi thanks added list use,issue,negative,positive,positive,positive,positive,positive
1030634947,"I applied the result of the review comments.

I made a test to check all augmentation and fixed some of them to return contiguous ndarray.

I appreciate it if you review this one.
",applied result review made test check augmentation fixed return contiguous appreciate review one,issue,negative,positive,neutral,neutral,positive,positive
1030558079,"I think I have made a test case to check all transforms. And I found that some transforms return non-contiguous output, the pytest summary is shown below.
Now I will fix these transforms to return contiguous results.

List of transforms that return non-contiguous output:
```
======================================================= short test summary info ========================================================
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[CenterCrop-params4] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[ChannelShuffle-params6] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[Crop-params9] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[CropNonEmptyMaskIfExists-params11] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[FDA-params16] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[RandomCrop-params48] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[RandomCropNearBBox-params49] - assert False
FAILED tests/test_augmentations.py::test_return_contiguous_ndarray[Transpose-params76] - assert False
```
",think made test case check found return output summary shown fix return contiguous list return output short test summary assert false assert false assert false assert false assert false assert false assert false assert false,issue,negative,negative,negative,negative,negative,negative
1028118240,"There is no bug in padding. But you should ensure that there are no image and mask dimensions mismatch.
```
import albumentations as A
import numpy as np

pad = A.PadIfNeeded(min_height=256, min_width=384)
image = np.zeros((253, 381))

print(pad(image=image)[""image""].shape) # (256, 384)
```",bug padding ensure image mask mismatch import import pad image print pad image,issue,negative,neutral,neutral,neutral,neutral,neutral
1027935993,I also thought to change the logic and change the default data type for keypoints and bboxes to `np.ndarray` and use vectorized operations. But I don't have enough time now to make it my own. I think it would be great if we will speed up this part of the code.,also thought change logic change default data type use enough time make think would great speed part code,issue,positive,positive,positive,positive,positive,positive
1024963855,"> @buiqhung127 see this comment [#459 (comment)](https://github.com/albumentations-team/albumentations/issues/459#issuecomment-734454278)

Thank you, I saw it. However, I currently work on Kaggle, which make the albumentations source interference more complicated,  so I am looking for a ""legal"" solution. I've also tried the other way but nothing works in my case.   ",see comment comment thank saw however currently work make source interference complicated looking legal solution also tried way nothing work case,issue,positive,negative,neutral,neutral,negative,negative
1024955487,"> I'm also getting this error using Albumentations in Tensorflow OD API. I've checked all the boxes and they are OK (none of them is outside the image boundaries).
> 
> I'm using `A.BboxParams(format='albumentations')` and everything looks good until on one example the training breaks with:
> 
> ```
> Expected x_max for bbox (A, B, C, D, 0) 
> to be in the range [0.0, 1.0], got C.
> ```
> 
> I've tried using `pascal_voc` but I still get the same error. How can I debug this problem?
> 
> P.S. I've solved it using a hacky solution by bypassing `check_box` in bbox_utils.py

how can you do that? can you share with me the way to bypass it?",also getting error od checked none outside image everything good one example training range got tried still get error problem hacky solution share way bypass,issue,negative,positive,positive,positive,positive,positive
1024733683,"Oh, now it makes sense! Essentially, it performs an online augmentation on the dataset, instead of an offline augmentation, if I'm getting it right.
Thank you!",oh sense essentially augmentation instead augmentation getting right thank,issue,negative,positive,positive,positive,positive,positive
1024730990,"When we train using SGD for example we sample the dataset over and over, each time applying a random transformation to the data, so that the dataset grows. Or you can apply the transformation randomly ahead of time to the data and then sample that; either way the dataset size gets bigger",train example sample time random transformation data apply transformation randomly ahead time data sample either way size bigger,issue,negative,negative,negative,negative,negative,negative
1024728614,"Hey, I am confused about the same. Did you find any explanation for this?",hey confused find explanation,issue,negative,negative,negative,negative,negative,negative
1023175661,"I closed this issue because I resolved it.  A decimal had snuck into one of my crop values.  It was hard to find because it was not the immediate augmentation I was working on, it was a planned run later. ",closed issue resolved decimal snuck one crop hard find immediate augmentation working run later,issue,negative,negative,negative,negative,negative,negative
1019308279,You can use `A.SomeOf` which should give you desired behavior.,use give desired behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
1018303472,"I haven't done any hard measurements but keypoints in albumentations are horribly slow because the operations are not vectorized at all. Would the maintainers be interested in changing this (i.e. to allow accepting numpy arrays, instead of lists of tuples)?",done hard horribly slow would interested allow instead,issue,negative,negative,negative,negative,negative,negative
1018228390,"I figured out what was wrong with my previous commits. I was using a test Dockerfile that didn't have torch installed, and some of the tests are silently not run if you don't have torch installed. These tests weren't failing locally, but would fail in the CI pipeline.

I have installed `torch` and updated both the files that have tests. I have checked `flake8` doesn't have any issues. It should be good for CI now :crossed_fingers: ",figured wrong previous test torch silently run torch failing locally would fail pipeline torch checked flake good,issue,negative,negative,neutral,neutral,negative,negative
1016205216,If you want to process batch of targets in the same way look to `additional_targets`:  https://albumentations.ai/docs/examples/example_multi_target/,want process batch way look,issue,negative,neutral,neutral,neutral,neutral,neutral
1016202437,Just merge the master branch and change reference files.,merge master branch change reference,issue,negative,neutral,neutral,neutral,neutral,neutral
1015023722,"The tests are failing because the stored file doesn't match. Is there anything special I need to do besides pulling from master before trying again? (also; bump, I guess)",failing file match anything special need besides master trying also bump guess,issue,negative,positive,positive,positive,positive,positive
1013687964,Is this Issue still ongoing? Can I take it?,issue still ongoing take,issue,negative,neutral,neutral,neutral,neutral,neutral
1013654503,"@BloodAxe Just for my sanity, I checked the docs again. It does not state clamping to [0...1]:

https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RandomBrightnessContrast

```
class albumentations.augmentations.transforms.RandomBrightnessContrast (brightness_limit=0.2, contrast_limit=0.2, brightness_by_max=True, always_apply=False, p=0.5) [view source on GitHub] ¶
Randomly change brightness and contrast of the input image.

Parameters:

Name	Type	Description
brightness_limit	[float, float] or float	
factor range for changing brightness. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).

contrast_limit	[float, float] or float	
factor range for changing contrast. If limit is a single float, the range will be (-limit, limit). Default: (-0.2, 0.2).

brightness_by_max	Boolean	
If True adjust contrast by image dtype maximum, else adjust contrast by image mean.

p	float	
probability of applying the transform. Default: 0.5.

Targets: image

Image types: uint8, float32
```",sanity checked state class view source randomly change brightness contrast input image name type description float float float factor range brightness limit single float range limit default float float float factor range contrast limit single float range limit default true adjust contrast image maximum else adjust contrast image mean float probability transform default image image float,issue,positive,negative,negative,negative,negative,negative
1013431848,"Hi, I have same question/issue as you raised. 

According to numpy's documentation, `np.linalg.eigh` returns eigenvectors in its column, i.e. `eig_vecs[:,i]` will contain the `i-th` eigenvector. This means, we should not `column_stack` later.

In addition to `column_stack` issue, we actually only need to reverse the `eig_vals` and corresponding `eig_vecs` as the eigenvalues are return in ascending order. From Numpy's [documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.eigh.html), 

> The eigenvalues in ascending order, each repeated according to its multiplicity.

Any update from maintainer would be appreciated.",hi raised according documentation column contain later addition issue actually need reverse corresponding return ascending order documentation ascending order repeated according multiplicity update maintainer would,issue,negative,neutral,neutral,neutral,neutral,neutral
1013103689,"RandomBrightnessContrast supports fp32. However, the range of float32 images assumed to stay in [0..1] range, which is stated in the docs. For values outside this range, a clamping to [0...1] range will occur.",however range float assumed stay range stated outside range range occur,issue,negative,neutral,neutral,neutral,neutral,neutral
1013091379,This should not be closed. The documentation does suggest that RandomBrightnessContrast works on both `uint8` and `float32`,closed documentation suggest work float,issue,negative,negative,neutral,neutral,negative,negative
1012088861,"> This is not a pixel mask. These are keypoints in our terms. Try this:
> 
> ```python
> transform = A.Compose([A.HorizontalFlip(p=1.)], keypoint_params=A.KeypointParams(""xy""))
> ```

Thank you， I understand.",mask try python transform thank understand,issue,negative,neutral,neutral,neutral,neutral,neutral
1012004886,Do you want to scale height and with independently?,want scale height independently,issue,negative,neutral,neutral,neutral,neutral,neutral
1011969566,"Doesnt work, changes the image shape altogether!",doesnt work image shape altogether,issue,negative,neutral,neutral,neutral,neutral,neutral
1011951080,"If you want to scale image between 0 to 1 use this settings:
```python
RandomScale(scale_limit=(-1, 0))
````",want scale image use python,issue,negative,neutral,neutral,neutral,neutral,neutral
1011731456,"> Yes, for this reason, you can use [RandomScale](https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale):
> 
> ```python
> RandomScale(scale_limit=(-1, 0))
> ```
> 
> Yes, it is a little bit strange to use this range, but it is a legacy problem.

According to definition:
`scaling factor range. If scale_limit is a single float value, the range will be (1 - scale_limit, 1 + scale_limit).`

In that case should the scale_limit be (1,0) if I want to scale the images between 0 to 1?",yes reason use python yes little bit strange use range legacy problem according definition scaling factor range single float value range case want scale,issue,positive,negative,negative,negative,negative,negative
1011710210,"@Dipet Thank you very much. I preferred setting person id for each keypoint as its label. I used strings on the list of keypoints_ids like below. the first value indicates the person_id, the second indicates the keypoints_id. it worked. thanks again
keypoints_ids = ['0_3', '0_5', '0_6', '0_7', '0_9', '0_11', '0_12', '0_13', '0_14', '0_15', '0_16',
                 '1_0', '1_1', '1_3', '1_5', '1_6', '1_7', '1_8', '1_9', '1_10', '1_11', '1_12', '1_13', '1_14', '1_15',
                 '1_16']",thank much preferred setting person id label used list like first value second worked thanks,issue,positive,positive,positive,positive,positive,positive
1011570949,"Yes, for this reason, you can use [RandomScale](https://albumentations.ai/docs/api_reference/augmentations/geometric/resize/#albumentations.augmentations.geometric.resize.RandomScale):
```python
RandomScale(scale_limit=(-1, 0))
```
Yes, it is a little bit strange to use this range, but it is a legacy problem.",yes reason use python yes little bit strange use range legacy problem,issue,negative,negative,negative,negative,negative,negative
1011567116,"Just read the docs:
```
Randomly rotate the input by 90 degrees zero or more times.
```
The image will be rotated 0, 1, 2, or 3 times 90 degrees. This means that the image will be randomly rotated 0, 90, 180, 270 degrees.
If you want to rotate the image to 90 degrees it's better to use `A.Rotate([90, 90], p=1)`",read randomly rotate input zero time image rotated time image randomly rotated want rotate image better use,issue,negative,negative,negative,negative,negative,negative
1011561729,"You need to flatten keypoints into `List[Tuple[float, float]]`.
If you want to split keypoints by persons you could use [additional_targets](https://albumentations.ai/docs/examples/example_multi_target/#applying-the-same-augmentation-with-the-same-parameters-to-multiple-images-masks-bounding-boxes-or-keypoints) or set person id for each keypoint as its label.

In your example you need flatten keypoints into `List[Tuple[float, float]]` and keypoints_ids into `List[int]`",need flatten list float float want split could use set person id label example need flatten list float float list,issue,negative,neutral,neutral,neutral,neutral,neutral
1011383356,"This is not a pixel mask. These are keypoints in our terms.
Try this:
```python
transform = A.Compose([A.HorizontalFlip(p=1.)], keypoint_params=A.KeypointParams(""xy""))
```",mask try python transform,issue,negative,neutral,neutral,neutral,neutral,neutral
1004796816,"What further information do you need to progress on this bug? My experience of using the function is that it just flat out does not work as expected (the bounding box does not rotate with the object in the image). Yun's code seems to be a minimum working example to replicate the issue and the behaviour seems to predate 1.0.0.

I can only presume the unit test for the function isn't functioning as you expect.
",information need progress bug experience function flat work bounding box rotate object image code minimum working example replicate issue behaviour predate presume unit test function expect,issue,negative,negative,neutral,neutral,negative,negative
1003467333,"I found the solution in this stack overflow thread https://stackoverflow.com/questions/70537488/cannot-import-name-registermattype-from-cv2-cv2. 

```
%pip uninstall opencv-python-headless
%pip install opencv-python-headless==4.1.2.30
```

Note that Colab version of the notebook here is currently broken because of this https://colab.research.google.com/github/albumentations-team/albumentations_examples/blob/colab/example_bboxes.ipynb ",found solution stack overflow thread pip pip install note version notebook currently broken,issue,negative,negative,negative,negative,negative,negative
1002666434,"> Correct package name is `opencv-python` not `opencv`

Sorry, seems like it's a typo, the package I'm referencing should be `opencv-python-headless`, not `opencv` (according to albumentations pip installation logging)

For uninstalling and then reinstall method, I've tried but it doesn't help.",correct package name sorry like typo package according pip installation logging reinstall method tried help,issue,positive,negative,negative,negative,negative,negative
1002564981,"Try these commands:
```
pip uninstall opencv
pip install --upgrade opencv-python
```",try pip pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
1002558814,I've tried importing `cv2` v4.5.5.62 and trying `cv2.getBuildInformation()` and both appear to be successful.,tried trying appear successful,issue,positive,positive,positive,positive,positive,positive
1002540212,"Could you just import `cv2` in Colab using  v4.5.5.62 and get success call on, say, cv2.getBuildInformation() ?
This issue does not seem to be related to Albumentations package itself, but rather to OpenCV. ",could import get success call say issue seem related package rather,issue,positive,positive,positive,positive,positive,positive
1001072695,"It looks like your question is not related to Albumentations.

We did not use any post-processing in the Carvana Challenge, models were good enough to predict an accurate mask.

You may try to use erosion / dilation to smooth your masks. https://docs.opencv.org/3.4/db/df6/tutorial_erosion_dilatation.html",like question related use challenge good enough predict accurate mask may try use erosion dilation smooth,issue,positive,positive,positive,positive,positive,positive
1001070848,"I did as follows to get the mask. 

```python
# my model 
model = [
    input = shape=(None, 224, 224, 3)
    encoder
    decoder
    output = layers.Conv2d(1, activation='sigmoid')
]

# inference
y_pred = (model.predict(x)  > 0.5).astype(np.int32)
y_pred.shape (224, 224, 1) # 

plt.imshow(y_pred, cmap=gray) # above mask 
```

To smooth the edge, I've checked [this](https://stackoverflow.com/questions/63392049/smoothing-the-edges-of-segmented-images). But all solution is not general for all inference samples.  ",get mask python model model input none output inference mask smooth edge checked solution general inference,issue,negative,positive,positive,positive,positive,positive
1001069976,Could you please provide the code that generated this mask?,could please provide code mask,issue,negative,neutral,neutral,neutral,neutral,neutral
1001069659,@ternaus I've found that you won [1st place](https://www.kaggle.com/c/carvana-image-masking-challenge/discussion/46176) in such a similar dataset. Can you give some suggestions? ,found st place similar give,issue,negative,neutral,neutral,neutral,neutral,neutral
998747366,"Hi. Thanks. AriSaf Tech was added to the list of companies that use Albumentations at https://albumentations.ai/.

",hi thanks tech added list use,issue,negative,positive,positive,positive,positive,positive
998269412,"> Thank you for the PR. Please, fix code style errors

I reformatted it with black, should be fine now",thank please fix code style black fine,issue,positive,positive,positive,positive,positive,positive
997997328,"Thank you for the PR. Please, fix code style errors",thank please fix code style,issue,positive,neutral,neutral,neutral,neutral,neutral
997177617,"You may be right, however I guess one need the brackets of the list. Otherwise one gets an not iterable error. @Dipet ",may right however guess one need list otherwise one iterable error,issue,negative,positive,positive,positive,positive,positive
997144357,pascal version was working fine for me i haven't checked upon yolo format,version working fine checked upon format,issue,negative,positive,positive,positive,positive,positive
997058299,"> Yet the issue I'm facing now with that approach is that pytest cannot resolve F.vflip for some reason after I introduce this change! Still investigating this issue, but looks like we have something is partially broken with our imports scheme...

Yes, this is a problem with `from module import *`. I think we need to think about changing logic with import using `__all__` to direct import names from modules.",yet issue facing approach resolve reason introduce change still investigating issue like something partially broken scheme yes problem module import think need think logic import direct import,issue,negative,negative,negative,negative,negative,negative
997050930,"> I also don't like huge files like `transforms.py`, but lots of small independent files are not an option either. This is not a problem at the moment, because we don't have that many files. But if you chose this approach, it might be better to import the transforms into `__init__.py` directly without using `__all__` and `from module import *`?

Ideally, they should be imported as follows: `from .dropout import *` in the main `__init__.py`. 
Yet the issue I'm facing now with that approach is that pytest cannot resolve `F.vflip` for some reason after I introduce this change! Still investigating this issue, but looks like we have something is partially broken with our imports scheme... ",also like huge like lot small independent option either problem moment many chose approach might better import directly without module import ideally import main yet issue facing approach resolve reason introduce change still investigating issue like something partially broken scheme,issue,positive,positive,positive,positive,positive,positive
997032537,"I also don't like huge files like `transforms.py`, but lots of small independent files are not an option either. This is not a problem at the moment, because we don't have that many files. But if you chose this approach, it might be better to import the transforms into `__init__.py` directly without using `__all__` and `from module import *`?",also like huge like lot small independent option either problem moment many chose approach might better import directly without module import,issue,positive,positive,positive,positive,positive,positive
997018072,"I'm not sure that proposed approach of splitting code is the right and optimal. However I personally find this way of splitting classes is definitely better than having a huge `transforms.py` class. I'm a huge fan of bite-sized content. So from this perspective having a one file per class is justified move (for me at least). 

Regarding `functional.py` - yes. Hovewer, there is a plan :) There will be a second PR to add a support of bboxes to dropout augs, with somewhat generalized approach that is to be reused by many classes. ",sure approach splitting code right optimal however personally find way splitting class definitely better huge class huge fan content perspective one file per class move least regarding yes plan second add support dropout somewhat generalized approach many class,issue,positive,positive,positive,positive,positive,positive
996989612,"Work in progress. I will add `mask_fill_value` and fix int32 data type, confused with uint32",work progress add fix data type confused,issue,negative,negative,negative,negative,negative,negative
996988482,"This PR doesn't change the meaning of docstring. In this case, `label_fields` are unnecessary.",change meaning case unnecessary,issue,negative,negative,negative,negative,negative,negative
996986910,"Are you sure splitting all classes into their unique files is a good idea? It looks Java style and I think it is overkill :). Also, it is a little bit strange that we have a unique file for each transform and single `functional` file,",sure splitting class unique good idea style think also little bit strange unique file transform single functional file,issue,positive,positive,positive,positive,positive,positive
996068205,"I'm also getting this error using Albumentations in Tensorflow OD API. I've checked all the boxes and they are OK (none of them is outside the image boundaries).

I'm using `A.BboxParams(format='albumentations')` and everything looks good until on one example the training breaks with:
``` 
Expected x_max for bbox (A, B, C, D, 0) 
to be in the range [0.0, 1.0], got C.
```

I've tried using `pascal_voc` but I still get the same error. How can I debug this problem?

P.S. I've solved it using a hacky solution by bypassing `check_box` in bbox_utils.py",also getting error od checked none outside image everything good one example training range got tried still get error problem hacky solution,issue,negative,positive,positive,positive,positive,positive
995427783,Oh; I found it. It's in a different repo. I'll open this over there.,oh found different open,issue,negative,neutral,neutral,neutral,neutral,neutral
995412478,The tests are passing locally. I'm not sure if there's a caching issue with the serialisation files or something?,passing locally sure issue something,issue,negative,positive,positive,positive,positive,positive
995403012,"After a little bit of experimentation, I found that keypoint angles are changing where they are pointing in the image when using either shear or non-square scaling. Personally, I would consider this a bug because I would expect that the angle has a relationship with with other pixels in the image, but it might be debatable.

<details>
<summary> Code </summary>

```python
import math

import cv2
import numpy as np
import albumentations as A
import matplotlib.pyplot as plt

kp_params = A.KeypointParams(format='xyas', remove_invisible=False, angle_in_degrees=True)
img = np.ones((100, 150, 3))*255

kps = [
    (0, 0, 0, 70),
    (0, 100, 0, 70),
    (150, 0, 180, 70),
    (150, 100, 180, 70),
    (25, 50, 0, 50),
    (75, 25, -90, 50)
]

mode = cv2.BORDER_CONSTANT
transform1 = A.Compose(
    [A.Affine(rotate=[10, 10], mode=mode, p=1)],
    keypoint_params=kp_params
)
transform2 = A.Compose(
    [A.Affine(shear=[20, 20], rotate=[10, 10], mode=mode, p=1)],
    keypoint_params=kp_params
)
transform3 = A.Compose(
    [A.Affine(scale={'x': [1, 1], 'y':[2,2]},
              rotate=[10, 10],
              mode=mode,
              p=1)],
    keypoint_params=kp_params
)
transform4 = A.Compose(
    [A.Affine(scale={'x': [1, 1], 'y':[2,2]},
              shear=[20, 20],
              rotate=[10, 10],
              mode=mode,
              p=1)],
    keypoint_params=kp_params
)
transform5 = A.Compose(
    [A.Perspective(scale=(0.1, 0.1), p=1)],
    keypoint_params=kp_params
)


def draw_keypoint(img, kp):
    x, y, a, s = kp
    dx = math.cos(math.radians(a))*s
    dy = math.sin(-math.radians(a))*s

    cv2.line(img, (int(x), int(y)), (int(x+dx), int(y+dy)), (0, 255, 0), 3)

count = 231
def show(res, name):
    global count
    img = res[""image""]
    for kp in res['keypoints']:
        draw_keypoint(img, kp)
    plt.subplot(count, title=f""{name} {res['keypoints'][0][-1]}"")
    plt.imshow(img)
    count += 1

fig = plt.figure(figsize=(15, 8))
show({""image"": img.copy(), ""keypoints"": kps}, ""original"")
show(transform1(image=img, keypoints=kps), ""Affine rotate"")
show(transform2(image=img, keypoints=kps), ""Affine rotate shear"")
show(transform3(image=img, keypoints=kps), ""Affine rotate scale"")
show(transform4(image=img, keypoints=kps), ""Affine rotate shear scale"")
show(transform5(image=img, keypoints=kps), ""Perspective"")
plt.show()
fig.savefig('out.png')
```
</details>

![out](https://user-images.githubusercontent.com/10515040/146301365-eb607015-a4ad-4362-a625-433e054de85f.png)

So, I would say that this pull request works for rotating non-square images at non-45 degree angles, so it solves the initial issue for `Affine`. If it is shown to work for Perspective, too, then I'd consider the initial issue solved. I would then make a separate issue for the non-square scale/shear + rotation problems in keypoint angles. Is that acceptable?",little bit experimentation found pointing image either shear scaling personally would consider bug would expect angle relationship image might debatable summary code python import math import import import import mode transform transform transform transform transform count show name global count image count name count fig show image original show transform affine rotate show transform affine rotate shear show transform affine rotate scale show transform affine rotate shear scale show transform perspective would say pull request work rotating degree initial issue affine shown work perspective consider initial issue would make separate issue rotation acceptable,issue,positive,positive,neutral,neutral,positive,positive
994874344,"> I have implemented a partial solution which is:
> 
>     * if the box entirely masked by dropout it is removed
> 
>     * if only one side of box is affected by cutout mask, box is trimmed to reflect the actual non-masked area
> 
>     * we can also think to have an overlap-based threshold when the 90%+ of box is masked out while it's bbox still remains the same and drop such instances as well
> 
> 
> This should land as a PR in a next few days

Thank you for detailing your plans!
An occlusion threshold would be useful.
Regarding the trimming feature:
-  How would it behave for the case when the visible part of the object is L shaped?
- If not already planned, please consider adding the possibility of deactivating this feature; for ex., in the case of an object detection model that uses anchors, the trimming may cause problems by creating bboxes that don't match any of the anchors
",partial solution box entirely masked dropout removed one side box affected cutout mask box reflect actual area also think threshold box masked still remains drop well land next day thank occlusion threshold would useful regarding trimming feature would behave case visible part object shaped already please consider possibility feature case object detection model trimming may cause match,issue,positive,positive,neutral,neutral,positive,positive
994866416,"> Hmm, this is a real problem, looks like we need to add flag to do not modify bboxes and keypoints. Just now you could try `Cutout`, it is deprecated, but it is change only the image

Thank you for the suggestion. This is my current direction",real problem like need add flag modify could try cutout change image thank suggestion current direction,issue,negative,positive,neutral,neutral,positive,positive
994819917,"I have implemented a partial solution which is:
- if the box entirely masked by dropout it is removed
- if only one side of box is affected by cutout mask, box is trimmed to reflect the actual non-masked area
- we can also think to have an overlap-based threshold when the 90%+ of box is masked out while it's bbox still remains the same and drop such instances as well

This should land as a PR in a next few days",partial solution box entirely masked dropout removed one side box affected cutout mask box reflect actual area also think threshold box masked still remains drop well land next day,issue,negative,negative,neutral,neutral,negative,negative
994808824,"Hmm, this is a real problem, looks like we need to add flag to do not modify bboxes and keypoints.
Just now you could try `Cutout`, it is deprecated, but it is change only the image",real problem like need add flag modify could try cutout change image,issue,negative,positive,positive,positive,positive,positive
994797169,"> This is because these transforms do not yet support bboxes https://github.com/albumentations-team/albumentations#spatial-level-transforms

Thank you for answering! For clarification, how would supporting bboxes look like? I imagine that CoarseDropout should keep the bboxes unmodified",yet support thank clarification would supporting look like imagine keep unmodified,issue,positive,positive,positive,positive,positive,positive
994490721,"Someone and me still faced this problem #955 #947 when using `.BboxParams(format=""coco""` with `RandomCrop, CenterCrop`. ",someone still faced problem coco,issue,negative,neutral,neutral,neutral,neutral,neutral
994423947,Thanks a lot for finding this bug and investigating the issue. Please have a look at my attempt to fix this issue: https://github.com/albumentations-team/albumentations/pull/1091,thanks lot finding bug investigating issue please look attempt fix issue,issue,positive,positive,positive,positive,positive,positive
994335938,Please provide a small piece of code to reproduce the problem.,please provide small piece code reproduce problem,issue,negative,negative,negative,negative,negative,negative
993066688,"Goodness me, I apologise for being extremely bad at keeping + and - straight in my head. `cv2.getRotationMatrix2D()` does, in fact, do a CCW rotation in imageland (+ve Y is down); my code scraps I used for testing were bad. 

So, there must be something in `albumentations.Affine` which flips the angle.",goodness extremely bad keeping straight head fact rotation code used testing bad must something angle,issue,negative,negative,negative,negative,negative,negative
992128666,"After a bit of research, I think that OpenCV might just be clockwise because of a bug. Most libraries adjust their definition of ""clockwise"" and ""counter-clockwise"" to match the ""+ve Y is down"" paradigm of images despite the incompatibility with the ""+ve Y is up"" paradigm used in general mathematics. And the docs for [cv2.getRotationMatrix2D()](https://docs.opencv.org/3.4/da/d54/group__imgproc__transform.html#gafbbc470ce83812914a70abfb604f4326) say that it's supposed to be counter-clockwise, too.

Here are some libraries that treat +ve as ""counter-clockwise"" while using ""+ve Y is down"":
- [MATLAB](https://au.mathworks.com/help/images/rotate-an-image.html) 
- [Matplotlib](https://github.com/matplotlib/matplotlib/blob/v3.5.0/lib/matplotlib/transforms.py#L1996-L2010)
- [Torchvision](https://pytorch.org/vision/master/generated/torchvision.transforms.functional.rotate.html#torchvision.transforms.functional.rotate)
- albumentations, of course.

So, I guess I should raise this with OpenCV. In the meantime, maybe it should be noted in the docs that rotation angle for `Affine` is clockwise?",bit research think might clockwise bug adjust definition clockwise match paradigm despite incompatibility paradigm used general mathematics say supposed treat course guess raise maybe noted rotation angle affine clockwise,issue,negative,positive,neutral,neutral,positive,positive
992055099,"Thanks for the response. Let me know if there's anything I can do that would help.

The current sampling method is very efficient. It implicitly creates quantisation boundaries by just converting to an int. Treating the range [0, 1) to be the normalised positions you can crop from without going outside the image also makes a lot of intuitive sense to me.

I'm not sure what's wrong with the current method. Is it that the [0, 1) does **not** refer to crop positions within the whole image? i.e. are you worried that people would expect `y1 = int((height + 1) * h_start)` instead?",thanks response let know anything would help current sampling method efficient implicitly converting treating range crop without going outside image also lot intuitive sense sure wrong current method refer crop within whole image worried people would expect height instead,issue,positive,positive,neutral,neutral,positive,positive
991685857,"Hey! Thanks for your PR and investigation. I'll be looking into this PR and get back to you as soon as possible.
At the first glance it does fix the abovementioned problem, yet I feel that it may be a good opportunity to revisit the design approach we use for sampling.   ",hey thanks investigation looking get back soon possible first glance fix problem yet feel may good opportunity revisit design approach use sampling,issue,positive,positive,positive,positive,positive,positive
991682507,Do we want to introduce optional parameter `mask_fill_value` to also apply dropout to masks?,want introduce optional parameter also apply dropout,issue,negative,neutral,neutral,neutral,neutral,neutral
991668848,"Everything will work correctly.
This warning means that, for example, if you provide an image and targets with different shapes than original data for reproduction (only when you call `transform.replay(**replay_args)`), it may not work correctly. ",everything work correctly warning example provide image different original data reproduction call may work correctly,issue,negative,positive,positive,positive,positive,positive
990924151,It looks like the library really doesn't have this feature yet. Closest is [CoarseDropout](https://albumentations.ai/docs/api_reference/full_reference/#albumentations.augmentations.transforms.CoarseDropout),like library really feature yet,issue,negative,positive,positive,positive,positive,positive
990641289,"With these changes:

```python
import numpy as np
import albumentations as A

# `image` is actually a map of the coordinates, so we can see where we sampled
image = np.indices([4, 4]).transpose([1,2,0])
rc = A.RandomCrop(2, 2)

print('Actual last possible crop location:')
print(image[-2, -2]) # [2 2]

for v in [0.3332, 0.3334, 0.6665, 0.6667, 0.9999, 1.0001]:
    crop = rc.apply(image, h_start=v, w_start=v)
    print(v, crop[0, 0])

# 0.3332 [0 0]
# 0.3334 [1 1]
# 0.6665 [1 1]
# 0.6667 [2 2]
# 0.9999 [2 2]
# 1.0001 [3 3]
```

(See current boundaries in linked issue)",python import import image actually map see image print last possible crop location print image crop image print crop see current linked issue,issue,negative,neutral,neutral,neutral,neutral,neutral
989825666,"It is because for replay we save noise, that will be added to the image and it is has the same shape as image
For ISONoise we just save hyper parameters to reproduce the noise.
It is not a bug.
To save memory better to save just hyper parameters, for augmentation speed better to save generated noise, because it is could be applied to multiple images. ",replay save noise added image shape image save hyper reproduce noise bug save memory better save hyper augmentation speed better save noise could applied multiple,issue,positive,positive,positive,positive,positive,positive
989731405,The strange thing is that this issue does not appear when using ISONoise rather than GaussNoise.,strange thing issue appear rather,issue,negative,negative,neutral,neutral,negative,negative
989728565,What is collected into the tr list is NOT image data. It is a small dictionary containing augmentation parameters.,collected list image data small dictionary augmentation,issue,negative,negative,negative,negative,negative,negative
989643399,"- Of course, it will be increased because you collect all new images into `tr` list. What kind of behavior do you expect?",course collect new list kind behavior expect,issue,positive,positive,positive,positive,positive,positive
989139397,"Sorry for the delay. How did you get this white background? I can not reproduce the problem. By default set `border_mode=cv2.BORDER_REFLECT_101`, which means that all pixels outside the image will be reflected starting from the image edge.",sorry delay get white background reproduce problem default set outside image reflected starting image edge,issue,negative,negative,negative,negative,negative,negative
988649572,"I realise this is a duplicate of #555 but I have explained why it is happening here, and why it is an error. Feel free to close this, if that's how you want to approach this.",duplicate happening error feel free close want approach,issue,negative,positive,positive,positive,positive,positive
987534576,"@bilal-sher-nyu ok, thanks, i will test it, may be i have input the wrong type.",thanks test may input wrong type,issue,negative,negative,negative,negative,negative,negative
987010698,"@Tim5Tang https://www.youtube.com/watch?v=NlhUpZKmzJ4&ab_channel=crazzylearners

Although majority of the video is in Hindi, the basic gist of it is that if you input a wrong datatype into a function, it will result in the OpenCV giving an error.

In the video he starts off by inputting an image, then he instead inputs a string and he gets the same error to spit out. He then inputs the image, but changes a different variable to produce a different error.

The error we're seeing is an OpenCV error due to inputting the wrong type.",although majority video basic gist input wrong function result giving error video image instead string error spit image different variable produce different error error seeing error due wrong type,issue,negative,negative,negative,negative,negative,negative
987001883,"Having a similar issue with OpenCV. 

Error Produced is:

```
Traceback (most recent call last):
  File ""c:/Users/bilal/NYU Courses/Fall 2021/02 - MSc Thesis Option/01 - Research/Yo Hou Dataset Testing/dataset.py"", line 104, in <module>
    image, mask = next(iter(train_ds))
  File ""c:/Users/bilal/NYU Courses/Fall 2021/02 - MSc Thesis Option/01 - Research/Yo Hou Dataset Testing/dataset.py"", line 66, in __getitem__
    augmentations = self.transform(image=image_, mask=masks_)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\core\composition.py"", line 210, in __call__
    data = t(force_apply=force_apply, **data)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\core\transforms_interface.py"", line 97, in __call__
    return self.apply_with_params(params, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\core\transforms_interface.py"", line 112, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\core\transforms_interface.py"", line 241, in apply_to_mask
    return self.apply(img, **{k: cv2.INTER_NEAREST if k == ""interpolation"" else v for k, v in params.items()})
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\augmentations\geometric\resize.py"", line 177, in apply
    return F.resize(img, height=self.height, width=self.width, interpolation=interpolation)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\augmentations\functional.py"", line 70, 
in wrapped_function
    result = func(img, *args, **kwargs)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\augmentations\geometric\functional.py"", line 277, in resize
    return resize_fn(img)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\albumentations\augmentations\functional.py"", line 185, in __process_fn
    chunk = process_fn(chunk, **kwargs)
cv2.error: OpenCV(4.5.4) :-1: error: (-5:Bad argument) in function 'resize'
> Overload resolution failed:
>  - src data type = 0 is not supported
>  - Expected Ptr<cv::UMat> for argument 'src'`
```

Code is:

```python
import torchvision.transforms as T
import os
from PIL import Image
from torch.utils.data import Dataset
import numpy as np
from pycocotools.coco import COCO
import torch
import cv2
import albumentations as A
from albumentations.pytorch import ToTensorV2

class YoHouDataset(Dataset):
    def __init__(self, image_dir, transform = None):
        self.image_dir = image_dir
        self.transform = transform
        self.images = os.listdir(image_dir)
        self.path_annotations = os.path.join(os.getcwd(),""Flug1_105Media_coco.json"")
        # self.f = open(self.path_annotations, 'r')
        # self.db = json.load(self.f)
        self.coco=COCO(self.path_annotations)
        
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, index):
        img_path = os.path.join(self.image_dir, self.images[index])
        image_id = index
        image_ = np.load(img_path,allow_pickle=True)[:,:,3]
        cv2.imshow('test_image',image_)
        cv2.waitKey(0)   #wait for a keyboard input
        cv2.destroyAllWindows()
        # image_ = torch.as_tensor(image_)
        image_height = image_.shape[0]
        image_width = image_.shape[1]
        
        #This will create the mask 
        mask_ = np.zeros((image_height, image_width))
        category = 0
        annIds = self.coco.getAnnIds(imgIds=image_id,catIds=category)
        anns = self.coco.loadAnns(annIds)
        for i in range(len(anns)):
            mask_ += (self.coco.annToMask(anns[i])*(i+1))
            mask_ += 1
        mask_ -= np.unique(mask_)[0]
        obj_ids = np.unique(mask_)
        obj_ids = obj_ids[1:]
        masks_ = mask_ == obj_ids[:, None, None]
        print(type(masks_))
        # masks = torch.as_tensor(masks, dtype=torch.uint8)
        print(f""Loaded Picture {index}"")
        
        if self.transform is not None:
            augmentations = self.transform(image=image_, mask=masks_)
            image = augmentations['image']
            masks = augmentations['mask']
            # image, masks = self.transform(image, masks)
        
        return image, masks
    
def get_transform(train):
    transforms = []
    transforms.append(T.ToTensor())
    if train:
        transforms.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transforms)



if __name__ == ""__main__"":
    IMAGE_HEIGHT = 2400 #Originally 2400
    IMAGE_WIDTH = 3200 #Originally 3200
    TRAIN_IMG_DIR = ""images/Flug1_105Media/""
    train_transforms = A.Compose(
        [
            # A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),
            A.Rotate(limit=35, p=1.0),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.1),
            A.Normalize(
                mean = [0.0, 0.0, 0.0],
                std = [1.0, 1.0, 1.0],
                max_pixel_value = 255.0,
            ),
            ToTensorV2(),
        ],
    )
    train_ds = YoHouDataset(
        image_dir=TRAIN_IMG_DIR,
        transform=train_transforms,#transform=get_transform(train=True),
    )
    image, mask = next(iter(train_ds))
```",similar issue error produced recent call last file thesis line module image mask next iter file thesis line file line data data file line return file line key file line return interpolation else file line apply return file line result file line resize return file line chunk chunk error bad argument function overload resolution data type argument code python import import o import image import import import coco import torch import import import class self transform none transform open self return self index index index wait keyboard input create mask category range none none print type print loaded picture index none image image image return image train train return originally originally mean image mask next iter,issue,negative,negative,negative,negative,negative,negative
983151394,"For future references to this thread, if you want to resize an image to NxN (while preserving aspect ratio) then 0/black pad the short side:
```   
A.LongestMaxSize(max_size=N, interpolation=1),
A.PadIfNeeded(min_height=N, min_width=N, border_mode=0, value=(0,0,0)),
```

By default `PadIfNeeded()` will mirror pad the border",future thread want resize image aspect ratio pad short side default mirror pad border,issue,negative,neutral,neutral,neutral,neutral,neutral
981492814,"
Hello @Dipet 
I meant, if after applying keypoint transformation, if the keypoint is going beyond the original image dimension, then the background of the image region (corresponding to that exact keypoint which has gone beyond the image dimension) is white. 

_Question/Confusion_: Is there a way to pad the image such that all the keypoints (even after transformation) has real image background, not white.

For example, please see that the image background is white when the keypoint has gone beyond the image.

![output_outpoint](https://user-images.githubusercontent.com/53333216/143848801-a397e7ca-edd6-484b-943a-8d35d3b0e261.png)

I have pasted my code [here ](https://pastecode.io/s/evxohojb)which I used to generate the scenario. Thank you

`

`",hello meant transformation going beyond original image dimension background image region corresponding exact gone beyond image dimension white way pad image even transformation real image background white example please see image background white gone beyond image pasted code used generate scenario thank,issue,positive,positive,positive,positive,positive,positive
980629349,"I am also seeing this issue using Yun's code example. Sometimes the box is successfully returned (usually for small angles), but for the majority of the time, an empty array is returned for the bbox.",also seeing issue code example sometimes box successfully returned usually small majority time empty array returned,issue,negative,positive,positive,positive,positive,positive
980462403,I didn't quite understand what you want. Could you give me a small example?,quite understand want could give small example,issue,negative,negative,negative,negative,negative,negative
980023947,"@Dipet Is there a way to pad the image such that keypoints (which are beyond the image region) background is not white?
I am using `ShiftScaleRotate` which is causing some keypoints to go beyond the image region.",way pad image beyond image region background white causing go beyond image region,issue,negative,neutral,neutral,neutral,neutral,neutral
979851976,"Sorry, I've overread that it only changes the minimum of the blur limit and not the maximum. 
",sorry overread minimum blur limit maximum,issue,negative,negative,negative,negative,negative,negative
979329360,"Hi. Sorry for the outdated docs. I have updated them. 

The correct way is now to use the `--no-binary qudida,albumentations` argument, e.g. `pip install -U albumentations --no-binary qudida,albumentations`",hi sorry outdated correct way use argument pip install,issue,negative,negative,negative,negative,negative,negative
977619057,"Thanks for reporting this bug.
Yes, looks like now `ReplayCompose` works incorrectly with `Lambda`. ",thanks bug yes like work incorrectly lambda,issue,positive,positive,positive,positive,positive,positive
977610119,"I'm not familiar with the `ReplayCompose`, but judging from the source code and the error message, I think a workaround is to use the `_restore_for_replay` method.

```py
re_param = transform_A(image=image)['replay']
lambda_transforms = {lam.name : lam for lam in transform_A if isinstance(lam, A.Lambda)}
augs = transform_A._restore_for_replay(re_param, lambda_transforms=lambda_transforms)
augs(force_apply=True, image=image)
```

The `replay(...)` method is a simple wrapper of the `_restore_for_replay(...)`, and the workaround code is almost the same as `replay()`, but the differences are the `lambda_transforms` is prepared from the original transform pipeline and pass it to the `_restore_for_replay(...)`.

https://github.com/albumentations-team/albumentations/blob/6de7dd01410a666c23c70cf69c548f171c94a1a7/albumentations/core/composition.py#L432-L435




",familiar source code error message think use method lam lam lam replay method simple wrapper code almost replay prepared original transform pipeline pas,issue,negative,positive,positive,positive,positive,positive
976531175,"I have encountered the same issue, `opencv-python-headless` is installed even though I already have `opencv-python`",issue even though already,issue,negative,neutral,neutral,neutral,neutral,neutral
976314120,"I think the equivalent for `np.random.randint(a,b)` would be a `random.randrange(a,b)`, which returns values from `[a,b)` range - https://docs.python.org/3/library/random.html#random.randrange",think equivalent would range,issue,negative,neutral,neutral,neutral,neutral,neutral
975968083,"`np.random.randint(a, b)`, `a` inclusive, `b` exclusive

`random.randint(a, b)` `a` and `b` inclusive


=> we cannot just replace functions.
",inclusive exclusive inclusive replace,issue,negative,neutral,neutral,neutral,neutral,neutral
975927168,Could you please add argument types to the function?,could please add argument function,issue,negative,neutral,neutral,neutral,neutral,neutral
975652419,"The BasicTransform (and all other transforms) take only 1 image a input and don't manage iterables themselves.
So I don't think it's possible without changing how albumentations or putting them as a different kind of object.

We would need to create an ""BasicIterableTransform"" than takes an iterable as input. But it wouldn't work with tools like torch.utils.data.Dataset without modification, need a new compose class, etc.

I agree it's a shame they're not included. It's super useful in detection problems, even though it requires quite a few modifications in albumentations and changes how the transforms would be used.

Any maintainer can tell us if it's the kind of features (namely Transforms taking iterables) that would be accepted/merged or it's just too different to how the library works now?

",take image input manage think possible without different kind object would need create iterable input would work like without modification need new compose class agree shame included super useful detection even though quite would used maintainer tell u kind namely taking would different library work,issue,positive,positive,positive,positive,positive,positive
974764526,"- Name is OK? - Maybe better to use Filter2D. But this name ok for me too.
- Is using np.random.uniform safe in this scenario? - It is not safe, but we have a lot of transforms that use np.random and we need to fix all of them in a single PR. But you could fix this problem like [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L2064) when you need to create `RandomState` first using `random.randint`
- ValueError text messages is OK? - To me they are ok",name maybe better use name safe scenario safe lot use need fix single could fix problem like need create first text,issue,positive,positive,positive,positive,positive,positive
974631753,"This is a problem for all transforms that work with boxes. Unfortunately, we do not have a solution how to implement different border modes for bboxes and keypoints.",problem work unfortunately solution implement different border,issue,negative,negative,negative,negative,negative,negative
974309937,"```python
transform = A.RingingOvershoot(
    limit=(7, 21), 
    cutoff=(np.pi / 4, np.pi / 2),
    p=1.0
)
```
Examples: 
![image](https://user-images.githubusercontent.com/15848838/142673277-dc118911-2686-42c9-903e-8f1d4b3ae3da.png) | ![image](https://user-images.githubusercontent.com/15848838/142673299-aa7c8d92-a996-434a-8269-0e7f61962346.png)
Ringing can be clearly seen on this one. 
![image](https://user-images.githubusercontent.com/15848838/142673442-0481d8ec-4f72-46e2-b911-d21f4c1aab9f.png) | ![image](https://user-images.githubusercontent.com/15848838/142673474-e89461dc-b96b-4622-beb4-14043cac768b.png)

",python transform image image ringing clearly seen one image image,issue,negative,positive,positive,positive,positive,positive
974164127,"Please, add this transform into serialization tests https://github.com/albumentations-team/albumentations/blob/master/tests/test_serialization.py#L66",please add transform serialization,issue,negative,neutral,neutral,neutral,neutral,neutral
974160715,"Tested that it works with both `uint8` and `float32` images.

Examples of applying this transform:
```python
transform = A.Compose([
    A.UnsharpMask(blur_limit=(7, 15), alpha=(0.7, 0.9), p=1.0)
])
```

![image](https://user-images.githubusercontent.com/15848838/142645439-8ca3df5f-e133-4659-98c8-935dcaab3ba6.png) | ![image](https://user-images.githubusercontent.com/15848838/142645472-edde7fa9-5f8c-4464-a1a8-d486cb24c80b.png)


![image](https://user-images.githubusercontent.com/15848838/142646398-115f78e1-3c8e-43c4-aec6-bfdf2ef14108.png) | ![image](https://user-images.githubusercontent.com/15848838/142646461-c3010d6b-90ab-4937-b5ec-e2fb8626968e.png)


",tested work float transform python transform image image image image,issue,negative,neutral,neutral,neutral,neutral,neutral
974145996,Could you provide some example images of how this transform works?,could provide example transform work,issue,negative,neutral,neutral,neutral,neutral,neutral
974086450,"I think would be great to set a link to the original article.

- Name is OK? -  I think yes.
- Do I use correct decorators for unsharp_mask function? - Looks good to me
- I added typehints to function, but not class implementation. Is it OK? - It would be great if you also add type hints to the class.
- Do we need to pass threshold as transform parameter or it's OK to have it fixed in function? - I think it's better to add this as a transformation parameter. 
- I currently have only one very basic test copy-pasted from GaussianBlur. Is it enough? - This is fine, but if you come up with more tests it would be great. ",think would great set link original article name think yes use correct function good added function class implementation would great also add type class need pas threshold transform parameter fixed function think better add transformation parameter currently one basic test enough fine come would great,issue,positive,positive,positive,positive,positive,positive
974066324,Could you also provide some examples with different arguments for transformation?,could also provide different transformation,issue,negative,neutral,neutral,neutral,neutral,neutral
973826518,"Oh, now I understand. Thanks a lot for finding this bug.
We have a bug in the deprecated API for imgaug transforms. [There](https://github.com/albumentations-team/albumentations/blob/master/albumentations/imgaug/transforms.py#L78
) we do not check if the image size is changed.

Please, remove `keep_size` from `crop_and_pad_bbox`, because it is not being used now.
And just remove test cases where `keep_size=False` from [here](https://github.com/albumentations-team/albumentations/blob/master/tests/test_imgaug.py#L259), because they are invalid.",oh understand thanks lot finding bug bug check image size please remove used remove test invalid,issue,positive,positive,positive,positive,positive,positive
973802159,"I did a draft implementation in my private project, will create a PR later today with it so we can discuss design choices there. ",draft implementation private project create later today discus design,issue,negative,neutral,neutral,neutral,neutral,neutral
973787217,"Thanks for the idea!
We need to think about how to implement this feature.
It looks like we need some kind of transformation that could apply any convolution filter to the image.",thanks idea need think implement feature like need kind transformation could apply convolution filter image,issue,positive,positive,positive,positive,positive,positive
973310809,"> Could you please give some examples to reproduce the problem? These changes break existing tests `tests/test_imgaug.py::test_compare_crop_and_pad`

This shows the problem:

```
import albumentations as A
import numpy as np

for keep_size in [True, False]:
    transform = A.Compose(
        [A.CropAndPad(px=(8, 8, 8, 8), keep_size=keep_size)],
        bbox_params=A.BboxParams(format=""yolo""),
    )

    transformed = transform(
        image=np.zeros((16, 16, 3), dtype=np.uint8),
        bboxes=[(0.5, 0.5, 0.5, 0.5, ""label"")],
    )

    print(keep_size, transformed[""bboxes""])
```

output:
```
True [(0.5, 0.5, 0.25, 0.25, 'label')]
False [(0.875, 0.875, 0.25, 0.25, 'label')]
```

These bounding box coordinates are measured on [0,1], so scaling the image with keep_size shouldn't affect them. The output given with keep_size=True is the correct output, as in this example the bbox is centered in the image and the image is equally padded on all sides.",could please give reproduce problem break problem import import true false transform transform label print output true false bounding box measured scaling image affect output given correct output example centered image image equally side,issue,negative,negative,neutral,neutral,negative,negative
973227259,"There is a separate GitHub repo dedicated to this problem (and working solution!):
https://github.com/assafshocher/ResizeRight
",separate problem working solution,issue,negative,neutral,neutral,neutral,neutral,neutral
971358493,Could you please give some examples to reproduce the problem? These changes break existing tests `tests/test_imgaug.py::test_compare_crop_and_pad`,could please give reproduce problem break,issue,negative,neutral,neutral,neutral,neutral,neutral
970294360,"Thanks so much for finding this bug! 
Looks like need to use an independent copy of the mask there https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/crops/transforms.py#L200",thanks much finding bug like need use independent copy mask,issue,positive,positive,positive,positive,positive,positive
968978645,"OpenCV IO cv2.imread/cv2.imwrite operates on images in BGR channels order. However, afte reading an image you convert it to RGB format. So in order to get expected results you should either avoid BGR2RGB conversion or convert it back to BGR before saving to disk.",io order however reading image convert format order get either avoid conversion convert back saving disk,issue,negative,neutral,neutral,neutral,neutral,neutral
968921669,"I added my code to the question.
Thank you.",added code question thank,issue,negative,neutral,neutral,neutral,neutral,neutral
968538913,"It would help if you provide a link / code that has the following issue and
we can go from there with helping you to address the problem.

пн, 15 нояб. 2021 г. в 03:31, ryotaro137 ***@***.***>:

> Then what should I do? Can you tell me?
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/1057#issuecomment-968430724>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YA5B75XFXEXVGTQPUDUMBPITANCNFSM5H7XDTHQ>
> .
>
",would help provide link code following issue go helping address problem tell reply directly view,issue,negative,positive,neutral,neutral,positive,positive
966628351,"I am also having the same issue, where my environment already has `opencv-contrib-python` and `pip install -U albumentations --no-binary imgaug,albumentations` is installing `opencv-python-headless`",also issue environment already pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
962884026,"`visualize_label_onto_image(training_dataset_plain[i][0], training_dataset_plain[i][1], 0.3)`

There you get different image and mask, because it is 2 different calls of transforms.
Change to:
```python
res = training_dataset_plain[i]
visualize_label_onto_image(res[0], res][1], 0.3)
```",get different image mask different change python,issue,negative,neutral,neutral,neutral,neutral,neutral
962879314,"Could you provide some example data to reproduce the problem?
This test shows that everything is fine:
```python
import numpy as np
import albumentations as A

img = np.random.randint(0, 256, [100, 100], dtype=np.uint8)
mask = img.copy()

transform = A.Compose([A.HorizontalFlip(p=0.5)])

N = 1000
for i in range(N):
    res = transform(image=img, mask=mask)
    assert np.allclose(res[""image""], res[""mask""])
```",could provide example data reproduce problem test everything fine python import import mask transform range transform assert image mask,issue,negative,positive,positive,positive,positive,positive
962693080,"Do not use transforms outside `Compose`.
Change:
`affine_transform = HorizontalFlip()`
To:
`affine_transform = Compose([HorizontalFlip()])`",use outside compose change compose,issue,negative,neutral,neutral,neutral,neutral,neutral
961872304,"TLDR: It's not bug, it's a feature (c)

At some point back in history, a clipping of float values to [0..1] looked like a decent idea for augmentations like random brightness/gamma/noise etc. So it became adopted and now exists for the sake of backward compatibility. If you utterly need this feature you can copy-paste augmentation to your code-base and remove @clipped decorator on apply funciton.

PS: Maybe some day we will add Unclipped versions of affected augmentations. 
",bug feature point back history clipping float like decent idea like random adopted sake backward compatibility utterly need feature augmentation remove clipped decorator apply maybe day add unclipped affected,issue,positive,negative,neutral,neutral,negative,negative
961369150,"That is a real problem and I do not know how to resolve it.

This time I can only suggest a solution like this, without using class_labels. Just pack labels into bboxes:

```python
import numpy as np
import albumentations as A
from albumentations.pytorch import ToTensorV2


def pack_labels(bboxes, labels):
    return [list(box) + [label] for box, label in zip(bboxes, labels)]


def upack_labels(bboxes):
    res_bboxes = []
    res_labels = []
    for box in bboxes:
        res_bboxes.append(box[:-1])
        res_labels.append(box[-1])
    return res_bboxes, res_labels


train_transform = A.Compose(
    [

        A.HorizontalFlip(p=0.5),
        A.RandomBrightnessContrast(p=0.2),
        A.Resize(480, 640),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(
        format='pascal_voc',
        min_area=1,
        min_visibility=0.1,
    ),
    additional_targets={'gt': 'bboxes'}
)

image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
set1 = [[0, 0, 10, 10], [20, 20, 30, 30]]
classes_set1 = [0, 1]
set2 = [[40, 40, 50, 50], [60, 60, 70, 70]]
classes_set2 = [2, 3]
a = pack_labels(set1, classes_set1)
b = pack_labels(set2, classes_set2)

transformed = train_transform(
    image=image,
    bboxes=pack_labels(set1, classes_set1),
    gt=pack_labels(set2, classes_set2)
)

set1, classes_set1 = upack_labels(transformed[""bboxes""])
set2, classes_set2 = upack_labels(transformed[""gt""])
print(classes_set1, classes_set2)

```",real problem know resolve time suggest solution like without pack python import import import return list box label box label zip box box box return image set set set set set set set set print,issue,positive,positive,positive,positive,positive,positive
960919000,"Thanks, @Dipet's reply helped to fix solve the issue.

https://github.com/albumentations-team/albumentations/blob/6c733c42647dcd5b3833a8d1b7b987f83170edde/albumentations/core/composition.py#L37-L58

Do you have a plan to make `transforms` argument get `list` only?",thanks reply fix solve issue plan make argument get list,issue,negative,positive,positive,positive,positive,positive
960779282,Is there any progress on this feature? This would be quite handy. ;),progress feature would quite handy,issue,negative,positive,positive,positive,positive,positive
958925018,"Hm, actually I was able to reproduce this issue. We will investigate this. Thanks for reporting!
",actually able reproduce issue investigate thanks,issue,negative,positive,positive,positive,positive,positive
958863996,"If problem was came from dataset as you said @BloodAxe, problem should occur when I run a single `Sequential` pipeline, but every time, every single `Sequential` pipelines used same .png file and works fine.

### 1.
```python3
tf = A.OneOf([
         A.Sequential(self._get_resize_crop(**kwargs), p=1.0),
         #A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
```

['/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MP_SEL_SUR_002440.jpg', '/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MASK/MP_SEL_SUR_002440.png']
![image](https://user-images.githubusercontent.com/46595649/140043932-03d5e825-086b-4abe-a927-1d76e2779112.png)



### 2.
```python3
tf = A.OneOf([
         #A.Sequential(self._get_resize_crop(**kwargs), p=1.0),
         A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
```

['/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MP_SEL_SUR_002440.jpg', '/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MASK/MP_SEL_SUR_002440.png']
![image](https://user-images.githubusercontent.com/46595649/140044095-09d5f6fc-2829-48fa-bf52-01dd6f54fa3c.png)


### 3.
```python3
tf = A.OneOf([
         #A.Sequential(self._get_resize_crop(**kwargs), p=1.0),
         #A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
```

['/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MP_SEL_SUR_002440.jpg', '/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MASK/MP_SEL_SUR_002440.png']
![image](https://user-images.githubusercontent.com/46595649/140044348-9b3b23b4-51d7-4b92-8979-3b2f7376bd94.png)



### 4.
```python3
tf = A.OneOf([
         #A.Sequential(self._get_resize_crop(**kwargs), p=1.0),
         #A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         #A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
```

['/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MP_SEL_SUR_002440.jpg', '/content/gdrive/MyDrive/data/pedestrian/surface_masking/Surface_021/MASK/MP_SEL_SUR_002440.png']
![image](https://user-images.githubusercontent.com/46595649/140044416-7d5eb89b-881a-4e6c-9a20-1810d1649075.png)



But when `A.OneOf()` gets double, triple `Sequential` pipeline, generates strange result.",problem came said problem occur run single sequential pipeline every time every single sequential used file work fine python image python image python image python image double triple sequential pipeline strange result,issue,negative,positive,neutral,neutral,positive,positive
958813637,"The second sequential operation could cause this:
```
 Sequential([
    RandomCrop(always_apply=False, p=1.0, height=780, width=1280),
    Resize(always_apply=False, p=1.0, height=480, width=640, interpolation=1),
  ], p=1.0),
```

Here you trying to get a random crop of size 1280x780. Depending on your dataset this may or may be not guaranteed that all images has sufficient size. If some image comes in smaller size, this operation will raise an exception (Like you see above). Could be your case.

",second sequential operation could cause sequential resize trying get random crop size depending may may sufficient size image come smaller size operation raise exception like see could case,issue,negative,negative,negative,negative,negative,negative
958786355,"@BloodAxe Thank you for advice! I couldn't search proper keyword like ""nested"" which wasn't familiar ​with me. :)
I updated `albumentation` from colab built-in version to `1.1.0` and removed `A.Compose` inside of `A.OneOf` block.
But another problem has occurred.

<br>

### 1.

```python3
tf = A.OneOf([
         #A.Sequential(self._get_resize_crop(**kwargs), p=1.0), # <-- this line still makes error.
         A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
return A.Compose(tf, p=1.0)
```

When print inside of transform components,
```python3
print(tf)
```

nothing seemed to matter,

`output`
```python3
OneOf([
  Sequential([
    RandomCrop(always_apply=False, p=1.0, height=780, width=1280),
    Resize(always_apply=False, p=1.0, height=480, width=640, interpolation=1),
  ], p=1.0),
  Sequential([
    PadIfNeeded(always_apply=False, p=1.0, min_height=1920, min_width=1920, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=None, mask_value=None),
    RandomCrop(always_apply=False, p=1.0, height=780, width=1280),
    Resize(always_apply=False, p=1.0, height=480, width=640, interpolation=1),
  ], p=1.0),
  Sequential([
    PadIfNeeded(always_apply=False, p=1.0, min_height=1920, min_width=1920, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=None, mask_value=None),
    Resize(always_apply=False, p=1.0, height=780, width=1280, interpolation=1),
    RandomCrop(always_apply=False, p=1.0, height=480, width=640),
  ], p=1.0),
], p=1.0)
```

but result images are same.

![image](https://user-images.githubusercontent.com/46595649/140035488-ef72be75-3a21-44c1-a274-42bc9e543d25.png)

<br>
<hr>
<br>

### 2.

There is something even strange. When run this code without annotation,

```python3
tf = A.OneOf([
         A.Sequential(self._get_resize_crop(**kwargs), p=1.0),
         A.Sequential(self._get_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_crop_resize(**kwargs), p=1.0),
         A.Sequential(self._get_pad_resize_crop(**kwargs), p=1.0),
        ], p=1.0)
return A.Compose(tf, p=1.0)
```

<br>

```python3
print(tf)
```

`output`
```python3
OneOf([
  Sequential([
    Resize(always_apply=False, p=1.0, height=780, width=1280, interpolation=1),
    RandomCrop(always_apply=False, p=1.0, height=480, width=640),
  ], p=1.0),
  Sequential([
    RandomCrop(always_apply=False, p=1.0, height=780, width=1280),
    Resize(always_apply=False, p=1.0, height=480, width=640, interpolation=1),
  ], p=1.0),
  Sequential([
    PadIfNeeded(always_apply=False, p=1.0, min_height=1920, min_width=1920, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=None, mask_value=None),
    RandomCrop(always_apply=False, p=1.0, height=780, width=1280),
    Resize(always_apply=False, p=1.0, height=480, width=640, interpolation=1),
  ], p=1.0),
  Sequential([
    PadIfNeeded(always_apply=False, p=1.0, min_height=1920, min_width=1920, pad_height_divisor=None, pad_width_divisor=None, border_mode=4, value=None, mask_value=None),
    Resize(always_apply=False, p=1.0, height=780, width=1280, interpolation=1),
    RandomCrop(always_apply=False, p=1.0, height=480, width=640),
  ], p=1.0),
], p=1.0)
```
*Note that `always_apply` argument always False, every `Sequential` group gradually decreases image width and height*

but error raises
```python3
ValueError: Requested crop size (780, 1280) is larger than the image size (480, 640)
```


### Entire error description

`output`
```python3
ValueError                                Traceback (most recent call last)
<ipython-input-17-4725b0ed649d> in <module>()
     50 
     51         start = time.time()
---> 52         image_train_debug, mask_train_debug = aug_fn_for_train(image, mask)
     53         end = time.time()
     54         print('training augmentation time cost (per image):', end - start)

6 frames
<ipython-input-16-90611d3e3759> in <lambda>(image, mask)
     23 
     24     def get_train_augmentation(self):
---> 25         return lambda image, mask: list(self.train_augmentation(image=image, mask=mask).values())
     26 
     27     def get_valid_augmentation(self):

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/core/composition.py in __call__(self, force_apply, *args, **data)
    213 
    214         for idx, t in enumerate(transforms):
--> 215             data = t(force_apply=force_apply, **data)
    216 
    217             if check_each_transform:

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/core/composition.py in __call__(self, **data)
    604     def __call__(self, **data):
    605         for t in self.transforms:
--> 606             data = t(**data)
    607         return data

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/core/transforms_interface.py in __call__(self, force_apply, *args, **kwargs)
     95                     )
     96                 kwargs[self.save_key][id(self)] = deepcopy(params)
---> 97             return self.apply_with_params(params, **kwargs)
     98 
     99         return kwargs

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/core/transforms_interface.py in apply_with_params(self, params, force_apply, **kwargs)
    110                 target_function = self._get_target_function(key)
    111                 target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
--> 112                 res[key] = target_function(arg, **dict(params, **target_dependencies))
    113             else:
    114                 res[key] = None

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/augmentations/crops/transforms.py in apply(self, img, h_start, w_start, **params)
     45 
     46     def apply(self, img, h_start=0, w_start=0, **params):
---> 47         return F.random_crop(img, self.height, self.width, h_start, w_start)
     48 
     49     def get_params(self):

/content/gdrive/MyDrive/ColabWorkspace/albumentations/albumentations/augmentations/crops/functional.py in random_crop(img, crop_height, crop_width, h_start, w_start)
     26             ""Requested crop size ({crop_height}, {crop_width}) is ""
     27             ""larger than the image size ({height}, {width})"".format(
---> 28                 crop_height=crop_height, crop_width=crop_width, height=height, width=width
     29             )
     30         )

ValueError: Requested crop size (780, 1280) is larger than the image size (480, 640)
```",thank advice could search proper like familiar version removed inside block another problem python line still error return print inside transform python print nothing matter output python sequential resize sequential resize sequential resize result image something even strange run code without annotation python return python print output python sequential resize sequential resize sequential resize sequential resize note argument always false every sequential group gradually image width height error python crop size image size entire error description output python recent call last module start image mask end print augmentation time cost per image end start de lambda image mask self return lambda image mask list self self data enumerate data data self data self data data data return data self id self return return self key key key else key none apply self apply self return self crop size image size height width crop size image size,issue,negative,negative,neutral,neutral,negative,negative
958732383,`A.Compose` designed to be top-level augmentation and not intended for nested use. You may want to change nested augmentations to `A.Sequential`,designed augmentation intended use may want change,issue,negative,neutral,neutral,neutral,neutral,neutral
953865554,"```python
def get_boxes(label_path):
    xml_path = os.path.join(label_path)

    tree = ET.parse(xml)
```
There you do not provide XML into function and global var will be used.

```python
for obj in obj_xml:
    bbox_original = obj.find('bndbox')
    bbox_original.find('xmin').text = str(int(bbox_mod[0][0]))
    bbox_original.find('ymin').text = str(int(bbox_mod[0][1]))
    bbox_original.find('xmax').text = str(int(bbox_mod[0][2]))
    bbox_original.find('ymax').text = str(int(bbox_mod[0][3]))

    del bbox_mod[0]
```

There you think that all boxes will be represented, but some of them may be cropped by `Rotation`.
If you want to save all boxes use only `SafeRotate` because it is doesn't crop the image.",python tree provide function global used python think may rotation want save use crop image,issue,positive,neutral,neutral,neutral,neutral,neutral
953557115,"This seems to be incorrect:
> `bbox_original.find('xmin').text = str(int(bbox_mod[0][0]))` 

You're always using first element for bbox_mod, regardless whether there are boxes at all or not. As a rule of thumb,
you should not make assumptions that order or number of bboxes will be preserved before & after augmentation. So I suggest to create the destination structure from scratch using the returned bboxes.
",incorrect always first element regardless whether rule thumb make order number augmentation suggest create destination structure scratch returned,issue,negative,positive,positive,positive,positive,positive
953432336,"Image Size

2048*1500


XML
```
<annotation>
	<folder>20481500NG</folder>
	<filename>NG_16.JPG</filename>
	<path>D:\20481500NG\NG_16.JPG</path>
	<source>
		<database>Unknown</database>
	</source>
	<size>
		<width>2048</width>
		<height>1500</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>
	<object>
		<name>Hei</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>581</xmin>
			<ymin>467</ymin>
			<xmax>606</xmax>
			<ymax>494</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hei</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>574</xmin>
			<ymin>490</ymin>
			<xmax>580</xmax>
			<ymax>499</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1734</xmin>
			<ymin>397</ymin>
			<xmax>1743</xmax>
			<ymax>405</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hei</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1707</xmin>
			<ymin>380</ymin>
			<xmax>1717</xmax>
			<ymax>393</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hei</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1882</xmin>
			<ymin>450</ymin>
			<xmax>1893</xmax>
			<ymax>462</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1805</xmin>
			<ymin>512</ymin>
			<xmax>1814</xmax>
			<ymax>523</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1735</xmin>
			<ymin>489</ymin>
			<xmax>1745</xmax>
			<ymax>504</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1594</xmin>
			<ymin>473</ymin>
			<xmax>1605</xmax>
			<ymax>484</ymax>
		</bndbox>
	</object>
	<object>
		<name>TuCeng</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1520</xmin>
			<ymin>464</ymin>
			<xmax>1534</xmax>
			<ymax>480</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1473</xmin>
			<ymin>441</ymin>
			<xmax>1483</xmax>
			<ymax>454</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1338</xmin>
			<ymin>426</ymin>
			<xmax>1350</xmax>
			<ymax>444</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1298</xmin>
			<ymin>530</ymin>
			<xmax>1309</xmax>
			<ymax>542</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1296</xmin>
			<ymin>428</ymin>
			<xmax>1308</xmax>
			<ymax>444</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1236</xmin>
			<ymin>445</ymin>
			<xmax>1249</xmax>
			<ymax>465</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1185</xmin>
			<ymin>444</ymin>
			<xmax>1197</xmax>
			<ymax>462</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1176</xmin>
			<ymin>499</ymin>
			<xmax>1189</xmax>
			<ymax>517</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1093</xmin>
			<ymin>497</ymin>
			<xmax>1104</xmax>
			<ymax>513</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hei</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1135</xmin>
			<ymin>529</ymin>
			<xmax>1144</xmax>
			<ymax>542</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1017</xmin>
			<ymin>409</ymin>
			<xmax>1027</xmax>
			<ymax>421</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1092</xmin>
			<ymin>524</ymin>
			<xmax>1100</xmax>
			<ymax>535</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1080</xmin>
			<ymin>508</ymin>
			<xmax>1089</xmax>
			<ymax>518</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1100</xmin>
			<ymin>436</ymin>
			<xmax>1108</xmax>
			<ymax>448</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>1169</xmin>
			<ymin>409</ymin>
			<xmax>1177</xmax>
			<ymax>420</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>969</xmin>
			<ymin>519</ymin>
			<xmax>979</xmax>
			<ymax>535</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>946</xmin>
			<ymin>408</ymin>
			<xmax>956</xmax>
			<ymax>418</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>957</xmin>
			<ymin>436</ymin>
			<xmax>970</xmax>
			<ymax>448</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>894</xmin>
			<ymin>402</ymin>
			<xmax>904</xmax>
			<ymax>415</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>887</xmin>
			<ymin>468</ymin>
			<xmax>897</xmax>
			<ymax>481</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>904</xmin>
			<ymin>510</ymin>
			<xmax>920</xmax>
			<ymax>521</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>784</xmin>
			<ymin>502</ymin>
			<xmax>795</xmax>
			<ymax>518</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>803</xmin>
			<ymin>449</ymin>
			<xmax>818</xmax>
			<ymax>463</ymax>
		</bndbox>
	</object>
	<object>
		<name>TuCeng</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>744</xmin>
			<ymin>415</ymin>
			<xmax>801</xmax>
			<ymax>456</ymax>
		</bndbox>
	</object>
	<object>
		<name>TuCeng</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>738</xmin>
			<ymin>482</ymin>
			<xmax>752</xmax>
			<ymax>507</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>692</xmin>
			<ymin>474</ymin>
			<xmax>705</xmax>
			<ymax>486</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>704</xmin>
			<ymin>512</ymin>
			<xmax>716</xmax>
			<ymax>525</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>687</xmin>
			<ymin>521</ymin>
			<xmax>695</xmax>
			<ymax>532</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>694</xmin>
			<ymin>537</ymin>
			<xmax>702</xmax>
			<ymax>546</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>620</xmin>
			<ymin>424</ymin>
			<xmax>632</xmax>
			<ymax>437</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>615</xmin>
			<ymin>459</ymin>
			<xmax>623</xmax>
			<ymax>472</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>509</xmin>
			<ymin>438</ymin>
			<xmax>521</xmax>
			<ymax>455</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>427</xmin>
			<ymin>497</ymin>
			<xmax>440</xmax>
			<ymax>508</ymax>
		</bndbox>
	</object>
	<object>
		<name>Dian</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>466</xmin>
			<ymin>496</ymin>
			<xmax>475</xmax>
			<ymax>506</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>381</xmin>
			<ymin>414</ymin>
			<xmax>401</xmax>
			<ymax>439</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>238</xmin>
			<ymin>426</ymin>
			<xmax>253</xmax>
			<ymax>442</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>225</xmin>
			<ymin>486</ymin>
			<xmax>234</xmax>
			<ymax>497</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>247</xmin>
			<ymin>488</ymin>
			<xmax>253</xmax>
			<ymax>498</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>204</xmin>
			<ymin>521</ymin>
			<xmax>215</xmax>
			<ymax>532</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>221</xmin>
			<ymin>524</ymin>
			<xmax>236</xmax>
			<ymax>541</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>246</xmin>
			<ymin>508</ymin>
			<xmax>257</xmax>
			<ymax>520</ymax>
		</bndbox>
	</object>
	<object>
		<name>TuCeng</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>318</xmin>
			<ymin>463</ymin>
			<xmax>335</xmax>
			<ymax>485</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>327</xmin>
			<ymin>526</ymin>
			<xmax>340</xmax>
			<ymax>540</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>293</xmin>
			<ymin>528</ymin>
			<xmax>309</xmax>
			<ymax>544</ymax>
		</bndbox>
	</object>
	<object>
		<name>Hui</name>
		<pose>Unspecified</pose>
		<truncated>0</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>489</xmin>
			<ymin>554</ymin>
			<xmax>498</xmax>
			<ymax>567</ymax>
		</bndbox>
	</object>
</annotation>

```
@Dipet ",image size annotation folder path source unknown size width height depth segmented object name hei pose unspecified truncated difficult object name hei pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name hei pose unspecified truncated difficult object name hei pose unspecified truncated difficult object name pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name hei pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name dian pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult object name pose unspecified truncated difficult,issue,negative,negative,negative,negative,negative,negative
952920299,Thanks for the information. We forgot to update the documents. We now have a [ColorJitter](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.ColorJitter) - the equivalent of a torchvision transform. ,thanks information forgot update equivalent transform,issue,negative,positive,positive,positive,positive,positive
952917295,"Are you sure the bboxes are not being cropped by the `Rotate`? 
Could you please provide the image size and bboxes to reproduce the problem? ",sure rotate could please provide image size reproduce problem,issue,negative,positive,positive,positive,positive,positive
951151838,I think you are right. Is there are way to see which interpreter pre-commit is using? Or is there a way to change the interpreter that pre-commit is using?,think right way see interpreter way change interpreter,issue,negative,positive,positive,positive,positive,positive
949991970,"but it looks like I found the tricky solution 

```python
transform = A.Compose([
   # ...
    A.Rotate(limit=15, border_mode=cv2.BORDER_REPLICATE),
   # ...
], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))

keypoints = [val + [i] for i, val in enumerate(keypoints)]
transformed = transform(image=image, keypoints=keypoints)
keypoints_trans = [i[:-1] for i in sorted(transformed['keypoints'], key=lambda x: x[2])]

# and return points to image
keypoints = [[max(min(x[0], width-1), 0), max(min(x[1], height-1), 0)] for x in keypoints_trans]

```

it works for me

@wangwei880403 it may helps",like found tricky solution python transform enumerate transform sorted return image min min work may,issue,negative,neutral,neutral,neutral,neutral,neutral
949976283,"@Dipet yeah good idea, but do not forget for `A.KeypointParams(format='xy', remove_invisible=False)`.",yeah good idea forget,issue,negative,positive,positive,positive,positive,positive
949671671,"Yes, this transform doesn't support bboxes.
You can check which transforms support which targets there: https://albumentations.ai/docs/api_reference/full_reference/#spatial-level-transforms",yes transform support check support,issue,positive,neutral,neutral,neutral,neutral,neutral
947875025,"```In the pipeline, use bbox_params=A.BboxParams(format='pascal_voc'))```
Works for me too! I have annotations in coco json format  but format=""pascal_voc"" worked fine for me. ",pipeline use work coco format worked fine,issue,negative,positive,positive,positive,positive,positive
947587552,"It looks like their implementation is also not in line with their own paper. I have created an issue [there](https://github.com/YanchaoYang/FDA/issues/36) as well. In the meantime, would you like me to create a PR, or want to wait for them to respond?",like implementation also line paper issue well would like create want wait respond,issue,positive,neutral,neutral,neutral,neutral,neutral
947536306,"It is strange that the results are different from the paper, because we took implementation from the original repo https://github.com/YanchaoYang/FDA/blob/master/utils/__init__.py#L64.
",strange different paper took implementation original,issue,negative,positive,positive,positive,positive,positive
947405599, Please show the full error message and check that you use the correct interpreter and environment.,please show full error message check use correct interpreter environment,issue,negative,positive,positive,positive,positive,positive
945230836,There was no error. I had a problem related to the cache,error problem related cache,issue,negative,neutral,neutral,neutral,neutral,neutral
945089228,"Are you sure that you are using the same interpreter as pre-commit? Because I see that you have installed `types-PyYAML`, but pre-commit can not find it.",sure interpreter see find,issue,negative,positive,positive,positive,positive,positive
945005755,"If you defined `KeypointsParams`, you must provide `keypoints` as arguments. Just try to provide `keypoints0` as `keypoints` and remove them from `additional_targets`.",defined must provide try provide remove,issue,negative,neutral,neutral,neutral,neutral,neutral
944945002,I propose to go for the largest and best known first. Less known after them. ,propose go best known first le known,issue,positive,positive,positive,positive,positive,positive
944943430,"So how exactly we rank companies in this list? It was previously in historical order, which to me the most clear and obvious one. This PR violates this and I wonder whether it's because of the size of the company or for another reason? ",exactly rank list previously historical order clear obvious one wonder whether size company another reason,issue,negative,negative,negative,negative,negative,negative
944890084,"In most cases, algorithms work with known image height and width.
For this reason, a combination of [`CenterCrop`](https://albumentations.ai/docs/api_reference/full_reference/#albumentations.augmentations.crops.transforms.CenterCrop) and [`PadIfNeeded`]() is enough.
If you need this transformation, you can implement it and add it to the library, we'd love to see you contribute to the library.",work known image height width reason combination enough need transformation implement add library love see contribute library,issue,positive,positive,positive,positive,positive,positive
944878327,"I also found there is NOT an operation such as ""CenterCropToAspectRatio"" in ""imgaug"" library (https://github.com/aleju/imgaug).
In other words, I think that most geometric transformation operations require specific values like exact height or width.
A user may NOT know the exact height or width.
",also found operation library think geometric transformation require specific like exact height width user may know exact height width,issue,negative,positive,positive,positive,positive,positive
944250530,"Great work, thank you. But I'm encountering errors when I use ""Normalize"" function from Albumentations. Can you explain me the issue pls ?",great work thank use normalize function explain issue,issue,positive,positive,positive,positive,positive,positive
939500337,"Hi @BloodAxe, as I understand [RGBShift](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.RGBShift) is used for RGB image with shape [H, W, 3], does `albumentations` has the equivalent transform for Grayscale image with shape [H, W]?",hi understand used image shape equivalent transform image shape,issue,negative,neutral,neutral,neutral,neutral,neutral
939218586,add keypoints support since piecewiseAffine is pretty slow and recommended elasticTransform does not support keypoints.,add support since pretty slow support,issue,positive,negative,neutral,neutral,negative,negative
938779428,"A combination of `[A.Normalize(), A.ToTensorV2()]` should do the job. Please refer to this [notebook](https://github.com/albumentations-team/albumentations_examples/blob/24f3bf740ffda010dd5ceda743d4cb41899b7d22/notebooks/migrating_from_torchvision_to_albumentations.ipynb) for an example.",combination job please refer notebook example,issue,negative,neutral,neutral,neutral,neutral,neutral
938760862,"@BloodAxe I understand your idea, but I think having a transformation like `A.RepeatChannels(3)` or `A.ToRGB` will be more easy-to-use rather than use `cv2.cvtColor` outside `A.Compose[...]`. Another question is how to scale image in range [0, 255] to [0, 1] like `torchvision`'s `ToTensor`, but when I try `albumentations.pytorch.transforms.ToTensorV2`, it just swaps channels from [H, W, C] to [C, H, W]. Does `albumentations` have any equivalent transform?",understand idea think transformation like rather use outside another question scale image range like try equivalent transform,issue,positive,neutral,neutral,neutral,neutral,neutral
938706242,"You can use cv2.cvtColor function from OpenCV to do this conversion. We
have support for grayscale image augmentation for vast majority of
transforms as well. You can also change the network to take the single
channel input instead of taking three channels. So there are many options.


Пт, 8 окт. 2021 г. в 17:49, Tran N.M. Hoang ***@***.***>:

> Hi, I'm using albumentations for my project, but my image is GrayScale
> with shape [H, W] but I need to use RGB image (3 channels or [H, W, 3])
> for training neural networks. Does the library provides any transformation
> to convert grayscale to RGB, which is similar to tf.image.grayscale_to_rgb?
> thank you
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/1032>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YG4FCVXRKXGP7ROCE3UF4AHXANCNFSM5FTZIUVA>
> .
>
",use function conversion support image augmentation vast majority well also change network take single channel input instead taking three many hi project image shape need use image training neural library transformation convert similar thank thread reply directly view,issue,positive,positive,positive,positive,positive,positive
936176431,"I think we need to call `np.ascontiguousarray` at the beginning of `A.Compose.__call__` and after each call of `apply_with_params`, to be sure that we get the valid image after each transform. Because we have non contiguousarray array after channel shuffle.",think need call beginning call sure get valid image transform non array channel shuffle,issue,negative,positive,positive,positive,positive,positive
936038762,"Good point! Thank you for your comment. I reproduced that problem in the last version.
I think we could resolve this problem by just adding `np.ascontiguousarray` for the image there https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/transforms_interface.py#L97",good point thank comment problem last version think could resolve problem image,issue,negative,positive,positive,positive,positive,positive
935594062,"We have [Contributing](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst) guide that will help you set up the development environment. Since abovementioned augmentations are image-only, you can take a look on `A.GaussNoise` for example as an example what should be implemented. ",guide help set development environment since take look example example,issue,negative,neutral,neutral,neutral,neutral,neutral
934824467,"@ternaus Hi, I'd like to work on this issue, can you please assign it to me? Also can you point me to existing examples of similar transforms in the codebase that I can refer to, in order to understand the workflow?",hi like work issue please assign also point similar refer order understand,issue,positive,neutral,neutral,neutral,neutral,neutral
934377575,"@BloodAxe I'm sure you do not understand me correctly. 
Take as example the dog keypoints you provide [here](https://albumentations.ai/docs/examples/example_keypoints/) and I want to apply Horizontal / Vertical augmentation. I guess that if I pass dummy image some operations occur on this image which are redundant since it's a dummy one and I won't use it.

I think that @Dipet understood what I mean.",sure understand correctly take example dog provide want apply horizontal vertical augmentation guess pas dummy image occur image redundant since dummy one wo use think understood mean,issue,negative,negative,neutral,neutral,negative,negative
934324417,"Image is a first-class citizen in Albumentations and all transformation that supports additional targets like keypoints or boxes assuming they are in the pixel-space. While you're trying to work on `pose keypoints space` instead, I don't see a fit here.

Mind sharing how the original keypoints look like, what augmentations you plan to use and how expected augmented keypoints should look like?",image citizen transformation additional like assuming trying work pose space instead see fit mind original look like plan use augmented look like,issue,positive,positive,positive,positive,positive,positive
934308192,"BTW @Dipet is it possible to pass the keypoints as `(x,y)` format, from what I see you pass them as `(x,y,angle,scale)` if that is not possible I guess that for 2D pose estimation I can manually pad the keypoints with zeros for the scale and angle. ",possible pas format see pas angle scale possible guess pose estimation manually pad scale angle,issue,negative,neutral,neutral,neutral,neutral,neutral
934295134,"@BloodAxe I'm sorry but still it does not make sense to me. let's say I have a model which acts on the pose keypoints space instead of the image space and I want to apply augmentation to it. I want to be able to compose augmentations and apply them on the keypoints without passing dummy image (as it will be more computationally costing).
So in my case (which is without passing a dummy image) I need to iterate over list of augmentation objects and apply them on the keypoints.

I think this could be a useful feature.",sorry still make sense let say model pose space instead image space want apply augmentation want able compose apply without passing dummy image costing case without passing dummy image need iterate list augmentation apply think could useful feature,issue,negative,positive,neutral,neutral,positive,positive
934285579,"You can use Compose directly but you have to pass an image (even if it's dummy one). 
The flip operation from your example does not make sense without an image, since it needs to know the dimensions of an image, and it assumes keypoint coordinates are given in pixel space. E,g keypoint with coordinates `(1,2)`  on the image of `(512, 512)` pixels after flip will have coordinates `(512 - 1, 2)`.  Hope this clarifies why we need image to be present. ",use compose directly pas image even dummy one flip operation example make sense without image since need know image given space image flip hope need image present,issue,negative,positive,neutral,neutral,positive,positive
934280982,ohhhh I see. so I can't use `Compose` obj directly? or I didn't get you right.,see ca use compose directly get right,issue,negative,positive,positive,positive,positive,positive
934271528,"Oh, no, library has too many logical bindings to `np.ndarray` to work correctly with all augs without an image.

But you can work with simple augs like this:
```python
import random
import albumentations as A

image_height = 100
image_width = 100
image_size = image_height, image_width
keypoints = [
    [0, 0, 0, 0],
    [10, 10, 0, 0]
]

augs = [
    A.HorizontalFlip(p=1),
]

for aug in augs:
    if random.random() > aug.p:
        continue
    keypoints = aug.apply_to_keypoints(keypoints, rows=image_height, cols=image_width)
print(keypoints)
```
",oh library many logical work correctly without image work simple like python import random import continue print,issue,negative,positive,neutral,neutral,positive,positive
934252161,"Hi @Dipet thank you for your response.
Can you be more specific on what should be passed to `transform.apply_to_keypoints` please?

I tried to define transformation as follow:
```
transform = A.Compose(
    [A.HorizontalFlip(p=1)], 
    keypoint_params=A.KeypointParams(format='xy')
)
```
and then tried to call `transform.apply_to_keypoints' but it turns out that the object does not have this method.",hi thank response specific please tried define transformation follow transform tried call turn object method,issue,positive,neutral,neutral,neutral,neutral,neutral
934196542,"The difference was not dramatic: a drop of 4% on the test partition of a small dataset (test partition with around 250 data points), which is about 10 extra misclassifications. I would guess that this is happening in data which was _difficult_ for the model (i.e., top2 classes have similar probability). Feel free to close if this is something you already had under your radar; I thought to let you know in case this wasn't expected behavior.",difference dramatic drop test partition small test partition around data extra would guess happening data model top class similar probability feel free close something already radar thought let know case behavior,issue,negative,positive,neutral,neutral,positive,positive
934189789,"There are two things that may cause this change:

- Different resize implementation that @Dipet pointed out. This indeed could introduce a small difference in the resized images. In Albumentations we prefer OpenCV over PIL for speed.
- Different image decoding. In PyTorch example you're using PIL's Image.open, while we again use OpenCV for the sake of speed. Different implementations of jpeg decoders (libjpeg, libjpeg-turbo) can and will produce slightly different images. 

I honestly don't think it is a big issue. If we look at NVidia DALI as preprocessing/augmentation pipeline, they will produce images that will differs from both OpenCV & PIL results. On the other hand, applying different image decoding and resizing methods at train time can be seen as additional augmentation scheme, where you enforce model to be robust to those distribution shifts.
",two may cause change different resize implementation pointed indeed could introduce small difference prefer speed different image example use sake speed different produce slightly different honestly think big issue look dali pipeline produce hand different image train time seen additional augmentation scheme enforce model robust distribution,issue,positive,positive,neutral,neutral,positive,positive
934182470,"I can not reproduce the problem
```python
import albumentations
import numpy as np
import random

seed = 0
random.seed(seed)
np.random.seed(seed)

aug = albumentations.Compose(
    [albumentations.ChannelShuffle(p=1), albumentations.RandomShadow(p=1)]
)

for _ in range(1000):
    aug(image=np.zeros([100, 100, 3], dtype=np.uint8))
```",reproduce problem python import import import random seed seed seed range,issue,negative,negative,negative,negative,negative,negative
934173881,"What difference did you find?  What loss do you get with torchvision transforms and albumentations?
I see only one difference - OpenCV could produce different artifacts when resizing images. It is a known issue https://arxiv.org/pdf/2104.11222.pdf",difference find loss get see one difference could produce different known issue,issue,negative,neutral,neutral,neutral,neutral,neutral
934168445,"The library is designed to augment images, for this reason, an image is necessary 
You can try calling `transform.apply_to_keypoints` directly, providing the keypoitns in albumentations format, but you also need to generate all needed params before the call. You can see how to get all needed params [there](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/transforms_interface.py#L81).

",library designed augment reason image necessary try calling directly providing format also need generate call see get,issue,negative,positive,neutral,neutral,positive,positive
934141408,"Hi! From time to time we see similar bug reports that claims there is inconsistency between image & mask augmentations. However, upon detailed look it happens to be a bug on user side in 99% cases. For all augmentations we have, there are unit tests that verify both image & mask transformed identically.

Likewise, in your case you instantiate `TianChiDataset` using `test_mode=False`. This means albumentations pipeline is not applied at all to your images (See `if not self.test_mode:`). So I don't see how reported issue relates to albumentations. Please provide a minimalistic example to reproduce the issue (Ideally in Colab or self-contained python snippet) and reopen the ticket if the problem persists.",hi time time see similar bug inconsistency image mask however upon detailed look bug user side unit verify image mask identically likewise case pipeline applied see see issue please provide example reproduce issue ideally python snippet reopen ticket problem,issue,negative,positive,positive,positive,positive,positive
934120155,"@Dipet This doesn't change anything. The above code results in:
```
cv2.error: OpenCV(4.5.3) :-1: error: (-5:Bad argument) in function 'fillPoly'
> Overload resolution failed:
>  - Layout of the output array img is incompatible with cv::Mat (step[ndims-1] != elemsize or step[1] != elemsize*nchannels)
>  - Expected Ptr<cv::UMat> for argument 'img'
```

The problem is that `np.ascontiguousarray` needs to be applied after `ChannelShuffle`, and you cannot control this if you have `Compose`. One workaround is:

```
aug = albumentations.Compose([
    albumentations.OneOf([
        albumentations.ChannelShuffle(p=0.5),
        albumentations.RandomShadow(p=0.5)
])
```

But it doesn't solve the problem when you want to apply both operations to the same image.
",change anything code error bad argument function overload resolution layout output array incompatible step step argument problem need applied control compose one solve problem want apply image,issue,negative,negative,negative,negative,negative,negative
933307637,"Why are you using augs separately?
Just combine them into single `Compose`
```python
import numpy as np
import albumentations as albu

augs = albu.Compose([
    albu.ChannelShuffle(p=1),
    albu.RandomShadow(p=1)
])

img = np.random.rand(100,100,3) * 255
img = img.astype(np.uint8)

res = augs(image=np.ascontiguousarray(img))[""image""]
```",separately combine single compose python import import image,issue,negative,negative,neutral,neutral,negative,negative
932176771,"Thanks, it works. Another example to illustrate the problem:

```
import numpy as np
import albumentations as albu

albu_ch_shuffle = albu.ChannelShuffle(p=1)
albu_shadow = albu.RandomShadow(p=1)

img = np.random.rand(100,100,3) * 255
img = img.astype(np.uint8)

img = albu_ch_shuffle(image=img)['image']
img = albu_shadow(image=np.ascontiguousarray(img))['image']
```
With `np.ascontiguousarray` works, without doesn't, as expected after studying this thread.

Now, I have a list of possible augmentations as `albumentations.Compose` and `ChannelShuffle` and `RandomShadow` are among them. I want to be able to apply both for the same image. How can I nicely apply `np.ascontiguousarray` after every call to `ChannelShuffle`?

",thanks work another example illustrate problem import import work without thread list possible among want able apply image nicely apply every call,issue,positive,positive,positive,positive,positive,positive
930419101,"By condition, some coordinates in `bbox` may be equal to 1 (when the center of bbox and height and with equal to image shape) . I didn't recognize this the first time.",condition may equal center height equal image shape recognize first time,issue,negative,positive,neutral,neutral,positive,positive
930162323,"> However, for faces which has an angle to camera, say [30degree, 30degree, 30degree,], it will be inaccurate to simply add like that

The solution is to convert the Euler angles to [quaternions](https://en.wikipedia.org/wiki/Quaternion) and work with them, and after augmentation, convert them back to Euler angles.",however angle camera say degree degree degree inaccurate simply add like solution convert work augmentation convert back,issue,positive,neutral,neutral,neutral,neutral,neutral
930078387,"@Dipet Actually I am not quite clear about the solution. 
But in my case, the euler angles in 3d are labeled as ground truth for each face image.
And during online augmentiation, the images are rotated, which could cause change of 3d angles.
For front face, for example, the angle of rotation could be added to angle of ""roll"".
However, for faces which has an angle to camera, say [30degree, 30degree, 30degree,], it will be inaccurate to simply add like that. 
This is a problem about computer graphics or math. Or it may be even unsolvable. I don't know.

",actually quite clear solution case ground truth face image rotated could cause change front face example angle rotation could added angle roll however angle camera say degree degree degree inaccurate simply add like problem computer graphic math may even unsolvable know,issue,positive,positive,neutral,neutral,positive,positive
930021101,"The image is not contiguous after permute, change the augs call to this:
```python
x_np = albu_shadow(image=np.ascontiguousarray(x_np.astype(np.uint8)))['image']
```",image contiguous permute change call python,issue,negative,neutral,neutral,neutral,neutral,neutral
930005817,"I don't know exactly how the head pose estimation algorithms work, but we could try to implement them with you. 
Do you have any examples of augmentations?
Could you describe a basic subset of augs that you need to work with head pose estimation?",know exactly head pose estimation work could try implement could describe basic subset need work head pose estimation,issue,negative,positive,positive,positive,positive,positive
930002654,"Hi, as far as I know, head pose estimation uses 3D coordinates (head rotation around 3 axes). Unfortunately, the library does not yet support keypoints and bboxes in 3D. ",hi far know head pose estimation head rotation around ax unfortunately library yet support,issue,negative,negative,negative,negative,negative,negative
927692457,"It's been some time since this issue has been closed but it seems to me that the problem or a similar one is still there. For the abovementioned snippet: 

```import torch
import numpy as np
import albumentations as albu

albu_shadow = albu.RandomShadow(p=1, num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1))

x = torch.rand(1,3,24,24)
x_np = x[0, :, :, :].permute([1, 2, 0]).numpy() * 255
x_np = albu_shadow(image=x_np.astype(np.uint8))['image']
```
I receive the following output:
```
Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    x_np = albu_shadow(image=x_np.astype(np.uint8))['image']
  File ""/home/michal/miniconda3/envs/test/lib/python3.8/site-packages/albumentations/core/transforms_interface.py"", line 90, in __call__
    return self.apply_with_params(params, **kwargs)
  File ""/home/michal/miniconda3/envs/test/lib/python3.8/site-packages/albumentations/core/transforms_interface.py"", line 103, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""/home/michal/miniconda3/envs/test/lib/python3.8/site-packages/albumentations/augmentations/transforms.py"", line 1381, in apply
    return F.add_shadow(image, vertices_list)
  File ""/home/michal/miniconda3/envs/test/lib/python3.8/site-packages/albumentations/augmentations/functional.py"", line 54, in wrapped_function
    result = func(img, *args, **kwargs)
  File ""/home/michal/miniconda3/envs/test/lib/python3.8/site-packages/albumentations/augmentations/functional.py"", line 915, in add_shadow
    cv2.fillPoly(mask, vertices, 255)
cv2.error: OpenCV(4.5.3) :-1: error: (-5:Bad argument) in function 'fillPoly'
> Overload resolution failed:
>  - Layout of the output array img is incompatible with cv::Mat (step[ndims-1] != elemsize or step[1] != elemsize*nchannels)
>  - Expected Ptr<cv::UMat> for argument 'img'

```

I have the following versions of libraries:
albumentations            1.0.3
numpy                     1.21.2
pytorch                   1.9.1
opencv-python             4.5.3.56

Anybody can help?",time since issue closed problem similar one still snippet import torch import import receive following output recent call last file line module file line return file line key file line apply return image file line result file line mask vertex error bad argument function overload resolution layout output array incompatible step step argument following anybody help,issue,negative,negative,neutral,neutral,negative,negative
927106238,"> In the pipeline, use `bbox_params=A.BboxParams(format='pascal_voc'))`

I had bboxes with cocojson formats but format=""pascal_voc"" worked fine for me. Thanks",pipeline use worked fine thanks,issue,positive,positive,positive,positive,positive,positive
926352142,you cannot do that， the keypoints augmentation is total different from polygons，the number of polygon vertex points maybe increase or decrease while some part of polygon is outside the new image,augmentation total different number polygon vertex maybe increase decrease part polygon outside new image,issue,negative,positive,neutral,neutral,positive,positive
923903470,I have the same problem. Hope to see an update on this issue,problem hope see update issue,issue,negative,neutral,neutral,neutral,neutral,neutral
922561030,"Examples

```python
import random
import numpy as np
import albumentations as A
import matplotlib.pyplot as plt


def check(img: np.ndarray, templates: list, img_weight: float, template_weight: float) -> None:
    t = A.Compose(
        [
            A.Resize(512, 512),
            A.TemplateTransform(
                templates,
                img_weight=img_weight,
                template_weight=template_weight,
                template_transform=A.Compose([A.Resize(512, 512)]),
                p=1,
            ),
        ]
    )

    res = t(image=img)[""image""]

    plt.figure(figsize=(15, 15))
    plt.subplot(221, title=""original"")
    plt.imshow(img, vmin=0, vmax=255)
    plt.subplot(222, title=""template"")
    plt.imshow(templates[0], vmin=0, vmax=255)
    plt.subplot(212, title=f""img_weight={img_weight} template_weight={template_weight}"")
    plt.imshow(res, vmin=0, vmax=255)
    plt.show()


seed = 0
random.seed(seed)
np.random.seed(seed)

img = A.read_rgb_image(""/home/dipetm/github/albumentations_examples/images/original_parrot.jpg"")
templates = [A.read_rgb_image(""/home/dipetm/github/albumentations_examples/notebooks/images/image_1.jpg"")]

check(img, templates, 0.5, 0.5)
check(img, templates, 0.3, 0.7)
check(img, templates, 0.7, 0.3)
check(img, templates, 1, 1)
check(img, templates, 1, 2)
check(img, templates, 2, 1)
```

![image](https://user-images.githubusercontent.com/7512250/133947365-6cba891b-4537-4d97-8b84-5ac9ce908d1d.png)
![image](https://user-images.githubusercontent.com/7512250/133947372-23971206-29fe-4780-99da-03259d3bfb22.png)
![image](https://user-images.githubusercontent.com/7512250/133947375-7f876bff-a86d-4837-891a-e42b41e5093d.png)
![image](https://user-images.githubusercontent.com/7512250/133947377-29420441-5e01-4aad-9245-06889a6e614b.png)
![image](https://user-images.githubusercontent.com/7512250/133947378-dc3963c0-400f-422f-be38-6913a1fddfa2.png)
![image](https://user-images.githubusercontent.com/7512250/133947382-f7cc82ff-6341-49be-b5c3-3ca2d4078cbd.png)
",python import random import import import check list float float none image original template seed seed seed check check check check check check image image image image image image,issue,negative,negative,neutral,neutral,negative,negative
922480472,"Forget about it. I should have worked with `uint8` image from the beginning, and convert to `float32` when normalizing the image. Sorry",forget worked image beginning convert float image sorry,issue,negative,negative,negative,negative,negative,negative
919513774,"thank you.

On Tue, Sep 14, 2021 at 1:53 PM Mikhail Druzhinin ***@***.***>
wrote:

> The current versions of the library now contain these transforms.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/853#issuecomment-919504306>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAJHBLEQNPPD2CDCLLPSEX3UB6Y4HANCNFSM4ZACW2FQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>


-- 
Ben Weinstein, Ph.D.
Postdoctoral Fellow
University of Florida
http://benweinstein.weebly.com/
",thank tue wrote current library contain reply directly view triage go mobile android ben postdoctoral fellow university,issue,negative,positive,neutral,neutral,positive,positive
919502333,"No, this line works correctly.
This is a documentation problem.
In this case we need to change description into this ` That **the same range** will be used for both x- and y-axis.`
And I think we need to add the `keep_ratio` argument to keep the aspect ratio of the image.",line work correctly documentation problem case need change description range used think need add argument keep aspect ratio image,issue,negative,neutral,neutral,neutral,neutral,neutral
919496552,"Just add a `albumentations.ToTensorV2` as the last transform into your pipeline to switch channel shape.
https://albumentations.ai/docs/api_reference/pytorch/transforms/#albumentations.pytorch.transforms.ToTensorV2",add last transform pipeline switch channel shape,issue,negative,neutral,neutral,neutral,neutral,neutral
919495118,Please pull the recent master branch and resolve conflicts,please pull recent master branch resolve,issue,positive,neutral,neutral,neutral,neutral,neutral
919175348,"@VirajBagal 
>  If you don't mind, shall I close this PR, recreate my fork, make all changes mentioned here and then raise a new PR?

Yes, lets try it.",mind shall close recreate fork make raise new yes try,issue,negative,positive,positive,positive,positive,positive
918411199,"Our fault, we got distracted and forgot about your PRs. :(

@akarsakov Could you please rebase PRs on the latest master and we will work on merging them.",fault got distracted forgot could please rebase latest master work,issue,negative,positive,positive,positive,positive,positive
917991004,@p4rallax The code is finished (please see PRs mentioned in this issue) but it haven't merged to master yet.,code finished please see issue master yet,issue,negative,neutral,neutral,neutral,neutral,neutral
917616907,"Just as a follow up, when I use:

```
traindir=/some/dir/with/images'
trainset = datasets.ImageFolder(traindir,transform=Transforms(transforms=A.Resize(32 , 32)))
```

I get:

```
RuntimeError: Given groups=1, weight of size [16, 3, 3, 3], expected input[1024, 32, 32, 3] to have 3 channels, but got 32 channels instead
```",follow use get given weight size input got instead,issue,negative,neutral,neutral,neutral,neutral,neutral
917336007,"@creafz  @Dipet  I `black` formatted all files, so we see many file changes.  May be, these many file changes is the reason that the checks aren't running. Moreover, my forked repository is a mess right now (sorry, this is my first PR). If you don't mind, shall I close this PR, recreate my fork, make all changes mentioned here and then raise a new PR?  ",black see many file may many file reason running moreover forked repository mess right sorry first mind shall close recreate fork make raise new,issue,negative,positive,positive,positive,positive,positive
917133911,"@VirajBagal For some strange reason GitHub doesn’t run checks for your latest commit https://github.com/albumentations-team/albumentations/pull/1005/commits/46d00af47b77ea9ad1ab199794d918f372a914e6

I will look into it. 

By the way this commit looks quite unusual because it suddenly reformatted many files.",strange reason run latest commit look way commit quite unusual suddenly many,issue,negative,positive,positive,positive,positive,positive
915841377,"Try this example:
```python
import random
import numpy as np
import albumentations as A

img = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
bboxes = [
    [0, 0, 10, 10, 0],
    [30, 30, 40, 40, 0],
    [70, 70, 99, 99, 0]
]

transforms = A.Compose([A.RandomCropNearBBox()], bbox_params=A.BboxParams(""pascal_voc""))
res = transforms(image=img, bboxes=bboxes, cropping_bbox=random.choice(bboxes))
print(res[""image""].shape, bboxes)
```",try example python import random import import print image,issue,negative,negative,negative,negative,negative,negative
914523419,"I added a method `check_range` to `CoarseDropout`. It checks the range if the given values are float. If not satisfied, it raises `ValueError`. ",added method range given float satisfied,issue,negative,positive,positive,positive,positive,positive
914488382,"I think that there should also be a check that verifies that all float values lie in the range `[0.0, 1.0)` to prevent errors when users pass a value in pixels but accidentally use incorrect datatype for it (e.g., 40.0).

",think also check float lie range prevent pas value accidentally use incorrect,issue,negative,neutral,neutral,neutral,neutral,neutral
914051009,@Dipet thanks for the guidance. Made the changes. Please take a look.,thanks guidance made please take look,issue,positive,positive,positive,positive,positive,positive
914011847,"I think is better to skip `_check_args` in inner transforms.
To do this we must check all transforms and if the transform is inherited from `Compose` to disable this check.
Something like this:
```python
class Compose:
    def __init__(...):
        ....
        self.is_check_args = True
        self._disable_check_args_for_transforms(self.transforms.transforms)
    
    @staticmethod
    def _disable_check_args_for_transforms(transforms: typing.List[typing.Union[BaseCompose, BasicTransform]]):
        for transform in transforms:
            if isinstance(transform, BaseCompose):
                Compose._disable_check_args_for_transforms(transform.transorms.transforms)
            if isinstance(transform, Compose):
                transform._disable_check_args()

    def _disable_check_args(self):
        self.is_check_args = False

    def __call__(self, *args, force_apply=False, **data):
        if self.is_check_args:
            self._check_args(**data)
```",think better skip inner must check transform compose disable check something like python class compose true transform transform transform compose self false self data data,issue,positive,positive,positive,positive,positive,positive
913985932,"`test_transform_pipeline_serialization_with_bboxes` has this example of augmentation: 
```python    
aug = A.Compose(
        [
            A.OneOrOther(
                A.Compose([A.RandomRotate90(), A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)])]),
                A.Compose([A.Rotate(p=0.5), A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1)]),
            ),
            A.SomeOf(
                [
                    A.HorizontalFlip(p=1),
                    A.Transpose(p=1),
                    A.HueSaturationValue(p=0.5),
                    A.RandomBrightnessContrast(p=0.5),
                ],
                n=5,
            ),
        ],
        bbox_params={""format"": bbox_format, ""label_fields"": [""labels""]},
    )
```

This augmentation raises error because `A.Compose` within `A.OneOrOther` doesn't have the `bbox_params` defined. 

We have two options now:
1.  Either we make the user enter `bbox_params` wherever he/she uses `A.Compose`. We would then need to make these changes in every test where `A.OneOrOther` and bboxes are used. 
2.  Or we come up with a logic to use `bbox_params` defined in the outermost `A.Compose`  and use the same in the inner `A.Compose` of `A.OneOrOther`. 

What do you think? 

I tried the test with this and it works: 

```python
aug = A.Compose(
    [
        A.OneOrOther(
            A.Compose([A.RandomRotate90(), A.OneOf([A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5)])], bbox_params={""format"": bbox_format, ""label_fields"": [""labels""]}),
            A.Compose([A.Rotate(p=0.5), A.OneOf([A.HueSaturationValue(p=0.5), A.RGBShift(p=0.7)], p=1)], bbox_params={""format"": bbox_format, ""label_fields"": [""labels""]}),
        ),
        A.SomeOf(
            [
                A.HorizontalFlip(p=1),
                A.Transpose(p=1),
                A.HueSaturationValue(p=0.5),
                A.RandomBrightnessContrast(p=0.5),
            ],
            n=5,
        ),
    ],
    bbox_params={""format"": bbox_format, ""label_fields"": [""labels""]},
)
```",example augmentation python format augmentation error within defined two either make user enter wherever would need make every test used come logic use defined outermost use inner think tried test work python format format format,issue,negative,neutral,neutral,neutral,neutral,neutral
913732716,"Tests found problems when we combine `Compose`, `OneOf`, or another such transform.
For example look to `test_transform_pipeline_serialization_with_bboxes`.

We need to add some logic to disable `_check_args` for all transforms inherited from `BaseCompose`",found combine compose another transform example look need add logic disable,issue,negative,neutral,neutral,neutral,neutral,neutral
913667019,"> Please, also add this test into `tests/test_core`
> 
> ```python
> def test_bbox_params_is_not_set(image, bboxes):
>     t = Compose([])
>     with pytest.raises(ValueError) as exc_info:
>         t(image=image, bboxes=bboxes)
>     assert str(exc_info.value) == ""bbox_params must be specified for bbox transformations""
> ```


Added the mentioned test. ",please also add test python image compose assert must added test,issue,negative,neutral,neutral,neutral,neutral,neutral
913641945,"Please, also add this test into `tests/test_core`

```python
def test_bbox_params_is_not_set(image, bboxes):
    t = Compose([])
    with pytest.raises(ValueError) as exc_info:
        t(image=image, bboxes=bboxes)
    assert str(exc_info.value) == ""bbox_params must be specified for bbox transformations""
```",please also add test python image compose assert must,issue,negative,neutral,neutral,neutral,neutral,neutral
912870890,"Thanks @maxfrei750, I think I got the idea. You are basically getting the data that was transformed and has a valid mask and then reconstructing the bounding boxes. Sounds like a good hotfix.",thanks think got idea basically getting data valid mask bounding like good,issue,positive,positive,positive,positive,positive,positive
912853004,"@guimaraesmarcelo I did the following:

Assume that `transform` is your albumentations transform, `image` is your image, and `masks` are your masks.

```
transformed_data = transform(image=image, masks=masks)

image = transformed_data[""image""]
masks = transformed_data[""masks""]

is_not_empty = [np.any(mask) for mask in masks]

masks = list(compress(masks, is_not_empty))
labels = list(compress(labels, is_not_empty))
scores = list(compress(scores, is_not_empty))

boxes = [extract_bounding_box(mask) for mask in masks]
```

With
```
def extract_bounding_box(mask) -> np.ndarray:
    """"""Extract the bounding box of a mask.
    :param mask: HxW numpy array
    :return: bounding box
    """"""
    pos = np.where(mask)

    if not (pos[0].size or pos[1].size):
        return np.array([0, 0, 0, 0])

    xmin = np.min(pos[1])
    xmax = np.max(pos[1]) + 1
    ymin = np.min(pos[0])
    ymax = np.max(pos[0]) + 1
    return np.array([xmin, ymin, xmax, ymax])
```

I hope that helps. For the complete code, please refer to: https://github.com/maxfrei750/paddle/blob/0b36b9e1543614f1800da297ccc54cf10b14f927/paddle/data/sample.py#L49",following assume transform transform image image transform image image mask mask list compress list compress list compress mask mask mask extract bounding box mask param mask array return bounding box mask return return hope complete code please refer,issue,positive,positive,neutral,neutral,positive,positive
912849093,"Hi @BloodAxe, I faced the same problem while training an instance segmentation model.  Would any solution be available yet?

In my case, I noticed that when I get empty masks, the number of empty masks is higher than the number of removed bounding boxes labels.",hi faced problem training instance segmentation model would solution available yet case get empty number empty higher number removed bounding,issue,negative,positive,positive,positive,positive,positive
911608101,"`RandomScale + Resize == NoOp ?` - nope, it's more like [Downscale](https://albumentations.ai/docs/api_reference/full_reference/#albumentations.augmentations.transforms.Downscale)
`RandomScale + Crop == RandomResizedCrop ?` - yes, looks the same.",resize noop nope like crop yes,issue,negative,neutral,neutral,neutral,neutral,neutral
911503716,"RandomScale resizes an image by a randomly selected factor.
To get a batch of images with the same image shapes try to add after `RandomScale` `Resize` transform or some crop transform.",image randomly selected factor get batch image try add resize transform crop transform,issue,negative,negative,negative,negative,negative,negative
909231432,"> Hey @glenn-jocher
> 
> We have released the [`1.0.3` version](https://github.com/albumentations-team/albumentations/releases/tag/1.0.3) that should deal with the numerical stability issue on our side.
version1.0.3  still raise the same error:   (-__-) Expected x_max for bbox (0.7486702127659575, 0.18541666666666667, 1.0053191489361701, 0.65, 0.0) to be in the range [0.0, 1.0], got 1.0053191489361701",hey version deal numerical stability issue side version still raise error range got,issue,negative,neutral,neutral,neutral,neutral,neutral
909207732,"The two issues are *not* duplicates, please note the distortion issue in #1001. ",two please note distortion issue,issue,negative,neutral,neutral,neutral,neutral,neutral
909186251,"It is because `Perspective` uses `np.random`. This behaviur will be changed in the future.
At this time, you must also set `np.random.seed(seed)` for reproducibility.",perspective future time must also set seed reproducibility,issue,negative,neutral,neutral,neutral,neutral,neutral
909040999,"Yeah you're right, it's just that in my case I also have segmentation masks so I know if some targets are removed or not. I didn't realize that my situation was very particular. 
Thanks !",yeah right case also segmentation know removed realize situation particular thanks,issue,positive,positive,positive,positive,positive,positive
908503787,"Yes, you assumptions are correct.
If you do cropping, some information can get lost and multi-label targets
may become invalid. There is no way to get correct labels using only
image-level targets as input.

Пн, 30 авг. 2021 г. в 19:09, Mordokkai ***@***.***>:

> Hi!
>
> I used albumentations for multi-target segmentation before without
> problems. Now I'm moving to multi-target classification, but I don't get
> the result that I want. From what I understood of albumentations, when you
> do augmentations for classification, only the input image is modified and
> not the target vector. This may be a problem for certain transform like
> Cropping, that may remove a target class from the image, while keeping the
> target vector intact. Did I miss something or is it a normal behavior ?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/999>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YADKXGKPVGUTVREF7DT7OUMVANCNFSM5DCGIDEQ>
> .
>
",yes correct information get lost may become invalid way get correct input hi used segmentation without moving classification get result want understood classification input image target vector may problem certain transform like may remove target class image keeping target vector intact miss something normal behavior thread reply directly view,issue,negative,positive,positive,positive,positive,positive
906132623,I guess what makes it difficult is that you would need to implement an augmentation that takes multiple images as input. Is that even possible in Albumentations?,guess difficult would need implement augmentation multiple input even possible,issue,negative,negative,negative,negative,negative,negative
905353344,"In many cases, you use `Any` type, but we can determine the exact type of functions.
Please, debug all the functions and check exactly what types they use.
You could start by debugging this code:
```python
import numpy as np
import albumentations as A


img = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
bboxes = [[0, 0, 10, 10, 1]]
keypoints = [[0, 0]]
mask = img

t = A.Compose([A.Resize(256, 256), A.Normalize()], bbox_params=A.BboxParams(""pascal_voc""), keypoint_params=A.KeypointParams(""xy""))
res = t(image=img, mask=mask, keypoints = keypoints, bboxes=bboxes)

```",many use type determine exact type please check exactly use could start code python import import mask,issue,negative,positive,positive,positive,positive,positive
904645014,"Hi @Dipet, who else can review this?
Should I add anything else before merging?",hi else review add anything else,issue,negative,neutral,neutral,neutral,neutral,neutral
903243553,"Yes, needed to add standard python type hints. As example #988 ",yes add standard python type example,issue,negative,neutral,neutral,neutral,neutral,neutral
903233074,@onlinehuazai Do you have an example of the code to reproduce the issue?,example code reproduce issue,issue,negative,neutral,neutral,neutral,neutral,neutral
903136493,"@HasnainRaz Could you please change the return type for `get_transform_init_args_names` (`resize.py` lines 99 and 148) from `Sequence[str]` to `Tuple[str, ...]`, so it will be compatible with the return type for the base class `BasicTransform` and mypy won't raise errors such as this - https://github.com/albumentations-team/albumentations/pull/930/checks?check_run_id=3389742084",could please change return type sequence compatible return type base class wo raise,issue,negative,negative,negative,negative,negative,negative
903083026,Does this mean to add type hints/ function annotations?,mean add type function,issue,negative,negative,negative,negative,negative,negative
902809722,"Would be cool to have this merged soon, is it possible to get it reviewed @ternaus @BloodAxe @creafz sometime soon possibly? Thanks",would cool soon possible get sometime soon possibly thanks,issue,positive,positive,positive,positive,positive,positive
900529910,"Can I try to fix it?

On Tue, Aug 17, 2021, 10:20 PM Mikhail Druzhinin ***@***.***>
wrote:

> Unfortunately, this issue has not been fixed yet.
>
> —
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/850#issuecomment-900461886>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ALPCLOOA4RNVQB7XAKMYJULT5KHNJANCNFSM4YTPK54A>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&utm_campaign=notification-email>
> .
>
",try fix tue wrote unfortunately issue fixed yet reply directly view triage go mobile android,issue,negative,negative,negative,negative,negative,negative
900461886,"Unfortunately, this issue has not been fixed yet. ",unfortunately issue fixed yet,issue,negative,negative,negative,negative,negative,negative
897711751,"No, we have no these functionality. Try to add `PadIfNeeded(468, 468, border_mode=cv2.BORDER_CONSTANT, value=0)` before copping, for example.",functionality try add copping example,issue,negative,neutral,neutral,neutral,neutral,neutral
897648230,"> @Dipet oh got it, ok! The main thing i was worried about was the extra cost in reversing a full image transform, but if only the keypoints are reversed and not the full images that's perfect.

Yes, only provided data will be reversed. Perhaps I will remove the `reverse_image` and leave only the `reverse_mask`. ",oh got main thing worried extra cost reversing full image transform reversed full perfect yes provided data reversed perhaps remove leave,issue,positive,positive,positive,positive,positive,positive
897644387,"@Dipet oh got it, ok! The main thing i was worried about was the extra cost in reversing a full image transform, but if only the keypoints are reversed and not the full images that's perfect.",oh got main thing worried extra cost reversing full image transform reversed full perfect,issue,negative,positive,positive,positive,positive,positive
897634267,"At the moment, this is just a blank. 

> For the TTA use case we do not need to reverse the images, only the boxes/keypoints. What's the use case for reversing image transforms? Aren't many image transforms non-reversible in any case?

We usually use the same function for segmentation masks, this function is only needed for them.",moment blank use case need reverse use case reversing image many image case usually use function segmentation function,issue,negative,positive,neutral,neutral,positive,positive
897629027,"@Dipet thanks for the PR! This would be super useful. 

For the TTA use case we do not need to reverse the images, only the boxes/keypoints. What's the use case for reversing image transforms? Aren't many image transforms non-reversible in any case?

",thanks would super useful use case need reverse use case reversing image many image case,issue,positive,positive,positive,positive,positive,positive
897487695,"Hi! I started some work on TTA, you can see how it might look in this PR: #974 
I will try to speed up the implementation of this function.",hi work see might look try speed implementation function,issue,negative,neutral,neutral,neutral,neutral,neutral
897038748,"Clipping bbox coordinates is like hiding the real issue under the carpet. I believe this should NOT be merged. 
Usually, user can see related error for two reasons:
1) There is an implementation bug in albumentations, which should be addressed and fixed at it's root cause.
2) There are errors in user-provided data, which should be also fixed, but on the user side.

Currently we  have mechanism for checking validity of bboxes for pascal voc, coco & yolo formats. These checks allows to spot issues quickly. Masking them is a bad design decision.",clipping like real issue carpet believe usually user see related error two implementation bug fixed root data also fixed user side currently mechanism validity coco spot quickly bad design decision,issue,negative,negative,neutral,neutral,negative,negative
895769666,It is based on your xml data format. Library returns a common list of boxes in described format. You could write them by yourself to any format that you want using another libraries.,based data format library common list format could write format want another,issue,negative,negative,negative,negative,negative,negative
895730529,"Please feel free to contribute!

Пн, 9 авг. 2021 г. в 23:07, Marijan Smetko ***@***.***>:

> Can I be assigned to this?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/981#issuecomment-895505654>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YEYTRAO7U7G6IVJC4LT4AYP7ANCNFSM5B2VPV5Q>
> .
>
",please feel free contribute assigned thread reply directly view,issue,positive,positive,positive,positive,positive,positive
894828353,"By the way, this kind of mistakes in code within docs could be avoided by using the standard library's module `doctest`. The code in docs would need some slight stylistic editing to accommodate for `doctest` though.",way kind code within could standard library module code would need slight stylistic accommodate though,issue,positive,positive,positive,positive,positive,positive
894694906,"Typo `tramsform` and `transform`:
```python
tramsform = A.Compose([transform], 
                      bbox_params=A.BboxParams(bbox_style),                                  
                     )
transformed = transform(image=im_origin, bboxes=bboxes)
```",typo transform python transform transform,issue,negative,neutral,neutral,neutral,neutral,neutral
890800859,Looks like a mistake and better to add this argument to transform,like mistake better add argument transform,issue,negative,positive,positive,positive,positive,positive
889191004,1.0.3 worked fine for me would you mind sharing your bbox inputs,worked fine would mind,issue,negative,positive,positive,positive,positive,positive
888890942,"Just use [Normalize](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.Normalize) transform:
```python
A.Normalize(mean=0, std=1)
```",use normalize transform python,issue,negative,neutral,neutral,neutral,neutral,neutral
888810108,Is someone working on this issue ? Can I pick this up and create a PR for this request ?,someone working issue pick create request,issue,negative,neutral,neutral,neutral,neutral,neutral
888119493,"Just read the entire text, not one sentence: 

> We first divide all pixel values of an image by 255, so each pixel's value will lie in a range [0.0, 1.0]. **Then we will subtract mean pixel values and divide values by the standard deviation.**

Normalization does these operations: 
```
img = (img - mean * max_pixel_value) / (std * max_pixel_value)
```

That's equivalent to this:
```
img = (img / max_pixel_value - mean) / std
```",read entire text one sentence first divide image value lie range subtract mean divide standard deviation normalization mean equivalent mean,issue,negative,negative,negative,negative,negative,negative
887658192,"[The documentation](https://albumentations.ai/docs/examples/migrating_from_torchvision_to_albumentations/) still refers to `ShiftScaleRotate`, which is what prompted my original question. Should this be opened as a new issue on the documentation Github?",documentation still original question new issue documentation,issue,negative,positive,positive,positive,positive,positive
887495143,"> Yes, we just forgot to close this issue.
> Our [Affine](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine) transform provides the same functionality.
> If you think this is not true, please give a reproducible example.

it worked perfectly, thank!",yes forgot close issue affine transform functionality think true please give reproducible example worked perfectly thank,issue,positive,positive,positive,positive,positive,positive
887489037,"Yes, we just forgot to close this issue. 
Our [Affine](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine) transform provides the same functionality.
If you think this is not true, please give a reproducible example. ",yes forgot close issue affine transform functionality think true please give reproducible example,issue,positive,positive,positive,positive,positive,positive
887479971,"> Have you tried [Affine](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine) transform?

yes thank you, I just tried A.Affine and it worked perfectly, At first I thought it is not for normal images because of the geometric class and inavailibility on Demo page. I think this issue should be closed now?",tried affine transform yes thank tried worked perfectly first thought normal geometric class page think issue closed,issue,positive,positive,positive,positive,positive,positive
886216763,"That command to install doesn't work.

ERROR: Could not find a version that satisfies the requirement albumentations-beta (from versions: none)
ERROR: No matching distribution found for albumentations-beta",command install work error could find version requirement none error matching distribution found,issue,negative,neutral,neutral,neutral,neutral,neutral
886106672,Yeah there is an issue in the bbox coordinates. There are variations in the image size and bboxes are drawn at a particular resolution. So there was a mismatch and that was causing an issue. Closing this issue! ,yeah issue image size drawn particular resolution mismatch causing issue issue,issue,negative,positive,positive,positive,positive,positive
885998185,"!pip install git+https://github.com/albumentations-team/albumentations.git

its working fine now try installing with this",pip install working fine try,issue,negative,positive,positive,positive,positive,positive
882537754,"> What is the data type of your input image?
Thanks for your attention. It turned out that this function was not built for float32 data and casting them to 'uint8' solved it perfectly.
",data type input image thanks attention turned function built float data casting perfectly,issue,positive,positive,positive,positive,positive,positive
882531830,What is the data type of your input image?,data type input image,issue,negative,neutral,neutral,neutral,neutral,neutral
882374537,"String from docs:

> Normalization is applied by the formula: img = (img - mean * max_pixel_value) / (std * max_pixel_value)

Just move the division by `max_pixel_value` into parentheses:
`(img - mean * max_pixel_value) / (std * max_pixel_value) = (img / max_pixel_value - mean) / std`",string normalization applied formula mean move division parenthesis mean mean,issue,negative,negative,negative,negative,negative,negative
881359491,"I would say that the majority, if not all of the augmentations will work for grayscale images if your treat them as RGB with 3 channels obtained from the grayscale image, say with `rgb_image = cv2.cvtColor(grayscale_image, cv2.COLOR_GRAY2RRGB)`

You may also check machine learning competitions at Kaggle where participants worked with the medical or other grayscale images.

P.S. I would guess, that you may get more detailed advice at Stackoverflow.",would say majority work treat image say may also check machine learning worked medical would guess may get detailed advice,issue,negative,positive,positive,positive,positive,positive
881206968,"Just print image shape and bboxes before sending them into albumentations pipeline.
It looks like you are using the wrong bboxes",print image shape sending pipeline like wrong,issue,negative,negative,negative,negative,negative,negative
880987543,"I did little analysis of a code and found this. 

I changed `albumentations/albumentations/bbox_utils.py` line 253-256 this way. 
```
    ....
    bbox_final = (x_min, y_min, x_max, y_max) + tail

    if source_format != ""yolo"":
        bbox_final = normalize_bbox(bbox_final, rows, cols)
    if check_validity:
        try:
            check_bbox(bbox_final)
        except Exception as ex:
            print(f""Original Bbox: {bbox} Transformed Bbox : {bbox_final} cols: {cols} rows: {rows}"")
            raise ValueError(f""Error: {ex}"")
    return bbox_final
```
What I got is this, 
```
    Original Bbox: [670, 0, 842, 114, 0] Transformed Bbox : (2.9910714285714284, 0.0, 3.7589285714285716, 0.5089285714285714, 0) cols: 224 rows: 224
```
Now I found an coordinates belongs to an image with a size `""width"": 1280, ""height"": 720 ` instead I am getting w=224, h=224. 
",little analysis code found line way tail try except exception ex print original raise error ex return got original found image size width height instead getting,issue,negative,positive,positive,positive,positive,positive
880943761,"Are you sure that you are using the correct bboxes?
Could you please provide a minimal reproducible example or at least the shape of the image and bbxoxes you caught this error on?",sure correct could please provide minimal reproducible example least shape image caught error,issue,negative,positive,neutral,neutral,positive,positive
880889578,"Hey @glenn-jocher 

We have released the [`1.0.3` version](https://github.com/albumentations-team/albumentations/releases/tag/1.0.3) that should deal with the numerical stability issue on our side.",hey version deal numerical stability issue side,issue,negative,neutral,neutral,neutral,neutral,neutral
880476464,I think to merge #958 after this PR and push version into PYPI,think merge push version,issue,negative,neutral,neutral,neutral,neutral,neutral
880070919,"Hey @arsenyinfo 

Thanks for your contribution. 

Currently, `qudida` has `opencv-python` as a dependency, but albumentations' wheel requires `opencv-python-headless`. So if we merge the current PR albumentations will start to download and install both `opencv-python-headless` (it own dependency) and `opencv-python` (qudida's dependency) into a single Python environment.

The simplest way to fix it, in my opinion, is to slightly modify the logic in qudida's `setup.py` and configure a CI pipeline to upload to PyPI 2 versions of the library:
1. `qudida`, which will be the current version that depends on `opencv-python`.
2. `qudida-headless`, which has `opencv-python-headless` as a dependency (and not `opencv-python`).

Then we will add `qudida-headless` as a dependency for albumentations.

If you want, I can contribute a PR with these changes to `qudida`",hey thanks contribution currently dependency wheel merge current start install dependency dependency single python environment way fix opinion slightly modify logic configure pipeline library current version dependency add dependency want contribute,issue,negative,negative,neutral,neutral,negative,negative
879517665,"> Can you please elaborate on what the bug actually about and case to reproduce it?

It seems that the rescaling of the bbox is necessary. Please refer to the following code.

`random.seed(1)

image = np.ones((200,200,3), dtype=np.uint8) * 255
bboxes = np.array([[195  ,   195 , 199  ,  199  ]],
      dtype=np.float32)
class_labels = np.zeros((bboxes.shape[0],), dtype=np.int32)
catted_boxes = np.concatenate((bboxes, class_labels.reshape(-1,1)), axis=1)

transform = A.Compose([
    A.SafeRotate(limit=10, border_mode=0,value=0 ,p=1),
], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.0), p=1)

transformed = transform(image=image, bboxes=catted_boxes)
transformed_image = transformed['image']

bboxes = transformed['bboxes']
assert len(bboxes) == catted_boxes.shape[0]`

![image](https://user-images.githubusercontent.com/52908171/125546800-84789e47-a13d-46b7-a34f-1395b1009fef.png)
![image](https://user-images.githubusercontent.com/52908171/125546848-8bae3ab8-ee5c-4362-97dc-13dac199dfb6.png)
![image](https://user-images.githubusercontent.com/52908171/125546890-3894ff51-c884-4267-b8d6-f2b6d4ca3d6c.png)
",please elaborate bug actually case reproduce necessary please refer following code image transform transform assert image image image,issue,positive,positive,positive,positive,positive,positive
879376810,"Can you attach a stack trace, input bboxes and image shape please?

Вт, 13 июля 2021 г. в 21:13, nordmannnick ***@***.***>:

> I have issues if I augment an image with settings of:
>
> transform = A.Compose( [A.SafeRotate(limit=45, p=1,
> border_mode=cv2.BORDER_CONSTANT),
> box_params=A.BboxParams(format='albumentations',
> label_fields=['gt_labels']) )
>
> I have spent quite a while tracking down the behaviour and hopefully it's
> an easy fix.
> I am using 1.0.2 installed via PyCharm's package manager (a pip backend).
> The bug is also present in 1.0.1, 1.0.0 and 0.5.2 (I gave up testing
> further back after those).
> I can successfully use HorizontalFlip, VerticalFlip and
> RandomSizedBBoxSafeCrop with no issues using the same surrounding code.
>
> I have edited augmentations/geometric/functional.py as per Yun's
> screenshot and it does not seem to solve the issue for me.
> Let me know if you need more information.
>
> —
> You are receiving this because you commented.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/961#issuecomment-879297438>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YHQPNPFTJU5CV5Y45DTXR623ANCNFSM5AILKWUA>
> .
>
",attach stack trace input image shape please augment image transform spent quite behaviour hopefully easy fix via package manager pip bug also present gave testing back successfully use surrounding code per seem solve issue let know need information reply directly view,issue,positive,positive,positive,positive,positive,positive
879297438,"I have issues if I augment an image with settings of:

`transform = A.Compose( [A.SafeRotate(limit=45, p=1, border_mode=cv2.BORDER_CONSTANT), box_params=A.BboxParams(format='albumentations', label_fields=['gt_labels']) )`

I have spent quite a while tracking down the behaviour and hopefully it's an easy fix.
I am using 1.0.2 installed via PyCharm's package manager (a pip backend). The bug is also present in 1.0.1, 1.0.0 and 0.5.2 (I gave up testing further back after those).
I can successfully use HorizontalFlip, VerticalFlip and RandomSizedBBoxSafeCrop with no issues using the same surrounding code.

I have edited augmentations/geometric/functional.py as per Yun's screenshot and it does not seem to solve the issue for me.
Let me know if you need more information.",augment image transform spent quite behaviour hopefully easy fix via package manager pip bug also present gave testing back successfully use surrounding code per seem solve issue let know need information,issue,positive,positive,positive,positive,positive,positive
878962216,Can you please elaborate on what the bug actually about and case to reproduce it?,please elaborate bug actually case reproduce,issue,negative,positive,positive,positive,positive,positive
878332644,"When `p1=1` `OneOf` block will never be applied. And you will have an flipped image horizontally and vertically. 
When `p1=0` only one flip from `OneOf` will be sampled and applied.",block never applied image horizontally vertically one flip applied,issue,negative,neutral,neutral,neutral,neutral,neutral
878332300,"> You have this pipeline:
> 
> ```python
> import random
> import albumentations as A
> 
> p1 = random.random()
> pipeline = A.Compose(
>     [
>         A.OneOf([A.HorizontalFlip(), A.VerticalFlip()], p=1 - p1),
>         A.HorizontalFlip(p1),
>         A.VerticalFlip(p1)
>     ]
> )
> ```
> 
> This way, each transform will be sampled independently, except for transforms inside `OneOf`.
> Why do you think that at every step you will have transformed image?
> 
> Another problem, that you may apply `HorizontalFlip` inside `OneOf` and after that do the same transform and you will have as a result original image.

Thanks alot,that turns out to be a silly mistake......
To prevent the second problem I rounded p1 so that it can only be 0 or 1",pipeline python import random import pipeline way transform independently except inside think every step image another problem may apply inside transform result original image thanks turn silly mistake prevent second problem rounded,issue,negative,negative,neutral,neutral,negative,negative
878328391,"You have this pipeline:

```python
import random
import albumentations as A

p1 = random.random()
pipeline = A.Compose(
    [
        A.OneOf([A.HorizontalFlip(), A.VerticalFlip()], p=1 - p1),
        A.HorizontalFlip(p1),
        A.VerticalFlip(p1)
    ]
)
```

This way, each transform will be sampled independently, except for transforms inside `OneOf`. 
Why do you think that at every step you will have transformed image?

Another problem, that you may apply `HorizontalFlip` inside `OneOf` and after that do the same transform and you will have as a result original image.",pipeline python import random import pipeline way transform independently except inside think every step image another problem may apply inside transform result original image,issue,negative,negative,neutral,neutral,negative,negative
878054708,"Already updated the package, before publishing the issue, and the error is still present. ",already package issue error still present,issue,negative,neutral,neutral,neutral,neutral,neutral
877979206,"#958 

yet to be merged

I think this PR should be solving the issue but I am not sure of it Kindly check",yet think issue sure kindly check,issue,positive,positive,positive,positive,positive,positive
877670901,"@Dipet it seems like float32 loses precision on xyxy to xywh and back conversions resulting in the error.

I didn't find a good solution so I simply implemented a clip with epsilon in 'Numerical stability fix for Albumentations' PR ultralytics/yolov5#3958. This adds about 50 us of overhead per image unfortunately in the dataloader.

An alternative might be to implement an epsilon value in your Albumentation checks, i.e. for normalized data this might be something like:
```python
eps = 1E-6
assert -eps < x < (1 + eps)
```

rather than the current hard bounds:
```python
assert 0.0 < x < 1.0
```",like float precision back resulting error find good solution simply clip epsilon stability fix u overhead per image unfortunately alternative might implement epsilon value data might something like python assert rather current hard python assert,issue,positive,negative,neutral,neutral,negative,negative
877669481,"# RandomSunFlare transform

Works straight away for 1-channel, the check:
https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py#L848
could be just removed

I guess it could be a separate issue and PR.",transform work straight away check could removed guess could separate issue,issue,negative,positive,positive,positive,positive,positive
877664949,"# RandomShadow transform

FYI I did a quick hack locally to handle 1-channel images, which works fine for me.
It basically detects if it's grayscale and if so converts to rgb first. At the end convert back from rgb to grayscale.

So to give you a taste, I put here: https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py#L909

lines:
```
    is_single_channel = (len(img.shape) == 2)
    if is_single_channel:
        # this is a quick hack to handle grayscale, faster implementation would have a separate logic for 1 channel images
        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)
```

and then I replaced: 
https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/functional.py#L921-L924

with:
```
    image_out = cv2.cvtColor(image_hls, cv2.COLOR_HLS2RGB)
   
    if is_single_channel:
        image_out = cv2.cvtColor(image_hls, cv2.COLOR_RGB2GRAY)
        
    if needs_float:
        image_out = to_float(image_out, max_value=255)
```

I guess it could be handled better by implementing compeltaly separate logic for adding shadows on grayscale (other than using HLS channels), but I'm too lazy :P Let me know if the proposal above sounds acceptable to you and I can make a PR.",transform quick hack locally handle work fine basically first end convert back give taste put quick hack handle faster implementation would separate logic channel guess could handled better separate logic lazy let know proposal acceptable make,issue,positive,positive,positive,positive,positive,positive
877615148,"i suggest @Dipet  @coldfir3  @jyauri @narain1   above solution to fix this issue. 
",suggest solution fix issue,issue,negative,neutral,neutral,neutral,neutral,neutral
877614977,"Inside the latest check_bbox procedure in boxutils
one simply has to do     ,that is  add round upto  4 decimal places. this will resolve  the issue
`if  not  0 <= round(value,4) <=1 `",inside latest procedure one simply add round decimal resolve issue round value,issue,positive,positive,neutral,neutral,positive,positive
877610603,"> I modified all probabilities to 0.0, but somehow I am still seeing the same error from albumentations. Now I'm doubly confused about why the boxes are being examined when 1) all probabilities are 0.0, and 2 ) there are no spatial transforms present that would affect the boxes.

We have simplified our code and so bboxes and keypoints preprocessors will now be called regardless of whether dual transforms exist in the pipeline or not.",somehow still seeing error doubly confused spatial present would affect simplified code regardless whether dual exist pipeline,issue,negative,negative,negative,negative,negative,negative
877608681,"This looks like float numbers rounding error, because `np.isclose(1, 1.000000024214387)` returns `True`.

This check is incorrect because it is doesn't check real bbox coordinates. Albumentations uses internal bbox format - some kind of normalized pascal_voc.
To correctly check bbox you need do this steps:
```python
x, y, h, w = bbox
x1 = x - w / 2
x2 = x1 + w
y1 = y - h / 2
y2  = y1 + h

if x1 < 0 or x2 > 1 or y1 < 0 or y2 > 1:
    print(f""Bbbox greater than image: {[x1, y1, x2, y2]}"")
```",like float rounding error true check incorrect check real internal format kind correctly check need python print greater image,issue,positive,positive,positive,positive,positive,positive
877278668,"@Dipet @ternaus to double check that YOLO is not creating out of bounds labels somehow I inserted this code right before the albumentations call:

```python
        if self.augment:
            if np.any(labels[:, 1:5] > 1.0):  # bboxes > 1.0
                print(labels[:, 1:5])
            
            # Albumentations
            img, labels = self.albumentations(img, labels)
```

And during my training nothing is printed before the albumentations error. So I suspect that albumentations is performing some operations on the labels that are causing them to lose numerical precision, even though no augmentations are asked for (probabilities are all zeros), and even though no spatial transforms are called for (labels should never change).

Since I see this with v1.0.2, which appears to include this PR, I would say that this PR has not resolved the issue.

<img width=""1746"" alt=""Screenshot 2021-07-09 at 17 41 48"" src=""https://user-images.githubusercontent.com/26833433/125103908-18005000-e0dd-11eb-97a4-c64607b73db6.png"">
",double check somehow inserted code right call python print training nothing printed error suspect causing lose numerical precision even though even though spatial never change since see include would say resolved issue,issue,negative,positive,positive,positive,positive,positive
877273673,"@Dipet @ternaus I modified all probabilities to 0.0, but somehow I am still seeing the same error from albumentations. Now I'm doubly confused about why the boxes are being examined when 1) all probabilities are 0.0, and 2 ) there are no spatial transforms present that would affect the bboxes.
```python
            self.transform = A.Compose([
                A.CLAHE(p=0.0),
                A.RandomBrightnessContrast(p=0.0),
                A.RandomGamma(p=0.0),
                A.Blur(p=0.0),
                A.MedianBlur(p=0.0),
                A.ToGray(p=0.0),
                A.ImageCompression(quality_lower=75, p=0.0),
            ],
```",somehow still seeing error doubly confused spatial present would affect python,issue,negative,negative,negative,negative,negative,negative
877268541,"@Dipet @ternaus is this PR present in v1.0.2? I'm getting this error today using v1.0.2 when training with the new YOLOv5 integration, and I'm certain that none of the YOLOv5 labels are out of bounds, as we strictly check for this on dataloader init.

```
ValueError: Expected y_max for bbox (0.6265270002186298, 0.9481956716626883, 0.700299333781004, 1.000000024214387, 0.0) to be in the range [0.0, 1.0], got 1.000000024214387.
```

<img width=""1746"" alt=""Screenshot 2021-07-09 at 17 25 12"" src=""https://user-images.githubusercontent.com/26833433/125101717-d40c4b80-e0da-11eb-8b07-549374792a8d.png"">


EDIT: Another anomaly here is that I have no spatial transforms, so I don't see why the bounding boxes would be examined or checked/modified by Albumentations. The transforms I use are here:
```python
            self.transform = A.Compose([
                A.CLAHE(p=0.1),
                A.RandomBrightnessContrast(p=0.0),
                A.RandomGamma(p=0.0),
                A.Blur(p=0.0),
                A.MedianBlur(p=0.0),
                A.ToGray(p=0.0),
                A.ImageCompression(quality_lower=75, p=0.0),
            ],
```",present getting error today training new integration certain none strictly check range got edit another anomaly spatial see bounding would use python,issue,negative,positive,positive,positive,positive,positive
876088190,"> Hey @NesterukSergey, thanks. Looks good to me! We can proceed with implementing this feature with Albumetnations.
> 
> I propose to create a new package `augmentors` in the [`albumentations` directory](https://github.com/albumentations-team/albumentations/tree/master/albumentations) and place all the required code into this package.

The idea is inline with the [copy-paste augmentation](https://arxiv.org/abs/2012.07177) method which achieves very promising performance improvements. This would be a great addition to the Albumentations augmentations.  ",hey thanks good proceed feature propose create new package directory place code package idea augmentation method promising performance would great addition,issue,positive,positive,positive,positive,positive,positive
875081128,"Yes, volumetric semantic segmentations require extensive data augmentation. It would be nice if 3D volumetric data augmentation is supported. ",yes volumetric semantic require extensive data augmentation would nice volumetric data augmentation,issue,positive,positive,positive,positive,positive,positive
874584160,"This would be a quick draft of an implementation and test

```python
import random
import albumentations as A


class RandomCropNearBBox(A.RandomCropNearBBox):
    def __init__(self, max_part_shift=0.3, sampling: str = 'consistent', always_apply=False, p=1.0):
        """"""
        Crops an image near a bounding box.

        Args:
            max_part_shift: relative random shift
            sampling: 'individual' then each corner of the box is individually shifted (i.e. variable output size),
             or 'consistent' which applies a single shift to x and y
             (i.e. output size is equal to specified cropping bbox).
            always_apply:
            p:
        """"""
        super().__init__(max_part_shift=max_part_shift, always_apply=always_apply, p=p)

        self._sampling = sampling

    def get_params_dependent_on_targets(self, params):
        if self._sampling == 'individual':
            return super().get_params_dependent_on_targets(params)

        elif self._sampling == 'consistent':
            bbox = params[""cropping_bbox""]
            h_max_shift = int((bbox[3] - bbox[1]) * self.max_part_shift)
            w_max_shift = int((bbox[2] - bbox[0]) * self.max_part_shift)

            x_shift = random.randint(-w_max_shift, w_max_shift)
            y_shift = random.randint(-h_max_shift, h_max_shift)

            x_min = bbox[0] + x_shift
            x_max = bbox[2] + x_shift

            y_min = bbox[1] + y_shift
            y_max = bbox[3] + y_shift

            return {""x_min"": x_min, ""x_max"": x_max, ""y_min"": y_min, ""y_max"": y_max}
        else:
            raise ValueError


# test
import torch
import pytest

from hand_segmentation.model.utils import processing


@pytest.mark.parametrize(""sampling"", ['individual', 'consistent'])
def test_random_crop_near_bbox(sampling):
    img = torch.rand(720, 1280, 3)

    cropping_box = [300, 400, 500, 600]

    my_impl = processing.RandomCropNearBBox(sampling=sampling)
    out = my_impl(image=img, cropping_bbox=cropping_box)

    if sampling == 'individual':
        pass
    elif sampling == 'consistent':
        assert out['image'].size() == torch.Size([200, 200, 3])
```",would quick draft implementation test python import random import class self sampling image near bounding box relative random shift sampling corner box individually variable output size single shift output size equal super sampling self return super return else raise test import torch import import sampling sampling sampling pas sampling assert,issue,positive,positive,neutral,neutral,positive,positive
874266475,GPU augmentations are very useful in nodes having many GPUs (8-16). In environments with many jobs running it is not acceptable to use 20+ cores for a single GPU job as machines run out of available cores before they run out of GPUs. Having (most of the time) very expensive GPUs idle is a massive waste of resources. So even if it was slightly slower it is still a crucial addition in my opinion.,useful many many running acceptable use single job run available run time expensive idle massive waste even slightly still crucial addition opinion,issue,negative,positive,neutral,neutral,positive,positive
874251763,"> As an alternative, we have the [`CropAndPad`](https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad) transform which does cropping and padding, but in the reverse order (first cropping, then padding).

Yes. I need to do padding and then cropping :)",alternative transform padding reverse order first padding yes need padding,issue,negative,positive,positive,positive,positive,positive
874214837,"@iss-hui @Dipet good news 😃! Your original issue may now be fixed ✅ in PR https://github.com/ultralytics/yolov5/pull/3882. This PR implements a [YOLOv5](https://github.com/ultralytics/yolov5) 🚀 + [Albumentations](https://github.com/albumentations-team/albumentations) integration. The integration will automatically apply Albumentations transforms during YOLOv5 training if `albumentations>=1.0.0` is installed in your environment.

## Get Started

To use albumentations simply `pip install -U albumentations` and then update the augmentation pipeline as you see fit in the `Albumentations` class in `yolov5/utils/augmentations.py`. Note these Albumentations operations run **in addition to** the YOLOv5 **hyperparameter** augmentations, i.e. defined in [hyp.scratch.yaml](https://github.com/ultralytics/yolov5/blob/master/data/hyps/hyp.scratch.yaml).

```python
class Albumentations:
    # YOLOv5 Albumentations class (optional, used if package is installed)
    def __init__(self):
        self.transform = None
        try:
            import albumentations as A
            check_version(A.__version__, '1.0.0')  # version requirement

            self.transform = A.Compose([
                A.Blur(p=0.1),
                A.MedianBlur(p=0.1),
                A.ToGray(p=0.01)],
                bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))

            logging.info(colorstr('albumentations: ') + ', '.join(f'{x}' for x in self.transform.transforms))
        except ImportError:  # package not installed, skip
            pass
        except Exception as e:
            logging.info(colorstr('albumentations: ') + f'{e}')

    def __call__(self, im, labels, p=1.0):
        if self.transform and random.random() < p:
            new = self.transform(image=im, bboxes=labels[:, 1:], class_labels=labels[:, 0])  # transformed
            im, labels = new['image'], np.array([[c, *b] for c, b in zip(new['class_labels'], new['bboxes'])])
        return im, labels
```

## Example Result

Example `train_batch0.jpg` on COCO128 dataset with Blur, MedianBlur and ToGray. See the YOLOv5 [Notebooks](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb) to reproduce: <a href=""https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> <a href=""https://www.kaggle.com/ultralytics/yolov5""><img src=""https://kaggle.com/static/images/open-in-kaggle.svg"" alt=""Open In Kaggle""></a>

![train_batch0](https://user-images.githubusercontent.com/26833433/124400879-ff331b80-dd25-11eb-9b67-fe85ac4ca104.jpg)


# Update

To receive this YOLOv5 update:

- **[Git](https://github.com/ultralytics/yolov5)** – `git pull` from within your `yolov5/` directory or `git clone https://github.com/ultralytics/yolov5` again
- **[PyTorch Hub](https://pytorch.org/hub/ultralytics_yolov5/)** – Force-reload with `model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True)`
- **[Notebooks](https://github.com/ultralytics/yolov5/blob/master/tutorial.ipynb)** – View updated notebooks  <a href=""https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""></a> <a href=""https://www.kaggle.com/ultralytics/yolov5""><img src=""https://kaggle.com/static/images/open-in-kaggle.svg"" alt=""Open In Kaggle""></a>
- **[Docker](https://hub.docker.com/r/ultralytics/yolov5)** – `sudo docker pull ultralytics/yolov5:latest` to update your image <a href=""https://hub.docker.com/r/ultralytics/yolov5""><img src=""https://img.shields.io/docker/pulls/ultralytics/yolov5?logo=docker"" alt=""Docker Pulls""></a>

Thank you for spotting this issue and informing us of the problem. Please let us know if this update resolves the issue for you, and feel free to inform us of any other **issues** you discover or **feature requests** that come to mind. Happy trainings with YOLOv5 🚀!",good news original issue may fixed rocket integration integration automatically apply training environment get use simply pip install update augmentation pipeline see fit class note run addition defined python class class optional used package self none try import version requirement except package skip pas except exception self new new zip new new return example result example coco blur see reproduce open open update receive update git git pull within directory git clone hub model view open open docker docker pull latest update image docker thank spotting issue u problem please let u know update issue feel free inform u discover feature come mind happy rocket,issue,positive,positive,positive,positive,positive,positive
874213390,"Yes, but this happens during the process of data augmentation performed with albumentation. 
The same issue was report in issue #922, #903 and other ones linked in issue  #922.",yes process data augmentation issue report issue linked issue,issue,negative,neutral,neutral,neutral,neutral,neutral
874209697,I think it happens because your box extends beyond the right edge of the input image.,think box beyond right edge input image,issue,negative,positive,positive,positive,positive,positive
873980490,"@Dipet @ternaus what's the current status of the YOLO normalization bug with Albumentations?

I have a YOLOv5 Albumentations PR in progress in https://github.com/ultralytics/yolov5/pull/3882, but scanning the YOLO-related issues here in the albumentations repo raises some concerns currently.

Can you explain the problem and the solution roadmap if this has not been resolved already? Thanks!",current status normalization bug progress scanning currently explain problem solution resolved already thanks,issue,positive,positive,neutral,neutral,positive,positive
873969961,"Looks like OpenCV function `CopyMakeBorder` always creates copy of image.
```python
import cv2 as cv
import numpy as np
import time

img = np.empty([512, 512, 3], dtype=np.uint8)
res = cv.copyMakeBorder(img, 0, 0, 0, 0, cv.BORDER_CONSTANT, value=0)
id(res), id(img), id(res) == id(img)
# (140702040314624, 140702040316144, False)
```",like function always copy image python import import import time id id id id false,issue,negative,negative,negative,negative,negative,negative
873963254,"> Or as an alternative for Option 2, maybe it is better to an explicit argument for all `DualTransform` subclasses, such as
> 
> ```
> t = A.Compose([
>     A.Resize(128, 128, mask_interpolation=cv2.INTER_NEAREST_EXACT)
> ])
> ```
> 
> What do you think?

I think approach better, but it is need to change `__init__` method for all transforms. If we will do this, we must to add `**kwargs` as an argument for all transforms to have possibility to add new features simpler",alternative option maybe better explicit argument think think approach better need change method must add argument possibility add new simpler,issue,negative,positive,positive,positive,positive,positive
873834763,"Or as an alternative for Option 2, maybe it is better to an explicit argument for all `DualTransform` subclasses, such as 

```
t = A.Compose([
    A.Resize(128, 128, mask_interpolation=cv2.INTER_NEAREST_EXACT)
])
```

What do you think?",alternative option maybe better explicit argument think,issue,negative,positive,positive,positive,positive,positive
873631310,"> Hey, @aksg87 
> 
> We are working on it, but currently, I don't have an ETA for this feature. As a baseline, I think you could stick to the current behavior of Albumentations, which seems to work reasonably well in practice.

Great! Yes I have used the baseline with a lot of success. Just curious if I should change default types for some optimization",hey working currently eta feature think could stick current behavior work reasonably well practice great yes used lot success curious change default optimization,issue,positive,positive,positive,positive,positive,positive
873573487,"Hey, @hassiahk 

Your way looks totally fine and efficient for me. 

As an alternative, we have the [`CropAndPad`](https://albumentations.ai/docs/api_reference/augmentations/crops/transforms/#albumentations.augmentations.crops.transforms.CropAndPad) transform which does cropping and padding, but in the reverse order (first cropping, then padding).",hey way totally fine efficient alternative transform padding reverse order first padding,issue,positive,positive,positive,positive,positive,positive
873570953,"Hey @gentlebreeze1 

We have plans to add a step-by-step tutorial for creating a new augmentation. For now, I think the best option is to check PRs that implement new augmentations as a reference, e.g., #843, #490 ",hey add tutorial new augmentation think best option check implement new reference,issue,positive,positive,positive,positive,positive,positive
873563922,"Hey, @aksg87 

We are working on it, but currently, I don't have an ETA for this feature. As a baseline, I think you could stick to the current behavior of Albumentations, which seems to work reasonably well in practice.",hey working currently eta feature think could stick current behavior work reasonably well practice,issue,negative,positive,neutral,neutral,positive,positive
873305345,@creafz do you know if `mask_interpolation` is going to be added? Any recommendations on current best practice for most segmentation problems?,know going added current best practice segmentation,issue,positive,positive,positive,positive,positive,positive
873128412,Would you mind sharing your measurement code so we compare apples to apples?,would mind measurement code compare,issue,negative,neutral,neutral,neutral,neutral,neutral
872814414,"Even I have a similar issue

>ValueError: Expected y_min for bbox (0.0087890625, -6.938893903907228e-18, 0.0625, 0.04882812499999992, 3) to be in the range [0.0, 1.0], got -6.938893903907228e-18.

but when I check the bounding boxes they seem to be completely normal and I am using PASCAL voc formating for input

`{'bboxes': array([[332.52173913, 503.7826087 , 458.52173913, 597.7826087 ]]),
 'labels': array([5])}`

#https://github.com/albumentations-team/albumentations/issues/459

and this has already been mentioned before but has'nt been solved yet I dont want to get into the mess of editing library files and importing them I would like to make a PR if the team is interested",even similar issue range got check bounding seem completely normal input array array already yet dont want get mess library would like make team interested,issue,positive,positive,neutral,neutral,positive,positive
872810113,Even Im cought up with the same issue If the team is interested in a PR I am willing to do,even issue team interested willing,issue,negative,positive,positive,positive,positive,positive
871594981,not affiliated with them.  edited the original post. we deploy for financial institution and that's why this was raised,original post deploy financial institution raised,issue,negative,positive,positive,positive,positive,positive
871591459,"According to stack trace, it seems you're running the code in the notebook, so there is a big change you mess somehow with variable names, or executed cells out-of-order. I could not reproduce your issue, therefore closing as non-reproducible. If the problem persist, please attach self-container code snippet that triggers this error.",according stack trace running code notebook big change mess somehow variable executed could reproduce issue therefore problem persist please attach code snippet error,issue,negative,negative,neutral,neutral,negative,negative
871590105,"I feel the whole point of raising this issue is to add ""Detected with snyk"" PR footer. ",feel whole point raising issue add footer,issue,negative,positive,positive,positive,positive,positive
871588844,"If you think it's wrong, please update your issue using proper issue template to elaborate your point.  ",think wrong please update issue proper issue template elaborate point,issue,negative,neutral,neutral,neutral,neutral,neutral
871588262,It's because ISO-noise naturally fits into RGB-domain nicely. We are community-driven and pull-request that extend functionality to support more dtypes and colorspaces are highly appreciated. ,naturally nicely extend functionality support highly,issue,positive,positive,positive,positive,positive,positive
869462980,@VasLem Hi ! I am also interested in such inverse transformation (particularly for geometrics) ... Did you finally find a way to do it simply ?,hi also interested inverse transformation particularly finally find way simply,issue,negative,positive,neutral,neutral,positive,positive
869153628,"@creafz I think your ci pipeline updated numpy version and all your code is not mypy correct now
The previous pipeline has this output in ""Install PyTorch on Linux and Windows"" step:
`Successfully installed numpy-1.20.3 pillow-8.2.0 torch-1.8.1+cpu torchvision-0.9.1+cpu typing-extensions-3.10.0.0`
But now:
`Successfully installed numpy-1.21.0 pillow-8.2.0 torch-1.8.1+cpu torchvision-0.9.1+cpu typing-extensions-3.10.0.0`",think pipeline version code correct previous pipeline output install step successfully successfully,issue,positive,positive,positive,positive,positive,positive
867122845,"@BloodAxe when I run it for the first time, I got `<class 'numpy.ndarray'> True`, but after I initialized my model and run it again as said above I got `<class 'torch.Tensor'> False`",run first time got class true model run said got class false,issue,negative,positive,neutral,neutral,positive,positive
867105910,"Can you print type of image after you loaded it from the dicom?
```
image=(x-x.min())/(x.max()-x.min())
print(type(image), isinstance(image, np.ndarray))
```",print type image loaded print type image image,issue,negative,neutral,neutral,neutral,neutral,neutral
866531772,I am having the same problem. Shouldn't albumentations round down or up the values instead of throwing an error?,problem round instead throwing error,issue,negative,negative,negative,negative,negative,negative
866326285,"@Dipet the best place to insert Albumentation augmentations into the YOLOv5 dataloader is here:
https://github.com/ultralytics/yolov5/blob/63060910a68bfde238872d629ab88e2e7bc736e8/utils/datasets.py#L536-L552

where `img` is the image and `labels` are the bounding box labels. Note that any Albumentation augmentations you add will be in addition to the existing automatic YOLOv5 augmentations defined in your hyperparameter file:
https://github.com/ultralytics/yolov5/blob/63060910a68bfde238872d629ab88e2e7bc736e8/data/hyp.scratch.yaml#L21-L33",best place insert image bounding box note add addition automatic defined file,issue,positive,positive,positive,positive,positive,positive
865701054,Please support 3D data augmentation. ,please support data augmentation,issue,positive,neutral,neutral,neutral,neutral,neutral
865161313,Hi! Could you please target branch of this PR to master?,hi could please target branch master,issue,negative,neutral,neutral,neutral,neutral,neutral
864849210,You also could to stack all mask into single multichannel mask: `np.dstack(masks)`,also could stack mask single mask,issue,negative,negative,neutral,neutral,negative,negative
864843198,"Transform `Rotate` rotate image to random angle between `[-90, 90]` degrees. If you want to rotate to 90 degrees only use `RandomRotate90`",transform rotate rotate image random angle want rotate use,issue,negative,negative,negative,negative,negative,negative
863404724,"I'm disappointed with albumentations. I see until today jun-2021 this bug was not corrected.

Albumentations version 0.5.2:
Python version 3.8.10:
OS windows 10

Thank you @abidKiller , your solution helped me.
",disappointed see today bug corrected version python version o thank solution,issue,negative,negative,negative,negative,negative,negative
863391781,"Hi. Thank you for the quick response and the suggested quick fix. That will indeed work for me. I have been working on instance segmentation for so long now, that it did not even occur to me! :wink: 

Thanks! :raised_hands:  ",hi thank quick response quick fix indeed work working instance segmentation long even occur wink thanks,issue,positive,positive,positive,positive,positive,positive
863302260,"Hi. We will take a look on this issue meanwhile.


As a quick fix, would a semantic mask (e.g each pixel can have values of {C}, where C - total number of classes) work for your case? ",hi take look issue meanwhile quick fix would semantic mask total number class work case,issue,negative,positive,positive,positive,positive,positive
861421952,"It looks like the problem is known. These issues looks the same: 
- #903
- #862 
- #883 
- #848 
- #679 

I will try to fix this problem today.",like problem known try fix problem today,issue,negative,neutral,neutral,neutral,neutral,neutral
861406916,"But why is it I got error like this when I'm trying to transform the image?
`ValueError: Expected x_max for bbox (0.9675, 0.19469026548672566, 1.0025, 0.3274336283185841, 0.0) to be in the range [0.0, 1.0], got 1.0025.`

This is the original bounding box
```
0 0.537500 0.227876 0.130000 0.323009
0 0.982500 0.263274 0.035000 0.137168
0 0.351250 0.471239 0.112500 0.172566
```",got error like trying transform image range got original bounding box,issue,negative,positive,positive,positive,positive,positive
861364866,"I am not sure that this code will work properly. Look how it is implemented in [RandomCropNearBBox](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/crops/transforms.py#L434)
You need to describe all params for transform in `targets_as_params` and implement `get_params_dependent_on_targets`.",sure code work properly look need describe transform implement,issue,negative,positive,positive,positive,positive,positive
861361331,Also you could to use `additional_targets`. Look example in the documentation https://albumentations.ai/docs/examples/example_multi_target/,also could use look example documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
861354424,"Hi! Yes, this looks as a right way to get what you want. If you pass extra arguments to call `self.transform(image=image, hlabel=hlabel)`, they will be available in `apply` function. ",hi yes right way get want pas extra call available apply function,issue,negative,positive,positive,positive,positive,positive
860557576,"Perhaps, you can split your transformation pipeline into two parts: 
1) Photometric transformations, unique for each image:
``` transform = A.Compose([ A.RandomBrightnessContrast(p=0.2), A.RGBShift(p=0.2),])```

2) Spatial transformations, same for all images in a 'row'
```transform = A.Compose([A.HorizontalFlip(p=0.5),])```

Then, it becomes way more easier to operate with your data. First, you apply augmentations independently for every image of your set (`With this transform, I want RandomBrightnessContrast and RGBShift to have different parameters for image, image0, ..., imageN,`) and secondly you can apply random flip to the whole set simultaneously using `ReplayCompose` or with `image=image, image0=image0, ..., imageN=imageN, mask=mask, mask0=mask0, ..., maskM=maskM` notation).
",perhaps split transformation pipeline two photometric unique image transform spatial transform becomes way easier operate data first apply independently every image set transform want different image image secondly apply random flip whole set simultaneously notation,issue,negative,positive,neutral,neutral,positive,positive
860529263,"To apply the transformations the same way to all images and masks, it indeed seems to be the right tool. However, it is not obvious to me how it would help with only replaying the image&mask transforms but not the image only transforms in a Compose.

```
transform = A.Compose([
    A.RandomBrightnessContrast(p=0.2),
    A.HorizontalFlip(p=0.5),
    A.RGBShift(p=0.2),
])

data = transform(image=image, image0=image0, ..., imageN=imageN, mask=mask, mask0=mask0, ..., maskM=maskM)
```
With this transform, I want `RandomBrightnessContrast` and `RGBShift` to have different parameters for image, image0, ..., imageN, but `HorizontalFlip` to have the same parameter for image, ..., imageN and mask, ..., maskM.
I feel like ReplayCompose would apply the same image transforms to all images, which is not what I want.",apply way indeed right tool however obvious would help image mask image compose transform data transform transform want different image image parameter image mask feel like would apply image want,issue,positive,positive,neutral,neutral,positive,positive
860512603,"A.ReplayCompose looks like a way to go with you task.

Пн, 14 июня 2021 г. в 11:38, JeremieHuteau ***@***.***>:

> I work with samples which are tuples of sets: a set of images and a set of
> masks form a single ""row"" of my dataset.
>
> I want to apply augmentations that work on both images and masks with the
> same parameters to all elements of both sets (eg horizontally flip (or not)
> all images and masks), but apply augmentations that work only on a modality
> with different parameters for all elements (eg color jitter all images with
> different parameters).
>
> How do I do that with albumentations? If it cannot be done natively, how
> should I go about implementing such an augmentation pipeline?
>
> (actually, I'm working with sets of images and texts, and use transforms
> on text the way it's done in VirTex
> <https://github.com/kdexd/virtex/blob/master/virtex/data/transforms.py>,
> but I'm expecting the answer not to depend on that)
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/920>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YGONOL6IUHW5CPCWELTSW5YLANCNFSM46UYR3PQ>
> .
>
",like way go task work set set form single row want apply work horizontally flip apply work modality different color jitter different done natively go augmentation pipeline actually working use text way done answer depend thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
859167471,"The normalize_bbox function might exceed the length and width. So I modified the original code to make sure it's within the right range. IF possible, please update your code.

def normalize_bbox(bbox, rows, cols):
    """"""Normalize coordinates of a bounding box. Divide x-coordinates by image width and y-coordinates
    by image height.

    Args:
        bbox (tuple): Denormalized bounding box `(x_min, y_min, x_max, y_max)`.
        rows (int): Image height.
        cols (int): Image width.

    Returns:
        tuple: Normalized bounding box `(x_min, y_min, x_max, y_max)`.

    Raises:
        ValueError: If rows or cols is less or equal zero

    """"""
    (x_min, y_min, x_max, y_max), tail = bbox[:4], tuple(bbox[4:])

    if rows <= 0:
        raise ValueError(""Argument rows must be positive integer"")
    if cols <= 0:
        raise ValueError(""Argument cols must be positive integer"")
    
    x_max = x_max if x_max < cols else cols
    y_max = y_max if y_max < rows else rows
    x_min, x_max = x_min / cols, x_max / cols
    y_min, y_max = y_min / rows, y_max / rows

    return (x_min, y_min, x_max, y_max) + tail",function might exceed length width original code make sure within right range possible please update code normalize bounding box divide image width image height bounding box image height image width bounding box le equal zero tail raise argument must positive integer raise argument must positive integer else else return tail,issue,positive,positive,positive,positive,positive,positive
854648492,"I just found the error. It is a mix between an error from my part and a weird interaction from albumentations.

I'm applying these transforms to images and masks, and in one case I had my mask rotated 90 degrees for some reason. That's my fault. 

Though, I think albumentations should check if the images and the masks have the same shape (or the same aspect ratio.) In the latter case, it should resize the crop params to adequate them for the rest of the items. 

EDIT: In case shape/aspect ratio don't match, I think the error should be different to be able to debug your data. Something like: ""image and mask have different shapes.""",found error mix error part weird interaction one case mask rotated reason fault though think check shape aspect ratio latter case resize crop adequate rest edit case ratio match think error different able data something like image mask different,issue,negative,positive,neutral,neutral,positive,positive
853865580,"Still same.. I am using google colab.... Is it mandatory now to give
arguments for additional targets in new version of albumentation?

On Thu, Jun 3, 2021, 2:27 PM Mikhail Druzhinin ***@***.***>
wrote:

> Try to upgrade to latest library version pip install --upgrade
> albumentations
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/918#issuecomment-853764910>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKEH4EIMI7ASA26BY5HHUELTQ5KHTANCNFSM4562OITQ>
> .
>
",still mandatory give additional new version wrote try upgrade latest library version pip install upgrade thread reply directly view,issue,negative,positive,positive,positive,positive,positive
853764910,Try to upgrade to latest library version `pip install --upgrade albumentations`,try upgrade latest library version pip install upgrade,issue,negative,positive,positive,positive,positive,positive
853593217,"Requirement already satisfied: albumentations in
/usr/local/lib/python3.7/dist-packages (0.1.12) Requirement already
satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from
albumentations) (1.4.1) Collecting imgaug<0.2.7,>=0.2.5 Downloading
https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz
(631kB) |████████████████████████████████| 634kB 8.1MB/s Requirement
already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages
(from albumentations) (1.19.5) Requirement already satisfied: opencv-python
in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)
Requirement already satisfied: scikit-image>=0.11.0 in
/usr/local/lib/python3.7/dist-packages (from
imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2) Requirement already
satisfied: six in /usr/local/lib/python3.7/dist-packages (from
imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0) Requirement already
satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from
scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5.1)
Requirement already satisfied: PyWavelets>=0.4.0 in
/usr/local/lib/python3.7/dist-packages (from
scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)
Requirement already satisfied: imageio>=2.3.0 in
/usr/local/lib/python3.7/dist-packages (from
scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)
Requirement already satisfied: pillow>=4.3.0 in
/usr/local/lib/python3.7/dist-packages (from
scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)
Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in
/usr/local/lib/python3.7/dist-packages (from
scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)
Requirement already satisfied: decorator<5,>=4.3 in
/usr/local/lib/python3.7/dist-packages (from
networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)
(4.4.2) Requirement already satisfied:
pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in
/usr/local/lib/python3.7/dist-packages (from
matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)
(2.4.7) Requirement already satisfied: cycler>=0.10 in
/usr/local/lib/python3.7/dist-packages (from
matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)
(0.10.0) Requirement already satisfied: kiwisolver>=1.0.1 in
/usr/local/lib/python3.7/dist-packages (from
matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)
(1.3.1) Requirement already satisfied: python-dateutil>=2.1 in
/usr/local/lib/python3.7/dist-packages (from
matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)
(2.8.1) Building wheels for collected packages: imgaug Building wheel for
imgaug (setup.py) ... done Created wheel for imgaug:
filename=imgaug-0.2.6-cp37-none-any.whl size=654019
sha256=8c01e192e46434466d8d883f9cc1f8b7338fff29c67a0d34907199571c6fc027
Stored in directory:
/root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0
Successfully built imgaug Installing collected packages: imgaug Found
existing installation: imgaug 0.2.9 Uninstalling imgaug-0.2.9: Successfully
uninstalled imgaug-0.2.9 Successfully installed imgaug-0.2.6

On Thu, Jun 3, 2021 at 9:56 AM Satriosnjya ***@***.***> wrote:

> What library that you mean ?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/918#issuecomment-853587645>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AKEH4EOI6K42D7XW4A7QRJDTQ4KR3ANCNFSM4562OITQ>
> .
>
",requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied six requirement already satisfied requirement already satisfied requirement already satisfied requirement already satisfied pillow requirement already satisfied requirement already satisfied decorator requirement already satisfied requirement already satisfied cycler requirement already satisfied requirement already satisfied building collected building wheel done wheel directory successfully built collected found installation successfully uninstalled successfully wrote library mean thread reply directly view,issue,positive,positive,positive,positive,positive,positive
853319161,Would you mind adding a version of the library you have?,would mind version library,issue,negative,neutral,neutral,neutral,neutral,neutral
853037167,Hey @Maxwell2Gyamfi I would like to do the same but my images are in pascal_voc type and not in yolo what are the changes I have to do in my case?,hey would like type case,issue,negative,neutral,neutral,neutral,neutral,neutral
852075782,Version [1.0.0](https://github.com/albumentations-team/albumentations/releases) has been released. Try to upgrade you package from [PyPi](https://pypi.org/project/albumentations/1.0.0/),version try upgrade package,issue,negative,neutral,neutral,neutral,neutral,neutral
852016906,"I'm not able to reproduce the error on purpose. If I do, I will update asap.",able reproduce error purpose update,issue,negative,positive,positive,positive,positive,positive
851435596,"@Dipet I believe so. At least, I have no augmentations on the validation step, so it seems nothing to do with albumentations. In any case, thank you for the support! Appreciate your help ❤️ ",believe least validation step nothing case thank support appreciate help,issue,negative,negative,negative,negative,negative,negative
851380264,"Hmm, strange. I can not reproduce the problem.
Could you give reproducible example with fixed seeds?
```python
import albumentations as A
import numpy as np
import random

seed = 0
random.seed(seed)
np.random.seed(seed)

img_size = 512
aug = A.Compose([A.RandomResizedCrop(img_size, img_size, scale=(0.20, 1.))])

img = np.zeros([1836, 3264, 3], dtype=np.uint8)
for i in range(1000):
    res= aug(image=img)[""image""]

img = np.zeros([3264, 1836, 3], dtype=np.uint8)
for i in range(1000):
    res= aug(image=img)[""image""]

```",strange reproduce problem could give reproducible example fixed python import import import random seed seed seed range image range image,issue,negative,negative,negative,negative,negative,negative
851359518,We are working on a release. Hopefully we will release a new version in PyPi soon.,working release hopefully release new version soon,issue,negative,positive,positive,positive,positive,positive
851355571,Looks good. I think current differences associated with the instability of algorithms and hardware.,good think current associated instability hardware,issue,negative,positive,positive,positive,positive,positive
850519881,"Okay, it seems like the fall back part is okay. Steps:

```
import cv2
import albumentations as A

!wget https://image.api.playstation.com/vulcan/img/rnd/202010/2621/KvgbdmY6MvPdLaa3dyVNGOw1.png?w=440

img = cv2.imread('/content/KvgbdmY6MvPdLaa3dyVNGOw1.png?w=440')
# img = img.transpose(1, 0, 2)
print(img.shape)

ratio = (0.75, 1.3333333333333333)

in_ratio = img.shape[1] / img.shape[0]

if in_ratio < min(ratio):
    w = img.shape[1]
    h = int(round(w / min(ratio)))
elif in_ratio > max(ratio):
    h = img.shape[0]
    w = int(round(h * max(ratio)))
else:  # whole image
    w = img.shape[1]
    h = img.shape[0]

print(h, w)
```",like fall back part import import print ratio min ratio round min ratio ratio round ratio else whole image print,issue,negative,negative,neutral,neutral,negative,negative
850513624,"Oh, I'm sorry. I thought it is error from `RandomCrop`. I will check this error. Thank you for your attention.",oh sorry thought error check error thank attention,issue,negative,negative,negative,negative,negative,negative
850512934,"@Dipet Even more, I'd say it triggers when it falls back to center crop, around line 360: https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/crops/transforms.py#L360.",even say back center crop around line,issue,negative,negative,neutral,neutral,negative,negative
850510049,"@Dipet Wait what? New H and W are chosen randomly inside the buggy function. I'm reporting this bug just precisely because something must be wrong inside `albumentations.RandomResizedCrop`, which first crops and then resizes the image. It should be something around here https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/crops/transforms.py#L338, which I believe is the place where W and H crop parameters are being chosen.",wait new chosen randomly inside buggy function bug precisely something must wrong inside first image something around believe place crop chosen,issue,negative,negative,neutral,neutral,negative,negative
850502320,"@Dipet glad you wrote back 🙌 

I think the fix from #911 greatly mitigated the variance of the metrics. Here is what I can see now:

<img width=""1441"" alt=""Screenshot 2021-05-28 at 18 26 40"" src=""https://user-images.githubusercontent.com/9402690/120007631-c85d3d80-bfe2-11eb-93ef-420b9e054cd8.png"">

Currently, losses and accuracies roughly vary by +-0.01. Is this something we expect to see?",glad wrote back think fix greatly variance metric see currently roughly vary something expect see,issue,negative,positive,positive,positive,positive,positive
850467136,"> ValueError: Requested crop size (2131, 1636) is larger than the image size (1836, 3264)

As described in message - images size is less then requested crop size. Try to rotate or resize image before cropping.",crop size image size message size le crop size try rotate resize image,issue,negative,neutral,neutral,neutral,neutral,neutral
850465487,"Oh, sorry. Looks like some of files are corrupted (has 0 size).
And if we talk about reproducibility, it would be great to have 2 groups of files from 2 independent runs.
Looks like you had a problems with trying to process batches inside albumentations pipeline. 
Have you tried to reproduce the results after fixing this issue?",oh sorry like corrupted size talk reproducibility would great independent like trying process inside pipeline tried reproduce fixing issue,issue,positive,positive,positive,positive,positive,positive
850410850,"@Dipet have you had a chance to open the replay ""black box"" of the albumentations I have shared with you? 😄 ",chance open replay black box,issue,negative,negative,neutral,neutral,negative,negative
850409543,"As it was discussed in #911, the albumentations expects to get a single image as input, otherwise you may experience issues I described there and here.

So whenever you use `image_dataset_from_directory()` to load your images, make sure to do the following in augmentation step:

```python
def augment_image(inputs, labels, augmentation_pipeline: a.Compose, seed: int = 42):
    def apply_augmentation(images):
        random.seed(seed)
        np.random.seed(seed)

        augmented_images = []

        for img in images:  # apply augmentation pipeline to single image (not to the batch)
            aug_data = augmentation_pipeline(image=img.astype('uint8'))
            augmented_images.append(aug_data['image'])

        return np.stack(augmented_images)

    inputs = tf.numpy_function(func=apply_augmentation, inp=[inputs], Tout=tf.uint8)

    return inputs, labels
```

This should help.",get single image input otherwise may experience whenever use load make sure following augmentation step python seed seed seed apply augmentation pipeline single image batch return return help,issue,positive,positive,neutral,neutral,positive,positive
850406256,"@Dipet yes, seems like the reason of the issue. Thank you for the help 👍",yes like reason issue thank help,issue,positive,neutral,neutral,neutral,neutral,neutral
849627520,"It looks like you are trying to apply augmentation to batch of images.
Try to change line
```python
aug_data = augmentation_pipeline(image=images.astype('uint8'))
```

to:
```python
res_images = []
for img in images:
    aug_data = augmentation_pipeline(image=img.astype('uint8'))
    res_images.append(aug_data[""image""])
return np.stack(res_images)
```",like trying apply augmentation batch try change line python python image return,issue,negative,neutral,neutral,neutral,neutral,neutral
849533939,"@Dipet thank you for replay!

It makes sense to me that the snippet above did not help to reproduce the issue. I feel like `image_dataset_from_directory()` has something to do with the issue (may be related to https://github.com/albumentations-team/albumentations/issues/905), so there would be more odds to reproduce it if you try to load images in the same way as me.

In any case, here is an achieve with ablumentations state after running:

```python
for image_batch, _ in train_dataset.take(1):
    for idx in range(9):
        image = image_batch[idx].numpy().astype('uint8')

        ax = plt.subplot(3, 3, idx + 1)
        plt.imshow(image)
        plt.axis('off')
```

<img width=""568"" alt=""Screenshot 2021-05-27 at 13 45 47"" src=""https://user-images.githubusercontent.com/9402690/119813926-8523a180-bef2-11eb-89de-c41a0a606b8f.png"">

https://drive.google.com/file/d/13Tf-iFM7hjBqH7jntys3SqFHQdE0BDPG/view?usp=sharing",thank replay sense snippet help reproduce issue feel like something issue may related would odds reproduce try load way case achieve state running python range image ax image,issue,positive,neutral,neutral,neutral,neutral,neutral
849521395,"I can not reproduce the problem.
```python
import albumentations as a
import cv2 as cv
import matplotlib.pyplot as plt

augs = a.Compose([
    a.VerticalFlip(p=1),
    a.HorizontalFlip(p=1),
    a.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.1, brightness_by_max=False, p=1),
    a.CoarseDropout(p=1.0, max_holes=20, max_height=8, max_width=8, min_holes=10, min_height=8, min_width=8),
    a.GaussNoise(p=1.0, var_limit=(10.0, 50.0)),
])

img = cv.imread(""/home/dipet/Pictures/paper01-000.png"")
img = cv.cvtColor(img, cv.COLOR_BGR2RGB)

plt.subplot(211)
plt.imshow(img, vmin=0, vmax=255)
plt.subplot(212)
plt.imshow(augs(image=img)[""image""], vmin=0, vmax=255)
plt.show()
```

Could you give random seed to reproduce the problem?
Or give dump of applied args from [`ReplayCompose`](https://albumentations.ai/docs/examples/replay/#debugging-an-augmentation-pipeline-with-replaycompose) and images for these arguments",reproduce problem python import import import image could give random seed reproduce problem give dump applied,issue,negative,negative,negative,negative,negative,negative
849509085,It looks like a bug that needs to be fixed.,like bug need fixed,issue,negative,positive,neutral,neutral,positive,positive
849508021,"Why do the `denormalize` and the `normalize` in the first step? 
`yolo => albumentations` can just be transformed by:

    x, y, w, h = bbox # from yolo
    w_half, h_half = w / 2, h / 2
   
    x_min, x_max = x - w_half, x + w_half
    y_min, y_max = y - h_half, y + h_half
    
    return

that omits the pixel errors in the first place. Is there a particular reason why it is not like this?",normalize first step return first place particular reason like,issue,negative,positive,positive,positive,positive,positive
849496544,"@Dipet yeah, just was in process of collecting the information. Although, I decided to store each augmentation run in a different pkl file:

```python
def augment_image(inputs, labels, augmentation_pipeline: a.Compose, seed: int = 42):
    def apply_augmentation(images):
        random.seed(seed)
        np.random.seed(seed)

        aug_data = augmentation_pipeline(image=images.astype('uint8'))

        with open(f'logs/debug/replay-{datetime.datetime.now().timestamp()}.pkl', 'wb') as outfile:
            pickle.dump(aug_data['replay'], outfile)

        return aug_data['image']

    inputs = tf.numpy_function(func=apply_augmentation, inp=[inputs], Tout=tf.uint8)

    return inputs, labels
```

I hope you are okay with that. 

Here is a zip archive with a few files generated by snippet above:

https://drive.google.com/file/d/1lH-YuY4abcVYk12cCwXJm5PAAUd1kXS5/view?usp=sharing",yeah process information although decided store augmentation run different file python seed seed seed open return return hope zip archive snippet,issue,positive,neutral,neutral,neutral,neutral,neutral
849488322,"Warnings are ok. 
I talked about saving applied arguments. Something like this:
```python
applied_transforms = []


def augment_image(inputs, labels, augmentation_pipeline: a.Compose, seed: int = 42):
    def apply_augmentation(images):
        random.seed(seed)
        np.random.seed(seed)

        aug_data = augmentation_pipeline(image=images.astype('uint8'))
        applied_transforms.append(data['replay'])
        return aug_data['image']

    inputs = tf.numpy_function(func=apply_augmentation, inp=[inputs], Tout=tf.uint8)

    return inputs, labels

# train
....

# save after train 
with open('data.pickle', 'wb') as f:
    pickle.dump(applied_transforms, f)
```

And after that we could compare applied arguments and transforms.",saving applied something like python seed seed seed data return return train save train open could compare applied,issue,positive,neutral,neutral,neutral,neutral,neutral
849482664,"So I fixed the errors for me by changing the conversion in `bbox_utils` for the yolo case.
Since yolo comes in already relative a conversion to absolute format and then back to relative will yield errors.

So instead of convertig to absolute I removed it and now with the coordinates I get from yolo

    x, y, w, h = yolo_bbox
    
    w_half, h_half = w / 2, h / 2
    x_min, x_max = x - w_half, x + w_half
    y_min, y_max = y - h_half, y + h_half

and vice versa in the `from_albumentation` case.  ",fixed conversion case since come already relative conversion absolute format back relative yield instead absolute removed get vice case,issue,negative,positive,neutral,neutral,positive,positive
849480957,"@Dipet I have noticed an interesting thing. After I enabled ReplayMode, I have started to see less variance in the training loss/accuracy, but validation metrics still vary by a lot:

<img width=""1042"" alt=""Screenshot 2021-05-27 at 12 19 36"" src=""https://user-images.githubusercontent.com/9402690/119800865-d6796400-bee5-11eb-9e5e-640dc7a38bc8.png"">

In addition, there were new warnings related to GaussNoise and CoarseDropout augmentations:

```
Epoch 1/10
UserWarning: albumentations.augmentations.transforms.GaussNoise could work incorrectly in ReplayMode for other input data because its' params depend on targets.
  warn(
46/63 [====================>.........] - ETA: 1s - loss: 1.2924 - accuracy: 0.3268
UserWarning: albumentations.augmentations.transforms.CoarseDropout could work incorrectly in ReplayMode for other input data because its' params depend on targets.
63/63 [==============================] - 9s 86ms/step - loss: 1.2859 - accuracy: 0.3304 - val_loss: 1.1152 - val_accuracy: 0.3417
...
```

So the only changes I have done was:

```python
args['train_augmentation'] = a.ReplayCompose([  # ReplayCompose() replaced Compose() method
    a.VerticalFlip(),
    a.HorizontalFlip(),
    a.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.1, brightness_by_max=False),
    a.CoarseDropout(max_holes=20, max_height=8, max_width=8, min_holes=10, min_height=8, min_width=8),
    a.GaussNoise(p=1.0, var_limit=(10.0, 50.0)),
])
```",interesting thing see le variance training validation metric still vary lot addition new related epoch could work incorrectly input data depend warn eta loss accuracy could work incorrectly input data depend loss accuracy done python compose method,issue,negative,positive,positive,positive,positive,positive
849465635,As another check you could use [`ReplayCompose`](https://albumentations.ai/docs/examples/replay/#debugging-an-augmentation-pipeline-with-replaycompose) and serialize all applied arguments. After that you could rerun and check if all arguments the same or not.,another check could use serialize applied could rerun check,issue,negative,neutral,neutral,neutral,neutral,neutral
849463850,"Hmm. All of a sudden, this issue starts looking more interesting than at
the beginning.

Чт, 27 мая 2021 г. в 11:57, Roman Glushko ***@***.***>:

> @Dipet <https://github.com/Dipet> sure, all tests were performed with the
> following configuration of augmentation pipeline:
>
> args['train_augmentation'] = a.Compose([
>     a.VerticalFlip(),
>     a.HorizontalFlip(),
>     a.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.1, brightness_by_max=False),
>     a.CoarseDropout(max_holes=20, max_height=8, max_width=8, min_holes=10, min_height=8, min_width=8),
>     a.GaussNoise(p=1.0, var_limit=(10.0, 50.0)),
> ])
> args['validation_augmentation'] = a.Compose([])
>
> I kept validation step augmentation-free as @BloodAxe
> <https://github.com/BloodAxe> suggested above.
>
> —
> You are receiving this because you were mentioned.
>
>
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/906#issuecomment-849462738>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YDBTXNOLU5VJ6VVWLTTPYCPFANCNFSM45LQYTDQ>
> .
>
",sudden issue looking interesting beginning sure following configuration augmentation pipeline kept validation step reply directly view,issue,positive,positive,positive,positive,positive,positive
849462738,"@Dipet sure, all tests were performed with the following configuration of augmentation pipeline:

```python
args['train_augmentation'] = a.Compose([
    a.VerticalFlip(),
    a.HorizontalFlip(),
    a.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.1, brightness_by_max=False),
    a.CoarseDropout(max_holes=20, max_height=8, max_width=8, min_holes=10, min_height=8, min_width=8),
    a.GaussNoise(p=1.0, var_limit=(10.0, 50.0)),
])

args['validation_augmentation'] = a.Compose([])
```

I kept validation step augmentation-free as @BloodAxe suggested above.",sure following configuration augmentation pipeline python kept validation step,issue,negative,positive,positive,positive,positive,positive
849459147,"Very strange. There are only 2 things in the library that control randomness.
Could you describe which transforms do you use?",strange library control randomness could describe use,issue,negative,negative,neutral,neutral,negative,negative
849452502,"@Dipet It has been already enabled in the entry point from the very beginning as I mentioned in the ticket, so I have tried to add the line to `augment_image()` function (which I had not checked before):

```python
def augment_image(inputs, labels, augmentation_pipeline: a.Compose, seed: int = 42):
    def apply_augmentation(images):
        random.seed(seed)
        np.random.seed(seed)

        aug_data = augmentation_pipeline(image=images.astype('uint8'))
        return aug_data['image']

    inputs = tf.numpy_function(func=apply_augmentation, inp=[inputs], Tout=tf.uint8)

    return inputs, labels
```

Unfortunately, 5 additional runs show that the picture has not changed much:

<img width=""1042"" alt=""Screenshot 2021-05-27 at 11 40 48"" src=""https://user-images.githubusercontent.com/9402690/119794685-6a483180-bee0-11eb-86cd-6253dccadf2a.png"">

- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/1s780uye
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/1smfse48
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/248nh3ad
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/12w4okqc
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/33e4ec6o",already entry point beginning ticket tried add line function checked python seed seed seed return return unfortunately additional show picture much,issue,negative,negative,negative,negative,negative,negative
849441081,Try also to set a `numpy.random.seed(seed)`.,try also set seed,issue,negative,neutral,neutral,neutral,neutral,neutral
849439378,"@BloodAxe I saw this usage in the examples associated with TF usage, so I tried that even before creating the ticket. However, I have just double checked that and still see the same undeterministic picture in W&B:

<img width=""1044"" alt=""Screenshot 2021-05-27 at 11 15 09"" src=""https://user-images.githubusercontent.com/9402690/119790840-ea6c9800-bedc-11eb-857d-9526c58e8e6e.png"">

- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/4c7vnq5l 
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/1l3vbp3z
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/h40o3k77
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/4yrhl31h
- https://wandb.ai/roma-glushko/rock-paper-scissors/runs/3tx8f5zr

Just for the record, the function was modified this way:

```python
def augment_image(inputs, labels, augmentation_pipeline: a.Compose, seed: int = 42):
    def apply_augmentation(images):
        random.seed(seed)  # fixing seed
        aug_data = augmentation_pipeline(image=images.astype('uint8'))
        return aug_data['image']

    inputs = tf.numpy_function(func=apply_augmentation, inp=[inputs], Tout=tf.uint8)

    return inputs, labels
``` ",saw usage associated usage tried even ticket however double checked still see picture record function way python seed seed fixing seed return return,issue,negative,neutral,neutral,neutral,neutral,neutral
849424101,"Do not use transforms without `Compose`
```python
import numpy as np
import albumentations as A
import cv2
import matplotlib.pyplot as plt

print(A.__version__)


def draw_rectangle(image, bboxes):
    image = image.copy()

    for x1, y1, x2, y2, _ in bboxes:
        cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)

    return image

# Create image
image = np.zeros((200, 200, 3))
box = (50, 50, 100, 100, 0)

# Create object of interest in white
image[box[0]:box[2], box[1]:box[3]] = 255

# augment it
augmentor = A.Compose([
    A.PadIfNeeded(min_height=1000, min_width=1000, border_mode=cv2.BORDER_CONSTANT, value=0)
],
    bbox_params=A.BboxParams(""pascal_voc""),
)
result = augmentor(image=image, bboxes=[box])

plt.subplot(121)
plt.imshow(draw_rectangle(image, [box]))
plt.subplot(122)
plt.imshow(draw_rectangle(result[""image""], result[""bboxes""]))
plt.show()
```
![image](https://user-images.githubusercontent.com/7512250/119788207-9b256800-beda-11eb-9620-e5d1b92b058a.png)
",use without compose python import import import import print image image image return image create image image box create object interest white image box box box box augment result box image box result image result image,issue,positive,neutral,neutral,neutral,neutral,neutral
849417343,"No, at this time library does not contain morphological transforms ",time library contain morphological,issue,negative,neutral,neutral,neutral,neutral,neutral
849388403,"Are you also transforming your bounding boxes? Or only the image? Because your result looks like you are not transforming the bounding boxes, or you don't put the class to the back. Your results are like really broke, but look at mine the bounding box is shifted only some pixels.
",also transforming bounding image result like transforming bounding put class back like really broke look mine bounding box,issue,negative,positive,neutral,neutral,positive,positive
849241688,"@Dipet , yes - this is where I am also a little confused. Does this library not include morphological transforms as part of image augmentation?",yes also little confused library include morphological part image augmentation,issue,negative,negative,negative,negative,negative,negative
849192178,"I can see similar behavior 

![Rzepecki Islands_south_2016_Chinstrap_penguins_96](https://user-images.githubusercontent.com/1208492/119744482-fb67cb80-be40-11eb-866c-10992e7ed57d.png)

```
        transform = A.Compose([            A.PadIfNeeded(min_height=600,min_width=600, border_mode=cv2.BORDER_CONSTANT, value=0),                        A.pytorch.ToTensorV2()        ])        
```
@Dipet I will work on a reproducible example tomorrow. I believe all of these issues are related to the normalization. I am using pascal, not yolo. I will rerun with 

```
        transform = A.Compose([
            A.PadIfNeeded(min_height=600,min_width=600, border_mode=cv2.BORDER_CONSTANT, value=0),            
            A.pytorch.ToTensorV2()
        ], A.BboxParams(format='pascal_voc'))
```


",see similar behavior transform work reproducible example tomorrow believe related normalization rerun transform,issue,negative,neutral,neutral,neutral,neutral,neutral
849162192,"So far I see in `bbox_utils` my yolo coordinates get first denormalized to absolute and then renormalized to albumentations format, in that process some `int` trimming is happening which seems to cause those issues, I could simply normalize without denormalizing by using the yolo format as it is and converting in the `float` domain. 
Seems like an extra step anyway. 
I also noticed that `SafeRotate` seems also to be broken. ",far see get first absolute format process trimming happening cause could simply normalize without format converting float domain like extra step anyway also also broken,issue,negative,positive,neutral,neutral,positive,positive
849063898,"@roma-glushko may I ask for another trial? What if you fix a seed inside the `apply_augmentation` function? That is to ensure `tf.numpy_function` does not introduce any unexpected issues with pseudorandom generator. This test will apply exactly same set of augmnetations, and results should be identical. ",may ask another trial fix seed inside function ensure introduce unexpected generator test apply exactly set identical,issue,negative,positive,positive,positive,positive,positive
849034998,"Sure, so the first image is one from the padded, and the second one from an unpadded case. The green bounding box is the ground truth. 
Just for clarification the red box is a prediction, just ignore it. The differences of the two green boxes are important.

Super bad for me have to redo 10 days of grid search for my thesis, super annoying...

With padding
![padded](https://user-images.githubusercontent.com/29997904/119715242-113ab800-be64-11eb-9d45-aed19660b803.png)

Without padding
![unpadded](https://user-images.githubusercontent.com/29997904/119715267-16980280-be64-11eb-9cdf-701ec7f587e1.png)

There are like 3 - 4 pixels difference.",sure first image one second one unpadded case green bounding box ground truth clarification red box prediction ignore two green important super bad redo day grid search thesis super annoying padding without padding unpadded like difference,issue,positive,negative,neutral,neutral,negative,negative
848954876,"Can you show an example, i've been running into similar problems.

On Wed, May 26, 2021 at 10:04 AM Dimfred ***@***.***> wrote:

> Btw padding an image produces also wrong bounding boxes. All bboxes are
> shifted to the left and to the top on the padded image.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/903#issuecomment-848950452>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAJHBLCLXXTEXPPFQSBDVOTTPUS2HANCNFSM45KA5YCA>
> .
>


-- 
Ben Weinstein, Ph.D.
Postdoctoral Fellow
University of Florida
http://benweinstein.weebly.com/
",show example running similar wed may wrote padding image also wrong bounding left top image thread reply directly view ben postdoctoral fellow university,issue,negative,positive,neutral,neutral,positive,positive
848950452,Btw padding an image produces also wrong bounding boxes. All bboxes are shifted to the left and to the top on the padded image.,padding image also wrong bounding left top image,issue,negative,neutral,neutral,neutral,neutral,neutral
846636024,"> But maybe just raise a value error there instead?

Bad idea, because we have transforms that used current logic and these changes will break their functionality.

> As a compromise can we atleast comment the code in bbox_random_crop to explicitly say that the output coordinates are relative to height - crop_height?

I agree, it would be great to add explanation of how this function works.

> I can open a PR on the wider goal of create a ZoomSafe augmentation. I think it will be of use to a wide audience. Its somewhere between RandomResizeCropSafe and Random_Crop. The safe aspect is important because torchvision detection models do not accept blank annotations.

It would be great. We will wait for your PR.
",maybe raise value error instead bad idea used current logic break functionality compromise comment code explicitly say output relative height agree would great add explanation function work open goal create augmentation think use wide audience somewhere safe aspect important detection accept blank would great wait,issue,positive,positive,positive,positive,positive,positive
846606502,"Sure, that's definitely true. But maybe just raise a value error there instead? It creates a disconnect between the function goal (return the coordinates from a crop of a given size and starting position) and the output. For example, imagine a user needs a crop at a specific location, this function makes it difficult to get the correct position. I had to debug for a few hours to get to this. As a compromise can we atleast comment the code in bbox_random_crop to explicitly say that the output coordinates are relative to height - crop_height? I don't think anyone could have anticipated this. I'm guessing that there are other pieces of logic in the codebase that have similar structures. I forked and changed that line, but it yielded strange boxes. 
![seabirds_rgb_279](https://user-images.githubusercontent.com/1208492/119272534-39ec5480-bbbb-11eb-83ce-e8c6b9a3dec6.png)

I can open a PR on the wider goal of create a ZoomSafe augmentation. I think it will be of use to a wide audience. Its somewhere between RandomResizeCropSafe and Random_Crop. The safe aspect is important because torchvision detection models do not accept blank annotations.
",sure definitely true maybe raise value error instead disconnect function goal return crop given size starting position output example imagine user need crop specific location function difficult get correct position get compromise comment code explicitly say output relative height think anyone could guessing logic similar forked line strange open goal create augmentation think use wide audience somewhere safe aspect important detection accept blank,issue,positive,positive,neutral,neutral,positive,positive
846588955,"@Dipet This absolutely saved me!!

I found that albumentations have some Composition API. 
-> https://albumentations.ai/docs/api_reference/core/composition/

I appreciate your support.",absolutely saved found composition appreciate support,issue,positive,positive,positive,positive,positive,positive
846588179,I can not understand what you want. What the result you expect? For me [erosion](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#gaeb1e0c1033e3f6b891a25d0511362aeb) and [dilation](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c) of image are morphological transforms ,understand want result expect erosion dilation image morphological,issue,negative,neutral,neutral,neutral,neutral,neutral
846587713,Because if we will use `y1 = int(height  * h_start)` we may get crop less then described crop size. For example `h_start=0.9` and `crop_height=200` when `rows=1000` ,use height may get crop le crop size example,issue,negative,neutral,neutral,neutral,neutral,neutral
846585357,Yolov5 is the common object detection task https://albumentations.ai/docs/examples/example_bboxes/,common object detection task,issue,negative,negative,negative,negative,negative,negative
846563603,"@BloodAxe, back to your suggestions:
Here is 6 train runs with validation augmentation disabled (6 runs are shown):

<img width=""1039"" alt=""Screenshot 2021-05-23 at 15 42 05"" src=""https://user-images.githubusercontent.com/9402690/119262358-919eb600-bbe3-11eb-826f-76fdc7426b50.png"">

Don't see a lot of differences to the previous runs with val augmentation enabled.

Here is how training run looks like when I disable albumentations completely (7 runs are shown):

<img width=""1049"" alt=""Screenshot 2021-05-23 at 16 25 19"" src=""https://user-images.githubusercontent.com/9402690/119262388-b430cf00-bbe3-11eb-9462-9151def39462.png"">

On the last plot, this is what I consider to be a reproducible pipeline: all metrics are the same/close at all epochs.
",back train validation augmentation disabled shown see lot previous augmentation training run like disable completely shown last plot consider reproducible pipeline metric,issue,negative,negative,neutral,neutral,negative,negative
846546028,"@BloodAxe thank you for the replay!
> Do you observe same behavior when not using any augmentations?

No, when I disable augmentations, the pipeline becomes deterministic in my experiments. It overfits and do it in the same way every time I rerun it (each epoch stats looks the same)

> PS: usually you don’t want to apply augmentations at validation stage

Will try to disable augmentation for the validation step. Let you know how it went.

> PPS: Pytorch is better

I know, I know 😌 In this particular project I use TF because of TF.js. I want to deploy my model as a server-less webapp.
",thank replay observe behavior disable pipeline becomes deterministic way every time rerun epoch usually want apply validation stage try disable augmentation validation step let know went better know know particular project use want deploy model,issue,positive,positive,positive,positive,positive,positive
846544877,"Do you observe same behavior when not using any augmentations?
PS: usually you don’t want to apply augmentations at validation stage
PPS: Pytorch is better

Вс, 23 мая 2021 г. в 12:30, Roman Glushko ***@***.***>:

> 🐛 Bug
>
> I could get my training work in reproducible way when albumentations added
> to the data pipeline. I followed this thread #93
> <https://github.com/albumentations-team/albumentations/issues/93> and
> fixed all possible seeds, so in overall my snippet that should have enabled
> reproducible experiments looks like this:
>
> import os
> import random
>
>
> import numpy as np
> import tensorflow as tf
>
>
> def set_random_seed(seed: int = 42):
>
>     """"""
>     Globally fix all possible sources of randomness to keep experiment reproducible
>     """"""
>
>     random.seed(seed)
>
>     np.random.seed(seed)
>
>     tf.random.set_seed(seed)
>
>     os.environ['PYTHONHASHSEED'] = str(seed)
>
>     os.environ['TF_DETERMINISTIC_OPS'] = '1'
>
>     os.environ['TF_CUDNN_DETERMINISTIC'] = '1'
>
> Unfortunately, this doesn't help me to get reproducible results. I have
> executed training process 6 times and got all different results. You can
> also see it in W&B:
>
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/2bdgnbwx
>    (best_val_acc: 0.7104, best_epoch: 3)
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/2qo9pbls
>    (best_val_acc: 0.7875, best_epoch: 8)
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/uf6cknge
>    (best_val_acc: 0.6771, best_epoch: 8)
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/tem3umbx
>    (best_val_acc: 0.7729, best_epoch: 6)
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/czsjm7px
>    (best_val_acc: 0.7208, best_epochs: 0 and 8)
>    - https://wandb.ai/roma-glushko/rock-paper-scissors/runs/29dif98z
>    (best_val_acc: 0.8, best_epoch: 9)
>
> [image: Screenshot 2021-05-23 at 12 29 29]
> <https://user-images.githubusercontent.com/9402690/119255115-98690100-bbc2-11eb-90c9-6c591dbfe629.png>
>
> Also, I tried to set random.seed() right before passing my batch into
> a.Compose() pipeline. That did not really help.
>
> However, when I comment out albumentations from my data pipeline or
> replace it with some pure TF augmentations, I can get my training
> reproducible.
>
> Any clues what's wrong here?
> To Reproduce
>
> Steps to reproduce the behavior:
>
>    1. Clone the project state at 0.1.0-bugrep tag:
>
> git clone --depth 1 --branch 0.1.0-bugrep https://github.com/roma-glushko/rock-paper-scissor
>
>
>    1. Pull dataset:
>
> cd data
>
> kaggle datasets download --unzip frtgnn/rock-paper-scissor
>
>
>
>    1. Install project deps:
>
> poetry install
>
>
>
>    1.
>
>    Uncomment any of the reported augmentations in the config file (they
>    are all commented out in the git):
>
>    https://github.com/roma-glushko/rock-paper-scissor/blob/master/configs/basic_config.py
>    2.
>
>    Run training a couple of times and you get results that differs by a
>    lot:
>
> python train.py
>
>
> Expected behavior
>
> In order to do experiments that analyze impact of different ideas and
> changes, I would like to see my training process reproducible.
> Environment
>
>    - Albumentations version (e.g., 0.1.8): 0.5.2
>    - Python version (e.g., 3.7): 3.8.6
>    - OS (e.g., Linux): Ubuntu 20.10
>    - How you installed albumentations (conda, pip, source): poetry
>    (pip-like)
>    - tensorflow-gpu: 2.5.0 (for the sake of compatibility with RTX3070
>    (ampere arch.))
>
> Additional context
>
> This report is reproduced in a project that is also mentioned in #905
> <https://github.com/albumentations-team/albumentations/issues/905>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/906>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YC6NZF4H4ROOKQT5E3TPDDMBANCNFSM45LQYTDQ>
> .
>
",observe behavior usually want apply validation stage better bug bug could get training work reproducible way added data pipeline thread fixed possible overall snippet reproducible like import o import random import import seed globally fix possible randomness keep experiment reproducible seed seed seed seed unfortunately help get reproducible executed training process time got different also see image also tried set right passing batch pipeline really help however comment data pipeline replace pure get training reproducible wrong reproduce reproduce behavior clone project state tag git clone depth branch pull data install project poetry install file git run training couple time get lot python behavior order analyze impact different would like see training process reproducible environment version python version o pip source poetry sake compatibility ampere arch additional context report project also thread reply directly view,issue,positive,negative,neutral,neutral,negative,negative
846330680,"I wanted to come back and give a simpler example to fully zoom in on the syntax that I must not be understanding. 

This passes

If you have a box at 100/1000 and make a 200 pixel crop starting at 0, the final bbox coordinate should be 0.5 
```
bbox = (0, 0, 0.1,0.1)
crop_height = 200
rows = 1000
h_start = 0
w_start = 0
cropped_bbox = F.bbox_random_crop(bbox=bbox, crop_height=crop_height, crop_width=crop_height, h_start=h_start, w_start=w_start, rows=rows, cols=rows)
cropped_bbox

assert cropped_bbox[3] == bbox[3] / ((crop_height/rows))
assert all([(y >= 0) & (y<=1) for y in list(cropped_bbox)])
```

If you shift everything over 100 pixels, the logic holds.

```
bbox = (0.1, 0.1, 0.2,0.2)
crop_height = 200
rows = 1000
h_start = 0
w_start = 0
cropped_bbox = F.bbox_random_crop(bbox=bbox, crop_height=crop_height, crop_width=crop_height, h_start=h_start, w_start=w_start, rows=rows, cols=rows)
cropped_bbox

assert cropped_bbox[3] == bbox[3] / ((crop_height/rows))
assert all([(y >= 0) & (y<=1) for y in list(cropped_bbox)])
```

but if you have begin the crop at any position besides 0, you get a strange result? 

```
bbox = (0.1, 0.1, 0.2,0.2)
crop_height = 200
rows = 1000
h_start = 0.1
w_start = 0.1
cropped_bbox = F.bbox_random_crop(bbox=bbox, crop_height=crop_height, crop_width=crop_height, h_start=h_start, w_start=w_start, rows=rows, cols=rows)
cropped_bbox
```

```
cropped_bbox
(0.1, 0.1, 0.6, 0.6)
```

The size of the box is correct, but the final bbox start coordinate should be 0,0 since, h_start == bbox[0] and w_start == bbox[1]? 

Digging into the function, you can see that 'get_random_crop_coords' produces a non-unintuitive result?

```
F.get_random_crop_coords(height=1000, width=1000, crop_height=200, crop_width=200, h_start=0.1, w_start=0.1)
(80, 80, 280, 280)
```
I would expect a 200 pixel crop starting at 0.1 with an original image size of 1000 to have crop coordinates of 100,300, not 80,280. Where is the 20 offset coming from?

Going one layer deeper to https://github.com/albumentations-team/albumentations/blob/cc8fbb6e2fcc4f6a4c87a29b6b0784391b0e2db4/albumentations/augmentations/crops/functional.py#L14
```
y1 = int((height - crop_height) * h_start)
```

Why is this not 

```
y1 = int(height  * h_start)
```
The size of the crop does not effect the location of the min coordinate, only the location of the max coordinate. Any crop starting at 0.1 of a 1000 row image should have a y1 coordinate of 100, regardless of crop size?
",come back give simpler example fully zoom syntax must understanding box make crop starting final assert assert list shift everything logic assert assert list begin crop position besides get strange result size box correct final start since digging function see result would expect crop starting original image size crop offset coming going one layer height height size crop effect location min location crop starting row image regardless crop size,issue,negative,positive,neutral,neutral,positive,positive
844487324,"I'm not understanding where the Functional Geometric API is located.
```
(Zooniverse_pytorch) benweinstein@Bens-MacBook-Pro DeepForest-pytorch % pip install git+https://github.com/albumentations-team/albumentations
...
(Zooniverse_pytorch) benweinstein@Bens-MacBook-Pro DeepForest-pytorch % python
Python 3.7.0 (default, Jun 28 2018, 07:39:16)
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import albumentations.augmentations.geometric.functional as FGeometric
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'albumentations.augmentations.geometric'
```
The convention is used throughout the repo. I'm making a custom bbox transform and mimicking the format  here, but the modules don't appear to exist.

https://github.com/albumentations-team/albumentations/blob/cc8fbb6e2fcc4f6a4c87a29b6b0784391b0e2db4/albumentations/augmentations/crops/transforms.py#L8
",understanding functional geometric pip install python python default clang anaconda type help copyright license information import recent call last file line module module convention used throughout making custom transform format appear exist,issue,negative,neutral,neutral,neutral,neutral,neutral
843923138,Try to use [Affine](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Affine) with `fit_output` parameter,try use affine parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
843837633,I don't think this issue is resolved with SafeRotate. SafeRotate compresses the rotated image to fit in the original size. Is it possible to skip the compression step? I.e. [torchvision.transform.RandomRotation](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.RandomRotation) with expand=True  ,think issue resolved rotated image fit original size possible skip compression step,issue,positive,positive,positive,positive,positive,positive
843091883,"Oh my god. I forgot about this PR. Sorry.
Need to update it is to add serialization support.
To do this need to implement `get_transform_init_args_names` method and add this transform to tests.
As example you can look at #888 ",oh god forgot sorry need update add serialization support need implement method add transform example look,issue,positive,negative,negative,negative,negative,negative
843057402,It would be nice if someone approve this. ) I always starting update with adding this functionality into albumentation code...,would nice someone approve always starting update functionality code,issue,negative,positive,positive,positive,positive,positive
842479846,"I got the same problem. The image is a usual image-numpy array with dtype=float32 and pixel-values ranging [0..1].
I'm using 
```
A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=25, val_shift_limit=5)
```

Doh ... while typing this, the error gets obvious, for pixel-values ranging from 0 to 1 you should also use the limits [0, 1].
Might be worth mentioning in the docs.",got problem image usual array ranging error obvious ranging also use might worth,issue,negative,positive,neutral,neutral,positive,positive
840757076,"Hi!
Could you provide a reproducible example?
In next release this transform will be deprecated. Try to install latest library version from Github `pip install git+https://github.com/albumentations-team/albumentations.git` and try to use new implementation of [Perspective](https://albumentations.ai/docs/api_reference/augmentations/geometric/transforms/#albumentations.augmentations.geometric.transforms.Perspective)",hi could provide reproducible example next release transform try install latest library version pip install try use new implementation perspective,issue,negative,positive,positive,positive,positive,positive
840714401,"I have installed albumentations 0.5.2 
 got the error: Expected number, tuple of two number, list of number or StochasticParameter for cval, got <class 'bool'>.",got error number two number list number got class,issue,negative,neutral,neutral,neutral,neutral,neutral
838465844,"I reproduced this problem only with `bboxes`
```python
import albumentations as A
import numpy as np

i = np.random.randint(0, 255, (256, 256), dtype=np.uint8) # image
m = np.zeros((256, 256), dtype=np.uint8) # mask
m[20:30, 20:40] = 1
m[120:130, 120:140] = 2
k = np.random.randint(0, 256, (10, 2)) # keypoints
bboxes = [
    [10, 10, 50, 70, 0],
]

# == This will throw an exception ==
transform = A.Compose([A.CropNonEmptyMaskIfExists(128, 128, p=1)],
                      keypoint_params=A.KeypointParams(format='xy', remove_invisible=True),
                      bbox_params=A.BboxParams(format=""pascal_voc"")
                      )
transform(image=i, mask=m, keypoints=k, bboxes=bboxes)

# == This will work good ==
transform = A.Compose([A.RandomCrop(128, 128, p=1)],
                      keypoint_params=A.KeypointParams(format='xy', remove_invisible=True))
transform(image=i, mask=m, keypoints=k, bboxes=bboxes)
```",problem python import import image mask throw exception transform transform work good transform transform,issue,negative,positive,positive,positive,positive,positive
836449919,"Please see the repository https://github.com/naver-ai/relabel_imagenet
It shows impressive accuracy improvements with CutMix

CutMix implementation: https://github.com/naver-ai/relabel_imagenet/blob/main/utils/data_augment.py#L54
It does mixing within the batch. 

This may be a simpler alternative to using multiple images. Looking forward to see this in albumentations",please see repository impressive accuracy implementation within batch may simpler alternative multiple looking forward see,issue,positive,positive,positive,positive,positive,positive
835202371,"Looks like, it's because `CropNonEmptyMaskIfExists` overrided `update_params` method, which in parent class `BasicTransform` is coded to add height and width to params:

```Python
params.update({""cols"": kwargs[""image""].shape[1], ""rows"": kwargs[""image""].shape[0]})
```

Should `CropNonEmptyMaskIfExists` then call to `super` or?",like method parent class add height width python image image call super,issue,positive,positive,positive,positive,positive,positive
832594374,"> A small suggestion regarding name of this transform - instead `NOf`, can we use `SomeOf`. To me, `SomeOf`' is more appealing in just how it looks in the code.

Thanks @BloodAxe 
done",small suggestion regarding name transform instead use appealing code thanks done,issue,negative,positive,positive,positive,positive,positive
831277650,"A small suggestion regarding name of this transform - instead `NOf`, can we use `SomeOf`. To me, `SomeOf`' is more appealing in just how it looks in the code.",small suggestion regarding name transform instead use appealing code,issue,negative,positive,positive,positive,positive,positive
830985170,"> With that said, would it be better to replace the definitions of `x_min` and `x_max` (and similarly for `y_min` and `y_max`) as follows?
> 
> ```
> x_min = max(int(x - width / 2), 0)
> x_max = min(int(x + width / 2 + 1), cols)
> ```
> 
> Having the `+ 1` in the calculation of `x_max` instead of `x_min` enlarges the bbox by _at most_ 2 pixels to the left and to the right but ensures that the object is still enclosed by the bbox. The `max(..., 0)` and `min(..., cols)` ensure that `x_min` and `x_max` are within acceptable values.

There's a flaw in this idea: the bounding boxes will only get larger for every transformation. This might not be a good idea especially when using a lot of transformations.",said would better replace similarly width min width calculation instead left right object still min ensure within acceptable flaw idea bounding get every transformation might good idea especially lot,issue,positive,positive,positive,positive,positive,positive
830841885,"Thanks @Dipet good point!
I couldn't find a neat way to get OneOf to serialise being a subclass of NOf, so reverted it to the original.
",thanks good point could find neat way get subclass original,issue,positive,positive,positive,positive,positive,positive
830803980,"Good PR, thank you!
Because you added 2 new arguments into pipeline, please, update `_to_dict` method and add add new arguments into it.
It is needed for serialization support. To check that it is work please add new transform into these tests:
- file `test_serialization.py`
    - `test_transform_pipeline_serialization`
    - `test_transform_pipeline_serialization_with_bboxes`
    - `test_transform_pipeline_serialization_with_keypoints`",good thank added new pipeline please update method add add new serialization support check work please add new transform file,issue,positive,positive,positive,positive,positive,positive
830732781,"@BloodAxe Thank you, I look forward to using this feature in Albumentations! I moved the internal functions for SafeRotate into functional.py, and I also moved the code for key points and bounding boxes into functional as well.

I updated the readme, and It seems the PR is green. It's one commit behind the master, but I don't know if that matters. Let me know if there is anything else that should be done before the PR is approved.",thank look forward feature internal also code key bounding functional well green one commit behind master know let know anything else done,issue,positive,negative,negative,negative,negative,negative
830732184,"@Dipet Thank you! I added the new feature into those unit tests.

As for the test_rotate_interpolation test, I see what you mean. I just changed the implementation to work better with these tests, and SafeRotate supports border modes now. It uses one cv2 transformation rather than padding and rotating separately which gets rid of any artifacts it had before.",thank added new feature unit test see mean implementation work better border one transformation rather padding rotating separately rid,issue,positive,positive,positive,positive,positive,positive
830269182,"Good work! Thanks for PR.
Please add transform into these tests:

- test_serialization.py
    -  test_augmentations_serialization
    - test_augmentations_serialization_with_custom_parameters
    - test_augmentations_for_bboxes_serialization
    - test_augmentations_for_keypoints_serialization
 
Also please look to `test_rotate_interpolation` in `tests_transforms` I am not sure that these implementations will use `INTER_NEAREST` for masks.",good work thanks please add transform also please look sure use,issue,positive,positive,positive,positive,positive,positive
830127583,"Greetings! Thanks for your PR. I believe it would be a valuable addition to Albumentations as it was also requested in past.  

Would you mind adding a few changes to your change-set, in order to ensure new code meets our coding conventions?

In particular:
1) Can you rename `__rotated_img_size__`, `__process_image__` to have more clear name reflecting what these functions do and  move them to `functional.py`?
2) Add new transformation to README.md (Failing unit-test will guide you how to generate new table with augmentations)
3) Ensure that PR is green (No flake & black issues remaining)

Overall, great work. Looking forward to get it merged. ",thanks believe would valuable addition also past would mind order ensure new code particular rename clear name reflecting move add new transformation failing guide generate new table ensure green flake black overall great work looking forward get,issue,positive,positive,neutral,neutral,positive,positive
828426445,"I agree, it's probably the best approach.",agree probably best approach,issue,positive,positive,positive,positive,positive,positive
828356723,"Try to inherit from `ImageOnlyTransform` . And use this transform as a common albumentations transform
```python
class mask_subset_aug(A.ImageOnlyTransform):
    def __init__(self, target_values, always_apply=False, p=0.5):
        super().__init__(always_apply, p)
        self.blend_color_values = target_values

    def get_transform_init_args_names(self):
        return ""target_values"",

    def blend_mask(self, image, spots_mask, blend_color, blend_amount):
        pass

    def apply(self, img, mask=None, **params):
        mask_subset = intersect(self.random_shapes, mask)
        blend_color = sample(self.blend_color_values)
        blend_amount = uniform(0.2, 0.45)
        return self.blend_mask(image, mask_subset, blend_color, blend_amount)

    def random_shapes(image_size):
        pass

    @property
    def targets_as_params(self):
        return [""mask""]

    def get_params_dependent_on_targets(self, params):
        return params
```",try inherit use transform common transform python class self super self return self image pas apply self intersect mask sample uniform return image pas property self return mask self return,issue,positive,positive,neutral,neutral,positive,positive
828262331,"> It looks like we can replace check `isinstance(param, (list, tuple))` with `isinstance(param, Sequence)` and everything will work fine. `Sequence` - is a standard type from typing module

Like the `Sequence` approach! ",like replace check param list param sequence everything work fine sequence standard type module like sequence approach,issue,positive,positive,positive,positive,positive,positive
828261928,"It seems hydra (Tested on 1.1) `instantiate` has additional parameter `_convert_`:
```
   _convert_: Conversion strategy
        none    : Passed objects are DictConfig and ListConfig, default
        partial : Passed objects are converted to dict and list, with
                  the exception of Structured Configs (and their fields).
        all     : Passed objects are dicts, lists and primitives without
                  a trace of OmegaConf containers
```

So adding `_convert_` should do the job:
```
augmentations:
- _target_: albumentations.HorizontalFlip
  p: 0.5
- _target_: albumentations.HueSaturationValue
  _convert_: all
  sat_shift_limit:
  - [10, 20]
```",tested additional parameter conversion strategy none default partial converted list exception structured without trace job,issue,negative,negative,neutral,neutral,negative,negative
828202089,"It looks like we can replace check `isinstance(param, (list, tuple))` with `isinstance(param, Sequence)` and everything will work fine. `Sequence` - is a standard type from typing module",like replace check param list param sequence everything work fine sequence standard type module,issue,positive,positive,positive,positive,positive,positive
828193844,"Thanks for the suggestion @BloodAxe! Calling `OmageConf.to_container()` before calling `instantiate()` won't solve the problem, because if you pass a tuple or list to `instantiate()`, it will convert it to `ListConfig`. The only way to work around the problem that I came up with is creating a function that converts the parameters before instantiating the augmentations:

```python
def instantiate_transform(transform, *args, **kwargs):
    kwargs = {k: tuple(v) if isinstance(v, ListConfig) else v
              for k, v in kwargs.items()}
    transform_class = getattr(albumentations, transform)
    return transform_class(*args, **kwargs)
```

The configuration then has to be changed slightly to use this function:

```yaml
augmentations:
- _target_: my_app.instantiate_transform
  transform: HorizontalFlip
  p: 0.5
- _target_: my_app.instantiate_transform
  transform: HueSaturationValue
  sat_shift_limit:
  - -10
  - 20
```

I wouldn't add a dependency to OmegaConf either, because it will only solve the problem in this particular case (users of Hydra). It's actually quite idiomatic in Python to rely on the fact that you can provide anything that acts like a list (be it a NumPy array or a PyTorch tensor) in place of a list, and that's why it's considered bad practice to check arguments for the exact type. (See what I wrote in ""Additional context"".)
The proper way would be to check whether the argument does what you want, for example check that ` param[0]` and `param[1]` are scalars.",thanks suggestion calling calling wo solve problem pas list convert way work around problem came function python transform else transform return configuration slightly use function transform transform would add dependency either solve problem particular case actually quite idiomatic python rely fact provide anything like list array tensor place list considered bad practice check exact type see wrote additional context proper way would check whether argument want example check param param,issue,negative,negative,neutral,neutral,negative,negative
827829149,"I think you can easily solve this issue by calling `OmegaConf.to_container` while calling `instantiate(a)`: `instantiate(OmegaConf.to_container(a))`. I'm seriously against adding extra dependency to hydra (although I'm using hydra by myself as well) to Albumentations to fix this issue. To me, `tuple` and `list` are standard containers, while `ListConfig` and `DictConfig` are not. One should not expect compatibility with custom classes that ""pretend"" to be `tuple` or `dict`.

Please let us know whether `to_container` solve this issue.",think easily solve issue calling calling seriously extra dependency although well fix issue list standard one expect compatibility custom class pretend please let u know whether solve issue,issue,positive,positive,positive,positive,positive,positive
827217504,"Hi, christian-cahig
ya its ok when min_visibility = 0.999.
need to notice about it.
thanks",hi ya need notice thanks,issue,negative,positive,positive,positive,positive,positive
826447109,"> Bboxes will generate tags larger than 1 when using 'albumentations' for data augmentation. Why? What should be done?
> 
> The error is as follows:
> ValueError: Expected x_max for bbox (0.284, 0.15315315315315314, 1.002, 0.993993993993994, 6.0) to be in the range [0.0, 1.0], got 1.002.

I raised an issue (https://github.com/albumentations-team/albumentations/issues/883) that could possibly explain this.",generate data augmentation done error range got raised issue could possibly explain,issue,negative,neutral,neutral,neutral,neutral,neutral
826072065,"sorry, my fault, a single spelling mistake in using attributes in `transform`. Now only noticed that there is a difference in `mask` (for one image) and `masks` (for multiple images)",sorry fault single spelling mistake transform difference mask one image multiple,issue,negative,negative,negative,negative,negative,negative
826066493,"> Please attach a fully reproducible minimal example. It's very likely you have some permutation that mess channel order somewhere before random crop.

@BloodAxe I just create a mask image using annotations instead of opening the image through opencv, here is the code I used below:
        image = utils.read_image(dataset_dict[""file_name""], format=self.image_format)  #array
        h, w, b = image.shape
        name=dataset_dict[""file_name""]
        
        name = name.split('/')[-1]
        utils.check_image_size(dataset_dict, image)
        output = np.zeros_like(image)
        bboxes = [ann[""bbox""] for ann in dataset_dict['annotations']]
        segmentations = [ann[""segmentation""] for ann in dataset_dict['annotations']]
        masks = convert_coco_poly_to_mask(segmentations, h, w)
        masks = masks.numpy()
        np_msk = np.array(masks,dtype='bool')
        mask_array = np.moveaxis(np_msk, 0, -1)
        num_instances = masks.shape[0]   #mask count
        print('num_instances',num_instances,name)
        mask_array_instance = []
        for i in range(num_instances):
            mask_array_instance.append(mask_array[:, :, i:(i+1)])
            masks = np.where(mask_array_instance[i] == True,255,output)
        labels = [ann['category_id'] for ann in dataset_dict['annotations']]
        class_labels = [CLASSES[label] for label in labels]
        if list(masks) and bboxes is not None:

             transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels, masks=list(masks))

There is no transformation before albumentation
",please attach fully reproducible minimal example likely permutation mess channel order somewhere random crop create mask image instead opening image code used image array name image output image ann ann ann segmentation ann mask count print name range true ann ann class label label list none transform transformation,issue,positive,negative,neutral,neutral,negative,negative
826063450,Please attach a fully reproducible minimal example. It's very likely you have some permutation that mess channel order somewhere before random crop.,please attach fully reproducible minimal example likely permutation mess channel order somewhere random crop,issue,negative,negative,negative,negative,negative,negative
825768702,"I agree this would be nice, if not for performance than just for ease of use. The best case scenario would be to be able to use transforms directly against a batched tensor in Pytorch or TF (channel first formatting also?), but even against a batched np array would make things cleaner. Here's what I'm using right now, the necessity of using named arguments and returning a dict also makes things rather annoying. 


```
import albumentations as A
import numpy as np
import torch

def apply_along_batch(func, arr):
    '''
    Apply a callable over a batch of 2+D numpy arrays or Pytorch tensors
    
    args:
        func (callable): A function that takes in a 2+D numpy array and
        returns a numpy array of the same dimensions
        
        arr (numpy.ndarray): A batched numpy (or pytorch) array of the shape (BATCH, H, W, C, ...)
        
    returns:
        (numpy.ndarray): A numpy array with func applied along batch axis (BATCH, H, W, C, ...)
    '''
    res = map(func, arr)
    res = map(lambda x: np.array(x), res)
    res = list(res)
    return(np.array(res))


batched_tensor = torch.rand((128, 64, 64, 3))

transform = A.RandomCrop(width=32, height=32)


batched_tensor_tfmd = apply_along_batch(func = lambda x: transform(image=x)['image'], 
                  arr = batched_tensor)
```",agree would nice performance ease use best case scenario would able use directly tensor channel first also even array would make cleaner right necessity also rather annoying import import import torch apply callable batch callable function array array array shape batch array applied along batch axis batch map map lambda list return transform lambda transform,issue,positive,positive,positive,positive,positive,positive
824796690,I see. But having it supporting batches might beneficial for the `tf.data` pipeline since `tf.numpy_function` is already very expensive. ,see supporting might beneficial pipeline since already expensive,issue,positive,negative,negative,negative,negative,negative
824778168,"Library applies transformations on CPU, we may achieve better performance with batches, but not so much than  if we will use GPU for processing. I think we will see better performance just only if we will use small images.
For this reason, the task of supports batches is of very low priority.",library may achieve better performance much use think see better performance use small reason task low priority,issue,positive,positive,positive,positive,positive,positive
824771225,"Library works only with named arguments and numpy array. For this reason, it is incompatible with torchvision.
Try to wrap transforms using this class
```python
class Transforms:
    def __init__(self, transforms: A.Compose):
        self.transforms = transforms

    def __call__(self, img, *args, **kwargs):
        return self.transforms(image=np.array(img))
```",library work array reason incompatible try wrap class python class self self return,issue,negative,neutral,neutral,neutral,neutral,neutral
823066673,"I figured if I load up the masks in grayscale `mask = cv2.imread(self.maskPaths[idx], 0)` this problem is gone. ",figured load mask problem gone,issue,negative,neutral,neutral,neutral,neutral,neutral
821943889,"I don't see what to comment here. This request is not an issue, but rather request on how to transform one dict to another. We do not provide here support for such kind of requests. Please take a time to read our example notebooks.",see comment request issue rather request transform one another provide support kind please take time read example,issue,positive,positive,positive,positive,positive,positive
821028124,"Hello,
Categorical targets in torch must be LongTensor and thus require int64 inputs it seems, so yeah i convert my transformed image right before passing them to the loss. 
A warning in these transfo would be great, simply to avoid a weird half-reproducible bug... (i haven't checked inside opencv why this behavior appears).",hello categorical torch must thus require yeah convert image right passing loss warning would great simply avoid weird bug checked inside behavior,issue,negative,positive,positive,positive,positive,positive
820568233,"I would love to get this feature merged and wonder what is currently blocking this PR. If any input is required, I am happy to help. @Dipet ",would love get feature wonder currently blocking input happy help,issue,positive,positive,positive,positive,positive,positive
820274265,"Do not use `int64` for masks and images. Library works incorrectly and very slow with this type of images.
I think that you don't need all `2^64` combinations of values for mask.
If you need `int64` dtype for mask just convert it is after transformation.

Looks like we need to add check that image and mask does not have `int64` or `uint64` dtype.",use library work incorrectly slow type think need mask need mask convert transformation like need add check image mask,issue,negative,negative,negative,negative,negative,negative
820250601,Check your bboxes. In error message I see that bbox contains only `NaN` values.,check error message see nan,issue,negative,neutral,neutral,neutral,neutral,neutral
820248453,"Check that albumentations upgraded correctly and you use correct interpreter and environment.
Try to print library version `print(albumentations.__version__)`",check correctly use correct interpreter environment try print library version print,issue,negative,neutral,neutral,neutral,neutral,neutral
820241379,"Yes we saw this blog post to. At this time we are thinking how to better write general test to prevent usage of `np.random`
But problem that some transforms uses `np.random` to produce random arrays (for example `GaussNoise`).
We will try to fix these problems as soon as possible",yes saw post time thinking better write general test prevent usage problem produce random example try fix soon possible,issue,negative,positive,neutral,neutral,positive,positive
819832047,Please use the issue template that helps to assemble informative request with all the necessary information to reproduce/understand your issue.  ,please use issue template assemble informative request necessary information issue,issue,negative,neutral,neutral,neutral,neutral,neutral
819811438,"I am seeing this the same/similar error for images that have no objects in them:

```
  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/bbox_utils.py"", line 330, in check_bbox
    ""to be in the range [0.0, 1.0], got {value}."".format(bbox=bbox, name=name, value=value)
ValueError: Expected x_min for bbox (tensor(nan), tensor(nan), tensor(nan), tensor(nan), tensor(-9223372036854775808)) to be in the range [0.0, 1.0], got nan.
```

Any advice on how I can fix this? If it helps, I'm following [this](https://www.kaggle.com/meemr5/traffic-light-detection-pytorch-starter) tutorial",seeing error file line range got value tensor nan tensor nan tensor nan tensor nan tensor range got nan advice fix following tutorial,issue,negative,neutral,neutral,neutral,neutral,neutral
819619588,"Please double-check you environment, since the sequence of commands that
you attach can easily mess the env. Clean uninstall and installation of
latest release should work

Ср, 14 апр. 2021 г. в 10:54, amenevatanparast ***@***.***>:

> hi
> I have this problem ""module 'albumentations' has no attribute 'Lambda""
> I test
> !pip install -U albumentations>=0.3.0 --user
> !pip install -U git+https://github.com/albu/albumentations --no-cache-dir
> pip install --upgrade albumentations
> pip install --upgrade git+
> https://github.com/albumentations-team/albumentations
>
> but the error is still
> please help me
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/851#issuecomment-819311992>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YDQLQDVDQFQYSVGFZLTIVCZ3ANCNFSM4YUZCEEA>
> .
>
",please environment since sequence attach easily mess clean installation latest release work hi problem module attribute test pip install user pip install pip install upgrade pip install upgrade error still please help thread reply directly view,issue,positive,positive,positive,positive,positive,positive
819311992,"hi
I have this problem ""module 'albumentations' has no attribute 'Lambda""
I test
!pip install -U albumentations>=0.3.0 --user
!pip install -U git+https://github.com/albu/albumentations --no-cache-dir
pip install --upgrade albumentations
pip install --upgrade git+https://github.com/albumentations-team/albumentations

but the error is still
please help me",hi problem module attribute test pip install user pip install pip install upgrade pip install upgrade error still please help,issue,negative,neutral,neutral,neutral,neutral,neutral
818928072,"fyi, need to ``pip uninstall albumentations`` then ``pip install git+https://github.com/albumentations-team/albumentations.git``",need pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
817858253,"@ozen thanks for pointing this out. In fact, `GaussianBlur` uses np.random for `ksize` but not for `sigma`.
`GaussNoise` sets the np.random.RandomState using the python PRNG, which avoids this problem. I noticed that this was done with other augmentations. 
As I said my notebook is not a thorough test, so I would expect more done to see which augs/params are affected.

There is an issue on the pytorch side: https://github.com/pytorch/pytorch/issues/5059",thanks pointing fact sigma python problem done said notebook thorough test would expect done see affected issue side,issue,negative,positive,positive,positive,positive,positive
817630921,"I think you're mixing incompatible things. Albumentations is built for training a model. You're trying to use it for Test-Time-Augmentation, which is entirely out of the scope of this library. Therefore the short answer is no.

I'll take this request as an opportunity to advertise my library `pytorch-toolbelt` which offers wide spectrum of TTA, including 5- and 10-crop TTA with very easy & flexible API: https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/inference/tta.py#L112

So you can write something like this:
```
from pytorch_toolbelt.inference import tta

# Validation
with torch.no_grad():
  model.eval()
  for image, labels in valid_loader:
     predictions = tta.fivecrop_image2label(model, image.cuda(), crop_size=224)
   
```
",think incompatible built training model trying use entirely scope library therefore short answer take request opportunity advertise library wide spectrum easy flexible write something like import validation image model,issue,positive,positive,neutral,neutral,positive,positive
817534656,"@YassineYousfi as I said albumentations uses both in different places (I think more than IAA transforms). You wouldn't see it if you test for specifically the parts it uses numpy. 
For example, Rotate uses Python, GaussianBlur uses numpy.",said different think would see test specifically example rotate python,issue,negative,neutral,neutral,neutral,neutral,neutral
817436601,"I also saw the blog post on Twitter and did a quick sanity check: 
https://colab.research.google.com/drive/1ZW_7NslpNVhDt0eRSBECdJN0Gmogjuxo?usp=sharing
I believe Albumentations uses the python RNG (besides IAA transforms I think), my tests didn't show identical samples as in the blog post. But I would love to see a more thorough test! ",also saw post twitter quick sanity check believe python besides think show identical post would love see thorough test,issue,positive,positive,positive,positive,positive,positive
817406006,"@ternaus @BloodAxe I also came across the blog post mentioned by @ozen, highlighting the issue with using PyTorch dataloaders (num_workers > 1) together with numpy RNG, which leads to identical random behaviour by all processes. It seems like a critical issue to check for Albumentations, as an augmentation library. ",also came across post issue together identical random behaviour like critical issue check augmentation library,issue,negative,negative,negative,negative,negative,negative
817092170,"Hey @NesterukSergey, thanks. Looks good to me! We can proceed with implementing this feature with Albumetnations.

I propose to create a new package `augmentors` in the [`albumentations` directory](https://github.com/albumentations-team/albumentations/tree/master/albumentations) and place all the required code into this package.",hey thanks good proceed feature propose create new package directory place code package,issue,positive,positive,positive,positive,positive,positive
816777203,"Uhm, I don't see why we should change existing class hierarchy, given the fact that composition classes differs a lot from regular transformation classes. Your solution looks clean and I honestly see nothing that worth changing on our side",see change class hierarchy given fact composition class lot regular transformation class solution clean honestly see nothing worth side,issue,positive,positive,positive,positive,positive,positive
816510029,Maybe just separate `.functional` to subpackage like PyTorch do?,maybe separate subpackage like,issue,negative,neutral,neutral,neutral,neutral,neutral
815511674,"@BloodAxe I appreciate the team efforts for putting up such a great library for augmentations. I just want to re-raise the problem with **RandomScale** augmentation behaviour, since this still remains unresolved and can cause a silent bug in deep learning codebases, significantly harming training processes. 

As previously raised by others, with the current implementation, using a scale_limit =(0.5, 1.5), counter-intuitively scales data by a factor of (1.5, 2.5), thus always resulting in upsampled images. Reading the docs, one would expect this augmentation to uniformly pick a random value x in range (0.5,1.5) on the fly and resize the data according to the random scale selected. ",appreciate team great library want problem augmentation behaviour since still remains unresolved cause silent bug deep learning significantly training previously raised current implementation scale data factor thus always resulting reading one would expect augmentation uniformly pick random value range fly resize data according random scale selected,issue,positive,positive,neutral,neutral,positive,positive
814903678,"Sorry, the problem was caused by I called `__getitem__()` twice to display the result, it's a stupid problem, thanks for your reply. 😞 ",sorry problem twice display result stupid problem thanks reply,issue,negative,negative,negative,negative,negative,negative
814797447,"I again can not reproduce the problem
```python
from torch.utils.data import Dataset
from torchvision import transforms
import numpy as np
import albumentations as A
import matplotlib.pyplot as plt
import random


class CrossMoDADataSet(Dataset):
    def __init__(self):
        self.augment = self.build_augment()
        self.to_tensor_and_normalize = transforms.Compose([transforms.ToTensor()])

    def __len__(self):
        return 1

    def __getitem__(self, index):
        image = np.load(""D:\\crossmoda_22_ceT1_40.npy"")
        label = np.load(""D:\\crossmoda_22_Label_40.npy"")

        image = (image / np.max(image) * 255).astype(np.uint8)  # scale the value to [0, 255]
        label = label.astype(np.uint8)
        image_orig = image.copy()
        label_orig = label.copy()

        if self.augment is not None:
            augmented = self.augment(image=image, mask=label)
            image = augmented['image']
            label = augmented['mask']

        return image, label, image_orig, label_orig

    def build_augment(self):
        return A.Compose([A.OneOf([A.HorizontalFlip(p=0.5),
                                   A.VerticalFlip(p=0.5),
                                   A.Transpose(p=0.5)], p=1),
                          A.RandomRotate90(p=0.5),
                          A.GridDistortion(p=0.5)])


seed = 0
random.seed(seed)
np.random.seed(seed)
a, b, c, d = CrossMoDADataSet()[0]

plt.subplot(221, title=""original"")
plt.imshow(c, vmin=0, vmax=255, cmap=""gray"")
plt.subplot(222, title=""original"")
plt.imshow(d, vmin=0, vmax=1, cmap=""gray"")

plt.subplot(223, title=""result"")
plt.imshow(a, vmin=0, vmax=255, cmap=""gray"")
plt.subplot(224, title=""result"")
plt.imshow(b, vmin=0, vmax=1, cmap=""gray"")

plt.show()
```
![image](https://user-images.githubusercontent.com/7512250/113851707-73503880-97a4-11eb-9623-a7a6d1947e6d.png)
",reproduce problem python import import import import import import random class self self return self index image label image image image scale value label none augmented image augmented label augmented return image label self return seed seed seed original gray original gray result gray result gray image,issue,positive,positive,neutral,neutral,positive,positive
814576717,"@Dipet @BloodAxe 

One more thing, because the format of image source file was `.nii`, so I firstly convert it to `np.ndarray`.

You can get the image and mask `np.ndarray` file from: 
- https://drive.google.com/file/d/12cXJqkS9nMAYbaMteKFULZnBC59A_2Fc/view?usp=sharing
- https://drive.google.com/file/d/1kC5My4VJmaMylWoJAXyzKZOdhkC6Apse/view?usp=sharing

And this is the full code, you may modify it to reproduce the problem:
```python
# -*- coding: utf-8 -*
from torch.utils.data import Dataset
from torchvision import transforms
import torch
import json
import numpy as np
import os
import albumentations as A


class CrossMoDADataSet(Dataset):

    def __init__(self, image_dir, json_file, resize_size=None, crop_size=None, is_aug=False):
        self.image_list, self.label_list = self.get_path_list(image_dir, json_file)
        self.preprocess = self.build_preprocess(resize_size, crop_size)
        self.augment = self.build_augment() if is_aug else None
        # self.to_tensor_and_normalize = transforms.Compose([transforms.ToTensor(), transforms.Normalize()])  # TODO: modify normalize params
        self.to_tensor_and_normalize = transforms.Compose([transforms.ToTensor()])

        print('Info: Scan {} images from {}'.format(self.__len__(), json_file))

    def __len__(self):
        return len(self.image_list)

    def __getitem__(self, index):
        image = np.load(self.image_list[index])
        label = np.load(self.label_list[index]) if self.label_list is not None else None

        image = (image / np.max(image) * 255).astype(np.uint8)  # scale the value to [0, 255]
        label = label.astype(np.uint8)

        if self.preprocess is not None:
            augmented = self.preprocess(image=image, mask=label)
            image = augmented['image']
            label = augmented['mask']

        if self.augment is not None:
            augmented = self.augment(image=image, mask=label)
            image = augmented['image']
            label = augmented['mask']

        return self.to_tensor_and_normalize(image), torch.from_numpy(label) if label is not None else None

    def get_path_list(self, image_dir, json_file):
        with open(json_file) as f:
            json_data = json.load(f)

        image_list = [os.path.join(image_dir, i['npy_path']) for i in json_data]
        if 'label_path' in json_data[0]:
            label_list = [os.path.join(image_dir, i['label_path']) for i in json_data]
        else:
            label_list = None

        return image_list, label_list

    def build_augment(self):
        return A.Compose([A.OneOf([A.HorizontalFlip(p=0.5),
                                   A.VerticalFlip(p=0.5),
                                   A.Transpose(p=0.5)], p=1),
                          A.RandomRotate90(p=0.5),
                          A.GridDistortion(p=0.5)])
        # return A.Compose([
        #     # A.HorizontalFlip(p=0.5),
        #     # A.VerticalFlip(p=0.5),
        #     # A.Transpose(p=0.5),
        #     A.RandomRotate90(p=0.5),
        #     # A.GridDistortion(p=0.5)
        # ])
        # return None

    def build_preprocess(self, resize_size, crop_size):
        transform_list = []
        if resize_size is not None:
            transform_list.append(A.Resize(height=resize_size[0], width=resize_size[1], p=1))
        if crop_size is not None:
            transform_list.append(A.CenterCrop(height=crop_size[0], width=crop_size[1], p=1))

        return A.Compose(transform_list) if len(transform_list) > 0 else None
```",one thing format image source file firstly convert get image mask file full code may modify reproduce problem python import import import torch import import import o import class self else none modify normalize print scan self return self index image index label index none else none image image image scale value label none augmented image augmented label augmented none augmented image augmented label augmented return image label label none else none self open else none return self return return return none self none none return else none,issue,negative,positive,positive,positive,positive,positive
814517819,"I think about this the other way around: what task do they make sense for? Because it's not clear to me what their intended purpose is from reading the documentation [here](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/) nor the docstring itself:

>     label_fields (list): list of fields that are joined with boxes, e.g labels.
>        Should be same type as boxes.

The only purpose that comes to my mind is for tracking who's who if there's the possibility that your Compose pipeline makes one or more boxes disappear. Are there other use cases for the label fields?",think way around task make sense clear intended purpose reading documentation list list type purpose come mind possibility compose pipeline one disappear use label,issue,negative,positive,positive,positive,positive,positive
814486305,"@aparico By itself, `LongestMaxSize` will resize the image and preserve aspect ratio. For example, if my image is `(2100, 3200)` and I use `LongestMaxSize(max_size=512)` then the result will be `(336, 512)`.

However, you can add additional transforms to your composition pipeline after `LongestMaxSize` to apply cropping or padding. E.g., you could use [`PadIfNeeded`](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PadIfNeeded) to ensure the result is square `(512, 512)` using the padding mode of your choice.",resize image preserve aspect ratio example image use result however add additional composition pipeline apply padding could use ensure result square padding mode choice,issue,negative,neutral,neutral,neutral,neutral,neutral
814253056,"I can not reproduce the problem.
Please, provide random seed when you can reproduce the problem.
```python
import cv2 as cv
import albumentations as A
import matplotlib.pyplot as plt
import random
import numpy as np

seed = 42
random.seed(seed)
np.random.seed(seed)

img = cv.imread(""D:\\img.png"")
mask = cv.imread(""D:\\mask.png"", cv.IMREAD_GRAYSCALE)
mask = (mask > 128).astype(np.uint8)

aug = A.Compose([
      A.OneOf([
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),
            A.Transpose(p=0.5)
      ], p=1),
      A.RandomRotate90(p=0.5),
      A.GridDistortion(p=0.5)
])

res = aug(image=img, mask=mask)

plt.subplot(221, title=""original"")
plt.imshow(img, vmin=0, vmax=255)
plt.subplot(222, title=""original"")
plt.imshow(mask, vmin=0, vmax=1, cmap=""gray"")

plt.subplot(223, title=""result"")
plt.imshow(res[""image""], vmin=0, vmax=255)
plt.subplot(224, title=""result"")
plt.imshow(res[""mask""], vmin=0, vmax=1, cmap=""gray"")

plt.show()
```
![image](https://user-images.githubusercontent.com/7512250/113744225-3422d880-970d-11eb-8b67-68e7e9110ec1.png)
",reproduce problem please provide random seed reproduce problem python import import import import random import seed seed seed mask mask mask original original mask gray result image result mask gray image,issue,negative,negative,neutral,neutral,negative,negative
814140086,Please use the issue template to create a bug report. This helps us to reproduce your issue. ,please use issue template create bug report u reproduce issue,issue,positive,neutral,neutral,neutral,neutral,neutral
814137694,"> Indeed. It seems you have pretty old version of the library. Would you mind updating to 0.5+ and re-run your pipeline? I beleive it should be fixed in recent releases.

I have updated it to version 0.5.2, but I'm sorry that the problem remained. 😢 ",indeed pretty old version library would mind pipeline fixed recent version sorry problem,issue,negative,negative,neutral,neutral,negative,negative
814070144,Indeed. It seems you have pretty old version of the library. Would you mind updating to 0.5+ and re-run your pipeline? I beleive it should be fixed in recent releases.,indeed pretty old version library would mind pipeline fixed recent,issue,negative,positive,positive,positive,positive,positive
813206767,"Hi. No, it works in a different way.
The total number of samples in your train does not change. Rather the
samples themselves change. Probabilities in transformations define the odds
of particular transformation being applied.


Пн, 5 апр. 2021 г. в 04:49, BaeSukkyoung ***@***.***>:

> Hi ~
> I want to ask some simple(?) question.
> While I use one of albumentation on my training, I'm curious about how the
> scale of training data changes.
> For example, I use 1 images of trainingset and Rotate(p=0.5),
> I'm curious to see if the basic imgae entered as trainingset is
> unconditionally included, and if Rotate is used, one more image is added so
> totally 2 images and if it is not used, it becomes 1 as it was.
> Is it right ?
> Thanks:)
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/866>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YAKUGA4C5TIY3FDVXLTHEJJ5ANCNFSM42MAQDWQ>
> .
>
",hi work different way total number train change rather change define odds particular transformation applied hi want ask simple question use one training curious scale training data example use rotate curious see basic unconditionally included rotate used one image added totally used becomes right thanks thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
811601497,"Before data augmentation, the length and width of GT bboxes are subtracted by one pixel, which has little effect on the label bounding box, but it can prevent the overflow of values and prevent the generation of labels greater than 1.

labels format: yolo
image.shape[0]: width of image
image.shape[1]: length of image
bboxes: [[x, y, w, h, class], [x, y, w, h, class], ...]

```python
for i in range(len(bboxes)):
    bboxes[i][2] = np.abs(bboxes[i][2] - 0.5 / image.shape[0])
    bboxes[i][3] = np.abs(bboxes[i][3] - 0.5 / image.shape[1])
```
To further reduce the impact of processing, I only subtracted 0.5 pixels from the code.",data augmentation length width one little effect label bounding box prevent overflow prevent generation greater format width image length image class class python range reduce impact code,issue,negative,positive,positive,positive,positive,positive
811600969,"I have found a solution to this problem.
Before data augmentation, the length and width of GT bboxes are subtracted by one pixel, which has little effect on the label bounding box, but it can prevent the overflow of values and prevent the generation of labels greater than 1.

labels format: yolo
image.shape[0]: width of image
image.shape[1]: length of image
bboxes: [[x, y, w, h, class], [x, y, w, h, class], ...]

```python
for i in range(len(bboxes)):
    bboxes[i][2] = np.abs(bboxes[i][2] - 0.5 / image.shape[0])
    bboxes[i][3] = np.abs(bboxes[i][3] - 0.5 / image.shape[1])
```
To further reduce the impact of processing, I only subtracted 0.5 pixels from the code.",found solution problem data augmentation length width one little effect label bounding box prevent overflow prevent generation greater format width image length image class class python range reduce impact code,issue,negative,positive,positive,positive,positive,positive
811147416,"What’s the use case for this feature would be?

Ср, 31 марта 2021 г. в 17:54, Mert Yilmaztürk ***@***.***>:

> Currently one cannot apply a transformation e.g. to a segmentation mask
> without passing an image as an input. It would be great if passing an image
> parameter becomes optional.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/863>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YG2ZWYKUSKPDY6KMZDTGMZQRANCNFSM42E25K3Q>
> .
>
",use case feature would currently one apply transformation segmentation mask without passing image input would great passing image parameter becomes optional thread reply directly view,issue,positive,positive,positive,positive,positive,positive
810136114,"@creafz 
Thanks!
But I wanna augmenting input data by a determinded value. The value is the hyper-parameters. I wan to optimize it.
I have achieved it in this way. I inherit the class and reconstructed the function `get_params(self).`
e.g.:
def get_params(self):
return {""random_state"": 1000}",thanks wan na input data value value wan optimize way inherit class reconstructed function self self return,issue,positive,negative,neutral,neutral,negative,negative
810116683,"Hey @hwx10 

As a quick workaround, you can set a seed value before augmenting input data, e.g.:


```
import random
import albumentations as A

transform = A.Compose([ A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1), ])

random.seed(1000)
augmented = transform(image=image, mask=mask)
```",hey quick set seed value input data import random import transform augmented transform,issue,negative,negative,neutral,neutral,negative,negative
810068654,"Hey, @hwx10 

Generally speaking, there are no well-established rules of which augmentations you should apply to images from a particular domain (medical images in your case). There are two popular ways of applying augmentations. You could try them both and see which works better.

1. Apply a lot of strong augmentations that will heavily alter input images, and those images won't look like real images.
2. Apply only those augmentations that will make augmented images look like real images. 

Option 1 is more suitable if your dataset is small, while option 2 usually works the same or better than option 1 for large datasets.
",hey generally speaking apply particular domain medical case two popular way could try see work better apply lot strong heavily alter input wo look like real apply make augmented look like real option suitable small option usually work better option large,issue,positive,positive,positive,positive,positive,positive
804742690,Maybe you want to see [this](https://albumentations.ai/docs/faq/#augmentations-have-a-parameter-named-p-that-sets-the-probability-of-applying-that-augmentation-but-they-also-have-the-always_apply-parameter-that-can-either-be-true-or-false-what-is-the-difference-between-p-and-always_apply-is-always_applytrue-equals-to-p10). The document gives a good explanation.,maybe want see document good explanation,issue,negative,positive,positive,positive,positive,positive
804355295,"Update:
- Fixed bboxes transformation. The previous did not work correctly
- Added test when `keep_size=True`
- Get scale factor from matrix and change keypoint scale using it is",update fixed transformation previous work correctly added test get scale factor matrix change scale,issue,negative,negative,neutral,neutral,negative,negative
803980387,"Hello!
They are different because you write config, not the pipeline code.
These augmentation does not implemented inside library so I think it is impossible to describe it using these config. For more information on how to describe a custom transformation within configurations, I advise you to ask this question in the MMDetection library",hello different write pipeline code augmentation inside library think impossible describe information describe custom transformation within advise ask question library,issue,negative,negative,negative,negative,negative,negative
803969301,Should we also add a unit-test to this fix to help in future?,also add fix help future,issue,negative,neutral,neutral,neutral,neutral,neutral
803906378,"Oh, no. It looks like the problem is inside our code with `keep_size` argument. I need more time to check this.",oh like problem inside code argument need time check,issue,negative,neutral,neutral,neutral,neutral,neutral
803891689,"Hello!
Thank you for information. Looks like a problem with the transform functions of OpenCV and skimage.
OpenCV and skimage give the same artifacts.
Unfortunately I don't know how to solve this problem because it depends on the behavior of the backend functions.",hello thank information like problem transform give unfortunately know solve problem behavior,issue,negative,negative,negative,negative,negative,negative
797330838,"This augmentation is not yet in the pip version of the library, try installing the library from git.
`pip install git+https://github.com/albumentations-team/albumentations`",augmentation yet pip version library try library git pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
796628777,"Hey @creafz 
We have tried different approach to solve this issue but still no progress. 
We have also tried your suggestion an found out that Albumentations works fine, the issue raises when generator returns the images.
We think that there is a conflict between Albumentation and this Keras Module.

Actually, we have diverted our workflow to tf.data to be able to use Albumentations.
",hey tried different approach solve issue still progress also tried suggestion found work fine issue generator think conflict module actually able use,issue,negative,positive,positive,positive,positive,positive
793435777,"I don't understand what's wrong. Replay compose does not work in your case or what?
For me it is works ok. And yes, parameters are list of lambdas https://github.com/albumentations-team/albumentations/blob/ae50578a46e5981eccaafee311a3ee0b4b7992b2/albumentations/augmentations/transforms.py#L2834",understand wrong replay compose work case work yes list,issue,negative,negative,negative,negative,negative,negative
792829656,"The image might be proprietary, but here is one box causing the issue:

Yolo Format:
[0.99662423, 0.7520255 , 0.00675154, 0.01446759]

My albumentations conversion:
[0.99324846, 0.7447917, 1.0, 0.7592593]

Exception for this box:
Exception has occurred: ValueError
Expected x_max for bbox (0.9934413580246914, 0.7450810185185185, 1.0001929012345678, 0.7592592592592593, 0) to be in the range [0.0, 1.0], got 1.0001929012345678.

My conversion code:
```
def yolo_to_norm_voc(bboxes):
    x_mean, y_mean, w, h = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 3]

    x_min = x_mean - (w/2)
    y_min = y_mean - (h/2)

    x_min, x_max, y_min, y_max = x_min, x_min + w, y_min, y_min + h
    return np.stack([x_min, y_min, x_max, y_max], 1)
```",image might proprietary one box causing issue format conversion exception box exception range got conversion code return,issue,negative,neutral,neutral,neutral,neutral,neutral
791890641,"A workaround I am using for now is converting the 4 corners of my bounding box to ""keypoints"" -> transform them -> then create the transformed bounding boxes from the new keypoints.",converting bounding box transform create bounding new,issue,negative,positive,positive,positive,positive,positive
791498983,"I do agree that we need a way to fix the current behavior of OpenCV's nearest interpolation that we use for masks. 

However, we need a way to inform the current users of different behaviors of `cv2.INTER_NEAREST` and `cv2.INTER_NEAREST_EXACT` and let them choose the one they prefer. By following this path, we will not introduce a sudden significant change that will break the current behavior of augmentation pipelines (even if the current behavior is not optimal and may be seen as 'buggy', it is still how Albumentations resizes masks since the first release).

So my proposal is the following:
1. Add a way for users to specify the interpolation algorithm for masks. For example, we could use a parameter for `A.Compose`, e.g.
```
transform = A.Compose([...], mask_interpolation=cv2.INTER_NEAREST_EXACT)
```
2. In the nearest (no pun intended) release of Albumentations, add a warning which says that the next release will use `cv2.INTER_NEAREST_EXACT` instead of `cv2.INTER_NEAREST` for mask interpolation, and to preserve the old behavior, the user must explicitly specify the mask interpolation method for an augmentation pipeline. Let's make a separate article in the documentation that tells the difference between those interpolation methods.
3. After that, in a new release, we will change the default interpolation method to `cv2.INTER_NEAREST_EXACT`, but still, show a warning if the user didn't specify the interpolation method and Albumentations used the default value.
",agree need way fix current behavior nearest interpolation use however need way inform current different let choose one prefer following path introduce sudden significant change break current behavior augmentation even current behavior optimal may seen still since first release proposal following add way specify interpolation algorithm example could use parameter transform nearest pun intended release add warning next release use instead mask interpolation preserve old behavior user must explicitly specify mask interpolation method augmentation pipeline let make separate article documentation difference interpolation new release change default interpolation method still show warning user specify interpolation method used default value,issue,positive,positive,neutral,neutral,positive,positive
791267302,"Which version of the library are you using?
Try to update it:
`pip install --upgrade albumentations`
or
`pip install --upgrade git+https://github.com/albumentations-team/albumentations`",version library try update pip install upgrade pip install upgrade,issue,negative,neutral,neutral,neutral,neutral,neutral
791253958,"It seems we can use `cv2.INTER_NEAREST_EXACT` interpolation mode to be consistent with PIL, scikit-image or Matlab. @Dipet @creafz @ternaus what are your thoughts on this?",use interpolation mode consistent,issue,negative,positive,positive,positive,positive,positive
791166707,"Thank you for your reply. There is no effect on the iou accuracy of segmentation model. Because mask using opencv resize_nearest causes the ground truth has an offset. If I use the model to predict a image, the result will have about one pixel offset in my model. Examples are below. Left is using the origin albumentations library. Right is pillow resize nearest mode. 
![tmp](https://user-images.githubusercontent.com/13464354/110070815-bee28180-7db5-11eb-98aa-3bfbeee7626e.jpg) ![tmp2](https://user-images.githubusercontent.com/13464354/110070819-c1dd7200-7db5-11eb-94ac-2aedcdfeb0c8.jpg)
It's more obvious when resize mask to a very small size. If input is large, it will be not obvious. That is if resize scale is mall, it will not obvious, if resize scale is large, it will be obvious . Resize-nearest in pillow or opencv will have no effect on the iou accuracy, because it affects the ground truth.",thank reply effect accuracy segmentation model mask ground truth offset use model predict image result one offset model left origin library right pillow resize nearest mode obvious resize mask small size input large obvious resize scale mall obvious resize scale large obvious pillow effect accuracy ground truth,issue,positive,positive,neutral,neutral,positive,positive
790740146,"That's a good point which is worth investigation. From what I know, our library were key component in winning many image segmentation competitions and getting SOTA results on some segmentation benchmarks. Therefore even if there is an issue with NN interpolation of masks, the impact on trained model accuracy is not so obvious. Since one pixel offset is rarely the biggest problem. 

I can think of having `nearest-pillow` interpolation mode, but the effort in implementing this feature is not clear at this point. There are lot of usages of OpenCV interpolation under the hood in many augmentations. Would it be possible to you to run ablation study and demonstrate how big the change in accuracy of the segmentation mask for OpenCV and Pillow interpolation modes are? ",good point worth investigation know library key component winning many image segmentation getting segmentation therefore even issue interpolation impact trained model accuracy obvious since one offset rarely biggest problem think interpolation mode effort feature clear point lot interpolation hood many would possible run ablation study demonstrate big change accuracy segmentation mask pillow interpolation,issue,positive,positive,positive,positive,positive,positive
790120743,Could you please provide an example that reproduces the behavior?,could please provide example behavior,issue,negative,neutral,neutral,neutral,neutral,neutral
790113708,"Actually upon further investigation, there is a bug somewhere.
When I convert the boxes on my own from 'yolo' to 'albumentations' format, it caps at 1, and does not throw an exception. So somewhere it is adding a small amount, maybe in rounding somewhere, causing it to be greater than 1.",actually upon investigation bug somewhere convert format throw exception somewhere small amount maybe rounding somewhere causing greater,issue,negative,positive,neutral,neutral,positive,positive
790085541,"It is not a bug. It is a feature.

It is much easier to pre-process labels once before model training, rather than catching the challenging corner cases.

Feel free to add one line that clips the bounding boxes to your dataloader :)",bug feature much easier model training rather catching corner feel free add one line clip bounding,issue,positive,positive,positive,positive,positive,positive
789202649,"@Dipet
Annotation of partly non-visible objects, be it truncation or occlusion, is helpful when you want to train your model to ""guess"" the object size. Let's say you want to do some 3D reconstruction, if you have a partial box, there is not much you can do. But if you have at least an estimate of the box size (bottom edge and height or width) then you can do at least some measurements and put a higher variance on these.
I would very much like this feature, but rather without any flag. Basically just removing the assert and allowing boxes to be outside of the image. Are there any transformations where this could lead to problems?",annotation partly truncation occlusion helpful want train model guess object size let say want reconstruction partial box much least estimate box size bottom edge height width least put higher variance would much like feature rather without flag basically removing assert outside image could lead,issue,positive,negative,neutral,neutral,negative,negative
788805717,"Thanks for the reply. I want to compose both images in the same way, except channel relevant augmentations like e.g. normalization. But your answer inspired me, splitting it in another way. Haven't thought about it. Thanks!
```
transform_both = A.Compose([
    ...,
    A.ShiftScaleRotate()
])

transform_grayscale = A.Compose([
    A.Normalize(mean=..., std=...)
])

transform_rgb = A.Compose([
    A.Normalize(mean=..., std=...)
])

transformed = transform_both(image=rgb_image, gray_image=gray_image)
normalized_grey = transform_grayscale(transformed[""gray_image""])
normalized_rgb = transform_rgb(transformed[""image""])
```

Have not yet tried it though.",thanks reply want compose way except channel relevant like normalization answer inspired splitting another way thought thanks image yet tried though,issue,positive,positive,positive,positive,positive,positive
788085554,"I see your point BloodAxe, I am not sure it would necessarily be more error prone, but I agree that it shifts the problem on finding the desired bbox after every transformation in case the points gets misaligned.

Actually I just saw that one maybe could go around all of this by specifying the keypoints.

GridDistortion did not implement keypoints movement, `NotImplementedError: Method apply_to_keypoint is not implemented in class GridDistortion`

But `imgaug.transforms.IAAPiecewiseAffine` works well for example.
If bbox gets too distorted during a transformation one might even think if it makes sense thought...
I think custom strategies could be implemented depending on the different transformations and this already covers at least some cases of interest

",see point sure would necessarily error prone agree problem finding desired every transformation case actually saw one maybe could go around implement movement method class work well example distorted transformation one might even think sense thought think custom could depending different already least interest,issue,negative,positive,neutral,neutral,positive,positive
788013195,"Parametrization via `[x1, y1, x2, y2, x3, y3, x4, y4]` allows handing on arbitrary polygon of 4 vertices, while parametrization via `x,y,w,h,angle` - only for rotated rectangles, and therefore is less error-prone. Based on what we want to achieve we can select one of them.
My personal take is `x,y,w,h,angle` for internal representation with conversion where needed.

",via arbitrary polygon vertex via angle rotated therefore le based want achieve select one personal take angle internal representation conversion,issue,negative,negative,neutral,neutral,negative,negative
787990973,"Just a thought, could it be possible to store internally the 4 points of the box? something like bbox8 format [x1, y1, x2, y2, x3, y3, x4, y4]
This would lead to a more verbose parametrization but maybe this would work in both cases, rotated or not?
",thought could possible store internally box something like format would lead verbose maybe would work rotated,issue,negative,neutral,neutral,neutral,neutral,neutral
787914759,"Hey, @irvingzhang0512, thanks for creating an issue. I have updated an example for multi-target augmentations. Now the example has a note about ReplayCompose - https://albumentations.ai/docs/examples/example_multi_target/",hey thanks issue example example note,issue,negative,positive,positive,positive,positive,positive
784843228,"[I made a small notebook](https://nbviewer.jupyter.org/github/aaroswings/RandomToneCurveTests/blob/main/RandomToneCurveTests.ipynb) to explore what the outliers look like for a handful of random samples and different values of ""scale"", fwiw",made small notebook explore look like handful random different scale,issue,negative,negative,negative,negative,negative,negative
784248868,"Hey @art23130 

It is difficult to debug this issue without access to the input data. I wrote a simple script, that passes values to `augment_image_labels` and checks whether they were augmented in the same way, and it works fine with the latest version of Alumentations (0.5.2). At a first glance, I cant see why the Keras Generator could make those values incorrect.

```
image = np.repeat(np.eye(16, 16)[:, :, np.newaxis], repeats=4, axis=2).astype(np.float32)
mask = np.copy(image[:, :, 0]).astype(np.int8)
for iter_ in range(100):
    augmented = augment_image_labels(image, mask)
    augmented_image = augmented['image']
    augmented_mask = augmented['mask']
    for channel in range(4):
        assert np.array_equal(augmented_image[:, :, channel], augmented_mask)
```

A few ideas:
1. Could you try to update Albumentations to the latest version (0.4.6 -> 0.5.2) and see whether the problem still occurs?
2. On [line 72](https://gist.github.com/art23130/bab7307daf7b63b0d713d69c654c628b#file-keras_generator-py-L72) the generator loads labels as `np.int8` (signed int8), but on [line 100](https://gist.github.com/art23130/bab7307daf7b63b0d713d69c654c628b#file-keras_generator-py-L100) it uses the `np.uint8` datatype (unsigned int8). May be there is a problem with negative integer values.",hey art difficult issue without access input data wrote simple script whether augmented way work fine latest version first glance cant see generator could make incorrect image mask image range augmented image mask augmented augmented channel range assert channel could try update latest version see whether problem still line generator line unsigned may problem negative integer,issue,negative,positive,positive,positive,positive,positive
783211073,"Hey, @narain1 

I have started porting functional augmentations from Albumentations to PyTorch while working on AutoAlbument because I needed augmentations to be differentiable w.r.t. their parameters. You can find the code here - https://github.com/albumentations-team/autoalbument/tree/master/autoalbument/albumentations_pytorch

This could be a starting point for porting them to JAX, but currently, we do not have plans to use JAX.",hey functional working differentiable find code could starting point currently use,issue,negative,neutral,neutral,neutral,neutral,neutral
783200470,"To prevent such errors maybe we can add a special check inside `BaseCompose`, that verifies that each transform is an instance of an allowed class, something like the following code inside [`__init__`](https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py#L65):
```
for i, t in enumerate(transforms):
    if not isinstance(t, (BasicTransform, BaseCompose)):
        class_hierarchy = ' -> '.join(str(c) for c in inspect.getmro(t.__class__))
        raise TypeError(
            f'Transform must be an instance of `BasicTransform` or `BaseCompose` subclass, '
            f'but got object `{t}` with the following class hierarchy: {class_hierarchy} '
            f'for transform at index {i} inside `{self.__class__.__name__}`.'
        )
```
This will produce the following exception for the example above:

```
TypeError: Transform must be an instance of `BasicTransform` or `BaseCompose` subclass, but got object `[[ 31 204 220  33 210 223  17  91  86  68]
 [110 126 131  38 227 143 149  31 178  28]
 [ 20 151  69 161  32 216 207 119  97  24]
 [ 93 233  21 234 203  95 164 203 144 100]
 [ 16 106 246 102  87 143  76 243 128  49]
 [174 176 252   0 173 232 191 164 145  48]
 [158 157  31 228 100   7 244 109  76  62]
 [198  52  35  64  81  75 165 204 239 229]
 [247  82 181 100 134  38 120 174 156 224]
 [ 75  61 136 181 179  14 146  52  85  60]]` with the following class hierarchy: <class 'numpy.ndarray'> -> <class 'object'> for transform at index 0 inside `Compose`.

```

@Dipet what do you think?
",prevent maybe add special check inside transform instance class something like following code inside enumerate raise must instance subclass got object following class hierarchy transform index inside produce following exception example transform must instance subclass got object following class hierarchy class class transform index inside compose think,issue,positive,positive,neutral,neutral,positive,positive
783171391,"Hey, @RGring 

You need to apply different normalization values (e.g., different mean or std) to RGB and Grayscale images. Do I understand the case correctly?

In such a situation, I think the simplest way is to define two separate augmentation pipelines and use the appropriate pipeline for an input image. For example:

```
transform_rgb = A.Compose([
    ...,
    A.Normalize(mean=..., std=...)
])

transform_grayscale = A.Compose([
    ...,
    A.Normalize(mean=..., std=...)
])

if is_rgb(image):
    transformed = transform_rgb(image=image)
else:
    transformed = transform_grayscale(image=image)
```
",hey need apply different normalization different mean understand case correctly situation think way define two separate augmentation use appropriate pipeline input image example image else,issue,negative,positive,neutral,neutral,positive,positive
782872642,"Hey @DanielAguileraGarcia 

I have added a description of how to handle this case with pip to docs: https://albumentations.ai/docs/getting_started/installation/#note-on-opencv-dependencies.

Briefly speaking, you need to use the `--no-binary imgaug,albumentations` argument to pip, e.g. `pip install --no-binary imgaug,albumentations -e .`",hey added description handle case pip briefly speaking need use argument pip pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
782523599,"Hello there, could you also elaborate on the reason of failure of importing ""Sharpen""?",hello could also elaborate reason failure sharpen,issue,negative,positive,neutral,neutral,positive,positive
781462881,"Thanks, didn't know about the symmetry-aware transformations.

On Thu, Feb 18, 2021 at 8:54 AM Alex Parinov <notifications@github.com>
wrote:

> @cheind <https://github.com/cheind> No, class_sides is an example of how
> you can pass multiple sets of keypoints' labels to a transformation
> function, and it doesn't have any semantic meaning inside Albumentations.
> For now, the only way to pass information about side-sensitive keypoints to
> Albumentations is to use the symmetry-aware augmentations from
> albumentations-experimental.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/840#issuecomment-781131026>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAAJNJMHCNCATMGFDNAPX23S7TBU5ANCNFSM4XO3JJUQ>
> .
>
",thanks know wrote example pas multiple transformation function semantic meaning inside way pas information use reply directly view,issue,negative,positive,positive,positive,positive,positive
781131026,"@cheind No, `class_sides` is an example of how you can pass multiple sets of keypoints' labels to a transformation function, and it doesn't have any semantic meaning inside Albumentations. For now, the only way to pass information about side-sensitive keypoints to Albumentations is to use the symmetry-aware augmentations from `albumentations-experimental`.",example pas multiple transformation function semantic meaning inside way pas information use,issue,negative,neutral,neutral,neutral,neutral,neutral
780356995,"I propose two changes that could be either implemented in this PR or in a new PR:
1. Use [`FutureWarning`](https://docs.python.org/3/library/exceptions.html#FutureWarning) instead of `DeprecationWarning` for imgaug augmentations. [`DeprecationWarning` is suppressed by default](https://docs.python.org/3/library/exceptions.html#DeprecationWarning) so most users won't see messages about deprecated augmentations.
2. Explicitly name the augmentation in a warning. So use `IAAFliplr is deprecated...` instead of `This augmentation is deprecated...`. In that case, users will know which augmentation is causing problems without a need to dig into the source code.",propose two could either new use instead suppressed default wo see explicitly name augmentation warning use instead augmentation case know augmentation causing without need dig source code,issue,negative,positive,positive,positive,positive,positive
779817620,"@creafz thanks for the update! i wonder if albumentations/HorizontalFlip could automatically correct the ordering of the keypoints? I've seen the usage of `class_sides`, but I believe it does not affect the behaviour of HorizontalFlip. ",thanks update wonder could automatically correct seen usage believe affect behaviour,issue,negative,positive,positive,positive,positive,positive
779814752,"Hey @cheind , good catch, thanks. For now, I have updated the article and added a note about side-sensitive keypoints at the beginning.",hey good catch thanks article added note beginning,issue,positive,positive,positive,positive,positive,positive
778856881,"Hello!
Good work! Try to run `python tools/make_transforms_docs.py make` and copy output into README.md. I don't know why, but spatial-level transforms table is changed in your PR.",hello good work try run python make copy output know table,issue,negative,positive,positive,positive,positive,positive
778855844,"Hello!
I think it is a good idea!
As a first step we need to provide a flag, that this is rotated bbox.
I see only two ways how to do this:
1. Provide this bboxes as another argument with different name. For example: `rotated_bboxes`. In this scenario we will need to add another function to work with this type of bboxes `apply_to_rotated_bbox`.
2. Expand current bboxes and add option into `BbboxParams(with_angle=True)`. When `with_angle=True` we will work with bbox as bounding box in format `[x1, y1, x2, y2, angle]`. For example you can look how it works with keypoints formats. But in this case we need to implement some tricks to work with our current interface.

Try to look how it currently works.
- `BboxParams` and `KeypointParams` https://github.com/albumentations-team/albumentations/blob/ff83de8a51184bca97be3a9fc6905c56b44c494a/albumentations/core/composition.py#L403
- How transformation process bboxes and keypoints https://github.com/albumentations-team/albumentations/blob/ff83de8a51184bca97be3a9fc6905c56b44c494a/albumentations/augmentations/geometric/rotate.py#L96
- `BboxProcessor` needs to convert bboxes between different formats https://github.com/albumentations-team/albumentations/blob/ff83de8a51184bca97be3a9fc6905c56b44c494a/albumentations/augmentations/bbox_utils.py#L21
- `KeypointsProcessor` https://github.com/albumentations-team/albumentations/blob/ff83de8a51184bca97be3a9fc6905c56b44c494a/albumentations/augmentations/keypoints_utils.py#L24",hello think good idea first step need provide flag rotated see two way provide another argument different name example scenario need add another function work type expand current add option work bounding box format angle example look work case need implement work current interface try look currently work transformation process need convert different,issue,negative,positive,positive,positive,positive,positive
777774864,"Thanks for the warm response 😀 Glad you think this could be a useful contribution.
I ran black and flake8 on my branch, when I run `tools/make_transforms_docs.py make` I noticed that the class I added doesn't show in the outputs. Is there something I'm likely to be missing?",thanks warm response glad think could useful contribution ran black flake branch run make class added show something likely missing,issue,positive,positive,positive,positive,positive,positive
777378892,"Hi! Many thanks for your first PR. The implementation itself looks very clear 💪 👍 
It seems there are some minor issues still remaining to fix before we can merge it:
- Please ensure that black and flake8 are happy with code formatting
- Re-generate docs of transformation list in README.md (See  `tools/make_transforms_docs.py`)",hi many thanks first implementation clear minor still fix merge please ensure black flake happy code transformation list see,issue,positive,positive,positive,positive,positive,positive
774633898,"+1 for this very good suggestion !
It would be interesting to see the speed comparison between Albumentations and DALI at least for the most common augmentations.
It think it would be also useful to compare entire typical augmentation pipelines and not just individual augmentations.",good suggestion would interesting see speed comparison dali least common think would also useful compare entire typical augmentation individual,issue,positive,positive,neutral,neutral,positive,positive
774474735,"BloodAxe,
Thanks for the reply! I don't have a numpy example.  Sorry, I'm just getting started.
But basically, I would to crop the images vertically, based on bbox coordinates:

![cat-2](https://user-images.githubusercontent.com/68970725/107118918-0bd44600-6852-11eb-87f2-5155ff9eb2b6.jpg)

",thanks reply example sorry getting basically would crop vertically based,issue,negative,negative,negative,negative,negative,negative
774469690,"Hi. Would you mind attaching a minimal example in numpy of the desired
functionality?  I’m not sure did I understand you idea from the given image.


Сб, 6 февр. 2021 г. в 13:58, AnchorAnalytics <notifications@github.com>:

> Hi,
>
> I'm fairly new to Albumentations and it great! I have reviewed the
> documentation and have not seen a vertical or horizontal crop function. My
> goal is to crop an image by bounding box coordinates
> [image: cat_LI]
> <https://user-images.githubusercontent.com/68970725/107117470-91eb8f00-6848-11eb-9469-2d729ef4367b.jpg>
> either vertically or horizontally. See attachment for example.
>
> Is there a crop function in Albumentations that crops an image based on
> bbox coordinates?
>
> Thank!
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/838>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YFDXBSOMKXYW6L7BTDS5UVFJANCNFSM4XGGVTFA>
> .
>
",hi would mind minimal example desired functionality sure understand idea given image hi fairly new great documentation seen vertical horizontal crop function goal crop image bounding box image either vertically horizontally see attachment example crop function image based thank thread reply directly view,issue,positive,positive,positive,positive,positive,positive
774198337,"thank you @BloodAxe I was able to install it using the no deps.
@Dipet yes I installed py-opencv from the mini-conda environment but the pip install couldn't check the existence of it, but it did work with @BloodAxe suggestion of omitting the dependencies, I did have to install the other dependencies manually one by one but it worked.",thank able install yes environment pip install could check existence work suggestion install manually one one worked,issue,positive,positive,positive,positive,positive,positive
773712392,"+1, this makes it a pain to use in a production system that needs the `opencv-contrib-*` dependency.",pain use production system need dependency,issue,negative,neutral,neutral,neutral,neutral,neutral
773521643,"So, if you have bbox like on example image it is will be incorrect after crop.
For this reason I think better augment only masks, because they are much more correct.
So if you will augment only mask and then create bboxes it is better than create some strange code to augment both bboxes and mask and check them after each transformation.",like example image incorrect crop reason think better augment much correct augment mask create better create strange code augment mask check transformation,issue,positive,positive,positive,positive,positive,positive
773516019,"It's possible for mask inside bbox to disappear while bbox is still exists. It will be rare case but still possible. Check image:

https://www.dropbox.com/s/l365ell59u2qfo2/BBox.png?dl=0

Yes I use similar code:
```
def bbox_for_mask(img):
    rows = np.any(img, axis=1)
    cols = np.any(img, axis=0)
    rmin, rmax = np.where(rows)[0][[0, -1]]
    cmin, cmax = np.where(cols)[0][[0, -1]]
    return cmin, rmin, cmax + 1, rmax + 1
```",possible mask inside disappear still rare case still possible check image yes use similar code return,issue,negative,positive,neutral,neutral,positive,positive
773511016,"> There can also be problem if box remains, but mask became zero

But if mask 0 and you have a bbox something is wrong.

> Curently I avoid this problem by recalculate boxes based on masks after augmentation, but it's rather expencieve...

Hmm. You have rotated bbox or why you think that it is expansive? I think that check mask after each transform much more expensive that annotate bbox by mask.
If you have only one bbox for each mask it is very simple to get bbox:
```python
row = np.where(np.any(arr, axis=0))[0]
col = np.where(np.any(arr, axis=1))[0]

x_min, y_min, x_max, y_max = row[0], col[0], row[-1], col[-1]
```",also problem box remains mask zero mask something wrong avoid problem recalculate based augmentation rather rotated think expansive think check mask transform much expensive annotate mask one mask simple get python row col row col row col,issue,negative,negative,negative,negative,negative,negative
773486402,In instance segmentation each mask connected to some bbox (MaskRCNN for example). I'm not sure if albumentation supported instance segmentation. I just wanted to use albumentation for this scenario and find this problem....,instance segmentation mask connected example sure instance segmentation use scenario find problem,issue,negative,positive,positive,positive,positive,positive
773481889,"For me it is a little bit strange scenario when you need to remove masks. Because if you work with segmentation problem in common case network want masks for each class, so you need to save masks for each class.

Unfortunately, right now library does not have ability to bind masks and bboxes. Did you find information in the documentation that it works the way you describe it?",little bit strange scenario need remove work segmentation problem common case network want class need save class unfortunately right library ability bind find information documentation work way describe,issue,negative,negative,neutral,neutral,negative,negative
773478660,"There can also be problem if box remains, but mask became zero. ) 

Curently I avoid this problem by recalculate boxes based on masks after augmentation, but it's rather expencieve...",also problem box remains mask zero avoid problem recalculate based augmentation rather,issue,negative,neutral,neutral,neutral,neutral,neutral
773476363,"Normally library does not change order of arguments.
Try do this:
```python
masks = [i for i in masks if np.any(i)]
```",normally library change order try python,issue,negative,positive,positive,positive,positive,positive
773468153,Let's say we have instance segmentation problem. You have 20 boxes and 20 masks. After rotation box 7 and box 18 removed. I expected mask 7 and mask 18 also removed. I'm not sure how to do it in current code to stay consistent between masks and boxes.,let say instance segmentation problem rotation box box removed mask mask also removed sure current code stay consistent,issue,negative,positive,positive,positive,positive,positive
773466140,"I am a little bit confused.
Do you mean that if the whole mask equal to 0 the library should remove it?",little bit confused mean whole mask equal library remove,issue,negative,negative,negative,negative,negative,negative
773095608,"Oh, it's my fault, thanks for your reply. 😃 ",oh fault thanks reply,issue,negative,positive,positive,positive,positive,positive
773090937,"You are working with float32 images in the wrong order.
float32 image must be normalized in range `[0, 1]`.
```python
img_f32 = img_uint8.astype(np.float32) / 255.0
```",working float wrong order float image must range python,issue,negative,negative,negative,negative,negative,negative
771625292,"Greetings! Thanks for your PR. I believe this is good improvement on sampling augmentations. It seems there are couple of failing unit tests, would you mind fixing them following our [guidelines](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst)? 

",thanks believe good improvement sampling couple failing unit would mind fixing following,issue,positive,positive,positive,positive,positive,positive
770699490,Thanks for your quick answer. My bad!,thanks quick answer bad,issue,negative,negative,neutral,neutral,negative,negative
770697535,Hi. `A.Emboss()` is not in pip package yet. To get the latest development version you can install albumentations via `pip install git+https://github.com/albumentations-team/albumentations.git`.,hi pip package yet get latest development version install via pip install,issue,negative,positive,positive,positive,positive,positive
770679833,"I didn't understand what you want to do. If you work with bboxes in `yolo` format you don't need to convert them to albumentations format.

```python
import numpy as np
import albumentations as A
import random
import cv2 as cv
import matplotlib.pyplot as plt


def draw_bboxes(img, bboxes):
    height, width = img.shape[:2]
    for x, y, w, h, label in bboxes:
        x *= width
        y *= height
        w *= width
        h *= height

        x1 = int(x - w / 2 + 1)
        x2 = int(x1 + w)
        y1 = int(y - h / 2 + 1)
        y2 = int(y1 + h)

        cv.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), thickness=height // 100)


random.seed(0)
np.random.seed(0)

image = np.zeros([1000, 1000, 3], dtype=np.uint8)
bboxes = [
    [0.05, 0.05, 0.1, 0.1, 0],
    [0.3, 0.3, 0.25, 0.25, 0],
    [0.25, 0.17, 0.38, 0.34, 0],
    [0.7, 0.3, 0.11, 0.33, 0],
]

transforms = A.Compose([A.Resize(100, 100)], bbox_params=A.BboxParams(format=""yolo""))

res = transforms(image=image, bboxes=bboxes)

plt.subplot(211, title=""original"")
draw_bboxes(image, bboxes)
plt.imshow(image, vmin=0, vmax=255)

plt.subplot(212, title=""result"")
draw_bboxes(res[""image""], res[""bboxes""])
plt.imshow(res[""image""], vmin=0, vmax=255)

plt.show()

```",understand want work format need convert format python import import import random import import height width label width height width height image original image image result image image,issue,negative,negative,neutral,neutral,negative,negative
770666215,"As I see, all works fine
```python
import numpy as np
import albumentations as A
import random
import cv2 as cv


random.seed(0)
np.random.seed(0)

image = np.zeros([100, 100, 3], dtype=np.uint8)
keypoints = [
    [10, 10],
    [10, 90],
    [50, 50],
    [90, 10],
    [90, 90],
]

transforms = A.Compose([A.Rotate(10, p=10)], keypoint_params=A.KeypointParams(format=""xy""))

res = transforms(image=image, keypoints=keypoints)

print(keypoints)            # [[10, 10], [10, 90], [50, 50], [90, 10], [90, 90]]
print(res[""keypoints""])     # [(11, 9), (9, 89), (50, 50), (91, 11), (89, 91)]

```",see work fine python import import import random import image print print,issue,negative,negative,neutral,neutral,negative,negative
770644865,"This command `pip install git+https://github.com/albumentations-team/albumentations.git` also needs `opencv-python-headless`?
Are you sure that you install `albumentations` in same envirornment? You may check that OpenCV installed with this command `pip freeze | grep opencv`
",command pip install also need sure install may check command pip freeze,issue,negative,positive,positive,positive,positive,positive
770345862,"Somehow DeepSource suddenly reported 115 new issues. Most of them are not even relevant to the code changed in this PR. After a brief look, some of those issues look very disputable. I think we should disable DeepSource completely until someone is willing to resolve all those issues manually. This will help to reduce the excessive amount of warnings for contributors who create new PRs.",somehow suddenly new even relevant code brief look look disputable think disable completely someone willing resolve manually help reduce excessive amount create new,issue,positive,positive,positive,positive,positive,positive
769674649,"Thanks for your advice. And is there any plan to fix this bug in the near release?

Thanks
B. R. 
",thanks advice plan fix bug near release thanks,issue,positive,positive,positive,positive,positive,positive
768869329,"I think this is a good idea and doesn't require big code changes.
Looks like we need to add new arguments into [this](https://github.com/albumentations-team/albumentations/blob/13f44d1397f11cb371436715d50739a8c90b6abb/albumentations/augmentations/bbox_utils.py#L345) function and add function to replace pixels values",think good idea require big code like need add new function add function replace,issue,positive,positive,positive,positive,positive,positive
768865614,"The problem is not in imagaug augmentations. You have keypoints outside of image. If you want to use and save them set `remove_invisible=False`
```python
keypoint_params=albu.KeypointParams(format=""xy"", remove_invisible=False)
```",problem outside image want use save set python,issue,negative,neutral,neutral,neutral,neutral,neutral
768689175,"> Do you mean when we drop bbox by `min_area` or `min_visibility` conditions, we don't just simple remove it, but we remove and fill the bbox area by some values, for example zeros?

Yes! And this behavior is controlled by a flag or param, like `min_visibility`",mean drop simple remove remove fill area example yes behavior flag param like,issue,negative,negative,negative,negative,negative,negative
768339203,"> I a little bit rewrote test. In my tests I found problems only in `IAACropAndPad`

Rewrote ""Code to reproduce the behavior:"" (your version with minor changes and fewer lines), and still the same problems.

> We are planning to create our own implementations of these transforms.
> Now library has own implementations of `Emboss`, `Sharpen` and `Perspective`

Looking forward to the next release :)

",little bit test found code reproduce behavior version minor still create library emboss sharpen perspective looking forward next release,issue,negative,negative,neutral,neutral,negative,negative
768254025,"im = np.random.rand(512,256,3) * 255
im = np.array(Image.open(image_path).convert('RGB'))
mask = np.random.choice([0, 1], size=(512,256), p=[1./3, 2./3])
locations = [[6.14975, 76.1165], [13.89175, 79.07975], [13.0165, 81.74075], [7.29775, 73.5655], [-9.433, 93.50725], [-0.8295, 95.27225], [-2.5525, 98.82875], [-7.417, 90.01375]]

colorpallete = {'black': (0,0,0), 'white': (255,255,255)}

ranh = int(image.shape[0]/randrange(4,10))
ranw = int(image.shape[1]/randrange(4,10))
ranc = random.choice(list(colorpallete.values()))

augment = albu.Compose(
    [
        albu.HueSaturationValue(p=0.5),
        albu.RGBShift(p=0.5),
        albu.RandomBrightnessContrast(p=0.5),
        albu.MotionBlur(p=0.5),
        albu.Cutout(num_holes=6, max_h_size=ranh, max_w_size=ranw, fill_value=ranc, always_apply=False, p=0.5),
        # TODO: shearing/translation/rotation doesnt work - issues with the keypoints
        albu.IAAAffine(scale=1, translate_percent=0.0, translate_px=None, rotate=0.0, shear=[25,25], order=1, cval=0, mode='edge', always_apply=False, p=0.5),
        albu.GaussNoise(p=0.5),
        albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),
    ],
    keypoint_params=albu.KeypointParams(format=""xy"")
)

augmented = augment(image=im, mask=mask, keypoints=locations)

I suspect it's because some of my keypoints are ""outside"" the image. This is part of the task of the model. Would I need to pad to circumvent this?

",mask list augment doesnt work augmented augment suspect outside image part task model would need pad circumvent,issue,negative,neutral,neutral,neutral,neutral,neutral
768237028,"I can not reproduce the problem. Can you provide reproducible example?
Unfortunately library does not have shear transformation right now. You may try `ShiftScaleRotate`",reproduce problem provide reproducible example unfortunately library shear transformation right may try,issue,negative,negative,negative,negative,negative,negative
768225288,"It is based on your pipeline, tasks and augmentations.
There you can find benchmark results for our auto augmentation tool https://github.com/albumentations-team/autoalbument-benchmarks",based pipeline find auto augmentation tool,issue,negative,neutral,neutral,neutral,neutral,neutral
768221275,"@Dipet Thank you for your quick reply.
 
 > But could you a little bit rewrite your PR?
 
 Of course! Now, I updated my PR.
 
> @creafz @BloodAxe @ternaus Here is a little bit strange implementation. Maybe we will remove normalization before applying blur? After these changes transform will work fine, but the result may differ from this implementation by 2 abs(old_result - new_result).max() <= 2

I modified the test case to allow for the difference between uint8 and float at most 2. 

If any fix is needed about this issue, please let me know.",thank quick reply could little bit rewrite course little bit strange implementation maybe remove normalization blur transform work fine result may differ implementation test case allow difference float fix issue please let know,issue,positive,positive,neutral,neutral,positive,positive
768140045,"Thanks for reply, I also want to know that by using Albumentations API how much accuracy we can increase in faster RCNN and mask RCNN model(just give me rough idea like 5%,10% etc). ",thanks reply also want know much accuracy increase faster mask model give rough idea like,issue,positive,positive,positive,positive,positive,positive
768122873,"Do you mean when we drop bbox by `min_area` or `min_visibility` conditions, we don't just simple remove it, but we remove and fill the bbox area by some values, for example zeros?",mean drop simple remove remove fill area example,issue,negative,negative,negative,negative,negative,negative
768102903,"Unfortunately, the library does not have this functionality right now.",unfortunately library functionality right,issue,negative,negative,negative,negative,negative,negative
768100386,"I a little bit rewrote test. In my tests I found problems only in `IAACropAndPad`
We are planning to create our own implementations of these transforms.
Now library has own implementations of `Emboss`, `Sharpen` and `Perspective`

```python
import albumentations as A
import numpy as np
import random


class SimpleDataset:
    def __init__(self, transform=None, size=1000):
        self.arr = [np.random.randint(0, 256, (64, 64, 3), dtype=np.uint8) for _ in range(size)]
        self.transform = transform

    def __len__(self):
        return len(self.arr)

    def __getitem__(self, idx):
        data = {'image': self.arr[idx]}

        if self.transform is not None:
            data = self.transform(**data)

        return data['image']


def apply_transform(image, transform, seed):
    random.seed(seed)
    np.random.seed(seed)

    transform = A.Compose([transform])

    return transform(image=image)[""image""]


if __name__ == '__main__':
    check_transforms = [A.Blur(), A.IAAEmboss(), A.IAASharpen(), A.IAACropAndPad(),
                        A.IAAAffine(), A.IAAPiecewiseAffine(), A.IAAPerspective()]

    seed = 14

    dataset = SimpleDataset()

    for aug in check_transforms:
        equal = True
        for i in range(2):
            image = dataset[i]

            img1 = apply_transform(image, aug, seed)
            img2 = apply_transform(image, aug, seed)

            equal = equal and (img1 == img2).all()
        print(f'{aug.__class__.__name__}:{""OK"" if equal else ""Mismatch""}')

```",little bit test found create library emboss sharpen perspective python import import import random class self range size transform self return self data none data data return data image transform seed seed seed transform transform return transform image seed equal true range image image seed image seed equal equal print equal else mismatch,issue,positive,negative,neutral,neutral,negative,negative
768095572,"> I have no idea how to implement this request, because the library doesn't know such specific information about your data.
> If you have independent boxes for helmet and head you can check labels after transformations.
> Also if you have some ideas how to implement this changes for your data you can inherit from `RandomCrop` and implement your own `apply_to_bbox` method.

the library dosen't need to know such info about the data, Just need a flag. if the flag is true, wipe the content of the invalid bounding boxes when dropping it. the flag is controlled by the user, not the library. Maybe add a extra option in BBoxParam ?",idea implement request library know specific information data independent helmet head check also implement data inherit implement method library dose need know data need flag flag true wipe content invalid bounding dropping flag user library maybe add extra option,issue,negative,positive,neutral,neutral,positive,positive
768083713,"I have no idea how to implement this request, because the library doesn't know such specific information about your data.
If you have independent boxes for helmet and head you can check labels after transformations.
Also if you have some ideas how to implement this changes for your data you can inherit from `RandomCrop` and implement your own `apply_to_bbox` method.",idea implement request library know specific information data independent helmet head check also implement data inherit implement method,issue,negative,neutral,neutral,neutral,neutral,neutral
766997136,"If you provide bboxes to `Compose` in coco format, they must be returned in that format. If not, please provide a minimal example to reproduce the problem.",provide compose coco format must returned format please provide minimal example reproduce problem,issue,negative,negative,neutral,neutral,negative,negative
766535354,"I checked for my dataset that I was using, that the bounding boxes were not an issue. I tried @abidKiller's solution and it worked for me. I wonder where the floating point precision issue resides, be great to fix it!",checked bounding issue tried solution worked wonder floating point precision issue great fix,issue,positive,positive,positive,positive,positive,positive
765428291,"> Well, I wonder what did you expect to get as a result? Some augmentations, require rgb input, which is clearly stated in the documentation. Second, your have different modalities in your RGB-D image. Albumentations does not support depth map augmentation as of now. However you can treat your depth map as a mask, and augment it as follows:
> 
> ```
> result = transform(image=input[..., :3], mask=input[...,3])
> ```
> 
> This will give you consistent spatial augmentation of the depth map. Note that interpolation mode for masks is hard-coded to ""nearest neighbor"".

If the depth map has already be normalized to 0-255, can we directly transform the 4-channel RGBD image?
`result = transform(image=input[..., :4], mask=mask)`
",well wonder expect get result require input clearly stated documentation second different image support depth map augmentation however treat depth map mask augment result transform give consistent spatial augmentation depth map note interpolation mode nearest neighbor depth map already directly transform image result transform,issue,positive,positive,neutral,neutral,positive,positive
764715210,"Hi,
the reasoning is rather simple, I thought it was a functionality worth having accessible to every user. I know I could easily implement something like the solution you proposed or a BBoxSafeCompose on top of Compose.

I thought having the functionality inside the default Compose could have been better. It is indeed not clean to implement it on top, since it would require copying and pasting the existing __call__ implementation and adding the required code.",hi reasoning rather simple thought functionality worth accessible every user know could easily implement something like solution top compose thought functionality inside default compose could better indeed clean implement top since would require pasting implementation code,issue,positive,positive,positive,positive,positive,positive
764713556,"> Well, I wonder what did you expect to get as a result? Some augmentations, require rgb input, which is clearly stated in the documentation. Second, your have different modalities in your RGB-D image. Albumentations does not support depth map augmentation as of now. However you can treat your depth map as a mask, and augment it as follows:
> 
> ```
> result = transform(image=input[..., :3], mask=input[...,3])
> ```
> 
> This will give you consistent spatial augmentation of the depth map. Note that interpolation mode for masks is hard-coded to ""nearest neighbor"".

Thanks for your patiently reply！I get it！",well wonder expect get result require input clearly stated documentation second different image support depth map augmentation however treat depth map mask augment result transform give consistent spatial augmentation depth map note interpolation mode nearest neighbor thanks patiently get,issue,positive,positive,positive,positive,positive,positive
764641982,"Well, I wonder what did you expect to get as a result? Some augmentations, require rgb input, which is clearly stated in the documentation. Second, your have different modalities in your RGB-D image. Albumentations does not support depth map augmentation as of now. However you can treat your depth map as a mask, and augment it as follows:

```
result = transform(image=input[..., :3], mask=input[...,3])
```

This will give you consistent spatial augmentation of the depth map. Note that interpolation mode for masks is hard-coded to ""nearest neighbor"".",well wonder expect get result require input clearly stated documentation second different image support depth map augmentation however treat depth map mask augment result transform give consistent spatial augmentation depth map note interpolation mode nearest neighbor,issue,positive,positive,neutral,neutral,positive,positive
764639322,"Greetings,

It's unclear to me, what is the reasoning behind this proposal. In fact, I see an easy implementation of this behavior on the user-side, without touching library code (Yes, we try to follow [open-closed](https://en.wikipedia.org/wiki/Open%E2%80%93closed_principle) principle).
```
transform = A.Compose(....)
should_retry = True
while should_retry:
    data = transform(**input)
    should_retry= len(data[""bboxes""]) == 0
```
 This is clean solution from my perspective.  Let us know if that solves your issue and if not, we are looking forward for your comments.",unclear reasoning behind proposal fact see easy implementation behavior without touching library code yes try follow principle transform true data transform input data clean solution perspective let u know issue looking forward,issue,positive,positive,positive,positive,positive,positive
764176361,"> The simplest way to do this is to use 4 channel image with shape `[Height, Width, 4]`.

Hello,i used the 4 channel image(numpy) just like (256,256,4), but it still made some errors, just like 'F.shift_hsv' will raise 'ValueError: cannot reshape array of size 196608 into shape (256,256,4)'.
Thanks!
",way use channel image shape height width hello used channel image like still made like raise reshape array size shape thanks,issue,positive,positive,positive,positive,positive,positive
762061385,Hi @Dipet. How to do black padding instead of mirroring?,hi black padding instead,issue,negative,negative,negative,negative,negative,negative
762042380,"> I have found the API I needed `albumentations.augmentations.transforms.LongestMaxSize()`

Hi @misads. Does it use black padding to keep the aspect ratio or does it crop the image?",found hi use black padding keep aspect ratio crop image,issue,negative,negative,negative,negative,negative,negative
761847492,"Weirdly, after a few days, I didn't get the error above anymore.

@Maxwell2Gyamfi Thank you for your code! It had some problems at first, but I just edited this:
`for filename in os.listdir(imagespath):`
to this:
`for filename in sorted(os.listdir(imagespath)):`

Thank you so much! I'm closing this issue now.",weirdly day get error thank code first sorted thank much issue,issue,negative,negative,neutral,neutral,negative,negative
761661710,"I'm experiencing issues with the bounding box with YOLO. The center coordinates are correct, though the width and height coordinates of the bounding box are way off. This is the chunk of code that I have to read in a YOLO annotation, apply augmentations to the data, and then re-save the augmentation (for a single object) 

``` 
# Read in the object bounding boxes
        with open(WORK_DIR + '/renders/' + img[:-4] + '.txt', 'r') as f:
            labels = f.readline()
            coords = labels.split(' ')[1:]
            coords = [float(i) for i in coords]

        coords.append(0)

        # Apply the transforms to the image 
        image = imread(WORK_DIR + '/renders/' + img, pilmode='RGB')
        image = transforms(image=image, bboxes=[coords])

        print(f""Applying background and augmentations to {WORK_DIR + '/renders/' + img}"")

        print(f""before: {coords}after: {image['bboxes']}"")

        imsave(WORK_DIR + '/renders/' + img, image[""image""])

        # Save the new
        with open(WORK_DIR + '/renders/' + img[:-4] + '.txt', 'w') as f:
            f.write(f""0 {image['bboxes'][0][0]} {image['bboxes'][0][1]} {image['bboxes'][0][2]} {image['bboxes'][0][3]}"")
```",bounding box center correct though width height bounding box way chunk code read annotation apply data augmentation single object read object bounding open float apply image image image print background print image image image save new open image image image image,issue,negative,negative,neutral,neutral,negative,negative
760930409,"I think using PIL needs a little modifications to your example:
 
```
import albumentations as A
import numpy as np
import PIL

# Declare an augmentation pipeline 
transform = A.Compose([
    A.RandomCrop(width=256, height=256),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
])

# Read an image with PIL and convert it to numpy array
image = np.asarray(PIL.Image.open(""image.jpg""))

# Augment an image
transformed = transform(image=image)
transformed_image = transformed[""image""]
```
 ",think need little example import import import declare augmentation pipeline transform read image convert array image augment image transform image,issue,negative,negative,negative,negative,negative,negative
760884010,"Hi. Albumentations does not support images in PIL format. We work with numpy arrays, which provide faster access to underlying memory and better performance when doing interop with native libraries (such as OpenCV).",hi support format work provide faster access underlying memory better performance native,issue,positive,positive,positive,positive,positive,positive
760723917,"Hello.
That is a some kind of bug, because all transforms must generate all parameters in specific methods, but `LongestMaxSize` does not do this and calculates this parameters for each call.
Bug with `PadIfNeeded` happens because this transform update his parameters using function `update_params` and it is called inside `apply_with_params` that called after copying all parameters to replay dict.
https://github.com/albumentations-team/albumentations/blob/1f4fa59389f07a692a3934ba5c2c7d3c4e89e9f6/albumentations/core/transforms_interface.py#L89

Simple fix for `PadIfNeeded` is move `apply_with_params` call before copying parameters to replay dict and rewrite code like this:
```python
res = self.apply_with_params(params, **kwargs)
if self.deterministic:
    if self.targets_as_params:
        warn(
            self.get_class_fullname() + "" could work incorrectly in ReplayMode for other input data""
            "" because its' params depend on targets.""
        )
    kwargs[self.save_key][id(self)] = deepcopy(params)
return res
```",hello kind bug must generate specific call bug transform update function inside replay simple fix move call replay rewrite code like python warn could work incorrectly input data depend id self return,issue,positive,positive,positive,positive,positive,positive
760713741,"Hello!
If I understand you correctly symmetric keypoints augmentations may help you.
Also you may to try use `ReplayCompose` to check which aumgentations was applied https://albumentations.ai/docs/examples/replay/
Or another way to add additional information about keypoints order to tail of each keypoint and after that reconstruct their order.",hello understand correctly symmetric may help also may try use check applied another way add additional information order tail reconstruct order,issue,negative,neutral,neutral,neutral,neutral,neutral
760704785,"The simplest way to do this is to use 4 channel image with shape `[Height, Width, 4]`.
Also you could use additional targets for this purpose https://albumentations.ai/docs/examples/example_multi_target/
But keep in mind that the additional channel must be the same height and width.",way use channel image shape height width also could use additional purpose keep mind additional channel must height width,issue,negative,neutral,neutral,neutral,neutral,neutral
759980770,"Unfortunately at this time we have no quick solution how to implement bboxes reflection.
We recommend to do not use reflection mode for borders in any transformations if you apply these transforms to bboxes.
We will add information about this problem to all transformations documentation that uses border mode as soon as possible. 
",unfortunately time quick solution implement reflection recommend use reflection mode apply add information problem documentation border mode soon possible,issue,negative,negative,neutral,neutral,negative,negative
758657156,"actually, this should fix my problem
[https://albumentations.ai/docs/experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.FlipSymmetricKeypoints](https://albumentations.ai/docs/experimental/api_reference/augmentations/transforms/#albumentations_experimental.augmentations.transforms.FlipSymmetricKeypoints)
am I right?",actually fix problem right,issue,negative,positive,positive,positive,positive,positive
758631816,"> Greetings,
> 
> That's indeed the only way to know what augmentation and which what parameters has been applied. May I ask what is your use-case scenario that require to change annotation depending on the applied augmentations?

good to know, thanks!

I'm playing with pose estimation and I had the idea of including a label for each key point so that to know where the point is w.r.t. the object center. A point can be in front, back, left, or right.

When I do the horizontal flip, I need to change left with right and right with left labels. Front and back are not affected by this operation

<img width=""856"" alt=""41DC05A0-72E2-4168-B772-30EC095954AF"" src=""https://user-images.githubusercontent.com/4661737/104315956-a18ae880-54d3-11eb-8d84-6609b423150a.png"">
 ",indeed way know augmentation applied may ask scenario require change annotation depending applied good know thanks pose estimation idea label key point know point object center point front back left right horizontal flip need change left right right left front back affected operation,issue,positive,positive,positive,positive,positive,positive
758576936,"Greetings,

That's indeed the only way to know what augmentation and which what parameters has been applied.  May I ask what is your use-case scenario that require to change annotation depending on the applied augmentations? ",indeed way know augmentation applied may ask scenario require change annotation depending applied,issue,negative,neutral,neutral,neutral,neutral,neutral
758573721,"to check for `HorizontalFlip` I'm doing this

```
    # check if horizontal flip has been applied
    hflipname = '.'.join(
        [""albumentations"", ""augmentations"", ""transforms"", ""HorizontalFlip""])
    for t in augmented[""replay""][""transforms""]:
        if t[""__class_fullname__""] == hflipname:
            if t[""applied""]:
                print(""Done horizontal flip"")
```

where `augmented` is from `ReplayCompose`.

I guess there's no other way?",check check horizontal flip applied augmented replay applied print done horizontal flip augmented guess way,issue,negative,neutral,neutral,neutral,neutral,neutral
757621428,"Hi, I found the `params` of `LongestMaxSize` and  `PadIfNeeded` is always empty when print `data['replay']`.

Do you know how to fix it? and are there any alternatives?

My code is
```
transform = A.ReplayCompose([
    A.LongestMaxSize(224),
    A.PadIfNeeded(224, 224, border_mode=cv2.BORDER_CONSTANT),
    A.RandomCrop(224, 224, always_apply=True),
    A.Rotate(limit=(-20,20), border_mode=cv2.BORDER_CONSTANT, p=1.0),
    A.MotionBlur(blur_limit=(3,7), p=1.0),
], keypoint_params=A.KeypointParams(format='xy'))
```

Thanks!

",hi found always empty print data know fix code transform thanks,issue,negative,positive,neutral,neutral,positive,positive
756130867,"Hi this should help
It reads an image from a folder which has a yolo label
Reads the yolo and applies the selected augmentation and save augmentation as an image along with the bounding boxes
Repeats for each image in the folder

Change line 14 for your folder containing your images
Modify the function get transform to add or remove augmentations

```
import albumentations as A
from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage
from bbaug import policies
import cv2
import os
from pascal_voc_writer import Writer
from xml.dom import minidom
import imgaug as ia
import imgaug.augmenters as iaa
import math
import random
import copy
import glob
imagespath = 'C:/Users/max_x/Desktop/test2/'
random.seed(7)


def readImage(filename):
    # OpenCV uses BGR channels
    img = cv2.imread(imagespath+filename)
    return img


def getCoordinates(filename):
    allbb = []
    xmldoc = minidom.parse(imagespath+filename)
    itemlist = xmldoc.getElementsByTagName('object')

    size = xmldoc.getElementsByTagName('size')[0]
    width = int((size.getElementsByTagName('width')[0]).firstChild.data)
    height = int((size.getElementsByTagName('height')[0]).firstChild.data)

    for item in itemlist:
        classid = (item.getElementsByTagName('name')[0]).firstChild.data
        xmin = ((item.getElementsByTagName('bndbox')[
            0]).getElementsByTagName('xmin')[0]).firstChild.data
        ymin = ((item.getElementsByTagName('bndbox')[
            0]).getElementsByTagName('ymin')[0]).firstChild.data
        xmax = ((item.getElementsByTagName('bndbox')[
            0]).getElementsByTagName('xmax')[0]).firstChild.data
        ymax = ((item.getElementsByTagName('bndbox')[
            0]).getElementsByTagName('ymax')[0]).firstChild.data

        xmin = int(xmin)
        ymin = int(ymin)
        xmax = int(xmax)
        ymax = int(ymax)

        b = [xmin, ymin, xmax, ymax, classid]
        allbb.append(b)
    return allbb


def start():
    count = 3000
    for filename in os.listdir(imagespath):

        if filename.endswith("".jpg"") or filename.endswith("".JPG""):
            title, ext = os.path.splitext(os.path.basename(filename))
            image = readImage(filename)
        if filename.endswith("".txt""):
            xmlTitle, txtExt = os.path.splitext(os.path.basename(filename))
            if xmlTitle == title:
                # bboxes = getCoordinates(filename)
                bboxes = readYolo(imagespath+xmlTitle+'.txt')
                for i in range(0, 9):
                    img = copy.deepcopy(image)
                    transform = getTransform(i)
                    try:
                        transformed = transform(image=img, bboxes=bboxes)
                        transformed_image = transformed['image']
                        transformed_bboxes = transformed['bboxes']
                        name = title+str(count)+'.jpg'
                        cv2.imwrite(name, transformed_image)
                        # print(transformed_bboxes)
                        # writeVoc(transformed_bboxes, count, transformed_image)
                        writeYolo(transformed_bboxes, count, title)
                        count = count+1
                    except:
                        print(""bounding box issues"")
                        pass

                # bboxes = [[int(float(j)) for j in i] for i in bb]


def writeVoc(bboxes, count, image):
    height, width, channels = image.shape
    imagepath = 'C:/Users/max_x/Desktop/all/'+str(count)+'.jpg'
    xmlpath = 'C:/Users/max_x/Desktop/all/'+str(count)+'.xml'
    writer = Writer(imagepath, width, height)

    for i in bboxes:
        writer.addObject(i[4], int(i[0]), int(i[1]), int(i[2]), int(i[3]))
    writer.save(xmlpath)


def readYolo(filename):
    coords = []
    with open(filename, 'r') as fname:
        for file1 in fname:
            x = file1.strip().split(' ')
            x.append(x[0])
            x.pop(0)
            x[0] = float(x[0])
            x[1] = float(x[1])
            x[2] = float(x[2])
            x[3] = float(x[3])
            coords.append(x)
    return coords


def writeYolo(coords, count, name):

    with open(name+str(count)+'.txt', ""w"") as f:
        for x in coords:
            f.write(""%s %s %s %s %s \n"" % (x[-1], x[0], x[1], x[2], x[3]))


def getTransform(loop):
    if loop == 0:
        transform = A.Compose([
            A.HorizontalFlip(p=1),
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 1:
        transform = A.Compose([
            A.RandomBrightnessContrast(p=1),
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 2:
        transform = A.Compose([
            A.MultiplicativeNoise(multiplier=0.5, p=0),
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 3:
        transform = A.Compose([
            A.VerticalFlip(p=1)
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 4:
        transform = A.Compose([
            A.Blur(blur_limit=(50, 50), p=0)
        ], bbox_params=A.BboxParams(format='pascal_voc'))
    elif loop == 5:
        transform = A.Compose([
            A.Transpose(1)
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 6:
        transform = A.Compose([
            A.RandomRotate90(p=1)
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 7:
        transform = A.Compose([
            A.JpegCompression(quality_lower=0, quality_upper=1, p=0.2)
        ], bbox_params=A.BboxParams(format='yolo'))
    elif loop == 8:
        transform = A.Compose([
            A.Cutout(num_holes=50, max_h_size=40,
                     max_w_size=40, fill_value=128, p=0)
        ], bbox_params=A.BboxParams(format='pascal_voc'))

    return transform

start()
```
",hi help image folder label selected augmentation save augmentation image along bounding image folder change line folder modify function get transform add remove import import import import import o import writer import import import import math import random import copy import return size width height item return start count title image title range image transform try transform name count name print count count title count except print bounding box pas float count image height width count count writer writer width height open file float float float float return count name open count loop loop transform loop transform loop transform loop transform loop transform loop transform loop transform loop transform loop transform return transform start,issue,positive,negative,negative,negative,negative,negative
753486085,"ToTensor should be the last op.


Сб, 2 янв. 2021 г. в 16:30, Mohamed Adel <notifications@github.com>:

> 🐛 Bug
>
> when I include ToTensorV2(), in the augmentation pipeline, an error
> occurred
> TypeError: Expected Ptr<cv::UMat> for argument 'src'
> To Reproduce
>
> Steps to reproduce the behavior:
>
>
>
> import albumentations as A
>
> import cv2
>
>
>
> # Declare an augmentation pipeline
>
> transform = A.Compose([
>
>         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
>
>         ToTensorV2(),
>
>         A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0.5, always_apply=True, p=1.0),
>
>         A.GaussNoise (var_limit=(10.0, 50.0), mean=0, always_apply=True, p=1.0)
>
>
>
>                   ])
>
>  # Read an image with OpenCV and convert it to the RGB colorspace
>
> image = cv2.imread(""./image.jpeg"")
>
> image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
>
>
>
> # Augment an image
>
> transformed = transform(image=image)
>
> transformed_image = transformed[""image""]
>
>
> ------------------------------
>
> TypeError                                 Traceback (most recent call last)
>
> <ipython-input-130-c8245c65e8aa> in <module>
>
>      17
>
>      18 # Augment an image
>
> ---> 19 transformed = transform(image=image)
>
>      20 transformed_image = transformed[""image""]
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/core/composition.py in __call__(self, force_apply, *args, **data)
>
>     180                     p.preprocess(data)
>
>     181
>
> --> 182             data = t(force_apply=force_apply, **data)
>
>     183
>
>     184             if dual_start_end is not None and idx == dual_start_end[1]:
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/core/transforms_interface.py in __call__(self, force_apply, *args, **kwargs)
>
>      87                     )
>
>      88                 kwargs[self.save_key][id(self)] = deepcopy(params)
>
> ---> 89             return self.apply_with_params(params, **kwargs)
>
>      90
>
>      91         return kwargs
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/core/transforms_interface.py in apply_with_params(self, params, force_apply, **kwargs)
>
>     100                 target_function = self._get_target_function(key)
>
>     101                 target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
>
> --> 102                 res[key] = target_function(arg, **dict(params, **target_dependencies))
>
>     103             else:
>
>     104                 res[key] = None
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/augmentations/transforms.py in apply(self, image, ksize, sigma, **params)
>
>    2619
>
>    2620     def apply(self, image, ksize=3, sigma=0, **params):
>
> -> 2621         return F.gaussian_blur(image, ksize, sigma=sigma)
>
>    2622
>
>    2623     def get_params(self):
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/augmentations/functional.py in wrapped_function(img, *args, **kwargs)
>
>      52     def wrapped_function(img, *args, **kwargs):
>
>      53         shape = img.shape
>
> ---> 54         result = func(img, *args, **kwargs)
>
>      55         result = result.reshape(shape)
>
>      56         return result
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/augmentations/functional.py in gaussian_blur(img, ksize, sigma)
>
>     732     # When sigma=0, it is computed as `sigma = 0.3*((ksize-1)*0.5 - 1) + 0.8`
>
>     733     blur_fn = _maybe_process_in_chunks(cv2.GaussianBlur, ksize=(ksize, ksize), sigmaX=sigma)
>
> --> 734     return blur_fn(img)
>
>     735
>
>     736
>
>
>
> ~/.local/lib/python3.6/site-packages/albumentations/augmentations/functional.py in __process_fn(img)
>
>     182                 else:
>
>     183                     chunk = img[:, :, index : index + 4]
>
> --> 184                     chunk = process_fn(chunk, **kwargs)
>
>     185                     chunks.append(chunk)
>
>     186             img = np.dstack(chunks)
>
>
>
> TypeError: Expected Ptr<cv::UMat> for argument 'src'
>
>
>
>
> Expected behavior
>
> if I call it without ToTensorV2(), No Error.
>
>
>
> import albumentations as A
>
> import cv2
>
>
>
> # Declare an augmentation pipeline
>
> transform = A.Compose([
>
>         A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),
>
>         #ToTensorV2(),
>
>         A.GaussianBlur(blur_limit=(3, 7), sigma_limit=0.5, always_apply=True, p=1.0),
>
>         A.GaussNoise (var_limit=(10.0, 50.0), mean=0, always_apply=True, p=1.0)
>
>
>
>                   ])
>
>  # Read an image with OpenCV and convert it to the RGB colorspace
>
> image = cv2.imread(""./image.jpeg"")
>
> image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
>
>
>
> # Augment an image
>
> transformed = transform(image=image)
>
> transformed_image = transformed[""image""]
>
>
> Environment
>
>    - Albumentations version ('0.5.2'):
>    - Python version (e.g., 3.6.9):
>    - OS (e.g., Linux):
>    - Pytorch '1.0.1'
>    - How you installed albumentations (pip ):
>
> Additional context
>
> tried with different image format .png / .jpg same.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/813>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YEOY557YVHDGJPFR33SX4U2DANCNFSM4VRDBW5A>
> .
>
",last bug bug include augmentation pipeline error argument reproduce reproduce behavior import import declare augmentation pipeline transform read image convert image image image augment image transform image recent call last module augment image transform image self data data data data none self id self return return self key key key else key none apply self image sigma apply self image return image self shape result result shape return result sigma sigma return else chunk index index chunk chunk chunk argument behavior call without error import import declare augmentation pipeline transform read image convert image image image augment image transform image environment version python version o pip additional context tried different image format thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
752865982,"Totally my mistake. I had another file ""albumentations.py"" in the same folder. Problem solved!",totally mistake another file folder problem,issue,negative,neutral,neutral,neutral,neutral,neutral
752865927,"Is there any response to this problem?

I think this is quite critical because it may produce background training patches which should be positive samples!
btw, I encounter this problem at using ShiftScaleRotate.",response problem think quite critical may produce background training positive encounter problem,issue,negative,positive,positive,positive,positive,positive
751534828,"@bmabey Randomly sampling within the dataloader for multiple images. Would be nice to augment labels as well. These can then be fed in with 
```
AUGMENTATION = MIXUP(p=0.6)  # Second image is 1-p, although ideally this scales for n mixups
aug = albumentations.AUGMENTATION(images=[image1, image2, ...], labels=[[0, 0, 1], [0, 1, 0]]) 
print(aug['labels'])
-->
[0, 0.4, 0.6]
```",randomly sampling within multiple would nice augment well fed augmentation second image although ideally scale image image print,issue,positive,positive,positive,positive,positive,positive
751479086,"I installed Version 0.5.2 via `conda install -c conda-forge albumentations`
``` shell
$ conda list
# packages in environment at D:\anaconda3:
#
# Name                    Version                   Build  Channel
_ipyw_jlab_nb_ext_conf    0.1.0                    py38_0
alabaster                 0.7.12                     py_0
albumentations            0.5.2              pyhd8ed1ab_0    conda-forge
anaconda                  2020.11                  py38_0
....                                   .....                          ......
```",version via install shell list environment name version build channel alabaster anaconda,issue,negative,neutral,neutral,neutral,neutral,neutral
750417107,"There already exists a ""scale"" parameter in both the transformations. What is the need for an explicit declaration?
Can you please provide a reference link? I don't get exactly what is to implement.
",already scale parameter need explicit declaration please provide reference link get exactly implement,issue,negative,positive,positive,positive,positive,positive
749852188,The easiest would be to look at the code for the existing transformations and implement new transforms in a similar way.,easiest would look code implement new similar way,issue,negative,positive,neutral,neutral,positive,positive
749418561,The library does not currently support border mods for bboxes and keypoints.,library currently support border,issue,negative,neutral,neutral,neutral,neutral,neutral
748940532,"@BloodAxe Yes, I read more about GaussNoise and now I understand that both images need to be the same size, I will try to resize them before applying this transformation. Thanks for your reply.",yes read understand need size try resize transformation thanks reply,issue,positive,positive,positive,positive,positive,positive
748651909,"~Would you mind adding shapes & dtypes of both targets?~

Generally speaking, this augmentation depends on target input, and therefore it generate noise image of the same shape as input image. If second target has different shape it will obviously fail. So I don't see any issue here. 

Maybe you can enforce size of images to be the same by moving PadIfNeeded to be the first transform.",mind generally speaking augmentation target input therefore generate noise image shape input image second target different shape obviously fail see issue maybe enforce size moving first transform,issue,negative,negative,neutral,neutral,negative,negative
748490666,"The short answer is no.



Сб, 19 дек. 2020 г. в 17:05, soumochatterjee <notifications@github.com>:

> Is there any way by which we can directly apply albumentation transforms
> on torch tensors or variable , not by converting torch tensor into numpy
> array and then applying albumentation transforms? I know it can be a lame
> question to put but as I am unable to find this out on google that why I
> thought to put this thing here directly.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/799>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YEOPA7FP3KEJ3FGOJTSVS6MFANCNFSM4VCLH2IA>
> .
>
",short answer way directly apply torch variable converting torch tensor array know lame question put unable find thought put thing directly thread reply directly view,issue,negative,negative,negative,negative,negative,negative
748446190,I'm not sure I got your question. But image augmentation based on depth map is not supported.,sure got question image augmentation based depth map,issue,negative,positive,positive,positive,positive,positive
748039001,"Got it, maybe the documentation can be more specific as yours is very similar to [OpenCV's](https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html) but the behavior is different.",got maybe documentation specific similar behavior different,issue,negative,neutral,neutral,neutral,neutral,neutral
748029515,"ToGray converts image to grayscale color space, but does not change shape. 
Transform implementaion:
```
def to_gray(img):
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    return cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)
```",image color space change shape transform gray return gray,issue,negative,neutral,neutral,neutral,neutral,neutral
747949900,"@BloodAxe our repository is finally online, please have a look: [image-statistics-matching](https://github.com/continental/image-statistics-matching), we can talk about the integration, it would be great if you could give us a short overview of Albumentation's architecture.",repository finally please look talk integration would great could give u short overview architecture,issue,positive,positive,positive,positive,positive,positive
747309326,"Might be that this is not the correct way to go.
But transformations should not be applied in the same way to an image and to a mask, I can't pass the mask as an image.

Simple example: I have a ReplayCompose. I apply a bunch of augmentations to an image.
Later, I generate a mask for this image in some way (the mask was not available when I first augmented the image).
Then I want to replay the same augmentations on that mask. I can't simply pass the mask as 'image'.
",might correct way go applied way image mask ca pas mask image simple example apply bunch image later generate mask image way mask available first augmented image want replay mask ca simply pas mask,issue,negative,positive,positive,positive,positive,positive
745237649,"Did you make any changes to the code of the library? To me this is likely to be the case, as ""fill_applie2d"" does not exists in our repo at all.",make code library likely case,issue,negative,neutral,neutral,neutral,neutral,neutral
745235703,Can not reproduce the problem. Can you provide a minimal example to reproduce the problem?,reproduce problem provide minimal example reproduce problem,issue,negative,negative,neutral,neutral,negative,negative
744651115,"You also need to add the transforms to other tests like 

https://github.com/albumentations-team/albumentations/blob/master/tests/test_transforms.py

https://github.com/albumentations-team/albumentations/blob/master/tests/test_augmentations.py

https://github.com/albumentations-team/albumentations/blob/master/tests/test_serialization.py

etc ",also need add like,issue,negative,neutral,neutral,neutral,neutral,neutral
744478684,"Yes, in my enviroment it works also different
I'm trying to figure out why

All remarks have been notified, thanks!",yes work also different trying figure notified thanks,issue,positive,positive,neutral,neutral,positive,positive
744457281,"Thank you for your help.
What version of imgaug your implementation based on?
It looks like in latest imgaug this transform works a little bit different.",thank help version implementation based like latest transform work little bit different,issue,positive,positive,positive,positive,positive,positive
744446612,"Hi! Thanks for your effort on helping us with imgaug.

I can give my 5cents with your first question:
> should we call new functions from imgaug-dependent ones(with ""deprecated"" warning) or should we completely destroy imgaug-dependent augs?

I'd say it's better to keep original names in the code & imports (But not in docs, he-he) and raise a warning:

```python
class Sharpen(A.ImageOnlyTransform):
   def __init__(...):
    ....
   ....

class IAASharpen(Sharpen):
   def __init__(...):
      warnings.warn(""This augmentation is deprecated. Please use A.Sharpen instead"").
```


Regarding second question - whenever possible we should keep the default behavior so the sake of backward compatibility. But exposing more parameters for more granular control of the augmentation is also highly appreciated!",hi thanks effort helping u give first question call new warning completely destroy say better keep original code raise warning python class sharpen class sharpen augmentation please use instead regarding second question whenever possible keep default behavior sake backward compatibility granular control augmentation also highly,issue,positive,positive,positive,positive,positive,positive
744044380,Why you do not provide mask as image? I do not think that these changes are a good idea,provide mask image think good idea,issue,negative,positive,positive,positive,positive,positive
740853771,Try to install directly from github: `!pip install git+https://github.com/albumentations-team/albumentations.git`,try install directly pip install,issue,negative,positive,neutral,neutral,positive,positive
739934749,Good point. I was too quick to bring this to table. Closing PR,good point quick bring table,issue,negative,positive,positive,positive,positive,positive
739851352,This could happen if image dimensions are  less than `min_height` and `min_width`. I agree that error message can be less cryptic.,could happen image le agree error message le cryptic,issue,negative,neutral,neutral,neutral,neutral,neutral
738069854,"Are you sure that OpenCV supports float16 data type? If not, many transforms will fail with this data type.",sure float data type many fail data type,issue,negative,positive,positive,positive,positive,positive
737866089,"Hi,

I'm really doubt this is caused by an error on our side. Please double-check your pipeline and if the problem persist, re-open this issue with the minimal example to reproduce unexpected behavior. It would also help if you follow issue template, to provide us all the relevant information. ",hi really doubt error side please pipeline problem persist issue minimal example reproduce unexpected behavior would also help follow issue template provide u relevant information,issue,negative,positive,positive,positive,positive,positive
737415062,It would be very useful to avoid installing a different version of opencv if one already exists,would useful avoid different version one already,issue,negative,positive,positive,positive,positive,positive
737346794,"Thank you for example when our code fails.
I think, that problem that some libraries adds to `x_min` and `y_min` 1 pixel, but another libraries don't do this.
I do not know how resolve this problem beautifully.
In my opinion this code will fix this problem:
```python
x, y, width, height = denormalize_bbox(bbox, rows, cols)

x_min = x - width / 2 + 1
x_max = x_min + width
y_min = y - height / 2 + 1
y_max = y_min + height

if x_max > rows:
    x_max -= 1
    x_min -= 1
    y_max -= 1
    y_min -= 1
```

Also would be great if you will add this case to test in `tests/test_bbox.py`",thank example code think problem another know resolve problem beautifully opinion code fix problem python width height width width height height also would great add case test,issue,positive,positive,positive,positive,positive,positive
737308756,"It is not a bug. It is done on purpose.

It is much better to do label cleaning at the data preparation step rather than during training.

I would recommend fixing bounding box coordinates before augmentations.
",bug done purpose much better label cleaning data preparation step rather training would recommend fixing bounding box,issue,positive,positive,positive,positive,positive,positive
736549258,"Hi,

I don't quite see how this PR address negative values. Can you please elaborate?",hi quite see address negative please elaborate,issue,negative,positive,neutral,neutral,positive,positive
736521648,"I think that better to resolve this problem on dataloder side. As an example, ff your targets are empty you can provide empty lists for bboxes and keypoints.",think better resolve problem side example empty provide empty,issue,negative,positive,positive,positive,positive,positive
734454278,"Adding my solution (might help someone else )
If you are sure there is  no problem in your bounding boxes ,only floating point precision issue then you can add this block to the function 
``` python 
def check_bbox(bbox):
    """"""Check if bbox boundaries are in range 0, 1 and minimums are lesser then maximums""""""
   #my added block 
    bbox=list(bbox)
    for i in range(4):
      if (bbox[i]<0) :
        bbox[i]=0
      elif (bbox[i]>1) :
        bbox[i]=1
    bbox=tuple(bbox)
   #end of block
   #rest of the code as it is
```
the directory 
```
/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/bbox_utils.py
```
N.B : also check  the indentation when  add the code segment  ",solution might help someone else sure problem bounding floating point precision issue add block function python check range lesser added block range end block rest code directory also check indentation add code segment,issue,negative,positive,positive,positive,positive,positive
734236681,Library supports  4 channels image. Change default values for `Normalize` transform. They are must be single value or their length must equal to image channels count.,library image change default normalize transform must single value length must equal image count,issue,negative,negative,neutral,neutral,negative,negative
732065629,"Lot of things makes Albumentations unique. To start, we offer the largest number of image augmentations than any other image augmentations library. This includes color augmentations, spatial transformations, style transfers, various dropout techniques and may others.

Secondly, Albumentations is battle-tested. Meaning it has been widely adopted in the ML industry (https://github.com/albumentations-team/albumentations#used-by) and in competitive ML, being a part of winning many solutions (https://github.com/albumentations-team/albumentations#competitions-won-with-the-library).

Third, we offer native support for augmentation of image & mask, keypoints & bounding boxes, which covers the most use cases for image classification, image segmentation, object detection and instance segmentation. 

Lastly, unlike torchvision, Albumentations is not pytorch-only library. It can be used together with any DL framework of your choice or even without it. 

And yes, it is much faster, than torchvision - https://github.com/albumentations-team/albumentations#benchmarking-results.",lot unique start offer number image image library color spatial style various dropout may secondly meaning widely adopted industry competitive part winning many third offer native support augmentation image mask bounding use image classification image segmentation object detection instance segmentation lastly unlike library used together framework choice even without yes much faster,issue,positive,positive,positive,positive,positive,positive
731038164,Try latest version from repository `pip install git+https://github.com/albumentations-team/albumentations`. I think we fixed this problem,try latest version repository pip install think fixed problem,issue,negative,positive,positive,positive,positive,positive
730321063,"Thanks for information. We will try to resolve this problem us soon us possible.
At this time you may try to concatenate your mask along last axis to create single mask with shape `[Height, Width, NumMasks]` and provide it with keyword `mask`",thanks information try resolve problem u soon u possible time may try concatenate mask along last axis create single mask shape height width provide mask,issue,positive,positive,neutral,neutral,positive,positive
728845619,"> What does this change do?
> 
> We do not use `histogram_matching` in the rest of the file.

please check https://github.com/scikit-image/scikit-image/blob/v0.15.x/skimage/transform/histogram_matching.py the function name is match_histograms

Starting from version 0.16.x they move this function match_histograms to this file https://github.com/scikit-image/scikit-image/blob/v0.16.x/skimage/exposure/histogram_matching.py

However they still have the same function in https://github.com/scikit-image/scikit-image/blob/v0.16.x/skimage/transform/histogram_matching.py

so for precautions i put it in a try except if in case they remove it.",change use rest file please check function name starting version move function file however still function put try except case remove,issue,negative,neutral,neutral,neutral,neutral,neutral
728839716,You need to ask yolov5 developers. As I know they are use their custom transforms. I think you need to write your own dataset and image processing pipeline.,need ask know use custom think need write image pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
728190310,"What does this change do?

We do not use `histogram_matching` in the rest of the file.
",change use rest file,issue,negative,neutral,neutral,neutral,neutral,neutral
727868264,"Order of keypoints does not changed, but this is not guaranteed. You can add additional data to the tail of your keypoint to restore order order.
For example:
```python
keypoints = [val + [i] for i, val in enumerate(keypoints)]
res = transform(image=image, keypoints=keypoints)
keypoints = [i[:-1] for i in sorted(res['keypoints'])]
```",order add additional data tail restore order order example python enumerate transform sorted,issue,negative,neutral,neutral,neutral,neutral,neutral
727520081,"> This function is available in `scikit-image` >= 0.18.
> We should prehaps update https://github.com/albumentations-team/albumentations/blob/master/conda.recipe/meta.yaml with this dependency.

I search for scikit-image >= 0.18 using IBM Anaconda powerai-release=1.6.2 but unfortunately i couldn't find even in their github [https://github.com/scikit-image/scikit-image](url). I check anther PC that have scikit-image = 0.16.2 and it work fine.

The IBM server I'm using is incomparable with scikit-image = 0.16.2 due to conflicts in packages.

Anyway i found that match_histograms was in transforms folder back in version 0.15 so what i did is changing this line:
```
~/anaconda2/envs/wmlce_env/lib/python3.6/site-packages/albumentations/augmentations/domain_adaptation.py in <module>
      8 from .functional import clipped, preserve_shape
      9
---> 10 from skimage.exposure import match_histograms
     11
     12 from .utils import read_rgb_image
```

to be

`from skimage.transform import histogram_matching`

and that solve the problem of importing `import albumentations as A.` i don't know if there will be any other problem related to this change in other functions or not since i didn't check.

@BloodAxe thanks.
",function available prehaps update dependency search anaconda unfortunately could find even check anther work fine server incomparable due anyway found folder back version line module import clipped import import import solve problem import know problem related change since check thanks,issue,negative,positive,neutral,neutral,positive,positive
727278294,"This function is available in `scikit-image` >= 0.18.
We should prehaps update https://github.com/albumentations-team/albumentations/blob/master/conda.recipe/meta.yaml with this dependency.",function available prehaps update dependency,issue,negative,positive,positive,positive,positive,positive
727278050,"You need to set `mask_fill_value` when creating `A.CoarseDropout` to apply this augmentation to mask too. By default, dropout to mask is not applied.",need set apply augmentation mask default dropout mask applied,issue,negative,neutral,neutral,neutral,neutral,neutral
726198077,"Read documentation https://albumentations.ai/docs/examples/example_bboxes/
All additional data must be at the end of bbox.",read documentation additional data must end,issue,negative,neutral,neutral,neutral,neutral,neutral
725527211,"> Of course, this would work only for non-overlapping instances.

That's exactly the problem.",course would work exactly problem,issue,negative,positive,positive,positive,positive,positive
723977960,"You can add person id to tail of each keypoint or use additional target for different persons.
https://albumentations.ai/docs/examples/example_multi_target/",add person id tail use additional target different,issue,negative,neutral,neutral,neutral,neutral,neutral
723854031,"Yes yes, but that is a very basic usage. Say you want to do this simple feature: create a transform that given a tuple of (image, boxes, mask) in numpy it transforms it to torch.Tensor... How would you do it? Well, I come up with this, but to the best of my knowledge, how to do more advanced  stuff is not documented.

```python
class ToTensorCustom(A.BasicTransform):
    """"""Convert image and mask to `torch.Tensor`
    * Image numpy: [H, W, C] -> Image tensor: [C, H, W]
    * Mask numpy: [H, W, 1] -> Mask tensor: [1, H, W]
    """"""
    def __init__(self, always_apply=True, p=1.0):
        super().__init__(always_apply=always_apply, p=p)

    @property
    def targets(self):
        return {""image"": self.apply, ""mask"": self.apply_to_mask}

    def apply(self, img, **params):
        """"""Image from numpy [H, W, C] to tensor [C, H, W]""""""
        return torch.from_numpy(img.transpose(2, 0, 1))

    def apply_to_mask(self, mask, **params):
        """"""Mask from numpy [H, W] to tensor [1, H, W]""""""
        # Adding channel to first dim if mask has no channel
        if mask.ndim == 2:
            mask = np.expand_dims(mask, axis=0)
        # Transposing channel to channel first if mask has channel
        elif mask.ndim == 3:
            # [H, W, C] to tensor [C, H, W] in case mask has C > 1
            mask = mask.transpose(2, 0, 1)
        else:
            raise ValueError('Mask should have shape [H, W] without, '
                             'channel however provided mask shape was: '
                             '{}'.format(mask.shape))
        # To numpy
        return torch.from_numpy(mask)
```

So far so good. But I have not been able to do the same for bounding boxes. I know you have to add it to the targets (which is just slightly mentioned in the docstring from the core.py...) but I tried everything (including using a DualTrasnform) and could not get it working. So I think this should be better documented, what is the flow of the albumentations lib, I went through it with the debugger but it is not very clear (specially the argument passing to the functions transforms)",yes yes basic usage say want simple feature create transform given image mask would well come best knowledge advanced stuff python class convert image mask image image tensor mask mask tensor self super property self return image mask apply self image tensor return self mask mask tensor channel first dim mask channel mask mask channel channel first mask channel tensor case mask mask else raise shape without however provided mask shape return mask far good able bounding know add slightly tried everything could get working think better flow went clear specially argument passing,issue,positive,positive,positive,positive,positive,positive
723018281,"In the [documentation](https://albumentations.ai/docs/getting_started/mask_augmentation/), the use of multiple masks using a list of masks is mentioned. At the moment I have to use `additional_targets` for `ToTensorV2` to work. Is there a chance to get it working with the `masks` parameter?",documentation use multiple list moment use work chance get working parameter,issue,negative,neutral,neutral,neutral,neutral,neutral
722945336,Please give reproducible example. I do not understand what you trying to do and what data you  are using.,please give reproducible example understand trying data,issue,negative,neutral,neutral,neutral,neutral,neutral
722622031,"For instance segmentation you can use one image of uint16 dtype, where each instance object is encoded with unique value. Of course, this would work only for non-overlapping instances. ",instance segmentation use one image instance object unique value course would work,issue,negative,positive,positive,positive,positive,positive
722620967,"This is not a bug at all. If you have only bounding boxes (not instance masks), you don't have information of the shape of the object inside. Therefore when you rotate an image, you cannot guess what the new shape of the object is.  All augmentations that support augmentation of bounding boxes works in a similar way - they augment 4 corners and make a new bounding box. In case of rotation it will lead to possibly inflated boxes. This is a known behavior and not a bug.",bug bounding instance information shape object inside therefore rotate image guess new shape object support augmentation bounding work similar way augment make new bounding box case rotation lead possibly inflated known behavior bug,issue,negative,positive,neutral,neutral,positive,positive
722411230,"Good point. However, I'm dealing with instance segmentation, where you have multiple masks per image.",good point however dealing instance segmentation multiple per image,issue,negative,positive,positive,positive,positive,positive
722405419,"It is very strange behavior to remove empty mask, because when you train segmentation model you need mask. In common if mask empty all values will be equal to 0.",strange behavior remove empty mask train segmentation model need mask common mask empty equal,issue,negative,negative,negative,negative,negative,negative
721757465,I think it is good feature. So let it be open.,think good feature let open,issue,negative,positive,positive,positive,positive,positive
721750737,"Ah yes ok ok, currently I was thinking to only change the color in mask region. Obviously, it seems difficult to apply a rotation only in mask region.

In any case, thank you for your answer. Do I close this issue with your `maskwrapper` solution ?",ah yes currently thinking change color mask region obviously difficult apply rotation mask region case thank answer close issue solution,issue,positive,negative,negative,negative,negative,negative
721745582,"### In the record format

All images are of shape 512x512.
I can provide you the list of Bounding boxes as a pth file.
To upload it here, I renamed it bboxes.zip, I guess you can rename it back to bboxes.pth and it should work.

Then you can do:
bboxes = torch.load('bboxes.pth')
to check all of them.
[bboxes.zip](https://github.com/albumentations-team/albumentations/files/5488130/bboxes.zip)

### In the dataset format
Then I am using transformation via the icevision library on those records. The one used is called aug_transforms and is called like this line:
train_tfms = tfms.A.Adapter([*tfms.A.aug_tfms(size=size, presize=presize), tfms.A.Normalize()])
with size=512, presize=1050.
aug_tfms is documented [here](https://airctic.com/albumentations/).
I also joined the bouding boxes after transformations, called bboxes_dataset.zip (that should be renamed as bboxes_dataset.pth)
[bboxes_datasets.zip](https://github.com/albumentations-team/albumentations/files/5488168/bboxes_datasets.zip)
",record format shape provide list bounding file guess rename back work check format transformation via library one used like line also,issue,negative,neutral,neutral,neutral,neutral,neutral
721722707,"In example I combine geometric(`LongestMaxSize`) and color transforms.
At this time I do not know how apply geometric transforms like rotations and flips only in mask regions.
If you will use my wrapper, you need to wrap transforms, that you want to apply only on mask region.

And yes, this modification increase computation time.",example combine geometric color time know apply geometric like mask use wrapper need wrap want apply mask region yes modification increase computation time,issue,positive,neutral,neutral,neutral,neutral,neutral
721716157,"Ok Thank you. If I understand well, this way of doing would allow to apply color transformation only on image[mask]. And if I want to do it also when there is geometrics transformations, it might not work ? Is it possible do just apply transformations one  after the other, like : 

```
sample  = augmentation_geometric(image=image, mask=mask) # flip, scale, rotate, shift, perspective, distortion ...
sample  = augmentation_mask_color(image=sample['image'], mask=sample['mask']) # rgbshift where mask is, with MaskWrapper
sample  = augmentation_color(image=sample['image'], mask=sample['mask']) # GaussNoise, ISONoise, luminosity, blur, contrast ...
```
_Though, it might increase the computation time ?_

",thank understand well way would allow apply color transformation image mask want also might work possible apply one like sample flip scale rotate shift perspective distortion sample mask sample luminosity blur contrast might increase computation time,issue,positive,neutral,neutral,neutral,neutral,neutral
721699864,"Interesting idea.
But at this time I do not know how to do so that it would work with any transforms.
For now you can use this wrapper:
```python
import numpy as np
import albumentations as A
import matplotlib.pyplot as plt


class MaskWrapper(A.Sequential):
    def __call__(self, **data):
        image = data['image'].copy()

        data = super(MaskWrapper, self).__call__(**data)

        mask = data['mask'].astype(np.bool)
        image[mask] = data['image'][mask]
        data['image'] = image

        return data

transforms = A.Compose([
    MaskWrapper([A.GaussianBlur(p=1), A.RGBShift([50, 50], p=1)]),
    A.LongestMaxSize(512),
    MaskWrapper([A.HueSaturationValue(p=1)])
])

img = A.read_rgb_image(""true.png"")
mask = np.zeros_like(img)
mask[:500, :500] = 1

res = transforms(image=img, mask=mask)

plt.subplot(211, title=""original"")
plt.imshow(img, vmin=0, vmax=255)
plt.subplot(212, title=""result"")
plt.imshow(res['image'], vmin=0, vmax=255)
plt.show()

```
![image](https://user-images.githubusercontent.com/7512250/98111116-2e506f00-1eb1-11eb-9a03-0c0cc5903452.png)
",interesting idea time know would work use wrapper python import import import class self data image data data super self data mask data image mask data mask data image return data mask mask original result image,issue,positive,positive,positive,positive,positive,positive
721695507,Now the installation of the latest version correctly works in Google Colab and on macOS. Closing the issue.,installation latest version correctly work issue,issue,negative,positive,positive,positive,positive,positive
721689225,"At this time you can provide [additional_targets](https://albumentations.ai/docs/examples/example_multi_target/) to work with sequences of images.
We will try to add more clear API for work with images, but we can not tell how many time it's need.",time provide work try add clear work tell many time need,issue,negative,positive,positive,positive,positive,positive
721686181,Can you provide image shape and bboxes to reproduce this problem?,provide image shape reproduce problem,issue,negative,neutral,neutral,neutral,neutral,neutral
721142862,"And

```
import imgaug
imgaug.random.seed(123)
```

for IAA transforms, they use a different seed.",import use different seed,issue,negative,neutral,neutral,neutral,neutral,neutral
721112735,Worked for me: `pip install -U pip` before installing albumentations.,worked pip install pip,issue,negative,neutral,neutral,neutral,neutral,neutral
721040287,"It looks like the problem is related to uploading a new release of OpenCV to PyPI and should be resolved automatically when all wheels with the latest version of OpenCV will be uploaded - https://github.com/skvark/opencv-python/issues/411. In the meantime, a workaround from @akatasonov should help.",like problem related new release resolved automatically latest version help,issue,negative,positive,positive,positive,positive,positive
719694798,This would be a very useful feature.  Any plants to address this?  It would be useful to pass in an array of frames as opposed to naming each one as an element of a dictionary so that you could pass in videos of variable length.,would useful feature address would useful pas array opposed naming one element dictionary could pas variable length,issue,positive,positive,positive,positive,positive,positive
717889537,"Yes! I had to write a custom TensorV2 transform! I really think this Issue is necessary, my contribution:

```python
 def apply_to_mask(self, mask, **params):
        """"""Mask from numpy [H, W] to tensor [1, H, W]""""""
        # Adding channel to first dim if mask has no channel
        if mask.ndim == 2:
            mask = np.expand_dims(mask, axis=0)
        # Transposing channel to channel first if mask has channel
        elif mask.ndim == 3:
            # [H, W, C] to tensor [C, H, W] in case mask has C > 1
            mask = mask.transpose(2, 0, 1)
        else:
            raise ValueError('Mask should have shape [H, W] without, '
                             'channel however provided mask shape was: '
                             '{}'.format(mask.shape))
        # To numpy
        return torch.from_numpy(mask)
```",yes write custom transform really think issue necessary contribution python self mask mask tensor channel first dim mask channel mask mask channel channel first mask channel tensor case mask mask else raise shape without however provided mask shape return mask,issue,negative,positive,positive,positive,positive,positive
717786419,"All transforms are optimized to work in single thread.
Library assumes that you will manage with threads by himself, for example using multithreaded data loader from [pytorch](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).",work single thread library manage example multithreaded data loader,issue,negative,negative,neutral,neutral,negative,negative
717690941,"Do you guys have a recommended way to run many images through an albumentation pipeline so as to utilize all CPU cores available? Let's say images are all in a numpy array, int8.",way run many pipeline utilize available let say array,issue,negative,positive,positive,positive,positive,positive
717118988,"If we will use `kornia==0.4.1` we can benchmark more transforms
`Posterize`
```python
    def kornia_op(self, img):
        return KF.apply_posterize(img, {""bits_factor"": 4})
```

`Solarize`
```python
    def kornia_op(self, img):
        return KF.apply_solarize(img)
```

`Euqalize`
```python
    def kornia_op(self, img):
        return KF.apply_equalize(img)
```",use posterize python self return solarize python self return python self return,issue,negative,neutral,neutral,neutral,neutral,neutral
717106602,"Hey, @megaserg.

Could you please add the following changes to PR?

1. Rebase the branch on `master`.  The master branch already contains fixes for working with the latest version of solt.
2. Add a command-line argument that specifies a device for Kornia (should be `cpu` by default). The benchmarking code should move PyTorch Tensors to the specified device.",hey could please add following rebase branch master master branch already working latest version add argument device default code move device,issue,negative,positive,positive,positive,positive,positive
715850884,"I just had another project with similar bottelneck. In my understanding, problem is that augmented images are used only once in a training cycle of a model, not even across several epochs. So it's a huge waste of GPU cycles while CPU is struggling. I considered DALI, but it is not keras-friendly, and my goal was to stay with Keras.
I found following solution: 
1) I generate as many augmentations in RAM as I can
2) I use them for training across a frame of epochs, 10 to 30, whatever it takes to get a noticeable convergence
3) after that I generate new batch of augmented images (by implementing on_epoch_end) and process goes on automatically.

This approach most of the time keeps GPU busy, while being able to benefit from data augmentation. I use custom Sequence subclass to generate augmentation and fix classes imbalance at the same time.",another project similar understanding problem augmented used training cycle model even across several huge waste struggling considered dali goal stay found following solution generate many ram use training across frame whatever get noticeable convergence generate new batch augmented process go automatically approach time busy able benefit data augmentation use custom sequence subclass generate augmentation fix class imbalance time,issue,negative,positive,positive,positive,positive,positive
714497121,"print(A.__version__)
0.5.0
Os is Ubuntu
Python is 3.8
----------------------------------------
Update:
Solved.
I need to do the following:
from albumentations import DualTransform,BasicTransform
By then, restart the console, now it works.",print o python update need following import restart console work,issue,negative,neutral,neutral,neutral,neutral,neutral
714485442,"Please double check your pipeline and/or environment since it cannot be
reproduced by me too.

чт, 22 окт. 2020 г. в 10:57 AM, Mikhail Druzhinin <notifications@github.com
>:

> It is very strange error, I can't reproduce it.
> Try to print albumentations version and check that it is correct:
> print(A.__version__)
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/733#issuecomment-714307430>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YCNRT4QUD4S5CUGW5TSL7QYJANCNFSM4S2PZ4BQ>
> .
>
",please double check pipeline environment since strange error ca reproduce try print version check correct print thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
714307430,"It is very strange error, I can't reproduce it.
Try to print albumentations version and check that it is correct: `print(A.__version__)`",strange error ca reproduce try print version check correct print,issue,negative,negative,neutral,neutral,negative,negative
713819784,"but when  I try to read the image as np.uint8. It gives
a big error with 
KeyError: dtype('float64')",try read image big error,issue,negative,neutral,neutral,neutral,neutral,neutral
713802483,Try to update albumentations to latest version.,try update latest version,issue,negative,positive,positive,positive,positive,positive
713780207,"If you will look to [documentations about this transform](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.CLAHE) you can see that it is supports only `uint8` data type.
Do image normalization in the last step because not all transforms can work with `float` data type. Another reason to use `uint8` data type - it is much more faster then float",look transform see data type image normalization last step work float data type another reason use data type much faster float,issue,negative,positive,neutral,neutral,positive,positive
713449926,It seems that I can use a filed of label_fields contains face id. Thank you,use face id thank,issue,negative,neutral,neutral,neutral,neutral,neutral
713443062,"Another question. My keypoints is a nested list of (x, y) tuples like this:
```
[
 [(923, 517), (914, 526)], 
 [(1133, 553), (1156, 562), (1179, 571)], 
 [(147, 661), (105, 670), (62, 679), (20, 688)], 
 [(1000, 526), (1010, 535)],
]
```
That is I have keypoints of four faces. Each face has different number of keypoints. When I passed this nested list to transform, I got an error. So I flat the nested list, but this op will not make sense because some keypoints will lie out of image after one transform in a compose. Since I couldn't get keypoints after every transform. I could only get the keypoints after all transforms in a compose. Possibly only filter on keypoints after all transforms in a compose will get a wrong result. Do you have any idea on how to process this question? Thank you   ",another question list like four face different number list transform got error flat list make sense lie image one transform compose since could get every transform could get compose possibly filter compose get wrong result idea process question thank,issue,negative,negative,negative,negative,negative,negative
713435036,Do you mean pass the keypoint index as a filed of label_fields to the transform?,mean pas index transform,issue,negative,negative,negative,negative,negative,negative
713369688,"Keypoints list may change in situations where we need to remove invisible keypoints. Otherwise, the order of the keypoints will be the same.
To ensure that the order hasn't  changed, you can add the keypoint index to the tail of the keypoint.",list may change need remove invisible otherwise order ensure order add index tail,issue,negative,neutral,neutral,neutral,neutral,neutral
709846315,"In the last PR this thing was implemented as `limit_bbox` parameter to `GridDistortion`, now it's a new class `NormalizedGridDistortion`. I'm not really sure which approach is better. I ended up creating a new class since I started implementing this outside of albumentations and deriving from `GridDistortion`.",last thing parameter new class really sure approach better ended new class since outside,issue,positive,positive,positive,positive,positive,positive
709241856,"Masks are the images, so they are must have the shape `[height, width, channels]` and must be `numpy.ndarray`
The author of the issue provided mask in the wrong format, as I understand in the format `[channels, height, width]`

If you want to change masks you need at first decode them to `numpy.ndarrya` and then encode them into polygons. For this purposes you can use pycocotools.",must shape height width must author issue provided mask wrong format understand format height width want change need first decode encode use,issue,negative,negative,negative,negative,negative,negative
708877997,"@ternaus Has this been fixed? I have coco json polygon annotations and I want to crop them (image, bbox, and mask) then write the new cropped coco_json files.",fixed coco polygon want crop image mask write new,issue,negative,positive,positive,positive,positive,positive
708410604,Finally we are added our implementation of `CollorJitter` that equal to torchvision transform.,finally added implementation equal transform,issue,negative,neutral,neutral,neutral,neutral,neutral
708318861,"Use this function to visualize angles:
```python
def vis_keypoints(image, keypoints, diameter=15):
    image = image.copy()

    for (x, y, s, a) in keypoints:
        cv2.circle(image, (int(x), int(y)), diameter, KEYPOINT_COLOR, -1)

        # (x0,y0) is a point at an angle of `a` from (x,y) and at a distance of `s`
        x0 = x + s * np.cos(a)
        y0 = y - s * np.sin(a)
        cv2.arrowedLine(image, (int(x), int(y)), (int(x0), int(y0)), KEYPOINT_COLOR, 2)

    plt.axis('off')
    plt.imshow(image)
```
`np.pi / 2.0` equal to 90 degree, so arrow must look to the top of image. Positive angle means counter-clockwise rotation. It is common interpretation of angles.",use function visualize python image image image diameter point angle distance image image equal degree arrow must look top image positive angle rotation common interpretation,issue,positive,positive,positive,positive,positive,positive
708104455,"@KyloRen1 This is a good question.

The legal story in the world of machine learning is not clear at all.

BatchNorm and DropOut are patented - we use them.

[Pytorch](https://github.com/pytorch/vision/issues/2597), [Tensorflow](https://github.com/tensorflow/models/issues/9131), [OpenVino](https://github.com/openvinotoolkit/open_model_zoo/issues/1433) teams have no clue if their pre-trained models could be used for commercial purposes.

I have mixed feelings and our opinions within the core team diverged.

Let's stay in the safe zone and not add the transform. We may revisit this if there will be evidence that it adds a lot of value to the model training.",good question legal story world machine learning clear dropout use clue could used commercial mixed within core team let stay safe zone add transform may revisit evidence lot value model training,issue,positive,positive,positive,positive,positive,positive
707134613,"Hello, this problem is still relevant. 
![Screenshot from 2020-10-12 16-52-32](https://user-images.githubusercontent.com/16117657/95754236-664a0500-0cab-11eb-9552-fabbe77f1dd9.png) 
The readme data is misleading.
",hello problem still relevant data misleading,issue,negative,positive,positive,positive,positive,positive
706592095,Is it ok with the Albumentation policy to add algorithms covered by active patents? https://github.com/scikit-image/scikit-image/issues/3646,policy add covered active,issue,negative,negative,negative,negative,negative,negative
706190779,"Some of your transforms need expensive computations.
For example in your pipeline I see `GlassBlur`, this transform repeats `GaussianBlur` several times and do many substitutions on image.
If you will remove `GlassBlur` you will see high speed increase.",need expensive example pipeline see transform several time many image remove see high speed increase,issue,negative,positive,neutral,neutral,positive,positive
706167087,"> Can you provide keras pipeline that is faster, than Albumentations?
> I will be very grateful to see it and trying to improve Albumentations results.
> Maybe albumentations used single thread? Many our transforms by default used single thread, but keras can use more than 1 thread for some functions.
> 
> About GPU augmentations it is complex problem.
> In many cases we can not have enough memory and GPU power to do augmentations on GPU. For this reason current strategy to delegate part of work to CPU. When CPU do augmentations GPU do inference on another batch.
> At this time I recommend to use [kornia](https://github.com/kornia/kornia) or [DALI](https://github.com/NVIDIA/DALI) if you need GPU augmentations.

I did not mean keras pipeline was faster than Albumentations... Albumentations pipepline had much more different possible transformations to. I was just wondering if 0.5 sec is acceptable for a 800x600 image and the above mentioned pipeline. Thanks for pointing at Dali!",provide pipeline faster grateful see trying improve maybe used single thread many default used single thread use thread complex problem many enough memory power reason current strategy delegate part work inference another batch time recommend use dali need mean pipeline faster much different possible wondering sec acceptable image pipeline thanks pointing dali,issue,positive,positive,neutral,neutral,positive,positive
706161482,"Can you provide keras pipeline that is faster, than Albumentations?
I will be very grateful to see it and trying to improve Albumentations results.
Maybe albumentations used single thread? Many our transforms by default used single thread, but keras can use more than 1 thread for some functions.

About GPU augmentations it is complex problem.
In many cases we can not have enough memory and GPU power to do augmentations on GPU. For this reason current strategy to delegate part of work to CPU. When CPU do augmentations GPU do inference on another batch.
At this time I recommend to use [kornia](https://github.com/kornia/kornia) or [DALI](https://github.com/NVIDIA/DALI) if you need GPU augmentations.",provide pipeline faster grateful see trying improve maybe used single thread many default used single thread use thread complex problem many enough memory power reason current strategy delegate part work inference another batch time recommend use dali need,issue,positive,positive,neutral,neutral,positive,positive
704674002,"I would like to add that for images with 3 dimensions (in my case just a single color channel just to sync up the dimensions with the image), it is very unnatural how only the image's dimensions are transposed:
```
    def apply(self, img, **params):  # skipcq: PYL-W0613
        return torch.from_numpy(img.transpose(2, 0, 1))

    def apply_to_mask(self, mask, **params):  # skipcq: PYL-W0613
        return torch.from_numpy(mask)
```
I had to manually squeeze the mask to get rid of the dimension, then unsqueeze it after transformation like @lartpang above.
",would like add case single color channel sync image unnatural image apply self return self mask return mask manually squeeze mask get rid dimension transformation like,issue,positive,negative,neutral,neutral,negative,negative
703872690,Turns out it was trivial. I was calling the generator twice to get the image and mask.,turn trivial calling generator twice get image mask,issue,negative,neutral,neutral,neutral,neutral,neutral
703856804,"Don't use openCV for this image processing step. 
scipy.signal.medfilt2d(image, kernel_size) allows for float64 with larger kernel sizes",use image step image float kernel size,issue,negative,neutral,neutral,neutral,neutral,neutral
703447138,"Oh. So, errors happen due to Normalize call, I did not realize that from start, thank you! I removed Normalize entirely, because actually for training a Detector it's not needed, and for training a Recognizer keras-ocr does scaling to 0..1 on its own. Now all my images are uint8 throughout whole pipeline, errors are gone, however, there is no visible speed improvement.

Regarding 2), ~0.5 sec per image is pure overhead of Albumentations, compared to a ""clean"" keras-ocr pipeline. Is it normal? Seems a bit high for a modern CPU, or not?

Also, I forgot to mention, are there any plans to move at least some of transforms to GPU? As it's not utilized at all, while CPU is struggling and is an obvious bottleneck in the ML pipeline.",oh happen due normalize call realize start thank removed normalize entirely actually training detector training recognizer scaling throughout whole pipeline gone however visible speed improvement regarding sec per image pure overhead clean pipeline normal bit high modern also forgot mention move least struggling obvious bottleneck pipeline,issue,positive,positive,neutral,neutral,positive,positive
703416194,"Greetings.

1)
Usually all augmentation transforms i. Albumentations has docs, which
states what image types are supported. For sake speed it’s recommended to
stay within uint8 style as it could be up to 4 times faster than float32.

Usually you want A.Normaize to be a last operation in your pipeline.

 2)
When we compare speed, it’s always to measure apples to apples. Could you
show augmentation pipeline in keras that you comparing to?


3) 4-corner polygons is a different kind of targets, which is good to have.
Yet it is not on the roadmap. However we are very open to PR with new
features

пн, 5 окт. 2020 г. в 8:34 AM, fingoldo <notifications@github.com>:

>
>
> I am trying to extend keras-ocr with albumentations by wrapping its native
> image generator with custome one:
>
>
> def get_image_generator(**args):
>
>     import albumentations as A
>
>
>
>     p=0.9
>
>     pipe=A.Compose([
>
>         ### Weather
>
>         A.OneOf([
>
>             A.RandomRain(p=p),
>
>             A.RandomSunFlare(flare_roi=(0, 0, 1, 0.5), angle_lower=0.5, p=p),
>
>             A.RandomShadow(num_shadows_lower=1, num_shadows_upper=3, shadow_dimension=7, shadow_roi=(0, 0.5, 1, 1), p=p),
>
>         ],p=p),
>
>         ### Colors, channels
>
>         A.OneOf([
>
>             A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15,p=p),
>
>             A.ToGray(p=p,),
>
>             A.ToSepia(p=p,),
>
>             A.RandomBrightnessContrast(p=p),
>
>             A.RandomGamma(p=p)    ,
>
>             A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=50, val_shift_limit=50, p=p),
>
>             A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225),p=p),
>
>             A.Equalize(p=p,),
>
>             A.ChannelShuffle(p=p),
>
>             A.ChannelDropout(p=p),
>
>         ],p=p),
>
>
>
>         ### Blurring/sharpening
>
>         A.OneOf([
>
>             A.Blur(p=p,blur_limit=30),
>
>             A.GlassBlur(p=p,),
>
>             A.MotionBlur(p=p,blur_limit=20),
>
>             A.MedianBlur(blur_limit=9, p=p),
>
>             A.IAASharpen(p=p,),
>
>         ],p=p),
>
>         ### Noise
>
>         A.OneOf([
>
>             A.IAAAdditiveGaussianNoise(p=p,),
>
>             A.GaussNoise(p=p,),
>
>             A.CoarseDropout (p=p,max_height=40,max_width=40),
>
>             A.Downscale(p=p,),
>
>             A.MultiplicativeNoise(p=p,multiplier=(1.1,8)),
>
>             A.IAASuperpixels(p=p),
>
>         ],p=p),
>
>         ],p=0.5)
>
>     inner_gen=keras_ocr.data_generation.get_image_generator(**args)
>
>     for image,lines in inner_gen:
>
>         new_image=pipe(image=image)['image']
>
>         new_lines=lines
>
>         yield new_image, new_lines
>
>
>
> *1) I am randomly getting several kinds of errors when running this
> pipeline:*
>
>
>
>
> ~/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/imgaug/dtypes.py
> in gate_dtypes(dtypes, allowed, disallowed, augmenter)
>
>
> 330 augmenter.name,
>
>
> 331 augmenter.*class*.*name*,
>
>
> --> 332 "", "".join(disallowed)
>
>
> 333 ))
>
>
> 334 else:
>
>
> ValueError: Got dtype 'float32' in augmenter 'UnnamedSuperpixels' (class
> 'Superpixels'), which is a forbidden dtype (uint128, uint256, int128,
> int256, float16, float32, float64, float96, float128, float256).
>
>
>
>
> and
>
>
>
>
> File
> ""/home/ubuntu/anaconda3/envs/tensorflow2_latest_p37/lib/python3.7/site-packages/albumentations/augmentations/functional.py"",
> line 759, in median_blur
>
>
> ""Invalid ksize value {}. For a float32 image the only valid ksize values
> are 3 and 5"".format(ksize)
>
>
> ValueError: Invalid ksize value 7. For a float32 image the only valid
> ksize values are 3 and 5
>
>
>
>
> Is this a know behaviour of IAASuperpixelsand MedianBlur transforms in a
> pipeline? How can I assure they always get input of uint8 format, if it's
> obviously previous transforms in my pipeline which return float32 ('cause
> keras-ocr returns image = (alpha * text_image[..., :3] + (1 - alpha) *
> current_background).astype('uint8'))? Also, as I'm feeding outputs to a
> neural network, is there a way to guarantee a pipeline to always return
> rescaled images?
>
>
> *2) Speed consideration:*
>
>
> Abovementioned pipeline has average generation speed of 3.81 im/sec on
> 800x600 size images, compared with 4.26 im/s native performance of the
> keras-ocr generator. CPU is 100% busy. Is ~0.5 sec/image albumentations
> overhead normal to expect?
>
>
>
>
>
>    1. I can't use spatial transformations, because albumentations seems
>    to only support 2-points, 4 coordinates notation for bounding boxes, and
>    they are expected to be rectangular and strictly parallel to the x,y axes.
>    Which is not the case with
>
>
>    keras-ocr <https://freeimage.host/i/2XEyjR>, which uses 4 points, 8
>    coordinates to store its bounding boxes. Are there any plans to support
>    such extended format of bounding boxes in albumentations? Do you have any
>    advice for my use case?
>
>
>
>
> *My system info:*
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
> *Albumentations: 0.4.6Tensorflow: 2.3.0System Platform: linuxSystem
> Platform: linuxSystem Version: 3.7.6 (default, Jan 8 2020, 19:59:22)[GCC
> 7.3.0]Machine: x86_64Platform:
> Linux-5.3.0-1035-aws-x86_64-with-debian-buster-sidPocessor: x86_64System
> OS: LinuxRelease: 5.3.0-1035-awsVersion: #37
> <https://github.com/albumentations-team/albumentations/pull/37>-Ubuntu SMP
> Sun Sep 6 01:17:09 UTC 2020Number of CPUs: 8Number of Physical CPUs: 4*
>
>
>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/717>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YDOWJXOR54LLEE33YLSJFLE5ANCNFSM4SEHF4WA>
> .
>
>
>
",usually augmentation image sake speed stay within style could time faster float usually want last operation compare speed always measure could show augmentation pipeline different kind good yet however open new trying extend wrapping native image generator one import weather color noise image yield randomly getting several running pipeline augmenter class name else got augmenter class forbidden float float float float float float file line invalid value float image valid invalid value float image valid know behaviour pipeline assure always get input format obviously previous pipeline return float image alpha alpha also feeding neural network way guarantee pipeline always return speed consideration pipeline average generation speed size native performance generator busy overhead normal expect ca use spatial support notation bounding rectangular strictly parallel ax case store bounding support extended format bounding advice use case system system platform platform version default machine o sun number number physical thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
702960138,"Similar to #713, if I pull the augmentation function out and perform augmentation directly on it, I get the expected results.  It seems we both have issues when using it within the data generator (or batch loader as he calls it).",similar pull augmentation function perform augmentation directly get within data generator batch loader,issue,negative,positive,neutral,neutral,positive,positive
702007657,Thanks a lot for your answer ! What do you mean by check pipeline ?,thanks lot answer mean check pipeline,issue,negative,negative,neutral,neutral,negative,negative
702006292,"Thank you for your answers.

I tried to visualize the results launching my ```augment_data()``` function directly on the images and it worked correctly.
So it seems that the problem is coming from my ```batchloader```.

Sorry for the disturbance.
All is ok.",thank tried visualize function directly worked correctly problem coming sorry disturbance,issue,negative,negative,negative,negative,negative,negative
701973251,"You can provide images in both formats: `(height, width)` and `(height, width, 1)`.
Most of transforms will work fine with grayscale images. But for obvious reasons some transforms doesn't work like `RGBShift`.
For mask same rules as for images.

With current API you can provide multiple mask using keyword `masks` like this: `transforms(image=image, masks=[mask1, mask2])`. You also can provide multiple images, but you need describe `additional_targets` for this puprose. More info about `additional_targets` here: https://albumentations.ai/docs/examples/example_multi_target/

For images the following ranges:
- `uint8` - `[0, 255]`
- `float32` - `[0, 1]`

For masks you can use full diapason of values, but is customary to use range `[0, 1]`

But I advise you to check pipeline before using it to avoid problems.",provide height width height width work fine obvious work like mask current provide multiple mask like mask mask also provide multiple need describe following float use full diapason customary use range advise check pipeline avoid,issue,positive,positive,positive,positive,positive,positive
701964028,"I can not reproduce problem with example image.
Can you show problem on the image? Also would be great if you could provide an example with fixed random seeds to reproduce problem.
![image](https://user-images.githubusercontent.com/7512250/94783529-7de4dc00-03d5-11eb-9e11-e5ffcfd5ae2b.png)
![image](https://user-images.githubusercontent.com/7512250/94783539-82a99000-03d5-11eb-8e24-78a563d013d1.png)

",reproduce problem example image show problem image also would great could provide example fixed random reproduce problem image image,issue,negative,positive,positive,positive,positive,positive
701952686,Can you please attach original image and mask?,please attach original image mask,issue,positive,positive,positive,positive,positive,positive
701289904,"I'm having the opposite problem: albumentations installs the non-headless version even when I don't specify it. Minimal example:

```
$ virtualenv venv
$ . venv/bin/activate
$ pip install albumentations
Processing /home/gsk/.cache/pip/wheels/38/db/df/d6cb0be184075a7799c1fd79240c389c16f51dfe18dc3332fa/albumentations-0.4.6-py3-none-any.whl
Collecting opencv-python>=4.1.1
  Using cached opencv_python-4.4.0.44-cp36-cp36m-manylinux2014_x86_64.whl (49.5 MB)
Collecting imgaug>=0.4.0
  Using cached imgaug-0.4.0-py2.py3-none-any.whl (948 kB)
Collecting scipy
  Using cached scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9 MB)
Processing /home/gsk/.cache/pip/wheels/e5/9d/ad/2ee53cf262cba1ffd8afe1487eef788ea3f260b7e6232a80fc/PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl
Collecting numpy>=1.11.1
  Using cached numpy-1.19.2-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)
Collecting imageio
  Using cached imageio-2.9.0-py3-none-any.whl (3.3 MB)
Collecting Pillow
  Using cached Pillow-7.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)
Collecting Shapely
  Using cached Shapely-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)
Collecting six
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting scikit-image>=0.14.2
  Using cached scikit_image-0.17.2-cp36-cp36m-manylinux1_x86_64.whl (12.4 MB)
Collecting matplotlib
  Using cached matplotlib-3.3.2-cp36-cp36m-manylinux1_x86_64.whl (11.6 MB)
Collecting tifffile>=2019.7.26
  Using cached tifffile-2020.9.3-py3-none-any.whl (148 kB)
Collecting PyWavelets>=1.1.1
  Using cached PyWavelets-1.1.1-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)
Collecting networkx>=2.0
  Using cached networkx-2.5-py3-none-any.whl (1.6 MB)
Collecting kiwisolver>=1.0.1
  Using cached kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88 kB)
Collecting cycler>=0.10
  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)
Collecting python-dateutil>=2.1
  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)
Collecting certifi>=2020.06.20
  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3
  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Collecting decorator>=4.3.0
  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)
Installing collected packages: numpy, opencv-python, Pillow, imageio, Shapely, six, scipy, tifffile, kiwisolver, cycler, python-dateutil, certifi, pyparsing, matplotlib, PyWavelets, decorator, networkx, scikit-image, imgaug, PyYAML, albumentations
Successfully installed Pillow-7.2.0 PyWavelets-1.1.1 PyYAML-5.3.1 Shapely-1.7.1 albumentations-0.4.6 certifi-2020.6.20 cycler-0.10.0 decorator-4.4.2 imageio-2.9.0 imgaug-0.4.0 kiwisolver-1.2.0 matplotlib-3.3.2 networkx-2.5 numpy-1.19.2 opencv-python-4.4.0.44 pyparsing-2.4.7 python-dateutil-2.8.1 scikit-image-0.17.2 scipy-1.5.2 six-1.15.0 tifffile-2020.9.3

$ pip freeze | grep opencv
opencv-python==4.4.0.44
```

EDIT: apparently it's related to caching; `pip install albumentations --no-cache-dir` downloads the headless version. The strange thing is that the headless version is also cached, so it doesn't explain why the non-headless version is selected. Can the same logic be applied regardless to whether a dependency is already cached or not?",opposite problem version even specify minimal example pip install pillow shapely six cycler decorator collected pillow shapely six cycler decorator successfully pip freeze edit apparently related pip install headless version strange thing headless version also explain version selected logic applied regardless whether dependency already,issue,negative,positive,positive,positive,positive,positive
699871759,"Greetings. Maintainer here. Can you please elaborate more on what you're trying to achieve?
Albumentations supports boxes & keypoints augmentations as first-class citizens - https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_bboxes.ipynb and you can also trace what augmentation has been applied using `A.ReplayCompose` mechanism. Demo: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/replay.ipynb
Hope it helps.",maintainer please elaborate trying achieve also trace augmentation applied mechanism hope,issue,positive,positive,positive,positive,positive,positive
699484757,"@ternaus, I'd like to take up this enhancement task.",like take enhancement task,issue,negative,neutral,neutral,neutral,neutral,neutral
697746870,"I am closing this PR, because it currently works different for uint8 and float32 data types",currently work different float data,issue,negative,neutral,neutral,neutral,neutral,neutral
694978573,"ok i think that's just one difference. I fixed some bugs in my code and now the differences between this implementation and torchvision version less than 2 for `uint8` for `brightness`, `saturation` and `contrast`",think one difference fixed code implementation version le brightness saturation contrast,issue,negative,positive,neutral,neutral,positive,positive
694942339,"At first in OpencCV hue in range `[0, 180]` for `HSV` format. But for Pillow this range is`[0, 255]`
```python
import random
import numpy as np
import cv2 as cv
from PIL import Image

seed = 0
random.seed(seed)
np.random.seed(0)

image = np.random.randint(0, 256, [3, 1, 3], dtype=np.uint8)

res1 = cv.cvtColor(image, cv.COLOR_RGB2HSV)
res2 = np.array(Image.fromarray(image).convert('HSV'))

print(""OpenCV:"")
print(res1)
print()
print(""Pillow"")
print(res2)

# OpenCV:
# [[[158 240 172]]
# 
#  [[143 185 170]]
# 
#  [[ 13 103 196]]]
# 
# Pillow
# [[[224 240 172]]
# 
#  [[202 184 170]]
# 
#  [[ 18 102 196]]]
```",first hue range format pillow range python import random import import import image seed seed image image image print print print print pillow print pillow,issue,negative,negative,negative,negative,negative,negative
693601651,"I need some tests with `XYZ`, `YUV` and `YCrCb` colorspaces. Looks like they are work different in `uint8` and `float32` formats.",need like work different float,issue,negative,neutral,neutral,neutral,neutral,neutral
693537209,"Yeah, that make sense. Moved them to domain_adaptation.py. Not sure whether we want to merge them into single class.",yeah make sense sure whether want merge single class,issue,positive,positive,positive,positive,positive,positive
693471668,"Maybe we can put this transform and [FDA](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.FDA) in one file and name them, I don't know, maybe `ImageMixTransforms`?",maybe put transform one file name know maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
693466399,I think this PR can solve one part of this issue. #641,think solve one part issue,issue,negative,neutral,neutral,neutral,neutral,neutral
693306264,"> I can not reproduce problem.
> Please try to determine random seed when the error occures

It's really strange. I ran the code in Pycharm with your loop to debug the code. And it works fine. 
But in Jupyter Notebook problem still exists. It looks like this is a problem with Jupyter.
Thank you for support!

",reproduce problem please try determine random seed error really strange ran code loop code work fine notebook problem still like problem thank support,issue,negative,negative,neutral,neutral,negative,negative
693270279,"I can not reproduce problem.
Please try to determine random seed when the error occures
```python
init_seed = 0
for i in range(10):
    seed = i + init_seed
    random.seed(seed)
    np.random.seed(seed)

    transform = A.Compose([
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        # A.InvertImg(p=0.5),
        # A.ShiftScaleRotate(shift_limit=0, scale_limit=0.1, rotate_limit=15, p=0.5),
        A.ToFloat(max_value=255.)
    ], keypoint_params=A.KeypointParams(format='xy'))

    path2png_imgs = os.getcwd()
    df = pd.read_csv('/home/druzhinin/HDD/NewDownloads/vgg_annotate_crop.csv', header=0)

    data = CustomSeq(path2png_imgs, df, 1, augmentations=transform)

    images, point = data.__getitem__(0)
    images = [cv2.cvtColor(i, cv2.COLOR_GRAY2RGB) for i in images]

    images[0] = cv2.circle(images[0], list(map(tuple, point.astype(np.int).tolist()))[0], 30, (1, 0, 0), -1)
    plt.figure(figsize=(10, 10))
    plt.imshow(images[0], cmap='gray')
    plt.show()
```",reproduce problem please try determine random seed error python range seed seed seed transform data point list map,issue,negative,negative,negative,negative,negative,negative
692932561,Another problem is that current transform is not fully equal to torchvisions `ColorJitter`,another problem current transform fully equal,issue,negative,neutral,neutral,neutral,neutral,neutral
692931507,In the past we especially removed Pillow from dependencies #370,past especially removed pillow,issue,negative,negative,negative,negative,negative,negative
692638647,"The most probable cause of this is wrong image type. Albumentations works only with numpy arrays.
Based on the context of your stacktrace, it's maybe not the case. I'm not familiar with mmdetection, but I assume it may be sending PIL image directly to our pipeline. ",probable cause wrong image type work based context maybe case familiar assume may sending image directly pipeline,issue,negative,negative,neutral,neutral,negative,negative
692270957,Would changing H and S using HLS colorspace with this (composed with `RandomBrightnessContrast`) correspond to the perturbation `ColorJitter` does? A 1:1 comparison with that augmentation would be very useful for people hoping to migrate from torchvision. ,would composed correspond perturbation comparison augmentation would useful people migrate,issue,negative,positive,positive,positive,positive,positive
692252156,"> Please, make it public to download the samples.

Sorry. I fix it. ",please make public sorry fix,issue,negative,negative,negative,negative,negative,negative
692223898,Thanks @ternaus for reminding me that we need a test to compare this transforms. `ColorJitter` works with image in different way.,thanks need test compare work image different way,issue,negative,positive,neutral,neutral,positive,positive
692176159,If it helps to resolve https://github.com/albumentations-team/albumentations/issues/698 we need a test that shows that for a set of the parameters CollorJitter and RandomColorShift match.,resolve need test set match,issue,negative,neutral,neutral,neutral,neutral,neutral
691976012,Also would be great to write tests for new functionality.,also would great write new functionality,issue,positive,positive,positive,positive,positive,positive
691397718,Thanks for the advice but I request the developers to add this feature so that we can directly apply mapping to the batched dataset.,thanks advice request add feature directly apply,issue,negative,positive,positive,positive,positive,positive
690982737,Hi. We are still thinking about how to implement this functionality using library interface.,hi still thinking implement functionality library interface,issue,negative,neutral,neutral,neutral,neutral,neutral
690978051,"Was fixed in #696 , try to install latest version from github.",fixed try install latest version,issue,negative,positive,positive,positive,positive,positive
689923286,"Any updates for this?
This library seems to augment a set of keypoints and bounding boxes separately! But having many tasks both keypoints and bounding boxes have to be augmented together for any single object, etc. [RetinaFace](https://arxiv.org/pdf/1905.00641.pdf), [MTCNN](https://arxiv.org/pdf/1604.02878.pdf)
The below image shows a sample of landmark and bounding box have to go along with each face:
![sample](https://user-images.githubusercontent.com/35292523/92672315-9f0c5c80-f342-11ea-8059-20ac882d5f60.jpg)
",library augment set bounding separately many bounding augmented together single object image sample landmark bounding box go along face sample,issue,negative,positive,positive,positive,positive,positive
689900950,thank you very much for your help. You save the day 👍 ,thank much help save day,issue,positive,positive,positive,positive,positive,positive
688963060,"I agree. If this can be solved it would have to be some pip magic. 
It seems to me like the `pkg_resources.get_distribution` that is used only finds packages that are already installed. 
One would have to have a `pip.get_packages_currently_being_resolved`, which probably does not exist.",agree would pip magic like used already one would probably exist,issue,positive,positive,positive,positive,positive,positive
688939445,"At this time I do not know how to resolve the problem, because I think that it is a pip issue.
Pip installs packages asynchronously, so when we check dependencies library does not see that package installed.
Maybe @creafz has ideas",time know resolve problem think pip issue pip check library see package maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
688933842,"It seems like my pip and setuptools are up to date:
```
(venv)$ pip install --upgrade pip
Requirement already up-to-date: pip in ./venv/lib/python3.7/site-packages (20.2.3)
(venv)$ pip install --upgrade setuptools
Requirement already up-to-date: setuptools in ./venv/lib/python3.7/site-packages (50.3.0)
```
I have to add that in this test I do not have to wait for a compilation of opencv-python-headless. Either because it is cached or because that is a peculiarity of our test agent's system. But this bug report is not about that but about the fact that the logic in [choose_requirment](https://github.com/albumentations-team/albumentations/blob/master/setup.py#L27) only works if opencv has been installed before but not when it is installed at the same time as albumentations. 

The fix would have to be not testing what is already installed, but somehow finding out what is being installed. ",like pip date pip install upgrade pip requirement already pip pip install upgrade requirement already add test wait compilation either peculiarity test agent system bug report fact logic work time fix would testing already somehow finding,issue,negative,neutral,neutral,neutral,neutral,neutral
688928988,"I just tried again:
```
$ virtualenv venv
Using base prefix '/pstore/apps/Python/3.7.1-2018.12'
New python executable in /home/tom/ONED/tmp/venv/bin/python
Installing setuptools, pip, wheel...
done.
$ source venv/bin/activate
(venv)$ cat requirements.txt 
opencv-python
albumentations
(venv)$ pip install -r requirements.txt 
Collecting opencv-python
  Using cached opencv_python-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (49.4 MB)
Collecting albumentations
  Downloading albumentations-0.4.6.tar.gz (117 kB)
     |████████████████████████████████| 117 kB 1.8 MB/s 
Collecting numpy>=1.14.5
  Using cached numpy-1.19.1-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)
Collecting scipy
  Downloading scipy-1.5.2-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)
     |████████████████████████████████| 25.9 MB 12.3 MB/s 
Collecting imgaug>=0.4.0
  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)
     |████████████████████████████████| 948 kB 15.6 MB/s 
Processing 
cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd/PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl
Collecting opencv-python-headless>=4.1.1
  Downloading opencv_python_headless-4.4.0.42-cp37-cp37m-manylinux2014_x86_64.whl (36.6 MB)
     |████████████████████████████████| 36.6 MB 20.1 MB/s 
Collecting scikit-image>=0.14.2
  Using cached scikit_image-0.17.2-cp37-cp37m-manylinux1_x86_64.whl (12.5 MB)
Collecting matplotlib
  Downloading matplotlib-3.3.1-cp37-cp37m-manylinux1_x86_64.whl (11.6 MB)
     |████████████████████████████████| 11.6 MB 26.6 MB/s 
Collecting six
  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)
Collecting Shapely
  Downloading Shapely-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (1.0 MB)
     |████████████████████████████████| 1.0 MB 27.7 MB/s 
Collecting imageio
  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)
     |████████████████████████████████| 3.3 MB 23.5 MB/s 
Collecting Pillow
  Downloading Pillow-7.2.0-cp37-cp37m-manylinux1_x86_64.whl (2.2 MB)
     |████████████████████████████████| 2.2 MB 28.8 MB/s 
Collecting networkx>=2.0
  Downloading networkx-2.5-py3-none-any.whl (1.6 MB)
     |████████████████████████████████| 1.6 MB 30.6 MB/s 
Collecting tifffile>=2019.7.26
  Downloading tifffile-2020.9.3-py3-none-any.whl (148 kB)
     |████████████████████████████████| 148 kB 33.2 MB/s 
Collecting PyWavelets>=1.1.1
  Using cached PyWavelets-1.1.1-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)
Collecting python-dateutil>=2.1
  Using cached python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB)
Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3
  Using cached pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)
Collecting kiwisolver>=1.0.1
  Using cached kiwisolver-1.2.0-cp37-cp37m-manylinux1_x86_64.whl (88 kB)
Collecting certifi>=2020.06.20
  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)
Collecting cycler>=0.10
  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)
Collecting decorator>=4.3.0
  Using cached decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)
Building wheels for collected packages: albumentations
  Building wheel for albumentations (setup.py) ... done
  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65167 sha256=e335bd6ff657382d0a4036f9fe6fd7b28e45a19754143a7b83e464032b793cf0
  Stored in directory: 
cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc
Successfully built albumentations
Installing collected packages: numpy, opencv-python, scipy, six, python-dateutil, pyparsing, kiwisolver, certifi, Pillow, cycler, matplotlib, imageio, decorator, networkx, tifffile, PyWavelets, scikit-image, Shapely, imgaug, PyYAML, opencv-python-headless, albumentations
Successfully installed Pillow-7.2.0 PyWavelets-1.1.1 PyYAML-5.3.1 Shapely-1.7.1 albumentations-0.4.6 certifi-2020.6.20 cycler-0.10.0 decorator-4.4.2 imageio-2.9.0 imgaug-0.4.0 kiwisolver-1.2.0 matplotlib-3.3.1 networkx-2.5 numpy-1.19.1 opencv-python-4.4.0.42 opencv-python-headless-4.4.0.42 pyparsing-2.4.7 python-dateutil-2.8.1 scikit-image-0.17.2 scipy-1.5.2 six-1.15.0 tifffile-2020.9.3
```

I end up with `opencv-python-4.4.0.42` _and_ `opencv-python-headless-4.4.0.42`. 
I should not, but it makes total sense. At the time of executing [choose_requirments](https://github.com/albumentations-team/albumentations/blob/master/setup.py#L27) opencv-python is not yet installed and therefore albumentations' setup.py requires the installation of opencv-python-headless even though opencv-python is being installed alongside. ",tried base prefix new python executable pip wheel done source cat pip install six shapely pillow cycler decorator building collected building wheel done wheel directory successfully built collected six pillow cycler decorator shapely successfully end total sense time yet therefore installation even though alongside,issue,positive,positive,positive,positive,positive,positive
688808482,"I can not reproduce this problem. For me all works fine.
Maybe you forget upgrade pip or setuptools?",reproduce problem work fine maybe forget upgrade pip,issue,negative,positive,positive,positive,positive,positive
688775454,"@creafz your suggestion can be a welcome addition, but it is out of scope for fixing this bug (and yes, yolo is float and normalised).

I think it's low on the priority list, but I do find myself having to query the data types of the albu inputs and restoring them afterwards - reason: albumentations transforms are performed inside a `tf.numpy_function`, which requires strict output types.
Alternatively, just use type annotations and specify that the output bboxes of albumentations will always be float64, that's fair as well",suggestion welcome addition scope fixing bug yes float think low priority list find query data afterwards reason inside strict output alternatively use type specify output always float fair well,issue,positive,positive,positive,positive,positive,positive
688746760,"You also can try to change `HueSaturationValue` by set flag `brightness_by_max=False`.
Converting image from Pillow to albumentations reduce the speed, but I do not know how much.

Try to remove `GaussianBlur` and `ColorJitter`, if results will be close, try after that to add `CollorJitter`.
If problem not in `ColorJitter`, I think problem may be only in `GaussianBlur`
",also try change set flag converting image pillow reduce speed know much try remove close try add problem think problem may,issue,negative,positive,positive,positive,positive,positive
688621642,"I tried changing `hue_shift_limit` to `int(0.8*180)` instead and unfortunately it did not really improve. Inspired by this though, I removed color jittering from both the torchvision and albumentations versions and got similar performance, which suggests the difference probably is in there somehow. Can you think of anything else that could be different about the way I've tried to do color jittering with Albumentations?

If I understand about `ToFloat`, it's not needed if you set `max_pixel_value` of `A.Normalize` to `255` instead of `1` like I did - I just found this more readable since the `torchvision` `Normalize` assumes max of 1, since `ToTensor` divides. 

In case it's relevant: instead of loading with opencv, I'm using the default torchvision loading with Pillow and am applying the Albumentations transformations as a drop-in replacement (after casting to `np.array`), could that cause discrepancies? I'm also seeing a slowdown of about 2x when using Albumentations instead of torchvision that I wonder could be related to data loading.  ",tried instead unfortunately really improve inspired though removed color got similar performance difference probably somehow think anything else could different way tried color understand set instead like found readable since normalize since case relevant instead loading default loading pillow replacement casting could cause also seeing slowdown instead wonder could related data loading,issue,positive,positive,positive,positive,positive,positive
688389530,"Also, please, add transform to serialization tests",also please add transform serialization,issue,negative,neutral,neutral,neutral,neutral,neutral
688303172,If you need to use `albumentations.pytorch` you need it import directly `import albumentations.pytorch`.,need use need import directly import,issue,negative,positive,neutral,neutral,positive,positive
688301735,"Maybe difference in `GaussianBlur`, but I am not sure, because Pillow compute parameters for blur different to OpenCV.
Also what a difference in result accuracy of training? Maybe this differences not great and may be also based on random seeds.",maybe difference sure pillow compute blur different also difference result accuracy training maybe great may also based random,issue,positive,positive,positive,positive,positive,positive
688290620,"Also you do not need to call `ToFloat`, because you always do the same thing when you call `A.Normalize`. But it is not a problem there.",also need call always thing call problem,issue,negative,neutral,neutral,neutral,neutral,neutral
688289979,"Maybe you need  little fix for `HueSaturationValue`, hue in range `[0, 180]` for `uint8` data type, so we need change this values `hue_shift_limit=int(0.8 * 180)`
And hue in range `[0, 360]` for `float32` data type. So hue limit changes to `hue_shift_limit=int(0.8 * 360)`",maybe need little fix hue range data type need change hue range float data type hue limit,issue,negative,negative,negative,negative,negative,negative
688288533,"A little fix, `hue` in range `[0, 180]` for `uint8` data type, so we need change this values `hue_shift_limit=int(0.8 * 180)`
And hue in range `[0, 360]` for `float32` data type. So hue limit changes to `hue_shift_limit=int(0.8 * 360)`",little fix hue range data type need change hue range float data type hue limit,issue,negative,negative,negative,negative,negative,negative
688282610,"Thank you for information. You must be careful with float dtype, because in float hue is measured in degrees in range `[0, 360]`",thank information must careful float float hue measured range,issue,negative,negative,neutral,neutral,negative,negative
688271627,"@creafz You a little bit confused me :)
Yolo format is normalized float format, so after transforms we always get floats.
As you can see after conversion to int I call `normalize_bbox` function that returns floats.",little bit confused format float format always get see conversion call function,issue,negative,negative,negative,negative,negative,negative
688250703,"Great work!
Can you add simple example how to use this transform and reference to article into docstring?
For many peoples this API is not clear.
Something like this:
```python
class FDA(ImageOnlyTransform):
    """"""
    Fourier Domain Adaptation from https://github.com/YanchaoYang/FDA
    Source paper ""FDA: Fourier Domain Adaptation for Semantic Segmentation""
    Important: you need to pass target image as a parameter `target_image` in __call__

    Args:
        beta_limit (float or tuple of float): coefficient beta from paper. Recommended less 0.3.

    Targets:
        image

    Image types:
        3-channel uint8 images only

    Reference:
        https://github.com/YanchaoYang/FDA
        https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_FDA_Fourier_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2020_paper.pdf

    Example:
        >>> import numpy as np
        >>> import albumentations as A
        >>> image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
        >>> target_image = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
        >>> aug = A.Compose([A.FDA(p=1)])
        >>> result = aug(image=image, target_image=target_image)

    """"""
```",great work add simple example use transform reference article many clear something like python class domain adaptation source paper domain adaptation semantic segmentation important need pas target image parameter float float coefficient beta paper le image image reference example import import image result,issue,positive,positive,positive,positive,positive,positive
688244862,"@thingumajig solution resolves the problem.
Also you can rewrite your function that calls albumentations pipeline like this:
```python
def aug_fn(image, img_size):
    data = {""image"":image}
    if image.ndim == 4:
        aug_data = np.stack([transforms(image=i) for i in image], axis=0)
    else:
        aug_data = transforms(**data)
    aug_img = aug_data[""image""]
    aug_img = tf.cast(aug_img/255.0, tf.float32)
    aug_img = tf.image.resize(aug_img, size=[img_size, img_size])
    
    return aug_img
```",thingumajig solution problem also rewrite function pipeline like python image data image image image else data image return,issue,negative,neutral,neutral,neutral,neutral,neutral
688240463,"Problem with invalid image shape.
Our library currently can not work with batches of images.
In your example you provide batch of images with shape `(32, 120, 120, 3)`  into albumentation pipeline.",problem invalid image shape library currently work example provide batch shape pipeline,issue,negative,neutral,neutral,neutral,neutral,neutral
688233243,"In my case, I'd like to perform a same random resized crop on 2 images of different size..
I can rewrite my pipeline with current API, but I think the current design could be improved if the cropping parameters also consider the additional targets.",case like perform random crop different size rewrite pipeline current think current design could also consider additional,issue,negative,negative,negative,negative,negative,negative
688231542,"The main idea for additional targets is different.
Current idea, that all additional images must be the same shape.
If you need to apply same transformation on different targets with different shapes in the main it would be wrong.
I think you was confused because scale parameter is shape independent.
To change current behavior we need to make a lot of changes to our main code. And I don't think that they are needed.

I think, that we can to rewrite you pipeline with our current API, if you describe what you want.",main idea additional different current idea additional must shape need apply transformation different different main would wrong think confused scale parameter shape independent change current behavior need make lot main code think think rewrite pipeline current describe want,issue,negative,negative,neutral,neutral,negative,negative
688225019,"@Dipet 
I understand they should use equal crop parameters. I mean that while determining the params, it should also consider the shape of the additional images (ex: when they are much smaller)",understand use equal crop mean also consider shape additional ex much smaller,issue,negative,negative,negative,negative,negative,negative
688222762,"I do not understand what you mean.
Additional targets needs if you want to create crop of few images with equal crop parameters.
If we simplify transform logic it will be looks like this:
```python
import numpy as np

images = [np.random.randint(0, 256, [100, 100, 3]) for i in range(10)]
x_min, x_max = 10, 20
y_min, y_max = 55, 73

result = [i[y_min:y_max, x_min:x_max] for i in images]
```",understand mean additional need want create crop equal crop simplify transform logic like python import range result,issue,positive,negative,negative,negative,negative,negative
687911958,"> Rearranging the pipeline works but it does not vectorize the `.map()` calls . When the `.map()` calls are expensive we want to vectorize them by applying `batch` before `map`. In this case it might be okay to apply `batch` after `map`. But applying `batch` before `map` should work.

If you want applying `batch` before `map` your `process_data` should handle a batch of images!",pipeline work expensive want batch map case might apply batch map batch map work want batch map handle batch,issue,negative,negative,negative,negative,negative,negative
685514463,Rearranging the pipeline works but it does not vectorize the ```.map()``` calls . When the ```.map()``` calls are expensive we want to vectorize them by applying ```batch``` before ```map```. In this case it might be okay to apply ```batch``` after ```map```. But applying ```batch``` before ```map``` should work.,pipeline work expensive want batch map case might apply batch map batch map work,issue,negative,negative,negative,negative,negative,negative
685391531,"For all other bounding boxes formats, Albumentations returns augmented coordinates as floats. May we should also convert int coordinates to float before returning them to preserve the same output data type?",bounding augmented may also convert float preserve output data type,issue,negative,neutral,neutral,neutral,neutral,neutral
684882346,"Thank you @Dipet , that was really fast. I wanted to ask if I could be of any help after a long weekend but I see it's already implemented and approved. Awesome work!
I'll keep an eye on the release notes for the next albu versions, but let me know if you need any testing from my side",thank really fast ask could help long weekend see already awesome work keep eye release next let know need testing side,issue,positive,positive,positive,positive,positive,positive
683917756,"Try masks=... and not mask=

пн, 31 авг. 2020 г. в 5:22 PM, Khoa Nguyen-Tuan <notifications@github.com>:

>
>
>
> 🐛 Bug
>
>
>
>
>
> I can't input several masks to target following this tutorial One image
> and several masks
> <https://medium.com/pytorch/multi-target-in-albumentations-16a777e9006e>
> or this tutorial
> <https://albumentations.ai/docs/getting_started/mask_augmentation/>.
>
>
>
> To Reproduce
>
>
> Steps to reproduce the behavior:
>
>
>
> import numpy as np
>
>
> import albumentations as A
>
>
>
>
>
> transform = A.Compose([
>
>
>     A.RandomCrop(width=5, height=5),
>
>
>     A.HorizontalFlip(p=0.5),
>
>
>     A.RandomBrightnessContrast(p=0.2),
>
>
> ])
>
>
>
>
>
> image = np.arange(100, dtype=""uint8"").reshape([10,10])
>
>
> target = np.arange(100, dtype=""uint8"").reshape([10,10])
>
>
>
>
>
> masks = [target, target, target]
>
>
> transformed = transform(image=image, mask=masks)
>
>
>
>
>
>
>
> *Error messages*
>
>
>
> AttributeError: 'list' object has no attribute 'shape'
>
>
>
>
>
> Expected behavior
>
>
>
>
>
> Environment
>
>
>
>
>
>
>    - Albumentations version (e.g., 0.1.8): 0.4.6
>
>
>    - Python version (e.g., 3.7): 3.7
>
>
>    - OS (e.g., Linux): Linux
>
>
>    - How you installed albumentations (conda, pip, source): *pip*
>
>
>    - Any other relevant information:
>
>
>
>
>
>
> Additional context
>
>
>
>
>
>
>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/691>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YG6EKISBZ2R3OXLV5DSDOW2ZANCNFSM4QQSLPKA>
> .
>
>
>
>
",try bug bug ca input several target following tutorial one image several tutorial reproduce reproduce behavior import import transform image target target target target transform error object attribute behavior environment version python version o pip source pip relevant information additional context thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
683506704,"I think you should rearrange the pipeline:
![image](https://user-images.githubusercontent.com/8703531/91674827-e1fc5200-eb42-11ea-9331-ef04cd50c8ec.png)
",think rearrange pipeline image,issue,negative,neutral,neutral,neutral,neutral,neutral
683143626,"Problem with floating point arithmetic.
Example:
```
HEIGHT = 720
WIDTH = 1280

pascal_bbox = [1122, 17, 1280, 720]
albu_bbox = (0.8765625, 0.02361111111111111, 1.0, 1.0)
yolo = (0.9375, 0.5104166666666666, 0.1234375, 0.9763888888888889)

reverse_pascal = (1122.0, 17.5, 1280.0, 720.5) # <- 0.5 error is here. Because height 703 does not divisible by 2,
```

I think cliping using conversion to int for x_min and x_max will fix this error. 
Also I will check rounding
",problem floating point arithmetic example height width error height divisible think conversion fix error also check rounding,issue,negative,neutral,neutral,neutral,neutral,neutral
682716092,"This unfortunately is regularly happening for me as well. Here's my env along with a reproducible script.

It seems to me that all of these errors seem to deal with numerical errors when doing geometric operations on totally legit bounding boxes right at the image boundary. 
One way to deal with the issue would be to allow an ""unsafe"" transformation by letting the user control the `check_validity` flag from `BBoxParams` and deal with truncations manually afterwards: that's still a better option than having a training job fail because of uncontrollable exceptions.

CC: @BeckerFelix

### Environment
- Albumentations version: 0.4.6, 0.3.3
- Python version: 3.6.12
- OS (e.g., Linux): ubuntu 20.04
- How you installed albumentations (conda, pip, source): pip

```python
import numpy as np
import albumentations as A

np.random.seed(123)
HEIGHT, WIDTH = 720, 1280

def random_bbox():
    x1 = np.random.randint(low=0, high=WIDTH)
    y1 = np.random.randint(low=0, high=HEIGHT)
    x2 = np.random.randint(low=x1 + 1, high=WIDTH + 1)
    y2 = np.random.randint(low=y1 + 1, high=HEIGHT + 1)
    bbox_albu = A.convert_bbox_to_albumentations([x1, y1, x2, y2], source_format='pascal_voc', rows=HEIGHT, cols=WIDTH)
    bbox_yolo = A.convert_bbox_from_albumentations(bbox_albu, target_format='yolo', rows=HEIGHT, cols=WIDTH, check_validity=True)
    # NOTE: at this point the bounding box has been checked to be valid.

    return bbox_yolo


transform = A.Compose(
    [A.HorizontalFlip(), A.RandomBrightnessContrast()],
    bbox_params=A.BboxParams(format='yolo', label_fields=[""class_labels""])
)
img = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)

for i in range(1000):
    bboxes = [random_bbox()]
    try:
        transform(image=img, bboxes=bboxes, class_labels=[1])
    except:
        print(f""[{i}] Invalid transformation of box: {str(bboxes[0])}"")

>>> [327] Invalid transformation of box: (0.755859375, 0.5944444444444444, 0.48671875, 0.3611111111111111)
>>> [363] Invalid transformation of box: (0.373046875, 0.9409722222222222, 0.68828125, 0.11527777777777778)
>>> [683] Invalid transformation of box: (0.465625, 0.9881944444444445, 0.5765625, 0.020833333333333332)

```",unfortunately regularly happening well along reproducible script seem deal numerical geometric totally legit bounding right image boundary one way deal issue would allow unsafe transformation user control flag deal manually afterwards still better option training job fail uncontrollable environment version python version o pip source pip python import import height width note point bounding box checked valid return transform height width range try transform except print invalid transformation box invalid transformation box invalid transformation box invalid transformation box,issue,negative,positive,neutral,neutral,positive,positive
679290458,"""Better to throw warning, that bbox is not clipped, and give the user a chance to fix the problem."" - It is just what I am proposing! By default exception is rised, but if user needs it, he can fix it by setting parameter check_validity to false.

Here is quite a real usecase that forced me to make this PR.
I am working on human detection. A have annotated images, where human figures are large and hence often are cut by image borders. Such cut figures are annotated as humans as well.
When I train a network on cropped images, I need to ensure that bboxes annotated as humans are cropped not too much. It is just what `min_visibility` is designed for, right? Let's say, human must be visible at least at 50%. But if an initial box is already a part of human figure, cropped bbox with right visibility can represent inappropriate portion of a human figure (say, only hat). If I will use it for training it will spoil results.
I can estimate size of normal human figure at each place of image. So if annotation bbox is at the image border, I can evaluate the real bbox (including its part outside the image). And if I pass it to Albumentations, `min_visibility` filter will be applied correctly.
This real case is just one of possible similar situations. I've already proposed another one above: I can deal with only some part of image but annotation can be on on the whole image...
",better throw warning clipped give user chance fix problem default exception user need fix setting parameter false quite real forced make working human detection human large hence often cut image cut well train network need ensure much designed right let say human must visible least initial box already part human figure right visibility represent inappropriate portion human figure say hat use training spoil estimate size normal human figure place image annotation image border evaluate real part outside image pas filter applied correctly real case one possible similar already another one deal part image annotation whole image,issue,negative,positive,neutral,neutral,positive,positive
679164479,"It is very strange annotation, where objects annotated outside field of view, because we do not know how objects looks outside field of view. For example some object may be occluded, but we do not know it, because we do not see it.

Please give me annotation example where it will be justified.

In my opinion - PR approach may lead to errors. Better to throw warning, that bbox is not clipped, and give the user a chance to fix the problem.",strange annotation outside field view know outside field view example object may know see please give annotation example opinion approach may lead better throw warning clipped give user chance fix problem,issue,negative,positive,positive,positive,positive,positive
679124366,"@Dipet, motivation is provided in #617.
Bound boxes resulting from augmentation are always clipped so that they fit inside resulting image. I don't propose to change it. The problem fixed here is that augmentations require initial boxes to be inside initial image. Let's try to describe motivation from #617 in some other words: 
If you have image with boxes and for some reason you crop it before call to albumentations, you have to manually clip all boxes as well. The least problem is that it needs extra coding. But serious problem is that applying `min_area` and `min_visibility` criteria to such clipped boxes is erroneous because they do not represent whole objects.
If you need non-clipped rects as *output* (i.e. above result (-35.0, -45.0, 25.0, 125.0)), possibly it is usefull, it should be managed by quite another parameter (like it is done for keypoints, as far as I remembe) and represents another PR. As for me, I didn't need it until now. Remark: output boxes outside output image can result even from boxes inside an input image.",motivation provided bound resulting augmentation always clipped fit inside resulting image propose change problem fixed require initial inside initial image let try describe motivation image reason crop call manually clip well least problem need extra serious problem criterion clipped erroneous represent whole need output result possibly quite another parameter like done far another need remark output outside output image result even inside input image,issue,negative,negative,neutral,neutral,negative,negative
679009742,"Currently realization gives invalid results. We can see it by the test.
If we do not check validity, also we do not need check `min_area` and `min_visibility`.
In the test, we provide `(-10, -20, 50, 150, 0)` and get `(0, 0, 25, 50, 0)`, but if we do not check validity we must get non clipped bbox: `(-35.0, -45.0, 25.0, 125.0)`.
Otherwise I do not understand why we need this changes.",currently realization invalid see test check validity also need check test provide get check validity must get non clipped otherwise understand need,issue,negative,neutral,neutral,neutral,neutral,neutral
678727314,Is there anything blocking this PR from getting merged? This would be a very useful addition. Thanks! ,anything blocking getting would useful addition thanks,issue,negative,positive,positive,positive,positive,positive
678727193,"This would be awesome to have merged! Not only the current error message is inaccurate, but this problem makes albumentation a very rigid library hard to use in non-trivial projects. For example, my use case is that I have an experiment management library that instantiates augmentation pipelines from config files. The code building those pipelines doesn't know how the models are actually going to use them in advance. ",would awesome current error message inaccurate problem rigid library hard use example use case experiment management library augmentation code building know actually going use advance,issue,negative,positive,positive,positive,positive,positive
676152089,"If you need same result set fixed `clip_limit` by default it is in range `[1, 4]`
For example:
```python
A.CLAHE(clip_limit=(4, 4), always_apply=True, p=1.0)]
```",need result set fixed default range example python,issue,negative,positive,neutral,neutral,positive,positive
675882424,"@Dipet Current builds on ReadTheDocs raise the `ModuleNotFoundError: No module named 'skbuild'` exception. https://readthedocs.org/projects/albumentations/builds/11667932/

According to [opencv-python's FAQ](https://github.com/skvark/opencv-python#frequently-asked-questions), the correct way to fix this error is to use pip >= 19.3. However, according to the output of the ReadTheDocs' build pipeline, it already uses the latest version of pip (20.2.2). Since there is little control over the build pipeline in a default configuration, it is challenging to debug the issue further. So I switched to `.readthedocs.yml` file for configuring a build pipeline.",current raise module exception according correct way fix error use pip however according output build pipeline already latest version pip since little control build pipeline default configuration issue switched file build pipeline,issue,negative,positive,positive,positive,positive,positive
674134485,"Hi, any news on when these two transforms will be added to albumentations?",hi news two added,issue,negative,neutral,neutral,neutral,neutral,neutral
673540880,"@BloodAxe thanks for getting back to me! Yes, we are preparing at the moment a separate small repository with `Histogram Matching` and `Feature Distribution Matching` only, so that you can easily guide us through the integration. Our code will come with unit tests (`pytest`) and will satisfy `mypy` / `flake8` / `pylint` checks. I will ping you as soon as we are ready. I think we still need about 1-1.5 months.",thanks getting back yes moment separate small repository histogram matching feature distribution matching easily guide u integration code come unit satisfy flake ping soon ready think still need,issue,positive,positive,positive,positive,positive,positive
673490880,"python code for clahe on  gray scale imges?
pls share
",python code gray scale share,issue,negative,neutral,neutral,neutral,neutral,neutral
673014878,@aabramovrepo any updates on this? We are looking forward to see the draft and ready to help you with the integration of your PR.,looking forward see draft ready help integration,issue,positive,positive,positive,positive,positive,positive
671820262,"You can use different approach: resize with keep ratio and than pad to square
```python
Compose([
    LongestMaxSize(512),
    PadIfNeeded(512, 512)
])
```",use different approach resize keep ratio pad square python compose,issue,negative,neutral,neutral,neutral,neutral,neutral
671648298,"> 
> 
> Could you please provide links to the description of Mosaic Augmentation?

I think it was even mentioned in YOLOv4 paper. Here are some references:
https://www.kaggle.com/nvnnghia/awesome-augmentation
https://github.com/klauspa/Yolov4-tensorflow",could please provide link description mosaic augmentation think even paper,issue,negative,neutral,neutral,neutral,neutral,neutral
671642853,@ternaus Thanks for the reply. I understand the PadIfNeeded requires min_height and min_width parameters to be defined. But what if the input sizes are different for each image in the list of images and would like to pad them to square before preforming RESIZE to desired input size.,thanks reply understand defined input size different image list would like pad square resize desired input size,issue,positive,positive,neutral,neutral,positive,positive
671641346,"[PadIfNeeded](https://albumentations.ai/docs/api_reference/augmentations/transforms/#albumentations.augmentations.transforms.PadIfNeeded) will pad to the desired square. Associated key points will be transformed as well.
",pad desired square associated key well,issue,negative,neutral,neutral,neutral,neutral,neutral
671636212,Could you please provide links to the description of Mosaic Augmentation?,could please provide link description mosaic augmentation,issue,negative,neutral,neutral,neutral,neutral,neutral
670608245,"Sorry for the late reply.
Your test has invalid behavior.
When you call `dataset[idx][1] == dataset[idx][0]` you has 2 different calls of aumgentation, so on augmentation has 2 different random seed on call.
Try this:
```python
dataset = Dataset(transform)
for idx in range(100):
    res = dataset[idx]
    assert np.alltrue(res[1] == res[0])
```",sorry late reply test invalid behavior call different augmentation different random seed call try python transform range assert,issue,negative,negative,negative,negative,negative,negative
670590643,"> I also found this problem, how do you solve it?

Sadly, I didn't. I wrote my own augmentation functions instead.",also found problem solve sadly wrote augmentation instead,issue,negative,negative,negative,negative,negative,negative
670435561,"Hi ! Sorry for the delay, 

I've added the interpolation argument as a tuple, tell me what you think about it.

Cheers, ",hi sorry delay added interpolation argument tell think,issue,negative,negative,negative,negative,negative,negative
668448568,"Hi @Dipet , I see your point. There are two reasons hope you would kindly consider. The first is if randomness is restricted to a limit, there will be no false positive (it is similar as other augments methods). The second is it will help attracting more users from other tools, like me.  It will be easier for user to choose use a method or not than implementing one himself.",hi see point two hope would kindly consider first randomness restricted limit false positive similar second help like easier user choose use method one,issue,positive,positive,positive,positive,positive,positive
668424934,"If you need padding use `albumentations.PadIfNeeded` combined with resize transform: `A.Compose([A.RandomScale(), A.PadIfNeeded()])`.
For random scaling we have `albumentations.RansomScale`.
I do not think that randomly changing aspect ratio is good idea. I think this approach will give many false positive results in real world.",need padding use combined resize transform random scaling think randomly aspect ratio good idea think approach give many false positive real world,issue,positive,positive,neutral,neutral,positive,positive
668340680,"Hi @Dipet , thank you for your kind reply. I strongly recommend adding the feature that can randomly scale the image with random ratio of height and width with PADDING. Random pyramid scaling is so common in computer vision.",hi thank kind reply strongly recommend feature randomly scale image random ratio height width padding random pyramid scaling common computer vision,issue,positive,negative,negative,negative,negative,negative
667892445,"Hello.
Library does not have transform, that can randomly resize without keeping the ratio.
You can use `RandomScale`, but transform will keep ratio.
Also you can write your own transform, like this:
```python
import albumentations as A
import numpy as np
import cv2 as cv


class RandomResize(A.Resize):
    def __init__(self, height_limit=(100, 500), width_limit=(100, 500), interpolation=cv.INTER_LINEAR, always_apply=False, p=1):
        super(RandomResize, self).__init__(0, 0, interpolation, always_apply, p=p)
        self.height_limit = height_limit
        self.width_limit = width_limit

    def get_params(self):
        # This function will be called before each transformation.

        # Set result height and width
        self.height = np.random.randint(*self.height_limit)
        self.width = np.random.randint(*self.width_limit)

        return {}

    def get_transform_init_args_names(self):
        # Need for serialization and correct string representation
        return (""height_limit"", ""width_limit"", ""interpolation"")


image = np.zeros([1000, 1000, 3], dtype=np.uint8)
transform = A.Compose([RandomResize()])

for i in range(10):
    print(transform(image=image)['image'].shape)
```",hello library transform randomly resize without keeping ratio use transform keep ratio also write transform like python import import import class self super self interpolation self function transformation set result height width return self need serialization correct string representation return interpolation image transform range print transform,issue,positive,negative,neutral,neutral,negative,negative
667044197,"Yes, you can set fixed kernel size like `blur_limit=[23, 23]`, it is docs issue.",yes set fixed kernel size like issue,issue,positive,positive,neutral,neutral,positive,positive
667040791,"@Dipet Thank you for your fast response!
I have read the updated code and I'm wondering whether we can use a **fixed kernel size** while **random sigma** with the new code? For example, in SimCLR, they use a fixed kernel size of 23 with sigma uniformly sampled from [0.1, 2.0].

Maybe we can change `blur_limit` from `(int)` to `(int, (int, int))`?",thank fast response read code wondering whether use fixed kernel size random sigma new code example use fixed kernel size sigma uniformly maybe change,issue,negative,positive,neutral,neutral,positive,positive
667033768,"This PR similar to #536, but int #536 if sigma limits is set, then ksize will be computed from sigma.
We can do the same thing if we will set `blur_limit=(0, 0)` and `sigma != (0, value)`",similar sigma set sigma thing set sigma value,issue,negative,neutral,neutral,neutral,neutral,neutral
667003820,"Unfortunately at this time you can not set range for sigma value. In current implementation sigma value computes from kernel size.
For example:
- `ksize=3, sigma=0.8`
- `ksize=5, sigma=1.1`
- `ksize=7, sigma=1.4`",unfortunately time set range sigma value current implementation sigma value kernel size example,issue,negative,negative,negative,negative,negative,negative
667000295,"In `HueSaturationValue` arguments is absolute value, so if your image has `float` data type you do not need change anything, because values in in float images in range `[0, 1]`. If image has `uint8` data type you need multiply torchvision factor value to 255.
For example this transforms get similar results:
```python
torchvision.transforms.ColorJitter(brightness=0, contrast=0, saturation=0.6, hue=0.8)
A.HueSaturationValue(hue_shift_limit=int(0.8 * 255), sat_shift_limit=int(0.6 * 255), val_shift_limit=0)
```",absolute value image float data type need change anything float range image data type need multiply factor value example get similar python,issue,positive,positive,neutral,neutral,positive,positive
665552236,"> Feel free to check https://github.com/kornia/kornia

Does it allows to transform annotations at the same time ?",feel free check transform time,issue,positive,positive,positive,positive,positive,positive
665112354,"Yep. But I expect the behavior of the box being cropped to happen at the side of the image which is fine.

In the middle of the image, it looks strange indeed.",yep expect behavior box happen side image fine middle image strange indeed,issue,negative,positive,positive,positive,positive,positive
665086463,"> When part of the box is cropped, key points that got cut should be clipped to the side of the image.

Are you sure that this behavior is correct?
For me this behavior is strange on example below.

![image](https://user-images.githubusercontent.com/7512250/88681928-c1cc1400-d0fa-11ea-9766-1df2ba4ce5ac.png)
",part box key got cut clipped side image sure behavior correct behavior strange example image,issue,negative,positive,positive,positive,positive,positive
664481553,"Hi.

We've updated the release page for 0.4.6. Now it contains information about improvements and bug fixes in this release - https://github.com/albumentations-team/albumentations/releases/tag/0.4.6",hi release page information bug release,issue,negative,neutral,neutral,neutral,neutral,neutral
664422245,"@Dipet  Yes you are correct maybe it's a bug within the `tf.keras.models.Sequential API` & `run_eagerly` does significantly make the `training` slower. 
Here's everything that I've tried so far: 
https://github.com/benihime91/tensorflow-keras-nbs/blob/master/tensorflow_example_updated.ipynb
Let me know for any changes. Thanks.

I think we should also add an example wherein we can load the data using `tf.keras.utils.Sequence` class. Maybe using `tf.keras.utils.Sequence` class won't throw errors (don't know). Worth checking out ?",yes correct maybe bug within significantly make training everything tried far let know thanks think also add example wherein load data class maybe class wo throw know worth,issue,positive,positive,positive,positive,positive,positive
664419018,"@benihime91 Thanks a lot for looking into it, very interesting your findings. As for your suspicion regarding the issue being related to `tf_ds.load`, I tried it as well with another dataset pipeline where I loaded images directly from .jpeg files using `tf.io.read_file`, which resulted in the same error, so it seems that isn't the issue - or at least not the only one.

`run_eagerly=True` is a good workaround for now, but I assume that this will make training significantly slower.
Very strange that the problem only arises when using the Sequential API... to me it seems that this is a bug of TensorFlow, so maybe we should open an issue in the TensorFlow repo.

",thanks lot looking interesting suspicion regarding issue related tried well another pipeline loaded directly error issue least one good assume make training significantly strange problem sequential bug maybe open issue,issue,negative,positive,positive,positive,positive,positive
664414210,@Dipet I have found 2 instances where this `data pipeline` doesn't work. Should i create a PR making note of these instances?,found data pipeline work create making note,issue,negative,neutral,neutral,neutral,neutral,neutral
664403539,"I think one problem could be this is :
while doing the `tf_ds.load` function as you can see when you load the inbuilt `dataset in tensorflow datasets using tf_ds.load()` it is automatically loading the data as a wrapper under the `tf.compat.v1.data.Dataset`. This is due to the outdated `tensorflow_datasets` version in `colab`
```
# load in the tf_flowers dataset
data, info= tfds.load(name=""tf_flowers"", split=""train"", as_supervised=True, with_info=True)
data

<DatasetV1Adapter shapes: ((None, None, 3), ()), types: (tf.uint8, tf.int64)>

tfds.__version__
2.1.0
```
if you lookup the definition of `DatasetV1Adapter` ---> Wraps a V2 `Dataset` object in the `tf.compat.v1.data.Dataset` API.

The latest version is `3.2.1` .

Also i tried every possible way to use a `tf.keras.Model` . These are my findings: 
- I found the error only whilst using the `Sequential API` that too if you `have your own layers as the first layers` using a `pretrained-model` as the `feature-extractor` in the `Seqential API` works fine. 
- If you `compile` the `model` using `run_eagerly=True` everything works fine. Any API `works`.
- Also while using the `functional API` I didn't find any problems all whatever the case.
- I didn't try with the `Subclassing API` (because I've never used it 😊)

Maybe need to make a note in the `tutorial` that using the `Sequential API` will `sometimes not work ..`

",think one problem could function see load inbuilt automatically loading data wrapper due outdated version load data train data none none definition object latest version also tried every possible way use found error whilst sequential first work fine compile model everything work fine work also functional find whatever case try never used maybe need make note tutorial sequential sometimes work,issue,negative,positive,positive,positive,positive,positive
664372389,"Even `Functional API` works fine :

```
from tensorflow import keras
input = keras.Input(shape=(120, 120, 3))
x = keras.layers.Conv2D(32, (3, 3), activation=""relu"")(input)
x = keras.layers.MaxPooling2D((2, 2))(x)
x = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)
x = keras.layers.MaxPooling2D((2, 2))(x)
x = keras.layers.Conv2D(64, (3, 3), activation='relu')(x)
x = keras.layers.Flatten()(x)
x = keras.layers.Dense(64, activation='relu')(x)
x = keras.layers.Dense(5, activation='softmax')(x)

model = keras.Model(input, x)
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(ds_alb, epochs=1)

115/115 [==============================] - 28s 245ms/step - loss: 1.4399 - accuracy: 0.3564
<tensorflow.python.keras.callbacks.History at 0x7f90f08d4780>
```",even functional work fine import input input model input loss accuracy,issue,negative,positive,positive,positive,positive,positive
664363593,"I just tried your notebook the model's training . How exactly did u get the error?

EDIT: I got the error. It's showing not showing error when u compile the model in `eagerly` but shows the error only when you compile the model `non-eargerly`. That to only in this particular model. `Transfer learning `seems to work fine. I personally never found this `error` let me check ..",tried notebook model training exactly get error edit got error showing showing error compile model eagerly error compile model particular model transfer learning work fine personally never found error let check,issue,negative,positive,positive,positive,positive,positive
664358160,"> Please open access.

Just changed the access permissinos. 

The model should be fine, as training works with the pure tensorflow tf.data pipeline example in the same notebook. Maybe only occurs with certain layers such as convolutional layers, which might be the reason why your model works",please open access access model fine training work pure pipeline example notebook maybe certain convolutional might reason model work,issue,positive,positive,positive,positive,positive,positive
664282895,"I tried `fit` on my own model. It worked fine . Maybe something wrong with your `model architecture`.?
Well, here's my `notebook` you can check it out: 
(https://github.com/benihime91/tensorflow-keras-nbs/blob/master/Copy_of_tensorflow_example.ipynb)",tried fit model worked fine maybe something wrong model architecture well notebook check,issue,negative,positive,positive,positive,positive,positive
664150565,"I also found this problem, how do you solve it?",also found problem solve,issue,negative,neutral,neutral,neutral,neutral,neutral
662886142,"look forward albu will support this method, it will boost the result especially in Scenes with Occlude",look forward support method boost result especially occlude,issue,positive,neutral,neutral,neutral,neutral,neutral
662885568,"

Yes, i find it, and i just rewrite a method with numpy to achieve",yes find rewrite method achieve,issue,negative,neutral,neutral,neutral,neutral,neutral
662872545,"Hi!
At this time CoarseDropout does not support targets. It is works only on pixel level.
All transforms, that supports targets, you can see here: https://github.com/albumentations-team/albumentations#spatial-level-transforms",hi time support work level see,issue,negative,neutral,neutral,neutral,neutral,neutral
662408199,"Default mean and std values it is Imagenet mean and std for BGR images.
As I understand you are using grayscale image, so on set your mean and std. For example `mean=0, std=1`",default mean mean understand image set mean example,issue,negative,negative,negative,negative,negative,negative
660131767,"For me, the previous behavior looks like a bug, so maybe we should enable `check_each_transform` by default by setting its default value to `True`?",previous behavior like bug maybe enable default setting default value true,issue,positive,positive,neutral,neutral,positive,positive
659331132,Python code case sensitive. `A.Compose` not `A.compose`,python code case sensitive,issue,negative,positive,neutral,neutral,positive,positive
659325476,"Hi, were you able to solve? I am getting a very similar error. Thank you.

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-44-cc5622792842> in <module>
      2     return tuple(zip(*batch))
      3 
----> 4 train_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())
      5 valid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())
      6 

<ipython-input-39-6bc9f86d57ba> in get_train_transform()
      2 
      3 def get_train_transform():
----> 4     return A.compose([
      5         A.Flip(0.5),
      6         ToTensorV2(p=0.1)

AttributeError: module 'albumentations' has no attribute 'compose'",hi able solve getting similar error thank recent call last module return zip batch return module attribute,issue,negative,positive,positive,positive,positive,positive
658607054,"In the current version, you can do this, as in the example below.
I think the current solution is enough.

```python
import albumentations as A
import numpy as np


image = np.zeros([100, 100, 3], dtype=np.uint8)
keypoints = [[0, 0, 1], [50, 50, 2], [90, 90, 3]]
aug = A.Compose([A.Crop(10, 10, 80, 80)], keypoint_params=A.KeypointParams('xy', remove_invisible=False))

res = aug(image=image, keypoints=keypoints)
res_keys = res['keypoints']
res_img = res['image']
print(res_keys)
print([(i[0] < 0 or i[1] < 0 or i[0] >= res_img.shape[1] or i[1] >= res_img.shape[1]) for i in res_keys])
```",current version example think current solution enough python import import image print print,issue,negative,neutral,neutral,neutral,neutral,neutral
658557229,"Perhaps, the easiest way would be to disable removal of invisible key
points via KeypointParams. After data augmentation step you can quickly
compute visibility mask by checking whether points within image boundary.


ср, 15 июля 2020 г. в 3:51 AM, xjh19971 <notifications@github.com>:

> Thank you for this excellent data augmentation repo! I was wondering if
> there is one way to return all keypoints but mark those invisible
> keypoints. This is because the current method can remove invisible
> keypoints but only return visible keypoints. I want my returned keypoint
> array to have same length. Could you tell me how to do that in current
> version or implement this feature in next version? Thanks!
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/661>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YAXKYURC7XSGK4GBKLR3T4SDANCNFSM4O2BAN3A>
> .
>
",perhaps easiest way would disable removal invisible key via data augmentation step quickly compute visibility mask whether within image boundary thank excellent data augmentation wondering one way return mark invisible current method remove invisible return visible want returned array length could tell current version implement feature next version thanks thread reply directly view,issue,positive,positive,positive,positive,positive,positive
656410568,"@dipet, thanks for your reply! 

It does indeed work if I first concatenate various masks into a single one (which might not be a good idea for masks with different dtypes). Nevertheless, this would also require to split them later on.

If I don't want to concatenate, I have a setup like follows:

```python
import albumentations as A
import numpy as np
import cv2

image = np.zeros([100, 100, 3], np.uint8)
mask_segm = np.zeros([100, 100], np.uint8)
mask_depth = np.zeros([100, 100], np.float32)
targets = OrderedDict({'image': 'image', 'segm': 'mask', 'depth': 'mask'})
sample = OrderedDict({'image': image, 'segm': mask_segm, 'depth': mask_depth})

pad_size = (200, 200)
pad_value = [255, 255, 255]
mask_value = [255, 0.5]

default_pad = A.Compose(
    [A.PadIfNeeded(*pad_size, cv2.BORDER_CONSTANT, pad_value, mask_value=mask_value)],
    additional_targets=targets)
default_output = default_pad(**sample)
assert np.all(default_output[""depth""][default_output['segm'] == 255] == 255)
```

I couldn't think of a way of adapting your code from the above to this setup (apart from concatenation), so I had to slightly re-write the PadIfNeeded class:

```python
import albumentations.augmentations.functional as F
from albumentations.core.transforms_interface import DualTransform

from itertools import cycle

class MultiMaskPadIfNeeded(A.PadIfNeeded):
    def __init__(
        self,
        min_height=1024,
        min_width=1024,
        border_mode=cv2.BORDER_REFLECT_101,
        value=None,
        mask_value=None,
        always_apply=False,
        p=1.0,
    ):
        super(MultiMaskPadIfNeeded, self).__init__(min_height, min_width, border_mode, value, mask_value, always_apply, p)
        self.mask_value = cycle(mask_value)
    
    def apply_to_mask(self, img, pad_top=0, pad_bottom=0, pad_left=0, pad_right=0, **params):
        return F.pad_with_params(
            img, pad_top, pad_bottom, pad_left, pad_right, border_mode=self.border_mode, value=next(self.mask_value)
        )
```

Now this works for different masks, too:

```python
multimask_pad = A.Compose(
    [MultiMaskPadIfNeeded(*pad_size, cv2.BORDER_CONSTANT, pad_value, mask_value=mask_value)],
    additional_targets=targets)
multi_output = multimask_pad(**sample)
assert np.all(multi_output[""depth""][multi_output['segm'] == 255] == 0.5)
```

I guess the above could have been re-written even more succinctly if passing mask-specific parameters (such as `mask_value` ) into `apply_to_mask` was allowed.",thanks reply indeed work first concatenate various single one might good idea different nevertheless would also require split later want concatenate setup like python import import import image sample image sample assert depth could think way code setup apart concatenation slightly class python import import import cycle class self super self value cycle self return work different python sample assert depth guess could even succinctly passing,issue,positive,positive,positive,positive,positive,positive
656224168,`a way of using different pad values for different masks`,way different pad different,issue,negative,neutral,neutral,neutral,neutral,neutral
656223126,About what? That we can describe pad values for each image channel?,describe pad image channel,issue,negative,neutral,neutral,neutral,neutral,neutral
656219532,"This is a non-trivial example.

@Dipet Could you please add a page to https://albumentations.ai/docs/",example could please add page,issue,negative,neutral,neutral,neutral,neutral,neutral
655993429,"If your mask has number of channels less, then 4 you can use default `PadIfNeeded`
```python
aug = A.Compose([A.PadIfNeeded(256, 256, cv2.BORDER_CONSTANT,  value=100, mask_value=[1, 2, 3, 4])])
```

Also you can use `Lambda` to do this thing.

```python
import numpy as np
import albumentations as A
import cv2


def wrap_pad(h, w, pad_value):
    aug = A.PadIfNeeded(h, w, cv2.BORDER_CONSTANT, pad_value)

    def func(image, **params):
        return aug(image=image)['image']

    return func


def wrap_pad_mask(h, w, pad_values):
    augs = [A.PadIfNeeded(h, w, cv2.BORDER_CONSTANT, mask_value=i) for i in pad_values]

    def func(mask, **params):
        result_mask = []

        for i in range(mask.shape[-1]):
            aug = augs[i]
            params = aug.update_params(params, image=mask)
            result_mask.append(aug.apply_to_mask(mask[..., i], **params))

        result_mask = np.stack(result_mask, axis=-1)

        return result_mask

    return func


img = np.zeros([100, 100, 3], np.uint8)
mask = np.zeros([100, 100, 10], np.uint8)

h, w = 256, 256

aug = A.Compose([A.Lambda(image=wrap_pad(h, w, 100), mask=wrap_pad_mask(h, w, [i for i in range(mask.shape[-1])]))])

res = aug(image=img, mask=mask)
img = res['image']
mask = res['mask']

print(img.max(), [mask[..., i].max() for i in range(mask.shape[-1])])
```",mask number le use default python also use lambda thing python import import import image return return mask range mask return return mask range mask print mask range,issue,negative,neutral,neutral,neutral,neutral,neutral
653845465,"I have already found the problem, My codes exist a bug.",already found problem exist bug,issue,negative,neutral,neutral,neutral,neutral,neutral
653026726,"Hi
Thanks for PR!
You catch this error with outputs from albumentations transforms?
This errors occurs when image is not contiguous, better to call `np.ascontiguousarray`",hi thanks catch error image contiguous better call,issue,negative,positive,positive,positive,positive,positive
651142950,"Greetings! It would be great if you can contribute by generating a PR to https://github.com/albumentations-team/albumentations_examples with example of using Albumentations in Tensorflow for some toy example (CIFAR/MNIST).
Also, if you prefer writing docs, consider making a new page under https://github.com/albumentations-team/albumentations/tree/master/docs.
Any help is greatly encouraged, so feel free to make a PR if it's not complete, we will do our best to help you with the process.",would great contribute generating example toy example also prefer writing consider making new page help greatly feel free make complete best help process,issue,positive,positive,positive,positive,positive,positive
651141175,"Hi. I could not reproduce this issues locally and on CI. Therefore it's hard to provide any help at this point.
Also, it seems your environment has two versions of pytorch installed:
```
torch                     1.5.1                    pypi_0    pypi
torchvision               0.5.0                py37_cu101    pytorch
pytorch                   1.4.0           cuda101py37h02f0884_0    defaults
```

My suggestion is to go with fresh environment and one pytorch version to see whether the issue is still happens.
But my bet - it will go away once you tidy up packages in your working environment.",hi could reproduce locally therefore hard provide help point also environment two torch suggestion go fresh environment one version see whether issue still bet go away tidy working environment,issue,positive,positive,positive,positive,positive,positive
650724931,"> Try this command to upgrade library:
> `!pip install -U git+https://github.com/albu/albumentations --no-cache-dir`

Thank you!",try command upgrade library pip install thank,issue,negative,neutral,neutral,neutral,neutral,neutral
644382310,"Do not worry, the latest flake8 has detected few problems in our code. This problem will be fixed after merge #638",worry latest flake code problem fixed merge,issue,negative,positive,positive,positive,positive,positive
644271750,"> https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst

I read the doc, and follow the steps. However, I just modify a comment line, so why can't I pass the static code check, what am i doing wrong?",read doc follow however modify comment line ca pas static code check wrong,issue,negative,neutral,neutral,neutral,neutral,neutral
643746210,"@BloodAxe thanks for your response! Actually we are planning to make the code with all experiments open source over this summer. However, only `Histogram Matching` and `Feature Distribution Matching` parts are relevant for `Albumentations`. I think I could prepare a pull request with both operations for the library (taking already implemented operations as an example) and then we can polish it, would this be fine?",thanks response actually make code open source summer however histogram matching feature distribution matching relevant think could prepare pull request library taking already example polish would fine,issue,positive,positive,positive,positive,positive,positive
643613535,"Looks like all works fine
```python
import requests
import matplotlib.pyplot as plt
import albumentations as A
import cv2
import numpy as np

response = requests.get(""https://raw.githubusercontent.com/albumentations-team/albumentations_examples/master/notebooks/images/parrot.jpg"")
img = cv2.imdecode(np.frombuffer(response.content, dtype=np.uint8), cv2.IMREAD_COLOR)
mask = np.zeros(img.shape[:2], dtype=np.float32)
mask[200:1000, 200:700] = 1

transforms = A.Compose([
    A.HorizontalFlip(p=1),
    A.ShiftScaleRotate(
        shift_limit=0,  # no resizing
        scale_limit=0.1,
        rotate_limit=10, # rotate
        p=1,
        border_mode=cv2.BORDER_CONSTANT
    ),
    A.RandomRotate90(p=1),
    A.Cutout(p=1),
    A.RandomBrightnessContrast(
        brightness_limit=0.2, contrast_limit=0.2, p=1
    ),
    A.GridDistortion(p=1),
    A.HueSaturationValue(p=1),
    A.RandomContrast(p=1),
    A.MedianBlur(p=1),
    A.Normalize(),
])

res = transforms(image=img, mask=mask)


plt.subplot(221)
plt.imshow(img)
plt.subplot(222)
plt.imshow(mask, cmap=""gray"")

plt.subplot(223)
plt.imshow(res[""image""])
plt.subplot(224)
plt.imshow(res[""mask""], cmap=""gray"")


plt.show()
```

![image](https://user-images.githubusercontent.com/7512250/84568188-616d5700-ad86-11ea-9995-4d017f9766b9.png)
",like work fine python import import import import import response mask mask rotate mask gray image mask gray image,issue,positive,positive,positive,positive,positive,positive
643603079,"Also, it seems like using flipping doesnt work on my mask properly
![download (1)](https://user-images.githubusercontent.com/8830319/84566288-8ff92b00-ada2-11ea-8c5c-6e5fba688794.png)
![download](https://user-images.githubusercontent.com/8830319/84566290-9091c180-ada2-11ea-933b-67356161c638.png)
It looks wrongly flipped",also like doesnt work mask properly wrongly,issue,negative,negative,negative,negative,negative,negative
643602816,"DataLoader

```
from PIL import Image
import torchvision

image_prefix = ""/content/segmentation_dataset/new_img/""
color_mask_prefix = ""/content/segmentation_dataset/new_color_mask/""
binary_mask_prefix = ""/content/segmentation_dataset/mask/""


class DPWHDataset(Dataset):
  def __init__(self, mean=None, std=None, phase=None, dataset=None):
    self.data = dataset
    self.mean = mean
    self.std = std
    self.phase = phase
    self.transforms = get_transforms(phase, mean, std)
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    image_name = self.data[idx]

    image_path = image_prefix + image_name + "".jpg""
    mask_path = binary_mask_prefix + image_name + ""_mask.png""
    # print(mask_path)

    mask = cv2.imread(mask_path, 0)

    # image = np.array(Image.open(image_path))
    # mask = np.array(Image.open(mask_path))
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    mask = create_channel_mask(mask)

    augmented = self.transforms(image=image, mask=mask)
    image = augmented['image']
    mask = augmented['mask']

    image = torchvision.transforms.ToTensor()(image)
    image = torchvision.transforms.Normalize(mean=self.mean, std=self.std)(image)
    mask = torchvision.transforms.ToTensor()(mask).permute(1, 2, 0)
    return image, mask
```

Creator of n_channel_binary_mask
```
def create_channel_mask(mask):
  mask_palette = [1, 2, 3, 4, 5]
  final_mask = []
  for palette in mask_palette:
    new_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.float32)
    new_mask[mask == palette] = 1
    final_mask.append(new_mask)

  final_mask = np.asarray(final_mask)
  return final_mask
```

Albumentations
```
import albumentations as albu

def get_transforms(phase, mean, std):
    list_transforms = []
    if phase == ""train"":
        list_transforms.extend(
            [
                # HorizontalFlip(p=0.5),
                # ShiftScaleRotate(
                #     shift_limit=0,  # no resizing
                #     scale_limit=0.1,
                #     rotate_limit=10, # rotate
                #     p=0.8,
                #     border_mode=cv2.BORDER_CONSTANT
                # ),
                # # albu.RandomRotate90(),
                # albu.Cutout(),
                albu.RandomBrightnessContrast(
                    brightness_limit=0.2, contrast_limit=0.2, p=0.2
                ),
                # # albu.GridDistortion(p=0.3),
                albu.HueSaturationValue(p=0.2),
                albu.RandomContrast(p=0.2),
                albu.MedianBlur(p=0.2)
            ]
        )
    # list_transforms.extend(
    #     [
    #         Resize(600, 900, interpolation=cv2.INTER_NEAREST)
    #         # Normalize(mean=mean, std=std, p=1),
    #         # ToTensor(),
    #     ]
    # )

    list_trfms = Compose(list_transforms)
    return list_trfms
```

Sample Mask image
![alligator10_color_mask](https://user-images.githubusercontent.com/8830319/84566236-34c73880-ada2-11ea-82bb-60162f0a0309.png)",import image import class self mean phase phase mean self return self print mask image mask image image image mask mask augmented image augmented mask augmented image image image image mask mask return image mask creator mask palette mask palette return import phase mean phase train rotate resize normalize compose return sample mask image,issue,negative,negative,negative,negative,negative,negative
643600910,"This error indicates that your image has 0 height or width.
```
error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/imgwarp.cpp:2594: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'
```

We need sample code with image and mask on which you receive these errors.",error image height width error error assertion function need sample code image mask receive,issue,negative,neutral,neutral,neutral,neutral,neutral
643599505,not really an empty image. how do i give reproducible example?,really empty image give reproducible example,issue,negative,negative,neutral,neutral,negative,negative
643597426,Can you give reproducible example? Looks like mask or image is empty image.,give reproducible example like mask image empty image,issue,negative,negative,neutral,neutral,negative,negative
643589847,"Now I am having this error when I use ShiftScale Rotate

Error
```
error                                     Traceback (most recent call last)
<ipython-input-164-6ec0d9d0ef8e> in <module>()
      1 fake_loader = DataLoader(train_dataset, batch_size=1,
      2                           pin_memory=True, shuffle=True)
----> 3 batch = next(iter(fake_loader)) # get a batch from the dataloader
      4 images, masks = batch

10 frames
/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py in shift_scale_rotate(img, angle, scale, dx, dy, interpolation, border_mode)
    143     matrix[0, 2] += dx * width
    144     matrix[1, 2] += dy * height
--> 145     img = cv2.warpAffine(img, matrix, (width, height), flags=interpolation, borderMode=border_mode)
    146     return img
    147 

error: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/imgwarp.cpp:2594: error: (-215:Assertion failed) src.cols > 0 && src.rows > 0 in function 'warpAffine'
```


My DataLoader/Dataset

```
class DPWHDataset(Dataset):
  def __init__(self, mean=None, std=None, phase=None, dataset=None):
    self.data = dataset
    self.mean = mean
    self.std = std
    self.phase = phase
    self.transforms = get_transforms(phase, mean, std)
  
  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    image_name = self.data[idx]

    image_path = image_prefix + image_name + "".jpg""
    mask_path = binary_mask_prefix + image_name + ""_mask.png""
    print(mask_path)

    mask = cv2.imread(mask_path, 0)

    # image = np.array(Image.open(image_path))
    # mask = np.array(Image.open(mask_path))
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    mask = create_channel_mask(mask)
    augmented = self.transforms(image=image, mask=mask)
    # image = augmented['image']
    # mask = augmented['mask']

    image = torchvision.transforms.ToTensor()(image)
    image = torchvision.transforms.Normalize(mean=self.mean, std=self.std)(image)
    mask = torchvision.transforms.ToTensor()(mask).permute(1, 2, 0)
    return image, mask
```
```",error use rotate error error recent call last module batch next iter get batch batch angle scale interpolation matrix width matrix height matrix width height return error error assertion function class self mean phase phase mean self return self print mask image mask image image image mask mask augmented image augmented mask augmented image image image image mask mask return image mask,issue,negative,negative,negative,negative,negative,negative
641849301,"Greetings Alexey! Thanks for your proposal. It's indeed looks interesting and this technique could be considered as augmentation. Do you have an open-source implementation of your work, so that we can take a look and see what's needed to make albumentations-compatible wrapper and measure processing time?  ",thanks proposal indeed interesting technique could considered augmentation implementation work take look see make wrapper measure time,issue,positive,positive,positive,positive,positive,positive
640426656,"If you need serialization, try to use `lambda_transforms` argument in `albumentations.load`.
Also you can create your custom transform:
```python
import albumentations as A


class TestAug(A.ImageOnlyTransform):
    def __init__(self, param1, param2, always_apply=False, p=0.5):
        super().__init__(always_apply=always_apply, p=p)
        self.param1 = param1
        self.param2 = param2

    def apply(self, img, **params):
        return img

    def get_transform_init_args_names(self):
        return (""param1"", ""param2"")


class TestSpatial(A.DualTransform):
    def __init__(self, param1, param2, always_apply=False, p=0.5):
        super().__init__(always_apply=always_apply, p=p)
        self.param1 = param1
        self.param2 = param2

    def apply(self, img, **params):
        return img

    def apply_to_bbox(self, bbox, **params):
        return bbox

    def apply_to_keypoint(self, keypoint, **params):
        return keypoint

    def get_transform_init_args_names(self):
        return (""param1"", ""param2"")


aug = A.Compose([TestAug(0, 1), TestSpatial(2, 3)])
A.save(aug, ""test.json"")

aug_res = A.load(""test.json"")
print(aug_res)

```",need serialization try use argument also create custom transform python import class self param param super param param apply self return self return param param class self param param super param param apply self return self return self return self return param param print,issue,positive,positive,positive,positive,positive,positive
639757698,"Before you normalize your boxes do:

width = 1 if width < 1 else width
height = 1 if height < 1 else height

worked for me",normalize width width else width height height else height worked,issue,negative,neutral,neutral,neutral,neutral,neutral
639443343,"Hi @Dipet, @BloodAxe,
Have you got a chance to look into code review?",hi got chance look code review,issue,negative,neutral,neutral,neutral,neutral,neutral
638627319,"A lambda inside Lambda ... you are right. Now it seems so easy. Thanks a lot. 
Just to extend your beautiful solution ... to use it inside a Compose, we have to add `**kwargs` to the lambda-parameter, like so:
```
A.Lambda(p=1, image=lambda img, **kwargs: pixel_dropout(img, name='pixel_dropout'))
```
Thank you, @Dipet ",lambda inside lambda right easy thanks lot extend beautiful solution use inside compose add like thank,issue,positive,positive,positive,positive,positive,positive
638406355,"I think the easiest way to do this is to use Python Lambdas:
```
A.Lambda(p=1, image=lambda img: pixel_dropout(img, name='pixel_dropout'))
```",think easiest way use python,issue,negative,neutral,neutral,neutral,neutral,neutral
638170633,"Hi! Thank you for your PR.
Tests found bug in our code. This bug will be fixed after merge #638 ",hi thank found bug code bug fixed merge,issue,negative,positive,neutral,neutral,positive,positive
638112918,"Tests are failing, but this not seem related to this PR:

```
tests/test_augmentations.py:522: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
albumentations/core/transforms_interface.py:87: in __call__
    return self.apply_with_params(params, **kwargs)
albumentations/core/transforms_interface.py:100: in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
albumentations/augmentations/transforms.py:2508: in apply
    return F.median_blur(image, ksize)
albumentations/augmentations/functional.py:54: in wrapped_function
    result = func(img, *args, **kwargs)
albumentations/augmentations/functional.py:752: in median_blur
    return blur_fn(img)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

img = array([[[0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        ...,
        [0, 0, 0, 0, ... 0, 0],
        ...,
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0]]], dtype=uint8)

def __process_fn(img):
        num_channels = get_num_channels(img)
        if num_channels > 4:
            chunks = []
            for index in range(0, num_channels, 4):
                chunk = img[:, :, index : index + 4]
>               chunk = process_fn(chunk, **kwargs)
E               cv2.error: OpenCV(4.2.0) /io/opencv/modules/imgproc/src/median_blur.simd.hpp:985: error: (-215:Assertion failed) src.depth() == CV_8U && (cn == 1 || cn == 3 || cn == 4) in function 'medianBlur'

albumentations/augmentations/functional.py:176: error
```
",failing seem related return key apply return image result return array index range chunk index index chunk chunk error assertion function error,issue,negative,neutral,neutral,neutral,neutral,neutral
638030937,"If you need only to convert image to tensor you has function `torch.from_numpy`. This transform is necessary to convert images from the library format to the torch image format. And torch work with images in `CHW`, but library works in `HWC`, so we need transpose.",need convert image tensor function transform necessary convert library format torch image format torch work library work need transpose,issue,negative,neutral,neutral,neutral,neutral,neutral
637889413,"Yes, I get that but a function named `ToTensor` must do.. just that.. convert to tensor. Adding extra (undocumented) functionality only leads to confusion.

I already have a pre-processed dataset in CHW format and I would like to use that for TTA. Using the current `ToTensorV2` unnecessarily requires me to transpose the channels just to pass to the function. 
Of course, I can have a workaround by having my own ToTensor, but doesn't that forego the idea of `ToTensorV2` in the first place?
",yes get function must convert tensor extra undocumented functionality confusion already format would like use current unnecessarily transpose pas function course forego idea first place,issue,negative,negative,neutral,neutral,negative,negative
636692937,"Library works on images in HWC format, so if you use library with images in CWH format, something wrong.",library work format use library format something wrong,issue,negative,negative,negative,negative,negative,negative
636549522,"Hi,
May I know what is the rationale behind transposing? What if my dataset has the correct form already like CWH?
For example, I have some preprocessed dataset in which the channel was the first dimension and now, to call ToTensorV2, I have to transpose the data before passing it. 

Can we at least have a flag that indicated if we need to transpose? Thanks!",hi may know rationale behind correct form already like example channel first dimension call transpose data passing least flag need transpose thanks,issue,positive,negative,neutral,neutral,negative,negative
633275535,"No, you didn't miss anything. This is definitely a place where we can improve our documentation.
I will close this issue for now and in next release we will include more clear description on mirroring to docs. Thanks for highlighting this issue. ",miss anything definitely place improve documentation close issue next release include clear description thanks issue,issue,positive,positive,neutral,neutral,positive,positive
633274152,"Thx!  good to know!
I think it might be a good idea to actually point that out in the documentation, otherwise the trainingdata gets corrupted if there is not extra caution for this topic.
Or did i missed this info in the docu somewhere?
",good know think might good idea actually point documentation otherwise corrupted extra caution topic somewhere,issue,positive,positive,positive,positive,positive,positive
633273846,"Hi,

unfortunately, adding extra bounding boxes to reflected regions of the image is not supported. 
You can change border mode to `cv2.BORDER_CONSTANT` if this causes significant impact on the training of your model. ",hi unfortunately extra bounding reflected image change border mode significant impact training model,issue,negative,positive,positive,positive,positive,positive
633273291,"Greetings,

What is the range of values in images in `train_x.npy`? Expected image intensity range for `float32 `dtype is [0..1]. It the value is outside this range, you may get clipped values. ",range image intensity range float value outside range may get clipped,issue,negative,neutral,neutral,neutral,neutral,neutral
630698813,"Our approach based on idea that all transforms get params in albumentations format. All this works do `Compose` class.
So if you need to check transforms directly you need convert bboxes and keypoints to albumentations format first. Look this [link for bboxes](https://github.com/albumentations-team/albumentations/blob/adfa32bda2f5d8ca9c7e6776d78bcc17ffcd41d8/albumentations/augmentations/bbox_utils.py#L21) and [this for keypoints](https://github.com/albumentations-team/albumentations/blob/adfa32bda2f5d8ca9c7e6776d78bcc17ffcd41d8/albumentations/augmentations/keypoints_utils.py#L24)

You can import targets processors like this:
```
from albumentations.augmentations.bbox_utils import BboxProcessor
from albumentations.augmentations.keypoints_utils import KeypointsProcessor
```",approach based idea get format work compose class need check directly need convert format first look link import like import import,issue,negative,positive,positive,positive,positive,positive
630694323,"```
def test_crop(meta,shape):
    img = meta['image']
    image_height,image_width = img.shape[:-1]

    # _RandomResizedCrop =RandomResizedCrop(height=shape[1],
    #                                       width=shape[0],
    #                                       scale=(0.2, 1.4),
    #                                       ratio=(0.6,1.4))
    # params = _RandomResizedCrop.get_params_dependent_on_targets(meta)

    _RandomSizedBBoxSafeCrop =RandomSizedBBoxSafeCrop(height=shape[1],
                                           width=shape[0],erosion_rate=0.3)

    params = _RandomSizedBBoxSafeCrop.get_params_dependent_on_targets(meta)
    print(params)
    xmin = int(params['w_start']*(image_width-params['crop_width']))
    ymin =int(params['h_start']*(image_height-params['crop_height']))
    xmax = xmin + params['crop_width']
    ymax = ymin + params['crop_height']
    roi=(xmin,ymin,xmax,ymax)
    print(roi)
    return roi
```
Thanks for your reply and great work , I really love this augmentation library !
Here is a simple test code.
Use Compose is more convenient of course ,but in my case I want to check the params first and see whether these params are suitable to crop images or masks . I test both RandomResizedCrop and RandomSizedBBoxSafeCrop which I think are same level , RandomResizedCrop works well ,however RandomSizedBBoxSafeCrop not.
Yes , if someone use Compose directly they will not met this bug, but maybe do a small check whether the bboxes are normalized first is more better ? If you think is't not necessary ,ignore this issue ,:)

 

 ",meta shape meta meta meta print print roi return roi thanks reply great work really love augmentation library simple test code use compose convenient course case want check first see whether suitable crop test think level work well however yes someone use compose directly met bug maybe small check whether first better think necessary ignore issue,issue,positive,positive,positive,positive,positive,positive
630680749,"In your examples `h_start` and `w_start` set to 0 and this looks like normalized value.
If you disagree, please get minimal example to reproduce problem.",set like value disagree please get minimal example reproduce problem,issue,negative,negative,neutral,neutral,negative,negative
630679193,"The `Compose` class normalizes all boxes. In `union_of_bboxes` I do not see denormalization. Yes, I see `x1, y1 = width, height`, but in for loop them set to normalized values `x1, y1 = np.min([x1, lim_x1]), np.min([y1, lim_y1])`",compose class see yes see width height loop set,issue,negative,neutral,neutral,neutral,neutral,neutral
630672228,"Hi, I know how to fix this bug and it may help you  ,please refer to https://github.com/albumentations-team/albumentations/pull/634 ,but my pr is refused ...",hi know fix bug may help please refer,issue,positive,neutral,neutral,neutral,neutral,neutral
630665719,"`x, y, x2, y2` is always normalized. If you need transform both images and bboxes you need wrap them into `Compose` like this

```python
Compose([RandomSizedBBoxSafeCrop(50, 50)], bbox_params=BboxParams(format='coco'))
```

Look examples: https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_bboxes.ipynb",always need transform need wrap compose like python compose look,issue,negative,neutral,neutral,neutral,neutral,neutral
630654664,"You need convert them to one of supported bbox formats: https://albumentations.readthedocs.io/en/latest/api/core.html#albumentations.core.composition.BboxParams
Look to example https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/example_bboxes.ipynb
More examples here https://github.com/albumentations-team/albumentations_examples/tree/master/notebooks",need convert one look example,issue,negative,neutral,neutral,neutral,neutral,neutral
627945663,"Function `check_validity` need to check that all bboxes are correct and in range `[0, 1]`.
We may miss an error with these changes.",function need check correct range may miss error,issue,negative,neutral,neutral,neutral,neutral,neutral
627304218,"Ty Dipet
Following command solved my issue
**python3 -m pip install --upgrade pip setuptools wheel**
pip3 install imagecodecs

Then finally,
pip3 install albumentations",following command issue python pip install upgrade pip wheel pip install finally pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
627302621,try to update pip: `pip install --upgrade pip`,try update pip pip install upgrade pip,issue,negative,neutral,neutral,neutral,neutral,neutral
626637837,"> @BloodAxe I sloved this problem by followed this [issue](https://github.com/Czaki/imagecodecs/issues/6). Just upgrade pip. `python3.6 -m pip install -U pip`

It worked for me. Thank you..",problem issue upgrade pip python pip install pip worked thank,issue,negative,neutral,neutral,neutral,neutral,neutral
626131296,@BloodAxe I sloved this problem by followed this [issue](https://github.com/Czaki/imagecodecs/issues/6). Just upgrade pip. `python3.6 -m pip install -U pip`,problem issue upgrade pip python pip install pip,issue,negative,neutral,neutral,neutral,neutral,neutral
626123987,"@BloodAxe Thanks, I  tried only install imagecodecs, the log is same. full log is below:

```bash
$: python3.6 -m pip install imagecodecs
Collecting imagecodecs
  Downloading http://pypi.doubanio.com/packages/75/95/3699360d15e5e5d450bbd16ea554fb185117947e76fc772078c4f59ae1c1/imagecodecs-2020.2.18.tar.gz (8.8MB)
    100% |████████████████████████████████| 8.8MB 9.8MB/s 
Collecting numpy>=1.15 (from imagecodecs)
  Downloading http://pypi.doubanio.com/packages/03/27/e35e7c6e6a52fab9fcc64fc2b20c6b516eba930bb02b10ace3b38200d3ab/numpy-1.18.4-cp36-cp36m-manylinux1_x86_64.whl (20.2MB)
    100% |████████████████████████████████| 20.2MB 12.6MB/s 
Installing collected packages: numpy, imagecodecs
  Running setup.py install for imagecodecs ... error
    Complete output from command /usr/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-ffn3ubwt/imagecodecs/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace
('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-u6ng7rau-record/install-record.txt --single-version-externally-managed --compile --user --prefix=:
    running install
    running build
    running build_py
    creating build
    creating build/lib.linux-x86_64-3.6
    creating build/lib.linux-x86_64-3.6/imagecodecs
    copying imagecodecs/imagecodecs.py -> build/lib.linux-x86_64-3.6/imagecodecs
    copying imagecodecs/__init__.py -> build/lib.linux-x86_64-3.6/imagecodecs
    copying imagecodecs/_imagecodecs.py -> build/lib.linux-x86_64-3.6/imagecodecs
    copying imagecodecs/__main__.py -> build/lib.linux-x86_64-3.6/imagecodecs
    creating build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-zopfli -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
copying imagecodecs/licenses/LICENSE-zlib -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-zfp -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-bzip2 -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-snappy -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-lzf -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-zstd -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-brunsli -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-bitshuffle -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-brotli -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-charls -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-lz4 -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-giflib -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-blosc -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libjpeg-turbo -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-liblzma -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-openjpeg -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libwebp -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-jpg_0xc3 -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-jpeg -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-lcms2 -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libaec -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-jxrlib -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libmng -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libpng -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-fastlz -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    copying imagecodecs/licenses/LICENSE-libtiff -> build/lib.linux-x86_64-3.6/imagecodecs/licenses
    running build_ext
    building 'imagecodecs._aec' extension
    creating build/temp.linux-x86_64-3.6
    creating build/temp.linux-x86_64-3.6/imagecodecs
    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Iimagecodecs -I/usr/include/python3.6m -I/home/u
ser/.local/lib/python3.6/site-packages/numpy/core/include -c imagecodecs/_aec.c -o build/temp.linux-x86_64-3.6/imagecodecs/_aec.o
    In file included from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,
                     from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
                     from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,
from imagecodecs/_aec.c:598:
    /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_AP
I_VERSION"" [-Wcpp]
     #warning ""Using deprecated NumPy API, disable it with "" \
      ^
    imagecodecs/_aec.c:602:20: fatal error: libaec.h: No such file or directory
    compilation terminated.
    error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
    
    ----------------------------------------
Command ""/usr/bin/python3.6 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-ffn3ubwt/imagecodecs/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__
, 'exec'))"" install --record /tmp/pip-u6ng7rau-record/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/pip-build-ffn3ubwt/imagecodecs/
```",thanks tried install log full log bash python pip install collected running install error complete output command import compile open install record compile user running install running build running build running building extension file included warning warning disable define warning disable fatal error file directory compilation error command exit status command import compile open install record compile user error code,issue,negative,positive,positive,positive,positive,positive
626123279,"Could you please include full build/install log? It seems some
third-third-party dependency is failing. But without full log it’s hard to
trace which one.

сб, 9 мая 2020 г. в 10:35 AM, 醉舟 <notifications@github.com>:

> 🐛 Bug
>
> pip install error, somthing related with imagecodecs, what other package
> should be installed?
> Environment
>
>    - Albumentations version : 0.4.5
>    - Python version : 3.6
>    - OS : Linux Ubuntu 16.04
>    - How you installed albumentations : pip
>    - Any other relevant information:
>
> Additional context
>
> running build_ext
>
> building 'imagecodecs._aec' extension
>
> creating build/temp.linux-x86_64-3.6
>
> creating build/temp.linux-x86_64-3.6/imagecodecs
>
> x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong-Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -Iimagecodecs -I/usr/include/python3.6m -I/home/user/.local/lib/python3.6/site-packages/numpy/core/include -c imagecodecs/_aec.c -o build/temp.linux-x86_64-3.6/imagecodecs/_aec.o
>
> In file included from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarraytypes.h:1832:0,
>
>                      from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,
>
>                      from /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/arrayobject.h:4,
>
>                      from imagecodecs/_aec.c:598:
>
> /home/user/.local/lib/python3.6/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning ""Using deprecated NumPy API, disable it with "" ""#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION"" [-Wcpp]
>
>      #warning ""Using deprecated NumPy API, disable it with "" \
>
>       ^
>
>     imagecodecs/_aec.c:602:20: fatal error: libaec.h: No such file or directory
>
>     compilation terminated.
>
>     error: command 'x86_64-linux-gnu-gcc' failed with exit status 1
>
>
>
>     ----------------------------------------
>
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/630>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YBSOGGN7UV5REMHKNTRQUBVDANCNFSM4M4V7FHQ>
> .
>
",could please include full log dependency failing without full log hard trace one bug bug pip install error related package environment version python version o pip relevant information additional context running building extension file included warning warning disable define warning disable fatal error file directory compilation error command exit status thread reply directly view,issue,negative,positive,positive,positive,positive,positive
625725617,"Hi,

Short answer is no. 

Long version:
You can easily achieve desired functionality by writing custom prediction function:

```
image = YOUR_INPUT_IMAGE
input = torch.from_numpy(image).permute(2,0,1).unsqueeze(0)
output = model(input)

augment = A.CoarseDropout(p=1)
for _ in range(10):
   augmented = augmentation(image=image)
   input = torch.from_numpy(augmented [""image""]).permute(2,0,1).unsqueeze(0)
   output += model(input)

output /= 11
```

This snippet is sub-optimal in terms of performance, but it's goal to illustrate an idea.  Hope it helps.",hi short answer long version easily achieve desired functionality writing custom prediction function image input image output model input augment range augmented augmentation input augmented image output model input output snippet performance goal illustrate idea hope,issue,positive,positive,positive,positive,positive,positive
625674952,"Sorry, mistake in my code caused it. This is correct.",sorry mistake code correct,issue,negative,negative,negative,negative,negative,negative
625158333,Looks correct. Can you give some example where this operations are wrong?,correct give example wrong,issue,negative,negative,negative,negative,negative,negative
625096931,"This PR definitely needs informative description, motivation section and benchmarks.",definitely need informative description motivation section,issue,positive,neutral,neutral,neutral,neutral,neutral
625054324,"> P.S. deep source is still failing. Is it possible to run it on all codebase and fix all possible issues?

этот дипсорс каждый день добавляет новые бредовые проверки. Это их фича такая. Поэтому я его фиксил его прибабахи отдельным коммитом внутри пул-реквеста. Это конечно неправильно, но зато помогает пройти проверки.
",deep source still failing possible run fix possible,issue,negative,neutral,neutral,neutral,neutral,neutral
624891338,"I've added Autocontrast and RandomShear transforms and made AugMix to accept list of transforms. 
I didn't introduce new composition transforms in order not to overcomplicate the implementation for now. 

P.S. deep source is still failing. Is it possible to run it on all codebase and fix all possible issues?",added made accept list introduce new composition order implementation deep source still failing possible run fix possible,issue,negative,positive,neutral,neutral,positive,positive
624261558,"Currently call to `res = aug(image=img)` results in:
```
  File ""D:\Programming\3rd_party\albumentations\albumentations\core\composition.py"", line 174, in __call__
    p.preprocess(data)
  File ""D:\Programming\3rd_party\albumentations\albumentations\core\utils.py"", line 62, in preprocess
    data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=""to"")
KeyError: 'bboxes'
```
that is really hard to interpret if you are not acquainted with albumentations' innards. ",currently call file line data file line data data really hard interpret acquainted,issue,negative,positive,neutral,neutral,positive,positive
624206816,"From my point of view behaviour will become more intuitive and will not tend to throw exceptions were they are not expected. Of cause pitfalls like this can be overcomed with some 'plyaski s bubnom' :) if you already know where they are, but more intuitive way is more comfortable. At least it takes some amount of time to find why fhings went wrong, when you face the problem 1st time (as it was with me). Currently you get key error somewhere deep in 3rd party code and it can be a problem for user who is not ready ro debug this code.",point view behaviour become intuitive tend throw cause like already know intuitive way comfortable least amount time find went wrong face problem st time currently get key error somewhere deep party code problem user ready code,issue,negative,negative,neutral,neutral,negative,negative
624116041,"Do you really need this changes?
Why you can not set bboxes on call to empty list?
With this changes code will be less predictable.",really need set call empty list code le predictable,issue,negative,negative,neutral,neutral,negative,negative
624047931,"Please, add some tests to check new behavior. ",please add check new behavior,issue,negative,positive,positive,positive,positive,positive
623982942,"Done. But had to change the 2nd test: rect(0,0,90,90) is not cropped as rects are considered as semi-open, only rect(0,0,91,91) is",done change test rect considered rect,issue,negative,neutral,neutral,neutral,neutral,neutral
623973138,"Please add this test into `test_bbox.py`
```python
@pytest.mark.parametrize(
    [""transforms"", ""bboxes"", ""result_bboxes"", ""min_area"", ""min_visibility""],
    [
        [[Crop(10, 10, 20, 20)], [[0, 0, 10, 10, 0]], [], 0, 0],
        [[Crop(0, 0, 90, 90)], [[0, 0, 1, 91, 0], [0, 0, 1, 90, 0]], [[0, 0, 1, 90, 0]], 0, 1],
        [[Crop(0, 0, 90, 90)], [[0, 0, 1, 10, 0], [0, 0, 1, 11, 0]], [[0, 0, 1, 10, 0], [0, 0, 1, 11, 0]], 10, 0],
    ]
)
def test_bbox_params_edges(transforms, bboxes, result_bboxes, min_area, min_visibility):
    image = np.empty([100, 100, 3], dtype=np.uint8)
    aug = Compose(transforms, bbox_params=BboxParams(""pascal_voc"", min_area=min_area, min_visibility=min_visibility))

    res = aug(image=image, bboxes=bboxes)[""bboxes""]
    assert np.allclose(res, result_bboxes)
```",please add test python crop crop crop image compose assert,issue,negative,neutral,neutral,neutral,neutral,neutral
623949466,"Very interesting.
Problem how pytorch, numpy and OpenCV share memory.
Until we find solution how to resolve problem you can use this fix to avoid problem:
```
x_np = albu_shadow(image=x_np.astype(np.uint8).copy())['image']
```
You need directly create numpy copy of image before provide it into albumentations transfromation",interesting problem share memory find solution resolve problem use fix avoid problem need directly create copy image provide,issue,negative,positive,positive,positive,positive,positive
623944525,"Sorry, let me reproduce what I get.
```
import torch
import numpy as np
import albumentations as albu

albu_shadow = albu.RandomShadow(p=1, num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1))

x = torch.rand(1,3,24,24)
x_np = x[0, :, :, :].permute([1, 2, 0]).numpy() * 255
x_np = albu_shadow(image=x_np.astype(np.uint8))['image']

```

with shape 
```
x_np.shape
(24, 24, 3)
```

error:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/tsunyi/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/transforms
_interface.py"", line 87, in __call__    return self.apply_with_params(params, **kwargs)
  File ""/home/tsunyi/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/core/transforms
_interface.py"", line 100, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""/home/tsunyi/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/transforms.py"", line 2053, in apply
    return F.add_shadow(image, vertices_list)
  File ""/home/tsunyi/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/functional.py"", line 54, in wrapped_function
    result = func(img, *args, **kwargs)
  File ""/home/tsunyi/anaconda3/envs/pytorch/lib/python3.7/site-packages/albumentations/augmentations/functional.py"", line 997, in add_shadow
    cv2.fillPoly(mask, vertices, 255)
TypeError: Expected Ptr<cv::UMat> for argument 'img'
```",sorry let reproduce get import torch import import shape error recent call last file line module file line return file line key file line apply return image file line result file line mask vertex argument,issue,negative,negative,negative,negative,negative,negative
623942291,"All works fine. In your example image has wrong shape. Try this:
```
import albumentations as albu

albu_shadow = albu.RandomShadow(p=1, num_shadows_lower=1, num_shadows_upper=1, shadow_dimension=5, shadow_roi=(0, 0.5, 1, 1))

import numpy as np

x_np = np.empty([100, 100, 3], dtype=np.uint8)
x_np = albu_shadow(image=x_np)['image']
```",work fine example image wrong shape try import import,issue,negative,negative,neutral,neutral,negative,negative
623940872,"To avoid this problems you can install pre-commit hook, like described here https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst
Or you can format code directly using the command `black --config=black.toml` inside the project root.",avoid install hook like format code directly command black inside project root,issue,negative,negative,neutral,neutral,negative,negative
623940217,"Please provide a minimal code snippet to reproduce the issue.

вт, 5 мая 2020 г. в 11:59 AM, Tsun-Yi Yang <notifications@github.com>:

> Just to be clear. My input is np uint8 array. It does not take it.
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/issues/614#issuecomment-623939309>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YFMU3CNGD7ZEUXXZKTRP7IQXANCNFSM4MYJXNDA>
> .
>
",please provide minimal code snippet reproduce issue yang clear input array take thread reply directly view,issue,positive,positive,neutral,neutral,positive,positive
623939309,Just to be clear. My input is np uint8 array. It does not take it.,clear input array take,issue,negative,positive,positive,positive,positive,positive
623936609,"I forced the same problem, but with bounding boxes. See discussion in #381. Two ways to handle it is proposed there: 1) by inheritence 2) using ReplayCompose and ['replay'].",forced problem bounding see discussion two way handle,issue,negative,negative,negative,negative,negative,negative
623931935,People! Somebody remove blank line at end of file functional.py to fix tests and merge it! Very usefull PR!,people somebody remove blank line end file fix merge,issue,negative,neutral,neutral,neutral,neutral,neutral
623417233,"Hm... Rather strange undocumented behaviour. It is a surprize.
At last, if you need som part of Compose to be executed always, why not to do it in a strict way:
```
Compose([SomeStuffToBeDoneAlways(...),
                 Compose([OptionalStuff1(...),
                                   OptionalStuff2(...)], p=0.5),
                 AnotherStuffToBeDoneAlways(...)])
```
instead of 
```
Compose(SomeStuffToBeDoneAlways(some  parameters, more parameters, etc, always_done=True),
                 OptionalStuff1(some  parameters, more parameters, etc, ),
                 OptionalStuff2(some  parameters, more parameters, etc, ),
                 AnotherStuffToBeDoneAlways(some  parameters, more parameters, etc, always_done=True)],
                 p=0.5)
```
From my oppinion the 2nd options is much more misleading.",rather strange undocumented behaviour last need part compose executed always strict way compose compose instead compose much misleading,issue,negative,positive,neutral,neutral,positive,positive
623399536,"Could anybody tell what is wrong with CI / Check code formatting with Black (3.8) here?
I can not see any descriptive error message.",could anybody tell wrong check code black see descriptive error message,issue,negative,negative,negative,negative,negative,negative
623367937,"Update: it seems better to add parameter `check_validity` instead of `check_is_inside` and turn off a whole check for such a specific case: it requires less modification in code, correspondes to same parameres of `convert_bbox_to_albumentations` and `convert_bbox_from_albumentations` functions, more simple to describe and more jeneral.",update better add parameter instead turn whole check specific case le modification code simple describe,issue,negative,positive,positive,positive,positive,positive
623346620,I see it was changed by @albu [on 29.09.18](https://github.com/albumentations-team/albumentations/commit/5b37676b10ddc2ffb7cf8ef227e85cea206192d6#diff-50e65af094e2fbe5d58c002f7036041bR210). Was there any reason for it other than to handle min_visibility==0 in a more simple way?,see reason handle simple way,issue,negative,neutral,neutral,neutral,neutral,neutral
623124124,"I suppose  #350 closes needs proposed here. If one needs postmortem information about what augmentations were applied (and with what params), he can replace `Compose` class with `ReplayCompose` and then write somthing like:
```
       aug_res = self.albumentations(image=aug_img, bboxes=rects)
       aug_bboxes = aug_res['bboxes']
       for t in aug_res['replay']['transforms']:
             if t['__class_fullname__'].endswith('.HorizontalFlip') and t['applied']:
                    aug_bboxes = [rect_hflip(b) for b in aug_bboxes]
```
Also approach with inheritance of HorizontalFlip class proposed above solves the problem.
So I close an issue.",suppose need one need information applied replace compose class write like also approach inheritance class problem close issue,issue,negative,neutral,neutral,neutral,neutral,neutral
622437301,"Я как раз думал ровно о том же и вот. Вот мои мысли.

У `Compose(`) надо добавить параметр `shufle=False|True` . По дефолту `shufle=False`. Если он `shufle=True`, то все трансформации перемешиваются в случайном порядке. (Это видимо аналог твоего `Mix()`-а ).

Добавить аналог `Compose()`: `Blend(n=(3,4))` (варианты названий `Map`, `MapReduce`, `Mix`, `SplitJoin`). Ещё вариант: вместо функции добавить параметр `n` у `Compose()`. Оно дублирует вход на `n` потоков, выполняет для каждого трансформации, а потом объединяет со случайными весами всё в одно изображение. Единственное, что мне тут не нравится: в одном месте делается 2 функции: дублирование на несколько потоков и обратное их слияние.

Для Bbox-ов, кстати, можно non maximum suppression применять при слиянии...",compose mix compose blend map mix compose non maximum suppression,issue,negative,neutral,neutral,neutral,neutral,neutral
622345968,"Yes, I agree with @MichaelMonashev If we will use `interpolation` as tuple we are will avoid all this problems.
For such case we can use this checks:
```
if interpolation is None:
    self.down_interpolation, self.up_interpolation = cv2.INTER_NEAREST, cv2.INTER_NEAREST
    warnings.warn(""Using default interpolation INTER_NEAREST, which is sub-optimal.""
                  ""Please specify interpolation mode for downscale and upscale explicitly.""
                  ""For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584"")
elif isinstance(interpolation, int):
    self.down_interpolation, self.up_interpolation = interpolation, interpolation
else:
    self.down_interpolation, self.up_interpolatio = interpolation
```",yes agree use interpolation avoid case use interpolation none default interpolation please specify interpolation mode upscale explicitly additional information see interpolation interpolation interpolation else interpolation,issue,negative,neutral,neutral,neutral,neutral,neutral
622344579,"We can keep back compibility if change `down_interpolation` and `up_interpolation` to `interpolation=(cv2.INTER_AREA, cv2.INTER_NEAREST)`. And provide documentation about first and second interpolation.",keep back change provide documentation first second interpolation,issue,negative,positive,neutral,neutral,positive,positive
622344089,"I think we should not rename or remove `interpolation` argument for backward compatibility. At this time our test failed for this reasons and for somebody will fail old pipelines.

I think we need to leave in place `interpolation` and add warning:
```
if interpolation is not None and down_interpolation is None and up_interpolation is None:
    down_interpolation = cv2.INTER_NEAREST
    up_interpolation = down_interpolation
    warnings.DeprecationWarning(""Argument interpolations is deprecated. Please specify interpolation mode directly for downscale and upscale. "")
```",think rename remove interpolation argument backward compatibility time test somebody fail old think need leave place interpolation add warning interpolation none none none argument please specify interpolation mode directly,issue,negative,negative,negative,negative,negative,negative
622340031,Also be cool if we will provide url to this PR in warning message for more information.,also cool provide warning message information,issue,negative,positive,positive,positive,positive,positive
622336250,"This issue is already fixed in master branch. There was a missing `list` type, but it's been recently fixed. 
https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L2540",issue already fixed master branch missing list type recently fixed,issue,negative,neutral,neutral,neutral,neutral,neutral
622331376,"Whenever possible, we do not want to change existing behavior of augmentations. That means, newly introduced `down_interpolation` and `up_interpolation` should have default values as they were before this PR. e.g `INTER_NEAREST`.
Obeying this rule causes some friction, but this way prevents from causing silently changes in training results. Imagine one has training pipeline with Downscale augmentation. After updating to never release he may gets totally different model performance since we have changed defaults. This should be avoided at all cost. Therefore, when possible - we should stick to defaults.

At the same time, I agree that default interpolation mode is sub-optimal. So here's my suggestions:
1) Let's make the following change in c'tor:

```
def __init__(self, scale_min=0.25, scale_max=0.25, down_interpolation=None, up_interpolation=None, always_apply=False, p=0.5):
  if down_interpolation is None:
    down_interpolation = cv2.INTER_NEAREST
    warnings.warn(""Using default interpolation INTER_NEAREST, which is sub-optimal. Please specify down_interpolation mode to Downscale explicitly. ""

  if up_interpolation is None:
    up_interpolation= cv2.INTER_NEAREST
    warnings.warn(""Using default interpolation INTER_NEAREST, which is sub-optimal. Please specify up_interpolation mode to Downscale explicitly. ""

```

In this way we 
a) Inform user that he can change interpolation to optimal values
b) Keep default behavior intact

> I've updated the transform code, it obviously fails at some tests since they are outdated, should I change them accordingly too ?

Pulling latest changes to your PR branch from master should resolve the issues

",whenever possible want change behavior newly default rule friction way causing silently training imagine one training pipeline augmentation never release may totally different model performance since cost therefore possible stick time agree default interpolation mode let make following change self none default interpolation please specify mode none default interpolation please specify mode way inform user change interpolation optimal keep default behavior intact transform code obviously since outdated change accordingly latest branch master resolve,issue,positive,positive,neutral,neutral,positive,positive
622317538,Why do you think we should update to latest pytorch? As a rule of thumb - if it's working fine - there is no need to update. Especially since the 1.5 release does not bring any critical bugfixes.,think update latest rule thumb working fine need update especially since release bring critical,issue,negative,positive,positive,positive,positive,positive
622294935,"@BloodAxe , the new downscale() interface is incompatible with the previous one. Is it ok?
",new interface incompatible previous one,issue,negative,negative,neutral,neutral,negative,negative
621917440,"I've updated the transform code, it obviously fails at some tests since they are outdated, should I change them accordingly too ? ",transform code obviously since outdated change accordingly,issue,negative,negative,negative,negative,negative,negative
621871968,"+1 for having two interpolation arguments

чт, 30 апр. 2020 г. в 4:44 PM, Nathan Hubens <notifications@github.com>:

> Ok I can add a second argument !
> @MichaelMonashev <https://github.com/MichaelMonashev> wouldn't a tuple be
> a little bit misleading for which interpolation corresponds to which phase
> (downscale/upscale) ? Also, as you'd want to have cv2.INTER_AREA for
> downscaling 99% of the type, it probably should be a default that you don't
> need to pass every time you call the function, what do you think of simply ?
>
> def downscale(img, scale, down_interpolation = cv2.INTER_AREA, up_interpolation=cv2.INTER_NEAREST):
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/pull/584#issuecomment-621861419>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAEB6YHOZEVRHAKJJT5IB2LRPF6CHANCNFSM4MDISQJA>
> .
>
",two interpolation add second argument would little bit misleading interpolation phase also want type probably default need pas every time call function think simply scale thread reply directly view,issue,negative,negative,neutral,neutral,negative,negative
621861419,"Ok I can add a second argument ! 
@MichaelMonashev wouldn't a tuple be a little bit misleading for which interpolation corresponds to which phase (downscale/upscale) ? Also, as you'd want to have `cv2.INTER_AREA` for downscaling 99% of the time, it probably should be a default that you don't need to pass every time you call the function, what do you think of simply ?

```python
def downscale(img, scale, down_interpolation = cv2.INTER_AREA, up_interpolation=cv2.INTER_NEAREST):
```",add second argument would little bit misleading interpolation phase also want time probably default need pas every time call function think simply python scale,issue,negative,negative,neutral,neutral,negative,negative
621666985,"@Dipet, @BloodAxe 
Could you please take a look? These changes are small but blocking 2 PRs.",could please take look small blocking,issue,negative,negative,negative,negative,negative,negative
621453571,DeepSource failed not in my changes... Probably it would be better to fix issues in separate PR.,probably would better fix separate,issue,negative,positive,positive,positive,positive,positive
621447532,It is great if you will use albumentations as a base for your package,great use base package,issue,positive,neutral,neutral,neutral,neutral,neutral
621443579,"Sure, in the middle of my work I realized that it is not fitting very good fit in the current package, so I started to work slowly on my stuff taking a lot from your code(mostly cutting things I don't need), because for me images are not very helpful and my work lays in a bit different domain. And your code for me is like a good standard where I should aim at.
Thank you for the great package!

Would it be okay if I can use your code as a base for the package on 3d data? Sorry for not saying this few days later. I would try to give as much credit as I can",sure middle work fitting good fit current package work slowly stuff taking lot code mostly cutting need helpful work bit different domain code like good standard aim thank great package would use code base package data sorry saying day later would try give much credit,issue,positive,positive,neutral,neutral,positive,positive
621410873,"@erezposner , you need split your transform to 2 parts. Move this code https://github.com/albumentations-team/albumentations/pull/571/files#diff-a80a9cab03e62b28e741d05ef8182abcR1170 to functional.py . After moving you can use it without `fix_params`.",need split transform move code moving use without,issue,negative,neutral,neutral,neutral,neutral,neutral
621379417,"I think second interpolation argument adds flexibility. It can be passed via tuple:
`interpolation=(cv2.INTER_AREA, cv2.INTER_NEAREST)`",think second interpolation argument flexibility via,issue,negative,neutral,neutral,neutral,neutral,neutral
621349020,"> Can you show full traceback of error?

 AttributeError: 'GaussNoise' object has no attribute 'var_limit' 
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/IPython/core/formatters.py in __call__(self, obj)
    700                 type_pprinters=self.type_printers,
    701                 deferred_pprinters=self.deferred_printers)
--> 702             printer.pretty(obj)
    703             printer.flush()
    704             return stream.getvalue()

/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/IPython/lib/pretty.py in pretty(self, obj)
    392                         if cls is not object \
    393                                 and callable(cls.__dict__.get('__repr__')):
--> 394                             return _repr_pprint(obj, self, cycle)
    395 
    396             return _default_pprint(obj, self, cycle)

/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/IPython/lib/pretty.py in _repr_pprint(obj, p, cycle)
    682     """"""A pprint that just redirects to the normal repr function.""""""
    683     # Find newlines and replace them with p.break_()
--> 684     output = repr(obj)
    685     lines = output.splitlines()
    686     with p.group():

/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/albumentations/core/transforms_interface.py in __repr__(self)
    111     def __repr__(self):
    112         state = self.get_base_init_args()
--> 113         state.update(self.get_transform_init_args())
    114         return ""{name}({args})"".format(name=self.__class__.__name__, args=format_args(state))
    115 

/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/albumentations/core/transforms_interface.py in get_transform_init_args(self)
    181 
    182     def get_transform_init_args(self):
--> 183         return {k: getattr(self, k) for k in self.get_transform_init_args_names()}
    184 
    185     def _to_dict(self):

/local/hekrdmir/Applications/miniconda3/envs/smp_v0/lib/python3.7/site-packages/albumentations/core/transforms_interface.py in <dictcomp>(.0)
    181 
    182     def get_transform_init_args(self):
--> 183         return {k: getattr(self, k) for k in self.get_transform_init_args_names()}
    184 
    185     def _to_dict(self):

AttributeError: 'GaussNoise' object has no attribute 'var_limit'",show full error object attribute recent call last self return pretty self object callable return self cycle return self cycle cycle normal function find replace output self self state return name state self self return self self self return self self object attribute,issue,negative,positive,positive,positive,positive,positive
621333199,Need more clearly implementations for support of gray images  for all transforms.,need clearly support gray,issue,positive,positive,positive,positive,positive,positive
621322142,"Yes in PYPI we can find many wrappers around OpenCV. But our library is not focused on manage packages dependencies. We added hack for opecv-python and opencv-python-headless because they are the most must popular packages.

If you have or need other wrappers you can install library with flag `--no-deps`.",yes find many around library manage added hack must popular need install library flag,issue,positive,positive,positive,positive,positive,positive
621314948,Thanks for PR. We will look to your code and possible create a tool in albumentaions_example for that purposes.,thanks look code possible create tool,issue,positive,positive,neutral,neutral,positive,positive
621313060,"Thank for your PR. But we decide to focus only on images and targets on images. So this functionality is overhead for single library.
You can create your own package with supports this targets.",thank decide focus functionality overhead single library create package,issue,positive,negative,neutral,neutral,negative,negative
621312960,"Amen

On Wed, 29 Apr 2020, 20:09 Mikhail Druzhinin, <notifications@github.com>
wrote:

> This PR changes default behavior of transform. In that case it is
> unjustified.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albumentations-team/albumentations/pull/568#issuecomment-621310817>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABHC5XPAXGX4TYVOBWDAH2LRPBGM5ANCNFSM4LOSNGDA>
> .
>
",amen wed wrote default behavior transform case unjustified thread reply directly view,issue,negative,positive,neutral,neutral,positive,positive
621310817,This PR changes default behavior of transform. In that case it is unjustified.,default behavior transform case unjustified,issue,negative,neutral,neutral,neutral,neutral,neutral
621309587,"This PR is to old. If you need this functionality, please create new PR.",old need functionality please create new,issue,positive,positive,positive,positive,positive,positive
621295164,"You have installed very old version. Please update to latest, which is 0.4.5.",old version please update latest,issue,negative,positive,positive,positive,positive,positive
621098449,Better create a new PR for a clear commits tree,better create new clear tree,issue,positive,positive,positive,positive,positive,positive
621095535,"Hi guys,
Do you want me to rebase again from master, or start a fresh PR to clear to failing lint tests from core?
I can do either one, if you are still interested in merging this... just let me know
Thanks!",hi want rebase master start fresh clear failing lint core either one still interested let know thanks,issue,positive,positive,positive,positive,positive,positive
621041363,"A tuple is better, after some time someone will need this functionality.",better time someone need functionality,issue,negative,positive,positive,positive,positive,positive
621040332,"Или сделать аргумент у Scale() или в виде одной цифры или в виде тупла для каждой координаты:
- scale ((float, float) or float) scaling factor.",scale scale float float float scaling factor,issue,negative,neutral,neutral,neutral,neutral,neutral
620815974,"@BloodAxe Maybe the best way to do `Resize` as simple wrapper around `cv2.resize`?
`cv2.resize` default arguments are:
`src, dsize, dst=None, fx=None, fy=None, interpolation=None`
To do this we need add `fx` and `fy` into our `functional.resize`",maybe best way resize simple wrapper around default need add,issue,positive,positive,positive,positive,positive,positive
620808140,"It is very tempting to approve this PR, however I'm determined that we should follow single responsibility principle and not mix `A.Resize` operation (when dimensions are known) with `A.Scale ` (where scale factor and image size defines image output size). 

My suggestion is to add new augmentation Scale that would do exactly one transformation, without need to hack with `width=None, height=None`. ",tempting approve however determined follow single responsibility principle mix operation known scale factor image size image output size suggestion add new augmentation scale would exactly one transformation without need hack,issue,negative,positive,positive,positive,positive,positive
620789605,"We do not have file normalize.py in source code of albumentations for current release and many previous versions. Maybe you have some really old version of albumentations. Therefore I strongly encourage you to double-check what version do you have installed and update if necessary. If you still think the issue exists in current release - please re-open this issue and attach a link to exact place in the code, where you think the error is.",file source code current release many previous maybe really old version therefore strongly encourage version update necessary still think issue current release please issue attach link exact place code think error,issue,positive,positive,positive,positive,positive,positive
619516519,"Let me clarify my statement.

To start, I'm not arguing that Mosaic augmentation is useless. Indeed, there are use case scenarios, where it brings value to model accuracy. Applying it to object detection is quite natural. But it's not so straightforward to apply it to image classification (e.g this would require to change objective function to multi-label classification). 

Second, API design of Albumentations assumes **single entity augmentation** (By entity I mean image and optionally corresponding mask, keypoints and bboxes). That is the main blocker from my perspective. And I'm not convinced that we should change our API to support training augmentations that goes above the single-entity scope. 

Third, mosaic augmentation seems can be implemented quite easy aside albumentations:

```
def __getitem(self, index):
  indexes = random.choice(np.arange(len(self)), 4)
  images = self.images[indexes]
  bboxes = self.bboxes[indexes]
  labels = self.labels[indexes]
  
  augmented_data = [self.augment(image=i, bboxes=b, labels=l) for (i,b,l) in zip(image, bboxes, labels)]
  
  image, bboxes, labels = grid_stack_images(augmented_data )


```

Hope this clarifies my line of thinking.

",let clarify statement start mosaic augmentation useless indeed use case value model accuracy object detection quite natural straightforward apply image classification would require change objective function classification second design single entity augmentation entity mean image optionally corresponding mask main blocker perspective convinced change support training go scope third mosaic augmentation quite easy aside self index self zip image image hope line thinking,issue,positive,positive,neutral,neutral,positive,positive
619395599,Ultralytics yolov3 repo uses it too. I think this type of augmentation is one of the key reasons why their model is so good compared to the original yolov3-spp weights ,think type augmentation one key model good original,issue,positive,positive,positive,positive,positive,positive
619380695,"> This augmentation has quite narrow use-case

That is quite a bold statement considering current state-of-the-art object detection YOLOv4 utilizes _mosaic augmentation_: https://arxiv.org/abs/2004.10934",augmentation quite narrow quite bold statement considering current object detection,issue,negative,positive,neutral,neutral,positive,positive
618985623,"There is a problem with how you apply augmentations to your image. The right way to apply augmentations is the following:

```
data = transformation(image=img)
augmented_image = data[""image""]
```

Please, follow examples & tutorials at https://github.com/albumentations-team/albumentations_examples.",problem apply image right way apply following data transformation data image please follow,issue,negative,positive,positive,positive,positive,positive
617427881,I would like to take up this enhancement.,would like take enhancement,issue,negative,neutral,neutral,neutral,neutral,neutral
617425589,Deep source has been failed not in my changes. Not sure how to properly suppress it.,deep source sure properly suppress,issue,negative,positive,positive,positive,positive,positive
617420677,"I've decided to add this ability only for CoarseDropout, MaskDropout, and GridDropout. Looks like it doesn't make a lot of sense for others.
Created PR: #597 ",decided add ability like make lot sense,issue,positive,neutral,neutral,neutral,neutral,neutral
616764323,"This augmentation has quite narrow use-case and somewhat similar to mixup/mix-match/aug-mix training tricks, which is naturally done quite effectively on GPU. Therefore, duplicating this functionality in Albumentations is doubtful. ",augmentation quite narrow somewhat similar training naturally done quite effectively therefore functionality doubtful,issue,negative,negative,neutral,neutral,negative,negative
616746901,"@Dipet thanks for taking a look at it! Indeed, there's probably more neat and consistent approach in #594, hope it would be merged soon and become available for use.",thanks taking look indeed probably neat consistent approach hope would soon become available use,issue,positive,positive,positive,positive,positive,positive
616680109,"Fixed at last commit. Try to install latest version from github
```
pip install git+https://github.com/albumentations-team/albumentations.git
```",fixed last commit try install latest version pip install,issue,negative,positive,positive,positive,positive,positive
616621066,"No, library does not provide interface to set different parameters for each target and choose which target to be transformed.",library provide interface set different target choose target,issue,negative,neutral,neutral,neutral,neutral,neutral
616600675,"Hi, thanks for PR. Transforms is not suitable for these purposes because bboxes formats changes in [BbboxProcessor](https://github.com/albumentations-team/albumentations/blob/9c225a99a379594098dbea2a077fd22da684ade9/albumentations/augmentations/bbox_utils.py#L21)

I am resolved it in #594 ",hi thanks suitable resolved,issue,positive,positive,positive,positive,positive,positive
616504094,"Sorry to intrude, I've just experienced the same problem!
I've been able to replicate the problem with this code, by using the example images from the TGS salt challenge. 

```
import albumentations as A
import cv2
from matplotlib import pyplot as plt
%matplotlib inline

def visualize(image, mask, original_image=None, original_mask=None):
    fontsize = 18
    
    if original_image is None and original_mask is None:
        f, ax = plt.subplots(2, 1, figsize=(8, 8))

        ax[0].imshow(image)
        ax[1].imshow(mask)
    else:
        f, ax = plt.subplots(2, 2, figsize=(8, 8))

        ax[0, 0].imshow(original_image)
        ax[0, 0].set_title('Original image', fontsize=fontsize)
        
        ax[1, 0].imshow(original_mask)
        ax[1, 0].set_title('Original mask', fontsize=fontsize)
        
        ax[0, 1].imshow(image)
        ax[0, 1].set_title('Transformed image', fontsize=fontsize)
        
        ax[1, 1].imshow(mask)
        ax[1, 1].set_title('Transformed mask', fontsize=fontsize)
        

image = cv2.imread('0fea4b5049_image.png')
mask = cv2.imread('0fea4b5049.png', 0)

aug = A.Compose([
            A.GridDistortion(always_apply=True),
        ])

result = aug(image=image, mask=mask)
visualize(result[""image""], result[""mask""])
```
![grid_distortion](https://user-images.githubusercontent.com/26741549/79748691-231b6480-830e-11ea-98ce-d95aa2078dc9.png)

",sorry intrude experienced problem able replicate problem code example salt challenge import import import visualize image mask none none ax ax image ax mask else ax ax ax image ax ax mask ax image ax image ax mask ax mask image mask result visualize result image result mask,issue,negative,positive,positive,positive,positive,positive
616487301,"Could you please share a minimal code example to reproduce?
",could please share minimal code example reproduce,issue,positive,negative,neutral,neutral,negative,negative
616486648,"You can modify parameters of `Resize` instance after you've created `Compose` to whatever you like:

`tfms.transforms[0].width = 256`",modify resize instance compose whatever like,issue,negative,neutral,neutral,neutral,neutral,neutral
616118306,I install this on IBM pp64cle which maybe the reason. Thanks,install maybe reason thanks,issue,negative,positive,positive,positive,positive,positive
615848996,"@Bryce1010

Please post more information about your environment (OS, Python version, package list, etc.), because it may be due to dependency interference.",please post information environment o python version package list may due dependency interference,issue,negative,negative,negative,negative,negative,negative
613762236,"I think the difference is that transform.ColorJitter random scale the contrast/brightness/saturation/hue value, while HueSaturationValue random shift. In other words, multiply vs add.
It would be nice to have an equivalent implementation.",think difference random scale value random shift multiply add would nice equivalent implementation,issue,positive,negative,neutral,neutral,negative,negative
612839054,"Well, in many cases we will work only with points. But what about `Scale` transform? I think than when we scale points, we need scale image as well. We need an interface for image processing or some mechanism to get the applied transform params.",well many work scale transform think scale need scale image well need interface image mechanism get applied transform,issue,negative,positive,positive,positive,positive,positive
612598491,"Hi @Dipet,
I've made TemplateTransform non-serializable similar to Lambda. I had to rename 'lambda_transforms' argument in 'from_dict' method and marked it as deprecated. ",hi made similar lambda rename argument method marked,issue,negative,positive,neutral,neutral,positive,positive
612527170,"@Dipet, Yes, it is working only on point clouds and gives a basic interface to it.
Images and point clouds have a different nature, one data is sparse, another is dense. Connecting them is not trivial. 

There two types of data I saw:
1. For 3d object detection datasets we usually have lidar and images from different points in space. While it might be easy to apply some transformations like scale or rotation to both of them, applying something like elastic transformation or populating point cloud with ground truth might not be that easy to combine. Difficulties in fusion are partly the reason why most of the methods so far are working only with lidar data.
2. Segmentation or classification datasets add color information as one of the features of a point. And if dropout applied on the point level it would also affect color information. Which requires the whole package to be able to work with variable size images. And adding an image interface wouldn't do anything for these types of datasets.

My idea was to provide the same interface as albumentations, but for working with point clouds. Not all of the tasks require working with images and point clouds together. And most of the methods are working only with point clouds.

What might be a compromise is adding an interface for extrinsic/intrinsic camera parameters later. It will ensure that camera positions are moved correctly for the task of 3d object detection.",yes working point basic interface point different nature one data sparse another dense trivial two data saw object detection usually different space might easy apply like scale rotation something like elastic transformation point cloud ground truth might easy combine fusion partly reason far working data segmentation classification add color information one point dropout applied point level would also affect color information whole package able work variable size image interface would anything idea provide interface working point require working point together working point might compromise interface camera later ensure camera correctly task object detection,issue,positive,positive,positive,positive,positive,positive
612440295,"> INTER_AREA interpolation should be the default

I agree that this should be the default. But in the current implementation, we can only change upscale behavior, with an argument we can change both of them.",interpolation default agree default current implementation change upscale behavior argument change,issue,negative,neutral,neutral,neutral,neutral,neutral
612436798,"Hi, thank you for your answer.

> I think in many task we need more artifacts on images, because we need to simulate bad image quality. 

I agree with that but I don't think the artifacts we can see here are relevant to the intended behavior of the function.

The goal of the Downscale data augmentation is to make the network robust images smaller than what you trained it with, and that will probably be upscaled before inference to meet the expected input size of the network. By using a downscaling interpolation different than `INTER_AREA`, we introduce artifacts that the network will **never** see on real data.

> I think we can add new argument, like intepolation_downscale and add info to documentation about this behavior

This would partially solve the issue, but IMO the `INTER_AREA` interpolation should be the default because there are not much use cases where you want your network to be robust to aliasing effects. Or even better, this should be the purpose of another augmentation of albumentation, but not `Downscale` as it is ambiguous. 
",hi thank answer think many task need need simulate bad image quality agree think see relevant intended behavior function goal data augmentation make network robust smaller trained probably inference meet input size network interpolation different introduce network never see real data think add new argument like add documentation behavior would partially solve issue interpolation default much use want network robust effect even better purpose another augmentation ambiguous,issue,positive,positive,positive,positive,positive,positive
612416547,"Thank you for PR.
Maybe you do not installed you version of albumentations?
You need modify this lines in README
```
| Transform                                                                                                                                                           | Image | Masks | BBoxes | Keypoints |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :---: | :----: | :-------: |
| [BBoxSafeRandomCrop](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.BBoxSafeRandomCrop)             | ✓     | ✓     | ✓      |           |
| [CenterCrop](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.CenterCrop)                             | ✓     | ✓     | ✓      | ✓         |
```",thank maybe version need modify transform image,issue,negative,neutral,neutral,neutral,neutral,neutral
612414851,"Thank you for PR.
Can you explain to me, this transforms works without images, only on point clouds?
As I see it, this transforms will work with images and point clouds from lidar. So we need there interface to work with images to.",thank explain work without point see work point need interface work,issue,negative,neutral,neutral,neutral,neutral,neutral
612409501,"Hi, thanks for PR and interesting investigation.

I am not agree with you here
> So we can see that now the images are of way better quality and better recreates what actual image resizing might look like, which is the main goal of the data transformation.

I think in many task we need more artifacts on images, because we need to simulate bad image quality.
So I think is better to get possibility to set different interpolation methods for `downscaled` image.
I think we can add new argument, like `intepolation_downscale` and add info to documentation about this behavior",hi thanks interesting investigation agree see way better quality better actual image might look like main goal data transformation think many task need need simulate bad image quality think better get possibility set different interpolation image think add new argument like add documentation behavior,issue,positive,positive,positive,positive,positive,positive
611997618,"Hi all,
I would like to take up this enhancement. 
I propose to set fill_value parameter 'random' string for example in order to generate it randomly. It's gonna to affect following transforms:
- PadIfNeeded
- ShiftScaleRotate
- OpticalDistortion
- GridDistortion
- ElasticTransform
- Cutout
- CoarseDropout
- ChannelDropout
- MaskDropout
- GridDropout
Probably I missed something.. 

@albu, @ternaus, @kirillbobyrev, @BloodAxe, @tatigabru,
What do you think?",hi would like take enhancement propose set parameter string example order generate randomly gon na affect following cutout probably something think,issue,negative,negative,negative,negative,negative,negative
611955833,"Hi @Dipet, @BloodAxe,
I've rebased branch to current master. Now all checks have passed. ",hi branch current master,issue,negative,neutral,neutral,neutral,neutral,neutral
611908090,"The suggestion did not give any accuracy increase where as the below gave good increase in score.
transforms.ColorJitter(0.3, 0.3, 0.3)",suggestion give accuracy increase gave good increase score,issue,positive,positive,positive,positive,positive,positive
611874203,"You can use the combination of `RandomBrightnessContrast` and `HueSaturationValue` to get the behavior of the CollorJitter from torchvision.

Ex:
```
import albumentations as albu

color_jitter_transform = albu.Compose([
   albu.RandomBrightnessContrast(),
   albu.HueSaturationValue(),
], p=p)
```

",use combination get behavior ex import,issue,negative,neutral,neutral,neutral,neutral,neutral
611005057,"@albu That link is now broken, how can I achieve multi-cahnnel mask augmentation?",link broken achieve mask augmentation,issue,negative,negative,negative,negative,negative,negative
609713369,"- rebased master (which seems to have brought some deepsource errors in core?) 
- aligned gaussian_blur with #551 as @BloodAxe suggested
- use default ksize from cv2 as @Dipet requested (ksize=0 when sigma is set)
- added unit-tests for GaussianBlur",master brought core use default sigma set added,issue,negative,neutral,neutral,neutral,neutral,neutral
607576870,"Definition of rotating transforms use [F.rotate](https://github.com/albumentations-team/albumentations/blob/4f35cda5adf73ab8ceacbba827400a1348cfb77f/albumentations/augmentations/functional.py#L187) internally. Image corners are being cropped since this function uses opencv which crop images by default. 

I think having an option to expand images instead of crop would be nice. Like `expand` argument in [torchvision.RandomRotation](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomRotation)

[This stackoverflow answer](https://stackoverflow.com/a/47248339/8503884) might be helpful implementing this.",definition rotating use internally image since function crop default think option expand instead crop would nice like expand argument answer might helpful,issue,positive,positive,positive,positive,positive,positive
605230108,"this aug should be Compose-ish like, not ImageOnlyTransformation. should be better move this in different PR",like better move different,issue,positive,positive,positive,positive,positive,positive
605177294,"Hi @Dipet,
Ok, I see. So you propose to get list of images as input? 
And are you ok with template_transform logic? Or we can assume that some augmentation could be done on DataLoader side.",hi see propose get list input logic assume augmentation could done side,issue,negative,neutral,neutral,neutral,neutral,neutral
605026085,Also I think better get templates as images. With this approach we can get templates from some dataloader class.,also think better get approach get class,issue,negative,positive,positive,positive,positive,positive
605024000,"Thank you for PR.
I think better, to mark this transformm as not serializable without custom serialization function. Look to `Lambda` for example.",thank think better mark without custom serialization function look lambda example,issue,positive,positive,positive,positive,positive,positive
604821425,neither. `always_apply` forces a transform to be applied even in case when top-level containers are not applied. for example if you have `Compose` with probability `p=0.9` and underlying `Resize` (or `ToTensorV2`) which you want to apply all the time - you have to pass `always_apply=True` to this `Resize` (`ToTensorV2` already has `always_apply=True`),neither transform applied even case applied example compose probability underlying resize want apply time pas resize already,issue,negative,neutral,neutral,neutral,neutral,neutral
604371165,A warning has also been added into `shift_hsv`. So this issue is outdated.,warning also added issue outdated,issue,negative,negative,negative,negative,negative,negative
604369269,"Hmm, at this time I am not sure, that we need `AdditiveNoise`, bacause at this time the same way works `GaussNoise`",time sure need time way work,issue,negative,positive,positive,positive,positive,positive
604349377,"Hi @Dipet !
I would like to take up this enhancement.",hi would like take enhancement,issue,negative,neutral,neutral,neutral,neutral,neutral
603436740,"I've chosen second option and made TemplateTransform accept filenames as inputs. 
In order to properly test it I introduced ImreadMock. ",chosen second option made accept order properly test,issue,negative,neutral,neutral,neutral,neutral,neutral
603174215,"Hi @Dipet, @BloodAxe,
I've updated code according to your comments, but 'DeepSource: Python' check is failed and probably not in my code.",hi code according python check probably code,issue,negative,neutral,neutral,neutral,neutral,neutral
602917531,"I missed serialization test and have to fix serialization of templates (numpy arrays to yaml). I see two options here:
1. Add custom deserialization of templates (preserving dtype)
2. Change interface for template transform and require filenames of template files.

I prefer first option, since second interface is less handy and will make testing more complicated (at least requires to store test data in repo). 
First option has disadvantage: storing of lots of templates in plaintext could increase serialization file size significantly. 
",serialization test fix serialization see two add custom change interface template transform require template prefer first option since second interface le handy make testing complicated least store test data first option disadvantage lot could increase serialization file size significantly,issue,negative,positive,neutral,neutral,positive,positive
602305938,@creafz Thanks! I'll install the latest version.,thanks install latest version,issue,negative,positive,positive,positive,positive,positive
602238491,@ni4muraano Fixed in https://github.com/albumentations-team/albumentations/commit/7658a91eeffe29ccb50403370bcadccb7e45fc0e. For now you need to install the latest version of the library from the master branch to get the fixed version.,fixed need install latest version library master branch get fixed version,issue,negative,positive,positive,positive,positive,positive
601902027,"Hi @Dipet, @BloodAxe 
Sorry for late response. I've resolved all your comments. ",hi sorry late response resolved,issue,negative,negative,negative,negative,negative,negative
600664229,breaking changes are not allowed at all. the only possible solution is to add parameter to select another behaviour,breaking possible solution add parameter select another behaviour,issue,negative,neutral,neutral,neutral,neutral,neutral
600656977,Thanks for your PR. Balanced upscale and downscale is important improvement. We may need some time to review your PR and end up with a solution that fixes this imbalance while keeping backward compatibility. ,thanks balanced upscale important improvement may need time review end solution imbalance keeping backward compatibility,issue,positive,positive,positive,positive,positive,positive
600206622,We do not have time and the opportunity to run full training cycle. Please give **MINIMAL** example to reproduce problem.,time opportunity run full training cycle please give minimal example reproduce problem,issue,negative,positive,positive,positive,positive,positive
600198702,"@Dipet Thanks for the reply. It is actually a [repo](https://github.com/sneddy/pneumothorax-segmentation). Here is the code:

Dataset:
```
class PneumothoraxDataset(Dataset):
    def __init__(self, data_folder, mode, transform=None,
                 fold_index=None, folds_distr_path=None):
        
        self.transform = transform
        self.mode = mode
        
        # change to your path
        self.train_image_path = '{}/train/'.format(data_folder)
        self.train_mask_path = '{}/mask/'.format(data_folder)
        self.test_image_path = '{}/test/'.format(data_folder)
        
        self.fold_index = None
        self.folds_distr_path = folds_distr_path
        self.set_mode(mode, fold_index)
        self.to_tensor = ToTensor()

    def set_mode(self, mode, fold_index):
        self.mode = mode
        self.fold_index = fold_index

        if self.mode == 'train':
            folds = pd.read_csv(self.folds_distr_path)
            folds.fold = folds.fold.astype(str)
            folds = folds[folds.fold != fold_index]
            
            self.train_list = folds.fname.values.tolist()
            self.exist_labels = folds.exist_labels.values.tolist()
            
            z= list(zip(self.train_list, self.exist_labels))
            random.shuffle(z)
            
            self.train_list[:], self.exist_labels[:]= zip(*z)

            self.num_data = len(self.train_list)

        elif self.mode == 'val':
            folds = pd.read_csv(self.folds_distr_path)
            folds.fold = folds.fold.astype(str)
            folds = folds[folds.fold == fold_index]
            
            self.val_list = folds.fname.values.tolist()
            self.num_data = len(self.val_list)

        elif self.mode == 'test':
            self.test_list = sorted(os.listdir(self.test_image_path))
            self.num_data = len(self.test_list)

    def __getitem__(self, index):
        if self.fold_index is None and self.mode != 'test':
            print('WRONG!!!!!!! fold index is NONE!!!!!!!!!!!!!!!!!')
            return
        
        if self.mode == 'test':
            image = cv2.imread(os.path.join(self.test_image_path, self.test_list[index]), 1)
            if self.transform:
                sample = {""image"": image}
                sample = self.transform(**sample)
                sample = self.to_tensor(**sample)
                image = sample['image']
            image_id = self.test_list[index].replace('.png', '')
            return image_id, image
        
        elif self.mode == 'train':
            filename= os.path.join(self.train_image_path, self.train_list[index])
            print('About to be read from folder: ', filename)
            image = cv2.imread(os.path.join(self.train_image_path, self.train_list[index]), 1)
#             print('Successfully read: ', image.shape)
            if self.exist_labels[index] == 0:
                label = np.zeros((1024, 1024))
            else:
                label = cv2.imread(os.path.join(self.train_mask_path, self.train_list[index]), 0)           

        elif self.mode == 'val':
            image = cv2.imread(os.path.join(self.train_image_path, self.val_list[index]), 1)
            label = cv2.imread(os.path.join(self.train_mask_path, self.val_list[index]), 0)

        if self.transform:
            sample = {""image"": image, ""mask"": label}
            sample = self.transform(**sample)
            sample = self.to_tensor(**sample)
            image, label = sample['image'], sample['mask']
        
        return image, label
    
    def __len__(self):
        return self.num_data
```

Code:
```
train_dataset = PneumothoraxDataset(
            data_folder=dataset_folder, mode='train', 
            transform=train_transform, fold_index=fold_id,
            folds_distr_path=folds_distr_path,
        )
train_config = load_yaml(config_folder)
train_transform = albu.load(train_config['TRAIN_TRANSFORMS']) 
iterations= len(train_dataloader)
dataloader_iterator = iter(train_dataloader)
print('Iterations: ', iterations)
for i in range(iterations):
    data, target = next(dataloader_iterator)
```

Here are the transforms that are being loaded from the YAML file:

```
{
    ""__version__"":""0.3.1"",
    ""transform"":{
        ""__class_fullname__"":""albumentations.core.composition.Compose"",
        ""p"":1.0,
        ""transforms"":[
            {
                ""__class_fullname__"":""albumentations.augmentations.transforms.HorizontalFlip"",
                ""always_apply"":false,
                ""p"":0.5
            },
            {
                ""__class_fullname__"":""albumentations.core.composition.OneOf"",
                ""p"":0.3,
                ""transforms"":[
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.RandomContrast"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""limit"":[
                            -0.2,
                            0.2
                        ]
                    },
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.RandomGamma"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""gamma_limit"":[
                            80,
                            120
                        ]
                    },
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.RandomBrightness"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""limit"":[
                            -0.2,
                            0.2
                        ]
                    }
                ]
            },
            {
                ""__class_fullname__"":""albumentations.core.composition.OneOf"",
                ""p"":0.3,
                ""transforms"":[
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.ElasticTransform"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""alpha"":120,
                        ""sigma"":6.0,
                        ""alpha_affine"":3.5999999999999996,
                        ""interpolation"":1,
                        ""border_mode"":4,
                        ""value"":null,
                        ""mask_value"":null,
                        ""approximate"":false
                    },
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.GridDistortion"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""num_steps"":5,
                        ""distort_limit"":[
                            -0.3,
                            0.3
                        ],
                        ""interpolation"":1,
                        ""border_mode"":4,
                        ""value"":null,
                        ""mask_value"":null
                    },
                    {
                        ""__class_fullname__"":""albumentations.augmentations.transforms.OpticalDistortion"",
                        ""always_apply"":false,
                        ""p"":0.5,
                        ""distort_limit"":[
                            -2,
                            2
                        ],
                        ""shift_limit"":[
                            -0.5,
                            0.5
                        ],
                        ""interpolation"":1,
                        ""border_mode"":4,
                        ""value"":null,
                        ""mask_value"":null
                    }
                ]
            },
            {
                ""__class_fullname__"":""albumentations.augmentations.transforms.ShiftScaleRotate"",
                ""always_apply"":false,
                ""p"":0.5,
                ""shift_limit"":[
                    -0.0625,
                    0.0625
                ],
                ""scale_limit"":[
                    -0.09999999999999998,
                    0.10000000000000009
                ],
                ""rotate_limit"":[
                    -45,
                    45
                ],
                ""interpolation"":1,
                ""border_mode"":4,
                ""value"":null,
                ""mask_value"":null
            },
            {
                ""__class_fullname__"":""albumentations.augmentations.transforms.Resize"",
                ""always_apply"":true,
                ""p"":1,
                ""height"":1024,
                ""width"":1024,
                ""interpolation"":1
            }
        ],
        ""bbox_params"":{

        },
        ""keypoint_params"":{

        },
        ""additional_targets"":{

        }
    }
}
```
I hope this helps.",thanks reply actually code class self mode transform mode change path none mode self mode mode list zip zip sorted self index none print fold index none return image index sample image image sample sample sample sample image sample index return image index print read folder image index print read index label else label index image index label index sample image image mask label sample sample sample sample image label sample sample return image label self return code iter print range data target next loaded file transform false false limit false false limit false alpha sigma interpolation value null null approximate false false interpolation value null null false interpolation value null null false interpolation value null null true height width interpolation hope,issue,positive,negative,negative,negative,negative,negative
600181281,Without reproducible example we are can not help you.,without reproducible example help,issue,negative,neutral,neutral,neutral,neutral,neutral
600180022,"@imazerty Have you solved this particular issue? I have been getting the same issue in the middle of training. My images are stored locally and this happens only for a few images. This happens in-between during the training. I have tried printing the path and tried to read the image from a separate code and it works.

@BloodAxe Any solutions?",particular issue getting issue middle training locally training tried printing path tried read image separate code work,issue,negative,positive,neutral,neutral,positive,positive
599569480,"Thank you, the issue was resolved.

I had installed the package via conda as mentioned in README
```
conda install -c conda-forge imgaug
conda install albumentations -c albumentations
```
It installed the version 0.1.12 by default",thank issue resolved package via install install version default,issue,negative,neutral,neutral,neutral,neutral,neutral
599453817,"Maybe it some points it will be added. However as of now it's on low-priority list.
As usual, we are more than happy to see pull-requests that adds different benchmark strategies. 
",maybe added however list usual happy see different,issue,positive,positive,positive,positive,positive,positive
599423515,"Please, use the suggested issue template. By explaining the issue in a structured way, you are helping maintainers to understand and address your issue faster. 

More specifically, there is parameter to `A.Compose` that controls how keypoints are removed during augmentations. It is not clear what parameter value you are using, therefore as it is now, the issue cannot be reproduced. ",please use issue template explaining issue structured way helping understand address issue faster specifically parameter removed clear parameter value therefore issue,issue,positive,positive,positive,positive,positive,positive
599413058,"Please do not use screenshots for code, copy past it into issue as text and wrap it into code specifiers.",please use code copy past issue text wrap code,issue,negative,negative,negative,negative,negative,negative
599408337,"Update albumentations to the latest version: `pip install --upgrade albumentations`
Also you can install latest version from git: `pip install git+https://github.com/albumentations-team/albumentations.git`",update latest version pip install upgrade also install latest version git pip install,issue,negative,positive,positive,positive,positive,positive
599405665,"Torchvision works with Pillow images, we are use numpy arrays. So you can try to wrap transforms like this:
```
transfroms = lambda x: data_transforms(image=np.array(x))['image']
```",work pillow use try wrap like lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
599176776,Can I use this in augment and show function? Could you please explain more..,use augment show function could please explain,issue,negative,neutral,neutral,neutral,neutral,neutral
599040201,"> pls take a look at the docs. only numpy arrays are supported and you have to pass data into augmentations as named arguments. https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/showcase.ipynb

I passed arguments as keyword arguments but still i am getting this error.
`import numpy as np
import albumentations as A
import eva4datatransforms
from torchvision import transforms
channel_means = (0.49139968, 0.48215841, 0.44653091)
channel_stdevs = (0.24703223, 0.24348513, 0.26158784)
trans = eva4datatransforms.Transforms(normalize=True, mean=channel_means, stdev=channel_stdevs)
fillmeans = (np.array(channel_means)*255).astype(np.uint8)
# Train Phase transformations
train_transforms = trans.train_transforms([
                                    
                                       A.augmentations.transforms.Rotate(limit=(-15.0,15.0), interpolation=1,
                                        border_mode=4, value=list(fillmeans), mask_value=None, always_apply=False, p=0.5),
                                       #transforms.RandomHorizontalFlip(),
                                       A.augmentations.transforms.HorizontalFlip(always_apply=False, p=0.5),
                                       ], 
                                       
                                       [#transforms.RandomErasing(value = tuple(channel_means)),
                                        A.augmentations.transforms.Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=channel_means, always_apply=False, p=0.5)
                                        ])

# Test Phase transformations
test_transforms = trans.test_transforms()`

Error occured while writing this code
`%matplotlib inline
import matplotlib.pyplot as plt
dataiter=iter(train_loader)
images,labels=next(dataiter)` 

",take look pas data still getting error import import import import train phase value test phase error writing code import,issue,negative,neutral,neutral,neutral,neutral,neutral
599038435,"Have you started supporting integer datatype in albumentation 0.4.5. I am facing the issue while working on CIFAR 10 dataset.

data_transforms = albumentations.Compose([
                  # albumentations.Resize(160, 320),
                  albumentations.HorizontalFlip(),
                  albumentations.RandomBrightness(),
                  albumentations.ShiftScaleRotate(rotate_limit=15, scale_limit=0.10),
                  albumentations.JpegCompression(80),
                  albumentations.HueSaturationValue(),
                  albumentations.Normalize(),
                  ToTensor()
                  ])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=data_transforms)

SEED = 1
cuda = torch.cuda.is_available()
print(""CUDA Available?"", cuda)
torch.manual_seed(SEED)

if cuda:
    torch.cuda.manual_seed(SEED)
dataloader_args = dict(shuffle=True, batch_size=128, num_workers=4, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64

### train dataloader
train_loader = torch.utils.data.DataLoader(trainset, **dataloader_args)

classes = ('plane', 'car', 'bird', 'cat',
           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

### functions to show an image

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))

### get some random training images
dataiter = iter(train_loader)
images, labels = dataiter.next()

### show images
imshow(torchvision.utils.make_grid(images))
### print labels
print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

-------------ERROR------------------
---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

<ipython-input-7-d147ed814b70> in <module>()
      7 # get some random training images
      8 dataiter = iter(train_loader)
----> 9 images, labels = dataiter.next()
     10 
     11 # show images

3 frames

/usr/local/lib/python3.6/dist-packages/torch/_utils.py in reraise(self)
    392             # (https://bugs.python.org/issue2651), so we work around it.
    393             msg = KeyErrorMessage(msg)
--> 394         raise self.exc_type(msg)

TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py"", line 178, in _worker_loop
    data = fetcher.fetch(index)
  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/usr/local/lib/python3.6/dist-packages/torchvision/datasets/cifar.py"", line 125, in __getitem__
    img = self.transform(img)
TypeError: __call__() takes 1 positional argument but 2 were given
",supporting integer facing issue working seed print available seed seed else train class show image get random training iter show print print class range recent call last module get random training iter show reraise self work around raise caught worker process original recent call last file line data index file line fetch data file line data file line positional argument given,issue,negative,positive,neutral,neutral,positive,positive
599037813,"Maybe it will be nice to check that `image` and `mask`, if it is exists, is `np.ndarray`?",maybe nice check image mask,issue,negative,positive,positive,positive,positive,positive
599035203,pls take a look at the docs. only numpy arrays are supported and you have to pass data into augmentations as named arguments. https://github.com/albumentations-team/albumentations_examples/blob/master/notebooks/showcase.ipynb,take look pas data,issue,negative,neutral,neutral,neutral,neutral,neutral
599033710,"> I’ve just been reading through the docs. I use torchvision, so I just didn’t realize that you have to specify the arguments to the transformer when you call it, like image=

Can you plz explain it elaborately.I ma also facing the same issue.Force_apply shou;ld be boolean but it is getting PIL image",reading use realize specify transformer call like explain also facing shou getting image,issue,negative,neutral,neutral,neutral,neutral,neutral
598088671,"That's great point! 
Moreover looks like it would be better to allow setting list of desired images as template and pick the random one. ",great point moreover like would better allow setting list desired template pick random one,issue,positive,positive,positive,positive,positive,positive
597588763,"My usecase was essentially using albumentations as a convenience function library for some image manipulations for a thin line segmentation problem. The 1px cropping was intended to randomly offset one of the input channels, the channel offset+scale problem this would introduce was intended to simulate chromatic aberration. I wouldn't say it is a big problem as I can simply use a custom function, but it was certainly unexpected.",essentially convenience function library image thin line segmentation problem intended randomly offset one input channel problem would introduce intended simulate chromatic aberration would say big problem simply use custom function certainly unexpected,issue,negative,negative,negative,negative,negative,negative
597558718,"In many computer vision functions uses rounding to smallest, not to closest. So many transform works with 1 pixel accuracy.
I am not sure that we need to fix this behavior.",many computer vision rounding many transform work accuracy sure need fix behavior,issue,positive,positive,positive,positive,positive,positive
597556892,"Question for curiosity. In your case, does this 1 pixel really matter?",question curiosity case really matter,issue,negative,positive,positive,positive,positive,positive
597256730,"Once https://github.com/albumentations-team/albumentations/pull/551 is merged, please update this PR to use new `gaussian_blur` with `sigma_x` and `sigma_y` parameter from there.",please update use new parameter,issue,negative,positive,positive,positive,positive,positive
596979124,"Yes you can create new PR.

Closed: Empty PR.",yes create new closed empty,issue,negative,negative,neutral,neutral,negative,negative
596954897,It seems that in this transformation we can use any image templates..  I think it's best to name this transformation as a `TemplateTranform`. And give the opportunity to set any image as a template.,transformation use image think best name transformation give opportunity set image template,issue,positive,positive,positive,positive,positive,positive
596951723,"Hi @ternaus,

Frost transform requires precalculated images with ice crystals. 
What's the best way of storing this images in albumentations code (all images require ~2MB)?
",hi frost transform ice best way code require,issue,positive,positive,positive,positive,positive,positive
596436348,pip get scikit-image when resolves imgaug dependencies. I think need to include scikit-image into albumentations dependencies directly.,pip get think need include directly,issue,negative,positive,neutral,neutral,positive,positive
596390691,"Thanks a lot for the package release, especially for conda!

When I install the package with ""conda install -c albumentations albumentations=0.4.5"" and then run ""from albumentations import Compose"", I got an error:

File ""/home/x/miniconda3/envs/x/lib/python3.6/site-packages/albumentations/__init__.py"", line 8, in <module>
    from .augmentations.transforms import *
  File ""/home/x/miniconda3/envs/x/lib/python3.6/site-packages/albumentations/augmentations/transforms.py"", line 8, in <module>
    from skimage.measure import label
ModuleNotFoundError: No module named 'skimage'

Is ""scikit-image"" package missing in the conda package dependencies? Should it be added to [here](https://github.com/albumentations-team/albumentations/blob/master/conda.recipe/meta.yaml)?

By the way, why does [imgaug](https://anaconda.org/conda-forge/imgaug) not present in [conda dependencies](https://github.com/albumentations-team/albumentations/blob/master/conda.recipe/meta.yaml)?",thanks lot package release especially install package install run import compose got error file line module import file line module import label module package missing package added way present,issue,negative,neutral,neutral,neutral,neutral,neutral
596244548,"
[sample_patch.zip](https://github.com/albumentations-team/albumentations/files/4303789/sample_patch.zip)

Here is a sample patch, its a compressed npz, feel free to refer to m code and follow through me code to extract stuff.

Thanks for your help,
Siddhesh",sample patch compressed feel free refer code follow code extract stuff thanks help,issue,positive,positive,positive,positive,positive,positive
596240661,"Can you experiment with my code and your data using different seeds and `p`?
Would be great if you can give to us 1 image with it is mask to reproduce problem using this code.",experiment code data different would great give u image mask reproduce problem code,issue,negative,positive,positive,positive,positive,positive
596240385,"With `p=0.5` all also is good. `p` does not used directly in transforms, this parameter works equal for all transforms inherited from `BasicTransform`.",also good used directly parameter work equal,issue,negative,positive,positive,positive,positive,positive
596226455,"Hey @Dipet  You are close.
can you just change the p=1 to p=0.5 and give it a try.
When you do p=1, it forces the probability to be 1 which in turn forces augmentation.

Let me know if you have trouble.",hey close change give try probability turn augmentation let know trouble,issue,negative,negative,negative,negative,negative,negative
596174757,"> The initial assumption is incorrect.

What assumption, could you elaborate please? 

Say I want a random downscale or upscale within 2x of the image size. With the current logic:
- if `scale_limit = 1`. Then `scale` is a value within `[0, 2]`. And this is not what a user would expect. Will it work for values > 1? (what if I want augment within 3x of the image size?
- if `scale_limit = (-0.5, 1)` then `scale` is drawn as uniform `[-0.5, 1]` and result is a uniform `[0.5, 2]`. Which has a 2/3 chance to upscale and 1/3 to downscale. Which is not what user would expect.

Solution for a single `scale_limit` might be following:
```
scale = uniform(1, scale_limit) # upscale by default
if uniform(0, 1) < 0.5: # downscale 50%
  scale = 1 / scale
return scale
```

For two values we might need to sample 2 times - from upscale interval and from downscale interval. And then draw another binary value to choose whether to upscale or downscale. 

",initial assumption incorrect assumption could elaborate please say want random upscale within image size current logic scale value within user would expect work want augment within image size scale drawn uniform result uniform chance upscale user would expect solution single might following scale uniform upscale default uniform scale scale return scale two might need sample time upscale interval interval draw another binary value choose whether upscale,issue,positive,negative,neutral,neutral,negative,negative
596141271,"I am can not reproduce problem or I misunderstand the problem.
Before:
![image](https://user-images.githubusercontent.com/7512250/76153011-37493080-60d7-11ea-8d18-eeb2b750a1c3.png)
After:
![image](https://user-images.githubusercontent.com/7512250/76152896-09afb780-60d6-11ea-9a8e-9d4bcec4ed4e.png)

```
import numpy as np
import cv2 as cv
import json
from albumentations import *
import matplotlib.pyplot as plt
from pathlib import Path
import random

random.seed(0)
np.random.seed(0)


def polygon_to_mask(poly, image_shape):
    mask = np.zeros(image_shape[:2], dtype=np.uint8)
    poly = np.array(poly).reshape((1, -1, 2)).astype(np.int32)
    cv.drawContours(mask, poly, 0, 1, -1)
    return mask


def get_image(image_id, data, images_dir):
    image_data = None
    for sub_data in data['images']:
        if sub_data['id'] == image_id:
            image_data = sub_data
            break

    file_path = Path(images_dir).joinpath(image_data['file_name'])
    image = cv.imread(str(file_path), cv.IMREAD_COLOR)
    image = cv.cvtColor(image, cv.COLOR_BGR2RGB)
    return image


def get_masks(image_id, anno, image_shape):
    masks = []
    for item in anno['annotations']:
        if item['image_id'] == image_id:
            segm = item['segmentation']
            if not isinstance(segm, list):
                continue
            masks.append(polygon_to_mask(segm, image_shape))
    return np.stack(masks, axis=-1)


def show_image_masks(image, masks):
    plt.subplot(231)
    plt.imshow(image)

    for i in range(5):
        mask = masks[..., i]
        mask *= 255
        plt.subplot(2, 3, 2 + i)
        plt.imshow(mask, cmap='gray')
    plt.show()

with open(r'D:\coco\annotations\instances_val2017.json', 'r') as file:
    coco = json.load(file)

image_id = 494869
images_dir = Path(r""D:\coco\val2017"")
image = get_image(image_id, coco, images_dir)
masks = get_masks(image_id, coco, image.shape)

show_image_masks(image, masks)

augs = Compose([
    RandomBrightnessContrast(p=1),
    HueSaturationValue(p=1),
    RandomGamma(p=1),
    GaussNoise(p=1),
    GaussianBlur(p=1),
    HorizontalFlip(p=1),
    VerticalFlip(p=1),
    ])
res = augs(image=image, mask=masks)
show_image_masks(res['image'], res['mask'])
```",reproduce problem misunderstand problem image image import import import import import import path import random poly mask poly poly mask poly return mask data none data break path image image image return image item item item list continue return image image range mask mask mask open file coco file path image coco coco image compose,issue,negative,negative,negative,negative,negative,negative
595909036,"I shall post pictures
![Figure_1](https://user-images.githubusercontent.com/20500704/76113008-55e0f600-6009-11ea-87b5-ae8459700c77.png)
![Figure_2](https://user-images.githubusercontent.com/20500704/76113011-57122300-6009-11ea-8421-1262e047eb45.png)

In the first one, I have turned off the horizontalflip and the verticalflip, but in the second one, I have turned it on and you can see the results, the masks dont correspond to the cells.

`
        class TumorSegDataset(Dataset):
            def __init__(self, csv_file, params, valid=False):
            self.df = pd.read_csv(csv_file, header=0)
            self.params = params
            self.valid = valid
            self.aug = Compose([
                            RandomBrightnessContrast(),
                            HueSaturationValue(),
                            RandomGamma(),
                            GaussNoise(),
                            GaussianBlur(),
                            HorizontalFlip(),
                            VerticalFlip()
                            ])

    def __len__(self):
        return len(self.df)

    def __getitem__(self, patient_id):
        image_path = os.path.join(self.df.iloc[patient_id, 0])
        z = np.load(image_path)
        image = z['patch']
        gt_data = z['patch_gt']
        # print(""Pre : "", image.shape, gt_data.shape)
        gt_data = gt_data.swapaxes(0, 2)
        # print(""Pre swapped: "", image.shape, gt_data.shape)
        # gt_data = gt_data[:4, :, :]
        if not self.valid:
            augmented = self.aug(image=image, mask=gt_data)
            image = augmented['image']
            gt_data = augmented['mask']
        image = (image/255).astype(np.float32)
        # print(""Post Augment :"", image.shape, gt_data.shape)
        image = image.swapaxes(0, 2)
        gt_data = gt_data.swapaxes(0, 2)
        # print(""Post Augment Swapped: "", image.shape, gt_data.shape)
        image = torch.FloatTensor(image)
        gt_data = torch.FloatTensor(gt_data)

        return image, gt_data`

Sorry, but i dont know how to post code properly",shall post first one turned second one turned see dont correspond class self valid compose self return self image print print augmented image augmented augmented image print post augment image print post augment image image return image sorry dont know post code properly,issue,negative,negative,neutral,neutral,negative,negative
595831341,"As @BloodAxe said, please, give a minimal example to reproduce the problem.",said please give minimal example reproduce problem,issue,negative,negative,neutral,neutral,negative,negative
595800439,"Mask and Image dimensions have been mentioned as :

> Image shape : 256, 256, 3
> Mask Shape : 5, 256, 256

I have previously tried HWC instead of CHW but for some reasons the channels get swapped when using HWC in masks.",mask image image shape mask shape previously tried instead get,issue,negative,negative,negative,negative,negative,negative
595762326,"Masks format must be `HW` or `HWC`, but you use `CHW` format.
Do not use `swapaxes` before augmentations.",format must use format use,issue,negative,neutral,neutral,neutral,neutral,neutral
595762128,"Please provide a minimal reproducible case. From provided code snippet it is unclear what shape `image` and `gt_data` objects have. Since flip operation is working identically to image and mask targets, I don't see any reason why it would provide this behavior. Please, double check the shape of your input data.",please provide minimal reproducible case provided code snippet unclear shape image since flip operation working identically image mask see reason would provide behavior please double check shape input data,issue,negative,negative,neutral,neutral,negative,negative
595688230,"The initial assumption is incorrect.
Look how work `to_tuple` [method](https://github.com/albumentations-team/albumentations/blob/bd2c655bc753a72cb48b49eceeabc8dbbc22b9d8/albumentations/core/transforms_interface.py#L16): 
Scale defined as: `scale = uniform([-scale_limit, scale_limit]) + bias`, if `scale_limit` is single value.
If `scale_limit` is tuple, then `scale = uniform([scale_limit[0], scale_limit[1]]) + bias`",initial assumption incorrect look work method scale defined scale uniform bias single value scale uniform bias,issue,negative,negative,neutral,neutral,negative,negative
593089399,"Should I send another pr? Btw, if I remember correctly, I chose readme branch

Sent from ProtonMail mobile

-------- Original Message --------
On Mar 1, 2020, 4:57 PM, Alexander Buslaev wrote:

> Probably you specified wrong branch for this pr
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, [view it on GitHub](https://github.com/albumentations-team/albumentations/pull/544?email_source=notifications&email_token=ALTFRUIQ63B73VZ27UFV6LDRFJESJA5CNFSM4K3CRFQ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENM5BVQ#issuecomment-593088726), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ALTFRUMGQGDKGP3CATCA5QTRFJESJANCNFSM4K3CRFQQ).",send another remember correctly chose branch sent mobile original message mar wrote probably wrong branch thread reply directly view,issue,negative,negative,neutral,neutral,negative,negative
593087569,"Hm. If that's the case, my bad. I wanted to add a company logo to readme. Since Promobot uses albumentations, I decided to send a pr to add its logo.

Sent from ProtonMail mobile

-------- Original Message --------
On Mar 1, 2020, 4:40 PM, Alexander Buslaev wrote:

> Looks like description is different from what you did. What did you want to do?
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, [view it on GitHub](https://github.com/albumentations-team/albumentations/pull/544?email_source=notifications&email_token=ALTFRUN4NP3GHDQFSJBZ5D3RFJCRDA5CNFSM4K3CRFQ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOENM4XDQ#issuecomment-593087374), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ALTFRUJWAU7QUZE4E6N4BQDRFJCRDANCNFSM4K3CRFQQ).",case bad add company since decided send add sent mobile original message mar wrote like description different want thread reply directly view,issue,negative,negative,neutral,neutral,negative,negative
593087374,Looks like description is different from what you did. What did you want to do?,like description different want,issue,negative,neutral,neutral,neutral,neutral,neutral
590797499,"In #511 you can see case when `range(0, height, y_step)` creates sequence with wrong number of elements, i.e. `len(list(range(0, height, y_step))) > num_steps`",see case range height sequence wrong number list range height,issue,negative,negative,negative,negative,negative,negative
590454871,"Sorry for inconvenience, merged accidentally. Could you please make your logo the same size as other logos because we don't want to highlight any particular company out of the list @alexander-rakhlin ",sorry inconvenience accidentally could please make size logo want highlight particular company list,issue,negative,negative,negative,negative,negative,negative
589629808,"yes, it's better to write at least some tests which ensures that augmentation does what expected.",yes better write least augmentation,issue,positive,positive,neutral,neutral,positive,positive
589614230,"Should i write tests, that includes every augmentation?(except integer only ones) ",write every augmentation except integer,issue,negative,neutral,neutral,neutral,neutral,neutral
589043088,"To fix black errors do `black --config=black.toml `
You mark transform like `ImageOnlyTransform` but you create docs for `DualTransform`
If Transform works with masks, change base class to `DualTransform`",fix black black mark transform like create transform work change base class,issue,positive,negative,negative,negative,negative,negative
588823688,"> Thank you for your PR. Our CI test catch some problems with code formatting. Please use pre-commit hook to avoid this problems, like described [there](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst) Or format code manually using this command `black --config=black.toml` and check format `flake8`

Have no problem with flake8, but black test didn't passed 
also, test for docs didn't pass either, but i add AugMix URL in Readme.md
how can i solve this?",thank test catch code please use hook avoid like format code manually command black check format flake problem flake black test also test pas either add solve,issue,negative,negative,negative,negative,negative,negative
588775446,"Thanks. @albu @Dipet 
I ignore that the `dtype` of my mask is `bool`, which results in the problem. Instead, after converting dtype to be as `np.uint8`, everything is OK. ",thanks ignore mask bool problem instead converting everything,issue,negative,positive,positive,positive,positive,positive
588765212,"Thank you for your PR. Our CI test catch some problems with code formatting. Please use pre-commit hook to avoid this problems, like described [there](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst) Or format code manually using this command `black --config=black.toml` and check format `flake8`",thank test catch code please use hook avoid like format code manually command black check format flake,issue,positive,negative,negative,negative,negative,negative
588738685,"I can not reproduce this problem with code below.
```
np.random.seed(0)

img = np.random.randint(0, 256, [1000, 1000, 3], dtype=np.uint8)
mask = np.random.randint(0, 2, [1000, 1000, 3], dtype=np.uint8)

res = augmentation(img, mask)
img = res['image']
mask = res['mask']

print(img.shape, mask.shape)
```

Can you get example of code to reproduce this problem?",reproduce problem code mask augmentation mask mask print get example code reproduce problem,issue,negative,neutral,neutral,neutral,neutral,neutral
588282273,"If we pass ksize = 0 we fall into this piece of code below right?
```
    // automatic detection of kernel size from sigma
    if( ksize.width <= 0 && sigma1 > 0 )
        ksize.width = cvRound(sigma1*(depth == CV_8U ? 3 : 4)*2 + 1)|1;
    if( ksize.height <= 0 && sigma2 > 0 )
        ksize.height = cvRound(sigma2*(depth == CV_8U ? 3 : 4)*2 + 1)|1;
```
and cvRound(sigma1*(depth == CV_8U ? 3 : 4)*2 + 1)|1 will be equal to 1 if sigma is below 0.25
e.g. round(0.2*1*2 + 1)|1 == 1
Any kernel of 1x1 is equal to [1.0] so it's the same as nothing (identity) so I'd like to limit the kernel size to at least 3x3
You can argue that sigma 0.25 is too small to make any difference anyway...
```
cv2.getGaussianKernel(ksize=3, sigma=0.25)
Out[3]: 
array([[3.35237708e-04],
       [9.99329525e-01],
       [3.35237708e-04]])
```
Then we could alternatively just limit the minimum sigma to 0.25, but I'd think we could leave that to the user to decide?
I'm not too fussy either way ",pas fall piece code right automatic detection kernel size sigma sigma sigma depth sigma sigma depth sigma depth equal sigma round kernel equal nothing identity like limit kernel size least argue sigma small make difference anyway array could alternatively limit minimum sigma think could leave user decide fussy either way,issue,negative,negative,neutral,neutral,negative,negative
588269718,"Bounding boxes don't work outside of compose class, please take a look at the docs and showcase on how to use augmentations on bounding boxes ",bounding work outside compose class please take look showcase use bounding,issue,negative,neutral,neutral,neutral,neutral,neutral
587096481,"> @selimarikan what format of images you have ?
> 
> Well, in the readme of the library it also says ""The library works with images in HWC format."" :)

I have greyscale images and I would say this is a common use case.
It is also good to keep in mind that TIFF images can have much more layers (>3) and some CNN-based detectors can make use of these multilayer TIFFs.

Best thing would be to handle this dynamically based on layer/channel count. ",format well library also library work format would say common use case also good keep mind tiff much make use best thing would handle dynamically based count,issue,positive,positive,positive,positive,positive,positive
586385333,"I have an additional example of the usage here - it really brings DALI usage inline with Albumentations/PyTorch Compose usage, so would be very cool to have integrated

Usability wise this code is a super handy way of dropping GPU acceleration as a nearly 'drop in' replacement to existing PyTorch/Albumentations workloads - we just need to find a home for it.

Take a look! Here is another Colab:

https://colab.research.google.com/drive/1XDRzDeIeteTPyzYbZp2XWjKtUWMLXqcg",additional example usage really dali usage compose usage would cool usability wise code super handy way dropping acceleration nearly replacement need find home take look another,issue,positive,positive,positive,positive,positive,positive
585382831,hi @jmerkow I don't really remember reasons why `ReplayCompose` is not serializable so you can try to fix it.,hi really remember try fix,issue,negative,positive,positive,positive,positive,positive
584900028,Also library works only with grayscale images in `HW` or RGB images in `HWC` format. So we have 2 or 3 dimensions,also library work format,issue,negative,neutral,neutral,neutral,neutral,neutral
584706834,"Had the same bug.

My code:
```
        RandomSunFlare(
            flare_roi=(0, 0, 1, 1), 
            src_radius=random.randint(10, 200),
            p=.25,
        )
```

Environment
- Albumentations version 0.4.4
- Python 3.5.3
- OS Debian GNU/Linux 9.11 (stretch)
- pip installation from repo (`pip install git+https://github.com/albu/albumentations`)",bug code environment version python o stretch pip installation pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
584581693,"@selimarikan what format of images you have ?

Well, in the readme of the library it also says ""The library works with images in HWC format."" :)",format well library also library work format,issue,negative,neutral,neutral,neutral,neutral,neutral
584113129,"I've read the code of #520, I think it's better to add my code on #520 as well
Will do it later.
I'm closing this PR.",read code think better add code well later,issue,positive,positive,positive,positive,positive,positive
582007945,"Oh, sorry. I updated some files in my repo and i had to cancel them) (I have actually done it)",oh sorry cancel actually done,issue,negative,negative,negative,negative,negative,negative
581927725,"I also found the cause of the bug.
The bug occurs not only when `max_h_size == 1` or `max_w_size == 1`, but also when they are odd.",also found cause bug bug also odd,issue,negative,negative,negative,negative,negative,negative
581853999,I had a similar problem some time ago. In my case OpenCv caused the segmentation fault. Downgrading OpenCV got rid of the segmentation fault ,similar problem time ago case segmentation fault got rid segmentation fault,issue,negative,neutral,neutral,neutral,neutral,neutral
581810625,"We have some interesting code to port into data augs libs like albumentations

https://github.com/pytorch/ignite/blob/f02a91a28153e08092dbb625472e0211f642923d/ignite/contrib/engines/dali.py#L27

and 

https://github.com/pytorch/ignite/pull/493#issuecomment-581778218

Maybe we can try to build something like that here:
- `ComposeOps`
- generalized `TransformPipeline`

Certainly it wont support all your augs, only DALI ones, but this could help user to avoid copying common stuff. 
 
@albu @ternaus what do you think ?

I can try to provide a draft PR if you are interested.",interesting code port data like maybe try build something like generalized certainly wont support dali could help user avoid common stuff think try provide draft interested,issue,positive,positive,positive,positive,positive,positive
579867224,"I think is better wait when we will merge #520, then @haqishen can add his improvements like different transform mods and rotations. In this thing I agree with @tatigabru",think better wait merge add like different transform thing agree,issue,positive,positive,positive,positive,positive,positive
579829395,"Hi @haqishen, yes, in #520 PR implemented the same things, but I added more user-friendly parameters. And I did not add rotation to avoid overloading it, as there are other augmentations that cover it (you can rotate the image and then apply grid mask, with the same effect)
If rotation is needed within this augmentation, maybe when I finish my PR you could add your rotation to it, for example? I do not know, what to do for the case of the double-PR
",hi yes added add rotation avoid cover rotate image apply grid mask effect rotation within augmentation maybe finish could add rotation example know case,issue,negative,neutral,neutral,neutral,neutral,neutral
579792819,Please look to #520 This PR implements the same things. I think you need to decide with author whats to do with this transformation.,please look think need decide author whats transformation,issue,negative,neutral,neutral,neutral,neutral,neutral
579770789,"Please add information about transform like described [here](https://github.com/albumentations-team/albumentations#contributing)
And reformat code using black, for this purposes you can use [pre-commit hook](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst#to-create-a-pull-request)",please add information transform like code black use hook,issue,positive,negative,negative,negative,negative,negative
577106466,"Library use `RGB` images in `HWC` format. I can not reproduce problem.
```
import albumentations as A
import numpy as np

t = A.CoarseDropout()
img_rgb = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
res = t(image=img_rgb)['image']
print(res.shape)
```",library use format reproduce problem import import print,issue,negative,neutral,neutral,neutral,neutral,neutral
577105975,"You can work with RGB images like this:
```
import albumentations as A
import numpy as np


t = A.CoarseDropout()
img_rgb = np.random.randint(0, 256, [100, 100, 3], dtype=np.uint8)
res = t(image=img_rgb)['image']
print(res.shape)
```",work like import import print,issue,negative,neutral,neutral,neutral,neutral,neutral
577105751,"Library uses images in standard format for OpenCV: `HWC`
So this changes is wrong.",library standard format wrong,issue,negative,negative,negative,negative,negative,negative
576669069,Please reformat code using `black` and `flake8`. You also can use [pre-commit](https://github.com/albumentations-team/albumentations/blob/master/docs/contributing.rst) hook for this purposes.,please code black flake also use hook,issue,negative,negative,negative,negative,negative,negative
576089484,"Where do you suggest I add the test? There doesn't seem to be a file for augmentation-specific issues. 
test_augmentations.py work?",suggest add test seem file work,issue,negative,neutral,neutral,neutral,neutral,neutral
574294340,@ZFTurbo could you please rebase on the current master?,could please rebase current master,issue,negative,neutral,neutral,neutral,neutral,neutral
574181191,"I had the same issue, also using GridDistortion. 

This config:
```opencv 4.1.2 python 3.6.8 albu 0.4.3```
solved it (only change in my case was downgrading Python).

So probably some weird interplay happening between Python/OpenCV?

EDIT:
Scratch that; still getting seg faults.
",issue also python change case python probably weird interplay happening edit scratch still getting seg,issue,negative,negative,negative,negative,negative,negative
573457628,"OK, I misunderstood what problem you were talking about.
Looks like problem in imgaug. For bboxes we use imgaug processor.
Try to install latest version of imgaug manually, because by default albumentations uses 0.2.5 or 0.2.6 version.",misunderstood problem talking like problem use processor try install latest version manually default version,issue,negative,positive,positive,positive,positive,positive
573456283,"Changes to your code:
1. Red box to see transformed area
2. Different cat
3. `keep_size=False`
```
import albumentations as A
import cv2 as cv
import matplotlib.pyplot as plt

img = cv.imread(r'cat.jpg')
bbox = [[100, 100, 200, 200, 0]]
cv.rectangle(img, (int(bbox[0][0]), int(bbox[0][1])), (int(bbox[0][2]), int(bbox[0][3])),(0,0,255), -1)
transform = A.Compose([A.IAAPerspective(p=1,keep_size=False)], bbox_params=A.BboxParams(""pascal_voc""))

res = transform(image=img, bboxes=bbox)

res_img = res['image']
res_bbox = res['bboxes']

for x1, y1, x2, y2, label in bbox:
    cv.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)

for x1, y1, x2, y2, label in res_bbox:
    cv.rectangle(res_img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)

plt.subplot(211)
plt.title('original')
plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))

plt.subplot(212)
plt.title('result')
plt.imshow(cv.cvtColor(res_img, cv.COLOR_BGR2RGB))

plt.show()
```

![image](https://user-images.githubusercontent.com/15197972/72225479-c5bb8e80-353a-11ea-9fd7-23519bb5f97a.png)
",code red box see area different cat import import import transform transform label label image,issue,negative,neutral,neutral,neutral,neutral,neutral
573453546,"I still can not reproduce.
Can you get code for reproduce error?",still reproduce get code reproduce error,issue,negative,neutral,neutral,neutral,neutral,neutral
573362569,"@Dipet, thanks for quick response and example! I didn't provide enough details. Please try the same function, but with `keep_size=False`.
I.e.:
```
transform = A.Compose([A.IAAPerspective(p=1,keep_size=False)], bbox_params=A.BboxParams(""pascal_voc""))```",thanks quick response example provide enough please try function transform,issue,positive,positive,positive,positive,positive,positive
573332243,"I am can not reproduce error.

```
import albumentations as A
import cv2 as cv
import matplotlib.pyplot as plt

img = cv.imread(r'notebooks/images/image_1.jpg')
bbox = [[100, 100, 200, 200, 0]]
transform = A.Compose([A.IAAPerspective(p=1)], bbox_params=A.BboxParams(""pascal_voc""))

res = transform(image=img, bboxes=bbox)

res_img = res['image']
res_bbox = res['bboxes']

for x1, y1, x2, y2, label in bbox:
    cv.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)

for x1, y1, x2, y2, label in res_bbox:
    cv.rectangle(res_img, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 3)

plt.subplot(211)
plt.title('original')
plt.imshow(cv.cvtColor(img, cv.COLOR_BGR2RGB))

plt.subplot(212)
plt.title('result')
plt.imshow(cv.cvtColor(res_img, cv.COLOR_BGR2RGB))

plt.show()
```

![image](https://user-images.githubusercontent.com/7512250/72207387-be987000-34a9-11ea-8717-af7d267416d4.png)

",reproduce error import import import transform transform label label image,issue,negative,neutral,neutral,neutral,neutral,neutral
571183588,This library consumes images and masks as `numpy` arrays. It seems you're trying to send a PIL image instead.,library trying send image instead,issue,negative,neutral,neutral,neutral,neutral,neutral
570779443,"I think using symetric keypoints is owerhead.
What kind of symmetrion we need, horizontal, vertical, diagonal?

We can resolve this problem using inheritance from transform and reimplement needed method. It is not hard and do not need many lines of code.",think kind need horizontal vertical diagonal resolve problem inheritance transform method hard need many code,issue,negative,positive,positive,positive,positive,positive
570245492,"> @arsenyinfo can you please add following requirement to `setup.py`: `setup( ...., python_requires="">=3.5"", ...)`. Thanks!

@BloodAxe done! ",please add following requirement setup thanks done,issue,positive,positive,neutral,neutral,positive,positive
570154396,"@arsenyinfo  can you please add following requirement to `setup.py`: `setup( ...., python_requires="">=3.5"", ...)`. Thanks!

",please add following requirement setup thanks,issue,positive,positive,neutral,neutral,positive,positive
570044756,FYI: the tests are red for python2 only - does it make sense to fix those issues in 2020 or you'll fix CI before? ,red python make sense fix fix,issue,negative,neutral,neutral,neutral,neutral,neutral
569934875,"Approach with `self.data_length` has a shortcoming that it fails if not only bbox but another data fields are used i.e len(self.data_fields)>1 and these data fields have different data length.
I've fixed the problem that this PR fixes, as well as the above one, as a side effect of fixing #503 in PR #504, and in a more elegant manner, I suppose.",approach shortcoming another data used data different data length fixed problem well one side effect fixing elegant manner suppose,issue,negative,positive,positive,positive,positive,positive
569596132,"! pip install -U git + https: //github.com/albu/albumentations
After that, I was able to import BboxParams !
I don't know why : (",pip install git able import know,issue,negative,positive,positive,positive,positive,positive
568883067,"> In addition use transforms like HueSaturationValue for 1-channel images is strange, because it has the meaning only for RGB images.

Hmm, yes, now I found transform for brightness, it's ok.
Could you advice easy way (or lib, or func) to convert `(64, 64, 1)` to `(64, 64, 3)`?

> Support of 1-channel images for all transforms is not a prior thing, so we can not tell when it will be added.

I got you.
But 1-channel images use in NN very often in my cases. I choose it when think what human would can detect on b/w image, if it possible task then I use 1-channel. Maybe it's wrong way, but working for me.",addition use like strange meaning yes found transform brightness could advice easy way convert support prior thing tell added got use often choose think human would detect image possible task use maybe wrong way working,issue,positive,negative,neutral,neutral,negative,negative
568881190,"It is need only for check this error.
```
def aug():
    return A.Compose([A.HueSaturationValue(p=1)], p=1)
```

Support of 1-channel images for all transforms is not a prior thing, so we can not tell when it will be added.

In addition use  transforms like `HueSaturationValue` for 1-channel images is strange, because it has the meaning only for RGB images.",need check error return support prior thing tell added addition use like strange meaning,issue,negative,negative,neutral,neutral,negative,negative
568874326,"> need set p=1

p=1 for what? HueSaturationValue?

> Use RGB image, all transforms supports them.

When all transforms will be support 1-channel images? Is it in the plans?",need set use image support,issue,negative,neutral,neutral,neutral,neutral,neutral
568873108,"ok, may fail. This transform is newer executed, need set `p=1`

By default not all transforms supports 1-channel images.
Use RGB image, all transforms supports them.",may fail transform executed need set default use image,issue,negative,negative,negative,negative,negative,negative
568868373,"Very strange. After restart PyCharm I run this code again and all works fine, but all other runs is failed with traced as above. 
I try restart again and it works in every runs.",strange restart run code work fine try restart work every,issue,negative,positive,positive,positive,positive,positive
568866366,"```
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\core\composition.py"", line 176, in __call__
    data = t(force_apply=force_apply, **data)
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\core\transforms_interface.py"", line 87, in __call__
    return self.apply_with_params(params, **kwargs)
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\core\transforms_interface.py"", line 100, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\augmentations\transforms.py"", line 2110, in apply
    return F.shift_hsv(image, hue_shift, sat_shift, val_shift)
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\augmentations\functional.py"", line 387, in shift_hsv
    return _shift_hsv_non_uint8(img, hue_shift, sat_shift, val_shift)
  File ""C:\Users\User\Anaconda3\envs\DLDM\lib\site-packages\albumentations\augmentations\functional.py"", line 369, in _shift_hsv_non_uint8
    img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)
cv2.error: OpenCV(4.1.2) c:\projects\opencv-python\opencv\modules\imgproc\src\color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function '__cdecl cv::impl::`anonymous-namespace'::CvtHelper<struct cv::impl::`anonymous namespace'::Set<3,4,-1>,struct cv::impl::A0x3b52564f::Set<3,-1,-1>,struct cv::impl::A0x3b52564f::Set<0,5,-1>,2>::CvtHelper(const class cv::_InputArray &,const class cv::_OutputArray &,int)'
> Invalid number of channels in input image:
>     'VScn::contains(scn)'
> where
>     'scn' is 1
```

Maybe point is in float64?
![image](https://user-images.githubusercontent.com/2697890/71439467-d7173500-270a-11ea-84e0-d6da6884ec8a.png)",file line data data file line return file line key file line apply return image file line return file line error unspecified error function anonymous class class invalid number input image maybe point float image,issue,negative,neutral,neutral,neutral,neutral,neutral
568865539,"I am cannot reproduce this error.
Please add code for image generation and error traceback.
```
import numpy as np
import albumentations as A


def aug(p=0.5):
    return A.Compose([A.HueSaturationValue(p=0.3)], p=p)


print(A.__version__) # 0.4.3
np.random.seed(0)
images_1 = [np.random.randint(0, 256, [64, 64, 1], dtype=np.uint8)]


for image_index in range(len(images_1)):
    augmentation = aug(p=0.9)
    data = {'image': images_1[image_index]}
    augmented = augmentation(**data)
```",reproduce error please add code image generation error import import return print range augmentation data augmented augmentation data,issue,negative,neutral,neutral,neutral,neutral,neutral
568292566,@Erlemar Removing the test that does not pass is not really an option.,removing test pas really option,issue,negative,positive,positive,positive,positive,positive
567964148,"The code can be rewritten with 1 cycle instead of using product with 3 cycles.
I've tested both approaches and 1 cycle approach is ~2x faster. See the code below (need to be tested)
```
hs = np.arange(img.shape[0]-max_delta, max_delta, -1)
ws = np.arange(img.shape[1]-max_delta, max_delta, -1)
h = np.tile(hs, ws.shape[0])
w = np.repeat(ws, hs.shape[0])

for i in range(iterations):
    dy = np.random.randint(-max_delta, max_delta, size=(h.shape[0],))
    dx = np.random.randint(-max_delta, max_delta, size=(h.shape[0],))
    x[h, w], x[h+dy, w+dx] = x[h+dy, w+dx], x[h, w]
```
the same approach with numba works ~7x faster, than code in PR
deltas can be generated before cycle and it works a bit faster (almost the same), than numba code, but needs much more memory",code cycle instead product tested cycle approach faster see code need tested range approach work faster code cycle work bit faster almost code need much memory,issue,negative,positive,positive,positive,positive,positive
567454072,"please ensure that you are on the latest version of albumentation and that your bounding box format is correct. https://albumentations.readthedocs.io/en/latest/api/core.html#albumentations.core.composition.BboxParams
also bounding boxes coordinates must not exceed image dimensions.",please ensure latest version bounding box format correct also bounding must exceed image,issue,positive,positive,positive,positive,positive,positive
567453072,"@albu Hi.albu, thanks for your great work! I also have the same problem. I output the intermediate results for your reference：
``
'bboxes': [[478.1674, 278.39395, 686.9629, 418.23538, 0], [435.63498, 469.1454, 477.52298, 525.2108, 1], [397.6136, 536.16614, 441.43488, 590.94275, 2], [462.70108, 572.8987, 505.2335, 625.742, 3], [498.78918, 507.166     78, 541.3216, 559.36566, 4], [401.48016, 68.30963, 423.3908, 85.06482, 5], [389.88043, 73.46507, 426.61298, 97.309, 6], [386.01385, 80.55381, 422.74637, 103.7533, 7], [381.50284, 86.998116, 418.8798, 110.84204, 8], [374.4141, 93.44241, 415.65765, 118.5752, 9], [370.54752, 389.88043, 417.59094, 422.74637, 10],      [376.34738, 409.85776, 408.5689, 432.41284, 11], [359.5922, 419.52423, 366.0365, 426.61298, 12], [366.0365, 420.16867, 382.14725, 435.63498, 13], [380.21396, 431.12396, 399.54688, 445.30145, 14], [352.50345, 429.19067, 376.34738, 447.23474, 15], [378.28067, 444.657, 393.10257, 456.90118, 16], [392.45816, 449.8     1247, 392.45816, 449.81247, 17], [347.99246, 440.146, 366.68094, 454.3235, 18], [332.52612, 397.6136, 364.74765, 431.7684, 19], [318.99307, 438.85715, 380.8584, 478.1674, 20], [315.1265, 445.9459, 340.25928, 463.98993, 21], [345.41473, 463.98993, 376.99182, 485.25613, 22], [606.40906, 542.6105, 657.9635, 579.3     4296, 23], [552.9213, 641.8527, 574.18756, 659.8968, 24], [581.27625, 658.6079, 605.1202, 676.652, 25], [599.9648, 558.7212, 655.3858, 593.52045, 26], [589.6539, 578.69855, 645.0749, 612.8534, 27]]
``
Can you give me some suggestions ？",thanks great work also problem output intermediate give,issue,positive,positive,positive,positive,positive,positive
567410125,"I think you work with incompatible version of imgaug. Library work with `imgaug>=0.2.5,<0.2.7`.
I got this error with `imgaug=0.3.0`",think work incompatible version library work got error,issue,negative,neutral,neutral,neutral,neutral,neutral
566884313,"I'll soon make a PR with a new transform named `BBoxSafeRandomCrop` which does not resize output images as implementation posted above. 

With this new transform, I think the original implementation `RandomSizedBBoxSafeCrop` should be replaced with a much simpler implementation like below.

```python
class RandomSizedBBoxSafeCrop(BBoxSafeRandomCrop):
    """"""class docstring is omitted""""""

    def __init__(self, height, width, erosion_rate=0.0, interpolation=cv2.INTER_LINEAR, always_apply=False, p=1.0):
        super(RandomSizedBBoxSafeCrop, self).__init__(erosion_rate, always_apply, p)
        self.height = height
        self.width = width
        self.interpolation = interpolation

    def apply(self, img, crop_height=0, crop_width=0, h_start=0, w_start=0, interpolation=cv2.INTER_LINEAR, **params):
        crop = super(RandomSizedBBoxSafeCrop, self).apply(img, crop_height=0, crop_width=0, h_start=0, w_start=0, interpolation=cv2.INTER_LINEAR, **params)
        return F.resize(crop, self.height, self.width, interpolation)
```
In this way, we can have clean and simple code without duplicated parts, while not breaking the backward compatibility. 
",soon make new transform resize output implementation posted new transform think original implementation much simpler implementation like python class class self height width super self height width interpolation apply self crop super self return crop interpolation way clean simple code without breaking backward compatibility,issue,positive,positive,positive,positive,positive,positive
565167854,I second the need to apply the same transform to an array of images. ,second need apply transform array,issue,negative,neutral,neutral,neutral,neutral,neutral
562813248,"@DecentMakeover Yes, but I'm new to 3D data and there maybe lots of things to test.",yes new data maybe lot test,issue,negative,positive,positive,positive,positive,positive
562466856,"@DecentMakeover 
I wrote a simple 3D data augmentation package [here](https://github.com/ashawkey/volumentations) inspired by albumentations and batchgenerators, maybe you will find it helpful.",wrote simple data augmentation package inspired maybe find helpful,issue,positive,neutral,neutral,neutral,neutral,neutral
561816766,There is already this kind of noise (based on poisson process) called `A.ISONoise`. To me it does not make sense to have duplicate.,already kind noise based process make sense duplicate,issue,positive,positive,positive,positive,positive,positive
560541802,"Just add the name of the transform to all lists like: 

https://github.com/albumentations-team/albumentations/blob/master/tests/test_transforms.py#L244
",add name transform like,issue,negative,neutral,neutral,neutral,neutral,neutral
560467479,Also we can set `height` and `width` to `None` by default . And if one of them is `None` do not use resize,also set height width none default one none use resize,issue,negative,neutral,neutral,neutral,neutral,neutral
560461594,"Also this transform named such `Sized`, that mean it will be resized to some `height` and `width`. If you need safe crop without resize you can add add new transform `RandomSizedBBoxSafeCrop` with you signature.",also transform sized mean height width need safe crop without resize add add new transform signature,issue,negative,positive,positive,positive,positive,positive
560459741,"Yes current implementation has a little bit overhead, but we cannot broke backward compatibility for existing transforms. In next transforms we will try to create much simpler and brief code.",yes current implementation little bit overhead broke backward compatibility next try create much simpler brief code,issue,negative,positive,neutral,neutral,positive,positive
560070775,you need to stack on last dimension or pass masks separately. https://github.com/albu/albumentations/blob/master/notebooks/example_multi_target.ipynb,need stack last dimension pas separately,issue,negative,neutral,neutral,neutral,neutral,neutral
559728189,"Issue #298 seems to be related to this.
I tried to make this change in a PR, but it failed some test due to change of class signature.

",issue related tried make change test due change class signature,issue,negative,negative,neutral,neutral,negative,negative
558624417,"@albu 
Is it still the case?

Interestingly enough, I forked your augs back when you posted them for carvana challenge (and used them for several satellite competitions) and they worked for multi-channel masks ...
",still case interestingly enough forked back posted challenge used several satellite worked,issue,negative,neutral,neutral,neutral,neutral,neutral
558207298,"@ZFTurbo, could you also implement a `mask_from_bbox` as a function as well, as you've done with the `bbox_for_mask`. It would be really useful for other mask-only transforms.
P.S. I thinks it's better to rename `bbox_for_mask` to `bbox_from_mask`, makes more sense to me this way.",could also implement function well done would really useful better rename sense way,issue,positive,positive,positive,positive,positive,positive
558131140,Also update code using black `black --config=black.toml`,also update code black black,issue,negative,negative,negative,negative,negative,negative
557913323,"The complete process to install shapely is described in this article which works perfectly fine and this article will clear your related error as well. The link of that article is
<a href=""https://www.akshatvasistha.com/2019/10/how-do-real-time-image-segmentation-mask-rcnn.html""> Real-time image segmentation with MaskRCNN </a>",complete process install shapely article work perfectly fine article clear related error well link article image segmentation,issue,positive,positive,positive,positive,positive,positive
557876543,Thanks for your PR. Please update the README as written here https://github.com/albumentations-team/albumentations#contributing,thanks please update written,issue,positive,positive,positive,positive,positive,positive
557792495,"@verdoganai Hi , How did you manage to augment your 3d data?
",hi manage augment data,issue,negative,neutral,neutral,neutral,neutral,neutral
554659008,Any idea when this fix will be merged into master? Currently using the albumentations package in a project where I need to work with YOLO style bboxes.,idea fix master currently package project need work style,issue,negative,neutral,neutral,neutral,neutral,neutral
553625274,"Library does not supports `float64` images, only `float32`. At this transform we need original image type for normalization. With your changes we will get incorrect results for `uint8` images, because we will get always `MAX_VALUES_BY_DTYPE[dtype] == 1`, but we need at this case `MAX_VALUES_BY_DTYPE[dtype] == 255`",library float float transform need original image type normalization get incorrect get always need case,issue,negative,positive,positive,positive,positive,positive
552999145,"Now when I checked the API, you are right. For CutMix, if the mixed image is fixed it can be treated as an input variable (numpy array etc.) but Mixup is more tricky obviously.",checked right mixed image fixed input variable array tricky obviously,issue,negative,positive,neutral,neutral,positive,positive
552991846,I am looking into contributing both Mixup and CutMix augmentations.  Both of these augmentations take multiple images in order to do the augmentation. As far as I can tell all of the existing augmentations in the library only operates on a single image and the entire API is build around a single image being fetched in the datalaoder. Is that correct? To implement these new augmentations we would be operating on the batch level. Do you have any thoughts on how this should be implemented API wise within the library? ,looking take multiple order augmentation far tell library single image entire build around single image fetched correct implement new would operating batch level wise within library,issue,negative,positive,positive,positive,positive,positive
552545709,"1. Original + transformed is ok.
2. We do not have a code to generate such a page. But it does not look like a lot of manual work.",original code generate page look like lot manual work,issue,positive,positive,positive,positive,positive,positive
552325842,"Update Albumentations to **0.4.2** can fix this error, I'm sorry to disturb you. 😅 ",update fix error sorry disturb,issue,negative,negative,negative,negative,negative,negative
552126631,"> Could you please add a similar page to the documentation? https://albumentations.readthedocs.io/en/latest/augs_overview/image_only/image_only.html#multiplicativenoise

Yes, I will.

But I have a couple of clarifying questions:
1. ToSepia is similar to ToGray - so there is no randomness or variability. It just a fixed transformation. The example page will have only two images - original and transformed. Is it ok? Or should I add a ""level of sepianess"" variable and add some randomness to it?
2. Do you have a code for the generation of such pages (at least partial), or it should be done manually?",could please add similar page documentation yes couple similar randomness variability fixed transformation example page two original add level variable add randomness code generation least partial done manually,issue,positive,positive,neutral,neutral,positive,positive
551972254,"Could you please add a similar page to the documentation? https://albumentations.readthedocs.io/en/latest/augs_overview/image_only/image_only.html#multiplicativenoise
",could please add similar page documentation,issue,negative,neutral,neutral,neutral,neutral,neutral
549859862,"Hi!
Is simplest way to use CropNonEmptyMaskIfExists for bboxes it is create mask from your bboxes like this:
```
mask = np.zeros(image.shape[:2], dtype=np.uint8)
for x1, y1, x2, y2 in bboxes:
    mask[y1:y2, x1:x2] = 1

result = transforms(image=image, mask=mask, bboxes=bboxes)
image = result['image']
bboxes = result['bboxes']
```

",hi way use create mask like mask mask result image result result,issue,positive,neutral,neutral,neutral,neutral,neutral
549814630,"@Dipet, I thought that Enum class is only in python3, I was wrong.

I don't think that we need classmethod like has_value, we can instead:
```
BboxesFormat.has_value('coco')
```
Check in another way:
```
BboxesFormat('coco') in BboxesFormat
```
In this case, we will have an informative error. If some value is not in the Bbox Enum.
",thought class python wrong think need like instead check another way case informative error value,issue,negative,negative,negative,negative,negative,negative
549797678,"At this time we support only python 2.7 so we can use Enum class.

@katsura-jp I think, the best solution is to add Enumeration to `bbox_utils.py` like said @maruschin This changes is similar, but better than what @BloodAxe  suggested.

```
from enum import Enum

class BboxesFormat(Enum):
    COCO = 'coco'
    PASCAL_VOC = 'pascal_voc'
    YOLO = 'yolo'
    XYXY = 'xyxy'
    XYWH = 'xywh'
    XYWH_CENTER_NORM = 'xywh_center_norm'
    XYXY_NORM = 'xyxy_norm'
    XYWH_NORM = 'xywh_norm'
    XYWH_CENTER = 'xywh_center'

    @classmethod
    def has_value(cls, value):
        return value in cls._value2member_map_
```",time support python use class think best solution add enumeration like said similar better import class coco value return value,issue,positive,positive,positive,positive,positive,positive
549666517,"If we stop supporting python2, we can use Enum for this PR:

Create Enum Class:
```
from enum import Enum

class BboxesFormat(Enum):
    COCO = 'coco'
    PASCAL_VOC = 'pascal_voc'
    YOLO = 'yolo'
    XYXY = 'xyxy'
    XYWH = 'xywh'
    XYWH_CENTER_NORM = 'xywh_center_norm'
    XYXY_NORM = 'xyxy_norm'
    XYWH_NORM = 'xywh_norm'
    XYWH_CENTER = 'xywh_center'
```

And then check:
```
if BboxesFormat(target_format) in {BboxesFormat.COCO, BboxesFormat.XYXY}: ...
```

And we don't need check:
```
if target_format not in {""coco"", ""pascal_voc"", ""yolo""}:
```
Our code raise `ValueError` in this place: `BboxesFormat(target_format)`.",stop supporting python use create class import class coco check need check coco code raise place,issue,negative,positive,positive,positive,positive,positive
549511763,@Arquestro can you please update your PR with respect to latest updates from master? Thanks!,please update respect latest master thanks,issue,positive,positive,positive,positive,positive,positive
549027837,"hi @BloodAxe and @Dipet . Great! so we had an internal meeting and we decided that we will implement some of the existing functionalities using raw pytorch operators to leverage differentiable data augmentation. We will try to align with albumentations API and potentially with torchvision too.

Do you have any public roadmap so that we can join efforts ? I've seen that @Dipet already started to dig into kornia.

Would be great to brainstorm between frameworks so that both we can coexist instead of mutual excluding ourselves :D

Please check our updates: https://github.com/kornia/kornia/issues/312
Board meeting notes: https://github.com/kornia/kornia/wiki/2019#2019-10-31",hi great internal meeting decided implement raw leverage differentiable data augmentation try align potentially public join seen already dig would great coexist instead mutual excluding please check board meeting,issue,positive,positive,positive,positive,positive,positive
549010865,"@BloodAxe 

Thank you for your comment. 
In which file should `UPPERCASE_NAMES` be written?
Is it all right with `albumentations/augmentations/bbox_utils.py`?
",thank comment file written right,issue,negative,positive,positive,positive,positive,positive
548776285,Also it is possibly due to rounding error when your `ymax` coord in bbox equal to image shape.,also possibly due rounding error equal image shape,issue,negative,negative,neutral,neutral,negative,negative
548767248,"I think, bboxes greater than image to 1 pixel. Maybe you forget reduce bbox coords by 1 pixel, when produce bboxes. Original bbox converter from VOC to YOLO format looks like this:
```
(x_min, y_min, x_max, y_max), tail = bbox[:4], bbox[4:]
x = (x_min + x_max) / 2 - 1
y = (y_min + y_max) / 2 - 1
width = x_max - x_min
height = y_max - y_min
bbox = normalize_bbox((x, y, width, height) + tail, rows, cols)
```

https://github.com/pjreddie/darknet/blob/f6d861736038da22c9eb0739dca84003c5a5e275/scripts/voc_label.py#L12",think greater image maybe forget reduce produce original converter format like tail width height width height tail,issue,positive,positive,positive,positive,positive,positive
548764337,"You catch this error, because some of your bboxes greater than image size.",catch error greater image size,issue,negative,positive,positive,positive,positive,positive
548763790,"Please, use Markdown formatting for your messages. Without it it is really hard to understand anything in your message.

> I tried to use any of transforms like `VerticalFlip`, `RandomSizedBBoxSafeCrop` and others box coordinate transformations but always i got the error `Expected x_max for bbox (0.9515625, 0.5316840277777778, 1.003125, 0.6955729166666667, 0) to be in the range [0.0, 1.0], got 1.003125`. 
if i replace   lines `x_min, x_max = x_min / cols, x_max / cols,  y_min, y_max = y_min / rows, y_max / rows`   in `bbox_utils.py` in `normalize_bbox` method by  `x_min, x_max = min(x_min / cols, 1.0), min(x_max / cols, 1.0),  y_min, y_max = min(y_min / rows, 1.0), min(y_max / rows, 1.0)` . it works correctly.
               
> ## To Reproduce

> Steps to reproduce the behavior:
> 1.    
> ```
> transforms = [
>     VerticalFlip(),
>     RandomBrightnessContrast(),
>     RandomShadow(p=0.5),
>     RandomSnow(p=0.5),
>     RandomFog(),
>     JpegCompression()]
>     augmentor = Compose(transforms, bbox_params=BboxParams(format='yolo', label_fields=['category_id']))
> ```
> 2. Input bboxes 
> ```
> [[0.492578125, 0.5118055555555555, 0.01328125, 0.02638888888888889], [0.501171875, 0.5013888888888889, 0.01171875, 0.019444444444444445], [0.509765625, 0.5020833333333333, 0.01328125, 0.020833333333333332], [0.51640625, 0.51875, 0.0265625, 0.034722222222222224], [0.581640625, 0.5131944444444444, 0.02265625, 0.029166666666666667], [0.613671875, 0.5145833333333333, 0.02734375, 0.034722222222222224], [0.7546875, 0.5319444444444444, 0.0859375, 0.08055555555555556], [0.46796875, 0.5423611111111111, 0.065625, 0.10138888888888889], [0.9734375, 0.6097222222222223, 0.0515625, 0.1638888888888889]]
> ```
> <!-- If you have a code sample, error messages, stack traces, please provide it here as well -->
> ```
> Traceback (most recent call last):
>   File ""/home/robo/Code/Python/ONNX/mobilenetv2.py"", line 655, in <module>
>     for batch_data, boxes in det_dataset.get_batchGPU(batch_size):
>   File ""/home/robo/Code/Python/ONNX/mobilenetv2.py"", line 609, in get_batchGPU
>     max_length_box = self.get_image(start_index, batch_size, batch, labels)
>   File ""/home/robo/Code/Python/ONNX/mobilenetv2.py"", line 579, in get_image
>     sample = self.getItemGPURandomGreed(start_index)
>   File ""/home/robo/Code/Python/ONNX/mobilenetv2.py"", line 569, in getItemGPURandomGreed
>     return self.getItemGPUVariableGreed(indx, np.random.randint(1, 3), np.random.randint(1, 3))
>   File ""/home/robo/Code/Python/ONNX/mobilenetv2.py"", line 564, in getItemGPUVariableGreed
>     aug = augmentor(**annotation)
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/core/composition.py"", line 174, in __call__
>     p.preprocess(data)
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/core/utils.py"", line 63, in preprocess
>     data[data_name] = self.check_and_convert(data[data_name], rows, cols, direction=""to"")
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/core/utils.py"", line 71, in check_and_convert
>     return self.convert_to_albumentations(data, rows, cols)
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/augmentations/bbox_utils.py"", line 51, in convert_to_albumentations
>     return convert_bboxes_to_albumentations(data, self.params.format, rows, cols, check_validity=True)
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/augmentations/bbox_utils.py"", line 305, in convert_bboxes_to_albumentations
>     return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/augmentations/bbox_utils.py"", line 305, in <listcomp>
>     return [convert_bbox_to_albumentations(bbox, source_format, rows, cols, check_validity) for bbox in bboxes]
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/augmentations/bbox_utils.py"", line 253, in convert_bbox_to_albumentations
>     check_bbox(bbox)
>   File ""/home/robo/.local/lib/python3.6/site-packages/albumentations/augmentations/bbox_utils.py"", line 332, in check_bbox
>     ""to be in the range [0.0, 1.0], got {value}."".format(bbox=bbox, name=name, value=value)
> ValueError: Expected x_max for bbox (0.9515625, 0.5316840277777778, 1.003125, 0.6955729166666667, 0) to be in the range [0.0, 1.0], got 1.003125.
> ```
> <!-- A clear and concise description of what you expected to happen. -->
> 
> ## Environment
>  - Albumentations version 0.4.2:
>  - Python version 3.6.8:
>  - OS Ubuntu 18.04:
>  - pip :",please use markdown without really hard understand anything message tried use like box always got error range got replace method min min min min work correctly reproduce reproduce behavior compose input code sample error stack please provide well recent call last file line module file line batch file line sample file line return file line annotation file line data file line data data file line return data file line return data file line return file line return file line file line range got value range got clear concise description happen environment version python version o pip,issue,positive,negative,neutral,neutral,negative,negative
548715521,"@katsura-jp 

Thanks for your PR! Can you please updated your PR such that we use `UPPERCASE_NAMES` for all bboxes formats and use them everywhere in code. It seems we are having to much repetition of ""xyxy"", ""xywh"" etc in the code which is error prone. 

E.g declare what boxes we have as follows:

```
BBOXES_FORMAT_COCO = ""coco""
BBOXES_FORMAT_PASCAL_VOC= ""coco""
BBOXES_FORMAT_PASCAL_VOC = ""yolo""
...
```

And use these constants elsewhere",thanks please use use everywhere code much repetition code error prone declare coco coco use elsewhere,issue,negative,positive,positive,positive,positive,positive
548264473,"oh, tilted boxes are not supported yet. you can try to emulate them with keypoints",oh yet try emulate,issue,negative,neutral,neutral,neutral,neutral,neutral
548263949,"@GYxiaOH 
but in bbox_utils.py line 335,you judge:
    if y_max <= y_min:
        raise ValueError(""y_max is less than or equal to y_min for bbox {bbox}."".format(bbox=bbox))
but in some ocr tasks,Bbox is tilted,so y_max maybe more than y_min",line judge raise le equal maybe,issue,negative,neutral,neutral,neutral,neutral,neutral
548251982,box is valid if its' coordinates are smaller then image size,box valid smaller image size,issue,negative,neutral,neutral,neutral,neutral,neutral
547324787,"I was following the notebook examples when I met this error, and I just found that the installed version is not same as master version.
when I install albumentation again, then I get 0.4.2 version.

Sorry for the silly mistake.
",following notebook met error found version master version install get version sorry silly mistake,issue,negative,negative,negative,negative,negative,negative
547312116,"@Dipet I have reverted `cv2.connectedComponents` -> `label` from `scikit-image`.
Reason:
1) We already have scikit-image (`import scimage`) in requirements
2) OpenCV implementation supports only uint8 masks",label reason already import implementation,issue,negative,neutral,neutral,neutral,neutral,neutral
547305880,"@edgarriba greetings Edgar!

Definitely! Having GPU-accelerated image augmentation is one of our areas of interest and is pretty often asked by our users. There is already some study has been done by @Dipet (using PyTorch API) and myself (for OpenCL) and we really would appreciate your input on this. 

We do have slack channel `#tools_albumentations` in `ods.ai` Slack if you mind joining it. Alternatively feel free to drop me an email `ekhvedchenya@gmail.com` and we can schedule a call. And as a last resort we can use this thread :)",definitely image augmentation one interest pretty often already study done really would appreciate input slack channel slack mind joining alternatively feel free drop schedule call last resort use thread,issue,positive,positive,positive,positive,positive,positive
547147247,"> skimage

Probably not. I will try to use `cv2.connectedComponents` instead",probably try use instead,issue,negative,neutral,neutral,neutral,neutral,neutral
546692498,"Docker is also a very lightweight abstraction layer (when it runs on a Linux machine). Performance penalties when using Docker come mainly from network and storage virtualisation. Since the benchmark doesn't use any networking features and keeps all the images in memory, the cost of containerisation should be close to zero.

On the other side, using Docker helps to keep a more reproducible environment. With virtualenv there are many more moving parts like Python version and system libraries versions, so it is harder to obtain consistent benchmarking results.
",docker also lightweight abstraction layer machine performance docker come mainly network storage since use memory cost close zero side docker keep reproducible environment many moving like python version system harder obtain consistent,issue,negative,positive,positive,positive,positive,positive
546684871,"Maybe better to use virtualenv, then docker? Because virtualenv is lighter then docker.",maybe better use docker lighter docker,issue,negative,positive,positive,positive,positive,positive
546625323,@ternaus How you imagine it? New page with description of this transforms or only add images to API page with this transform?,imagine new page description add page transform,issue,negative,positive,positive,positive,positive,positive
546624769,"I'm the creator of the conda-forge feedstock and I just discovered this discussion. I'll add the people listed in https://github.com/albu/albumentations/issues/291#issuecomment-511430166 to the feedstock. If anyone shouldn't be included anymore, please tell me.",creator discovered discussion add people listed anyone included please tell,issue,negative,neutral,neutral,neutral,neutral,neutral
546408514,"I like the change, but as you already have images with examples, do you mind adding a page to documentation with them describing this transform?",like change already mind page documentation transform,issue,negative,neutral,neutral,neutral,neutral,neutral
546108538,"Sure thing, images, masks, boxes and key points are transformed in a consistent way.",sure thing key consistent way,issue,negative,positive,positive,positive,positive,positive
545691701,Do you mind if I try? I used hypothesis recently to test rules for a policy system.,mind try used hypothesis recently test policy system,issue,negative,neutral,neutral,neutral,neutral,neutral
545150428,"i am using pytorch,how can i use speckle noise or something like multiplyElementwise in this kernel of mine? : https://www.kaggle.com/mobassir/unet-with-se-resnext50-32x4d-encoder-for-stage-2

please help a little bit,thank you a lot in advance",use speckle noise something like kernel mine please help little bit thank lot advance,issue,positive,negative,negative,negative,negative,negative
545090374,If I understand correctly this noise is the same as Gaussian noise but multiplicative?,understand correctly noise noise multiplicative,issue,negative,neutral,neutral,neutral,neutral,neutral
545070632,"> In ultrasound images, the noise content is multiplicative and non Gaussian. Such noise is generally more difficult to remove than additive noise, because the intensity of the noise varies with the image intensity. A model of multiplicative noise is given by
> 
> yij= Xij * nij
> 
> where the speckle image yij is the product of the original image Xij, and the non-Gaussian noise nij.

From this article https://www.intechopen.com/books/advancements-and-breakthroughs-in-ultrasound-imaging/speckle-noise-reduction-in-medical-ultrasound-images",noise content multiplicative non noise generally difficult remove additive noise intensity noise image intensity model multiplicative noise given speckle image product original image noise article,issue,negative,negative,neutral,neutral,negative,negative
545065746,Do we have a way to generate this noise? Maybe equation?,way generate noise maybe equation,issue,negative,neutral,neutral,neutral,neutral,neutral
544905998,"All my knowledge's about speckle noise based on the paper, that you linked.
I think the nearest to speckle noise is [MultiplyElementwise](https://imgaug.readthedocs.io/en/latest/source/overview/arithmetic.html#multiplyelementwise) from imgaug.",knowledge speckle noise based paper linked think nearest speckle noise,issue,negative,neutral,neutral,neutral,neutral,neutral
544896137,"i used your gaussiannoise several time in the past but my supervisor forcing me to use speckle noise,i said him that we have gaussiannoise that can do good job but he is forcing me to use speckle where i can'r find speckle noise implementation in any data augmentation library,is spackle better than gaussiannoise?
thank you @Dipet  for your comment.",used several time past supervisor forcing use speckle noise said good job forcing use speckle find speckle noise implementation data augmentation library better thank comment,issue,positive,positive,positive,positive,positive,positive
544854253,"As I understand, speckle is a multiplicative noise and it is formula `y = Noise * x`. If i wrong, please, correct me.
As I know, in our library currently implemented only additive noise (look [GaussNoise](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.GaussNoise)).
I will try implement multiplicative noise in the coming few days.",understand speckle multiplicative noise formula noise wrong please correct know library currently additive noise look try implement multiplicative noise coming day,issue,negative,negative,negative,negative,negative,negative
544635281,"Hi @ternaus 
Speckle noise is an undesirable effect. The source of this type of noise is caused due to random interference between the coherent returns issued from the so many scatterers present on a earth surface, on the scale of a wavelength of the incident radar wave. 

Speckle reduction is an important step prior to the processing and analysis of the medical ultrasound images.

please check the ""image quality assessment"" section from this  link : https://www.intechopen.com/books/advancements-and-breakthroughs-in-ultrasound-imaging/speckle-noise-reduction-in-medical-ultrasound-images

if you don't have direct implementation of speckle noise then  i would like to know if you have similar thing in your library or not?",hi speckle noise undesirable effect source type noise due random interference coherent many present earth surface scale incident radar wave speckle reduction important step prior analysis medical please check image quality assessment section link direct implementation speckle noise would like know similar thing library,issue,negative,positive,neutral,neutral,positive,positive
544628603,"What is speckle noise?

You may use https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda

to apply user-defined transforms to your images.",speckle noise may use apply,issue,negative,neutral,neutral,neutral,neutral,neutral
544506313,sorry I could not do justice to this. was about to look into this today.,sorry could justice look today,issue,negative,negative,negative,negative,negative,negative
543965628,"It really helps if you are augmenting large dataset in something like Google Colab, and you are able to use this with the GPU to accelerate your augmentations, it really helps.",really large something like able use accelerate really,issue,negative,positive,positive,positive,positive,positive
543390330,"@tatigabru I managed to get the filtered indexes using label_fields:

`params = A.BboxParams(format=""coco"", label_fields=[""bbox_ids""])`
`augumentor = A.Compose(transforms, **params)`
`auged = augumentor(image=image, bboxes=bboxes, bbox_ids=np.arange(len(bboxes)), masks=...)`

Then you could use `auged[""bbox_ids""]` to filter masks and keypoints.",get coco could use filter,issue,negative,neutral,neutral,neutral,neutral,neutral
543373007,"It would be really nice to have this feature for instance segmentation. I had to set min_area and min_visibility as a negative value to keep all the boxes, since I have no way to figure out which boxes get removed.",would really nice feature instance segmentation set negative value keep since way figure get removed,issue,negative,positive,positive,positive,positive,positive
542720593,"Hi, I would like to work on this. How do you want the docstrings to be updated?",hi would like work want,issue,negative,neutral,neutral,neutral,neutral,neutral
542281313,"Maybe i'm wrong, but if you look closely at
```
angle = math.atan2(-s, c)
angle = math.atan2(s, -c)
```
will result in the same angle with up to `pi`
There will be not a horizontal flip in hflip, but vertical, imho",maybe wrong look closely angle angle result angle pi horizontal flip vertical,issue,negative,negative,negative,negative,negative,negative
542278301,"OMG, it is really my fail. I did not read this code carefully. The code is right, but a little bit overcomplicated",really fail read code carefully code right little bit,issue,negative,negative,negative,negative,negative,negative
542266460,"Hello!
My proposition for this issue is the following:
Instead of this:
```
    c = math.cos(angle)
    s = math.sin(angle)
    angle = math.atan2(-s, c)
```
1. For VerticalFlip we need to mirror the angle, right? Why not just do `angle = -angle`?
2. Same way of thinking for HorizontalFlip, but a little different formula: `angle = math.pi - angle`
3. Flip is a composition of vflip and hflip, it LGTM and should be correct after changes 1 & 2.

If it's needed we can add angle normalization, like put the angle in range `[0, 2*pi]`
I guess such issue already exists: https://github.com/albu/albumentations/issues/411",hello proposition issue following instead angle angle angle need mirror angle right angle way thinking little different formula angle angle flip composition correct add angle normalization like put angle range pi guess issue already,issue,negative,positive,neutral,neutral,positive,positive
542195840,"Sorry, I found the solution like 5 minutes later. Just in case anyone else is wondering, here is the minimal example.


```python
from albumentations import (
    HorizontalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,
    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,
    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine,
    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose
)

def strong_aug(p=0.5):
    return Compose([
        RandomRotate90(),
        Flip(),
        Transpose(),
        OneOf([
            IAAAdditiveGaussianNoise(),
            GaussNoise(),
        ], p=0.2),
        OneOf([
            MotionBlur(p=0.2),
            MedianBlur(blur_limit=3, p=0.1),
            Blur(blur_limit=3, p=0.1),
        ], p=0.2),
        ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),
        OneOf([
            OpticalDistortion(p=0.3),
            GridDistortion(p=0.1),
            IAAPiecewiseAffine(p=0.3),
        ], p=0.2),
        OneOf([
            CLAHE(clip_limit=2),
            IAASharpen(),
            IAAEmboss(),
            RandomBrightnessContrast(),
        ], p=0.3),
        HueSaturationValue(p=0.3),
    ], p=p,
    additional_targets={""image2"" : ""image""})

import numpy as np
from skimage import data
from matplotlib import pyplot as plt

image1 = data.astronaut().astype(np.uint8)
image2 = data.astronaut().astype(np.uint8)
aug_input = {""image"" : image1, ""image2"" : image2}
aug = strong_aug(0.9)
augmented_data = aug(**aug_input)
i1 = augmented_data[""image""]
i2 = augmented_data[""image2""]

plt.subplot(121)
plt.imshow(i1)
plt.subplot(122)
plt.imshow(i2)
```",sorry found solution like later case anyone else wondering minimal example python import transpose blur flip compose return compose flip transpose blur image image import import data import image image image image image image image image,issue,positive,negative,negative,negative,negative,negative
541354932,"Hi @Sriharsha-hatwar `convert_keypoint_from_albumentations` is internal function.
 It is used in `Compose` method.
Stacktrace looks like this:
1. We are create transform and call it:
```
transform = albumentations.Compose(transforms)
result = transform(image=image, keypoints=keypoints)
```
2. [`__call__`](https://github.com/albu/albumentations/blob/ff1a17f52f363a3901f2b7a2d720b7745e1c3bc4/albumentations/core/composition.py#L163) method of Compose find and call `convert_keypoint_to_albumentations` in [`preprocess`](https://github.com/albu/albumentations/blob/ff1a17f52f363a3901f2b7a2d720b7745e1c3bc4/albumentations/core/composition.py#L174) method. 
3. After apply transform called [`postprocess`](p.postprocess(data)).
`postprocess` method find [`convert_keypoint_from_albumentations`](https://github.com/albu/albumentations/blob/ff1a17f52f363a3901f2b7a2d720b7745e1c3bc4/albumentations/core/utils.py#L65) and call it.

If you does not understand, write about this, and I will try to explain it better.
",hi internal function used compose method like create transform call transform result transform method compose find call method apply transform data method find call understand write try explain better,issue,positive,positive,positive,positive,positive,positive
541314288,"Hi @Dipet , would definitely like to work on this, need to understand a bit about `convert_keypoint_from_albumentations` is there any usage example of this.  ",hi would definitely like work need understand bit usage example,issue,positive,neutral,neutral,neutral,neutral,neutral
541215060,"i have solved my problem,thanks @Dipet 
i found terminal option in homepage",problem thanks found terminal option,issue,negative,positive,positive,positive,positive,positive
541212730,You may try write `!sudo apt-get install libxrender1` from jupyter. To execute this command you need administrator privileges on this system.,may try write install execute command need administrator system,issue,negative,neutral,neutral,neutral,neutral,neutral
541198247,"i don't find any sudo installing option in paperspace,i am running jupyter notebook and i can do pip install commands there.any help please? @Dipet ",find option running notebook pip install help please,issue,positive,neutral,neutral,neutral,neutral,neutral
541171698,"Maybe in your system does not installed libXrender. Try install it.
`sudo apt-get install libxrender1`",maybe system try install install,issue,negative,neutral,neutral,neutral,neutral,neutral
541170390,"Try this command to upgrade library:
`!pip install -U git+https://github.com/albu/albumentations --no-cache-dir`",try command upgrade library pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
540477415,"https://github.com/albu/albumentations/issues/290
https://github.com/albu/albumentations/issues/180
I'm not sure if we fixed it.

Need additional testing for all transforms on grayscale images that they work as expected because initially we didn't work on grayscale (and I thought noone made grayscale support)",sure fixed need additional testing work initially work thought made support,issue,positive,positive,positive,positive,positive,positive
540458365,"But I do not sure that current PR version is right, because we provide augmentations and I think they should not change images shapes without explicit directions.
I think  we need leave `preserve_channels=True`, so that the default behavior will not change.",sure current version right provide think change without explicit think need leave default behavior change,issue,negative,positive,positive,positive,positive,positive
540456038,"As I can see Flips works well with Grayscale images.
If we provide Grayscale image into Compose pipeline it is does not converts to 3 channel.
As I know all our transforms can work with grayscale images, except something special, like `ChannelShuffle`",see work well provide image compose pipeline channel know work except something special like,issue,positive,positive,positive,positive,positive,positive
540405023,I don't think it's common so prefer not to add `MaskParams`. You can make it as new transform,think common prefer add make new transform,issue,negative,negative,neutral,neutral,negative,negative
540193355,"Things that I typically do with masks as a part of the postprocessing step:

1. Check is `mask.sum() < threshold` and zero the mask out if it is the case.
2. Pick `N` largest components.
3. Zero out all connected components in the mask that are smaller than `min_area`.
4. Fill holes in the mask.

We can implement them as `MaskParams`. But we can also have `LambdaMask` transform that will work in a similar way as `Lambda` but on masks.",typically part step check threshold zero mask case pick zero connected mask smaller fill mask implement also transform work similar way lambda,issue,negative,negative,neutral,neutral,negative,negative
540189223,"I think we can add `minimal_mask_size` and `max_connected_components` as `MaskParams` to `Compose` class, by analogy with `BboxParams` and `KepointParams`.

* Minimal mask size - count of not null pixels on image for each masks channel.
*  MaxConnectedComponent we can compute using OpenCV `connectedComponents` function

**Related files**
* `albumentations/core/composition.py`

@ternaus what you think about it?",think add compose class analogy minimal mask size count null image channel compute function related think,issue,negative,negative,neutral,neutral,negative,negative
540181817,At last version works properly.,last version work properly,issue,negative,neutral,neutral,neutral,neutral,neutral
540174259,"Current version of this transforms does not change keypoints angle.
Example how it is implements you can look at `Transpose` transform.

**Related files**
* `albumentations/augmentations/transforms.py`
* `albumentations/augmentations/functional.py`",current version change angle example look transpose transform related,issue,negative,neutral,neutral,neutral,neutral,neutral
540169973,"**Description**
For implement this feature we need remove hardcoded `INTER_NEAREST` from `DualTransform`.

At all spatial transforms, that change image size (like `Resize`) and/or has `interpolation` flag, implement ability to set interpolation mode in `apply_to_mask` method if it exists, or in `apply` method otherwise.

**Related files**
* `albumentations/core/transforms_interface.py`
*  `albumentations/augmentations/transforms.py`",description implement feature need remove spatial change image size like resize interpolation flag implement ability set interpolation mode method apply method otherwise related,issue,positive,neutral,neutral,neutral,neutral,neutral
539902400,Seems that in latest versions this bug is fixed with [_maybe_process_in_chunks](https://github.com/albu/albumentations/blob/master/albumentations/augmentations/functional.py#L154) method,latest bug fixed method,issue,negative,positive,positive,positive,positive,positive
539701624,"Also, I do not see transforms:

```
        Posterize(),
        Solarize(),
        Equalize(),
```",also see posterize solarize equalize,issue,negative,neutral,neutral,neutral,neutral,neutral
539696628,"benchmark uses functional api, so need set this flag",functional need set flag,issue,negative,neutral,neutral,neutral,neutral,neutral
539681339,`birghtness_by_max=True` is the default behavior. I believe it is picked automatically.,default behavior believe picked automatically,issue,negative,neutral,neutral,neutral,neutral,neutral
539672453,"Need update Brightness - `albumentations.brightness_contrast_adjust(img, beta=0.5, beta_by_max=True)` - it is new behavior",need update brightness new behavior,issue,negative,positive,positive,positive,positive,positive
538791766,"Looks great! 

But I would also add link from Readme to this doc from the section https://github.com/albu/albumentations#citing",great would also add link doc section,issue,positive,positive,positive,positive,positive,positive
538704865,Sorry for following up late - I added custom transforms that take a lot of time hence caching made sense. That said I understand this isn't the main objective of the library.,sorry following late added custom take lot time hence made sense said understand main objective library,issue,negative,negative,negative,negative,negative,negative
538675511,"these limits are not strictly int limits. we need to clarify it in docs, thanks.

btw uniform returns float but we cast it to the needed dtype (int or float)",strictly need clarify thanks uniform float cast float,issue,negative,positive,positive,positive,positive,positive
538668231,"if you want to use albumentations with torchvisions without creating a custom dataloader
that's what i did and passed it to torchvision dataloader and it worked
```python
def get_training_augmentation():
    train_transform = [
        albu.Resize(32 , 32)
    ]
    transforms =  albu.Compose(train_transform)
    return lambda img:transforms(image=np.array(img))
```",want use without custom worked python return lambda,issue,negative,neutral,neutral,neutral,neutral,neutral
538590909,"I would prefer second option.

We already have two `RandomSized` transforms. Would prefer to avoid generating third :)",would prefer second option already two would prefer avoid generating third,issue,negative,neutral,neutral,neutral,neutral,neutral
538537088,"okay, let's just make it `always_apply` by default, it should be fine",let make default fine,issue,negative,positive,positive,positive,positive,positive
538507663,We can use `**kwargs` without passing it to super constructor,use without passing super constructor,issue,negative,positive,positive,positive,positive,positive
538477836,I think we shouldn't give ability to not apply totensor.. maybe fix it from the other side?,think give ability apply maybe fix side,issue,negative,neutral,neutral,neutral,neutral,neutral
538307807,"To be honest I agree with @ternaus. I think, If we need this type of transform I see 2 options how we can do this:
1. Create new transform like RandomSizedCropWithPad;
2. Add flag `pad_if_need` or `accept_large_crop` to transforms that uses `random_crop`",honest agree think need type transform see create new transform like add flag,issue,positive,positive,positive,positive,positive,positive
538304919,"It is assumed that we use random transforms with a lot of random images, so we we do not need cache this images, because on every call we get different image. Also cache does not work well with multiprocessing.
If you need cache images, you can cache it in your dataset.",assumed use random lot random need cache every call get different image also cache work well need cache cache,issue,negative,negative,negative,negative,negative,negative
538299311,"Unclear and unexpected padding may be a source of bugs.
If you need this type of transform you can create [Lambda](https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.Lambda) transform with this changes.",unclear unexpected padding may source need type transform create lambda transform,issue,negative,positive,neutral,neutral,positive,positive
538203424,"@ternaus
Thanks,how to handle three images case,such as unet which has a triplet (image,mask,weight,map) need to be performed identical spatial transformation.",thanks handle three case triplet image mask weight map need identical spatial transformation,issue,negative,positive,positive,positive,positive,positive
538091984,"I do not support these changes. Unclear and unexpected padding may be a source of bugs.

For Crops, we have an Exception. We may update it and recommend PadIfNeeded to be used before the crop function to make it more explicit.

For RandomSizedCrop we should probably make crop smaller if it fits into the proposed range of the transformation and throw an Exception otherwise.",support unclear unexpected padding may source exception may update recommend used crop function make explicit probably make crop smaller range transformation throw exception otherwise,issue,negative,positive,neutral,neutral,positive,positive
537992100,"@AlexLuya 


```
transformed = SpatialTransforms(image=image, mask=mask)

image = transformed['image']
mask = transformed['mask']
```
will do the job. We already took care of all random seeds that are involved.

As @albu pointed out, it may help to check examples in notebooks.
",image mask job already took care random involved pointed may help check,issue,positive,negative,negative,negative,negative,negative
536849473,"What is the use case?

For images that are not too large, it would not create a big advantage.",use case large would create big advantage,issue,negative,positive,positive,positive,positive,positive
535570901,"Thank you for your PR!

It looks like you need to add one empty line between names and `code, solution` to make formatting consistent.",thank like need add one empty line code solution make consistent,issue,positive,positive,neutral,neutral,positive,positive
535428746,"Sorry this is a mistake on my part, my labels didn't have the same length as my bounding boxes. Though it would be good to warn the user that this is happening",sorry mistake part length bounding though would good warn user happening,issue,negative,positive,neutral,neutral,positive,positive
535212436,"```
class SignHorFlip(HorizontalFlip):
    def __init__(self, label_flipping={'turn_left': 'turn_right', 'turn_right': 'turn_left'}, always_apply=False, p=1.):
        super(SignHorFlip, self).__init__(always_apply=always_apply, p=p)
        self.label_flipping = label_flipping

    def apply_to_label(self, label, **params):
        return self.label_flipping.get(label, label)

    @property
    def targets(self):
        return {
            'image': self.apply,
            'label': self.apply_to_label
        }
```",class self super self self label return label label property self return,issue,positive,positive,positive,positive,positive,positive
535209974,It would be nice to add a link to the notebook to the readme.,would nice add link notebook,issue,negative,positive,positive,positive,positive,positive
535199816,"I've added an example how new replay mode works. https://github.com/albu/albumentations/blob/deterministic_poc/notebooks/replay.ipynb 

As for your question - you probably need to add new class which will have a method `apply_to_label`",added example new replay mode work question probably need add new class method,issue,negative,positive,positive,positive,positive,positive
535127066,"See discussion under PR #382 
Probably required functionality will be available after merge of PR #350",see discussion probably functionality available merge,issue,negative,positive,positive,positive,positive,positive
535098285,"Don't worry, this PR is a really good idea, we thought about it for a long time.

What's done in #350 :
- https://github.com/albu/albumentations/pull/350/files#diff-2f0d114a776157b2c551dc907cec3253R67 writes transformation params to results dict.
- https://github.com/albu/albumentations/pull/350/files#diff-c3271807771eaa1e34cfc16dd8660db5R342 fills `applied` field. 

So you receive all applied augmentations' params and flag (is_applied) for every image in resulting dict.
We have to use another class `ReplayCompose` because if you pass dict to `ToTensor` and `DataLoader` in pytorch - all the dimensions of all args have to be the same. I'm not sure if dimensions of resulting dicts will be the same and how it will interact with torch.

You can just replace normal `Compose` by `ReplayCompose`. The only limitation right now - it cannot be serialized.

I'll add example of usage to #350 to make it more clear.

If you have ideas on how to improve #350 - you are really welcomed.

Cheers",worry really good idea thought long time done transformation applied field receive applied flag every image resulting use another class pas sure resulting interact torch replace normal compose limitation right add example usage make clear improve really,issue,positive,positive,positive,positive,positive,positive
535069443,"I've looked at #350. I don't see in what way it is the same. I understand it provides a possibility to replay transforms with the same parameters. It is not what I am going to get. See description in #381. 

Multiprocess execution really can be a problem as far as in this case the same transform can be run with different parameters in different threads, so applied parameters are not a property of a Compose object but a property of a specific (parallel) call to it. Also, my approach can be incorrect if augmentation contains OneOf or smth. similar. Better approach is to store fired transforms and their parameters and return it in an augmentation results dict. This approach is partly implemented in #350, but it is invoked only in a replay mode. Why do you need a special ReplayCompose class there? Why not to store parameters of invoked transforms always and not to return them with other results?

So I agree that this PR is not a good idea and should be declined, I will return to this problem when #350 will be merged.",see way understand possibility replay going get see description execution really problem far case transform run different different applied property compose object property specific parallel call also approach incorrect augmentation similar better approach store fired return augmentation approach partly replay mode need special class store always return agree good idea declined return problem,issue,negative,positive,positive,positive,positive,positive
534906667,"Properties `was_applied` and `params` were added to all transforms to keep an information about was this transform applied and with what parameters.
Usage examle:
```python
aug_res = self.albumentations(image=img, bboxes=rects)
aug_bboxes = aug_res['bboxes']
for t in self.albumentations.transforms:
    if isinstance(t, T.VerticalFlip) and t.was_applied:
        aug_bboxes = [vflip_rect_label(b) for b in aug_bboxes]
    if isinstance(t, T.HorizontalFlip) and t.was_applied:
        aug_bboxes = [hflip_rect_label(b) for b in aug_bboxes]
```
",added keep information transform applied usage python,issue,negative,neutral,neutral,neutral,neutral,neutral
533909282,close due to harsh merge conflicts,close due harsh merge,issue,negative,negative,negative,negative,negative,negative
533907882,"> Why would you prefer to skip helper code like benchmark, etc?

You are right. I think it is better to apply isort to those files as well if we decide to use it someday.

> Should we close it as isort and black do not work together yet.

Yep, closing this pull request for now.

",would prefer skip helper code like right think better apply well decide use someday close black work together yet yep pull request,issue,positive,positive,positive,positive,positive,positive
533906912,Should we close it as `isort`  and `black` do not work together yet.,close black work together yet,issue,negative,negative,negative,negative,negative,negative
533846086,"1. Added Pre-commit hook for black.
2. Moved text with contributions to contributions.rst
3. Fixed First/Last names in docs.
4. Pre-commit hook was enabled while [3] => conf.py was updated to black standards.",added hook black text fixed hook black,issue,negative,negative,neutral,neutral,negative,negative
533775634,Very interested in extending cutout to be applied to masks. Seems like it would be trivial to implement. Is there any reason for this not already being an option? I can spend some time implementing it if need be and send a pull request,interested extending cutout applied like would trivial implement reason already option spend time need send pull request,issue,positive,positive,positive,positive,positive,positive
533693919,"Thanks for fixing this. It seems to me that the code in `get_params_dependent_on_targets` assumes that the bboxes are in the `albumentations` format. Is that intentional? I got an unexpected result, because I passed bboxes in a different format, but there is nothing to warn the user against doing that.

Maybe this could be fixed with a call to `convert_bboxes_to_albumentations` ?",thanks fixing code format intentional got unexpected result different format nothing warn user maybe could fixed call,issue,negative,positive,neutral,neutral,positive,positive
533666976,"Why would you prefer to skip helper code like benchmark, etc?",would prefer skip helper code like,issue,positive,neutral,neutral,neutral,neutral,neutral
533187606,"From the documentation, contribution instructions are:

> Clone the repository:
> `git clone git@github.com:albu/albumentations.git`

So, updating that portion of the documentation to make it clear that you want users to fork and then submit PRs.",documentation contribution clone repository git clone git portion documentation make clear want fork submit,issue,negative,positive,positive,positive,positive,positive
533184170,"I believe `fork -> modify -> PR` is a standard process for contributing to open-source projects.

Which part is confusing? ",believe fork modify standard process part,issue,negative,neutral,neutral,neutral,neutral,neutral
533133787,"I'm pretty sure the fix here is:

Currently:
```
~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/functional.py in __process_fn(img)
    157                 chunk = process_fn(chunk, **kwargs)
    158                 chunks.append(chunk)
--> 159             img = np.concatenate(chunks, axis=2)
    160         else:
    161             img = process_fn(img, **kwargs)
```

Becomes:
```
~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/functional.py in __process_fn(img)
    157                 chunk = process_fn(chunk, **kwargs)
    158                 chunks.append(chunk)
--> 159             img = np.dstack(chunks)
    160         else:
    161             img = process_fn(img, **kwargs)
```",pretty sure fix currently chunk chunk chunk else becomes chunk chunk chunk else,issue,positive,positive,positive,positive,positive,positive
533130387,"This'll affect all the augmentations that use _maybe_process_in_chunks:  
- rotate
- resize
- shift_scale_rotate
- blur
- gaussian_blur
- median_blur
- motion_blur
- grid_distortion
- elastic_transform
- elastic_transform_approx",affect use rotate resize blur,issue,negative,neutral,neutral,neutral,neutral,neutral
533127770,"Here's a minimal reproducible example:

```
import albumentations
import bumpy as np

im = np.ones((128, 128, 3))
mask = np.ones((128, 128, 5))
data = {'image': im, 'mask': mask}

apply_augmentations = albumentations.Compose([
    albumentations.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=45, p=1),
])
augmented = apply_augmentations(**data)
```

With traceback:
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-507-5a7c15e6062f> in <module>
      7     albumentations.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=45, p=1),
      8 ])
----> 9 augmented = apply_augmentations(**data)

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/core/composition.py in __call__(self, force_apply, **data)
    156                     p.preprocess(data)
    157 
--> 158             data = t(force_apply=force_apply, **data)
    159 
    160             if dual_start_end is not None and idx == dual_start_end[1]:

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/core/transforms_interface.py in __call__(self, force_apply, **kwargs)
     63                     target_function = self._get_target_function(key)
     64                     target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
---> 65                     res[key] = target_function(arg, **dict(params, **target_dependencies))
     66                 else:
     67                     res[key] = None

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/transforms.py in apply_to_mask(self, img, angle, scale, dx, dy, **params)
    515 
    516     def apply_to_mask(self, img, angle=0, scale=0, dx=0, dy=0, **params):
--> 517         return F.shift_scale_rotate(img, angle, scale, dx, dy, cv2.INTER_NEAREST, self.border_mode, self.mask_value)
    518 
    519     def apply_to_keypoint(self, keypoint, angle=0, scale=0, dx=0, dy=0, rows=0, cols=0, interpolation=cv2.INTER_LINEAR,

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/functional.py in wrapped_function(img, *args, **kwargs)
     56     def wrapped_function(img, *args, **kwargs):
     57         shape = img.shape
---> 58         result = func(img, *args, **kwargs)
     59         if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:
     60             result = np.expand_dims(result, axis=-1)

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/functional.py in shift_scale_rotate(img, angle, scale, dx, dy, interpolation, border_mode, value)
    199     warp_affine_fn = _maybe_process_in_chunks(cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation,
    200                                               borderMode=border_mode, borderValue=value)
--> 201     return warp_affine_fn(img)
    202 
    203 

~/miniconda3/envs/asu/lib/python3.7/site-packages/albumentations/augmentations/functional.py in __process_fn(img)
    157                 chunk = process_fn(chunk, **kwargs)
    158                 chunks.append(chunk)
--> 159             img = np.concatenate(chunks, axis=2)
    160         else:
    161             img = process_fn(img, **kwargs)

ValueError: all the input arrays must have same number of dimensions
```",minimal reproducible example import import bumpy mask data mask augmented data recent call last module augmented data self data data data data none self key key key else key none self angle scale self return angle scale self shape result shape shape result result angle scale interpolation value width height return chunk chunk chunk else input must number,issue,negative,negative,neutral,neutral,negative,negative
533100573,"> It would not be better to write
> 
> ```
> if not isinstance(force_apply, (bool, int)):
>     raise TypeError('force_apply must have bool or int type')
> ```
> 
> instead of `assert` ?

So, I used 'assert' because it already used in albumentation for check type of input arguments  (for example: https://github.com/albu/albumentations/blob/master/albumentations/augmentations/functional.py#L403).

But I see that 'raise' also used (for example: https://github.com/albu/albumentations/blob/master/albumentations/augmentations/functional.py#L271).

I think it should be determined in contribution guide.",would better write bool raise must bool type instead assert used already used check type input example see also used example think determined contribution guide,issue,positive,positive,positive,positive,positive,positive
533098149,"Should not this test : https://github.com/albu/albumentations/blob/8ba400e3f048b5f596670046e25ed90650065f8d/tests/test_pytorch.py#L19-L31
 fail if `ToTensorV2` would not be applied to additional targets or am I missing something ?",test fail would applied additional missing something,issue,negative,negative,negative,negative,negative,negative
533096583,"It would not be better to write 
```
if not isinstance(force_apply, (bool, int)):
    raise TypeError('force_apply must have bool or int type')
```
instead of `assert` ?

*Edit:* a known argument against using `assert` : 
```
> PYTHONOPTIMIZE=1 python -c ""assert False""
> PYTHONOPTIMIZE=0 python -c ""assert False""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
AssertionError
```
",would better write bool raise must bool type instead assert edit known argument assert python assert false python assert false recent call last file string line module,issue,negative,negative,neutral,neutral,negative,negative
533095784,"Apologies. Further testing shows that I'm able to do this with RandomFog only, that there's no interactive effect here.

Reproduce:
```
    return albumentations.Compose([
        albumentations.RandomFog(fog_coef_lower=0, fog_coef_upper=1, alpha_coef=0.08, p=0.5),
    ])
```",testing able interactive effect reproduce return,issue,negative,positive,positive,positive,positive,positive
532551195,"It`s work `A.Blur(p=1, blur_limit = (9,9))`.


Correct a document?

> `class albumentations.augmentations.transforms.Blur(blur_limit=7, always_apply=False, p=0.5)`
> `blur_limit = (?,?)`
",work correct document class,issue,negative,neutral,neutral,neutral,neutral,neutral
532549460,"In my case all works.
```
image = np.random.randint(0, 256, (256, 256, 3), dtype=np.uint8)
result = []
aug = Blur(p=1, blur_limit = 9)
for i in tqdm(range(10000)):
    img = aug(image=image)['image']
    result.append(int(np.all(image == img)))
result = np.array(result)
print(result.sum(), len(result))
```

How you check that images are not blurry?",case work image result blur range image result result print result check blurry,issue,negative,neutral,neutral,neutral,neutral,neutral
532548150,"Hi. Can you please clarify what is the meaning of `80% response` in this context? I don't think there is something wrong with probability, perhaps what you want is `A.Blur(p=1, blur_limit = (9,9))`. ",hi please clarify meaning response context think something wrong probability perhaps want,issue,negative,negative,negative,negative,negative,negative
532475085,"---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-16-be4664df3575> in <module>
      2 dataset = Dataset(root_dir=""../data/train_images"", ann_file=""train.csv"", dataframe=train_df, num_classes=4, augmentation=get_training_augmentation())
      3 
----> 4 image, mask = dataset[0] # get some sample
      5 visualize(image=image, background=mask[:,:,0], class1=mask[:,:,1], class2=mask[:,:,2], class3=mask[:,:,3], class4=mask[:,:,4])

<ipython-input-5-4abd81e4a173> in __getitem__(self, i)
     85         # apply augmentations
     86         if self.augmentation:
---> 87             sample = self.augmentation(image=image, mask=mask)
     88             image, mask = sample['image'], sample['mask']
     89 

/mnt/wenhongdiao/.pylib/3/albumentations/core/composition.py in __call__(self, force_apply, **data)
    156                     p.preprocess(data)
    157 
--> 158             data = t(force_apply=force_apply, **data)
    159 
    160             if dual_start_end is not None and idx == dual_start_end[1]:

/mnt/wenhongdiao/.pylib/3/albumentations/core/transforms_interface.py in __call__(self, force_apply, **kwargs)
     63                     target_function = self._get_target_function(key)
     64                     target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
---> 65                     res[key] = target_function(arg, **dict(params, **target_dependencies))
     66                 else:
     67                     res[key] = None

/mnt/wenhongdiao/.pylib/3/albumentations/augmentations/transforms.py in apply_to_mask(self, img, angle, **params)
    414 
    415     def apply_to_mask(self, img, angle=0, **params):
--> 416         return F.rotate(img, angle, cv2.INTER_NEAREST, self.border_mode, self.mask_value)
    417 
    418     def get_params(self):

/mnt/wenhongdiao/.pylib/3/albumentations/augmentations/functional.py in wrapped_function(img, *args, **kwargs)
     56     def wrapped_function(img, *args, **kwargs):
     57         shape = img.shape
---> 58         result = func(img, *args, **kwargs)
     59         if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:
     60             result = np.expand_dims(result, axis=-1)

/mnt/wenhongdiao/.pylib/3/albumentations/augmentations/functional.py in rotate(img, angle, interpolation, border_mode, value)
    172     warp_fn = _maybe_process_in_chunks(cv2.warpAffine, M=matrix, dsize=(width, height), flags=interpolation,
    173                                        borderMode=border_mode, borderValue=value)
--> 174     return warp_fn(img)
    175 
    176 

/mnt/wenhongdiao/.pylib/3/albumentations/augmentations/functional.py in __process_fn(img)
    157                 chunk = process_fn(chunk, **kwargs)
    158                 chunks.append(chunk)
--> 159             img = np.concatenate(chunks, axis=2)
    160         else:
    161             img = process_fn(img, **kwargs)

ValueError: all the input arrays must have same number of dimensions",recent call last module image mask get sample visualize self apply sample image mask sample sample self data data data data none self key key key else key none self angle self return angle self shape result shape shape result result rotate angle interpolation value width height return chunk chunk chunk else input must number,issue,negative,neutral,neutral,neutral,neutral,neutral
532388193,"I can not reproduce this problem.
Can you give traceback for this error?",reproduce problem give error,issue,negative,neutral,neutral,neutral,neutral,neutral
532250083,seems like a problem is in your data format. please send us few data points (boxes for one failing image should be enough). Everything is not valid without preprocessing.,like problem data format please send u data one failing image enough everything valid without,issue,negative,neutral,neutral,neutral,neutral,neutral
532248435,"Currently fixed it by changing [this](https://github.com/albu/albumentations/blob/master/albumentations/core/composition.py#L156) line from
`p.preprocess(data)`
to
```
try:
    p.preprocess(data)
except:
    print('data:', data)
```
This is костыль, and it doesn't augment some amount of images, but it works :crab:.",currently fixed line data try data except print data augment amount work crab,issue,negative,positive,neutral,neutral,positive,positive
532194876,"My dataset has some bboxes with height or width <1, >0. Therefore I modified them so they have W>1, H>1. 
After this training doesn't fail with albumentation: `HorizontalFlip` (UPDATE: no, it fails also but later). But failed with adding `ShiftScaleRotate`.",height width therefore training fail update also later,issue,negative,negative,negative,negative,negative,negative
532191867,"Hmm, when I remove all albumentations except HFlip, I still get such Error, but without any albumentations everything is OK.",remove except still get error without everything,issue,negative,neutral,neutral,neutral,neutral,neutral
531572419,"Version from PyPi does not contains this transform. You need get latest version from GitHub
Use this code for upgrade your current version
```
pip install -U git+https://github.com/albu/albumentations
```
Or this code for install latest version from GitHub:
```
pip install git+https://github.com/albu/albumentations.git --no-cache-dir
```",version transform need get latest version use code upgrade current version pip install code install latest version pip install,issue,negative,positive,positive,positive,positive,positive
531527969,"Try install last version from git repo:
```
pip install git+https://github.com/albu/albumentations.git --no-cache-dir
```",try install last version git pip install,issue,negative,neutral,neutral,neutral,neutral,neutral
531482582,Thanks for your answer. This has worked out perfectly! (I was thinking of extending the pipeline to automatically cache intermediate results but didn't find a good/obvious way to do so. So I'm splitting the pipeline into two portions for now.),thanks answer worked perfectly thinking extending pipeline automatically cache intermediate find way splitting pipeline two,issue,positive,positive,positive,positive,positive,positive
530990684,"I see now in the current torchvision master
https://github.com/pytorch/vision/blob/master/torchvision/transforms/transforms.py#L626
They have `int(round(` which I don't have strong opinions about.",see current master round strong,issue,positive,positive,neutral,neutral,positive,positive
530867905,I’d suggest to use this solution to rely on official opencv package with fallback to headless. https://stackoverflow.com/questions/49222824/make-an-either-or-distinction-for-install-requires-in-setup-py/49338206#49338206,suggest use solution rely official package fallback headless,issue,negative,neutral,neutral,neutral,neutral,neutral
530823059,@apatsekin Our CI tool in `tools/make_transforms_docs.py` expects that README will contain info for those two deprecated functions. To make CI tests pass could you please copy code from https://github.com/albu/albumentations/commit/090e55d49a510fd96ebd7afc60d37497d2e10116 to your branch (e.g. by running `git cherry-pick 090e55d49a510fd96ebd7afc60d37497d2e10116`). After that CI tests should pass fine.,tool contain two make pas could please copy code branch running git pas fine,issue,negative,positive,positive,positive,positive,positive
530782194,"@apatsekin one last thing - can you please update docs by running `tools/make_transforms_docs.py` ?
Otherwise, CI will not pass.",one last thing please update running otherwise pas,issue,negative,neutral,neutral,neutral,neutral,neutral
530527766,"@Dipet could you please resolve conflicts, please?",could please resolve please,issue,positive,neutral,neutral,neutral,neutral,neutral
529290422,I believe we need to add min/max values of all input parameters and check if input value is in allowed range. https://github.com/albu/albumentations/issues/337,believe need add input check input value range,issue,negative,neutral,neutral,neutral,neutral,neutral
527367531,first option does not work at all. so from two evils second is better. I just wonder if there are other better options..,first option work two second better wonder better,issue,positive,positive,positive,positive,positive,positive
527365131,"@albu 

> second option will force developers to replace ToTensor twice. first in this release - by ToTensorV2 and in 0.4.0 ToTensorV2 -> ToTensor. it's weird

I agree, but like that we are keeping BC. Assuming that users mostly use current version `ToTensor`, they are just become aware of deprecation and replacement. Others (like me) do not use `ToTensor` but their own version of it and they will just switch to `A.ToTensorV2` and since 0.4.0 to `A.ToTensor()`.

Otherwise, what do you think about the 1st option ?
",second option force replace twice first release weird agree like keeping assuming mostly use current version become aware deprecation replacement like use version switch since otherwise think st option,issue,positive,positive,neutral,neutral,positive,positive
527362492,second option will force developers to replace ToTensor twice. first in this release - by ToTensorV2 and in 0.4.0 ToTensorV2 -> ToTensor. it's weird,second option force replace twice first release weird,issue,negative,negative,neutral,neutral,negative,negative
527269815,I would prefer the second option.,would prefer second option,issue,negative,neutral,neutral,neutral,neutral,neutral
527241765,"@albu how do you prefer to keep the old behavior : 
- Old `ToTensor` -> `ToTensor_deprecated` with warning about removal of the old behaviour
- New `ToTensor`

or 

- Old `ToTensor` -> `ToTensor` with warning about removal of the old behaviour
- New `ToTensor` -> `ToTensorV2` and rename it `ToTensor` once in v0.4.0


",prefer keep old behavior old warning removal old behaviour new old warning removal old behaviour new rename,issue,negative,positive,positive,positive,positive,positive
527239576,looks fine but we still have to preserve old behavior for all these who uses ToTensor,fine still preserve old behavior,issue,negative,positive,positive,positive,positive,positive
527198358,"It looks like Torchvision deprecated `RandomSizedCrop` in favor of `RandomResizedCrop`

I would love to see:
1. Right now (0.3) to have `RandomSizedCrop` that we had earlier.
2. `RandomResizedCrop` with the API and functionality of what torchvision has https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.RandomResizedCrop
3. Deprecation warning in `RandomSizedCrop`

=> in 0.4 we will match the behavior of Torchvision, keeping users informed of the future breaking changes.

So, to me, looks like adding second transform `RandomResizedCrop` is the best solution.",like favor would love see right functionality deprecation warning match behavior keeping informed future breaking like second transform best solution,issue,positive,positive,positive,positive,positive,positive
527162987,"> I wonder if just adding new crop with different name could be better and safer solution..

I agree. I let you decide between maintainers what names to choose for the new class. ",wonder new crop different name could better solution agree let decide choose new class,issue,positive,positive,positive,positive,positive,positive
527162176,"> I refactored transforms to avoid some deprecation warnings with `RandomSizedCropTorchVision` and updated input args of `RandomSizedCrop`.
> 
> @albu Are you ok with that `RandomSizedCrop` is a **method** and not a class ?

it could break documentation and probably something else.

I wonder if just adding new crop with different name could be better and safer solution..",avoid deprecation input method class could break documentation probably something else wonder new crop different name could better solution,issue,negative,positive,positive,positive,positive,positive
527161032,"Would you be interested by the following `ToTensor` implementation : 
```python
class ToTensor(BasicTransform):

    def __init__(self):
        super(ToTensor, self).__init__(always_apply=True)

    @property
    def targets(self):
        return {
            'image': self.apply,
            'mask': self.apply_to_mask
        }

    def apply(self, img, **params):
        return torch.from_numpy(img.transpose(2, 0, 1))

    def apply_to_mask(self, mask, **params):
        return torch.from_numpy(mask)
```",would interested following implementation python class self super self property self return apply self return self mask return mask,issue,positive,positive,positive,positive,positive,positive
527154984,"I refactored transforms to avoid some deprecation warnings with `RandomSizedCropTorchVision` and updated input args of `RandomSizedCrop`.

@albu Are you ok with that `RandomSizedCrop` is a **method** and not a class ?",avoid deprecation input method class,issue,negative,neutral,neutral,neutral,neutral,neutral
527032382,"Benchmark and default behavior are different now. 

Let's delete the changes in the benchmark file. In version 0.4, they will start working automatically.",default behavior different let delete file version start working automatically,issue,negative,neutral,neutral,neutral,neutral,neutral
527012746,"1. I remove clipping from uint8 version (~2 times faster )
2.  Remove mean calculation from image and use dtype maximum.
imgaug [Add](https://github.com/aleju/imgaug/blob/38f305aaa5df833228d95d95de36e77d1a4ac88c/imgaug/augmenters/arithmetic.py#L51) and solt [ImageRandomBrightness](https://github.com/MIPT-Oulu/solt/blob/a6891d4dfa6bfd0d01a4ca1d6e7c03d15468d9c3/solt/transforms/_transforms.py#L1030) transforms use values from your interval but it will be in range [-dtype.max, dtype.max].
For example, we set interval [-25, 25] in this implementation it will be equivalent to [-25/255, 25/255]",remove clipping version time faster remove mean calculation image use maximum add use interval range example set interval implementation equivalent,issue,negative,negative,negative,negative,negative,negative
526967559,"This speedup looks tempting. But we get it because of the change in the algorithm.

Could you please share what was the motivation behind it? Is it the way it is implemented in other libraries?",tempting get change algorithm could please share motivation behind way,issue,positive,negative,negative,negative,negative,negative
525276939,you can write a wrapper around imgaug version or even contribute by adding support of bounding boxes to our elastic transform,write wrapper around version even contribute support bounding elastic transform,issue,negative,neutral,neutral,neutral,neutral,neutral
525258895,"OK, is there a possibility to circumvent or tell the library to use the imgaug version instead ?
Would really require annotation support for me project.",possibility circumvent tell library use version instead would really require annotation support project,issue,negative,positive,positive,positive,positive,positive
525252957,"It's not strongly a wrapper, only few transforms are wrapped from imgaug. Elastic is not one of them, it's our own optimized version",strongly wrapper wrapped elastic one version,issue,negative,positive,positive,positive,positive,positive
524623812,"> [#320 (comment)](https://github.com/albu/albumentations/pull/320#issuecomment-523861825)

@BloodAxe Sorry for the long silence.
1. Pillow and OpenCV get different equalization results, for somebody it may by important, and i think 2 implementations is not a big overhead, because they are simple. Current realization does not use Pillow for transform, I use only numpy and OpenCV functions. Pillow used only in test functions to show thats Pillow and my implementation gets equal results.
2. I use histogram calculations only for equalization by mask case, because OpenCV does not support equalization by mask.",comment sorry long silence pillow get different equalization somebody may important think big overhead simple current realization use pillow transform use pillow used test show thats pillow implementation equal use histogram equalization mask case support equalization mask,issue,negative,negative,neutral,neutral,negative,negative
524607283,"yes, it's undocumented but you can use it like this: https://github.com/albu/albumentations/blob/master/albumentations/core/transforms_interface.py#L58
override `targets_as_params` to pass params from dict to `get_params_dependent_on_targets` (which is the same as `get_params`.
example: https://github.com/albu/albumentations/blob/master/albumentations/augmentations/transforms.py#L639",yes undocumented use like override pas example,issue,positive,neutral,neutral,neutral,neutral,neutral
524552647,"oh sorry, we just merged BboxParams and not yet updated it in pip. thanks for pointing!",oh sorry yet pip thanks pointing,issue,negative,negative,negative,negative,negative,negative
524541589,"Hi @albu 
Thanks for the quick reply! It seems there's an issue when installing from PyPI, as using
!pip install albumentations

gave me the following error:
```
ImportError 
<ipython-input-3-a7c310633abd> in <module>()
----> 1 from albumentations import (
      2     BboxParams,
      3     HorizontalFlip,
      4     VerticalFlip,
      5     Resize,

ImportError: cannot import name 'BboxParams'
```

Afterwards, I used the
!pip install -U git+https://github.com/albu/albumentations
directly and it worked just fine !

So just a head's up for you, as there's either a bug with BboxParams in the version on PyPI or maybe BboxParams is just new feature (and therefore not on the offical readme docs yet as well) ?",hi thanks quick reply issue pip install gave following error module import resize import name afterwards used pip install directly worked fine head either bug version maybe new feature therefore yet well,issue,negative,positive,positive,positive,positive,positive
523907890,Feel free to re-open this issue when you have a proposal draft for the reversal. ,feel free issue proposal draft reversal,issue,positive,positive,positive,positive,positive,positive
523906917,"You are right, although the segmentation problems require inverse transformation of the resulting masks, for pixel-pixel comparison... I will try and come back with a pull request if successful",right although segmentation require inverse transformation resulting comparison try come back pull request successful,issue,positive,positive,positive,positive,positive,positive
523905301,"Definitely an overkill. Also, some transformations are not reversible by definition.
To perform comparison between images, you can store original data and transformed ones and do whatever you want with them.
For TTA you can check https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/tests/test_tta.py",definitely also reversible definition perform comparison store original data whatever want check,issue,positive,positive,positive,positive,positive,positive
523861825,"@Dipet I have two questions related this PR:
1) Do we really need two (PIL and CV) histogram equalization implementations? To me - one is enough. AFAIK, the only place where we use PIL is IAA-based augmentations, but rest of library is PIL-free, and I'd like to keep that.
2) It seems, the code for CV implementation can be simplified by replacing histogram building and LUT by `cv2.equalizeHist` https://docs.opencv.org/master/d6/dc7/group__imgproc__hist.html#ga7e54091f0c937d49bf84152a16f76d6e. Can we use it instead of computing histogram manualy? Unless there is real need to do manual calculations, we should try our best to use library functions.
",two related really need two histogram equalization one enough place use rest library like keep code implementation simplified histogram building lut use instead histogram unless real need manual try best use library,issue,positive,positive,positive,positive,positive,positive
523107215,Maybe we should use `ValueError` instead of a generic `Exception` class for exceptions that we raise when input parameters are incorrect? ,maybe use instead generic exception class raise input incorrect,issue,negative,neutral,neutral,neutral,neutral,neutral
522935194,"Thanks a lot for your fast reply!!! It works. I reinstall the latest one and the code has been updated.
`pip install -U git+https://github.com/albu/albumentations`
Thanks !!!",thanks lot fast reply work reinstall latest one code pip install thanks,issue,positive,positive,positive,positive,positive,positive
522851104,Is there an equivalent way to achieve `Sometimes`? @albu ,equivalent way achieve sometimes,issue,negative,neutral,neutral,neutral,neutral,neutral
522259243,"> but if we want to change it to something - we will change it in only one place and will get two different versions of the same thing. life is like programming :)

Do you propose parsing readme and adding the info to doc or parsing doc to add info to readme? :)

When we will come to a better structure for docs we can make it more `high style`.

Our readme is long, and I am not sure that docs are in the most useful form. But that is what we have.",want change something change one place get two different thing life like propose doc doc add come better structure make high style long sure useful form,issue,positive,positive,positive,positive,positive,positive
522168121,but if we want to change it to something - we will change it in only one place and will get two different versions of the same thing. life is like programming :) ,want change something change one place get two different thing life like,issue,negative,neutral,neutral,neutral,neutral,neutral
522119393,"Professional deformation from software engineering is so strong in you :)

Duplication is not always evil. If we knew that there is an amazing place to put it once so that people would see it => sure, should be mentioned once.

But because we do not know such a great place, we can put it in two ok places.",professional deformation engineering strong duplication always evil knew amazing place put people would see sure know great place put two,issue,positive,positive,positive,positive,positive,positive
522085771,"The experiment showed that not everyone can get to the bottom of the long README.

I believe, that if users are missing it there, we should duplicate it somewhere else and documentation looks like a good place for this.",experiment everyone get bottom long believe missing duplicate somewhere else documentation like good place,issue,negative,positive,positive,positive,positive,positive
521731841,Could you please share an example of the code that will allow reproducing?,could please share example code allow,issue,positive,neutral,neutral,neutral,neutral,neutral
521722443,so why do we need to tell the same thing twice?,need tell thing twice,issue,negative,neutral,neutral,neutral,neutral,neutral
521415483,"var_limit must be >= 0, because function use square from this value",must function use square value,issue,negative,neutral,neutral,neutral,neutral,neutral
521325566,"@albu For bounding boxes, it is undefined unless we will find a meaningful way to define them.",bounding undefined unless find meaningful way define,issue,negative,positive,positive,positive,positive,positive
521266080,"opencv-python-headless doen't contain lastest OpenCV version (previous version had bug: opencv/opencv#14978). So, I can't fix ci errors",doe contain version previous version bug ca fix,issue,negative,negative,negative,negative,negative,negative
521247984,"There is example of transformation. Split the image into tiles by the grid and shuffle them

<img width=""627"" alt=""Example"" src=""https://user-images.githubusercontent.com/29678445/63025090-2b82ec00-beb1-11e9-9881-0d920b345e26.png"">",example transformation split image grid shuffle example,issue,negative,neutral,neutral,neutral,neutral,neutral
521215452,please also provide examples of what transform does,please also provide transform,issue,negative,neutral,neutral,neutral,neutral,neutral
520583437,@Dipet Could you please add a test that shows that this implementation gives the same result as one by the Pillow?,could please add test implementation result one pillow,issue,negative,neutral,neutral,neutral,neutral,neutral
520503234,"please note that library is py27-compatible so it does not support typing.

other problems reported by CI:
```
./albumentations/augmentations/transforms.py:1558:1: D204 1 blank line required after class docstring
./albumentations/augmentations/transforms.py:1558:1: D208 Docstring is over-indented
```",please note library support blank line class,issue,positive,neutral,neutral,neutral,neutral,neutral
519818102,Can we fill the regions with random noise (pixel-level) instead of a value? From the documentation it seems like it just accepts a scalar value to fill in.,fill random noise instead value documentation like scalar value fill,issue,positive,negative,negative,negative,negative,negative
518639477,"`numpy==1.17.0 albumentations==0.3.1`
Same problem
But `pip install --upgrade scikit-image` fix this problem
Thanks to https://stackoverflow.com/questions/54241226/importerror-cannot-import-name-validate-lengths
",problem pip install upgrade fix problem thanks,issue,negative,positive,positive,positive,positive,positive
518308899,Not at the moment. Everything seems to be working on my end though. Thanks for the insight! ,moment everything working end though thanks insight,issue,negative,positive,positive,positive,positive,positive
518039385,"For me: When I tried to update everything to the latest version...
```
The following NEW packages will be INSTALLED:                                                                                                                                                                                                  
                                                                                                                                                                                                                                               
  albumentations     conda-forge/noarch::albumentations-0.3.1-py_0
  imgaug             conda-forge/noarch::imgaug-0.2.9-py_1
                                                                         
```
This bug still exists.
When I do `pip install imgaug==0.2.7`,
Although it has warnings
```
ERROR: albumentations 0.3.1 requires opencv-python-headless, which is not installed.                                                                                      
ERROR: albumentations 0.3.1 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.7 which is incompatible.
```
It no longer causes the problem.
I did modify some of the code in albumentations, but after I changed it back, it still causes the problem. I don't know if anybody else can reproduce it.",tried update everything latest version following new bug still pip install although error error requirement incompatible longer problem modify code back still problem know anybody else reproduce,issue,negative,positive,positive,positive,positive,positive
517395908,Could you please share an example that reproduces the issue?,could please share example issue,issue,positive,neutral,neutral,neutral,neutral,neutral
517344549,"For some reason my mask is not skipping the normalization step. Actually, I'm not sure what is happening with it. Ideally, I'd like both the mask and image to undergo the same transformations that are spatially focused and not colors, etc.. But I'm finding that not to be the case and am not sure if it is normalization.",reason mask skipping normalization step actually sure happening ideally like mask image undergo spatially color finding case sure normalization,issue,positive,positive,positive,positive,positive,positive
517341729,"Oh, what should I use then for porting to pytorch? Just direct conversion torch.tensor(...,dtype=torch.float) ???",oh use direct conversion,issue,negative,positive,neutral,neutral,positive,positive
517181963,"Так и делаю сейчас: сначала общая аугментация всей картинки, потом отдельная аугментация для каждого кропа. 

Пример моего текущего кода:
```
def aug_train1(p=1.0):
    return Compose([
        LongestMaxSize(max_size=768, always_apply=True),
        PadIfNeeded(min_height=768, min_width=768, always_apply=True), 
        RandomBrightnessContrast(),
        GaussNoise(),
        VerticalFlip(p=0.5),
        RandomScale(scale_limit=(0.5,2.0), always_apply=True),
    ], p=p)

def aug_train2(p=1.0):
    return Compose([
        RandomCrop(224, 224, always_apply=True), 
        RandomRotate90(always_apply=True),
        Normalize(),
    ], p=p)

```
Плюс код, который обрабатывает одну картинку несколько раз на подобие Вашего.

Всё это могло бы быть переписано так:
```
def aug_train1(p=1.0):
    return Compose([
        LongestMaxSize(max_size=768, always_apply=True),
        PadIfNeeded(min_height=768, min_width=768, always_apply=True), 
        RandomBrightnessContrast(),
        GaussNoise(),
        VerticalFlip(p=0.5),
        RandomScale(scale_limit=(0.5,2.0), always_apply=True),
        Repeat([
                RandomCrop(224, 224, always_apply=True), 
                RandomRotate90(always_apply=True),
        ], n=8),
        Normalize(),
    ], p=p)
```
ИМХО, нагляднее и удобнее аугментацию картинок полностью сосредоточить в одном месте.

Хотя согласен, что подобное изменение потребует возможно сильных изменений в коде библиотеки.",return compose return compose normalize return compose repeat normalize,issue,negative,neutral,neutral,neutral,neutral,neutral
517036806,"A reparametrized version of `A.RandomSizedCrop`:
```python
import math
import cv2
import albumentations as A


class RandomSizedCropV2(A.RandomSizedCrop):
    
    def __init__(self, height, width, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=cv2.INTER_LINEAR,
                 always_apply=False, p=1.0):

        super(RandomSizedCropV2, self).__init__(min_max_height=0, height=height, width=width, 
                                                w2h_ratio=1.0, interpolation=interpolation, 
                                                always_apply=always_apply, p=p)
        self.scale = scale
        self.ratio = ratio

    def get_params_dependent_on_targets(self, params):        
        img = params['image']
        area = img.shape[0] * img.shape[1]

        for attempt in range(10):
            target_area = random.uniform(*self.scale) * area
            log_ratio = (math.log(self.ratio[0]), math.log(self.ratio[1]))
            aspect_ratio = math.exp(random.uniform(*log_ratio))

            w = int(round(math.sqrt(target_area * aspect_ratio)))
            h = int(round(math.sqrt(target_area / aspect_ratio)))

            if w <= img.shape[1] and h <= img.shape[0]:
                i = random.randint(0, img.shape[0] - h)
                j = random.randint(0, img.shape[1] - w)
                return {
                    'crop_height': h,
                    'crop_width': w,
                    'h_start': i * 1.0 / (img.shape[0] - h + 1e-10),
                    'w_start': j * 1.0 / (img.shape[1] - w + 1e-10)
                }
        
        # Fallback to central crop
        in_ratio = img.shape[1] / img.shape[0]
        if (in_ratio < min(self.ratio)):
            w = img.shape[1]
            h = w / min(self.ratio)
        elif (in_ratio > max(self.ratio)):
            h = img.shape[0]
            w = h * max(self.ratio)
        else:  # whole image
            w = img.shape[1]
            h = img.shape[0]
        i = (img.shape[0] - h) // 2
        j = (img.shape[1] - w) // 2
        return {
            'crop_height': h,
            'crop_width': w,
            'h_start': i * 1.0 / (img.shape[0] - h + 1e-10),
            'w_start': j * 1.0 / (img.shape[1] - w + 1e-10)
        }

    def get_params(self):
        return {}

    @property
    def targets_as_params(self):
        return ['image']
```

Compare with torchvision:
```python
import random
from torchvision import transforms as T

t1 = T.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(0.75, 1.33333))
random.seed(15)
res_pil_img = t1(pil_img)

# vs

t2 = RandomSizedCropV2(height=224, width=224, scale=(0.08, 1.0), ratio=(0.75, 1.33333))

# Set random seed inside get_params_dependent_on_targets for demo purposes
def with_fixed_seed(func, seed):
    def wrapper(*args, **kwargs):
        random.seed(seed)
        return func(*args, **kwargs)
    return wrapper

t2.get_params_dependent_on_targets = with_fixed_seed(t2.get_params_dependent_on_targets, 15)
res_img = t2(image=img)
F.to_pil_image(res_img['image'][:, :, [1, 2, 3]])
```

![image](https://user-images.githubusercontent.com/2459423/62250870-65062280-b3ee-11e9-8ca5-c367b6766688.png)

",version python import math import import class self height width super self scale ratio self area attempt range area round round return fallback central crop min min else whole image return self return property self return compare python import random import set random seed inside seed wrapper seed return return wrapper image,issue,positive,negative,negative,negative,negative,negative
517023171,"yeah, we will consider supporting non-rgb or even add warning to all transforms which does not support grayscale",yeah consider supporting even add warning support,issue,positive,positive,positive,positive,positive,positive
516899683,"This is a subtle bug, spent an hour figuring it out. There should be a warning for non three channel cases. As of now, it returns the transposed image (weird, yeah)
@albu what do you think?",subtle bug spent hour warning non three channel image weird yeah think,issue,negative,negative,negative,negative,negative,negative
516623414,"I would argue PerChannel works only with pixelwise transforms in a sensible way. Hence why I only modify the image and pass through everything else. 

Applying a geometric transform per-channel means that the underlying mask (or bbox or keypoint) no longer makes any sense, unless you choose one channel as a reference and modify the mask based on that one alone. Or, as in my example, you can make small random perturbations to each channel and assume that the original masks etc are still valid.

I'm not sure splitting the image and concating after is an improvement (it's still not clear to me what the geometric labels would transform to), or would you suggest doing e.g. an OR of the transformed masks for each channel?",would argue work sensible way hence modify image pas everything else geometric transform underlying mask longer sense unless choose one channel reference modify mask based one alone example make small random channel assume original still valid sure splitting image improvement still clear geometric would transform would suggest channel,issue,positive,positive,neutral,neutral,positive,positive
516428236,"Thank you for your contribution. I just wonder if we can somehow split image to multiple images using transformation and then concat channels back.
The reason is that new compose does not work with bounding boxes, keypoints, masks at all",thank contribution wonder somehow split image multiple transformation back reason new compose work bounding,issue,negative,positive,neutral,neutral,positive,positive
515991370,"точно такую же проблему вызывает RGBShift() . Пример аугментации, воспроизводящей проблему:
```
IMG_HEIGHT = 96
IMG_WIDTH = 48

    def aug_train(p=1.0):
        return Compose([
            LongestMaxSize(max_size=max(IMG_HEIGHT, IMG_WIDTH)),
            PadIfNeeded(min_height=IMG_HEIGHT, min_width=IMG_WIDTH),
            RGBShift(),
            ChannelDropout(),
            Blur(),
            ToGray(p=1.0),
            Cutout(),
        ], p=p)
```",return compose blur cutout,issue,negative,neutral,neutral,neutral,neutral,neutral
513338069,"Examples here: https://github.com/albu/albumentations/blob/master/notebooks/example_keypoints.ipynb
although the source code is also using `label_fields`

    Compose(transformations, keypoint_params=dict(format='xy'))(image=image, keypoints=points)
",although source code also compose,issue,negative,neutral,neutral,neutral,neutral,neutral
513319099,"@ternaus @GazizovMarat
I already made a PR fixing this issue. 
https://github.com/albu/albumentations/pull/287
Just merge it and everything should be fine.
",already made fixing issue merge everything fine,issue,negative,positive,positive,positive,positive,positive
512090745,Could you please share an example code to reproduce an issue?,could please share example code reproduce issue,issue,positive,neutral,neutral,neutral,neutral,neutral
511637381,"As I went to create the feedstock, I discovered that another user had created one on June 5: https://github.com/conda-forge/albumentations-feedstock

Looks like this isn't necessary, I'll close.",went create discovered another user one june like necessary close,issue,positive,neutral,neutral,neutral,neutral,neutral
511545556,"OK great, I'll get that up and running this week.

On Mon, Jul 15, 2019 at 11:59 AM Alex Parinov <notifications@github.com>
wrote:

> @nrweir <https://github.com/nrweir> Yes, you are right, those are the
> correct people to include.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/issues/291?email_source=notifications&email_token=ADDLNI3FOYOGP3N2PJAW6D3P7SNGNA5CNFSM4H7FQYIKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6EYGQ#issuecomment-511462426>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ADDLNI5Z7AIUCOHPHLNWRSLP7SNGNANCNFSM4H7FQYIA>
> .
>
",great get running week mon wrote yes right correct people include reply directly view mute thread,issue,positive,positive,positive,positive,positive,positive
511462426,"@nrweir Yes, you are right, those are the correct people to include.",yes right correct people include,issue,negative,positive,positive,positive,positive,positive
511430166,"@creafz thanks for the response! I don't think I need any help from you all to get it up and running. The only question I have is who I should list as maintainers: along with myself, I assume I should also include @albu, @ternaus, @BloodAxe, and you - let me know if you don't want any of those 5 users listed as maintainers on the conda-forge repo.",thanks response think need help get running question list along assume also include let know want listed,issue,positive,positive,positive,positive,positive,positive
511122284,"Hey @nrweir 
Your help with creating and maintaining a conda-forge feedstock would be much appreciated. Do you need any assistance with this task from our side?",hey help would much need assistance task side,issue,negative,positive,positive,positive,positive,positive
511048967,Are you taking bets on that? I bet that there are some best practices that are task-independent. But whatever.,taking bet best whatever,issue,positive,positive,positive,positive,positive,positive
510956214,"it 100% depends on task, you just have to run augmentations and see what happens with image",task run see image,issue,negative,neutral,neutral,neutral,neutral,neutral
510896532,"I am looking for some best practices for augmentation for classification, shall I go with the `hard_transform` you pointed to? It the readme of albumentations is says that you won several kaggle competitions using the library, what are the typical augmentations you use?",looking best augmentation classification shall go pointed several library typical use,issue,positive,positive,positive,positive,positive,positive
509965291,"> this method is a little unstable. you need to pass bounding box to crop by as argument ""cropping_bbox"" and run cropping before all other augmentations.

Could you give me an example in code?   It still has some running errors. Thx.",method little unstable need pas bounding box crop argument run could give example code still running,issue,negative,negative,negative,negative,negative,negative
509579729,"this method is a little unstable. you need to pass bounding box to crop by as argument ""cropping_bbox"" and run cropping before all other augmentations.",method little unstable need pas bounding box crop argument run,issue,negative,negative,negative,negative,negative,negative
509561043,"Sorry for the delay, this was indeed the problem, thank you for your help, closing the issue..",sorry delay indeed problem thank help issue,issue,negative,negative,negative,negative,negative,negative
509293091,"I mean number of channels. We don't support grayscale (two channels), only 3-channel color images. You can convert it to 3-channel image using opencv or just stack channels",mean number support two color convert image stack,issue,negative,negative,negative,negative,negative,negative
509245691,"Hi, what is input image shape?",hi input image shape,issue,negative,neutral,neutral,neutral,neutral,neutral
508481803,"Hi. ""Line too long"" warning can be easily fixed by breaking long lines into two. Please show some respect to code-style ;)
Second, would you please add unit tests to ensure this change has expected behavior and does not change existing correct functionality?",hi line long warning easily fixed breaking long two please show respect second would please add unit ensure change behavior change correct functionality,issue,positive,neutral,neutral,neutral,neutral,neutral
508220472,"The build doesn't pass due to flake8 warnings ""Line too long"" but except that everything passes",build pas due flake line long except everything,issue,negative,negative,neutral,neutral,negative,negative
507510539,"import sys
import cv2
import albumentations as A
import matplotlib.pyplot as plt
from mmcv.runner import obj_from_dict
print(A.__version__)
class Augtor(object):
    
    def __init__(self,
                 albumentations = None):
        self.abmtt_transforms = []
        
        if albumentations is not None:
            for _transform in albumentations:                
                self.abmtt_transforms.append(obj_from_dict(_transform, parent=A))
                
    def __call__(self, img):
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        for transform in self.abmtt_transforms: #rgb
            img = transform(image=img)['image']
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
        return img
sampler = Augtor(albumentations=[
               dict(
                    max_h_size=100,
                    max_w_size=100,
                    p=1.0,
                    num_holes = 2,
                    fill_value=[255,0,0],#rgb
                    type='Cutout'
                )
])
I = cv2.imread(img_fname)
sample_img = sampler(I[:1000,:1000])
plt.imshow(cv2.cvtColor(sample_img,cv2.COLOR_BGR2RGB).astype(np.int))
plt.show()

Thanks for your reply! the 'img_fname' have to be set by yourself.
The following image is my result.

![image](https://user-images.githubusercontent.com/22048231/60482026-81fde780-9cc2-11e9-887f-dc7050700b10.png)

",import import import import import print class object self none none self transform transform return sampler sampler thanks reply set following image result image,issue,negative,positive,neutral,neutral,positive,positive
507504307,"That's strange. Please sharead your code and I'll try and diagnose the
problem.

On Tue, Jul 2, 2019, 5:38 AM John-Yao <notifications@github.com> wrote:

> hello, I had tried to set the value of Cutout's fill_value . It does work
> well. All the cut hole is fill with black.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/pull/267?email_source=notifications&email_token=AJO6FM45JIDYNTZ6VKQJGL3P5K5RRA5CNFSM4HP2PFR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODY74CUQ#issuecomment-507494738>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AJO6FM7O7R4WERVBAKNL243P5K5RRANCNFSM4HP2PFRQ>
> .
>
",strange please code try diagnose problem tue wrote hello tried set value cutout work well cut hole fill black thread reply directly view mute thread,issue,negative,negative,neutral,neutral,negative,negative
507494738,"hello, I had tried to set the value of Cutout's fill_value . It does work well. All the cut hole  is fill with black.",hello tried set value cutout work well cut hole fill black,issue,negative,negative,negative,negative,negative,negative
507324903,hi @adizhol looks like multi channel masks are not officially supported. if your classes don't intersect you can try to encode them into single-channel long mask. hope that helps,hi like channel officially class intersect try encode long mask hope,issue,positive,negative,neutral,neutral,negative,negative
507037191,"This is how I changed albumentations\core\transforms_interface.py:
line 54:
```
...
for key, arg in kwargs.items():
	if arg is not None:
		target_function = self._get_target_function(key)
		target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
		if key == 'mask':
			res[key] = []
			for i in range(arg.shape[-1]):
				res[key].append(target_function(arg[:,:,i], **dict(params, **target_dependencies)))

			res[key] = numpy.dstack(res[key])
		else:
			res[key] = target_function(arg, **dict(params, **target_dependencies))
	else:
		res[key] = None


```",line key none key key key key range key key key else key else key none,issue,negative,neutral,neutral,neutral,neutral,neutral
505303148,"@ternaus, has the issue been fixed or is it not of a priority right now? Thanks.",issue fixed priority right thanks,issue,negative,positive,positive,positive,positive,positive
501740727,"I would say, it is just the definition of the Gaussian Noise. Should have zero mean.

We clip on both sides [0, 255] => Everything should work.",would say definition noise zero mean clip side everything work,issue,negative,negative,negative,negative,negative,negative
501429185,"According to docs https://docs.opencv.org/master/d2/de8/group__core__array.html#gab55b8d062b7f5587720ede032d34156f, LUT operates on uint8 types only. That makes sense, since lookup tables for uint16 would occupy CPU cache making the LUT operation inefficient. To me this PR has a value of speeding up transform for uint8 types. However I have few suggestions:

1) Can you please add unit tests to ensure that transform in uint16 and uint32 types works as intended.
2) Update table of performance comparison between Albumentations and the rest :)",according lut sense since table would occupy cache making lut operation inefficient value speeding transform however please add unit ensure transform work update table performance comparison rest,issue,positive,neutral,neutral,neutral,neutral,neutral
499639873,"Yes, I know I made some changes that caused the fail and just haven't
gotten around to correcting it yet.

On Thu, Jun 6, 2019, 10:27 PM Vladimir Iglovikov <notifications@github.com>
wrote:

> Does not pass the tests.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/pull/267?email_source=notifications&email_token=AJO6FM2PCOSVUPM3GJ3VPJ3PZFQJHA5CNFSM4HP2PFR2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXD5ASY#issuecomment-499634251>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AJO6FMYMUAT5GM7DGEQ3IJLPZFQJHANCNFSM4HP2PFRQ>
> .
>
",yes know made fail gotten around correcting yet wrote pas thread reply directly view mute thread,issue,negative,negative,negative,negative,negative,negative
497846981,"Hi! I have recently proposed `Cutout` improvements in #263 and this looks like a subcase for the third improvement (patch modification). I've implemented `CoarseDropout` which deprecates `Cutout` in #264 and I think this patch should at least be applied to `CoarseDropout`. If you could find a way to generalize it and build a high-level interface for patch modifications that could be even better, though I don't have much experience with the project so I don't know what would be the best way to do that.",hi recently cutout like subcase third improvement patch modification cutout think patch least applied could find way generalize build interface patch could even better though much experience project know would best way,issue,positive,positive,positive,positive,positive,positive
495994612,"@albu Great, thanks! I've fixed CI issues and the patch should be ready for the merge now.",great thanks fixed patch ready merge,issue,positive,positive,positive,positive,positive,positive
495934555,"Yes, thanks. I think it's perfect option. Just make few style fixes as reported by travis and we can merge it",yes thanks think perfect option make style travis merge,issue,positive,positive,positive,positive,positive,positive
495914582,"@albu I'm not sure what's the best way to do that, but I've tried to address your comment in my last commit by introducing `CoarseDropout` and deprecating `Cutout` in favor of it. Is that what you suggested?",sure best way tried address comment last commit cutout favor,issue,positive,positive,positive,positive,positive,positive
495602108,Nice package! Working now in my pipeline!,nice package working pipeline,issue,negative,positive,positive,positive,positive,positive
495589559,And so for segmentation masks you can just add an image2? Is that how it works?,segmentation add image work,issue,negative,neutral,neutral,neutral,neutral,neutral
495589329,"I’ve just been reading through the docs. I use torchvision, so I just didn’t realize that you have to specify the arguments to the transformer when you call it, like image=",reading use realize specify transformer call like,issue,negative,neutral,neutral,neutral,neutral,neutral
494812726,"we have to preserve old api to not break pipelines with updates, let's make two versions and deprecation warning for previous (for 0.4.0)",preserve old break let make two deprecation warning previous,issue,negative,negative,neutral,neutral,negative,negative
494173097,"`Allow cutting out a random number of regions of random size (sample num_holes and [height, width] from uniform random distribution).` is implemented => good enough. 

Let's merge it and work on the other modifications like extra targets and patch modifications later. (You may keep this feature request so that we would not forget.)",allow cutting random number random size sample height width uniform random distribution good enough let merge work like extra patch later may keep feature request would forget,issue,positive,negative,negative,negative,negative,negative
492663738,"Lot of thanks!
But in Anaconda it need to be done in new created environment even if in base  environment I did it.
All works!",lot thanks anaconda need done new environment even base environment work,issue,negative,negative,negative,negative,negative,negative
490993085,"(base) C:\WINDOWS\system32>pip install shapely
Collecting shapely
  Using cached https://files.pythonhosted.org/packages/a2/fb/7a7af9ef7a35d16fa23b127abee272cfc483ca89029b73e92e93cdf36e6b/Shapely-1.6.4.post2.tar.gz
    ERROR: Complete output from command python setup.py egg_info:
    ERROR: Traceback (most recent call last):

(base) C:\WINDOWS\system32>conda install shapely
Solving environment: failed

PackagesNotFoundError: The following packages are not available from current channels:

  - shapely

Current channels:

  - https://conda.anaconda.org/r/win-64
  - https://conda.anaconda.org/r/noarch
  - https://conda.anaconda.org/conda/win-64
  - https://conda.anaconda.org/conda/noarch

Could anybody help? I used to install only using pip

",base pip install shapely shapely error complete output command python error recent call last base install shapely environment following available current shapely current could anybody help used install pip,issue,negative,negative,negative,negative,negative,negative
485672917,"```
./albumentations/augmentations/functional.py:1144:1: D401 First line should be in imperative mood
./albumentations/augmentations/functional.py:1162:1: D401 First line should be in imperative mood
./albumentations/augmentations/functional.py:1215:1: D401 First line should be in imperative mood
```",first line imperative mood first line imperative mood first line imperative mood,issue,negative,positive,positive,positive,positive,positive
485668942,"> ```
> ./tests/test_functional.py:849:1: W293 blank line contains whitespace
> ./albumentations/augmentations/bbox_utils.py:53:1: E302 expected 2 blank lines, found 1
> ./albumentations/augmentations/transforms.py:106:9: F821 undefined name 'uper'
> ./albumentations/augmentations/functional.py:1144:1: D401 First line should be in imperative mood
> ./albumentations/augmentations/functional.py:1162:1: D401 First line should be in imperative mood
> ./albumentations/augmentations/functional.py:1176:37: E226 missing whitespace around arithmetic operator
> ./albumentations/augmentations/functional.py:1177:37: E226 missing whitespace around arithmetic operator
> ./albumentations/augmentations/functional.py:1215:1: D401 First line should be in imperative mood
> ```

Hi, thanks. I will fix that.",blank line blank found undefined name first line imperative mood first line imperative mood missing around arithmetic operator missing around arithmetic operator first line imperative mood hi thanks fix,issue,negative,positive,neutral,neutral,positive,positive
485666094,"```
./tests/test_functional.py:849:1: W293 blank line contains whitespace
./albumentations/augmentations/bbox_utils.py:53:1: E302 expected 2 blank lines, found 1
./albumentations/augmentations/transforms.py:106:9: F821 undefined name 'uper'
./albumentations/augmentations/functional.py:1144:1: D401 First line should be in imperative mood
./albumentations/augmentations/functional.py:1162:1: D401 First line should be in imperative mood
./albumentations/augmentations/functional.py:1176:37: E226 missing whitespace around arithmetic operator
./albumentations/augmentations/functional.py:1177:37: E226 missing whitespace around arithmetic operator
./albumentations/augmentations/functional.py:1215:1: D401 First line should be in imperative mood
```",blank line blank found undefined name first line imperative mood first line imperative mood missing around arithmetic operator missing around arithmetic operator first line imperative mood,issue,negative,positive,neutral,neutral,positive,positive
485665155,"> @CarlosUziel Hi! Thanks for your PR. It seems there are few conflicts that prevents merging this augmentation. Can you please address these issues so that we can review the PR?

Thank you for your reply. I believe all conflicts have been resolved, please let me know if there is any of them remaining.",hi thanks augmentation please address review thank reply believe resolved please let know,issue,positive,positive,positive,positive,positive,positive
485532294,@CarlosUziel Hi! Thanks for your PR. It seems there are few conflicts that prevents merging this augmentation. Can you please address these issues so that we can review the PR?,hi thanks augmentation please address review,issue,positive,positive,positive,positive,positive,positive
485065479,"`./albumentations/augmentations/functional.py:781:120: E501 line too long (122 > 119 characters)`
please correct error before review",line long please correct error review,issue,negative,negative,neutral,neutral,negative,negative
482576234,https://github.com/albu/albumentations/blob/master/notebooks/example_multi_target.ipynb like this. please re-open if it's still not clear,like please still clear,issue,positive,positive,positive,positive,positive,positive
481878913,"conda install shapely
pip install imgaug
these solved my problem, thanks",install shapely pip install problem thanks,issue,negative,positive,positive,positive,positive,positive
479356909,"You can either convert bboxes from YOLO format to COCO or VOC before sending to pipeline, or attach YOLO keypoints format reference here and wait until it's support will be added to the library.",either convert format coco sending pipeline attach format reference wait support added library,issue,negative,neutral,neutral,neutral,neutral,neutral
478323004,I achived the aim by using same seed(for numpy.random and random) for images in one video and another seed for other video.,aim seed random one video another seed video,issue,negative,negative,negative,negative,negative,negative
478225999,Could you please share an example of the code snippet that generates an error? And also albumentations version.,could please share example code snippet error also version,issue,negative,neutral,neutral,neutral,neutral,neutral
476817757,"@sakvaua Hi, albumentations does not support this by design. However this is popular task, so I take a chance and pr my library with handy PyTorch extensions including TTA: https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/inference/tta.py",hi support design however popular task take chance library handy,issue,positive,positive,positive,positive,positive,positive
475609312,"Hi, i receive this error too. Code below to reproduce it.

```
from albumentations import GridDistortion
from albumentations import Compose
import numpy as np
import cv2


augmentation = Compose([GridDistortion(num_steps=5, distort_limit=0.3, interpolation=cv2.INTER_LINEAR, border_mode=cv2.BORDER_CONSTANT, p=1)], p=1)

err_height = []
for i in range(200):
    try:
        image = np.random.random((i, 100))
        image = augmentation(image=image)['image']

    except Exception:
        err_height.append(i)

print(err_height)
```

Output:
`[0, 1, 2, 3, 4, 7, 8, 9, 14]`

Seems like there are some special numbers. By increasing the num_steps, the number of special numbers increase too.

At num_steps = 10 we have 

Output:
`[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 24, 25, 26, 27, 28, 29, 36, 37, 38, 39, 48, 49]`
",hi receive error code reproduce import import compose import import augmentation compose range try image image augmentation except exception print output like special increasing number special increase output,issue,positive,positive,positive,positive,positive,positive
473257722,"As I understand these augmentations works only for 3-channel image (assuming images are RGB format). If a user will use single-channel or multispectral image it will crash in `cv2.cvtColor` with a not-so-user-friendly exception message (Let's put aside a question why someone may use non-rgb image with AddSnow augmentation. It basically will happen). I'd love to see some type checking of input image for correct number of channels and user-friendly message how to use it properly.

I really like how it's made in Angular framework. When you do something wrong, they print what was wrong and what is typical workaround. ",understand work image assuming format user use image crash exception message let put aside question someone may use image augmentation basically happen love see type input image correct number message use properly really like made angular framework something wrong print wrong typical,issue,negative,negative,neutral,neutral,negative,negative
473255990,Can you please update notebooks with showcase of new augmentations? We will need fancy pictures for a new release :),please update showcase new need fancy new release,issue,negative,positive,positive,positive,positive,positive
473028559,"> @wamiq-reyaz Feel free to create a PR with this feature.

Sounds enticing. I am busy till the end of this month, but sure I will try and 
give it a shot.",feel free create feature enticing busy till end month sure try give shot,issue,positive,positive,positive,positive,positive,positive
472609318,@wamiq-reyaz Feel free to create a  PR with this feature.,feel free create feature,issue,positive,positive,positive,positive,positive,positive
472497595,"I was facing a similar issue.

Workaround provided by adizhol worked for me as well. 

Thank you!",facing similar issue provided worked well thank,issue,positive,neutral,neutral,neutral,neutral,neutral
472264145,"Also solved it by direclty installing shapely from conda, without having to install any binaries beforehand.

Windows 10, Python 3.6.8",also shapely without install beforehand python,issue,negative,neutral,neutral,neutral,neutral,neutral
471748898,Video object detection. Augmentation of the whole video. Can I use any other option to achive this?,video object detection augmentation whole video use option,issue,negative,positive,positive,positive,positive,positive
471427383,"@albu
> would it install opencv-python-headless in case if opencv-python is already installed?

Yes indeed.

But the point is that you cannot only trust pip on this one, 
as opencv-python can be installed but not able to works as some lib dependencies missed...",would install case already yes indeed point trust pip one able work,issue,positive,positive,positive,positive,positive,positive
471420481,"hi, thanks for the pr.
would it install opencv-python-headless in case if opencv-python is already installed?",hi thanks would install case already,issue,negative,positive,positive,positive,positive,positive
471344944,looks like it's about passing one more parameter in constructor,like passing one parameter constructor,issue,negative,neutral,neutral,neutral,neutral,neutral
471335531,"Thanks a lot for answering. 
While we are on the issue, the padding value for the mask and the images is the same. How difficult is to change the API to allow for configurable values?
",thanks lot issue padding value mask difficult change allow,issue,positive,negative,negative,negative,negative,negative
471326563,@creafz Do we have an instruction on how to update README when a new transformation is added?,instruction update new transformation added,issue,negative,positive,positive,positive,positive,positive
470996040,"Hi all,
First, thank you for such a useful library!
I noticed `ShiftScaleRotate` may still not be working correctly for bounding boxes. When applying only a rotation, the rotated bounding boxes are close to the objects in the rotated image but do not correspond exactly to them (at least for rectangular images of different height and width). Some objects actually end outside the rotated bounding box.

In the following function:
https://github.com/albu/albumentations/blob/09573604506b0f6b2eec3b8d6555faf10f0c5bc4/albumentations/augmentations/functional.py#L143-L153

Denormalizing the coordinates before the rotation and renormalizing again, solved the issues I had:
```
def bbox_shift_scale_rotate(bbox, angle, scale, dx, dy, interpolation, rows, cols, **params):
    height, width = rows, cols
    center = (width / 2, height / 2)
    matrix = cv2.getRotationMatrix2D(center, angle, scale)
    matrix[0, 2] += dx * width
    matrix[1, 2] += dy * height
    x = np.array([bbox[0], bbox[2], bbox[2], bbox[0]])
    y = np.array([bbox[1], bbox[1], bbox[3], bbox[3]])
    ones = np.ones(shape=(len(x)))
    points_ones = np.vstack([x, y, ones]).transpose()
    points_ones[:, 0] *= width
    points_ones[:, 1] *= height
    tr_points = matrix.dot(points_ones.T).T
    tr_points[:, 0] /= width
    tr_points[:, 1] /= height
    return [min(tr_points[:, 0]), min(tr_points[:, 1]), max(tr_points[:, 0]), max(tr_points[:, 1])]
```

It looks like normalization introduces different rescalings for each axis that make the transformation different from the one applied to images. Do you agree or did I miss something?",hi first thank useful library may still working correctly bounding rotation rotated bounding close rotated image correspond exactly least rectangular different height width actually end outside rotated bounding box following function rotation angle scale interpolation height width center width height matrix center angle scale matrix width matrix height width height width height return min min like normalization different axis make transformation different one applied agree miss something,issue,positive,positive,neutral,neutral,positive,positive
470854308,"https://github.com/albu/albumentations/blob/master/albumentations/augmentations/transforms.py#L412 you can pass same values as lower and upper bounds
```shift_limit=(0.0625, 0.0625), scale_limit=(0.1, 0.1), rotate_limit=(45, 45)``` 
and it will be applied deterministically",pas lower upper applied,issue,negative,neutral,neutral,neutral,neutral,neutral
469141571,already fixed in https://github.com/albu/albumentations/pull/221 which was merged previously. thanks for cooperation,already fixed previously thanks,issue,negative,positive,positive,positive,positive,positive
469068192,"@ternaus I did not have time to write the message to ask whether it is something like that you would like to have but you've already merge it :)

Anyway it would be cool to see the benchmark on HorizontalFlip with this add",time write message ask whether something like would like already merge anyway would cool see add,issue,positive,positive,positive,positive,positive,positive
469059224,"@vfdev-5 If you will modify PR so that the implementation of the transform `numpy` vs `cv2` was chosen based on the type of the data it would be really cool.

HorizontalFlip is the most widely used transform and the fact that we are lacking in the performance is not good. Hence, your PR is highly appreciated.",modify implementation transform chosen based type data would really cool widely used transform fact performance good hence highly,issue,positive,positive,positive,positive,positive,positive
469056017,looks like it does not work for float32 but works correctly for uint8,like work float work correctly,issue,negative,neutral,neutral,neutral,neutral,neutral
469054333,"cool, it would be perfect if you can add test for this error
thank you for cooperation 👍 ",cool would perfect add test error thank,issue,positive,positive,positive,positive,positive,positive
469031289,"We try to make all transformations compatible with a `np.float32` input, so they can work not only with a `np.uint8` input but with other data types as well. [Here is an example notebook](https://github.com/albu/albumentations/blob/master/notebooks/example_16_bit_tiff.ipynb) that shows how to work with 16-bit tiff images by converting them to float32 using `ToFloat` transformation (and since we can use `ToFloat` to convert any datatype to float32 we don't need to support all the datatypes in our transformations, we just need to support `np.uint8` and `np.float32` inputs). ",try make compatible input work input data well example notebook work tiff converting float transformation since use convert float need support need support,issue,positive,neutral,neutral,neutral,neutral,neutral
469013338,"@creafz yes, I confirm that opencv is slower on f32 similar to your results (not x3 but ~1x). 
Do you have use-cases of images in float32 after reading from disk ? But you're right it makes sense to check the performances on int16, int32 input data.

For example, 5 bands (uint16)
```python
x = np.random.randint(0, 255, size=(281, 500, 5)).astype(np.uint16)

%%timeit -r 7 -n 10
for _ in range(1000):
    hflip(x)
> 627 ms ± 3.25 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

%%timeit -r 7 -n 10
for _ in range(1000):
    hflip_cv2(x)
> 603 ms ± 513 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
",yes confirm similar float reading disk right sense check input data example python range per loop mean dev range per loop mean dev,issue,negative,negative,neutral,neutral,negative,negative
468999232,"@vfdev-5 Could you also test a case with a np.float32 input? On my machine, OpenCV is slower than NumPy:
```
x = np.random.rand(281, 500, 3).astype(np.float32)
```

```
%%timeit -r 7 -n 10
for _ in range(1000):
    hflip(x)
> 509 ms ± 1.64 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```
```
%%timeit -r 7 -n 10
for _ in range(1000):
    hflip_cv2(x)
> 1.62 s ± 329 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```",could also test case input machine range per loop mean dev range per loop mean dev,issue,negative,negative,negative,negative,negative,negative
468860540,"You need to transform from numpy array to the torch tensor. The way to do it is:
```
from albumentations.pytorch.functional import img_to_tensor
```

and within a DataLoader to cast `img_to_tensor` operation to the augmented image.

```
...

return img_to_tensor(image), label
```

I updated the notebook in https://github.com/albu/albumentations/pull/208",need transform array torch tensor way import within cast operation augmented image return image label notebook,issue,negative,neutral,neutral,neutral,neutral,neutral
466498414,Could you please add a test for this case?,could please add test case,issue,negative,neutral,neutral,neutral,neutral,neutral
465069802,"I don’t think newer version of imgaug can broke anything 💪

Vào Th 3, 19 thg 2, 2019 lúc 16:58 Alexander Buslaev <
notifications@github.com> đã viết:

> looks like tests does not pass because we have too old imgaug @creafz
> <https://github.com/creafz>
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/pull/195#issuecomment-465065578>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AZl3ClMPoeD4uuBrSUHiUGxv7P8dEfMnks5vO8qrgaJpZM4a-zcb>
> .
>
",think version broke anything th like pas old thread reply directly view mute thread,issue,negative,positive,neutral,neutral,positive,positive
465061855,"Yeah, my PR make it passable 😂

Vào Th 3, 19 thg 2, 2019 lúc 16:46 Alexander Buslaev <
notifications@github.com> đã viết:

> it's False by default, let's keep default but passing this argument is
> good option.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/issues/194#issuecomment-465061316>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AZl3ChRVA34GYY3IU26Dt4O3NELl-nTrks5vO8ffgaJpZM4a-zbK>
> .
>
",yeah make passable th false default let keep default passing argument good option thread reply directly view mute thread,issue,negative,positive,positive,positive,positive,positive
465061316,"it's `False` by default, let's keep default but passing this argument is good option.",false default let keep default passing argument good option,issue,negative,positive,positive,positive,positive,positive
465010517,"Hi @ternaus, 

The problem is after affine transformation parts of the image may be outside of the image plane or the image might make up only a small part of the image plane. Keeping output image fit can be useful e.g. for rotations by 45 degrees to avoid that the image corners are outside of the image plane.

To do that just need to use `fit_output=True` argument when we call `imgaug`, my PR addressed that ;)",hi problem affine transformation image may outside image plane image might make small part image plane keeping output image fit useful avoid image outside image plane need use argument call,issue,negative,positive,neutral,neutral,positive,positive
465000415,"Hello,

Could you please share what is the use case, when you need both the original and transformed image?",hello could please share use case need original image,issue,positive,positive,positive,positive,positive,positive
464269460,"Hi, @BloodAxe. I had also tried to use the PIL and Tensor image (iterating over each dataset and not using the `ImageFolder`), but I have received an error (invalid type) when I was training the Network (Resnet 18, VGG 11, etc).

The` ImageFolder` and the `DataLoader` APIs are very bad, the preprocessing had taken a huge amount of time, to do simple things I had need to read the code of the methods. I have finished the work in question, but I'm going to use another framework to try data augmentation, synthetic data, etc. The PyTorch documentation sucks.

Thanks for the answer, I really appreciate that.",hi also tried use tensor image received error invalid type training network bad taken huge amount time simple need read code finished work question going use another framework try data augmentation synthetic data documentation thanks answer really appreciate,issue,negative,positive,neutral,neutral,positive,positive
464190900,"Closing due to inactivity. Since we added unit tests to ensure transformation does not introduce new classes for masks, it should be fine now.",due inactivity since added unit ensure transformation introduce new class fine,issue,positive,positive,positive,positive,positive,positive
464166339,"Hi. I'm not sure whether Albumentations is compatible with a `torchvision.datasets.ImageFolder`. 
I personally use my own dataset classes where transformations and image to tensor conversion already baked in.  Torchvision's transformation API is quite different to ours and I don't see an easy way to glue them together.
",hi sure whether compatible personally use class image tensor conversion already baked transformation quite different see easy way glue together,issue,positive,positive,positive,positive,positive,positive
464155438,This seems not to be an issue on Albumentations@0.1.12. Therefore I'm closing it. If the problem persist feel free to re-open it and attach a test case for reproducing.,issue therefore problem persist feel free attach test case,issue,negative,positive,positive,positive,positive,positive
464123810,"> Not sure about this because all other transforms do not support grayscale so it would work only in specific case of one transform.

I think all spatial transformations work well with grayscale. And probably some of the color ones.

we just added one more to the list.

I think for grayscale it is better to have CLAHE + spatial, rather than only spatial.",sure support would work specific case one transform think spatial work well probably color added one list think better spatial rather spatial,issue,positive,positive,positive,positive,positive,positive
463555065,"right now there is a limitation that at least one image should be named `image`.
you can pass `{'image': _img1, 'image2': _img2, 'mask': _target}` and you also have to pass `{'image2': 'image'}` to `additional_targets`",right limitation least one image image pas also pas,issue,negative,negative,neutral,neutral,negative,negative
463549552,"Can someone provide an example of how to use this with multi-image single mask task?  If I have a dictionary `sample = {'image1': _img1, 'image2': _img2, 'label': _target}` can I pass this to Compose directly?",someone provide example use single mask task dictionary sample pas compose directly,issue,negative,positive,neutral,neutral,positive,positive
461907084,It would be a good idea to add this transformation. Especially useful for medical imagery.,would good idea add transformation especially useful medical imagery,issue,positive,positive,positive,positive,positive,positive
461528106,"Thanks, I'll take a look on weekend on this issue.",thanks take look weekend issue,issue,negative,positive,positive,positive,positive,positive
461407151,"Actually... I'm going to reopen it.
Tried to add logging to https://github.com/albu/albumentations/blob/c26383ecd9eeb51d57185bfd699179a8a41f7b6d/albumentations/augmentations/functional.py#L858
method. And it doesn't look like keypoints were normalized. Also there is commit that removes `normalize_keypoint` function https://github.com/albu/albumentations/pull/145/commits/41a5fdff1a2e405197684afcd320dede636d192c",actually going reopen tried add logging method look like also commit function,issue,positive,neutral,neutral,neutral,neutral,neutral
461010997,"There is also a problem with apply method it is not allways contain ""image"" world as an argument, it sometimes ""img"".",also problem apply method contain image world argument sometimes,issue,negative,neutral,neutral,neutral,neutral,neutral
459363205,Thanks for your fix. Such a small typo cause big bug :(,thanks fix small typo cause big bug,issue,negative,negative,neutral,neutral,negative,negative
455997234,"Ok. Should compression type be string or enum? Currently `imencode` supports a few formats:
https://docs.opencv.org/3.0-beta/modules/imgcodecs/doc/reading_and_writing_images.html#imwrite",compression type string currently,issue,negative,neutral,neutral,neutral,neutral,neutral
455949100,"Hi, thanks for your feedback and merge.
I see it would be better to have `bbox_safe` option.
I will try to implement, and request for merge again once it is ready.",hi thanks feedback merge see would better option try implement request merge ready,issue,positive,positive,positive,positive,positive,positive
455778603,"I got the same issue installing imgaug, @adizhol solution worked well.",got issue solution worked well,issue,positive,neutral,neutral,neutral,neutral,neutral
455638312,"I think it's better to add compression type as argument so new class and method names would be like `compress_image`. Also make sure you have deprecation warning for jpeg compression, you can see it here https://github.com/albu/albumentations/blob/master/albumentations/augmentations/transforms.py#L925",think better add compression type argument new class method would like also make sure deprecation warning compression see,issue,negative,positive,positive,positive,positive,positive
454924625,"Perhaps, something like this can do the work:
```
x = { 'image': ..., 'mask': ... }
aug = A.Compose([...])
batch = default_collate([aug(**x) for x in repeat(x, batch_size)`])
```
A `default_collate` is a util function from Pytorch. It's used by DataLoader for assembling a batch from individual samples.",perhaps something like work batch repeat function used batch individual,issue,negative,neutral,neutral,neutral,neutral,neutral
454765169,"> should be fixed in 0.1.11, please reopen if you are still issuing this

Thank you ,this issus is disappeared.",fixed please reopen still issuing thank,issue,positive,positive,neutral,neutral,positive,positive
454754992,"should be fixed in 0.1.11, please reopen if you are still issuing this",fixed please reopen still issuing,issue,negative,positive,neutral,neutral,positive,positive
453811262,"Hi, sorry for not much attention to your PR.
Is it the same random sized crop but it ensures that all bboxes are still inside a cropped picture?
I just wonder if we can combine them and add flag to be `bbox_safe` if they share a lot of code",hi sorry much attention random sized crop still inside picture wonder combine add flag share lot code,issue,negative,negative,negative,negative,negative,negative
453747045,I want to try it and figure it out if we can simplify DALI usage because it's really cool but really weird,want try figure simplify dali usage really cool really weird,issue,negative,negative,neutral,neutral,negative,negative
452391538,"Thanks, I am going to fix it soon",thanks going fix soon,issue,negative,positive,positive,positive,positive,positive
449837124,"@albu I think you merged this one too earlier before uploading packages to your channel : https://anaconda.org/albu/albumentations. 

Someone who has access to push packages to channel `albu/albumentations` (certainly `albu` and other collabs: https://anaconda.org/albu/albumentations/settings/collaborators), you need to execute from linux machine:
```
export ANACONDA_TOKEN=<your token to push to albu/albumentations>
sh conda.recipe/build_upload.sh
```

Otherwise described in `README.md` method to install albumentations using conda does not work.
",think one channel someone access push channel certainly need execute machine export token push sh otherwise method install work,issue,negative,positive,positive,positive,positive,positive
449679396,"Yes, this implementation is pretty strange, we should consider replacing it with something simpler and more general.",yes implementation pretty strange consider something simpler general,issue,positive,positive,neutral,neutral,positive,positive
449625466,"Albumentations relies on OpenCV for colorspace conversion. In OpenCV conventional ranges for R, G, and B channel values are:

```
0 to 255 for CV_8U images
0 to 65535 for CV_16U images
0 to 1 for CV_32F images
```
Therefore if your image has values in [0..255] range you may want to convert it to `uint8` dtype explicitly: `im = np.random.randint(0,255,(224,224,3)).astype(np.uint8)`.
",conversion conventional channel therefore image range may want convert explicitly,issue,negative,negative,negative,negative,negative,negative
449623631,Hi @akojo. Can you please specify version of `albumentations` you're using and short snippet to reproduce it?,hi please specify version short snippet reproduce,issue,negative,neutral,neutral,neutral,neutral,neutral
449623562,"Hi. Partially yes. Due to their nature, some augmentations are intended for RGB images (For instance CLAHE, HSV and few others). Would you mind attaching your augmentation pipeline to this ticket?",hi partially yes due nature intended instance would mind augmentation pipeline ticket,issue,negative,negative,negative,negative,negative,negative
449517615,you can use `p=1` transforms or `always_apply` transforms if it's something like horizontal flip or vertical flip (determined except probability),use something like horizontal flip vertical flip determined except probability,issue,positive,neutral,neutral,neutral,neutral,neutral
449517249,"We will add some comments not to use ToTensor transforms. They are only presented for reference, because all the people have different data types and we cannot fit all needs. ",add use reference people different data fit need,issue,negative,positive,positive,positive,positive,positive
449427323,"Hi, so yes, we think it's normal for general bounding box (4 points). 
There is PR for multiple keypoints support: https://github.com/albu/albumentations/pull/145",hi yes think normal general bounding box multiple support,issue,positive,positive,neutral,neutral,positive,positive
449421934,"I think if there are only 4 points, this is normal behaviour for the affine transform at large angles. When there are more points available, the box is tighter and a better fit. Would it be worth adding another `format` field called something like `points` so that multiple x, y points are transformed before creating a box? 

This is a good dataset to test this on: https://www.kaggle.com/martinpiotte/bounding-box-model
However, this might be a bit niche since I'm not sure how many datasets this might be applicable to.",think normal behaviour affine transform large available box better fit would worth another format field something like multiple box good test however might bit niche since sure many might applicable,issue,positive,positive,positive,positive,positive,positive
449003791,"that would be great, thanks in advance!",would great thanks advance,issue,positive,positive,positive,positive,positive,positive
447707222,"Hi, no, basically you need to apply transform to images one by one. 
If you use pytorch - you can just use custom Dataset and pass albumentations as transforms. DataLoader will do job generating batches for you.

This PR is mainly for tasks like image-to-image or other multi-image or multi-mask tasks, where you have to apply the same transform for bunch of images.",hi basically need apply transform one one use use custom pas job generating mainly like apply transform bunch,issue,negative,positive,positive,positive,positive,positive
447658240,"Do I need to pass a list of dictionaries to apply the augmentation to an entire folder of images? Or is there some generator-like way to do it?

Thanks in advance,
Felix",need pas list apply augmentation entire folder way thanks advance,issue,negative,positive,neutral,neutral,positive,positive
445384528,"I believe some transformations may be supported in a hackish way, like split the volume into the separate images, apply the same transform to each of them and combine them back.

In this commit https://github.com/albu/albumentations/commit/ffe8bdf4cb82380a586201774e39bcf639d1b0d2 @albu added functionality that should make it possible.

But we did not check how will it work for the 3D volumes.",believe may way like split volume separate apply transform combine back commit added functionality make possible check work,issue,positive,neutral,neutral,neutral,neutral,neutral
445268305,Can we rehape 3D volumes and apply this library then reshape it back? ,apply library reshape back,issue,negative,neutral,neutral,neutral,neutral,neutral
445267333,"Hi, I believe currently this feature is not supported",hi believe currently feature,issue,negative,neutral,neutral,neutral,neutral,neutral
445247019,lets squash everything and push --force into this branch,squash everything push force branch,issue,negative,neutral,neutral,neutral,neutral,neutral
445220181,"Sorry, the above code I suggested was buggy. I fixed.",sorry code buggy fixed,issue,negative,negative,negative,negative,negative,negative
445218987,"Thanks for quick replying!
I'm not sure if this issue should be closed until the change will be merged, so I remain it opened.",thanks quick sure issue closed change remain,issue,positive,positive,positive,positive,positive,positive
445022459,Yep. However for me it looks like a simple fix. So it should be fixed soon.,yep however like simple fix fixed soon,issue,positive,positive,neutral,neutral,positive,positive
445020588,"@BloodAxe that means anything with IAA as the prefix wouldn't work in my case, right?",anything prefix would work case right,issue,negative,positive,positive,positive,positive,positive
445019185,"It seems a problem is related to `BasicIAATransform.deterministic_processor`. In case of `IAASharpen`, 
there is closure function created inside  `Sharpen` function, which cannot be picked. 
A workaround could be to remove `self.deterministic_processor` and create this instance in `get_params` and send it via named argument. ",problem related case closure function inside sharpen function picked could remove create instance send via argument,issue,negative,neutral,neutral,neutral,neutral,neutral
444435865,"It is not supported out of the box, but you may use a simple trick to fix random seed before applying augmentation at test time:

```
# TTA
random.seed(1234)
data = transform(**data)
```
",box may use simple trick fix random seed augmentation test time data transform data,issue,negative,negative,negative,negative,negative,negative
441440658,please re-open this issue if you still have this problem,please issue still problem,issue,negative,neutral,neutral,neutral,neutral,neutral
440765993,"@hexfaker please take a look on updated api, it should better fit your needs. thanks",please take look better fit need thanks,issue,positive,positive,positive,positive,positive,positive
440640017,"Hi, what is your albumentations version? It should have been fixed in latest release.",hi version fixed latest release,issue,negative,positive,positive,positive,positive,positive
439638281,"> Compose becomes too big, maybe we can somehow refactor it

Makes sense for me. I was thinking to have a `TransformationList` to represent a series of sequential transformations and a `TransformationGraph` to represent all transformations + pre/post processing rules, bbox/keypoint formats.",compose becomes big maybe somehow sense thinking represent series sequential represent,issue,negative,neutral,neutral,neutral,neutral,neutral
439637468,"Compose becomes too big, maybe we can somehow refactor it",compose becomes big maybe somehow,issue,negative,neutral,neutral,neutral,neutral,neutral
439603286,"I tested import both locally and in Google Colab using the latest version from PyPI and it works for me.

Here is a Google Colab notebook https://colab.research.google.com/drive/1duy6xMpQA3-slaHm14WqwpUZsJ7UB3X6, could you please execute the code from it locally (except the first cell, it just installs the library) and post the output?",tested import locally latest version work notebook could please execute code locally except first cell library post output,issue,negative,positive,positive,positive,positive,positive
436366077,"I'm going to improve custom targets in the nearest future, but this PR is quite ok for now",going improve custom nearest future quite,issue,negative,neutral,neutral,neutral,neutral,neutral
435691454,"`sample = np.arange(0, 28*28).reshape(28,28)`
returns an array with dtype `np.int64` which is not supported by OpenCV

Typically (not always) images are stored in the `np.uint8` with pixel values in the range [0, 255]

The two possible options:

1. `sample = np.arange(0, 28*28).reshape(28,28).astype(np.uint8)` if you are sure that your pixel values are in [0, 255]
2. `sample = np.arange(0, 28*28).reshape(28,28).astype(float)`",sample array typically always range two possible sample sure sample float,issue,negative,positive,positive,positive,positive,positive
435690642,"Thanks for the Normalize! :D Around here it sounds a bit impersonal but I want you to know that I am very grateful for your work. I'm a little new here, I had to have passed code from the beginning.


```
import numpy as np
import albumentations

def apply_albumentation(aug, image):
    image = aug(image=image)['image']
    return image

sample = np.arange(0, 28*28).reshape(28,28)
sample_transformed = apply_albumentation(albumentations.Rotate(limit=1,p=1), sample)
```
Gives me: 

> error: OpenCV(3.4.3) /io/opencv/modules/imgproc/src/imgwarp.cpp:1805: error: (-215:Assertion failed) ifunc != 0 in function 'remap'
",thanks normalize around bit impersonal want know grateful work little new code beginning import import image image return image sample sample error error assertion function,issue,negative,positive,neutral,neutral,positive,positive
435690103,"It is not completely clear what error do you get with Rotate and Resize?

Could you please share a minimal example of the code that reproduces the error? So that we would be able to run it on our computers? (Not a screenshot)",completely clear error get rotate resize could please share minimal example code error would able run,issue,negative,positive,positive,positive,positive,positive
435689963,"To apply Normalize to an image with a number of channels that is different from 3 you need to pass as parameters of the transformation `mean` and `std` that has the number of values equal to the number of channels in the image.
```
import albumentations as albu
import numpy as np

image = np.ones((5, 5))

aug = albu.Normalize(mean=(0.5), std=(0.2), p=1)
augmented_image = aug(image=image)['image']
```",apply normalize image number different need pas transformation mean number equal number image import import image,issue,negative,negative,negative,negative,negative,negative
435591196,"But then how should I do it? I've been looking at the documentation https://albumentations.readthedocs.io/en/latest/api/augmentations.html?highlight=normalize#albumentations.augmentations.transforms.Normalize and I do not know how to use it for a channel case.

On the other hand, the problem with Rotate and Resize also persists. Greetings.

EDIT: Working well with rgb images.",looking documentation know use channel case hand problem rotate resize also edit working well,issue,negative,neutral,neutral,neutral,neutral,neutral
435500673,"In this example, you are trying to apply Normalize with default parameters (3 channels) to a grayscale image with one channel.",example trying apply normalize default image one channel,issue,negative,neutral,neutral,neutral,neutral,neutral
435478455,"Yeah sure! It is very easy to be my fault: D For example, trying to load some MNIST data and normalize them gives me the error that I mentioned earlier.

![selection_002](https://user-images.githubusercontent.com/23385329/47935435-917e2400-deda-11e8-9167-d78863c779bc.png)
",yeah sure easy fault example trying load data normalize error,issue,negative,positive,positive,positive,positive,positive
435468688,@MarioProjects  Could you please share a minimal example of the code that reproduces the bug?,could please share minimal example code bug,issue,positive,negative,neutral,neutral,negative,negative
435439169,"
Okay, I understand what you mean, sometimes you do not. I have tried out the compose and the crops work correctly. Now I am testing the Resize (also separately) and it is giving me problems, as well as the Normalize (when using black and white images of 1 channel) and Rotate.

![selection_001](https://user-images.githubusercontent.com/23385329/47928284-1579e100-dec6-11e8-93a9-98faf115795b.png)
![selection_002](https://user-images.githubusercontent.com/23385329/47928291-1874d180-dec6-11e8-87b4-d6a912110a32.png)
![selection_003](https://user-images.githubusercontent.com/23385329/47928295-1a3e9500-dec6-11e8-98de-9e57dad36031.png)
",understand mean sometimes tried compose work correctly testing resize also separately giving well normalize black white channel rotate,issue,positive,negative,negative,negative,negative,negative
435123509,"Hi, it probably happens because all pipeline runs with `probability_apply_transforms` so sometimes you are not doing `Resize`. `Compose` actually has `preprocessing args` so it's better to put this operation there. We are working on better `Compose` API to overcome this issues, stay tuned!",hi probably pipeline sometimes resize compose actually better put operation working better compose overcome stay tuned,issue,positive,positive,positive,positive,positive,positive
434237997,"I found here....



**
class BasicTransform(object):
    def __init__(self, p=0.5):
        self.p = p

    def __call__(self, **kwargs):
        if random.random() < self.p:
            params = self.get_params()
            params = self.update_params(params, **kwargs)
            res = {}
            for key, arg in kwargs.items():
                target_function = self.targets.get(key, lambda x, **p: x)
                target_dependencies = {k: kwargs[k] for k in self.target_dependence.get(key, [])}
                if arg is None:
                    res[key] = None
                else:
                    res[key] = target_function(arg, **dict(params, **target_dependencies))
            return res
        return kwargs
**

It is work!",found class object self self key key lambda key none key none else key return return work,issue,negative,neutral,neutral,neutral,neutral,neutral
431951198,"Reshape operation does not move memory, so it's a zero-cost op. Benchmarks on mine PC (keras commented for purpose)

```
                 albumentations imgaug torchvision keras
RandomCrop64             951791      -       89839     -
PadToSize512               7447      -         466     -
HorizontalFlip             1691   1292        7780     -
VerticalFlip              16789   5490       11992     -
Rotate                     1632   1013         273     -
ShiftScaleRotate           2812   1338         232     -
Brightness                  354    331         681     -
ShiftHSV                    252    148          92     -
ShiftRGB                    415    407           -     -
Gamma                      1465      -        2100     -
Grayscale                  2204    520        1370     -
```",reshape operation move memory mine purpose rotate brightness gamma,issue,negative,neutral,neutral,neutral,neutral,neutral
431655647,Does it do nothing if shape is already fine? It's important to check this to not degrade speed.,nothing shape already fine important check degrade speed,issue,negative,positive,positive,positive,positive,positive
431330832,"Thanks for reporting. Looks like this caused by opencv functions that
silently squeeze output. Will be fixed.

On Fri, Oct 19, 2018, 01:04 Pavel Yakubovskiy <notifications@github.com>
wrote:

> Augmentation squeeze target shape:
>
> import numpy as npimport albumentations
> from albumentations import Flip, Compose
>
> sample = {
>     'image': np.ones((224, 224, 1)),
>     'mask': np.ones((224, 224, 1)),
> }
>
> aug = Compose([Flip(p=0.5)], p=1.)
>
> sample_1 = aug(**sample)
> sample_2 = aug(**sample)
> print(sample_1['mask'].shape)   # (224, 224, 1) - augmentation not appliedprint(sample_2['mask'].shape)   # (224, 224)
>
> Is it possible to fix this behavior?
> Obvious solution - squeeze before aug and expand after. But maybe you have
> another ideas (smth like ShapeGuard)?
>
> —
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/albu/albumentations/issues/104>, or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAgfYH6opBLSx1IP8dLqPvu9G9GZgcWJks5umPsEgaJpZM4XvUmv>
> .
>
",thanks like silently squeeze output fixed wrote augmentation squeeze target shape import import flip compose sample compose flip sample sample print augmentation possible fix behavior obvious solution squeeze expand maybe another like thread reply directly view mute thread,issue,positive,positive,neutral,neutral,positive,positive
431084651,"I believe, cases with and without reflection should be treated differently.",believe without reflection differently,issue,negative,neutral,neutral,neutral,neutral,neutral
430980758,"Yes, that works! Thanks a lot. :)",yes work thanks lot,issue,positive,positive,positive,positive,positive,positive
430975490,"Oh, sorry, actually you can try `ShiftScaleRotate` with `Shift` and `Rotate` both zeros. It should work as you want.",oh sorry actually try shift rotate work want,issue,negative,negative,negative,negative,negative,negative
430972499,"This only resizes the image and is not creating a image that is centered and has border pixels e.g. 0's. But I got it to work another way by first scaling, then padding, then resizing resulting in 
`albumentations.Compose([albumentations.RandomScale(scale_limit=(-0.5, 0.5), p=1, interpolation=1),
                                               albumentations.PadIfNeeded(768, 768, border_mode=cv2.BORDER_CONSTANT),
                                               albumentations.Resize(final_resize, final_resize, cv2.INTER_NEAREST),
                                                   ]):`

Left is the original picture and mid and right show random results.
![image](https://user-images.githubusercontent.com/15069697/47151015-c84c1b80-d2d8-11e8-9672-f1b3028f9187.png)

Maybe there is a more efficient way. imgaug seems to do something different in their scale function.
",image image centered border got work another way first scaling padding resulting left original picture mid right show random image maybe efficient way something different scale function,issue,positive,positive,neutral,neutral,positive,positive
430946090,"There is a weird opencv bug preventing me from running tests locally, i will see what i can do and fix the checks soon.",weird bug running locally see fix soon,issue,negative,negative,negative,negative,negative,negative
430946003,"Hi, yes, it's possible. You can pass tuple to `RandomScale` or `ShiftScaleRotate` methods. https://albumentations.readthedocs.io/en/latest/api/augmentations.html#albumentations.augmentations.transforms.RandomScale",hi yes possible pas,issue,negative,neutral,neutral,neutral,neutral,neutral
430915291,"Yeah, you are right. 

Thanks, we will try to improve API.",yeah right thanks try improve,issue,positive,positive,positive,positive,positive,positive
430775440,"Yes, for example - 
In [13]: %timeit np.ascontiguousarray(arr[:,::-1,...])
40.1 µs ± 685 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)

In [14]: %timeit cv2.flip(arr, 1)
85.4 µs ± 781 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)",yes example per loop mean dev per loop mean dev,issue,negative,negative,negative,negative,negative,negative
430735977,"Hi. 
I'm sorry, but it will not help. According to [`Compolse.__call__()`](https://github.com/albu/albumentations/blob/3567da84a7c091efa884c5773cef2cabeca085a4/albumentations/core/composition.py#L45) `boxes_postprocessing()` is called after `run_transforms_if_needed()` which contains call to `postprocessing_transforms`.  Thus, your solution will not work out. 

But if it will be fixed, it will be quite nice. Maybe multiple explicit examples in docs and notebooks, and mention in `ToTensor()` docstring will be enough.",hi sorry help according call thus solution work fixed quite nice maybe multiple explicit mention enough,issue,positive,positive,neutral,neutral,positive,positive
430726622,"Hi,

Compose method now has preprocessing and postprocessing transforms. https://albumentations.readthedocs.io/en/latest/api/core.html#albumentations.core.composition.Compose so you should use them for `ToTensor`.

I agree that it's not obvious. Do you have ideas?",hi compose method use agree obvious,issue,negative,neutral,neutral,neutral,neutral,neutral
430547264,"From my perspective, most probably not. DALI has they own augmentation pipeline which is not compatible with albumentations API. Therefore support of DALI would require to introduce breaking changes in albumentations with doubtful outcome. ",perspective probably dali augmentation pipeline compatible therefore support dali would require introduce breaking doubtful outcome,issue,negative,negative,negative,negative,negative,negative
429617205,"Good suggestion I think. We are low on free hands, so if you would like to make PR for adding support of building conda packages that would speed up this feature. ",good suggestion think low free would like make support building would speed feature,issue,positive,positive,positive,positive,positive,positive
429616913,Hi. Albumentations uses neither numpy random nor tensorflow random. It relies only on python random numbers generator. So to obtain reproducible augmentations you should fix python random seed. That should be enough to get consistent random numbers across runs.,hi neither random random python random generator obtain reproducible fix python random seed enough get consistent random across,issue,negative,negative,negative,negative,negative,negative
428500023,"Of course, I wrote it here as I was unsure about the feature request etiquette :)",course wrote unsure feature request etiquette,issue,negative,neutral,neutral,neutral,neutral,neutral
428454975,@marcocaccin Could you please make a PR with this change?,could please make change,issue,negative,neutral,neutral,neutral,neutral,neutral
426925504,"I solved it with 2 additional transformations:

```
from albumentations import ImageOnlyTransform
import cv2

class GrayToRGB(ImageOnlyTransform):
    """"""
    Targets:
        image

    Image types:
        uint8, float32
    """"""

    def __init__(self, p=1.0):
        super(GrayToRGB, self).__init__(p)

    def apply(self, img, **params):
        return cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)


class RGBToGray(ImageOnlyTransform):
    """"""
    Targets:
        image

    Image types:
        uint8, float32
    """"""

    def __init__(self, p=1.0):
        super(RGBToGray, self).__init__(p)

    def apply(self, img, **params):
        return cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
```

Shall I add such pull request? Or this is out of normal usage for your library?",additional import import class image image float self super self apply self return class image image float self super self apply self return shall add pull request normal usage library,issue,positive,positive,positive,positive,positive,positive
426904389,"Have the same issue when using RandomContrast and CLAHE.
The reason of problem is `gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)` without checking type of images",issue reason problem gray without type,issue,negative,neutral,neutral,neutral,neutral,neutral
426722548,We mainly focus on installation from pypi. `pip install albumentation`. Version there updates frequently. Installing from git is mainly for developers,mainly focus installation pip install version frequently git mainly,issue,negative,positive,positive,positive,positive,positive
426721600,"No, they are not available, we mainly focus on `Compose` functionality.",available mainly focus compose functionality,issue,negative,positive,positive,positive,positive,positive
424829167,"Hi, @xiching resulting array is a target for pytorch's CrossEntropyLoss - https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss . 

Originally this mask was encoded as multi-channel array, but now I thinks this code is not useful most of the time because masks are usually already encoded in this manner (single-channel)",hi resulting array target originally mask array code useful time usually already manner,issue,negative,positive,positive,positive,positive,positive
424612599,"For now, if the code is already PEP8 complaint we are not planning to enforce any additional specific formatting.",code already pep complaint enforce additional specific,issue,negative,negative,negative,negative,negative,negative
424610982,"Yeap, it can reformat code for appropriate codestyle defined by `.style.yapf`.
I will check its output with 120 right margin rule.",code appropriate defined check output right margin rule,issue,negative,positive,positive,positive,positive,positive
424609769,"As for the output in the `out.txt` file, it seems that `yapf` was run with the default settings for line length (79 symbols) and also it looks like it reformats some valid and PEP8 compatible code to another format, that was chosen by `yapf` authors.",output file run default line length also like valid pep compatible code another format chosen,issue,negative,neutral,neutral,neutral,neutral,neutral
424608056,"Hello. Thank you for the pull request, but we already have a PEP8 style checker, that tests each commit ([we are using Travis-CI for it](https://github.com/albu/albumentations/blob/master/.travis.yml#L26)), also you can run the linter locally by executing `flake8` command.

For now `flake8` doesn't report any PEP8 violations, if you have found any, could you please post them, so we can check the linter?

Note that we deliberately choose to set the maximum line length to 119 symbols as a reasonable compromise, and not 79 as recommended in PEP8. ",hello thank pull request already pep style checker commit also run linter locally flake command flake report pep found could please post check linter note deliberately choose set maximum line length reasonable compromise pep,issue,positive,positive,neutral,neutral,positive,positive
423488398,"Hi.

Thanks for your report. We will investigate this issue. Stay tuned!",hi thanks report investigate issue stay tuned,issue,negative,positive,positive,positive,positive,positive
421671949,"The workaround to use gray images: stack 3 of the same image, to make an img of shape (224,224,3), augment, then take slice of the result
upd. just noticed an official way to do it using custom targets. question is closed, but bug with usage ToGray function in Compose is still open",use gray stack image make shape augment take slice result official way custom question closed bug usage function compose still open,issue,negative,negative,neutral,neutral,negative,negative
420777666,"Yes, I can reproduce this error. Will solve it in nearest future, stay tuned and thank you for bugreport.",yes reproduce error solve nearest future stay tuned thank,issue,positive,neutral,neutral,neutral,neutral,neutral
420735908,"``` 
import cv2
from albumentations import OpticalDistortion, Compose
from urllib.request import urlopen
import numpy as np


def aug_optical_distortion(p=1):
    return Compose([
        OpticalDistortion(p=1)
    ], p=p)


def download_image(url):
    data = urlopen(url).read()
    data = np.frombuffer(data, np.uint8)
    image = cv2.imdecode(data, cv2.IMREAD_COLOR)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    return image


if __name__ == '__main__':
    source_img = download_image('https://d177hi9zlsijyy.cloudfront.net/wp-content/uploads/sites/2/2018/05/11202041/'
                                '180511105900-atlas-boston-dynamics-robot-running-super-tease.jpg')
    aug = aug_optical_distortion()
    augmented_image = aug(image=source_img)['image']
    cv2.imshow('Source image', source_img)
    cv2.imshow('Augmented image', augmented_image)
    cv2.waitKey() 
```

```
In [1]: import albumentations
In [2]: albumentations.__version__
Out[2]: '0.0.12'

In [3]: import cv2
In [4]: cv2.__version__
Out[4]: '3.4.3'

```",import import compose import import return compose data data data image data image image return image image image import import,issue,negative,neutral,neutral,neutral,neutral,neutral
420730558,can you please send full snippet - from image reading to getting result from augmentations? thanks,please send full snippet image reading getting result thanks,issue,positive,positive,positive,positive,positive,positive
415788617,"Very good. I have written a class that can be used in pytorch. Feel free to correct it
```
class ImageAug:
    def __init__(self, image_size, p):
        self.aug=Compose([
            RandomCrop(image_size, image_size),
            Flip(),
                Transpose(),
                OneOf([
                    IAAAdditiveGaussianNoise(),
                    GaussNoise(),
                ], p=0.1),
                OneOf([
                    MotionBlur(p=.2),
                    MedianBlur(blur_limit=3, p=.1),
                    Blur(blur_limit=3, p=.1),
                ], p=0.1),
                ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=20, p=.2),
                OneOf([
                    OpticalDistortion(p=0.3),
                    GridDistortion(p=.1),
                    IAAPiecewiseAffine(p=0.3),
                ], p=0.1),
                OneOf([
                    CLAHE(clip_limit=2),
                    RandomContrast(),
                    RandomBrightness(),
                ], p=0.3),
                HueSaturationValue(p=0.3),
            ], p=p)

    def __call__(self, img):
        img = np.array(img)
        img = self.aug(image=img)['image']
        return Image.fromarray(img)

```
In pytorch, it is
```
torchvision_transform = transforms.Compose([
    transforms.Scale((500, 500)),
    ImageAug (224,0.5)
    transforms.RandomHorizontalFlip()
])
```",good written class used feel free correct class self flip transpose blur self return,issue,positive,positive,positive,positive,positive,positive
415782660,"@John1231983 In #51 we added Resize augmentation, I have made an example notebook that shows how you can use albumentations instead of torchvision for resizing and cropping images - https://gist.github.com/creafz/a364b21d8625143aa5691e3f7f0347d5

You can install the latest version with Resize augmentation from GitHub: `pip install -U git+https://github.com/albu/albumentations`, we will upload the new version to PyPI soon.",added resize augmentation made example notebook use instead install latest version resize augmentation pip install new version soon,issue,negative,positive,positive,positive,positive,positive
415414644,"Yes, you can. 

Albumentations work with numpy arrays, hence you will probably need to do the following steps:

```
# Convert PIL image to numpy array
image_np = np.array(image)

# Apply transformations
augmented = transform(image=image_np)

# Convert numpy array to PIL Image
image = Image.fromarray(augmented[""image""])
```",yes work hence probably need following convert image array image apply augmented transform convert array image image augmented image,issue,negative,neutral,neutral,neutral,neutral,neutral
415374577,"Great. If my data loader uses PIL to read image, can I use your method for augmentation? I am working on classification problem",great data loader read image use method augmentation working classification problem,issue,negative,positive,positive,positive,positive,positive
414443583,"This pipeline may serve as an example:

https://github.com/ternaus/robot-surgery-segmentation/blob/master/dataset.py#L22",pipeline may serve example,issue,negative,neutral,neutral,neutral,neutral,neutral
414042249,"Out of the box this library supports image augmentation only by one (probably with mask). If you want to augment a lot of connected images in absolutely the same way - consider writing your own targets but with same processing function.

In case you want to augment batch of images in different way - just use augmentations in `Dataset` and fill this `Dataset` to `DataLoader`.",box library image augmentation one probably mask want augment lot connected absolutely way consider writing function case want augment batch different way use fill,issue,negative,positive,neutral,neutral,positive,positive
408190836,Let's just go with a warning for now and add larger kernel sizes to the TODO list.,let go warning add kernel size list,issue,negative,neutral,neutral,neutral,neutral,neutral
407652702,"For now, we don't use the `Image types` section for any tests.

I think the best way to add a warning it to use a `Note` section header:

```
	...
        Image types:
            uint8, float32

        Note:
            Warning message.
```

It will be displayed under the `Image types` section in the docs:
![2018-07-25_10-49-32](https://user-images.githubusercontent.com/681989/43184161-abbbf5c6-8ff8-11e8-854e-29c9766633c7.png)
",use image section think best way add warning use note section header image float note warning message displayed image section,issue,negative,positive,positive,positive,positive,positive
407649943,"@creafz done! I'm not sure if I should add a warning about worse performance on float32 into the docstring as the `Image types` may be used for some tests/CI purposes in future, and free speech may break it - please advice.",done sure add warning worse performance float image may used future free speech may break please advice,issue,negative,positive,positive,positive,positive,positive
407645785,Could you please also update the docstring for `JpegCompression`  and add `float32` as a supported image type? https://github.com/albu/albumentations/blob/master/albumentations/augmentations/transforms.py#L433,could please also update add float image type,issue,negative,neutral,neutral,neutral,neutral,neutral
407456013,"@arsenyinfo At a first glance, it looks like cv2.filter2D applies only linear filters to an image and won't help with implementing a median filter which is a nonlinear one.

Also I found this answer about larger kernel sizes, but it suggests making own implementation of the medianBlur function: http://answers.opencv.org/question/35431/median-filter-greater-than-5/?answer=35516#post-id-35516",first glance like linear image wo help median filter nonlinear one also found answer kernel size making implementation function,issue,positive,positive,positive,positive,positive,positive
407429200,"A pull request that adds support for float inputs will be greatly appreciated. 

I checked `imencode` and it looks like it converts all non-uint8 inputs to uint8 [[source]](https://github.com/opencv/opencv/blob/master/modules/imgcodecs/src/loadsave.cpp#L912-L918).

I think the following steps could be used to work with float inputs in `jpeg_compression`:
1. Multiply the input by 255 and cast it to uint8.
2. Apply cv2.imencode and cv2.imdecode functions as before.
3. Cast the output of cv2.imdecode back to float, divide all values by 255 and return the resulting image.

However, I could be mistaken and maybe there is a better way of handling float inputs.

As for checking outputs and their data types, we have some tests in [tests/test_augmentations.py](https://github.com/albu/albumentations/blob/master/tests/test_augmentations.py) that verify output values of augmentations. `jpeg_compression` is not tested with float inputs because in the latest version it should raise an exception if the data type of the input does not equal np.uint8 https://github.com/albu/albumentations/blob/master/albumentations/augmentations/functional.py#L216",pull request support float greatly checked like source think following could used work float multiply input cast apply cast output back float divide return resulting image however could mistaken maybe better way handling float data verify output tested float latest version raise exception data type input equal,issue,positive,positive,positive,positive,positive,positive
407129953," 1. I support this. 
 2. I think to limit the functionality of the medianBlur because the functionality of the OpenCV would be unwise. We should probably check kernel size and use ImgAug instead of OpenCV in that case.",support think limit functionality functionality would unwise probably check kernel size use instead case,issue,negative,neutral,neutral,neutral,neutral,neutral
407068320,"I think there are 2 reasons why those errors appear:

1. `np.random.rand` generates ndarrays with the float64 data type. This data type is not supported by `cv2.medianBlur` which is used internally in `MedianBlur`.
2. If the input type is not np.uint8, the only allowed ksize values for `cv2.medianBlur` are 3 and 5.
[Link to the OpenCV docs](https://docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9)

This code that uses float32 inputs and ksize values 3 and 5 work without errors for me:

```
from albumentations import MedianBlur
import numpy as np
success, fail = 0, 0
for _ in range(100):
    img = np.random.rand(100, 100)
    img = img.astype('float32') # Cast a float64 image to float32
    try:
        # Use blur_limit 5 instead of default 7. So this augmentation will
        # apply `cv2.medianBlur` using ksize=3 or ksize=5.
        img = MedianBlur(blur_limit=5)(image=img).get('image')
        success += 1
    except Exception:
        fail += 1
print(success, fail)
``` 

I am afraid as long as we are using OpenCV functions there is nothing we can do to support float64 inputs and ksize values larger than 5.

Maybe we should check the data type of input images in all our augmentations and raise an exception if the input data type is not supported, so users won't get those cryptic OpenCV errors? @albu @ternaus what do you think?",think appear float data type data type used internally input type link code float work without import import success fail range cast float image float try use instead default augmentation apply success except exception fail print success fail afraid long nothing support float maybe check data type input raise exception input data type wo get cryptic think,issue,negative,negative,negative,negative,negative,negative
406777210,"In 6bf45c9 we've added support for non-8-bit images. Now almost all augmentations support uint8 and float32 data types as inputs (in the [docs](https://albumentations.readthedocs.io/en/latest/api/augmentations.html) each augmentation has a section 'Image types' that describes which input data types are supported). 

In a float32 image all values must lie in the range [0.0, 1.0]. So to work with 16-bit images you first need to convert an image to a float data type (you can use a helper function [`ToFloat`](https://github.com/albu/albumentations/blob/master/albumentations/augmentations/transforms.py#L768)) and then apply required augmentations.

There is an example notebook [notebooks/example_16_bit_tiff.ipynb](https://github.com/albu/albumentations/blob/master/notebooks/example_16_bit_tiff.ipynb) which shows how augmentations can be applied to a 16-bit TIFF image.

This version of the library is not yet uploaded to PyPI, but you can install it from GitHub: `pip install -U git+https://github.com/albu/albumentations` ",added support non bit almost support float data augmentation section input data float image must lie range work first need convert image float data type use helper function apply example notebook applied tiff image version library yet install pip install,issue,positive,positive,positive,positive,positive,positive
405578014,"Could you try to replace these 2 lines:
```
image = np.ones((300, 300))
mask = np.ones((300, 300))
```

with this:

```
image = np.ones((300, 300, 3), dtype=np.uint8)
mask = np.ones((300, 300), dtype=np.uint8)
```

and see whether the errors still occur?

By default, np.ones returns a ndarray with the float64 data type, and some augmentations may work incorrectly with it. For now, the library fully supports uint8 inputs, but we are working on adding support for float input types, and I think we will release it soon.",could try replace image mask image mask see whether still occur default float data type may work incorrectly library fully working support float input think release soon,issue,negative,neutral,neutral,neutral,neutral,neutral
404754836,"Yeah, it add some noise in the hole instead of set to zero",yeah add noise hole instead set zero,issue,negative,neutral,neutral,neutral,neutral,neutral
404741733,"It looks like a cutout, but with one hole.",like cutout one hole,issue,negative,neutral,neutral,neutral,neutral,neutral
404223558,"Devil is in the details. 

Feel free to run the benchmark and compare the code of the corresponding augmentations.",devil feel free run compare code corresponding,issue,negative,positive,positive,positive,positive,positive
